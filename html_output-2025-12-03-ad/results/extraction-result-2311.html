<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2311 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2311</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2311</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-259951225</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.09072v2.pdf" target="_blank">Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)</a></p>
                <p><strong>Paper Abstract:</strong> Extrapolation remains a grand challenge in deep neural networks across all application domains. We propose an operator learning method to solve time-dependent partial differential equations (PDEs) continuously and with extrapolation in time without any temporal discretization. The proposed method, named Diffusion-inspired Temporal Transformer Operator (DiTTO), is inspired by latent diffusion models and their conditioning mechanism, which we use to incorporate the temporal evolution of the PDE, in combination with elements from the transformer architecture to improve its capabilities. Upon training, DiTTO can make inferences in real-time. We demonstrate its extrapolation capability on a climate problem by estimating the temperature around the globe for several years, and also in modeling hypersonic flows around a double-cone. We propose different training strategies involving temporal-bundling and sub-sampling and demonstrate performance improvements for several benchmarks, performing extrapolation for long time intervals as well as zero-shot super-resolution in time.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2311.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2311.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiTTO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diffusion-inspired Temporal Transformer Operator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural operator architecture that conditions a U-Net on continuous time via transformer-style time embeddings and diffusion-model-inspired conditioning to predict solutions of time-dependent PDEs continuously in time and perform temporal extrapolation and super-resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Time-dependent partial differential equations (PDEs): fluid dynamics (Burgers, Navier–Stokes, hypersonic flow), acoustic wave propagation, and climate surface-temperature forecasting</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn an operator mapping from initial conditions to the PDE solution at arbitrary future times (u(x,0) → u(x,t)) for many initial conditions, enabling continuous-in-time inference, temporal extrapolation, zero-shot temporal super-resolution, and real-time inference across multiple PDE domains.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Varies by problem: abundant/moderate for climate (multi-year daily grid data, training 2013–2015 with 1095 snapshots used for training/validation/test splits), limited for hypersonic flow (61 trajectories over Mach range), moderate-to-large for synthetic PDE datasets (e.g., 1000 trajectories with 200 timesteps for Navier–Stokes extrapolation experiments); data are labeled (numerical solver outputs) and publicly or synthetically generated depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured spatio-temporal grid data (tensors) — spatial grids (1D/2D/3D) concatenated with spatial coordinates and scalar temporal query; effectively high-dimensional time series / image-like fields over time; for some variants, unstructured point sets projected to N×d coordinate arrays.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: non-linear PDEs, multi-scale interactions, high gradients and shocks (hypersonic flows, high-Re Navier–Stokes), high-dimensional spatial grids (2D/3D+time), need for long-term extrapolation and temporal super-resolution; computational complexity arises from training on many initial conditions and fine temporal discretizations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature numerical PDE and climate modeling domains with well-established solvers and physics knowledge, but challenging due to multi-scale nonlinear dynamics and expensive conventional solvers; SciML/operator learning is an active emerging subfield applied to these mature domains.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium-to-high — scientific validity and robustness are important (e.g., capturing shocks, conservation properties, physical plausibility), but the approach uses data-driven black-box operators; interpretability is aided by operator formulation and conditioning but not inherently mechanistic.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>DiTTO neural operator (U-Net backbone with temporal transformer-style conditioning and attention blocks; diffusion-inspired conditioning)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Architecture: U-Net (Wide ResNet backbone) with spatial and channel attention blocks, ResNet blocks augmented by time-conditioning via a time-embedding MLP (sinusoidal positional encodings → d_emb vector → two-layer MLP with GELU) applied as channel-wise multiplicative conditioning. Training objective: supervised mean relative L2 loss mapping (initial condition, query time) → state at that time. Training strategies include temporal-bundling (predicting look-forward windows), sub-sampling (DiTTO-s), and variants DiTTO-point (1D convs with spatial positional encodings to handle point clouds/unstructured grids) and DiTTO-gate (gated skip connections to emphasize high-frequency features). The architecture repurposes diffusion-model conditioning (conditioning on time rather than noise) and uses attention modules; inference is continuous in time and immediate (no autoregressive rollout required).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised operator learning (neural operator), hybrid of diffusion-inspired conditioning and transformer-style conditioning (deep learning for scientific computing)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and appropriate: designed for time-dependent PDE operator learning where training datasets of solution trajectories exist; requires adaptation (time-conditioning, bundling, point variant) for high-dimensional/unstructured domains; computationally efficient at inference (real-time) but can be training-intensive for fine temporal discretizations without sub-sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Reported relative L2 errors vary by problem: climate five-year extrapolation mean relative L2 ≈ 0.014 (1.4%); Burgers (ν=0.01) DiTTO relative L2 ≈ 0.0058–0.0060 (N_test_t range); Navier–Stokes Re≈20 DiTTO relative L2 ≈ 0.029–0.30 depending on test temporal resolution and dataset; Hypersonic flow (no noise) DiTTO ≈ 0.0303, DiTTO-point ≈ 0.0152 (see Table 2); for 2D/3D acoustic wave problems DiTTO/DiTTO-s achieved the lowest errors (~0.009–0.015 reported). (Numbers are reported as relative L2 errors from the paper's tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>DiTTO generally outperforms or matches state-of-the-art neural operators (FNO) and U-Net baselines on interpolation, extrapolation, and zero-shot temporal super-resolution; demonstrates superior temporal extrapolation when trained with temporal-bundling; preserves accuracy across changes in temporal discretization (meshfree in time); demonstrates robustness to noise in hypersonic flow (DiTTO-point especially robust); attention and temporal conditioning materially improve performance (ablation: removing attention roughly doubled error). Limitations include higher training data / compute needs for some problems and sensitivity to hyperparameters and training strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High: enables continuous-time operator inference for expensive PDE simulations, long-term forecasts (five-year climate forecasts demonstrated), real-time inference for control/autonomy settings, zero-shot temporal super-resolution that can reduce need for fine temporal simulation grids, and noise-robust surrogates for high-gradient flows; generalizes to multiple scientific PDE domains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared experimentally against Fourier Neural Operator (FNO) and U-Net baselines across many benchmarks. DiTTO (and its variants) typically achieve lower relative L2 errors, particularly for temporal extrapolation and zero-shot super-resolution; DiTTO-point is most accurate in hypersonic flow tests and most noise-robust; FNO sometimes competitive on smooth/periodic cases but worse for non-periodic/boundary-driven wave problems and extrapolation; U-Net often performs poorly on high-dimensional/time-dependent problems or requires large memory.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Key factors: diffusion-inspired time-conditioning (conditioning on continuous time), transformer-style positional/time embeddings, spatial and channel attention blocks, temporal-bundling training strategy (reduces error accumulation and improves extrapolation), sub-sampling (DiTTO-s) for scalability and regularization, and specialized variants (DiTTO-point, DiTTO-gate) to address unstructured domains and high-frequency features.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Conditioning a multi-scale U-Net on a learned continuous time embedding—inspired by diffusion-model conditioning and enhanced with attention and training strategies like temporal-bundling and sub-sampling—yields a neural operator that is continuous in time, excels at temporal extrapolation and super-resolution, and outperforms FNO/U-Net baselines on challenging PDE problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2311.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2311.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiTTO-s</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DiTTO with sub-sampling (fast variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computationally faster DiTTO variant that trains on random sub-sequences (sub-sampling) of the time trajectory each epoch to reduce training cost and act as regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Same PDE domains as DiTTO (Burgers, Navier–Stokes, wave propagation)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Same operator-learning target but optimized to reduce training time and memory footprint when temporal discretization is fine by using random temporal sub-sampling during training.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Same as DiTTO; method designed to work even when many temporal samples exist by sampling a fraction α of timesteps per epoch (e.g., α=0.05–0.2 experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Spatio-temporal grid data; training uses sub-sampled timesteps per trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Same as DiTTO; alleviates complexity of scaling linearly with number of timesteps during training.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>As DiTTO.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — same considerations as DiTTO.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>DiTTO-s (sub-sampled training variant)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Selects random subsequences S_m ⊂ {0,...,T} for each initial condition during each epoch such that overall sampling fraction α<1 (e.g., α=0.05 or 0.1). Loss computed only on these sampled timesteps; subsequences re-sampled each epoch. Acts as computational acceleration and regularizer.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised operator learning with stochastic sub-sampling regularization</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate when temporal resolution is fine and training cost is high; works well when full coverage over many epochs is acceptable.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Often achieves comparable or slightly better errors than full DiTTO; e.g., for Burgers ν=0.01 DiTTO-s relative L2 ≈ 0.0055–0.0057 vs DiTTO ≈ 0.0058–0.0060; ablation showed α ≈ 0.1–0.2 gave best results.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>DiTTO-s retains accuracy while reducing training samples per epoch and providing regularization benefits; too small α (e.g., 0.05) can degrade performance.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Improves scalability of DiTTO to fine temporal discretizations and reduces memory, enabling training where full temporal coverage per epoch is infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to full DiTTO and FNO/U-Net; DiTTO-s often matched or slightly improved DiTTO and outperformed baselines on many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Random temporal sub-sampling (α choice), re-sampling each epoch providing regularization, and synergy with temporal-bundling training.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Sub-sampling temporal training data reduces cost and acts as effective regularization, often improving DiTTO generalization while scaling training to fine time discretizations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2311.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2311.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiTTO-point</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DiTTO point-cloud (1D convolution) variant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-efficient variant of DiTTO that treats spatial domains as point sets (N×d coordinates) and uses 1-D convolutions with spatial positional encodings to handle unstructured or high-dimensional spatial data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Hypersonic flow around double-cone, and other high-dimensional PDEs where structured grids are impractical</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn mappings between fields defined on unstructured point sets or large high-dimensional grids by treating data as N×d coordinate matrices and preserving spatial information through positional encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Typically limited for CFD experiments (e.g., 61 trajectories for hypersonic dataset), and synthetic/numerical simulation data for other PDEs.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Point cloud / unstructured spatial coordinates N×d paired with solution vectors of length N, plus temporal scalar conditioning — effectively high-dimensional, unstructured spatio-temporal data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: hypersonic flows with shocks and vortical structures, complex geometry; limited sample sizes increase difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature CFD domain; surrogate modeling for such geometries remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High for engineering trustworthiness; DiTTO-point aims for accurate surrogate predictions though interpretability remains limited.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>DiTTO-point (1D convolutional neural operator with spatial positional encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Projects domain to N×d matrix of coordinates and solution values, applies spatial positional encoding to coordinates (sinusoidal), concatenates with initial condition channel-wise, and uses 1-D convolutions so memory use is independent of physical dimension; time-conditioning same as DiTTO.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised operator learning adapted for unstructured data</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for unstructured domains or memory-limited settings where full d-dimensional convolutions are infeasible; demonstrated particularly successful for hypersonic flow and noise robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Hypersonic flow: DiTTO-point relative L2 ≈ 0.0152 (no noise) outperforming DiTTO (0.0303) and FNO (0.0241). Noise robustness: DiTTO-point maintains low error up to 100% noise levels (relative L2 ≈ 0.054 at 100% noise) compared to FNO (0.608 at 100% noise).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Highly effective and robust to input noise and small datasets in the hypersonic case; provides competitive or superior accuracy to full DiTTO and FNO for geometry/point-cloud settings.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Enables operator learning on unstructured geometries and memory-limited hardware, promising for CFD surrogate modeling, design, and uncertainty quantification in engineering contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperformed FNO and full DiTTO on the hypersonic dataset and showed superior noise robustness; comparable errors to DiTTO in many other PDE benchmarks while using less memory.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Preservation of spatial coordinate information via positional encoding, dimensionality reduction to 1D convolutions, and shared temporal conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Reducing spatial convolutions to a 1D point-based representation with positional encodings allows neural operators to scale to unstructured/high-dimensional problems while improving robustness to noise and limited data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2311.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2311.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiTTO-gate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DiTTO with gated skip-connections</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A DiTTO variant that modifies U-Net decoder skip connections by adding gated convolutional blocks to emphasize or filter high-frequency features, useful for problems with fine details and sharp features (e.g., high-Re flows).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>High-gradient PDEs: Navier–Stokes with high Reynolds number, problems exhibiting sharp features</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Improve recovery of high-frequency / sharp spatial features by gating skip connections to control information passed from encoder to decoder levels.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Applies where training datasets capture high-frequency dynamics; datasets for high-Re Navier–Stokes used in experiments (various sizes up to 5000 samples for some tests).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured spatio-temporal grid data with high spatial gradients and fine-scale features.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High complexity due to turbulence-like features and shocks, requiring ability to represent multi-scale spatial detail.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature fluid dynamics domain; surrogate modeling for turbulent/high-Re flows remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — capturing physically-consistent high-frequency features is important for fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>DiTTO-gate (gated skip-connection U-Net variant)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Adds a convolutional gating block on each decoder skip connection (convolutional block applied to skip input) to modulate high-frequency contributions into the decoder; combined with temporal conditioning and attention as in DiTTO.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised operator learning with architectural modification to enhance high-frequency feature learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Useful for PDEs with sharp spatial features where skip-connections may overly inject high-frequency detail; requires tuning but demonstrated effective.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Reported to help on high-feature problems (e.g., some DiTTO-point-s-gate entries in tables show comparable or improved relative L2 over non-gated variants; specific numbers vary by task; e.g., some entries show errors ~0.13–0.14 in challenging Navier–Stokes tests).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Improves ability to capture fine spatial details in problems with sharp features; provides an architectural lever to balance high-frequency injection from skip connections.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Improves surrogate fidelity for challenging high-gradient PDEs, beneficial for engineering simulations where sharp spatial accuracy matters.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared within DiTTO variants; gating improved or stabilized results for high-Re/sharp-feature problems versus vanilla DiTTO in some benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Targeted modification of skip connections to control high-frequency content, combined with temporal conditioning and attention.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Architectural gating on skip connections helps manage high-frequency feature flow in U-Net-based neural operators, improving fidelity for sharp-feature PDEs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2311.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2311.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diffusion models (DDPM-inspired conditioning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Denoising Diffusion Probabilistic Models (inspiration and conditioning mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generative diffusion models are used conceptually: DiTTO repurposes diffusion-model conditioning by replacing the noise-level conditioning with continuous temporal conditioning to map initial states to time-evolved states.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Denoising diffusion probabilistic models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Methodological adaptation for general time-dependent PDE operator learning across multiple physics domains (climate, fluid dynamics, waves)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Rather than perform iterative denoising of corrupted samples, use the diffusion-model idea of conditioning on a 'level' — here the temporal scalar — to condition a network to predict the state at different time 'levels' from the initial condition.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Not data-specific; applicable when labeled trajectory data exist.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Spatio-temporal fields (gridded tensor data) with discrete timesteps used to generate training pairs (initial condition, time) → state.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Conceptual complexity moderate: casting continuous-time operator learning into conditioning framework; computational complexity moved to training for mapping arbitrary t rather than iterative sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Diffusion models are a mature generative ML class; adaptation to SciML is emerging.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium: approach is a modeling trick rather than physics-informed; interpretability not inherent but mapping is conditioned on physical time.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Diffusion-inspired temporal conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Repurposes diffusion training concept: instead of conditioning on noise level ε (as in DDPM), condition on time scalar t (embedded via positional encodings and MLP) and train the network to map (x0, t) → xt. There is no denoising in practice because xt are true states; the conditioning nonetheless helps disentangle the mapping across time levels.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning with diffusion-model-inspired conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for continuous-time operator learning; enables instantaneous inference at arbitrary times without autoregressive rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Indirect: underpins DiTTO's continuous-time capabilities and contributes to reported errors (see DiTTO metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>The diffusion-inspired conditioning is central to DiTTO's ability to produce continuous-in-time predictions and to avoid iterative autoregressive accumulation of errors.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Introduces a transferable conditioning mechanism enabling continuous-time neural operators applicable across PDE domains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Alternative is explicit autoregressive training (predict next timestep) or mapping to fixed timesteps; diffusion-inspired conditioning avoids extreme error accumulation from autoregression and overfitting to training timesteps.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Using continuous scalar conditioning analogous to noise-level conditioning, combined with strong time embeddings and architecture that uses the conditioning multiplicatively in ResNet blocks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Replacing diffusion noise-level conditioning with a time scalar enables a continuous-time conditional operator that avoids autoregressive error accumulation and supports temporal super-resolution/extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2311.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2311.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temporal-bundling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporal-bundling (training strategy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training strategy where the network is trained to predict multiple-step look-forward windows (bundles) rather than purely next-step autoregression or full mapping, providing a balance that reduces error accumulation and improves temporal extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Time-series operator learning for PDEs (used for Navier–Stokes extrapolation and climate forecasting tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Enable extrapolation beyond training time interval by training the model to map u(x,t) → u(x,t+lf) for various look-forward windows lf and splitting trajectories into sub-trajectories, improving stability and lowering accumulated error.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires labeled trajectories over time; used on datasets with many timesteps (e.g., Navier–Stokes 1000 trajectories × 200 timesteps; climate 1095 snapshots).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Spatio-temporal grid sequences; training reshaped into sub-trajectories of length lf.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate to high: mitigates tradeoffs between autoregressive accumulation and overfitting to fixed timesteps.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Technique referenced from prior work and adapted here; applicable across operator learning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low — technique is an empirical training strategy to improve generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Temporal-bundling training</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>During training, split each trajectory into overlapping sub-trajectories of length lf and train the operator to map current state/time to the state at t+lf; experiments sweep lf values (1,5,10,20,50,100) and find a sweet spot (e.g., lf=20 in NS experiment, lf=365 for climate) that minimizes extrapolation error.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised training strategy for time-series/operator learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for improving temporal extrapolation and stability in learned dynamical operators; choice of lf influences bias-variance trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Example: in Navier–Stokes extrapolation experiment (trained on first 100 timesteps), lf=20 achieved minimum error and lowest uncertainty when extrapolating to 200 timesteps; for climate, lf=365 was used to split 1095-day trajectory into 730 sub-trajectories for five-year extrapolation with mean L2 error 0.014.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Offers a compromise between pure autoregression (lf=1) which accumulates error and full mapping (lf=nt) which overfits; temporal-bundling reduces error accumulation and uncertainty in long-horizon extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Improves reliability of data-driven long-term forecasts and extrapolation in scientific time-dependent problems; general strategy applicable to many operator learning frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to autoregressive training and full mapping; temporal-bundling provided lower accumulated error and uncertainty across tested lf choices.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Appropriate selection of look-forward window lf, combination with DiTTO continuous-time conditioning, and sufficient trajectory diversity in training data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Training to predict intermediate look-forward windows exposes the operator to intermediate states and reduces both error accumulation (vs autoregression) and overfitting to fixed timesteps (vs mapping), improving extrapolation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2311.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2311.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer / Attention blocks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-style attention (spatial and channel attention blocks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Scaled dot-product attention modules integrated in DiTTO (both spatial and channel-wise) to extract long-range spatial correlations and inter-channel relationships within the U-Net backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Attention is all you need</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>General spatio-temporal PDE modeling across problems (Burgers, Navier–Stokes, waves, climate, hypersonic flow)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Enable the neural operator to learn nonlocal spatial correlations and cross-channel dependencies that are important for representing complex PDE solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires typical labeled spatio-temporal PDE solution datasets; no special additional data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Grid/image-like spatial representations where attention can operate on pixel/token patches or channels.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Attention helps address high complexity due to long-range dependencies and multi-scale interactions in PDE solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Transformers are well-established in ML; their adoption in SciML is growing.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — attention modules improve representational capacity but do not enforce mechanistic constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Scaled dot-product attention (spatial and channel-wise)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Implement Q,K,V projections via 1×1 convolutions to produce attention matrices for channel and spatial attention; out = softmax(QK^T / sqrt(d)) V used inside U-Net levels to capture nonlocal correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Architectural enhancement within supervised deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for problems with significant long-range dependencies or when cross-channel interactions matter; scales with memory and compute.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Ablation: removing attention roughly doubled errors in Burgers test (DiTTO with attention relative L2 ≈ 0.0060 vs DiTTO without attention ≈ 0.0132 across tested N_t values).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Attention materially improves accuracy and ability to capture spatial correlations, especially on problems with nonlocal interactions; contributes to DiTTO's superior performance.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Increases fidelity of neural operators on multi-scale PDEs and supports generalization across initial conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared ablation-wise to identical DiTTO without attention; attention substantially reduced error.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Appropriate placement of attention (spatial and channel blocks at multiple scales), and integration with temporal conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Integrating transformer-style attention into multiscale U-Net operators significantly improves representation of nonlocal spatial correlations, halving errors on tested PDEs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2311.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2311.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FNO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fourier Neural Operator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural operator that parameterizes integral kernels in Fourier space to learn mappings between function spaces, used here as a baseline comparison for DiTTO across several PDE benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fourier neural operator for parametric partial differential equations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Operator learning for PDEs — Burgers, Navier–Stokes, wave propagation, hypersonic flow (used as baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn mappings from initial conditions to solution fields using Fourier-domain convolution layers (global spectral representation), typically effective on smooth and periodic problems.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Used with the same datasets as DiTTO for direct comparison (synthetic solver outputs and public climate/hypersonic datasets); requires labeled trajectory data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured grid/tensor data amenable to Fourier transforms (works best with periodic boundary conditions but can be adapted).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Performs well on smooth/periodic PDEs but can struggle with boundary-driven or sparse-featured data; complexity increases with required number of Fourier modes for high detail.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established and widely-used neural operator method in SciML.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low — spectral representation provides interpretability in frequency domain but not explicit physics constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Fourier Neural Operator (FNO)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Parametrizes a global kernel in Fourier domain via learned multipliers on Fourier modes across layers; applied with hyperparameter search over number of modes and depth; comparisons conducted on same train/test splits and temporal discretizations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised operator learning (spectral approach)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for many PDE operator tasks, especially smooth/periodic settings; less suited for non-periodic Dirichlet problems and some high-gradient/sparse data where DiTTO outperforms.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Examples from tables: FNO relative L2 often higher than DiTTO on extrapolation/super-resolution tasks (e.g., Burgers and wave problems); hypersonic noise experiment FNO errors increase rapidly with noise (0%: 0.0241; 100% noise: 0.6080) compared to DiTTO variants.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>FNO is competitive on smooth/periodic cases and can be tuned via Fourier modes/layers; however, it is more sensitive to noise, boundary conditions, and temporal discretization changes than DiTTO; computational cost increases with added temporal dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Continuing baseline and useful method for many operator learning problems; limitations motivate DiTTO-style conditioning for temporal continuity and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Benchmarked extensively in paper; DiTTO variants outperform FNO on extrapolation, boundary-driven wave problems, temporal super-resolution, and noise robustness in hypersonic flows.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Global spectral representation, careful hyperparameter tuning (number of modes, layers); struggles when periodicity assumptions or spectral smoothness don't hold.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>FNO is strong for smooth/periodic PDEs but has limitations for temporal extrapolation, non-periodic/boundary-dominated problems, and noisy/high-gradient scenarios where conditional, attention-based operators like DiTTO excel.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2311.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2311.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>U-Net baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>U-Net with (or without) temporal conditioning / attention (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Convolutional encoder–decoder architecture used as a baseline (sometimes with attention), typically predicting discrete timesteps and lacking continuous-time conditioning; included for comparison against DiTTO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>U-net: Convolutional networks for biomedical image segmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Benchmarked across the same PDE problems (Burgers, Navier–Stokes, wave, hypersonic) as a simpler CNN baseline</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict solution fields at discrete timesteps from input initial conditions (no explicit continuous-time conditioning in baseline runs).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Same labeled PDE datasets; U-Net experiments often limited by memory for 3D time-dependent problems.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured spatial grids treated as images per timestep; lacks explicit temporal conditioning so discrete-time outputs are produced.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>U-Net struggles with high-dimensional time-dependent problems due to memory and inability to scale to continuous-time inference.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established image-to-image deep learning architecture; used widely as baseline in SciML.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low — black-box predictions; interpretability limited.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>U-Net (convolutional encoder–decoder baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Standard U-Net architecture (sometimes with attention) trained to map initial-condition input to solution at discrete timesteps; temporal conditioning removed in baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised convolutional learning baseline</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable for some PDE tasks with modest spatial/temporal size but not suitable for continuous-in-time extrapolation or large 3D+time problems due to memory constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Often substantially worse than DiTTO and FNO on many tasks; e.g., for some 2D wave problems U-Net relative L2 reported ≈ 1.39 vs DiTTO ≈ 0.009–0.03 in various settings (see tables).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>U-Net can produce reasonable results for certain fixed-timestep tasks but fails to scale to high-dimensional/time-dependent problems and temporal super-resolution; large memory footprint precluded training on some 3D tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Limited as a general-purpose operator learner for continuous-time PDE problems; still useful as simple baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Underperformed compared to DiTTO and often to FNO, especially on extrapolation and high-dimensional cases.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Effective for image-like tasks and when only discrete timesteps are needed; fails when continuous-time conditioning and scalability are required.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Standard U-Net baselines lack the temporal conditioning and architectural enhancements needed for robust continuous-time operator learning and extrapolation in challenging PDE problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2311.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2311.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepONet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Operator Network (DeepONet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An operator learning architecture (branch and trunk networks) mentioned and briefly experimented with; in this paper it required too much data / resources for multi-dimensional time-dependent problems and was ultimately not included in final comparisons for multidimensional tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning nonlinear operators via Deeponet based on the universal approximation theorem of operators.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Operator learning for time-dependent PDEs (attempted as alternative operator architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Map input functions (initial conditions) to output functions (solutions) via branch and trunk networks; applied to time-dependent problems in attempt to learn mappings across initial conditions and time.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Paper reports DeepONet required a very large amount of examples for time-dependent 1D problems and was infeasible for multidimensional tasks given hardware constraints; thus data requirements were judged high.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Functional input/output mapping; typically needs many function samples and coordinate evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High sample complexity for time-dependent multidimensional operator learning; memory/computational demand grows with problem dimension.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established operator learning approach with theoretical foundations; practical scaling to high-dimensional time-dependent PDEs remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — operator-focused architecture useful for interpretability of mapping but still data-driven.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>DeepONet (branch & trunk operator network)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Standard DeepONet with branch network taking function samples and trunk network taking coordinates; authors attempted training but removed examples due to high sample/compute needs.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised operator learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Potentially applicable but limited by required training data and compute for time-dependent multidimensional problems in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Authors found DeepONet required very large datasets to converge for time-dependent 1D tasks and was infeasible for multidimensional cases on available hardware, so it was not compared in final benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>DeepONet remains promising in theory but practical scaling to large time-dependent multidimensional PDE datasets is a current challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Not benchmarked in final comparisons due to practical constraints; authors focused comparisons on DiTTO, FNO and U-Net.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Strong theoretical operator approximation capability; success hindered by sample complexity and hardware limits in time-dependent multidimensional settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>DeepONet can approximate operators but may demand prohibitive data/hardware for complex, high-dimensional, time-dependent PDE operator learning compared to architectures tailored for temporal conditioning like DiTTO.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Denoising diffusion probabilistic models. <em>(Rating: 2)</em></li>
                <li>Fourier neural operator for parametric partial differential equations. <em>(Rating: 2)</em></li>
                <li>Attention is all you need <em>(Rating: 2)</em></li>
                <li>U-net: Convolutional networks for biomedical image segmentation. <em>(Rating: 1)</em></li>
                <li>Learning nonlinear operators via Deeponet based on the universal approximation theorem of operators. <em>(Rating: 1)</em></li>
                <li>Generative diffusion learning for parametric partial differential equations. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2311",
    "paper_id": "paper-259951225",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "DiTTO",
            "name_full": "Diffusion-inspired Temporal Transformer Operator",
            "brief_description": "A neural operator architecture that conditions a U-Net on continuous time via transformer-style time embeddings and diffusion-model-inspired conditioning to predict solutions of time-dependent PDEs continuously in time and perform temporal extrapolation and super-resolution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Time-dependent partial differential equations (PDEs): fluid dynamics (Burgers, Navier–Stokes, hypersonic flow), acoustic wave propagation, and climate surface-temperature forecasting",
            "problem_description": "Learn an operator mapping from initial conditions to the PDE solution at arbitrary future times (u(x,0) → u(x,t)) for many initial conditions, enabling continuous-in-time inference, temporal extrapolation, zero-shot temporal super-resolution, and real-time inference across multiple PDE domains.",
            "data_availability": "Varies by problem: abundant/moderate for climate (multi-year daily grid data, training 2013–2015 with 1095 snapshots used for training/validation/test splits), limited for hypersonic flow (61 trajectories over Mach range), moderate-to-large for synthetic PDE datasets (e.g., 1000 trajectories with 200 timesteps for Navier–Stokes extrapolation experiments); data are labeled (numerical solver outputs) and publicly or synthetically generated depending on task.",
            "data_structure": "Structured spatio-temporal grid data (tensors) — spatial grids (1D/2D/3D) concatenated with spatial coordinates and scalar temporal query; effectively high-dimensional time series / image-like fields over time; for some variants, unstructured point sets projected to N×d coordinate arrays.",
            "problem_complexity": "High: non-linear PDEs, multi-scale interactions, high gradients and shocks (hypersonic flows, high-Re Navier–Stokes), high-dimensional spatial grids (2D/3D+time), need for long-term extrapolation and temporal super-resolution; computational complexity arises from training on many initial conditions and fine temporal discretizations.",
            "domain_maturity": "Mature numerical PDE and climate modeling domains with well-established solvers and physics knowledge, but challenging due to multi-scale nonlinear dynamics and expensive conventional solvers; SciML/operator learning is an active emerging subfield applied to these mature domains.",
            "mechanistic_understanding_requirements": "Medium-to-high — scientific validity and robustness are important (e.g., capturing shocks, conservation properties, physical plausibility), but the approach uses data-driven black-box operators; interpretability is aided by operator formulation and conditioning but not inherently mechanistic.",
            "ai_methodology_name": "DiTTO neural operator (U-Net backbone with temporal transformer-style conditioning and attention blocks; diffusion-inspired conditioning)",
            "ai_methodology_description": "Architecture: U-Net (Wide ResNet backbone) with spatial and channel attention blocks, ResNet blocks augmented by time-conditioning via a time-embedding MLP (sinusoidal positional encodings → d_emb vector → two-layer MLP with GELU) applied as channel-wise multiplicative conditioning. Training objective: supervised mean relative L2 loss mapping (initial condition, query time) → state at that time. Training strategies include temporal-bundling (predicting look-forward windows), sub-sampling (DiTTO-s), and variants DiTTO-point (1D convs with spatial positional encodings to handle point clouds/unstructured grids) and DiTTO-gate (gated skip connections to emphasize high-frequency features). The architecture repurposes diffusion-model conditioning (conditioning on time rather than noise) and uses attention modules; inference is continuous in time and immediate (no autoregressive rollout required).",
            "ai_methodology_category": "Supervised operator learning (neural operator), hybrid of diffusion-inspired conditioning and transformer-style conditioning (deep learning for scientific computing)",
            "applicability": "Applicable and appropriate: designed for time-dependent PDE operator learning where training datasets of solution trajectories exist; requires adaptation (time-conditioning, bundling, point variant) for high-dimensional/unstructured domains; computationally efficient at inference (real-time) but can be training-intensive for fine temporal discretizations without sub-sampling.",
            "effectiveness_quantitative": "Reported relative L2 errors vary by problem: climate five-year extrapolation mean relative L2 ≈ 0.014 (1.4%); Burgers (ν=0.01) DiTTO relative L2 ≈ 0.0058–0.0060 (N_test_t range); Navier–Stokes Re≈20 DiTTO relative L2 ≈ 0.029–0.30 depending on test temporal resolution and dataset; Hypersonic flow (no noise) DiTTO ≈ 0.0303, DiTTO-point ≈ 0.0152 (see Table 2); for 2D/3D acoustic wave problems DiTTO/DiTTO-s achieved the lowest errors (~0.009–0.015 reported). (Numbers are reported as relative L2 errors from the paper's tables.)",
            "effectiveness_qualitative": "DiTTO generally outperforms or matches state-of-the-art neural operators (FNO) and U-Net baselines on interpolation, extrapolation, and zero-shot temporal super-resolution; demonstrates superior temporal extrapolation when trained with temporal-bundling; preserves accuracy across changes in temporal discretization (meshfree in time); demonstrates robustness to noise in hypersonic flow (DiTTO-point especially robust); attention and temporal conditioning materially improve performance (ablation: removing attention roughly doubled error). Limitations include higher training data / compute needs for some problems and sensitivity to hyperparameters and training strategy.",
            "impact_potential": "High: enables continuous-time operator inference for expensive PDE simulations, long-term forecasts (five-year climate forecasts demonstrated), real-time inference for control/autonomy settings, zero-shot temporal super-resolution that can reduce need for fine temporal simulation grids, and noise-robust surrogates for high-gradient flows; generalizes to multiple scientific PDE domains.",
            "comparison_to_alternatives": "Compared experimentally against Fourier Neural Operator (FNO) and U-Net baselines across many benchmarks. DiTTO (and its variants) typically achieve lower relative L2 errors, particularly for temporal extrapolation and zero-shot super-resolution; DiTTO-point is most accurate in hypersonic flow tests and most noise-robust; FNO sometimes competitive on smooth/periodic cases but worse for non-periodic/boundary-driven wave problems and extrapolation; U-Net often performs poorly on high-dimensional/time-dependent problems or requires large memory.",
            "success_factors": "Key factors: diffusion-inspired time-conditioning (conditioning on continuous time), transformer-style positional/time embeddings, spatial and channel attention blocks, temporal-bundling training strategy (reduces error accumulation and improves extrapolation), sub-sampling (DiTTO-s) for scalability and regularization, and specialized variants (DiTTO-point, DiTTO-gate) to address unstructured domains and high-frequency features.",
            "key_insight": "Conditioning a multi-scale U-Net on a learned continuous time embedding—inspired by diffusion-model conditioning and enhanced with attention and training strategies like temporal-bundling and sub-sampling—yields a neural operator that is continuous in time, excels at temporal extrapolation and super-resolution, and outperforms FNO/U-Net baselines on challenging PDE problems.",
            "uuid": "e2311.0",
            "source_info": {
                "paper_title": "Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "DiTTO-s",
            "name_full": "DiTTO with sub-sampling (fast variant)",
            "brief_description": "A computationally faster DiTTO variant that trains on random sub-sequences (sub-sampling) of the time trajectory each epoch to reduce training cost and act as regularization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Same PDE domains as DiTTO (Burgers, Navier–Stokes, wave propagation)",
            "problem_description": "Same operator-learning target but optimized to reduce training time and memory footprint when temporal discretization is fine by using random temporal sub-sampling during training.",
            "data_availability": "Same as DiTTO; method designed to work even when many temporal samples exist by sampling a fraction α of timesteps per epoch (e.g., α=0.05–0.2 experiments).",
            "data_structure": "Spatio-temporal grid data; training uses sub-sampled timesteps per trajectory.",
            "problem_complexity": "Same as DiTTO; alleviates complexity of scaling linearly with number of timesteps during training.",
            "domain_maturity": "As DiTTO.",
            "mechanistic_understanding_requirements": "Medium — same considerations as DiTTO.",
            "ai_methodology_name": "DiTTO-s (sub-sampled training variant)",
            "ai_methodology_description": "Selects random subsequences S_m ⊂ {0,...,T} for each initial condition during each epoch such that overall sampling fraction α&lt;1 (e.g., α=0.05 or 0.1). Loss computed only on these sampled timesteps; subsequences re-sampled each epoch. Acts as computational acceleration and regularizer.",
            "ai_methodology_category": "Supervised operator learning with stochastic sub-sampling regularization",
            "applicability": "Appropriate when temporal resolution is fine and training cost is high; works well when full coverage over many epochs is acceptable.",
            "effectiveness_quantitative": "Often achieves comparable or slightly better errors than full DiTTO; e.g., for Burgers ν=0.01 DiTTO-s relative L2 ≈ 0.0055–0.0057 vs DiTTO ≈ 0.0058–0.0060; ablation showed α ≈ 0.1–0.2 gave best results.",
            "effectiveness_qualitative": "DiTTO-s retains accuracy while reducing training samples per epoch and providing regularization benefits; too small α (e.g., 0.05) can degrade performance.",
            "impact_potential": "Improves scalability of DiTTO to fine temporal discretizations and reduces memory, enabling training where full temporal coverage per epoch is infeasible.",
            "comparison_to_alternatives": "Compared to full DiTTO and FNO/U-Net; DiTTO-s often matched or slightly improved DiTTO and outperformed baselines on many tasks.",
            "success_factors": "Random temporal sub-sampling (α choice), re-sampling each epoch providing regularization, and synergy with temporal-bundling training.",
            "key_insight": "Sub-sampling temporal training data reduces cost and acts as effective regularization, often improving DiTTO generalization while scaling training to fine time discretizations.",
            "uuid": "e2311.1",
            "source_info": {
                "paper_title": "Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "DiTTO-point",
            "name_full": "DiTTO point-cloud (1D convolution) variant",
            "brief_description": "A memory-efficient variant of DiTTO that treats spatial domains as point sets (N×d coordinates) and uses 1-D convolutions with spatial positional encodings to handle unstructured or high-dimensional spatial data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Hypersonic flow around double-cone, and other high-dimensional PDEs where structured grids are impractical",
            "problem_description": "Learn mappings between fields defined on unstructured point sets or large high-dimensional grids by treating data as N×d coordinate matrices and preserving spatial information through positional encodings.",
            "data_availability": "Typically limited for CFD experiments (e.g., 61 trajectories for hypersonic dataset), and synthetic/numerical simulation data for other PDEs.",
            "data_structure": "Point cloud / unstructured spatial coordinates N×d paired with solution vectors of length N, plus temporal scalar conditioning — effectively high-dimensional, unstructured spatio-temporal data.",
            "problem_complexity": "High: hypersonic flows with shocks and vortical structures, complex geometry; limited sample sizes increase difficulty.",
            "domain_maturity": "Mature CFD domain; surrogate modeling for such geometries remains challenging.",
            "mechanistic_understanding_requirements": "High for engineering trustworthiness; DiTTO-point aims for accurate surrogate predictions though interpretability remains limited.",
            "ai_methodology_name": "DiTTO-point (1D convolutional neural operator with spatial positional encodings)",
            "ai_methodology_description": "Projects domain to N×d matrix of coordinates and solution values, applies spatial positional encoding to coordinates (sinusoidal), concatenates with initial condition channel-wise, and uses 1-D convolutions so memory use is independent of physical dimension; time-conditioning same as DiTTO.",
            "ai_methodology_category": "Supervised operator learning adapted for unstructured data",
            "applicability": "Appropriate for unstructured domains or memory-limited settings where full d-dimensional convolutions are infeasible; demonstrated particularly successful for hypersonic flow and noise robustness.",
            "effectiveness_quantitative": "Hypersonic flow: DiTTO-point relative L2 ≈ 0.0152 (no noise) outperforming DiTTO (0.0303) and FNO (0.0241). Noise robustness: DiTTO-point maintains low error up to 100% noise levels (relative L2 ≈ 0.054 at 100% noise) compared to FNO (0.608 at 100% noise).",
            "effectiveness_qualitative": "Highly effective and robust to input noise and small datasets in the hypersonic case; provides competitive or superior accuracy to full DiTTO and FNO for geometry/point-cloud settings.",
            "impact_potential": "Enables operator learning on unstructured geometries and memory-limited hardware, promising for CFD surrogate modeling, design, and uncertainty quantification in engineering contexts.",
            "comparison_to_alternatives": "Outperformed FNO and full DiTTO on the hypersonic dataset and showed superior noise robustness; comparable errors to DiTTO in many other PDE benchmarks while using less memory.",
            "success_factors": "Preservation of spatial coordinate information via positional encoding, dimensionality reduction to 1D convolutions, and shared temporal conditioning.",
            "key_insight": "Reducing spatial convolutions to a 1D point-based representation with positional encodings allows neural operators to scale to unstructured/high-dimensional problems while improving robustness to noise and limited data.",
            "uuid": "e2311.2",
            "source_info": {
                "paper_title": "Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "DiTTO-gate",
            "name_full": "DiTTO with gated skip-connections",
            "brief_description": "A DiTTO variant that modifies U-Net decoder skip connections by adding gated convolutional blocks to emphasize or filter high-frequency features, useful for problems with fine details and sharp features (e.g., high-Re flows).",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "High-gradient PDEs: Navier–Stokes with high Reynolds number, problems exhibiting sharp features",
            "problem_description": "Improve recovery of high-frequency / sharp spatial features by gating skip connections to control information passed from encoder to decoder levels.",
            "data_availability": "Applies where training datasets capture high-frequency dynamics; datasets for high-Re Navier–Stokes used in experiments (various sizes up to 5000 samples for some tests).",
            "data_structure": "Structured spatio-temporal grid data with high spatial gradients and fine-scale features.",
            "problem_complexity": "High complexity due to turbulence-like features and shocks, requiring ability to represent multi-scale spatial detail.",
            "domain_maturity": "Mature fluid dynamics domain; surrogate modeling for turbulent/high-Re flows remains challenging.",
            "mechanistic_understanding_requirements": "High — capturing physically-consistent high-frequency features is important for fidelity.",
            "ai_methodology_name": "DiTTO-gate (gated skip-connection U-Net variant)",
            "ai_methodology_description": "Adds a convolutional gating block on each decoder skip connection (convolutional block applied to skip input) to modulate high-frequency contributions into the decoder; combined with temporal conditioning and attention as in DiTTO.",
            "ai_methodology_category": "Supervised operator learning with architectural modification to enhance high-frequency feature learning",
            "applicability": "Useful for PDEs with sharp spatial features where skip-connections may overly inject high-frequency detail; requires tuning but demonstrated effective.",
            "effectiveness_quantitative": "Reported to help on high-feature problems (e.g., some DiTTO-point-s-gate entries in tables show comparable or improved relative L2 over non-gated variants; specific numbers vary by task; e.g., some entries show errors ~0.13–0.14 in challenging Navier–Stokes tests).",
            "effectiveness_qualitative": "Improves ability to capture fine spatial details in problems with sharp features; provides an architectural lever to balance high-frequency injection from skip connections.",
            "impact_potential": "Improves surrogate fidelity for challenging high-gradient PDEs, beneficial for engineering simulations where sharp spatial accuracy matters.",
            "comparison_to_alternatives": "Compared within DiTTO variants; gating improved or stabilized results for high-Re/sharp-feature problems versus vanilla DiTTO in some benchmarks.",
            "success_factors": "Targeted modification of skip connections to control high-frequency content, combined with temporal conditioning and attention.",
            "key_insight": "Architectural gating on skip connections helps manage high-frequency feature flow in U-Net-based neural operators, improving fidelity for sharp-feature PDEs.",
            "uuid": "e2311.3",
            "source_info": {
                "paper_title": "Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Diffusion models (DDPM-inspired conditioning)",
            "name_full": "Denoising Diffusion Probabilistic Models (inspiration and conditioning mechanism)",
            "brief_description": "Generative diffusion models are used conceptually: DiTTO repurposes diffusion-model conditioning by replacing the noise-level conditioning with continuous temporal conditioning to map initial states to time-evolved states.",
            "citation_title": "Denoising diffusion probabilistic models.",
            "mention_or_use": "use",
            "scientific_problem_domain": "Methodological adaptation for general time-dependent PDE operator learning across multiple physics domains (climate, fluid dynamics, waves)",
            "problem_description": "Rather than perform iterative denoising of corrupted samples, use the diffusion-model idea of conditioning on a 'level' — here the temporal scalar — to condition a network to predict the state at different time 'levels' from the initial condition.",
            "data_availability": "Not data-specific; applicable when labeled trajectory data exist.",
            "data_structure": "Spatio-temporal fields (gridded tensor data) with discrete timesteps used to generate training pairs (initial condition, time) → state.",
            "problem_complexity": "Conceptual complexity moderate: casting continuous-time operator learning into conditioning framework; computational complexity moved to training for mapping arbitrary t rather than iterative sampling.",
            "domain_maturity": "Diffusion models are a mature generative ML class; adaptation to SciML is emerging.",
            "mechanistic_understanding_requirements": "Low-to-medium: approach is a modeling trick rather than physics-informed; interpretability not inherent but mapping is conditioned on physical time.",
            "ai_methodology_name": "Diffusion-inspired temporal conditioning",
            "ai_methodology_description": "Repurposes diffusion training concept: instead of conditioning on noise level ε (as in DDPM), condition on time scalar t (embedded via positional encodings and MLP) and train the network to map (x0, t) → xt. There is no denoising in practice because xt are true states; the conditioning nonetheless helps disentangle the mapping across time levels.",
            "ai_methodology_category": "Supervised learning with diffusion-model-inspired conditioning",
            "applicability": "Appropriate for continuous-time operator learning; enables instantaneous inference at arbitrary times without autoregressive rollouts.",
            "effectiveness_quantitative": "Indirect: underpins DiTTO's continuous-time capabilities and contributes to reported errors (see DiTTO metrics).",
            "effectiveness_qualitative": "The diffusion-inspired conditioning is central to DiTTO's ability to produce continuous-in-time predictions and to avoid iterative autoregressive accumulation of errors.",
            "impact_potential": "Introduces a transferable conditioning mechanism enabling continuous-time neural operators applicable across PDE domains.",
            "comparison_to_alternatives": "Alternative is explicit autoregressive training (predict next timestep) or mapping to fixed timesteps; diffusion-inspired conditioning avoids extreme error accumulation from autoregression and overfitting to training timesteps.",
            "success_factors": "Using continuous scalar conditioning analogous to noise-level conditioning, combined with strong time embeddings and architecture that uses the conditioning multiplicatively in ResNet blocks.",
            "key_insight": "Replacing diffusion noise-level conditioning with a time scalar enables a continuous-time conditional operator that avoids autoregressive error accumulation and supports temporal super-resolution/extrapolation.",
            "uuid": "e2311.4",
            "source_info": {
                "paper_title": "Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Temporal-bundling",
            "name_full": "Temporal-bundling (training strategy)",
            "brief_description": "A training strategy where the network is trained to predict multiple-step look-forward windows (bundles) rather than purely next-step autoregression or full mapping, providing a balance that reduces error accumulation and improves temporal extrapolation.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Time-series operator learning for PDEs (used for Navier–Stokes extrapolation and climate forecasting tasks)",
            "problem_description": "Enable extrapolation beyond training time interval by training the model to map u(x,t) → u(x,t+lf) for various look-forward windows lf and splitting trajectories into sub-trajectories, improving stability and lowering accumulated error.",
            "data_availability": "Requires labeled trajectories over time; used on datasets with many timesteps (e.g., Navier–Stokes 1000 trajectories × 200 timesteps; climate 1095 snapshots).",
            "data_structure": "Spatio-temporal grid sequences; training reshaped into sub-trajectories of length lf.",
            "problem_complexity": "Moderate to high: mitigates tradeoffs between autoregressive accumulation and overfitting to fixed timesteps.",
            "domain_maturity": "Technique referenced from prior work and adapted here; applicable across operator learning tasks.",
            "mechanistic_understanding_requirements": "Low — technique is an empirical training strategy to improve generalization.",
            "ai_methodology_name": "Temporal-bundling training",
            "ai_methodology_description": "During training, split each trajectory into overlapping sub-trajectories of length lf and train the operator to map current state/time to the state at t+lf; experiments sweep lf values (1,5,10,20,50,100) and find a sweet spot (e.g., lf=20 in NS experiment, lf=365 for climate) that minimizes extrapolation error.",
            "ai_methodology_category": "Supervised training strategy for time-series/operator learning",
            "applicability": "Appropriate for improving temporal extrapolation and stability in learned dynamical operators; choice of lf influences bias-variance trade-off.",
            "effectiveness_quantitative": "Example: in Navier–Stokes extrapolation experiment (trained on first 100 timesteps), lf=20 achieved minimum error and lowest uncertainty when extrapolating to 200 timesteps; for climate, lf=365 was used to split 1095-day trajectory into 730 sub-trajectories for five-year extrapolation with mean L2 error 0.014.",
            "effectiveness_qualitative": "Offers a compromise between pure autoregression (lf=1) which accumulates error and full mapping (lf=nt) which overfits; temporal-bundling reduces error accumulation and uncertainty in long-horizon extrapolation.",
            "impact_potential": "Improves reliability of data-driven long-term forecasts and extrapolation in scientific time-dependent problems; general strategy applicable to many operator learning frameworks.",
            "comparison_to_alternatives": "Compared to autoregressive training and full mapping; temporal-bundling provided lower accumulated error and uncertainty across tested lf choices.",
            "success_factors": "Appropriate selection of look-forward window lf, combination with DiTTO continuous-time conditioning, and sufficient trajectory diversity in training data.",
            "key_insight": "Training to predict intermediate look-forward windows exposes the operator to intermediate states and reduces both error accumulation (vs autoregression) and overfitting to fixed timesteps (vs mapping), improving extrapolation performance.",
            "uuid": "e2311.5",
            "source_info": {
                "paper_title": "Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Transformer / Attention blocks",
            "name_full": "Transformer-style attention (spatial and channel attention blocks)",
            "brief_description": "Scaled dot-product attention modules integrated in DiTTO (both spatial and channel-wise) to extract long-range spatial correlations and inter-channel relationships within the U-Net backbone.",
            "citation_title": "Attention is all you need",
            "mention_or_use": "use",
            "scientific_problem_domain": "General spatio-temporal PDE modeling across problems (Burgers, Navier–Stokes, waves, climate, hypersonic flow)",
            "problem_description": "Enable the neural operator to learn nonlocal spatial correlations and cross-channel dependencies that are important for representing complex PDE solutions.",
            "data_availability": "Requires typical labeled spatio-temporal PDE solution datasets; no special additional data.",
            "data_structure": "Grid/image-like spatial representations where attention can operate on pixel/token patches or channels.",
            "problem_complexity": "Attention helps address high complexity due to long-range dependencies and multi-scale interactions in PDE solutions.",
            "domain_maturity": "Transformers are well-established in ML; their adoption in SciML is growing.",
            "mechanistic_understanding_requirements": "Medium — attention modules improve representational capacity but do not enforce mechanistic constraints.",
            "ai_methodology_name": "Scaled dot-product attention (spatial and channel-wise)",
            "ai_methodology_description": "Implement Q,K,V projections via 1×1 convolutions to produce attention matrices for channel and spatial attention; out = softmax(QK^T / sqrt(d)) V used inside U-Net levels to capture nonlocal correlations.",
            "ai_methodology_category": "Architectural enhancement within supervised deep learning",
            "applicability": "Appropriate for problems with significant long-range dependencies or when cross-channel interactions matter; scales with memory and compute.",
            "effectiveness_quantitative": "Ablation: removing attention roughly doubled errors in Burgers test (DiTTO with attention relative L2 ≈ 0.0060 vs DiTTO without attention ≈ 0.0132 across tested N_t values).",
            "effectiveness_qualitative": "Attention materially improves accuracy and ability to capture spatial correlations, especially on problems with nonlocal interactions; contributes to DiTTO's superior performance.",
            "impact_potential": "Increases fidelity of neural operators on multi-scale PDEs and supports generalization across initial conditions.",
            "comparison_to_alternatives": "Compared ablation-wise to identical DiTTO without attention; attention substantially reduced error.",
            "success_factors": "Appropriate placement of attention (spatial and channel blocks at multiple scales), and integration with temporal conditioning.",
            "key_insight": "Integrating transformer-style attention into multiscale U-Net operators significantly improves representation of nonlocal spatial correlations, halving errors on tested PDEs.",
            "uuid": "e2311.6",
            "source_info": {
                "paper_title": "Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "FNO",
            "name_full": "Fourier Neural Operator",
            "brief_description": "A neural operator that parameterizes integral kernels in Fourier space to learn mappings between function spaces, used here as a baseline comparison for DiTTO across several PDE benchmarks.",
            "citation_title": "Fourier neural operator for parametric partial differential equations.",
            "mention_or_use": "use",
            "scientific_problem_domain": "Operator learning for PDEs — Burgers, Navier–Stokes, wave propagation, hypersonic flow (used as baseline comparisons)",
            "problem_description": "Learn mappings from initial conditions to solution fields using Fourier-domain convolution layers (global spectral representation), typically effective on smooth and periodic problems.",
            "data_availability": "Used with the same datasets as DiTTO for direct comparison (synthetic solver outputs and public climate/hypersonic datasets); requires labeled trajectory data.",
            "data_structure": "Structured grid/tensor data amenable to Fourier transforms (works best with periodic boundary conditions but can be adapted).",
            "problem_complexity": "Performs well on smooth/periodic PDEs but can struggle with boundary-driven or sparse-featured data; complexity increases with required number of Fourier modes for high detail.",
            "domain_maturity": "Established and widely-used neural operator method in SciML.",
            "mechanistic_understanding_requirements": "Low — spectral representation provides interpretability in frequency domain but not explicit physics constraints.",
            "ai_methodology_name": "Fourier Neural Operator (FNO)",
            "ai_methodology_description": "Parametrizes a global kernel in Fourier domain via learned multipliers on Fourier modes across layers; applied with hyperparameter search over number of modes and depth; comparisons conducted on same train/test splits and temporal discretizations.",
            "ai_methodology_category": "Supervised operator learning (spectral approach)",
            "applicability": "Appropriate for many PDE operator tasks, especially smooth/periodic settings; less suited for non-periodic Dirichlet problems and some high-gradient/sparse data where DiTTO outperforms.",
            "effectiveness_quantitative": "Examples from tables: FNO relative L2 often higher than DiTTO on extrapolation/super-resolution tasks (e.g., Burgers and wave problems); hypersonic noise experiment FNO errors increase rapidly with noise (0%: 0.0241; 100% noise: 0.6080) compared to DiTTO variants.",
            "effectiveness_qualitative": "FNO is competitive on smooth/periodic cases and can be tuned via Fourier modes/layers; however, it is more sensitive to noise, boundary conditions, and temporal discretization changes than DiTTO; computational cost increases with added temporal dimensions.",
            "impact_potential": "Continuing baseline and useful method for many operator learning problems; limitations motivate DiTTO-style conditioning for temporal continuity and robustness.",
            "comparison_to_alternatives": "Benchmarked extensively in paper; DiTTO variants outperform FNO on extrapolation, boundary-driven wave problems, temporal super-resolution, and noise robustness in hypersonic flows.",
            "success_factors": "Global spectral representation, careful hyperparameter tuning (number of modes, layers); struggles when periodicity assumptions or spectral smoothness don't hold.",
            "key_insight": "FNO is strong for smooth/periodic PDEs but has limitations for temporal extrapolation, non-periodic/boundary-dominated problems, and noisy/high-gradient scenarios where conditional, attention-based operators like DiTTO excel.",
            "uuid": "e2311.7",
            "source_info": {
                "paper_title": "Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "U-Net baseline",
            "name_full": "U-Net with (or without) temporal conditioning / attention (baseline)",
            "brief_description": "Convolutional encoder–decoder architecture used as a baseline (sometimes with attention), typically predicting discrete timesteps and lacking continuous-time conditioning; included for comparison against DiTTO.",
            "citation_title": "U-net: Convolutional networks for biomedical image segmentation.",
            "mention_or_use": "use",
            "scientific_problem_domain": "Benchmarked across the same PDE problems (Burgers, Navier–Stokes, wave, hypersonic) as a simpler CNN baseline",
            "problem_description": "Predict solution fields at discrete timesteps from input initial conditions (no explicit continuous-time conditioning in baseline runs).",
            "data_availability": "Same labeled PDE datasets; U-Net experiments often limited by memory for 3D time-dependent problems.",
            "data_structure": "Structured spatial grids treated as images per timestep; lacks explicit temporal conditioning so discrete-time outputs are produced.",
            "problem_complexity": "U-Net struggles with high-dimensional time-dependent problems due to memory and inability to scale to continuous-time inference.",
            "domain_maturity": "Established image-to-image deep learning architecture; used widely as baseline in SciML.",
            "mechanistic_understanding_requirements": "Low — black-box predictions; interpretability limited.",
            "ai_methodology_name": "U-Net (convolutional encoder–decoder baseline)",
            "ai_methodology_description": "Standard U-Net architecture (sometimes with attention) trained to map initial-condition input to solution at discrete timesteps; temporal conditioning removed in baseline comparisons.",
            "ai_methodology_category": "Supervised convolutional learning baseline",
            "applicability": "Applicable for some PDE tasks with modest spatial/temporal size but not suitable for continuous-in-time extrapolation or large 3D+time problems due to memory constraints.",
            "effectiveness_quantitative": "Often substantially worse than DiTTO and FNO on many tasks; e.g., for some 2D wave problems U-Net relative L2 reported ≈ 1.39 vs DiTTO ≈ 0.009–0.03 in various settings (see tables).",
            "effectiveness_qualitative": "U-Net can produce reasonable results for certain fixed-timestep tasks but fails to scale to high-dimensional/time-dependent problems and temporal super-resolution; large memory footprint precluded training on some 3D tasks.",
            "impact_potential": "Limited as a general-purpose operator learner for continuous-time PDE problems; still useful as simple baseline.",
            "comparison_to_alternatives": "Underperformed compared to DiTTO and often to FNO, especially on extrapolation and high-dimensional cases.",
            "success_factors": "Effective for image-like tasks and when only discrete timesteps are needed; fails when continuous-time conditioning and scalability are required.",
            "key_insight": "Standard U-Net baselines lack the temporal conditioning and architectural enhancements needed for robust continuous-time operator learning and extrapolation in challenging PDE problems.",
            "uuid": "e2311.8",
            "source_info": {
                "paper_title": "Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "DeepONet",
            "name_full": "Deep Operator Network (DeepONet)",
            "brief_description": "An operator learning architecture (branch and trunk networks) mentioned and briefly experimented with; in this paper it required too much data / resources for multi-dimensional time-dependent problems and was ultimately not included in final comparisons for multidimensional tests.",
            "citation_title": "Learning nonlinear operators via Deeponet based on the universal approximation theorem of operators.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Operator learning for time-dependent PDEs (attempted as alternative operator architecture)",
            "problem_description": "Map input functions (initial conditions) to output functions (solutions) via branch and trunk networks; applied to time-dependent problems in attempt to learn mappings across initial conditions and time.",
            "data_availability": "Paper reports DeepONet required a very large amount of examples for time-dependent 1D problems and was infeasible for multidimensional tasks given hardware constraints; thus data requirements were judged high.",
            "data_structure": "Functional input/output mapping; typically needs many function samples and coordinate evaluations.",
            "problem_complexity": "High sample complexity for time-dependent multidimensional operator learning; memory/computational demand grows with problem dimension.",
            "domain_maturity": "Established operator learning approach with theoretical foundations; practical scaling to high-dimensional time-dependent PDEs remains challenging.",
            "mechanistic_understanding_requirements": "Medium — operator-focused architecture useful for interpretability of mapping but still data-driven.",
            "ai_methodology_name": "DeepONet (branch & trunk operator network)",
            "ai_methodology_description": "Standard DeepONet with branch network taking function samples and trunk network taking coordinates; authors attempted training but removed examples due to high sample/compute needs.",
            "ai_methodology_category": "Supervised operator learning",
            "applicability": "Potentially applicable but limited by required training data and compute for time-dependent multidimensional problems in this work.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Authors found DeepONet required very large datasets to converge for time-dependent 1D tasks and was infeasible for multidimensional cases on available hardware, so it was not compared in final benchmarks.",
            "impact_potential": "DeepONet remains promising in theory but practical scaling to large time-dependent multidimensional PDE datasets is a current challenge.",
            "comparison_to_alternatives": "Not benchmarked in final comparisons due to practical constraints; authors focused comparisons on DiTTO, FNO and U-Net.",
            "success_factors": "Strong theoretical operator approximation capability; success hindered by sample complexity and hardware limits in time-dependent multidimensional settings.",
            "key_insight": "DeepONet can approximate operators but may demand prohibitive data/hardware for complex, high-dimensional, time-dependent PDE operator learning compared to architectures tailored for temporal conditioning like DiTTO.",
            "uuid": "e2311.9",
            "source_info": {
                "paper_title": "Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Denoising diffusion probabilistic models.",
            "rating": 2,
            "sanitized_title": "denoising_diffusion_probabilistic_models"
        },
        {
            "paper_title": "Fourier neural operator for parametric partial differential equations.",
            "rating": 2,
            "sanitized_title": "fourier_neural_operator_for_parametric_partial_differential_equations"
        },
        {
            "paper_title": "Attention is all you need",
            "rating": 2,
            "sanitized_title": "attention_is_all_you_need"
        },
        {
            "paper_title": "U-net: Convolutional networks for biomedical image segmentation.",
            "rating": 1,
            "sanitized_title": "unet_convolutional_networks_for_biomedical_image_segmentation"
        },
        {
            "paper_title": "Learning nonlinear operators via Deeponet based on the universal approximation theorem of operators.",
            "rating": 1,
            "sanitized_title": "learning_nonlinear_operators_via_deeponet_based_on_the_universal_approximation_theorem_of_operators"
        },
        {
            "paper_title": "Generative diffusion learning for parametric partial differential equations.",
            "rating": 1,
            "sanitized_title": "generative_diffusion_learning_for_parametric_partial_differential_equations"
        }
    ],
    "cost": 0.0247555,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)
8 Dec 2023</p>
<p>Oded Ovadia odedovadia@mail.tau.ac.il 
Department of Applied Mathematics
Tel Aviv University
69978Tel AvivIsrael</p>
<p>Vivek Oommen vivek_oommen@brown.edu 
School of Engineering
Brown University
02912ProvidenceRIUSA</p>
<p>Adar Kahana adar_kahana@brown.edu 
Division of Applied Mathematics
Brown University
02912ProvidenceRIUSA</p>
<p>Ahmad Peyvan ahmad_peyvan@brown.edu 
Division of Applied Mathematics
Brown University
02912ProvidenceRIUSA</p>
<p>Eli Turkel eliturkel@gmail.com 
Department of Applied Mathematics
Tel Aviv University
69978Tel AvivIsrael</p>
<p>George Em Karniadakis george_karniadakis@brown.edu 
School of Engineering
Brown University
02912ProvidenceRIUSA</p>
<p>Division of Applied Mathematics
Brown University
02912ProvidenceRIUSA</p>
<p>Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO)
8 Dec 20230DAEB6B21F01675D5066F3FEB5F6F26AarXiv:2307.09072v2[cs.LG]
Extrapolation remains a grand challenge in deep neural networks across all application domains.We propose an operator learning method to solve time-dependent partial differential equations (PDEs) continuously and with extrapolation in time without any temporal discretization.The proposed method, named Diffusion-inspired Temporal Transformer Operator (DiTTO), is inspired by latent diffusion models and their conditioning mechanism, which we use to incorporate the temporal evolution of the PDE, in combination with elements from the transformer architecture to improve its capabilities.Upon training, DiTTO can make inferences in real-time.We demonstrate its extrapolation capability on a climate problem by estimating the temperature around the globe for several years, and also in modeling hypersonic flows around a double-cone.We propose different training strategies involving temporal-bundling and sub-sampling and demonstrate performance improvements for several benchmarks, performing extrapolation for long time intervals as well as zero-shot super-resolution in time.Link to demo.</p>
<p>The field of scientific machine learning (SciML) has been growing rapidly in recent years, and many successful methods for modeling scientific problems using machine learning (ML) methods have been proposed [1][2][3][4][5].Many tools designed for standard ML and data science problems can also perform well on SciML tasks.With their unique properties, recent state-of-the-art methods can achieve higher accuracy with fewer data samples, fewer computational resources, more generalization and robustness for a wide range of problems across different spatio-temporal scales, such as modeling the temperature distribution over the globe for the next decade or resolving the finer vortices and shocks often encountered in hypersonic flow problems, both critical problems at the present time.The recent innovations in the field of ML primarily originate from the domains of natural language processing [6] and computer vision [7].Our work exploits and further develops the main idea of a recently proposed method called diffusion models [8] (used in generative AI) for solving forward partial differential equations (PDEs).</p>
<p>Solving time-dependent PDEs is an essential topic for the scientific community.This centuries-old research involves 1) formulating a problem from physical domains, biological research, chemical reactions, etc., and 2) solving them using discretization-based approaches like finite-element [9], finite-difference [10], finite-volume [11], or spectral methods [12].These numerical solvers have widespread application across diverse scientific domains, such as wave propagation [13,14], computational mechanics [15,16], materials science [17,18], fluid dynamics [19,20], seismic imaging [21], etc.However, if the underlying mathematical operators that govern the temporal evolution of the system are non-linear and/or there are observational data available, the task of assimilating and simulating such processes using discretization-based numerical methods can become increasingly challenging and computationally expensive.The burden associated with the traditional numerical solvers is further increased when separate simulation runs become mandatory for every new initial condition.Moreover, in certain application domains, such as autonomy and navigation or robotics, real-time inference is required.SciML methods such as neural operators specifically address these issues by significantly reducing the associated computational costs [2,3,22,23].</p>
<p>Several methods for solving PDE-related problems using ML methods, and specifically transformers, have been proposed [1,2,24].Herein, we solve the forward problem of modeling a time-dependent PDE by training the neural operator to accurately estimate the state field variable at a later time from a given initial condition.Specifically, forecasting a continuous in-time solution in real-time for a plurality of initial conditions and extrapolating beyond the training domain is the main focus of this work.Solving PDE-related problems involves several challenges.Two main challenges are generalizations for different problem conditions and dependence on the physical domain's discretization.To tackle the first, we utilize tools from the growing field of operator learning [2,3], where we use learning techniques to map a function space to another one.Thus, we are able to learn a family of solutions of PDEs corresponding to a family of initial conditions.For the second challenge, we propose a method that, while being dependent on the spatial discretization, is continuous in the temporal evolution of the solution, which is a prominent challenge in solving dynamical systems.</p>
<p>Recent works [25,26] have demonstrated the efficiency of U-Net based architectures for modeling time-dependent PDEs.However, the outputs of these U-Net-based architectures are discrete in time.Gupta et al. [27] performed a systematic ablation study to analyze the significance of Fourier layers, attention mechanism, and parameter conditioning in a U-Net-based neural operator.The DiTTO method proposed here is a diffusion-inspired model [28].The common use of diffusion models involves a generative process used to create data samples.It incorporates a Markov chain of diffusion steps, where in each stage a different texture is added to the data sample.Usually, the texture is noise, so new noise distributions are incrementally added in each step.The models have also been used with other kinds of textures, for example creating cartoonish images from plain ones.Herein, we use a similar framework, but instead of conditioning on the noise distribution, we do so for the temporal evolution.We explore several implementations and training strategies, in addition to the diffusion models themselves.These enhanced methods form the class of explored DiTTO models.Importantly, we demonstrate how this framework can be used for extrapolation, i.e., it can make accurate predictions for samples outside the time interval it was trained to handle.</p>
<p>Results</p>
<p>We approximate the time evolution of a PDE solution (forward process).Instead of incrementally adding noise to the inputs, as done with diffusion models, we incrementally evolve the PDE solution over time.We replace the noise level parameter ε with the temporal variable t.Then, we use the conditioning capabilities of diffusion models to learn the relations between the initial condition, the PDE solution, and the time domain.After the training is complete, the model can interpolate between the initial and final time, creating a numerical solution that is continuous in time.We define its time evolution {x t ]} as the following process:
{x t | t ∈ [0, t f inal ], x t := u(x, t)},(1)
where u is the solution of the differential equation we attempt to solve.Using this notation, the operator learning problem becomes:
x 0 −→ x t , ∀t ∈ [0, t f inal ],(2)x t ≈ G(x 0 )(t), ∀t ∈ [0, t f inal ],(3)
where G represents the surrogate operator, DiTTO, where the operator learning technique we employ is the diffusion process.It is discrete, while (1) is continuous.We discretize {x t } by taking a partitioning
{t n } T n=0 of [0, t f inal ], where 0 = t 0 &lt; t 1 &lt; . . . &lt; t T −1 &lt; t T = t f inal .
The discrete process is then defined as {x n } T n=0 , where x n := u(x, t n ).In PDE terms, given an initial condition x 0 , we approximate the analytic solution at a set of specific future time steps {t n } T n=1 .In operator learning terms, we map a family of functions of the form x 0 = u(x, 0) to another family of functions of the form u(x, t).</p>
<p>As outlined before, the role of the neural network in diffusion models is to perform conditional denoising in each step.We repurpose this network structure to solve a PDE-related problem.Since x 0 , x 1 , . . ., x T are directly taken from the analytical solution, we have no noise in this process.Therefore, there is no need for denoising.Thus, we replace the conditional denoising operation with a conditional temporal evolution (x 0 , t n ) −→ x n .</p>
<p>Next, we describe the DiTTO architecture.The network receives two main inputs: the initial condition x 0 = u(x, 0) and a time t = t n .Recall that x 0 is a d-dimensional tensor, and t is a nonnegative scalar.</p>
<p>For the temporal input t, we use an embedding mechanism based on the original Transformer positional encoding [29].Each scalar t is mapped into a vector of size d emb , and then passed through a simple multi-layer perceptron (MLP) with two linear layers and a GELU [30] activation function.</p>
<p>For the spatial input x 0 , we concatenate it with a discrete spatial grid and provide it as an input to a U-Net [31].We use a U-Net variant common in many diffusion models, such as DDPM [8].It follows the backbone of PixelCNN++ [32], a U-Net based on a Wide ResNet [33,34].A sketch of the spatio-temporal architecture is given in Figure 1, and more details can be found in Methods.This architecture is not limited to a specific dimension.The same mechanism can be implemented for d-dimensional problems, where d ∈ {1, 2, 3}.The only major difference is the usage of d-dimensional convolutions for the relevant problem.</p>
<p>Furthermore, we extend DiTTO to develop three variants: DiTTO-s, DiTTO-point and DiTTOgate.DiTTO-point is a memory-efficient version and DiTTO-gate incorporates a gated sub-architecture motivated by Runge-Kutta methods.We also demonstrate the efficacy of two training strategies -1) randomly sub-sampling the trajectory timesteps considered at each epoch (DiTTO-s), and 2) adopting the temporal-bundling [35] for enhancing the forecast capabilities.</p>
<p>Next, we investigate the capabilities and limitations of the proposed DiTTO method on a wide variety of time-dependent non-linear systems in multiple dimensions.First, we discuss the temporal-bundling training technique and illustrate accurate extrapolation on the climate problem.Next, we demonstrate the efficiency of DiTTO in approximating operators with high spatial gradients by learning high Mach number hypersonic flow past a double-cone geometry.We also train DiTTO to learn the following benchmark PDEs: 1D Burgers' equation, 2D incompressible Navier-Stokes with high and low Reynold's numbers, and 2D and 3D acoustic wave equation problems.We compare the DiTTO and its variants: DiTTO-s that utilizes fast sampling, DiTTO-point that reduces high-dimensional problems to 1D, and DiTTO-gate that incorporates a novel convolutional block inspired by stencil-based numerical methods.We compare the proposed methods to the FNO method [3], as well as a standard U-Net model with attention.We use the U-Net architecture, except we remove all temporal conditioning.We compute the relative L 2 error for each method and compare it to the ground truth data.</p>
<p>For each problem we either synthetically create a dataset for training or use data from external sources.Details regarding the data collection process and the solvers used for each problem are presented in the corresponding sections.In each case, the spatiotemporal resolution of the numerical solution is kept fixed.The size of the spatial grid, denoted by N x , N y , N z , is determined by the dimensionality of the problem.We use the same spatial grid for training and inference.For the number of timesteps N t (following standard PDE notation), we use different resolutions for training and inference to test the temporal interpolation and super-resolution capabilities of DiTTO.We train all models with, for example,
N</p>
<p>Extrapolation in time</p>
<p>In our application, extrapolation is a challenging problem due to the inherent nature of the data-driven surrogate networks to overfit the training distribution.Next, we discuss a training strategy that partially alleviates the difficulties associated with extrapolation in time in using DiTTO.As an example, we train DiTTO on the incompressible Navier-Stokes equations with Re = 20 for investigating the ability to extrapolate in time.The dataset consists of 1000 trajectories with 200 time steps.It was randomly split into training, validation and testing datasets in the ratio 80:10:10.During the training of DiTTO, the model is exposed only to the first 100 time steps.We train DiTTO with three types of time-series modeling strategies demonstrated in part a) of Figure 2 -i) autoregressive (look-forward window lf = 1), ii) temporal-bundling (1 &lt; lf &lt; nt) [35], and iii) mapping (lf = nt).We analyze their ability to extrapolate beyond the 100 th time step.Specifically, we train 6 different DiTTO models with lf =1, 5, 10, 20, 50, 100(= nt).During the training, DiTTO learns a mapping u(x, t) → u(x, t + lf ) such that t + lf is less than the number of time steps, nt = 100.Hence, each trajectory in the training dataset is split into nt − lf + 1 sub-trajectories.During the inference stage, DiTTO leaps from û(x, t) to û(x, t + lf ).Because the ground truth is not available, except at t = 0, we consider u as the ground truth and û as  a prediction from DiTTO.In Figure 2(b,c) we observe the minimum error with the lowest uncertainty occurs at lf = 20, suggesting that the temporal-bundling technique [35] offers a sweet spot between the autoregressive and the mapping strategies for extrapolating in time, with lower rates of error accumulation.Next, we investigate the extrapolation abilities of the temporal-bundling strategy for the climate problem.</p>
<p>Climate Modeling</p>
<p>Climate models are complex, involving non-linear dynamics and multi-scale interactions between multiple variables.Describing climate behavior using PDEs is not a straightforward task.Consequently, there are many different numerical models and ML-based models [36][37][38][39][40][41] that have been developed to model climate.Furthermore, climate-related problems are challenging to approximate accurately at a low computational cost.Our goal is to efficiently learn the temporal evolution of the surface air temperature across the globe, and make accurate forecasts by extrapolating beyond the training period.</p>
<p>To achieve this goal, we use a publicly available climate dataset provided by the Physical Sciences Laboratory meteorological data: https://psl.noaa.gov/data/gridded/index.html [42].This data includes measurements of climate parameters over time, projected onto a spatial grid.Specifically, we use the daily average surface temperature data (at 1000 millibar Pressure level) from January 1, 2013, to December 31, 2015, for training; January 1, 2016, to December 31, 2017, for validating; and January 1, 2018 to December 31, 2022 for testing (see Figure 3).The data is projected to a spatial grid of dimensions 144 × 72, which corresponds to a resolution of 2.5 • in both latitude and longitude.For this particular test case, we split the data to train, validation and test sets in the ratio 30:20:50, for investigating the real-time long-term temporal extrapolation capability of DiTTO.</p>
<p>The experiments demonstrated in Figure 2</p>
<p>Hypersonic flow</p>
<p>Next, we train DiTTO to learn the inviscid airflow around a double-cone object flying at a high Mach number.The flow physics at hypersonic Mach numbers around the double-cone geometry features complex transient events, stationary bow shock at the leading nose of the cone, and the interaction of the oblique shock wave originating from the leading edge with the bow shock formed around the upper part of the geometry of the cone.The narrow band between the wall of the double-cone geometry and the bow shock wave is called the shock layer.In the shock layer, due to the interaction of shock waves, a triple point forms and generates vortical flow structures that move downstream.This interaction is challenging to capture using numerical solvers.The details regarding data creation and the governing equations are given in SM.We now present two experiments conducted for this hypersonic problem.</p>
<p>First, we learn the temporal evolution of the density field near the double cone geometry.Specifically, we train the neural operator to learn the mapping from the incoming horizontal velocity field to the time-dependent density field around the double cone structure.The dataset comprises only 61 trajectories corresponding to Mach numbers M ∈ [8,10].It is split into training, validation, and testing datasets in the ratio 80:10:10.We use a cosine annealing type learning rate scheduler starting from 10 −3 that decays to 0 during the course of the training.In this study, we compare FNO, U-Net, DiTTO and DiTTO-s.Complete results are reported in SM.We observe in Figure 4 that DiTTO is able to accurately resolve the vortices close to the surface of the double cone.Quantitative results are shown in Table 1.</p>
<p>Additionally, we perform an experiment, where instead of conditioning on time as in most of the examples presented, we explore the ability to condition DiTTO on a different quantity: the Mach Number.We train the surrogate operator to learn the mapping from the density field at a specific time step and M = 8 to the density field at the same timestep but at different Mach numbers in the range M ∈ [8,10].We compare DiTTO with other neural operators.Our results suggest that DiTTO serves as an accurate surrogate conditioned on any scalar parameter, and not necessarily time.An illustrative result is presented in Figure 4.</p>
<p>We also evaluate how well the DiTTO method can handle noise in the hypersonic scenario.We use zero-mean Gaussian additive noise, which is a common choice.Since different PDEs can behave quite differently, we ensure that the variance of the Gaussian noise is dependent on the input data.The operation of adding noise is given by: x −→ x + γN (0, σ 2 D ), where x is an input sample in the dataset D, σ 2 D is the variance of the entire dataset, and γ is the desired noise level, e.g., γ = 0.1 is equivalent to 10% noise.Importantly, we add noise only to the testing set, hence, the model does not encounter noisy samples during the training process.Results are shown in Table 2 and Figure 4.For the case without noise, we see that all the models exhibit low errors, with the DiTTO-point model having the lowest error, followed by FNO.However, when adding noise, both the DiTTO and DiTTO-point methods are able to maintain lower errors compared to FNO.Furthermore, DiTTO-point is able to handle very large amounts of noise -up to a 100% with respect to the standard deviation -and still maintain a low error rate, thus demonstrating excellent noise robustness.</p>
<p>2D and 3D Wave Propagation</p>
<p>Next, we consider the following formulation of the acoustic wave equation in 2D and 3D [44,45]: .To create the dataset, we generate several initial conditions of the same type, randomly varying the spatial location x c of the center of the source.The locations are sampled using a discrete random uniform distribution on the indices of the grid.We generate the numerical solutions using a finite-difference numerical scheme with a grid of N x = N y = 64 spatial nodes in the 2D case, and
     u tt (x, t) = c 2 (x)∆u(x, t) x ∈ Ω, 0 ≤ t ≤ 2, u(x, 0) = u 0 (x) x ∈ Ω, u t (x, 0) = 0 x ∈ Ω,(4)N x = N y = N z = 32 in the 3D case.
The results are shown in Table 1.DiTTO and DiTTO-s both achieve the lowest errors.We note that the FNO and the U-Net fare much worse compared to the other PDE cases when changing the discretization.Specifically, the errors of DiTTO and DiTTO-s are an order of magnitude lower than FNO.We hypothesize that this might be related to the use of Dirichlet boundary conditions instead of periodic ones.Another reason might be the sparsity of the data in this case, which causes the solution to change rapidly over time and have sharp features (see Figure 5).</p>
<p>Discussion</p>
<p>We have developed a new neural operator, DiTTO, for modeling time-dependent PDEs.DiTTO comprises of U-Net with spatial and channel-wise attention blocks and modified ResNet blocks that incorporate a temporal conditioning operation.The attention blocks allow DiTTO to extract spatial correlations as well as the correlations between the different representations of the inputted field variable.The ResNet block with the temporal conditioning operation learns to project the several spatial representations of the inputted field variable with respect to coefficients which are functions of time.These coefficients are linear projections of a collection of functions of time learned by an MLP.Unlike FNO and U-Net, which predict the time-evolving systems at certain discrete timesteps, DiTTO estimates the underlying dynamical operator in a continuous sense.</p>
<p>The temporal conditioning operation embedded into DiTTO enables it to estimate the state of the field variables at any desired time step within the training time interval.This makes DiTTO an ideal neural operator for temporal super-resolution tasks.We validated our hypothesis through extensive computational experiments.Specifically, we trained the neural operators -DiTTO (and its variants), FNO and U-Net, on the training datasets with a temporal resolution of N train    Neural operators learn time-dependent systems from the mappings between the initial condition and the state of the field variable u at t th time step, where, 0 &lt; t &lt; T .However, a neural operator trained only on T mappings per trajectory would fail miserably if it infers beyond the T th timestep.To extrapolate beyond the T th timestep, the inferred output û at the T th time step must be fed as an input to the neural operator.Subsequently, the neural operator collapses because the û(T ) is very dissimilar to any of the inputs (u(0)) the neural operator ever encountered during its training.On the other hand, training the neural operator autoregressively, from t th to the t + 1 th timestep may not be a suitable choice because the error has already accumulated T times by the time DiTTO predicts the T th timestep.To this end, we adopted the temporal-bundling technique and demonstrated its efficiency compared to the other two training strategies in Figure 2 b) and c).The temporal-bundling offers a lower error with lower uncertainty for temporal extrapolation tasks.The three training strategies discussed in this work are independent of the neural operator and hence they can be combined with other operators as well.</p>
<p>Lastly, we observed that DiTTO scales in dimension well compared to its competitors.We also experimented with training DeepONet.For time-dependent 1D cases, it required a very large amount of examples to learn both different initial conditions and the entire time-dependent process before converging.In multidimensions, the required amount of data was too much for our hardware to handle, and we eventually removed the DeepONet examples.Simple UNet examples were added to the comparisons with DiTTO, but could not capture complex solutions, e.g., it was not possible to train the U-Net on 3D time-dependent problems, due to their large memory footprint.For the FNO method, the issue arises since adding a temporal element is done by adding a dimension, so in the view of the U-Net and FNO, our 3D time-dependent problems are actually four-dimensional, which introduces excessive computational cost.DiTTO, on the other hand, was able to scale well since the temporal axis is taken care of by conditioning.Also, with DiTTO-point, we reduced each problem to a 1D problem, reducing the computational cost for high-dimensional problems.DiTTO-point exhibits comparable error to the original DiTTO architecture, despite being one-dimensional.This is attributed to the positional embeddings in space, which enable the model to keep track of the original spatial locations.</p>
<p>Methods</p>
<p>DiTTO -Diffusion-inspired Temporal Transformer Operator</p>
<p>DiTTO is a multiscale neural operator architecture that can be trained to effectively approximate timedependent PDEs within a data-driven framework.The input to DiTTO is a vectorized representation of the initial condition (u(x, 0)) at predefined spatial grid locations and the query time (t).DiTTO is trained to output the vectorized representation of u(x, t), making the prediction of DiTTO discrete in space and continuous in time.DiTTO is built with two components: (i) a U-shaped convolutional network (U-Net), and (ii) a time-embedding network, as illustrated in Figure 1.</p>
<p>U-Net with attention blocks:</p>
<p>The U-Net type architecture enables DiTTO to learn the underlying dynamics at different latent scales.The spatial and channel attention blocks within the U-Net facilitate efficient extraction of spatio-temporal correlations.Specifically, we implement the scaled dot product attention by linearly projecting the input representation to query (Q), key (K) and value (V ) vectors using separate 1×1 convolution layers.Next, we compute the attention matrix and calculate the output as,
out = QK T √ d V,(5)
where QK T √ d is the attention matrix with a shape of n channels × n channels or n pixels × n pixels for channel or spatial attention blocks respectively.</p>
<p>Temporal-conditioning: The time-embedding network conditions the U-Net with respect to the scalar parameter t through element-wise product operation across the channels making the DiTTO output continuous in time.The temporal conditioning mechanism is incorporated into the ResNet block, which is another building block of the U-Net.See Supplementary Information (SI) for more details on the variants of DiTTO and diffusion models in general.</p>
<p>Extrapolation in time</p>
<p>Neural operators are trained to learn time-dependent systems with the help of datasets containing trajectories arising from multiple initial conditions.Conventionally, such neural operators are trained to learn the mappings from the initial condition to all the available later timesteps of the trajectory.However, the neural operators quickly overfit the training timesteps making the scope of temporal extrapolation bleak.On the other hand, if the neural operator is trained to predict only the next timestep and consequently predict the trajectories in a purely autoregressive fashion, the accumulation of errors at every timestep hinders accurate generalization.Therefore, we train the surrogate operator to predict a bundle of future timesteps thereby minimizing the detrimental effects associated with a large number of autoregressive steps.The temporal-bundling training technique also exposes the surrogate neural operator to having intermediate states as inputs and therefore regulates itself from overfitting to having only the initial states as the inputs, as mentioned earlier.The idea was originally introduced in [35] and we demonstrate its effectiveness for extrapolation in time in a systematic manner through our experiments illustrated in Figure 2.</p>
<p>Error Metrics</p>
<p>In this work, we compute and report the relative L 2 error for quantifying the goodness of the predictions presented.The relative L 2 error is defined as,
Rel. L 2 error(u(x, t)) = ||u true (x, t) − u pred (x, t)|| 2 ||u true (x, t)|| 2(6)
Transformers (ViT) [51] split images into small patches, tokenize them, and apply the attention mechanism.</p>
<p>In addition, they are computationally lighter than other alternatives and can be easily parallelized.</p>
<p>Transformers are also becoming increasingly popular in the SciML community.Transformers have been used for operator learning in many different ways [52][53][54][55].These methods show much promise by using the attention mechanism to find connections between points in the physical domain to function values.Some methods emphasize the attention mechanism itself [56,57] and adapt it to PDE-related problems.Others utilize existing transformer models to solve PDE problems more easily [58].In this work, we employ elements from the original Transformer architecture as part of the proposed neural network architecture.</p>
<p>A.1.3 Diffusion models</p>
<p>A diffusion model is a generative deep learning model that uses a Markov chain to produce samples that match a given dataset [28].These models aim to learn the underlying distribution of a given dataset.After learning this distribution, they are used to generate new samples of similar properties to those found in the training set.</p>
<p>In [8], Ho et al. introduced a new type of diffusion model called denoising diffusion probabilistic models (DDPM).It consists of a forward diffusion process and an inverse one.In the forward case, Gaussian noise is incrementally added to the original sample for a given number of iterations.For a sufficiently large number of iterations, the noise completely destroys the original signal.Then, in the reverse diffusion process, the goal is to reconstruct the original signal by performing iterative denoising steps using a neural network.Diffusion models have been used for SciML purposes, especially for generative artificial intelligence purposes [59][60][61].While we are not using their generative capabilities in this work, we briefly explain their standard training procedure.</p>
<p>We present a mathematical formulation mostly based on the works of Ho et al. [8] and Nichol et al. [62].Given a data distribution x 0 ∼ q(x 0 ), we define a forward noising process q which produces steps x 1 , . . ., x T by adding Gaussian noise at time t with variance β t ∈ (0, 1) as follows:
q(x 1 , . . . , x T |x 0 ) := T t=1 q(x t |x t−1 ),(9)q(x t |x t−1 ) := N (x t ; 1 − β t x t−1 , β t I). (10)
Given a sufficiently large T and a well-behaved schedule β t , the latent x T is nearly an isotropic Gaussian distribution.From (10), we see that x t is drawn from a conditional Gaussian distribution with mean µ t = √ 1 − β t x t−1 and variance σ 2 t = β t .In practice, this is done by randomly sampling a noise level parameter ε ∼ N (0, I), and setting:
x t = 1 − β t x t−1 + β t ε.(11)
Thus, if we know the exact reverse distribution q(x t−1 |x t ), we can sample x T ∼ N (0, I) and run the process in reverse to get a sample from q(x 0 ).However, since q(x t−1 |x t ) depends on the entire data distribution, we approximate it using a neural network with hyperparameters θ as follows:
p θ (x t−1 |x t ) := N (x t−1 ; µ θ (x t , t), Σ θ (x t , t)). (12)
The neural network needs to learn the mean and variance to complete the backward diffusion process.Importantly, using the formulation in (11), in each step, it is sufficient to know β t , x t , and ε to approximate x t−1 .Then, the network is used autoregressively to reconstruct x 0 .Assuming we know the schedules {β t } T t=1 , we can view the neural network as the following mapping:
(x t , ε) −→ x t−1 .(13)
In each step, the neural network performs a denoising operation, mapping x t to a slightly less noisy signal x t−1 .Including the noise level parameter ε is essential for the denoising operation.During training, various noise levels are sampled, and knowing the specific noise level that distinguishes between consecutive states x t and x t−1 , is crucial for effective denoising.Without this explicit knowledge of the noise level, the denoising process would become significantly more complicated, and the network may not converge.This means we have a conditional denoising operation, conditioned on ε (or equivalently on the timestep with β t ).</p>
<p>A.2 DiTTO: Diffusion-inspired temporal transformer operator</p>
<p>As described in Results, we propose using temporal conditioning, instead of conditioning on noise as in Supplement A.1.3.We use the initial condition of a differential equation, and infer the entire temporal process.It operates on various initial conditions, hence it is performing operator learning according to Supplement A.1.1.In addition, after training, the inference is possible on any real temporal value.Hence, the inference is continuous in time.As shown in the results, not only interpolation in time is possible, but also extrapolation.</p>
<p>To train the DiTTO network we gather data of multiple time-dependent procedures, varying in the initial condition.Then, we use the network architecture in Results, and select a training strategy (for example, temporal-bundling), to fit the data.We elaborate on these steps in the following subsections.</p>
<p>A.2.1 Training dataset</p>
<p>To train a neural network using the formulation presented in Results, we require a large set of initial conditions (inputs) and corresponding solutions (outputs).Let {I m (x)} M m=1 be a set of initial conditions with corresponding analytic solutions {u m (x, t)} M m=1 , where M is the desired number of training samples.Each sample consists of an initial condition and a PDE solution at the relevant timesteps.In practice, {u m (x, t)} M m=1 are numerical approximations of the analytic solutions and not analytic solutions which are often unavailable.Furthermore, the solutions are discretized in space using a grid that partitions the domain D. We emphasize that for all m = 1, . . ., M and t = 0, . . ., T , u m (x, t) is a matrix, and its dimensions depend on the spatial discretization parameters, i.e., the number of nodes along each axis.</p>
<p>We denote the forward process corresponding to the m-th initial condition and solution by {x m n } T n=0 , where x m n := u m (x, t n ).We define the following datasets:
X = {(x m 0 , t n )| n = 1, . . . , T, m = 1, . . . , M } Y = {x m n | n = 1, . . . , T, m = 1, . . . , M } .(14)
So, each solution of the PDE is transformed into T pairs of samples that correspond to the mapping described in Equation (3).</p>
<p>A.2.2 The DiTTO neural network architecture and parameters</p>
<p>We use the architecture described in Results.We mention that for the temporal input t, we use an embedding mechanism based on the original Transformer positional encoding [29]:
P E (pos,2i) = sin pos 10000 2i /d emb , P E (pos,2i+1) = cos pos 10000 2i /d emb ,(15)
where d emb is the desired embedding dimension.Each scalar t is mapped into a vector of size d emb .</p>
<p>We now describe the loss function used as a target for training the network.Let O θ be the neural network described in Results with hyperparameters θ.The goal of O θ is to learn the mapping described in (3), using the dataset (14).We split this dataset into training, validation, and testing sets.We split them in a way that makes sure that no initial conditions from the validation and testing sets appear in the training set.</p>
<p>Diffusion models are often trained with a probabilistic loss function.However, since we learn a PDE operator, other loss functions commonly used for SciML applications are more fitting.Consequently, we train the network with a mean relative L 2 loss:
loss := 1 M T M m=1 T n=1 ||O θ (x m 0 , t n ) − x m n || 2 ε + ||x m n || 2 , (16)
where ε is a small number used to prevent a zero denominator and stabilize the loss.The inputs and outputs of the model are d-dimensional, so they are converted into a one-dimensional array by column stacking (flattening) inside the loss function when needed.We describe the loss for the entire dataset for simplicity, but in practice, we divide it into batches.</p>
<p>Iterating over the entire dataset ( 14) can be time-consuming.For M train initial conditions in the training set, we have M train • T samples.So, the number of training steps scales linearly with T .This means the number of training samples is very large for fine temporal discretizations.</p>
<p>A similar problem occurs in generative diffusion models.The original DDPM [8] requires hundreds of forward passes to produce good results.Later works suggested ways to improve the performance aspect of DDPMs.For example, Song et al. [63] suggest using non-Markovian processes to improve the computational cost.Nichol et al. [62] present a way to significantly reduce the number of necessary steps by sub-sampling the original diffusion process.Both methods focus primarily on the inference speed.However, in the case of DiTTO, inference is immediate.In Results, we explained that we do not view x 0 , x 1 , . . ., x T as an iterative process.Instead, we treat each sample individually, significantly increasing the inference speed compared to generative models such as DDPM.Hence, we focus on speeding up the training process.</p>
<p>A.2.3 DiTTO-s</p>
<p>We propose DiTTO-s, a faster variant of DiTTO that relies on a sub-sampling mechanism similar to [62].Instead of iterating over the entire process, we iterate over a random subsequence.Recall that for the m-th initial condition in the training set, the full process is {x m n } T n=1 .Instead, we take a set of random subsequences S m ⊂ {0, 1, . . ., T }, such that M m=1 |S m | = αM T for some α &lt; 1.For example, choosing α = 0.05 means we only use 5% of the given samples in each epoch.The new DiTTO-s loss is given by:
loss α := 1 αM T M m=1 n∈Sm ||O θ (x m 0 , t n ) − x m n || 2 ε + ||x m n || 2 ,(17)
After each epoch, we randomly sample S m again using a uniform distribution.That way, given a sufficiently large number of epochs, we expect to cover a significant portion of samples in the dataset.</p>
<p>A.2.4 DiTTO-point</p>
<p>The architecture shown in Figure 1 can be used for problems in different dimensions.One way to accomplish that is to use d-dimensional convolutions in all the convolutional layers, where d ∈ {1, 2, 3} is the physical dimension of the problem.However, using high-dimensional convolutions requires a large amount of memory.Furthermore, many physical problems are not defined on structured grids.Thus, in order to use a neural operator framework, it is often necessary to project their solutions onto regular grids [48], which requires a change to the geometry of the problem.</p>
<p>To address these issues, we propose DiTTO-point, another variant of DiTTO that solves highdimensional problems using exclusively 1-D convolutions.With DiTTO-point we treat the domain as a set of points in space instead of a structured d-dimensional grid.A domain with N points is defined as a N × d matrix, where each row represents a spatial coordinate.Similarly, we define the solution on this domain using a vector of size N , corresponding to the values of the solution at each point of the domain.Using this formulation, regardless of the original dimensionality of the problem, the input to DiTTO-point would always be of size N × d.This enables the use of 1-D convolutions on this data, where d is the number of input channels.</p>
<p>However, directly using the architecture shown in Figure 1 with 1-D convolution does not work, without any modifications, on high-dimensional problems.Importantly, switching from a structured grid to a set of coordinates results in the loss of spatial information.This is especially true when the order in which the coordinates appear is not necessarily related to their physical distance.To solve this issue, we use another layer of positional encoding (see Equation ( 15)) based on the spatial coordinates of the grid.We apply this layer to the beginning of the overall architecture before we concatenate its output with the relevant initial condition in the latent space.This enables DiTTO-point to keep high-dimensional spatial information while using a one-dimensional architecture.</p>
<p>A.2.5 DiTTO-gate</p>
<p>We propose another variant of the architecture shown in Figure 1, called DiTTO-gate.In DiTTO-gate we modify the behavior of the skip connections of the U-Net decoder.An analysis of diffusion models shows that the U-Net skip connections introduce high-frequency features into the decoder [64].So, for scenarios with fine details and sharp features (such as Navier-Stokes with high Reynolds numbers), we put extra emphasis on the skip connections.So, we introduce a gate component, which operates directly on the skip connections.This component is composed of a standard convolutional block, as described in Results.So, in DiTTO-gate, we add such a component to each level of the decoder and use it on its incoming skip connection.</p>
<p>B Hypersonic flow data generation and details</p>
<p>We first generate a training dataset by solving the 2D Euler equations on a fluid domain around a double-cone geometry.The governing equations read
∂U ∂t + ∂F ∂x + ∂G ∂y = 0,(18)
where the vector of conservative variables, x-direction and y-direction flux vectors, are described as
U =     ρ ρu ρv ρE     , F =     ρu ρu 2 + p ρuv u(ρE + p)     , G =     ρv ρvu ρv 2 + p u(ρE + p)     .(19)
In Equation (18), t is time, x, y denote the spatial coordinates, u and v indicate x-direction and y-direction velocities, ρ is density, and p represents the pressure.The total energy in Equation ( 19) is illustrated as
ρE = p γ − 1 + 1 2 ρ(u 2 + v 2 ), (20)
where γ is the ratio of specific heats that is assumed constant with a value of 1.4.We solve the system of equations using Trixi.jlnumerical framework [65], which employs entropy stable discontinuous Galerkin spectral element (ES-DGSEM) approach [66][67][68] to solve hyperbolic and parabolic systems of equations.The ES-DGSEM features high accuracy and stability and also employs adaptive mesh refinement to adapt the mesh resolution automatically to the high gradient regions of the flow field.The 2D physical domain is shown in Figure 6, where the boundary conditions are specified.The domain is initialized using constant uniform values for primitive variables as u = M , v = 0, p = 1.0, and ρ = 1.4.We initialize the x-direction velocity with the value of the free-stream Mach number denoted by M .Each simulation is performed for a time span of t ∈ [0, 0.04], and the solution values are stored at 201 snapshots corresponding to equidistance instances of time.</p>
<p>C.1 One-dimensional Burgers' Equation</p>
<p>The one-dimensional time-dependent Burgers' equation for a viscous fluid is given by:
∂ t u(x, t) + ∂ x (u 2 (x, t)/2) = ν∂ xx u(x, t), x ∈ (0, 1), t ∈ (0, t f inal ] u(x, 0) = u 0 , x ∈ (0, 1) ,(21)
where ν ∈ R + is the viscosity coefficient and is subject to periodic boundary conditions.The initial condition u 0 (x) is sampled from a Gaussian random field according to the following distribution: N (0, 625(−∆ + 25I) −2 ), where N is the normal distribution, and ∆, I are the Laplacian and identity operators, respectively.We use the publicly available MATLAB [69] solver given in [3] to create three separate datasets with different parameters.The first dataset is created with ν = 0.01, t f inal = 1, and N x = 128.This is a relatively simple scenario since these parameters produce smooth solutions without shocks.In the second   7a to 7c, we see a comparison between the models at different times.We plot the predictions of the models alongside the reference solution (ground truth).dataset, we decrease the viscosity coefficient to ν = 0.001, which increases the shock-like behavior of the Burgers' equation.For this reason, we also increase the spatial discretization to N x = 256.Finally, the third dataset is the same as the second one, except we increase the final simulation time to t f inal = 2, which causes the shocks to be more pronounced.</p>
<p>The results for the three scenarios are shown in Table 1.DiTTO and DiTTO-s achieve the lowest errors for the three datasets.When N test t = N train t = 50, all methods with DiTTO and DiTTO-s having a slight advantage.However, when N test t ̸ = N train t , DiTTO and DiTTO-s significantly outperform the FNO and the U-Net.Moreover, we observe that the DiTTO and DiTTO-s results do not depend on the temporal discretization, as the errors stay roughly the same for all values of N test t .We also note that DiTTO-s has a slightly lower error than the full DiTTO.This demonstrates that the sub-sampling mechanism does not only require fewer training steps but also improves the model.The reason for this is that the sub-sampling mechanism acts as a form of regularization that helps decrease the error.</p>
<p>C.2 Two-dimensional Navier-Stokes Equations</p>
<p>The time-dependent two-dimensional incompressible Navier-Stokes equation for a viscous, incompressible fluid in vorticity form is given by:  where ω is the vorticity, u is the velocity field, ν is the viscosity, and ∆ is the two-dimensional Laplacian operator.We consider periodic boundary conditions.The source term f is given by f (x, y) = 0.1(sin(2π(x + y)) + cos(2π(x + y))), and the initial condition ω 0 (x) is sampled from a Gaussian random field according to the distribution N (0, 7 3/2 (−∆ 49I) −5/2 ).
     ∂ t ω(x, y, t) + u(x, y, t) • ∇ω(x, y, t) = ν∆ω(x, y, t) + f (x, y), x, y ∈ (0, 1) 2 , t ∈ (0, t f inal ] ∇ • u(x, y, t) = 0, (x, y) ∈ (0, 1) 2 , t ∈ (0, t f inal ] ω(x, y, 0) = ω 0 , (x, y) ∈ (0, 1) 2(22)
We use the available Python solver given in [3] to generate two datasets with a spatial resolution of N x = N y = 64.The first dataset is created with ν = 10 −3 and t f inal = 50, resulting in a Reynolds number Re ≈ 20.For the second dataset we use ν = 10 −5 and t f inal = 20, resulting in a Reynolds number Re ≈ 2, 000.</p>
<p>The error comparison for the two datasets is shown in Table 1.For Re ≈ 20, both DiTTO and DiTTO-s outperform the other models across all temporal discretizations while keeping similar error values.For Re ≈ 2, 000, it is clear that increasing the Reynolds number also increases the difficulty of the problem, as evidenced by the noticeably higher errors for all models.We also see that the</p>
<p>D Ablation study</p>
<p>Here we present an ablation study related to the results shown in Results.Due to the computational cost of many experiments, we conducted the ablation study on one fixed scenario.Specifically, we used the Burgers' dataset described in Supplement C.1 with ν = 0.01, T = 1, N x = 128, and N train t = 50.Influence of attention We examined the impact of the attention mechanism.To assess its contribution, we trained two models and compared them.The first model was DiTTO, as described in Results.The second model was the same, except we removed all attention layers.The full results are shown in Effect of sub-sampling rate We experiment with different values of α as described in Supplement A.2.3 and train several DiTTO-s models with corresponding sub-sampling rates.The results for α ∈ {0.05, 0.1, 0.2, 1} are shown in Table 5.We see that using the full data in each batch (i.e., α = 1) does not necessarily produce the best results.Choosing α = 0.1 and α = 0.2 produced lower errors compared to α = 1.However, α = 0.05 was too small and consequently increased the error.Hence, choosing α = 0.1 both improved the results and effectively reduced the batch size by 90%, and thus reduced the memory footprint of the model.FNO grid search To increase the validity of the comparison with FNOs, we conducted a hyperparameter search over various FNO architectures.We focused on the number of modes and the depth of the network, as defined in [3].We tested for N test t &gt;= 50 to avoid padding issues for higher numbers of Fourier modes.The results are presented in Table 6.While wider, deeper FNO models generally obtained better results for N test t = N train t , we found that 4 layers with 18 Fourier modes gave the best results in the zero-shot super-resolution case.We followed a similar process for FNO-3D and decided to use 4 layers with 12 Fourier modes for the two-dimensional time-dependent problems.</p>
<p>Figure 1 :
1
Figure 1: DiTTO architecture.The discretized initial condition u(x, 0) concatenated with the corresponding spatial grid, and the desired time t ∈ R + are the respective inputs to the U-Net and the time-embedding network comprising DiTTO.The U-Net illustrated here consists of ResNet blocks with temporal conditioning, a Spatial-Attention block, and Channel-Attention blocks at 4 levels of coarseness, and the corresponding residual connections across the same levels.The ResNet block conditions the non-linear representations of u(x, 0) with respect to the temporal embedding vector, ⃗ f (t), by performing element-wise multiplication across the channels.Spatial-Attention and Channel-Attention blocks learn to extract correlations across space and channels respectively.</p>
<p>Figure 2 :
2
Figure 2: Temporal-bundling for efficient extrapolation.a) demonstrates 3 types of time-series modeling strategies for extrapolating beyond the training interval.For display purposes, we consider a time-series with 20 time steps.b) visualizes the error accumulation for DiTTO models with different look-forward windows (lf ) trained in the time interval 0-100 and further extrapolated from 100-200.c) shows the relative L 2 errors (in percentage) at the 200 th time step, corresponding to DiTTO models with different lf.We note that the results in b) and c) come from the test dataset with previously unseen 100 initial conditions.The symbols in (b-c) denote the corresponding final time.</p>
<p>motivated the adoption of the temporal-bundling technique with lf = 365 for training DiTTO.The training dataset can be interpreted as a single trajectory consisting of 1095 global temperature snapshots arising from the initial condition on Jan. 1, 2013.We utilize temporal-bundling with lf = 365 to split this trajectory into 730 sub-trajectories, each comprising 365 time steps.We further reduce the computational costs associated with the training and inference, by performing a proper orthogonal decomposition and training DiTTO to learn the evolution of the eigen coefficients corresponding to the five dominant eigenvectors estimated from the global temperature profiles in the training dataset.The results in Figure 3 indicate that DiTTO is able to extrapolate for long time intervals without any substantial accumulation of errors.The contour plots illustrate the measured (left) and predicted (right) global temperature profiles in Spring, Summer, Autumn and Winter.DiTTO is able to reduce the growth of the relative L 2 error across the 5 years of extrapolation with a mean of 0.014.</p>
<p>Figure 3 :
3
Figure 3: Five-year climate forecast (extrapolation).DiTTO is trained on the global temperature data available in NCEP-NCAR Reanalysis 1 dataset from years 2013 to 2015, validated from 2016 to 2017, and extrapolated from 2018 to 2022.The contour plots on the left and right columns correspond to snapshots from five years of measured and forecasted global temperature distribution respectively.The four rows correspond to the global temperature profiles in Spring, Summer, Autumn, and Winter.The effectiveness of the proposed approach to learn the seasonal climate trends is visually illustrated using the contour plots and quantitatively reported on the error accumulation curve on the left: mean error is 1.4%.</p>
<p>where u(x, t) is the wave amplitude or acoustic pressure, c(x) is the wave propagation speed, Ω = [0, π] d where d ∈ {2, 3} is the physical domain, ∆ is the 2D/3D Laplacian operator, and the PDE is subject to homogenous Dirichlet (fully reflective) boundary conditions.We set the speed of sound of the medium as c(x, y) = (1 + sin (x) sin (y)) in 2D and c(x, y, z) = (1 + sin (2x) sin (y) sin (z)) in 3D.The initial condition u 0 is chosen to be a Gaussian source of the form: %beginequation u(x, 0) = e − ||x−xc || 2 2 10</p>
<p>t= 50
50
and made inferences on the test datasets with temporal resolutions of N test t = 10, 20, 50, 100, 200.</p>
<p>Figure 4 :
4
Figure 4: Modeling high-speed flow past a double-cone.a) The image of the double-cone model installed in the LENS XX tunnel [43].b) The histogram illustrates the sensitivity of (i) DiTTO-point, (ii) DiTTO, and (iii) FNO to different noise levels γ. c) Temporal evolution of the density field (ρ) corresponding to test Mach number (M ) = 9.18 around the double cone geometry predicted by DiTTO-point and its comparison with the solver simulation.d) Additional illustration of DiTTO-point conditioned on M , and trained to predict the final state of ρ.A video illustrating the flow is included in SI.</p>
<p>Figure 5 :&gt; 50 ,
550
Figure 5: Illustrations of 2D (left) and 3D (right) wave propagation numerical experiments.The predictions are made using the DiTTO-s variant.The initial emmitted wave is a small Gaussian eruption bell in 2D and sphere in 3D.The boundaries are reflective.Videos illustrating the process are included in SI.</p>
<p>Figure 6 :
6
Figure 6: The 2D domain of the double cone problem.The gray area is the domain and the highlighted lines show the surface of the double cone which is considered as a slip wall boundary condition.The small line at the bottom is defined as a symmetry condition and the Inflow and outflow boundaries are also shown.</p>
<p>Figure 7 :
7
Figure 7: Results for the Burgers' problem described in Supplement C.1 with ν = 0.001, t f inal = 2, N x = 256, N train t</p>
<p>Figure 8 :
8
Figure 8: Navier-Stokes equations Equation (22) with Re ≈ 2, 000.</p>
<p>FNO errors are closer to DiTTO-s compared to other cases, even having a slight advantage when N test t ∈ {50, 100}.</p>
<p>Figure 9 :
9
Figure 9: 2D Acoustic wave equation 4.</p>
<p>α</p>
<p>N test t allow us to examine the results in three different regimes. First, when N test t = 50, we test the model on the same temporal resolution it was trained on. We note that the model is able to handle coarser temporal grids for N test t ∈ {10, 20}. Finally, we consider a zero-shot super-resolution scenario in time for N test t ∈ {100, 200}, exploring the interpolation capabilities of the model on unseen temporal discretizations. Note that DiTTO and its variants are entirely meshfree in time due to the time conditioning mechanism.
train t= 50 time steps, and test on N test t∈ {10, 20, 50, 100, 200} (exact details are given per problem).These choices of</p>
<p>Table 1
1
indicates that</p>
<p>Table 1 :
1
A full error analysis of all cases comparing the various DiTTO methods with FNO and U-Net across different temporal resolutions.Results for additional cases are shown in SI.
ScenarioModelN test t= 10 N test t= 20 N test t= 50 N test t= 100 N test t= 200DiTTO0.22530.12640.02240.06010.08152D time-dependentDiTTO-point0.22630.12800.02510.05730.0779hypersonic flowDiTTO-s0.22430.13210.06550.07210.0854DiTTO-point-s0.20890.12770.11750.13640.1474DiTTO-point-s-gate0.23520.14900.07220.06990.0790FNO0.47020.12120.05100.0746N/A2D Navier Stokes Re ≈ 2, 000, #samples = 5, 000DiTTO-s DiTTO-point-s-gate DiTTO-point-s FNO0.1701 0.1936 0.1721 0.39230.1113 0.1322 0.1132 0.19860.0919 0.1014 0.0945 0.12600.0913 0.0944 0.0942 0.13120.0923 0.0919 0.0953 0.1398DiTTO0.00910.02460.00770.03790.04812D wave equationDiTTO-point DiTTO-s0.0140 0.03430.0137 0.03200.0122 0.03010.0324 0.03540.0391 0.0376DiTTO-point-s0.05190.04860.04500.05400.0576DiTTO-point-s-gate0.03350.03160.02960.03820.0412FNO1.32530.49030.14600.20930.2636UNet1.39561.39980.02021.25011.2595DiTTO-s0.01920.01660.01510.01480.01473D wave equationDiTTO-point-s DiTTO-point-s-gate0.0384 0.03730.0350 0.03390.0331 0.03190.0326 0.03140.0324 0.0311</p>
<p>Table 2 :
2
Relative L 2 test set errors for the hypersonic flow case Hypersonic flow for different noise levels.
ModelNoise level0%10%20%30%50%100%DiTTO0.0303 0.0360 0.0464 0.0591 0.0921 0.2030DiTTO-point0.0152 0.0155 0.0167 0.0188 0.0253 0.0540FNO0.0241 0.0795 0.1519 0.2229 0.3544 0.6080C Additional resultsScenarioModelN test t= 10 N test t= 20 N test t= 50 N test t= 100 N test t= 200DiTTO0.00600.00590.00580.00650.00701D Burgers'DiTTO-s0.00570.00560.00550.00610.0066t f inal = 1, ν = 0.01FNO0.16720.02170.00590.01000.0124U-Net0.40960.16680.00940.10170.1782DiTTO0.01640.01570.01540.01530.01521D Burgers'DiTTO-s0.01240.01190.01160.01160.0116t f inal = 1, ν = 0.001FNO0.28990.04610.02220.02650.0299U-Net0.47890.28070.02980.20420.3188DiTTO0.02080.01990.01950.01950.01961D Burgers'DiTTO-s0.01720.01640.01600.01600.0161t f inal = 2, ν = 0.001FNO0.35010.05820.02480.03150.0367U-Net0.66550.37950.03820.24260.3167DiTTO0.29480.15340.07110.04650.03812D Navier StokesDiTTO-point0.29820.15400.06680.04010.0291Re ≈ 20DiTTO-s0.29730.15530.07150.04750.0392DiTTO-point-s0.30620.15890.06940.04180.0307DiTTO-point-s-gate0.29640.15510.07070.04530.0359FNO0.32300.14110.05230.06900.0848UNet0.64670.83320.04890.63610.61542D Navier Stokes Re ≈ 2, 000, #samples = 1, 000DiTTO DiTTO-point DiTTO-s DiTTO-point-s0.2491 0.2344 0.2414 0.23050.1987 0.1813 0.1910 0.17640.1745 0.1550 0.1669 0.15050.1685 0.1485 0.1610 0.14430.1660 0.1458 0.1586 0.1418DiTTO-point-s-gate0.21870.16460.13810.13170.1291FNO0.35600.20400.16170.16510.1700UNet0.70330.67230.19550.63310.7385</p>
<p>Table 3 :
3
A full error analysis of all cases comparing the various DiTTO methods with FNO and U-Net across different temporal resolutions.</p>
<p>Table 4 .
4
It is clear that the attention mechanism roughly halved the error values across all choices of N t .
N test t= 10 N test t= 20 N test t= 50 N test t= 100 N test t= 200DiTTO0.00600.00590.00580.00650.0070DiTTO (no attention)0.01320.01280.01260.01270.0128</p>
<p>Table 4 :
4
Relative L 2 test set errors with and without the attention mechanism for the Burgers' scenario in Supplement C.1 with t f inal = 1, ν = 0.01, N x = 128, N train</p>
<p>t = 50.</p>
<p>Table 5 :
5
(21)tive L 2 test set errors using different sub-sampling rates α for the Burgers' scenario in Equation(21)with t f inal = 1, ν = 0.01, N x = 128, N train
N test t= 10 N test t= 20 N test t= 50 N test t= 100 N test t= 20010.00600.00590.00580.00650.00700.20.00580.00560.00560.00610.00660.10.00570.00560.00550.00610.00660.050.02500.02470.02470.02480.0248
t = 50.</p>
<p>Table 6 :
6
Relative L 2 test set errors using different FNO hyperparameter choices for the Burgers' scenario in Supplement C.1 with t f inal = 1, ν = 0.01, N x = 128, N train</p>
<h1>Fourier modes # Layers N test t= 50 N test t= 100 N test t= 2001240.00780.01120.01311840.00480.00800.01002440.00460.01460.01843040.00410.02620.03121280.00670.00870.01041880.00520.02050.02742480.00460.01620.02173080.00410.02500.0292</h1>
<p>t = 50.</p>
<p>AcknowledgementThis work was supported by the Vannevar Bush Faculty Fellowship award (GEK) from ONR (N00014-22-1-2795).It is also supported by the U.S. Department of Energy, Advanced Scientific Computing Research program, under the Scalable, Efficient and Accelerated Causal Reasoning Operators, Graphs and Spikes for Earth and Embedded Systems (SEA-CROGS) project, DE-SC0023191.Supplementary Information A Detailed methodsA.1 BackgroundA.1.1 Operator learningThe standard use of ML models for scientific computations involves fitting a function to map numerical inputs to outputs.These inputs are ordinarily coordinates, materials, boundary conditions, etc., and the outputs are usually solutions of forward PDEs.An example is physics-informed neural networks (PINNs)[1], which use a deep neural network to solve PDEs by embedding elements from the PDE problem into the loss function.In this way, the network trains on the given data while using prior information about the problem it is solving.One major drawback is that for each problem, one needs to re-train the network, which is computationally expensive.This includes any changes to the parameters defining the problem, such as the inputs mentioned above.The field of operator learning seeks to overcome this problem.Instead of fitting a function, one fits a mapping between two families of functions.Mathematically, consider a generic family of d-dimensional time-dependent PDE problems of the form:where the differential operator L and forcing term f define the PDE, the boundary operator B and boundary condition g define the solution on the boundary, t f inal is the final physical time, I is the initial condition, and D is a Euclidean domain in R d with boundary ∂D.We assume that the problem (7) is well-posed[46], so a unique solution exists.Let I be a function space containing initial conditions of(7).Then there exists another space U that contains their respective solutions.We define an operator G : I −→ U as follows:where I ∈ I, x ∈ D, and t ∈ [0, t f inal ].So, each initial condition I ∈ I is mapped into its corresponding solution u ∈ U.The goal is to approximate the operator G using a neural network.The first SciML operator learning method, called DeepONet, was proposed by Lu et al.[2].The main components of a DeepONet are two neural networks: the branch and the trunk.Each network can be a fully-connected neural network, convolutional, or any other architecture.Usually, the branch inputs are functions, and the trunk inputs are coordinates.DeepONets learn projections from the functions to a vector space, so they can map input functions to output functions at specific points.Another operator learning approach is the Fourier neural operator (FNO)[3,47].FNOs, similarly to DeepONets, learn mappings between function spaces using projections.Specifically, FNOs utilize the Fourier transform.They are effective and easy to implement, gaining traction in the SciML community.FNOs are accurate, especially for smooth and periodic problems[48].We note that while the Fourier kernel is continuous, in practice, it is necessary to use discrete versions for operator learning.Consequently, FNOs can be computationally costly when working with high-dimensional problems requiring many Fourier modes.A.1.2 Transformers and attentionFirst presented by Vaswani et al.[29], transformers have been widely used in the ML community.Transformers introduce a new type of mechanism called the scaled dot-product attention.The attention module attempts to gather context from the given input.It does so by operating on a discrete embedding of the data composed of discrete tokens.The original architecture was proposed for natural language processing purposes, where one encodes sentences using their enumerated locations in the vocabulary.Since then, their usage has been extended to many other domains, and they outperform many different deep learning architectures in a wide variety of tasks.These domains include time series analysis[49]and computer vision[50].For example, Vision
Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Maziar Raissi, Paris Perdikaris, George E Karniadakis, Journal of Computational physics. 3782019</p>
<p>Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, George Em Karniadakis, Nature machine intelligence. 332021</p>
<p>Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar, arXiv:2010.08895Fourier neural operator for parametric partial differential equations. 2020arXiv preprint</p>
<p>Pde-net: Learning pdes from data. Zichao Long, Yiping Lu, Xianzhong Ma, Bin Dong, International conference on machine learning. PMLR2018</p>
<p>Dl-pde: Deep-learning based data-driven discovery of partial differential equations from discrete and noisy data. Hao Xu, Haibin Chang, Dongxiao Zhang, arXiv:1908.044632019arXiv preprint</p>
<p>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-Rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, IEEE Signal Processing Magazine. 2962012</p>
<p>Imagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in Neural Information Processing Systems. 252012</p>
<p>Denoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in Neural Information Processing Systems. 202033</p>
<p>The Finite Element Method: Linear Static and Dynamic Finite Element Analysis. J R Thomas, Hughes, Courier Corporation. 2012</p>
<p>Finite difference method for numerical computation of discontinuous solutions of the equations of fluid dynamics. K Sergei, I Godunov, Bohachevsky, Matematičeskij Sbornik. 4731959</p>
<p>Finite volume methods. Robert Eymard, Thierry Gallouët, Raphaèle Herbin, Handbook of Numerical Analysis. 72000</p>
<p>George Karniadakis, Spencer J Sherwin, Spectral/HP Element Methods for Computational Fluid Dynamics. USAOxford University Press2005</p>
<p>Acoustic source localization. Tribikram Kundu, Ultrasonics. 5412014</p>
<p>Beyond the courant-friedrichs-lewy condition: Numerical methods for the wave problem using deep learning. Oded Ovadia, Adar Kahana, Eli Turkel, Shai Dekel, Journal of Computational Physics. 4421104932021</p>
<p>Physicsinformed neural networks (pinns) for fluid mechanics: A review. Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, George Em Karniadakis, Acta Mechanica Sinica. 37122021</p>
<p>Enrui Zhang, Adar Kahana, Eli Turkel, Rishikesh Ranade, Jay Pathak, George Em Karniadakis, arXiv:2208.13273A hybrid iterative numerical transferable solver (hints) for pdes based on deep operator network and relaxation methods. 2022arXiv preprint</p>
<p>Benchmark problems for the mesoscale multiphysics phase field simulator (memphis). Remi Philippe, Michel Dingreville, James Allen Stewart, Elton Y Chen, 2020Albuquerque, NM (United StatesSandia National Lab.(SNL-NM)Technical report</p>
<p>Learning two-phase microstructure evolution using neural operators and autoencoder architectures. Khemraj Vivek Oommen, Somdatta Shukla, Rémi Goswami, George Em Dingreville, Karniadakis, Computational Materials. 811902022</p>
<p>A spectral element method for fluid dynamics: laminar flow in a channel expansion. Anthony T Patera, Journal of Computational Physics. 5431984</p>
<p>Turbulence statistics in fully developed channel flow at low Reynolds number. John Kim, Parviz Moin, Robert Moser, Journal of Fluid Mechanics. 1771987</p>
<p>Robust automatic p-phase picking: an on-line implementation in the analysis of broadband seismogram recordings. Reinoud Sleeman, Torild Van Eck, Physics of the earth and planetary interiors. 1131-41999</p>
<p>Wavelet neural operator for solving parametric partial differential equations in computational mechanics problems. Tapas Tripura, Souvik Chakraborty, Computer Methods in Applied Mechanics and Engineering. 4041157832023</p>
<p>Qianying Cao, Somdatta Goswami, George Em Karniadakis, Lno, arXiv:2303.10528Laplace neural operator for solving differential equations. 2023arXiv preprint</p>
<p>Oded Ovadia, Adar Kahana, Panos Stinis, Eli Turkel, George Em Karniadakis, Vito, arXiv:2303.08891Vision transformer-operator. 2023arXiv preprint</p>
<p>Zachary E Md Ashiqur Rahman, Kamyar Ross, Azizzadenesheli, arXiv:2204.11127U-shaped neural operators. U-no2022arXiv preprint</p>
<p>Pdebench: An extensive benchmark for scientific machine learning. Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Daniel Mackinlay, Francesco Alesiani, Dirk Pflüger, Mathias Niepert, Advances in Neural Information Processing Systems. 202235</p>
<p>Towards multi-spatiotemporal-scale generalized pde modeling. K Jayesh, Johannes Gupta, Brandstetter, arXiv:2209.156162022arXiv preprint</p>
<p>Deep unsupervised learning using nonequilibrium thermodynamics. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli, International Conference on Machine Learning. PMLR2015</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Dan Hendrycks, Kevin Gimpel, arXiv:1606.08415Gaussian error linear units (gelus). 2016arXiv preprint</p>
<p>U-net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference. Munich, GermanySpringerOctober 5-9, 2015. 2015Proceedings, Part III 18</p>
<p>Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. Tim Salimans, Andrej Karpathy, Xi Chen, Diederik P Kingma, arXiv:1701.055172017arXiv preprint</p>
<p>Sergey Zagoruyko, Nikos Komodakis, arXiv:1605.07146Wide residual networks. 2016arXiv preprint</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Johannes Brandstetter, Daniel Worrall, Max Welling, arXiv:2202.03376Message passing neural pde solvers. 2022arXiv preprint</p>
<p>The community earth system model: a framework for collaborative research. Marika M James W Hurrell, Holland, Steven Peter R Gent, Jennifer E Ghan, Paul J Kay, J-F Kushner, William G Lamarque, Large, Keith Lawrence, Lindsay, 2013American Meteorological Society94</p>
<p>Learning bias corrections for climate models using deep neural operators. Aniruddha Bora, Khemraj Shukla, Shixuan Zhang, Bryce Harrop, Ruby Leung, George Em Karniadakis, arXiv:2302.031732023arXiv preprint</p>
<p>Learning operators with coupled attention. Georgios Kissas, Leonardo Jacob H Seidman, Ferreira Guilhoto, M Victor, George J Preciado, Paris Pappas, Perdikaris, The Journal of Machine Learning Research. 2312022</p>
<p>Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, arXiv:2202.11214A global data-driven high-resolution weather model using adaptive fourier neural operators. 2022arXiv preprint</p>
<p>Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Alexander Pritzel, Suman Ravuri, Timo Ewalds, Ferran Alet, Zach Eaton-Rosen, arXiv:2212.12794Learning skillful medium-range global weather forecasting. 2022arXiv preprint</p>
<p>Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K Gupta, Aditya Grover, arXiv:2301.10343Climax: A foundation model for weather and climate. 2023arXiv preprint</p>
<p>The ncep/ncar 40-year reanalysis project. Eugenia Kalnay, Masao Kanamitsu, Robert Kistler, William Collins, Dennis Deaven, Lev Gandin, Mark Iredell, Suranjana Saha, Glenn White, John Woollen, Renewable Energy. Routledge2018</p>
<p>Assessment of cfd capability for hypersonic shock wave laminar boundary layer interactions. Mehrnaz Rouhi, Youssefi , Doyle Knight, Aerospace. 42252017</p>
<p>Partial differential equations. C Lawrence, Evans, 2022American Mathematical Society19</p>
<p>Partial differential equations. Jürgen Jost, 2012Springer Science &amp; Business Media214</p>
<p>Sur les problèmes aux dérivées partielles et leur signification physique. Jacques Hadamard, Princeton university bulletin. 1902</p>
<p>Neural operator: Learning maps between function spaces. B Nikola, Zongyi Kovachki, Burigede Li, Kamyar Liu, Kaushik Azizzadenesheli, Andrew M Bhattacharya, Anima Stuart, Anandkumar, CoRR, abs/2108.084812021</p>
<p>A comprehensive and fair comparison of two neural operators (with practical extensions) based on fair data. Lu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, George Em Karniadakis, Computer Methods in Applied Mechanics and Engineering. 3931147782022</p>
<p>Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, Liang Sun, arXiv:2202.07125Transformers in time series: A survey. 2022arXiv preprint</p>
<p>A survey on vision transformer. Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, 202245</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, arXiv:2010.119292020arXiv preprint</p>
<p>Zijie Li, Kazem Meidani, Amir Barati, Farimani , arXiv:2205.13671Transformer for partial differential equations' operator learning. 2022arXiv preprint</p>
<p>Ht-net: Hierarchical transformer based operator learning model for multiscale pdes. Xinliang Liu, Bo Xu, Lei Zhang, arXiv:2210.108902022arXiv preprint</p>
<p>Zhongkai Hao, Chengyang Ying, Zhengyi Wang, Hang Su, Yinpeng Dong, Songming Liu, Ze Cheng, Jun Zhu, Jian Song, arXiv:2302.14376Gnot: A general neural operator transformer for operator learning. 2023arXiv preprint</p>
<p>Gnot: A general neural operator transformer for operator learning. Zhongkai Hao, Zhengyi Wang, Hang Su, Chengyang Ying, Yinpeng Dong, Songming Liu, Ze Cheng, Jian Song, Jun Zhu, International Conference on Machine Learning. PMLR2023</p>
<p>Choose a transformer: Fourier or galerkin. Shuhao Cao, Advances in neural information processing systems. 202134</p>
<p>Transformer meets boundary value inverse problems. Ruchi Guo, Shuhao Cao, Long Chen, arXiv:2209.149772022arXiv preprint</p>
<p>Crunchgpt: A chatgpt assisted framework for scientific machine learning. Varun Kumar, Leonard Gleyzer, Adar Kahana, Khemraj Shukla, George Em Karniadakis, arXiv:2306.155512023arXiv preprint</p>
<p>Generative diffusion learning for parametric partial differential equations. Ting Wang, Petr Plechac, Jaroslaw Knap, arXiv:2305.147032023arXiv preprint</p>
<p>A physics-informed diffusion model for high-fidelity flow field reconstruction. Dule Shu, Zijie Li, Amir Barati, Farimani , Journal of Computational Physics. 4781119722023</p>
<p>Generative artificial intelligence and its applications in materials science: Current situation and future perspectives. Yue Liu, Zhengwei Yang, Zhenyao Yu, Zitu Liu, Dahui Liu, Hailong Lin, Mingqing Li, Shuchang Ma, Maxim Avdeev, Siqi Shi, Journal of Materiomics. 2023</p>
<p>Improved denoising diffusion probabilistic models. Alexander Quinn, Nichol , Prafulla Dhariwal, International Conference on Machine Learning. PMLR2021</p>
<p>. Jiaming Song, Chenlin Meng, Stefano Ermon, arXiv:2010.025022020Denoising diffusion implicit models. arXiv preprint</p>
<p>Chenyang Si, Ziqi Huang, Yuming Jiang, Ziwei Liu, Freeu, arXiv:2309.11497Free lunch in diffusion u-net. 2023arXiv preprint</p>
<p>Michael Schlottke-Lakemper, Gregor J Gassner, Hendrik Ranocha, Andrew R Winters, Jesse Chan, Trixi, jl: Adaptive high-order numerical simulations of hyperbolic PDEs in Julia. 092021</p>
<p>Adaptive numerical simulations with Trixi.jl: A case study of Julia for scientific computing. Hendrik Ranocha, Michael Schlottke-Lakemper, Andrew Ross Winters, Erik Faulhaber, Jesse Chan, Gregor Gassner, Proceedings of the JuliaCon Conferences. the JuliaCon Conferences2022177</p>
<p>A purely hyperbolic discontinuous Galerkin approach for self-gravitating gas dynamics. Michael Schlottke-Lakemper, Andrew R Winters, Hendrik Ranocha, Gregor J Gassner, Journal of Computational Physics. 44211046706 2021</p>
<p>High-order methods for hypersonic flows with strong shocks and real chemistry. Ahmad Peyvan, Khemraj Shukla, Jesse Chan, George Karniadakis, Journal of Computational Physics. 4901123102023</p>
<p>. version: 9.13.0The MathWorks Inc. Matlab. 2022r2022b</p>            </div>
        </div>

    </div>
</body>
</html>
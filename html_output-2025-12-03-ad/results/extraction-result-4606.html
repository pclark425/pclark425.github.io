<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4606 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4606</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4606</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-273403983</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.13185v1.pdf" target="_blank">C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS</a></p>
                <p><strong>Paper Abstract:</strong> Effective research ideation is a critical step for scientific research. However, the exponential increase in scientific literature makes it challenging for researchers to stay current with recent advances and identify meaningful research directions. Recent developments in large language models~(LLMs) suggest a promising avenue for automating the generation of novel research ideas. However, existing methods for idea generation either trivially prompt LLMs or directly expose LLMs to extensive literature without indicating useful information. Inspired by the research process of human researchers, we propose a Chain-of-Ideas~(CoI) agent, an LLM-based agent that organizes relevant literature in a chain structure to effectively mirror the progressive development in a research domain. This organization facilitates LLMs to capture the current advancements in research, thereby enhancing their ideation capabilities. Furthermore, we propose Idea Arena, an evaluation protocol that can comprehensively evaluate idea generation methods from different perspectives, aligning closely with the preferences of human researchers. Experimental results indicate that the CoI agent consistently outperforms other methods and shows comparable quality as humans in research idea generation. Moreover, our CoI agent is budget-friendly, with a minimum cost of \$0.50 to generate a candidate idea and its corresponding experimental design.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4606.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4606.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoI Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Ideas (CoI) agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline/agent framework that organizes relevant papers into forward/backward chains of ideas and uses LLMs to extract ideas, predict future trends, generate novel research ideas and design experiments via multi-step prompting and iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chain-of-Ideas (CoI) agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs K branches of idea-chains per topic by: (1) converting topic -> queries; (2) retrieving an anchor paper per query; (3) extending each anchor forward via Semantic Scholar citation crawling and embedding-based ranking (OpenAI text-embedding-3-large) and backward via LLM-based reference selection; (4) extracting paper-level ideas, key entities, and experiment designs via LLM prompts; (5) summarizing inter-paper evolution to predict future trends; (6) prompting LLMs (GPT-4o primarily) to propose ideas and experiments conditioned on chains, trends, and entities; (7) running a novelty-checker agent and pairwise branch comparisons to select final idea; and (8) using a review/refinement loop for experiment designs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4o (primary); plus a lower-cost LLM for full-paper summarization (unnamed in text); OpenAI text-embedding-3-large for embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Semantic Scholar retrieval + embedding-based ranking for forward expansion; LLM-based full-paper summarization and reference relevance extraction for backward expansion; structured prompting to extract ideas, entities, and experiments from individual papers.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Chain-of-ideas progressive sequencing (preserves chronological/progressive relations), multi-branch aggregation (K branches), LLM multi-step summarization, trend prediction, novelty-checker iterative refinement, and pairwise tournament selection (Idea Arena style) across candidate ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>per chain up to 5 (configurable); default experiments used 3 branches × max length 5 → up to 15 papers</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer Science / AI research topics (evaluated on recent AI papers)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Novel research ideas (motivation, novelty, method) and corresponding experiment designs; also intermediate outputs: idea chains, trend summaries, entity lists.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Idea Arena ELO tournament (pairwise rankings) using criteria: Novelty, Significance, Clarity, Feasibility, Expected Effectiveness; human pairwise evaluation using same criteria; model-human agreement statistics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Outperformed all automated baselines in Idea Arena (both model- and human-judged). Specific reported margins: CoI outperforms GPT-Researcher and RAG by 108 and 56 ELO points respectively; high model-human agreement for LLM judge (GPT-4o ~70.8% average agreement with humans). Experiment-design outputs also ranked highest among automated methods.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>RAG (vanilla retrieval-augmented generation), ResearchAgent (Baek et al., 2024), GPT-Researcher (Assafelovic, 2023), AI-Scientist (Lu et al., 2024), Real Paper baseline (extracted human ideas/experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>CoI > GPT-Researcher by ~108 ELO and > RAG by ~56 ELO in idea generation; CoI ranked comparable to Real Paper on Novelty and Significance but lower on Feasibility compared to Real Paper; CoI's experiment designs outperform other automated methods (e.g., +~70 ELO vs RAG in human eval for experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Organizing literature into progressive chains that preserve development relations substantially improves LLM ideation quality; explicit future-trend prediction and entity grounding increase novelty and clarity; multi-branch CoIs reduce dependence on any single chain; CoI length improves results up to a saturation (~5 papers per chain).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Generated ideas still lag human (Real Paper) on feasibility and fully validated novelty; reliance on retrieval quality (irrelevant papers can degrade results if not well filtered); novelty checking depends on retrieval + LLM judgment and cannot guarantee absolute novelty; cost/compute trade-offs for long chains/branches.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Performance improves as chain length increases from 0→3 and continues up to ≈5 (saturation beyond 5); adding branches (width K) generally helps but yields diminishing returns; improvements are driven more by coherent progressive structure than sheer number of papers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4606.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4606.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Novelty-checker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Novelty-checker agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline component that retrieves relevant literature and prompts an LLM to assess the similarity between a candidate generated idea and existing papers, driving iterative refinement if high similarity is found.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Novelty-checker agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Given a candidate idea, it retrieves related papers (via Semantic Scholar / retrieval) and prompts an LLM to compare and score similarity; if similarity above threshold, the main idea generation loop re-runs to refine the idea.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4o (used as the LLM judge for similarity assessment in the pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval of relevant papers followed by LLM-based textual similarity/comparison prompts (no explicit external embedding threshold described beyond retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Pairwise LLM assessment of idea vs retrieved papers and iterative regeneration/refinement of ideas when high similarity is detected.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>not strictly specified in text; retrieves 'relevant papers' for similarity checking (implementation-dependent)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Applied in AI research ideation pipeline (generalizable to other domains)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Novelty-assessment judgments and triggers for idea refinement; qualitative similarity feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Implicit similarity/novelty judgments used within pipeline; no standalone numeric novelty metric reported besides Idea Arena outcomes influenced by novelty-checking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Used to reduce obvious duplication with existing literature in pipeline; no isolated quantitative metrics reported for novelty-checker alone.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>N/A (component of CoI pipeline; compared indirectly via ablation where removing novelty-check reduces idea novelty).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Ablation removing novelty-checker decreased novelty in generated ideas (exact ELO impact not separately quantified), indicating contribution to improving novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Iterative LLM-based novelty assessment helps reduce close duplication and encourages more novel proposals; depends heavily on retrieval coverage and LLM judgement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Cannot guarantee global novelty (depends on retrieval breadth and LLM comparison accuracy); sensitivity to retrieved corpus and LLM hallucination in similarity judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Effectiveness tied to retrieval recall; scaling to larger corpora requires more robust retrieval to avoid false negatives in novelty detection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4606.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4606.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Review agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Review agent (experiment design reviewer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based reviewer that evaluates clarity, comprehensiveness, and feasibility of candidate experiment designs and produces suggestions and search queries to refine the design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Review agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Analyzes a proposed experimental plan using structured prompts (Feasibility, Technical Quality, Clarity), returns concise suggestions and whether additional literature search is needed; then prompts a refinement module to search and update the plan.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4o (used as reviewer in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Structured prompting to extract critique and missing details from an experiment description; optional retrieval queries for targeted literature search.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Feedback loop: reviewer produces suggestions → retrieval of additional papers (if needed) → refinement by LLM to produce improved experiment design.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>unspecified; iterative searches invoked when reviewer requests more literature (implementation-dependent)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Experiment design for AI research ideas in pipeline (generalizable)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Refined experimental protocols, critique/suggestions, and search queries for literature to address weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Evaluated in Idea Arena for experiment designs on Feasibility, Technical Quality, and Clarity (pairwise ELO tournament, human and model judges).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Experiment designs produced by CoI (with review agent) ranked highest among automated methods and exhibited high model-human agreement; however they remain less feasible than Real Paper designs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against RAG, GPT-Researcher, ResearchAgent, AI-Scientist and Real Paper in experiment-design evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>CoI's experiment designs outperform other automated baselines by notable ELO margins (e.g., ~70 ELO vs RAG in human evaluation as reported), but still fall short of Real Paper in feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Structured review + targeted retrieval/refinement improves practical details and clarity of experiment plans; review agent is effective at identifying missing details and prompting targeted literature searches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Refinements depend on quality of retrieved literature and reviewer prompts; some suggested experiments still lack full feasibility compared to human-authored designs.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Iterative review improves designs but incurs additional retrieval and LLM cost; diminishing returns observed after limited refinement iterations (authors used 1 iteration for cost saving).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4606.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4606.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline approach where retrieved literature snippets are directly provided to an LLM to condition generation (here used as a vanilla baseline for idea and experiment generation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RAG (vanilla retrieval-augmented generation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Directly prompts an LLM with retrieved literature (via Semantic Scholar/Semantic Search) and asks it to generate research ideas and experiment designs without organizing the literature into progressive chains or multi-step trend analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4o (used to implement the RAG baseline in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Standard document retrieval (Semantic Scholar/Semantic Search) possibly with embedding ranking; no multi-step extraction beyond direct inclusion in prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Flat aggregation: LLM synthesizes across concatenated retrieved texts in a single prompt (no explicit chain or progressive structure).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>In -CoI variant baseline used 5 retrieved papers (implementation-dependent); generally small sets of top retrieved documents.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>AI research topics (as used in this paper's evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated research ideas and experiment designs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Idea Arena ELO tournament (Novelty, Significance, Clarity, Feasibility, Expected Effectiveness) and experiment-design ELO (Feasibility, Technical Quality, Clarity) with human/LLM judges.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Ranked below CoI and several other baselines in both idea and experiment-generation evaluations; CoI exceeded RAG by ~56 ELO in idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against CoI (this paper), ResearchAgent, GPT-Researcher, AI-Scientist, Real Paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Inferior to CoI in idea quality and novelty; mixing many unorganized papers can introduce irrelevant context and reduce logical coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Directly aggregating retrieved papers without organizing progressive relations leads to lower-quality, less novel ideas; organization and trend analysis are beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Vulnerable to interference from less relevant works when many papers are concatenated; lacks mechanisms to preserve development/progression among works.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Adding more retrieved documents without structure can harm quality; performance depends heavily on retrieval precision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4606.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4606.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ResearchAgent (Baek et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent framework that couples LLMs with an entity-centric academic knowledge graph and peer-review-like iterative refinement to generate and refine research ideas from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ResearchAgent (Iterative research idea generation over scientific literature with large language models)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Enhances literature retrieval with an academic knowledge graph focused on entities, and orchestrates multiple LLM agents that iteratively discuss and refine candidate ideas (peer discussion style), leveraging the knowledge graph to inform retrieval and grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Reproduced with GPT-4o as the LLM implementation in this paper's comparisons (original paper may have used different LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Knowledge-graph-enhanced retrieval and entity-centric extraction to identify relevant literature and grounding entities.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Multi-agent iterative refinement (peer discussions) that aggregate agent outputs into refined ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>unspecified in this paper's reproduction (depends on knowledge-graph retrieval); treated as retrieval-limited sets per topic.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific/AI research idea generation (evaluated on AI topics in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Candidate research ideas and experiment designs after iterative peer refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Idea Arena ELO tournament and human judgments (same evaluation used for other baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Performed competitively among automated baselines but was outperformed by CoI in this paper's evaluations (CoI showed superior ELO scores).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to CoI, RAG, GPT-Researcher, AI-Scientist, Real Paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>CoI achieved higher ELO scores than ResearchAgent (exact margin not singled out for ResearchAgent in text but CoI ranked top among automated methods).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using knowledge graphs and multi-agent iterative refinement improves grounding and iterative quality, but organizing literature by progressive development (CoI) yields further gains in ideation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>May still mix less-relevant literature if knowledge-graph signals are noisy; iterative peer discussion can be costly; novelty and feasibility remain challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Depends on scale/coverage of the academic knowledge graph; larger graphs may improve recall but incur more computation and noise control complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4606.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4606.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-Researcher</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-Researcher (Assafelovic, 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent framework designed for the research domain that augments planning-and-solving capabilities with retrieval to generate research outputs; used here as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>gpt-researcher. 2023</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-Researcher</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An LLM-agent research framework with plan-and-solve orchestration and retrieval-augmented generation components to propose research ideas and carry out steps of research planning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Reproduced with GPT-4o for baseline comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>RAG-style retrieval integrated with planning prompts; exact extraction pipeline unspecified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Plan-and-solve orchestration where the agent breaks tasks into subgoals and synthesizes retrieved evidence into solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>unspecified in this paper's implementation</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General research/AI topics (used as baseline on AI topics)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Research ideas and experiment designs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Idea Arena ELO tournament and human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Ranked below CoI; CoI outperformed GPT-Researcher by ~108 ELO in idea generation as reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against CoI, RAG, ResearchAgent, AI-Scientist, Real Paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Inferior to CoI in idea novelty and overall Idea Arena rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Plan-and-solve with retrieval offers structured generation but benefits from better literature organization and progressive trend modeling (as done by CoI).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>May produce less-novel ideas compared to systems that explicitly model research progression and trends; depends on retrieval scope.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not explicitly discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4606.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4606.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI-Scientist (Lu et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior system aiming for fully automated open-ended scientific discovery that generates full papers (ideas, methods, experiments) possibly including code and execution; here adapted as a baseline for idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The ai scientist: Towards fully automated open-ended scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>End-to-end automated discovery framework that attempts to propose ideas, implement methods and produce experimental outputs (originally oriented toward executable results), adapted here to extract only idea/design components for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Reproduced with GPT-4o for baseline comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not focused on multi-paper extraction — more oriented to generating executable experiments and full-paper content (method-specific extraction details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>End-to-end generation of research artifacts rather than multi-paper synthesis; may rely on internal LLM knowledge and retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>unspecified in this paper's use as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Automated scientific discovery (general); baseline applied to AI topics here.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Full-paper drafts, ideas, methods, experiments (here only idea/design parts used for evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Idea Arena ELO and experiment-design evaluations (same criteria as other systems).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Ranked lowest among automated baselines for idea generation in this paper (authors attribute low performance to its original design focus on full-paper/code generation rather than open-ended idea novelty).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with CoI, RAG, ResearchAgent, GPT-Researcher, Real Paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Underperformed other automated methods in this paper's idea-generation evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>An end-to-end paper-generation focus does not necessarily produce the most novel or feasible high-level research ideas when evaluated in isolation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Design focus (full paper/code generation) can limit open-ended ideation quality; may lack structured literature synthesis capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4606.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4606.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scimon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scimon: Scientific inspiration machines optimized for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work that augments academic retrieval with knowledge-graph signals to surface novel inspiration from literature; cited as related work for improving retrieval beyond text similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scimon: Scientific inspiration machines optimized for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Scimon</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Enhances retrieval systems with academic knowledge graph information to prioritize literature that maximizes novelty signals rather than only textual similarity (mentioned as prior related work on improving literature curation for LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Academic knowledge-graph-enhanced retrieval (entity-centric retrieval); specific extraction details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Retrieval-focused novelty optimization; not described as an LLM-driven multi-paper synthesizer within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>unspecified (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature retrieval (general; cited in AI research context)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Novelty-optimized retrieval results to assist inspiration/ideation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper (cited as prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Mentioned to improve retrieval for ideation tasks (no numbers provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned relative to classical textual-similarity retrieval systems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using knowledge-graph signals can help retrieval systems surface more novel/insightful literature for downstream ideation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Mentioned only in related work; details and limitations are in the original reference.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4606.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4606.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLR-Copilot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLR-Copilot: Autonomous machine learning research based on large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently-cited system that uses LLM agents to autonomously carry out machine learning research tasks; cited as related work in the space of LLM-driven research agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MLR-Copilot: Autonomous machine learning research based on large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MLR-Copilot</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An LLM-agent system aiming to automate ML research processes (planning, execution, literature interactions); cited as an example of LLM agents applied to research automation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not detailed in this paper (referenced as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Multi-agent orchestration for research tasks (details in cited paper).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>unspecified (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Machine learning research automation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Automated research steps, experiments and possibly reports</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Mentioned as recent work in automated research using LLM agents (no quantitative results provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>N/A (related work mention).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Representative of the trend toward LLM agents automating research tasks; details are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4606.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4606.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Si et al. study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can LLMs generate novel research ideas? a large-scale human study with 100+ NLP researchers (Si et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale human study investigating whether LLMs can generate novel research ideas; cited as evidence that LLMs can produce ideas judged more novel than humans in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can llms generate novel research ideas? a largescale human study with 100+ nlp researchers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Si et al. (LLM idea generation study)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Empirical study comparing LLM-generated ideas to human researchers' ideas across many topics and researchers, concluding LLMs can produce highly novel suggestions under some conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not a systems method for extracting from papers; an evaluation study of LLM ideation capability.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>N/A for multi-paper synthesis (study-based evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>N/A (human study rather than multi-paper synthesizer).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>NLP research idea generation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Empirical findings on LLM idea novelty compared to humans</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human judgments of novelty and other criteria in large-scale study.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported that LLMs can generate ideas rated more novel than human experts in the study (cited as a motivating result in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human expert ideas (100+ NLP researchers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LLMs found to be competitive or superior on novelty in their study (cited result).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can be strong ideation engines, motivating automated idea-generation research such as CoI.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>The paper notes novelty may not imply feasibility or full human-level experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here beyond study scope.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4606.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4606.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EvoResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EvoResearchAgent (proposed idea in CoI case study)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed multi-agent evolutionary framework (generated by the CoI agent) that models variation, crossover, and mutation among candidate ideas to enhance diversity and novelty of LLM-generated research ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>EvoResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Conceptual proposal: initialize a population of ideas via LLM, apply evolutionary operators (mutation, crossover) modeled by LLMs or agents to create diverse proposals, select and refine using novelty/fitness criteria (multi-agent evolutionary ideation). Not implemented in experiments—presented as a final idea/case study output from CoI.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not implemented; proposed to use LLM agents (implicitly GPT-class) for initialization, mutation, evaluation stages.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not applicable (proposal focuses on idea-space evolution rather than multi-paper extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Evolutionary multi-agent synthesis: recombination and variation of candidate ideas, selection via novelty/fitness evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not applicable / unspecified (a method for idea generation rather than paper synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General research idea generation (proposed concept in AI ideation context).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Diverse novel research ideas (conceptual proposal).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Proposal suggests using novelty/diversity metrics and standard idea-evaluation criteria, but no implemented metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not implemented or evaluated in paper; presented as a promising future direction derived by CoI.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>N/A (conceptual future method).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>N/A (no empirical comparison available).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multi-agent evolutionary modeling of idea variation was suggested by CoI as a way to increase diversity/novelty beyond single-pass LLM generations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Conceptual only in this paper; practical design, evaluation and retrieval grounding remain open challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed (future work).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4606.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e4606.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphGPT (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphGPT (graph instruction tuning for LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A finetuning approach leveraging graph neural network architectures for graph-related tasks, mentioned as an example of concept confusion when LLMs mix ideas from different methodological families.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphgpt: Graph instruction tuning for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GraphGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Finetuning approach where GNN-like structures/graph instruction data are used to adapt LLMs to graph tasks; cited in the paper as an example in a cautionary anecdote about naive idea mixing.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>N/A (mentioned as external method)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Graph/ML methods (mentioned in AI context)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Graph-augmented LLM capabilities (not used in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned to illustrate that naive aggregation of papers can lead LLMs to conflate incompatible methodological categories.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>N/A within this paper (external reference).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ResearchAgent <em>(Rating: 2)</em></li>
                <li>gpt-researcher. 2023 <em>(Rating: 2)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Scimon: Scientific inspiration machines optimized for novelty <em>(Rating: 2)</em></li>
                <li>Can llms generate novel research ideas? a largescale human study with 100+ nlp researchers <em>(Rating: 2)</em></li>
                <li>MLR-Copilot: Autonomous machine learning research based on large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4606",
    "paper_id": "paper-273403983",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "CoI Agent",
            "name_full": "Chain-of-Ideas (CoI) agent",
            "brief_description": "A pipeline/agent framework that organizes relevant papers into forward/backward chains of ideas and uses LLMs to extract ideas, predict future trends, generate novel research ideas and design experiments via multi-step prompting and iterative refinement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Chain-of-Ideas (CoI) agent",
            "system_description": "Constructs K branches of idea-chains per topic by: (1) converting topic -&gt; queries; (2) retrieving an anchor paper per query; (3) extending each anchor forward via Semantic Scholar citation crawling and embedding-based ranking (OpenAI text-embedding-3-large) and backward via LLM-based reference selection; (4) extracting paper-level ideas, key entities, and experiment designs via LLM prompts; (5) summarizing inter-paper evolution to predict future trends; (6) prompting LLMs (GPT-4o primarily) to propose ideas and experiments conditioned on chains, trends, and entities; (7) running a novelty-checker agent and pairwise branch comparisons to select final idea; and (8) using a review/refinement loop for experiment designs.",
            "llm_model_used": "GPT-4o (primary); plus a lower-cost LLM for full-paper summarization (unnamed in text); OpenAI text-embedding-3-large for embeddings.",
            "extraction_technique": "Semantic Scholar retrieval + embedding-based ranking for forward expansion; LLM-based full-paper summarization and reference relevance extraction for backward expansion; structured prompting to extract ideas, entities, and experiments from individual papers.",
            "synthesis_technique": "Chain-of-ideas progressive sequencing (preserves chronological/progressive relations), multi-branch aggregation (K branches), LLM multi-step summarization, trend prediction, novelty-checker iterative refinement, and pairwise tournament selection (Idea Arena style) across candidate ideas.",
            "number_of_papers": "per chain up to 5 (configurable); default experiments used 3 branches × max length 5 → up to 15 papers",
            "domain_or_topic": "Computer Science / AI research topics (evaluated on recent AI papers)",
            "output_type": "Novel research ideas (motivation, novelty, method) and corresponding experiment designs; also intermediate outputs: idea chains, trend summaries, entity lists.",
            "evaluation_metrics": "Idea Arena ELO tournament (pairwise rankings) using criteria: Novelty, Significance, Clarity, Feasibility, Expected Effectiveness; human pairwise evaluation using same criteria; model-human agreement statistics reported.",
            "performance_results": "Outperformed all automated baselines in Idea Arena (both model- and human-judged). Specific reported margins: CoI outperforms GPT-Researcher and RAG by 108 and 56 ELO points respectively; high model-human agreement for LLM judge (GPT-4o ~70.8% average agreement with humans). Experiment-design outputs also ranked highest among automated methods.",
            "comparison_baseline": "RAG (vanilla retrieval-augmented generation), ResearchAgent (Baek et al., 2024), GPT-Researcher (Assafelovic, 2023), AI-Scientist (Lu et al., 2024), Real Paper baseline (extracted human ideas/experiments).",
            "performance_vs_baseline": "CoI &gt; GPT-Researcher by ~108 ELO and &gt; RAG by ~56 ELO in idea generation; CoI ranked comparable to Real Paper on Novelty and Significance but lower on Feasibility compared to Real Paper; CoI's experiment designs outperform other automated methods (e.g., +~70 ELO vs RAG in human eval for experiments).",
            "key_findings": "Organizing literature into progressive chains that preserve development relations substantially improves LLM ideation quality; explicit future-trend prediction and entity grounding increase novelty and clarity; multi-branch CoIs reduce dependence on any single chain; CoI length improves results up to a saturation (~5 papers per chain).",
            "limitations_challenges": "Generated ideas still lag human (Real Paper) on feasibility and fully validated novelty; reliance on retrieval quality (irrelevant papers can degrade results if not well filtered); novelty checking depends on retrieval + LLM judgment and cannot guarantee absolute novelty; cost/compute trade-offs for long chains/branches.",
            "scaling_behavior": "Performance improves as chain length increases from 0→3 and continues up to ≈5 (saturation beyond 5); adding branches (width K) generally helps but yields diminishing returns; improvements are driven more by coherent progressive structure than sheer number of papers.",
            "uuid": "e4606.0",
            "source_info": {
                "paper_title": "C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Novelty-checker",
            "name_full": "Novelty-checker agent",
            "brief_description": "A pipeline component that retrieves relevant literature and prompts an LLM to assess the similarity between a candidate generated idea and existing papers, driving iterative refinement if high similarity is found.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Novelty-checker agent",
            "system_description": "Given a candidate idea, it retrieves related papers (via Semantic Scholar / retrieval) and prompts an LLM to compare and score similarity; if similarity above threshold, the main idea generation loop re-runs to refine the idea.",
            "llm_model_used": "GPT-4o (used as the LLM judge for similarity assessment in the pipeline)",
            "extraction_technique": "Retrieval of relevant papers followed by LLM-based textual similarity/comparison prompts (no explicit external embedding threshold described beyond retrieval).",
            "synthesis_technique": "Pairwise LLM assessment of idea vs retrieved papers and iterative regeneration/refinement of ideas when high similarity is detected.",
            "number_of_papers": "not strictly specified in text; retrieves 'relevant papers' for similarity checking (implementation-dependent)",
            "domain_or_topic": "Applied in AI research ideation pipeline (generalizable to other domains)",
            "output_type": "Novelty-assessment judgments and triggers for idea refinement; qualitative similarity feedback.",
            "evaluation_metrics": "Implicit similarity/novelty judgments used within pipeline; no standalone numeric novelty metric reported besides Idea Arena outcomes influenced by novelty-checking.",
            "performance_results": "Used to reduce obvious duplication with existing literature in pipeline; no isolated quantitative metrics reported for novelty-checker alone.",
            "comparison_baseline": "N/A (component of CoI pipeline; compared indirectly via ablation where removing novelty-check reduces idea novelty).",
            "performance_vs_baseline": "Ablation removing novelty-checker decreased novelty in generated ideas (exact ELO impact not separately quantified), indicating contribution to improving novelty.",
            "key_findings": "Iterative LLM-based novelty assessment helps reduce close duplication and encourages more novel proposals; depends heavily on retrieval coverage and LLM judgement.",
            "limitations_challenges": "Cannot guarantee global novelty (depends on retrieval breadth and LLM comparison accuracy); sensitivity to retrieved corpus and LLM hallucination in similarity judgments.",
            "scaling_behavior": "Effectiveness tied to retrieval recall; scaling to larger corpora requires more robust retrieval to avoid false negatives in novelty detection.",
            "uuid": "e4606.1",
            "source_info": {
                "paper_title": "C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Review agent",
            "name_full": "Review agent (experiment design reviewer)",
            "brief_description": "An LLM-based reviewer that evaluates clarity, comprehensiveness, and feasibility of candidate experiment designs and produces suggestions and search queries to refine the design.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Review agent",
            "system_description": "Analyzes a proposed experimental plan using structured prompts (Feasibility, Technical Quality, Clarity), returns concise suggestions and whether additional literature search is needed; then prompts a refinement module to search and update the plan.",
            "llm_model_used": "GPT-4o (used as reviewer in experiments)",
            "extraction_technique": "Structured prompting to extract critique and missing details from an experiment description; optional retrieval queries for targeted literature search.",
            "synthesis_technique": "Feedback loop: reviewer produces suggestions → retrieval of additional papers (if needed) → refinement by LLM to produce improved experiment design.",
            "number_of_papers": "unspecified; iterative searches invoked when reviewer requests more literature (implementation-dependent)",
            "domain_or_topic": "Experiment design for AI research ideas in pipeline (generalizable)",
            "output_type": "Refined experimental protocols, critique/suggestions, and search queries for literature to address weaknesses.",
            "evaluation_metrics": "Evaluated in Idea Arena for experiment designs on Feasibility, Technical Quality, and Clarity (pairwise ELO tournament, human and model judges).",
            "performance_results": "Experiment designs produced by CoI (with review agent) ranked highest among automated methods and exhibited high model-human agreement; however they remain less feasible than Real Paper designs.",
            "comparison_baseline": "Compared against RAG, GPT-Researcher, ResearchAgent, AI-Scientist and Real Paper in experiment-design evaluations.",
            "performance_vs_baseline": "CoI's experiment designs outperform other automated baselines by notable ELO margins (e.g., ~70 ELO vs RAG in human evaluation as reported), but still fall short of Real Paper in feasibility.",
            "key_findings": "Structured review + targeted retrieval/refinement improves practical details and clarity of experiment plans; review agent is effective at identifying missing details and prompting targeted literature searches.",
            "limitations_challenges": "Refinements depend on quality of retrieved literature and reviewer prompts; some suggested experiments still lack full feasibility compared to human-authored designs.",
            "scaling_behavior": "Iterative review improves designs but incurs additional retrieval and LLM cost; diminishing returns observed after limited refinement iterations (authors used 1 iteration for cost saving).",
            "uuid": "e4606.2",
            "source_info": {
                "paper_title": "C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A baseline approach where retrieved literature snippets are directly provided to an LLM to condition generation (here used as a vanilla baseline for idea and experiment generation).",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "mention_or_use": "use",
            "system_name": "RAG (vanilla retrieval-augmented generation)",
            "system_description": "Directly prompts an LLM with retrieved literature (via Semantic Scholar/Semantic Search) and asks it to generate research ideas and experiment designs without organizing the literature into progressive chains or multi-step trend analysis.",
            "llm_model_used": "GPT-4o (used to implement the RAG baseline in experiments)",
            "extraction_technique": "Standard document retrieval (Semantic Scholar/Semantic Search) possibly with embedding ranking; no multi-step extraction beyond direct inclusion in prompt.",
            "synthesis_technique": "Flat aggregation: LLM synthesizes across concatenated retrieved texts in a single prompt (no explicit chain or progressive structure).",
            "number_of_papers": "In -CoI variant baseline used 5 retrieved papers (implementation-dependent); generally small sets of top retrieved documents.",
            "domain_or_topic": "AI research topics (as used in this paper's evaluations)",
            "output_type": "Generated research ideas and experiment designs",
            "evaluation_metrics": "Idea Arena ELO tournament (Novelty, Significance, Clarity, Feasibility, Expected Effectiveness) and experiment-design ELO (Feasibility, Technical Quality, Clarity) with human/LLM judges.",
            "performance_results": "Ranked below CoI and several other baselines in both idea and experiment-generation evaluations; CoI exceeded RAG by ~56 ELO in idea generation.",
            "comparison_baseline": "Compared against CoI (this paper), ResearchAgent, GPT-Researcher, AI-Scientist, Real Paper.",
            "performance_vs_baseline": "Inferior to CoI in idea quality and novelty; mixing many unorganized papers can introduce irrelevant context and reduce logical coherence.",
            "key_findings": "Directly aggregating retrieved papers without organizing progressive relations leads to lower-quality, less novel ideas; organization and trend analysis are beneficial.",
            "limitations_challenges": "Vulnerable to interference from less relevant works when many papers are concatenated; lacks mechanisms to preserve development/progression among works.",
            "scaling_behavior": "Adding more retrieved documents without structure can harm quality; performance depends heavily on retrieval precision.",
            "uuid": "e4606.3",
            "source_info": {
                "paper_title": "C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "ResearchAgent",
            "name_full": "ResearchAgent (Baek et al., 2024)",
            "brief_description": "A multi-agent framework that couples LLMs with an entity-centric academic knowledge graph and peer-review-like iterative refinement to generate and refine research ideas from literature.",
            "citation_title": "ResearchAgent (Iterative research idea generation over scientific literature with large language models)",
            "mention_or_use": "use",
            "system_name": "ResearchAgent",
            "system_description": "Enhances literature retrieval with an academic knowledge graph focused on entities, and orchestrates multiple LLM agents that iteratively discuss and refine candidate ideas (peer discussion style), leveraging the knowledge graph to inform retrieval and grounding.",
            "llm_model_used": "Reproduced with GPT-4o as the LLM implementation in this paper's comparisons (original paper may have used different LLMs).",
            "extraction_technique": "Knowledge-graph-enhanced retrieval and entity-centric extraction to identify relevant literature and grounding entities.",
            "synthesis_technique": "Multi-agent iterative refinement (peer discussions) that aggregate agent outputs into refined ideas.",
            "number_of_papers": "unspecified in this paper's reproduction (depends on knowledge-graph retrieval); treated as retrieval-limited sets per topic.",
            "domain_or_topic": "General scientific/AI research idea generation (evaluated on AI topics in this paper)",
            "output_type": "Candidate research ideas and experiment designs after iterative peer refinement.",
            "evaluation_metrics": "Idea Arena ELO tournament and human judgments (same evaluation used for other baselines).",
            "performance_results": "Performed competitively among automated baselines but was outperformed by CoI in this paper's evaluations (CoI showed superior ELO scores).",
            "comparison_baseline": "Compared directly to CoI, RAG, GPT-Researcher, AI-Scientist, Real Paper.",
            "performance_vs_baseline": "CoI achieved higher ELO scores than ResearchAgent (exact margin not singled out for ResearchAgent in text but CoI ranked top among automated methods).",
            "key_findings": "Using knowledge graphs and multi-agent iterative refinement improves grounding and iterative quality, but organizing literature by progressive development (CoI) yields further gains in ideation quality.",
            "limitations_challenges": "May still mix less-relevant literature if knowledge-graph signals are noisy; iterative peer discussion can be costly; novelty and feasibility remain challenging.",
            "scaling_behavior": "Depends on scale/coverage of the academic knowledge graph; larger graphs may improve recall but incur more computation and noise control complexity.",
            "uuid": "e4606.4",
            "source_info": {
                "paper_title": "C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-Researcher",
            "name_full": "GPT-Researcher (Assafelovic, 2023)",
            "brief_description": "An agent framework designed for the research domain that augments planning-and-solving capabilities with retrieval to generate research outputs; used here as a baseline.",
            "citation_title": "gpt-researcher. 2023",
            "mention_or_use": "use",
            "system_name": "GPT-Researcher",
            "system_description": "An LLM-agent research framework with plan-and-solve orchestration and retrieval-augmented generation components to propose research ideas and carry out steps of research planning.",
            "llm_model_used": "Reproduced with GPT-4o for baseline comparisons in this paper.",
            "extraction_technique": "RAG-style retrieval integrated with planning prompts; exact extraction pipeline unspecified in this paper.",
            "synthesis_technique": "Plan-and-solve orchestration where the agent breaks tasks into subgoals and synthesizes retrieved evidence into solutions.",
            "number_of_papers": "unspecified in this paper's implementation",
            "domain_or_topic": "General research/AI topics (used as baseline on AI topics)",
            "output_type": "Research ideas and experiment designs",
            "evaluation_metrics": "Idea Arena ELO tournament and human judgments.",
            "performance_results": "Ranked below CoI; CoI outperformed GPT-Researcher by ~108 ELO in idea generation as reported.",
            "comparison_baseline": "Compared against CoI, RAG, ResearchAgent, AI-Scientist, Real Paper.",
            "performance_vs_baseline": "Inferior to CoI in idea novelty and overall Idea Arena rankings.",
            "key_findings": "Plan-and-solve with retrieval offers structured generation but benefits from better literature organization and progressive trend modeling (as done by CoI).",
            "limitations_challenges": "May produce less-novel ideas compared to systems that explicitly model research progression and trends; depends on retrieval scope.",
            "scaling_behavior": "Not explicitly discussed in this paper.",
            "uuid": "e4606.5",
            "source_info": {
                "paper_title": "C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "AI-Scientist",
            "name_full": "AI-Scientist (Lu et al., 2024)",
            "brief_description": "A prior system aiming for fully automated open-ended scientific discovery that generates full papers (ideas, methods, experiments) possibly including code and execution; here adapted as a baseline for idea generation.",
            "citation_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "mention_or_use": "use",
            "system_name": "AI-Scientist",
            "system_description": "End-to-end automated discovery framework that attempts to propose ideas, implement methods and produce experimental outputs (originally oriented toward executable results), adapted here to extract only idea/design components for comparison.",
            "llm_model_used": "Reproduced with GPT-4o for baseline comparisons in this paper.",
            "extraction_technique": "Not focused on multi-paper extraction — more oriented to generating executable experiments and full-paper content (method-specific extraction details not provided in this paper).",
            "synthesis_technique": "End-to-end generation of research artifacts rather than multi-paper synthesis; may rely on internal LLM knowledge and retrieval.",
            "number_of_papers": "unspecified in this paper's use as baseline",
            "domain_or_topic": "Automated scientific discovery (general); baseline applied to AI topics here.",
            "output_type": "Full-paper drafts, ideas, methods, experiments (here only idea/design parts used for evaluation).",
            "evaluation_metrics": "Idea Arena ELO and experiment-design evaluations (same criteria as other systems).",
            "performance_results": "Ranked lowest among automated baselines for idea generation in this paper (authors attribute low performance to its original design focus on full-paper/code generation rather than open-ended idea novelty).",
            "comparison_baseline": "Compared with CoI, RAG, ResearchAgent, GPT-Researcher, Real Paper.",
            "performance_vs_baseline": "Underperformed other automated methods in this paper's idea-generation evaluation.",
            "key_findings": "An end-to-end paper-generation focus does not necessarily produce the most novel or feasible high-level research ideas when evaluated in isolation.",
            "limitations_challenges": "Design focus (full paper/code generation) can limit open-ended ideation quality; may lack structured literature synthesis capabilities.",
            "scaling_behavior": "Not discussed in detail in this paper.",
            "uuid": "e4606.6",
            "source_info": {
                "paper_title": "C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Scimon",
            "name_full": "Scimon: Scientific inspiration machines optimized for novelty",
            "brief_description": "A prior work that augments academic retrieval with knowledge-graph signals to surface novel inspiration from literature; cited as related work for improving retrieval beyond text similarity.",
            "citation_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "mention_or_use": "mention",
            "system_name": "Scimon",
            "system_description": "Enhances retrieval systems with academic knowledge graph information to prioritize literature that maximizes novelty signals rather than only textual similarity (mentioned as prior related work on improving literature curation for LLMs).",
            "llm_model_used": "",
            "extraction_technique": "Academic knowledge-graph-enhanced retrieval (entity-centric retrieval); specific extraction details not provided in this paper.",
            "synthesis_technique": "Retrieval-focused novelty optimization; not described as an LLM-driven multi-paper synthesizer within this paper.",
            "number_of_papers": "unspecified (related work)",
            "domain_or_topic": "Scientific literature retrieval (general; cited in AI research context)",
            "output_type": "Novelty-optimized retrieval results to assist inspiration/ideation",
            "evaluation_metrics": "Not specified in this paper (cited as prior work).",
            "performance_results": "Mentioned to improve retrieval for ideation tasks (no numbers provided here).",
            "comparison_baseline": "Mentioned relative to classical textual-similarity retrieval systems.",
            "performance_vs_baseline": "Not quantified in this paper.",
            "key_findings": "Using knowledge-graph signals can help retrieval systems surface more novel/insightful literature for downstream ideation.",
            "limitations_challenges": "Mentioned only in related work; details and limitations are in the original reference.",
            "scaling_behavior": "Not discussed here.",
            "uuid": "e4606.7",
            "source_info": {
                "paper_title": "C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "MLR-Copilot",
            "name_full": "MLR-Copilot: Autonomous machine learning research based on large language models",
            "brief_description": "A recently-cited system that uses LLM agents to autonomously carry out machine learning research tasks; cited as related work in the space of LLM-driven research agents.",
            "citation_title": "MLR-Copilot: Autonomous machine learning research based on large language models",
            "mention_or_use": "mention",
            "system_name": "MLR-Copilot",
            "system_description": "An LLM-agent system aiming to automate ML research processes (planning, execution, literature interactions); cited as an example of LLM agents applied to research automation.",
            "llm_model_used": "",
            "extraction_technique": "Not detailed in this paper (referenced as related work).",
            "synthesis_technique": "Multi-agent orchestration for research tasks (details in cited paper).",
            "number_of_papers": "unspecified (related work)",
            "domain_or_topic": "Machine learning research automation",
            "output_type": "Automated research steps, experiments and possibly reports",
            "evaluation_metrics": "Not detailed here.",
            "performance_results": "Mentioned as recent work in automated research using LLM agents (no quantitative results provided in this paper).",
            "comparison_baseline": "N/A (related work mention).",
            "performance_vs_baseline": "Not discussed in this paper.",
            "key_findings": "Representative of the trend toward LLM agents automating research tasks; details are in the cited work.",
            "limitations_challenges": "Not discussed here.",
            "scaling_behavior": "Not discussed here.",
            "uuid": "e4606.8",
            "source_info": {
                "paper_title": "C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Si et al. study",
            "name_full": "Can LLMs generate novel research ideas? a large-scale human study with 100+ NLP researchers (Si et al., 2024)",
            "brief_description": "A large-scale human study investigating whether LLMs can generate novel research ideas; cited as evidence that LLMs can produce ideas judged more novel than humans in some settings.",
            "citation_title": "Can llms generate novel research ideas? a largescale human study with 100+ nlp researchers",
            "mention_or_use": "mention",
            "system_name": "Si et al. (LLM idea generation study)",
            "system_description": "Empirical study comparing LLM-generated ideas to human researchers' ideas across many topics and researchers, concluding LLMs can produce highly novel suggestions under some conditions.",
            "llm_model_used": "",
            "extraction_technique": "Not a systems method for extracting from papers; an evaluation study of LLM ideation capability.",
            "synthesis_technique": "N/A for multi-paper synthesis (study-based evaluation).",
            "number_of_papers": "N/A (human study rather than multi-paper synthesizer).",
            "domain_or_topic": "NLP research idea generation",
            "output_type": "Empirical findings on LLM idea novelty compared to humans",
            "evaluation_metrics": "Human judgments of novelty and other criteria in large-scale study.",
            "performance_results": "Reported that LLMs can generate ideas rated more novel than human experts in the study (cited as a motivating result in this paper).",
            "comparison_baseline": "Human expert ideas (100+ NLP researchers).",
            "performance_vs_baseline": "LLMs found to be competitive or superior on novelty in their study (cited result).",
            "key_findings": "LLMs can be strong ideation engines, motivating automated idea-generation research such as CoI.",
            "limitations_challenges": "The paper notes novelty may not imply feasibility or full human-level experimental validation.",
            "scaling_behavior": "Not discussed here beyond study scope.",
            "uuid": "e4606.9",
            "source_info": {
                "paper_title": "C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "EvoResearchAgent",
            "name_full": "EvoResearchAgent (proposed idea in CoI case study)",
            "brief_description": "A proposed multi-agent evolutionary framework (generated by the CoI agent) that models variation, crossover, and mutation among candidate ideas to enhance diversity and novelty of LLM-generated research ideas.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "EvoResearchAgent",
            "system_description": "Conceptual proposal: initialize a population of ideas via LLM, apply evolutionary operators (mutation, crossover) modeled by LLMs or agents to create diverse proposals, select and refine using novelty/fitness criteria (multi-agent evolutionary ideation). Not implemented in experiments—presented as a final idea/case study output from CoI.",
            "llm_model_used": "Not implemented; proposed to use LLM agents (implicitly GPT-class) for initialization, mutation, evaluation stages.",
            "extraction_technique": "Not applicable (proposal focuses on idea-space evolution rather than multi-paper extraction).",
            "synthesis_technique": "Evolutionary multi-agent synthesis: recombination and variation of candidate ideas, selection via novelty/fitness evaluation.",
            "number_of_papers": "Not applicable / unspecified (a method for idea generation rather than paper synthesis).",
            "domain_or_topic": "General research idea generation (proposed concept in AI ideation context).",
            "output_type": "Diverse novel research ideas (conceptual proposal).",
            "evaluation_metrics": "Proposal suggests using novelty/diversity metrics and standard idea-evaluation criteria, but no implemented metrics reported.",
            "performance_results": "Not implemented or evaluated in paper; presented as a promising future direction derived by CoI.",
            "comparison_baseline": "N/A (conceptual future method).",
            "performance_vs_baseline": "N/A (no empirical comparison available).",
            "key_findings": "Multi-agent evolutionary modeling of idea variation was suggested by CoI as a way to increase diversity/novelty beyond single-pass LLM generations.",
            "limitations_challenges": "Conceptual only in this paper; practical design, evaluation and retrieval grounding remain open challenges.",
            "scaling_behavior": "Not discussed (future work).",
            "uuid": "e4606.10",
            "source_info": {
                "paper_title": "C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GraphGPT (mention)",
            "name_full": "GraphGPT (graph instruction tuning for LLMs)",
            "brief_description": "A finetuning approach leveraging graph neural network architectures for graph-related tasks, mentioned as an example of concept confusion when LLMs mix ideas from different methodological families.",
            "citation_title": "Graphgpt: Graph instruction tuning for large language models",
            "mention_or_use": "mention",
            "system_name": "GraphGPT",
            "system_description": "Finetuning approach where GNN-like structures/graph instruction data are used to adapt LLMs to graph tasks; cited in the paper as an example in a cautionary anecdote about naive idea mixing.",
            "llm_model_used": "",
            "extraction_technique": "",
            "synthesis_technique": "",
            "number_of_papers": "N/A (mentioned as external method)",
            "domain_or_topic": "Graph/ML methods (mentioned in AI context)",
            "output_type": "Graph-augmented LLM capabilities (not used in this paper).",
            "evaluation_metrics": "",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Mentioned to illustrate that naive aggregation of papers can lead LLMs to conflate incompatible methodological categories.",
            "limitations_challenges": "N/A within this paper (external reference).",
            "scaling_behavior": "",
            "uuid": "e4606.11",
            "source_info": {
                "paper_title": "C HAIN OF I DEAS : R EVOLUTIONIZING R ESEARCH VIA N OVEL I DEA D EVELOPMENT WITH LLM A GENTS",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ResearchAgent",
            "rating": 2
        },
        {
            "paper_title": "gpt-researcher. 2023",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "rating": 2
        },
        {
            "paper_title": "Can llms generate novel research ideas? a largescale human study with 100+ nlp researchers",
            "rating": 2
        },
        {
            "paper_title": "MLR-Copilot: Autonomous machine learning research based on large language models",
            "rating": 1
        }
    ],
    "cost": 0.02388,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CHAIN OF IDEAS: REVOLUTIONIZING RESEARCH VIA NOVEL IDEA DEVELOPMENT WITH LLM AGENTS
30 Oct 2024</p>
<p>Long Li longli@zju.edu.cn 
DAMO Academy
Alibaba Group</p>
<p>Zhejiang University</p>
<p>Weiwen Xu weiwen.xuu@gmail.com 
DAMO Academy
Alibaba Group</p>
<p>Jiayan Guo 
DAMO Academy
Alibaba Group</p>
<p>Ruochen Zhao 
DAMO Academy
Alibaba Group</p>
<p>Xingxuan Li 
Yuqian Yuan 
DAMO Academy
Alibaba Group</p>
<p>Zhejiang University</p>
<p>Boqiang Zhang 
DAMO Academy
Alibaba Group</p>
<p>University of Science and Technology of China</p>
<p>Yuming Jiang 
DAMO Academy
Alibaba Group</p>
<p>Yifei Xin 
DAMO Academy
Alibaba Group</p>
<p>Ronghao Dang 
DAMO Academy
Alibaba Group</p>
<p>Yu Rong 
DAMO Academy
Alibaba Group</p>
<p>Deli Zhao 
DAMO Academy
Alibaba Group</p>
<p>Tian Feng 
DAMO Academy
Alibaba Group</p>
<p>Zhejiang University</p>
<p>Lidong Bing binglidong@gmail.com 
DAMO Academy
Alibaba Group</p>
<p>CHAIN OF IDEAS: REVOLUTIONIZING RESEARCH VIA NOVEL IDEA DEVELOPMENT WITH LLM AGENTS
30 Oct 2024083E5622302A7B916F2D7C1CEC581814arXiv:2410.13185v5[cs.AI]
Effective research ideation is a critical step for scientific research.However, the exponential increase in scientific literature makes it challenging for researchers to stay current with recent advances and identify meaningful research directions.Recent developments in large language models (LLMs) suggest a promising avenue for automating the generation of novel research ideas.However, existing methods for idea generation either trivially prompt LLMs or directly expose LLMs to extensive literature without indicating useful information.Inspired by the research process of human researchers, we propose a Chain-of-Ideas (CoI) agent, an LLMbased agent that organizes relevant literature in a chain structure to effectively mirror the progressive development in a research domain.This organization facilitates LLMs to capture the current advancements in research, thereby enhancing their ideation capabilities.Furthermore, we propose Idea Arena, an evaluation protocol that can comprehensively evaluate idea generation methods from different perspectives, aligning closely with the preferences of human researchers.Experimental results indicate that the CoI agent consistently outperforms other methods and shows comparable quality as humans in research idea generation.Moreover, our CoI agent is budget-friendly, with a minimum cost of $0.50 to generate a candidate idea and its corresponding experimental design 1 .</p>
<p>INTRODUCTION</p>
<p>Idea generation is a crucial aspect of scientific research for driving technological innovations and breakthroughs.Traditionally, this process has been predominantly human-driven, necessitating expert researchers to review extensive literature, identify limitations in existing solutions, and propose new research directions.However, the complexity and vastness of scientific literature, coupled with rapid technological advancements, have rendered this task increasingly challenging for researchers.</p>
<p>Recent advancements in large language models (LLMs) (Achiam et al., 2023;Dubey et al., 2024;Yang et al., 2024a) have enabled these models to exceed human experts in various scientific tasks, including mathematics (Yu et al., 2023), theorem proving (Yang et al., 2023), and coding (Chen et al., 2021).Building on this robust scientific foundation, one may hypothesize that LLMs could support a more abstract and creative research idea-generation task.Notably, Si et al. (2024); Kumar Existing methods seek to address two key challenges to improve the quality of generated ideas: curating pertinent literature for LLMs to gain inspiration and ensuring the novelty of generated ideas.To address the first challenge, previous research enhances traditional academic retrieval systems, which typically depend on textual similarity, with academic knowledge graphs (Baek et al., 2024;Wang et al., 2023).For the second challenge, existing approaches either apply predefined criteria such as novelty to guide the idea generation process (Baek et al., 2024) or iteratively refine ideas until they demonstrate low embedding similarities with existing papers (Wang et al., 2023).</p>
<p>However, in existing attempts, LLMs are presented with an extensive volume of research literature when asked to generate ideas.This makes LLMs vulnerable to the influence of less relevant works, potentially resulting in ideas that lack logical coherence and technological innovation.As shown in the upper part of Figure 1, the LLM borrows an idea from GraphGPT (Tang et al., 2024) and applies it into GoT framework (Besta et al., 2024) to generate what they interpret as a "novel idea".However, the resultant idea conflates two concepts: GoT is a prompting method while GraphGPT is a finetuning method leveraging graph neural network architecture (Zhou et al., 2020).In contrast, human researchers often trace the evolution of a research field by analyzing its progression from foundational works to the most recent advancements.This comprehensive perspective provides valuable insights into the key factors driving developments within the domain.Such an understanding enables researchers to critically assess the limitations of earlier studies while identifying emerging trends.Therefore, they are better grounded in devising innovative and impactful research ideas.</p>
<p>Motivated by the human practices in conducting research, we introduce a novel Chain-of-Ideas (CoI) agent framework to address the previously identified logical inconsistencies in the ideation processes of LLMs.As shown in the bottom part of Figure 1, CoI agent aims to provide a clear landscape of current research topics by systematically selecting and organizing the relevant papers and their ideas in a chain structure.CoI agent offers several distinctive advantages: Firstly, it minimizes the risk of interference from less relevant literature via carefully selecting papers (i.e. from CoT (Wei et al., 2022) to GoT).Second, LLMs are demonstrated with human practice to craft a novel idea.For example, SC (Wang et al., 2022) emerges as a novel idea derived from CoT.This can be viewed as a form of few-shot prompting strategy, which has been proven to enhance the overall LLM's generation capability (Brown et al., 2020).Third, CoI exemplifies a global progression in research development.As a result, LLMs can gain a deep understanding of the motivations behind these developmental trends, facilitating the identification of promising future research directions.Specifically, CoI agent first retrieves an anchor paper of the given research topic.Instead of indiscriminately aggregating all papers within the citation network of the anchor, as done in (Baek et al., 2024), we construct the CoI by selecting relevant and important literature from both the anchor's references and its subsequent works, thereby extending the chain backward and forward from the anchor.We then prompt the constructed CoI to an LLM for idea generation and experiment design.</p>
<p>During idea generation, we require the LLM to predict possible future trends.This prognostic result facilitates the gradual consolidation of the idea, beginning with the motivation for the proposed idea, progressing through an assessment of its potential impact, and culminating in the realization.However, as the evolution of scientific discovery can emerge from multiple perspectives, a single CoI may be insufficient to capture the most promising direction.Additionally, there is no guarantee that the generated ideas will be novel.To address these issues, we construct multiple CoI branches for different perspectives of a research topic.Additionally, a novelty-checker agent iteratively evaluates the draft idea against existing literature and refines it if substantial similarity is identified.</p>
<p>We compare our CoI agent against existing baselines on idea generation in the artificial intelligence (AI) field.Generating novel research ideas requires a profound comprehension of the respective research domain, coupled with a rigorous reasoning process.Previous endeavors (Lu et al., 2024;Baek et al., 2024) have sought to augment LLMs with relevant papers to facilitate the ideation process.However, these methods simply mix these papers into the prompt without effective organization.This scenario is akin to dropping an LLM at a chaotic intersection with no map in sight, leaving it uncertain about which path to take.To address this issue, we propose a Chain-of-Ideas agent framework.</p>
<p>As shown in Figure 2, a CoI, represented as
{I −M → • • • → I 0 → • • • → I N }
, is a sequence consisting of M + N + 1 ideas extracted from M + N + 1 research papers respectively, where they together show the evolution progress within a given research field.Specifically, given an initial research topic, we prompt the LLM to generate multiple queries, [q 1 , . . ., q K ], that reflect K different perspectives of this topic.The prompt is given in Table 7 of Appendix.Unless otherwise specified, all prompts of our framework are presented in the Appendix tables.The K queries are used to construct K branches of CoI.This reduces the reliance on a single CoI that may be insufficient to capture the most significant development and direction.For each query q k , we use it to retrieve a top-ranked paper, which we call anchor paper P k 0 .In Figure 2, ToT (Yao et al., 2024) is an illustrative example of an anchor paper.An anchor paper serves as the foundation for constructing a CoI.Specifically, a CoI is constructed by extending from the corresponding anchor paper to related papers in both directions: forward, tracing the progression of ideas, and backward, tracing their origins.</p>
<p>In the forward direction, starting from P k 0 , we identify subsequent papers that directly cite it by leveraging the Semantic Scholar API2 .We use OpenAI's text-embedding-3-large3 to rank these papers based on their cosine similarities to the concatenation of the initial research topic and the abstract of the anchor paper.Subsequently, we select the highest-ranked paper as P k 1 to extend the CoI in the forward direction (e.g.GoT in Figure 2).This process is repeated iteratively from P k i to P k i+1 , until either the length of the CoI reaches a preset value or the LLM finds that there is no valuable follow-up work (Table 8).</p>
<p>In the backward direction, starting from the anchor paper P k 0 , we instruct an LLM to thoroughly review the full paper and to identify candidate references based on the following criteria: 1) references that P k 0 directly built upon, 2) references that serve as baselines in P k 0 , and 3) references that tackle the same topic as P k 0 .With those candidate references, we ask the LLM to determine the most relevant one to the anchor paper (Tables 9 and 10), denoted as P k −1 (e.g.SC in Figure 2), to extend the CoI backward.This backward extension is also carried out iteratively from P k −i to P k −(i+1) to identify preceding papers (e.g.tracing backward from SC to CoT in Figure 2).It terminates when the length of CoI reaches a preset value or we encounter a milestone paper (defined as one with over 1,000 citations), indicating that the idea from the milestone paper could serve as a strong starting point for the CoI.Additionally, we instruct the LLM to terminate the search if no reference relevant to the original research topic is found (Table 8).</p>
<p>After we collect K paper chains, denoted as
{P k −M k → • • • → P k 0 → • • • → P k N k } K k=1
, we ask the LLM to extract ideas from these papers and inherit the progressive relation of the paper chains to form our CoIs 9 and 10).Then for each CoI, we ask the LLM to summarize the existing research trends by analyzing the evolution between any two adjacent ideas (Table 11).For example, the upper part of Figure 2 shows the evolution process from CoT to GoT step-by-step.Additionally, we extract experiment designs and the definition of key entities from these papers (Tables 9 and 10).The above information including CoIs and the derived knowledge will be used in the following idea generation and experiment design stages.
{I k −M k → • • • → I k 0 → • • • → I k N k } K k=1 (Tables</p>
<p>IDEA GENERATION</p>
<p>In this section, we use the above-constructed CoIs and their developing trends to guide the generation of a novel idea.For each generated CoI, the first step is to predict possible future trends.As shown in the lower-left section of Figure 2, we prompt the LLM with the CoI, the developing trends of existing works, and the key entities extracted from existing literature, as described in Sec.2.2 (Tables 12 and 13).These entities comprise relevant datasets and potential baseline models, which are important to clarify the concepts mentioned in the existing literature.After obtaining the future trend, we continue to prompt the LLM to articulate its motivation, novelty, and methodology, finally consolidate the idea (Tables 14 and 15).Through this step-by-step manner, COI can produce a more detailed idea.Following the previous practice (Wang et al., 2023;Lu et al., 2024), we also use a novelty-check agent to evaluate candidate ideas.It retrieves relevant papers and prompts another LLM to assess the similarity between the generated idea and the retrieved papers (Table 16).Based on this assessment, our framework determines if another round of generation is necessary.Finally, we pairwisely compare the generated ideas from all CoI branches and select the one with the highest winning rate as the final idea for the experiment design.This pairwise comparison follows the same method as Idea Arena, refer to Sec. 3.4 for details.</p>
<p>EXPERIMENT DESIGN</p>
<p>While our primary goal is to generate novel ideas, it is also useful to develop experimental plans that help users implement these ideas.Thus, we extended the CoI agent to include experiment design.As shown in the lower-right of Figure 2, we prompt the LLM with experiments from existing works obtained from Sec. 2.2 as few-shot examples, along with the proposed idea and key entities, to guide the LLM in designing experiments for our ideas (Table 17).</p>
<p>We also employ a review agent to assess the candidate experiment designs.Its main role is to evaluate the clarity and comprehensiveness of the protocol, ensuring all key elements-such as datasets and models-are clearly specified.Additionally, it checks if the design provides enough detail for practical implementation (Table 18).The review agent provides critical feedback on these aspects, subsequently utilizing this information to conduct further searches for relevant literature (Table 19) to help the LLM refine and enhance its previous experiment design (Table 20).Through this iterative process of review and refinement, we arrive at a final experiment design.</p>
<p>EXPERIMENTAL SETUPS</p>
<p>IMPLEMENTATIONS</p>
<p>In our CoI agent, we primarily use GPT-4o (05-13)4 as our LLM implementation.For some modules that require full-paper understanding, we use  to read the paper and summarize the core contents due to its lower price and good summarization capability.We use Semantic Scholar as our academic search engine.For the main experimental results, the maximum length of the CoI is set to 5 and the number of CoI branches is set to 3, and their analysis results are given later.The iteration number of self-refinement in the experiment design stage is set to 1 for cost saving.</p>
<p>DATA</p>
<p>To evaluate our CoI agent's ability to generate novel ideas, we collect recent research topics from Hugging Face's Daily Papers5 , known for its timely updates on AI research and the high quality of the featured papers.We select papers submitted between August 1 and September 15, 2024, ensuring that the topics are sufficiently new and the time frame is after the data cutoff of the LLM.We ask 10 skilled researchers with diverse interests in AI to identify papers that capture their interests.Subsequently, we prompt GPT-4o to extract research topics, proposed ideas, and their corresponding experiment designs from these selected papers (Tables 21, Table 22 and 23).The extracted topics will then be returned to the researchers for validation, ensuring that the extracted topics are valid and reasonable within their research domains.The extracted ideas and experiment designs will be utilized as our Real Paper baseline, as described in Section 3.3.Due to the substantial costs associated with generating and evaluating ideas and experiment designs, we adhere to the assessment scale of Lu et al. (2024); Wang et al. (2023) to collect 50 research topics in total for evaluation.</p>
<p>Preprint</p>
<p>BASELINES</p>
<p>We compare our CoI agent with recent works on idea generation and experiment design.To ensure a fair comparison, we employ GPT-4o and Semantic Search as the LLM and academic retriever implementations, respectively, across all baseline methods.Furthermore, we unify the output format of the generated ideas and experiment designs to minimize evaluation preference towards more structured outputs (Chiang et al., 2024).We compare with the following baselines:</p>
<p>• RAG: This is a vanilla retrieval augmented generation approach (Lewis et al., 2020), where we directly prompt the LLM with retrieved literature for idea generation and experiment design.• ResearchAgent (Baek et al., 2024): This work leverages additional academic knowledge graph for enhancing the literature retrieval and adopts a multi-agent framework to iteratively refine ideas through peer discussions.We follow the original paper to reproduce this baseline.• GPT-Researcher (Assafelovic, 2023): GPT-Researcher is an agent framework specifically designed for the research domain.The agent is enhanced with plan-and-solve and RAG capabilities.• AI-Scientist (Lu et al., 2024): This work originally aims to generate the entire paper with the idea, methods, and experimental results.We extract the components related to idea generation and experiment design to serve as our baseline.• Real Paper: Note that, in Sec.3.2, we extract topics from existing research papers.Therefore, the ideas and the experiment designs from these papers serve as a natural baseline to quantify the gap between model-generated ideas and genuine human ideas.</p>
<p>EVALUATION: IDEA ARENA</p>
<p>Model-based Evaluation.The open-ended nature of idea generation poses challenges for automatic evaluation.Prior work primarily uses LLM-based Likert scale system to score ideas (Baek et al., 2024;Lu et al., 2024).However, Si et al. (2024) show this method poorly aligns with human preferences.Instead, they show LLMs perform better in ranking ideas.To obtain reliable scores for evaluation, we propose Idea Arena, a pairwise evaluation system using a Round-Robin tournament to compute ELO scores for each idea-generation method.For a given topic, we require the LLM judge to rank the ideas generated by any pair of methods (Table 24).We evaluate each pair twice with order reversed to reduce the position bias.To comprehensively evaluate an idea from multiple perspectives, we incorporate criteria from ICML 2020 review guidelines6 , and those in Si et al. (2024), which consist of Novelty, Significance, Clarity, Feasibility, and Expected Effectiveness.</p>
<p>Finally, the resultant win-loss-tie records are utilized to calculate the ELO scores for each method, following the practices outlined in Zheng et al. (2024); Zhao et al. (2024).We also evaluate the experiment design in the same pairwise way, focusing on Feasibility, Technical Quality, and Clarity.Refer to Definitions for all metrics in Tables 5 and 6 of the Appendix.</p>
<p>Human Evaluation.We also perform human evaluation in a pairwise manner.The 10 researchers who review the extracted topics are asked to rank two ideas and experiment designs using the same criteria.To ensure fairness, we anonymize the source of the ideas by concealing the method identity.</p>
<p>RESULTS</p>
<p>4  Human-Model Agreements of Idea Arena.To assess the reliability of our model-based evaluation within Idea Arena, we analyze the agreements between the preferences of the human judges and the LLM judges.We follow Zheng et al. (2024) to compute the agreement, which is defined as the probability that two judges agree on the winner of one specific arena match.Figure 5 shows the pairwise agreement between humans and several state-ofthe-art LLMs, including GPT-4o, Gemini-1.5-Pro-Exp-08277, and Claude-3.5-Sonnet8 .We observe an average agreement of 70.8% between GPT-4o and humans.This finding indicates a strong alignment between humanbased and model-based evaluations, thereby highlighting the robustness of Idea Arena in evaluating the quality of generated research ideas.Moreover, GPT-4o demonstrates the highest level of agreement with humans among all tested LLMs.Therefore, we will utilize GPT-4o as the LLM judge for subsequent analytical experiments.Additionally, we present the agreement on individual criteria between GPT-4o and human evaluators in Omits the Future Trend Prediction module, prompting the LLM to consolidate ideas directly based on the provided input information.3) -Entities: Skips inputting entity definitions during idea generation.To ensure fair comparison, each variant is scored against the full CoI Agent, with 2/1/0 points for win/tie/lose in 50 matches, for a maximum of 100 points.</p>
<p>Results in Table 2 show that all variants negatively affect idea quality.Excluding the CoI construction stage has the most significant impact, emphasizing the importance of organizing literature based on progressive relationships to enhance the LLM's understanding of trends.Removing the Future Trend Prediction reduces novelty, as the LLM lacks insight into potential forward-thinking ideas.Although slight improvements in clarity and feasibility are observed, these are not substantial, likely due to evaluation variability.Finally, omitting entity information reduces clarity and effectiveness, as the LLM generates more abstract ideas without grounding in specific concepts.This highlights the value of entity information in enhancing the clarity and practical relevance of ideas.</p>
<p>CASE STUDY</p>
<p>We present an intriguing case study in Table 3 with the same topic of our paper -generating novel research ideas using LLMs.Given the input topic, our CoI agent first constructs the chain of ideas, extending I 0 (Baek et al., 2024) in both forward and backward directions.Then the agent analyzes current research trends for any two adjacent ideas.For instance, it identifies that the core development from I −1 to I 0 is the generation of ideas rather than hypotheses.After digesting the existing trends, the CoI agent realizes that LLMs have great potential in idea generation but are limited in novelty and diversity.Therefore, it proposes an evolutionary algorithm, which specifically models the variations between parents and children, as a possible future trend for novel and diverse idea generation.Finally, the agent consolidates its final idea by drawing on future trends and with practical implementations, such as crossover and mutation, to ensure effective realization.Therefore, the generated idea is viable and novel, deserving further exploration in our future work.As a byproduct of idea generation, we also require these baselines to develop potential experiment designs for realizing their proposed ideas.</p>
<p>EXPERIMENT DESIGN</p>
<p>Future Trend Prediction:</p>
<p>Given the previous research's progression and the identified gaps, a promising direction is to unleash the potential of LLM in ideation.We can develop a multi-agent system that leverages evolutionary algorithms to enhance the diversity and novelty of LLM-generated research ideas . . .</p>
<p>Final Idea: EvoResearchAgent: Enhancing Diversity and Novelty in Idea Generation with Evolution • Motivation: Using LLMs for idea generation has shown promising advancements.However, challenges persist, particularly concerning the diversity and novelty of LLM-generated ideas.Si et al. (2024) show that while LLMs can produce novel ideas, they often lack a broad range of perspectives and diversity.Additionally, Baek et al. (2024) have emphasized the need for a more systematic approach to improving the quality of generated ideas.To address these issues, we propose EvoResearchAgent, a multi-agent system that leverages evolutionary algorithms to enhance the diversity and novelty of generated ideas . . .• Method:</p>
<p>• Idea Initialize: An LLM generates some initial ideas as the start point of the evolutionary process . . .To examine the impact of the CoI length on the quality of generated ideas, we constructed variants with differing maximum chain lengths.Furthermore, we also adopt the "-CoI" variant in Sec.4.2 as a 0-length variant, which uses 5 retrieved papers but does not organize them in a chain structure.Figure 6 presents the idea arena results among these length variants.We observe a substantial improvement of idea-generation quality when we increase the length from 0 to 3.This indicates a clear developmental trend analysis is more pivotal than the quantity of related literature.Furthermore, the quality of generated ideas continues to improve as the length of the CoI increases.Longer CoIs offer more reliable and comprehensive insights into Preprint the evolving trends within the current research domain, thereby enabling the LLM to better capture future development trends.The quality of generated ideas levels off after reaching a maximum length of 5.This saturation point indicates that this length is sufficient to capture relevant trends, with additional literature offering diminishing returns.We also assess the impact of the width of CoI (i.e., the branch number K) on the quality of generated ideas.Figure 7 shows the trend of average ELO scores with varying branch numbers.Generally, increasing the branch numbers shows a positive correlation with idea quality.However, the disparity in ELO scores across different branch numbers is small.This phenomenon can likely be attributed to the fact that generating multiple chains primarily helps reduce the impact of any single CoI performing poorly.Fortunately, such low-quality CoIs are rare.</p>
<p>WIDTH OF COI</p>
<p>RELATED WORKS</p>
<p>Scientific Research Idea Generation.Idea generation is a fundamental step in scientific research.</p>
<p>Due to its innovative nature, idea generation has been primarily a human-driven activity.However, recent studies indicate that LLMs can generate plausibly novel and feasible ideas as those of human researchers (Si et al., 2024;Kumar et al., 2024).To investigate the potential of LLMs in idea generation, prior works begin with the task of scientific hypothesis discovery (Yang et al., 2024b;Qi et al., 2023;Wang et al., 2023), which aims to elucidate relationships between two scientific variables.Despite its utility, scientific hypothesis discovery may not fully capture the complexity and multifaceted nature of real-world problems.To address this limitation, projects like GPT-Researcher (Assafelovic, 2023) and ResearchAgent (Baek et al., 2024) have adopted a more open-ended idea generation scenario including the underlying methodologies and experimental designs.They leverage agent-based systems to enhance the quality of idea generation.Beyond ideation, numerous studies also explore the use of LLMs for executing experiments (Huang et al., 2024;Tian et al., 2024) or combining both idea generation and experimental execution (Li et al., 2024;Lu et al., 2024).However, these approaches often make minor modifications to existing ideas for drafting their ideas, which often lack depth and creativity.</p>
<p>Align LLMs with Human Cognitive Patterns.As LLMs are trained with vast amounts of human data (Brown et al., 2020), this may enable them to internalize human cognitive patterns.Firstly, CoT (Wei et al., 2022) indicates that LLMs can enhance their reasoning abilities when provided with step-by-step guidance.Further research supports this notion by showing that simply prompting LLMs to engage in step-by-step reasoning can trigger better reasoning capability (Kojima et al., 2022).Additionally, Fu et al. (2022)  Is there any obvious error or unreasonable part in the idea, and can the experiment be designed normally according to this idea.</p>
<p>Expected Effectiveness How likely the proposed idea is going to work well (e.g., better than existing baselines).</p>
<p>Table 6: Evaluation metrics of experiment design.</p>
<p>Metric Definition</p>
<p>Feasibility Can the experiment be realized with existing technology or methods?Are there any technical difficulties or bottlenecks?Is the experimental plan detailed and feasible?Are the experimental steps clear and logical?Is there any obvious error or unreasonable part in the experiment.Consider the rationality of its steps and the possibility that the idea can be successfully implemented.</p>
<p>Quality</p>
<p>Is there a clear rationale for each step of the experimental design?Are the baseline and evaluation metrics chosen appropriately?Has the design taken into account the potential advantages and limitations of the methods used?Can this experimental design effectively support the claims made in the idea.</p>
<p>Clarity</p>
<p>Is the experimental plan clearly written?Dose it provide enough information for the expert reader to understand the experiment?Is it well organized?Does it adequately inform the reader?</p>
<p>A.2 SPECIFIC PROMPTS</p>
<p>Here are the prompts used in this paper.</p>
<p>• Prompts used in CoI construction</p>
<p>-Prompt used to convert a topic into a search query for literature retrieval (Table 7) -Prompt used to evaluate whether a paper is relevant to the topic (Table 8) -Prompt used to extract idea, experiment, entities and references from paper (Table 9) and 10 -Prompt used to summarize current trends of CoI ( You are a scientific research expert tasked with summarizing the historical progression of research related to our current topic, based on the literature we have reviewed.</p>
<p>Here are the entities you need to know : [Entities] The topic you are studying is: : [Topic] The literature from early to late: [Idea chain] Your objective is to outline the historical evolution of the research in light of current trends.Please follow these requirements: Analysis of Published Viewpoints: Examine the progression of ideas across the identified papers.Detail how each paper transitions to the next--for instance, how Paper 0 leads to Paper 1, and so forth.Focus on understanding how Paper 1 builds upon the concepts in Paper 0. Elaborate on specific advancements made, including proposed modules, their designs, and the rationale behind their effectiveness in addressing previous challenges.Apply this analytical approach to each paper in the sequence.</p>
<p>Please present your findings in the following format: Trends: Paper 0 to Paper 1: ... Paper 1 to Paper 2: ... ...You are a scientific expert tasked with formulating a novel and innovative research idea based on your comprehensive literature review.Your objective is to propose a feasible approach that could significantly advance the field.</p>
<p>Here are the entities you need to know : [Entities]</p>
<p>The literature you have studied is as follows: [Chain of ideas] The following section delineates the progressive relationships among the previously summarized research papers: [Trend]</p>
<p>Based on previous research, analyze how human experts think and transition from previous methods to subsequent approaches.Focus on their reasoning logic and the sources of their thought processes.Learn to emulate their reasoning patterns to further develop and guide your own research direction in a natural and coherent manner.Additionally, you are encouraged to adopt the following three modes of thinking: Continue to next table →</p>
<p>Preprint</p>
<p>Table 13: Prompt used to predict future trend (Part II)</p>
<ol>
<li>Reflection: Reflect on scenarios where a specific method encounters significant challenges.Consider potential solutions that could effectively address these issues, make the solutions sounds reasonable, novel and amazing.2. Analogy: Identify a specific problem you are currently facing and research existing solutions that have successfully tackled similar challenges.Explore these solutions and adapt key principles and strategies to your situation.Think creatively about how tools and approaches from other domains can be re-imagined to devise a novel strategy for your issue.Encourage you to actively explore methods in other fields to solve your current problems.3. Deep Dive: Some methods may present specific approaches to addressing a particular problem.Consider whether there are aspects that could be modified to enhance their rationale and effectiveness.Note:Each article's limitations are specific to that particular piece and should not be applied to others.Carefully consider the task at hand and analyze the potential issues you might encounter if you proceed with your original approach, reflecting on the challenges previously faced.Then, think critically about how to address these issues effectively.You are encouraged to apply human reasoning strategies to identify future research directions based on prior studies.Aim for in-depth analysis rather than mere integration of existing ideas.Please avoid introducing unfamiliar information, ensuring that the trends you present are both authentic and reasonable.Before proposing any trends, take a moment to reflect on the principles underlying the methods you're employing and assess their relevance to your research area.The future research direction should be related to the topic:</li>
</ol>
<p>[Topic] Please present the future research direction in the following format: Future direction: ... The literature you have studied is as follows: [Chain of ideas] Your idea is composed of the following components: Motivation: 1. Provide a background for your idea, summarizing relevant work.2. Identify shortcomings in previous research and highlight the specific problems that remain unsolved and that you aim to address.Novelty: 1. Distinguish your proposed method from existing methods (preferably by naming specific approaches).2. Detail the improvements of your method compared to past work.3. Clearly outline at least three contributions your idea offers to the field, including the problems it resolves and the benefits it delivers.Method: 1. Present a detailed description of your idea, focusing on the core method, the specific problem it solves, and enhancements over earlier research (citing relevant literature with titles).2. Explain the step-by-step methodology, including the functions of each module and the rationale for why this approach effectively addresses previous challenges.Please adhere to the following guidelines: 1.Your research idea should be innovative, feasible, and contribute meaningfully to the field.Please carefully examine the idea you have proposed, avoid immediate perception, and try to be different from the previous methods as much as possible.2. Ensure your proposal is solid, clearly defined, and practical to implement.Logic should underpin your reasoning.3. Write in clear, concise language aimed at an audience with limited background knowledge in the subject.Avoid complex technical jargon, but when professional terms are necessary, provide thorough explanations.4. Refrain from introducing concepts from uncertain fields to prevent proposing ideas that may be incorrect or impractical.5.When referencing other research, please include the titles of the cited papers.6. Please avoid introducing unfamiliar information, ensuring that the trends you present are both authentic and reasonable.Before proposing any trends, take a moment to reflect on the principles underlying the methods you're employing and assess their relevance to your research area.Please propose a detailed experimental plan addressing the following points: 1. Experimental Design: Develop rigorous experiments to ensure the reliability and validity of your results.Provide a comprehensive explanation of the baseline used, comparative methods, ablation study design, and criteria for data analysis and result evaluation.Clarify how these components collectively reinforce and validate the conclusions of your research.Structure your experimental design in a clear, logical, and step-by-step manner, ensuring each step is well-defined and easy to understand. 2. Implementation of Technologies/Methods: If your experimental design involves specific technologies or methodologies, describe the implementation process in detail, including key technical aspects.For any critical concepts utilized, provide thorough explanations.For instance, if you propose a modular approach, detail its construction, components, and functionality.3. Feasibility Assessment: Ensure your experimental plan is realistic, considering technological availability, timelines, resources, and personnel.Identify potential challenges and propose strategies for addressing them.4. References to Previous Studies: When citing related literature, include titles and pertinent details of the original papers.Strive to use as many references as necessary to support your experimental design.5. Visual Aids: If useful, provide pseudo code or a flowchart to illustrate the implementation process.For example, you can use pseudo code to detail the core algorithm or the model architecture, or employ a flowchart to map out the experimental procedure and data flow.6. Clarity of Language: Use straightforward language to describe your methods, assuming the reader may have limited knowledge of the subject matter.Avoid complex jargon and utilize accessible terminology.If professional terms are necessary, please provide clear and detailed explanations.</p>
<p>Continue to next table →</p>
<p>Please output strictly in the following format: Experiment: Step1: ... Step2: ... ...</p>
<p>Preprint Table 18: Prompt used to review experiment</p>
<p>You are an expert in paper review.Your task is to analyze whether a given experiment can effectively verify a specific idea, as well as assess the detail and feasibility of the experiment.</p>
<p>Here are the related entities you need to know: [Entities] The idea presented is: [Idea] The corresponding experiment designed for this idea is:</p>
<p>[Experiment]</p>
<p>Please conduct your analysis based on the following criteria: 1. Can the experiment validate the idea?If not, identify the issues and suggest improvements to enhance its verification capability and feasibility.2. Are there specific experimental procedures that are confusing or poorly designed?Discuss any methods that may not be feasible, uncertainties in constructing the dataset, or a lack of explanation regarding the implementation of certain methods.3. Evaluate the clarity, detail, reasonableness, and feasibility of the experimental design.4. Provide suggestions for improving the experiment based on the shortcomings identified in your analysis.5. Focus solely on the experiment design; please refrain from altering the original idea.6. Ensure that your suggestions are constructive, concise, and specific.</p>
<p>Please strictly follow the following format for output: Suggestion: ... Please decide whether you need to search for relevant papers to obtain relevant knowledge to improve your experiment.If you need to search for relevant papers, please provide a search query for literature search, else provide "".For example: if suggestions say that the dynamic query additional information and update knowledge graph described in the experiment is not clearly described, so you need to output "dynamic knowledge graph update".</p>
<p>Please output strictly in the following format: Query:...</p>
<p>Preprint Table 20: Prompt used to refine experiment</p>
<p>You are a research expert tasked with refining and improving an experimental plan based on the feedback received.</p>
<p>The information of the literature you maybe need to refer to are as follows: [Searched paper information] The experimental plan you proposed is as follows: [Experiment] Please propose a detailed experimental plan addressing the following points: 1. Experimental Design: Develop rigorous experiments to ensure the reliability and validity of your results.Provide a comprehensive explanation of the baseline used, comparative methods, ablation study design, and criteria for data analysis and result evaluation.Clarify how these components collectively reinforce and validate the conclusions of your research.Structure your experimental design in a clear, logical, and step-by-step manner, ensuring each step is well-defined and easy to understand. 2. Implementation of Technologies/Methods: If your experimental design involves specific technologies or methodologies, describe the implementation process in detail, including key technical aspects.For any critical concepts utilized, provide thorough explanations.For instance, if you propose a modular approach, detail its construction, components, and functionality.3. Feasibility Assessment: Ensure your experimental plan is realistic, considering technological availability, timelines, resources, and personnel.Identify potential challenges and propose strategies for addressing them.4. References to Previous Studies: When citing related literature, include titles and pertinent details of the original papers.Strive to use as many references as necessary to support your experimental design.5. Visual Aids: If useful, provide pseudo code or a flowchart to illustrate the implementation process.For example, you can use pseudo code to detail the core algorithm or the model architecture, or employ a flowchart to map out the experimental procedure and data flow.6. Clarity of Language: Use straightforward language to describe your methods, assuming the reader may have limited knowledge of the subject matter.Avoid complex jargon and utilize accessible terminology.If professional terms are necessary, please provide clear and detailed explanations.You have received the following suggestions for improvement:[Suggestions] Please refine your experimental plan based on the feedback provided.Ensure your refined plan is feasible, clearly defined, and addresses the feedback you received.</p>
<p>Please output strictly in the following format: Experiment: ...</p>
<p>Preprint Table 22: Prompt used to extract idea from real paper</p>
<p>You are a research expert tasked with extracting the main idea from the provided paper information.</p>
<p>The main idea should encompass the motivation, solved problem, novelty, method of the paper.Please read the provided paper and extract the main idea from the paper.The paper content is as follows: [Content] Idea is composed of the following components: Motivation: Explain the background of the idea and past related work, identify the shortcomings of past work, identify the problems that need improvement, and identify the issues the paper want to address.Novelty: Explain the differences between the method and the current method (preferably list specific methods), explain what improvements the paper have made to the previous method, and then identify the problems that can be solved and the benefits that can be gained from these improvements.Method: Provide a detailed description of your idea, including the core method, the problem it solves, and the improvement compared with previous work(Cite the previous work with the title of the paper).Explain the specific steps of the method, the specific functions of each module, and the specific reasons why this method can solve the previous problem.Here are some tips for extracting the main idea: 1. Make idea easy to understand, use clear and concise language to describe, assuming the reader is someone who has few knowledge of the subject, avoid using complex technical terms, and try to use easy-to-understand terms to explain.If the paper use some professional terms, please explain them in detail.2. When the paper cite other papers, please indicate the title of the original paper.The final idea should be detailed and specific, clearly explain the origins, motivation, novelty, challenge, solved problem and method of the paper, and detail how the overcame these hurdles.Ensure your approach is innovative, specifying how this innovation is reflected in your experimental design.The final idea should be double-blind, i.e. no experimental results or codes should be shown.</p>
<p>Please output strictly in the following format: Final idea: ...You are a research expert tasked with extracting the specific experiment steps from the provided paper information.</p>
<p>The specific experiment steps should include the specific methods for each step.Please read the provided paper and extract specific experiment steps from the paper.</p>
<p>The paper content is as follows: [Content]</p>
<p>There are some tips for extracting the experiment steps: 1. Detail the Experimental Process: Describe the entire experimental process, including how to construct the dataset and each specific experimental step.Ensure that each experimental method is clearly and thoroughly detailed.2. If specific technologies are involved in the experimental design, describe the implementation process in as much detail as possible (i.e., technical details) 3. Make sure your experimental plan is concise and clear, and can be easily understood by others,should not be too complicated.4. Please provide a detailed explanation of the baseline used in the paper, the comparative methods, the ablation design and the experimental design.Specifically, elaborate on how these elements collectively support and validate the conclusions drawn in your research.5. Explain how your experimental design can help you verify the idea and how the experiment is detailed and feasible.Now please output strictly in the following format: Experiment: Step1: ... Step2: ... ...</p>
<p>•Figure 1 :
1
Figure 1: Comparison between the vanilla retrieval augmented generation (RAG) research agent and our Chain-of-Ideas agent on the idea generation task.et al. (2024) have validated this hypothesis, highlighting its substantial potential to expedite the discovery of novel concepts and uncharted research avenues.</p>
<p>FinalFigure 2 :
2
Figure 2: Our proposed CoI agent framework.</p>
<p>Figure 5 :
5
Figure 5: Agreements between human and LLM judges.</p>
<p>Current Trends: • I−3 → I−2: The progression from I−3 to I−2 marks a significant shift from the application of neural models for molecular generation to the broader scope of automating scientific research using LLMs . . .• I−2 → I−1: The transition from I−2 to I−1 focuses on refining the autonomous induction capabilities of LLMs, specifically in generating novel and valid scientific hypotheses . . .• I−1 → I0: I0 builds on the advancements made in I−1 by further extending the process of generating hypotheses to generating and refining research ideas autonomously . . .• I0 → I1: The transition from I0 to I1 emphasizes the importance of empirical validation of LLMs in generating novel research ideas and highlights the potential of LLMs to contribute to ideation . . .</p>
<p>Figure 6 :
6
Figure 6: Length analysis of the CoI.</p>
<p>Figure 7 :
7
Figure 7: Width analysis of the CoI.</p>
<p>You are a scientific expert tasked with designing rigorous, feasible experiments based on specified scientific questions and the methodologies derived from the idea I provide, along with relevant past research.Your goal is to assist researchers in systematically testing hypotheses and validating innovative discoveries that could significantly advance their fields.Past Related Research Experiments: [Past experiments]Here are the entities you need to know: [Entities] Here is the idea you need to design an experiment for: [Idea]</p>
<p>.1 IDEA GENERATION Main results.Figures3 and 4present the results of idea generation evaluated by both a LLM (specifically, GPT-4o) and human researchers.Detailed scores are in Table26of Appendix.Overall, our CoI agent demonstrates superior performance compared to all other automated methods in both model-based and human-based evaluations.Notably, It substantially outperforms the second-best baselines, GPT-Researcher and RAG, by margins of 108 and 56 ELO scores, respectively, in the two evaluation settings.Our CoI agent's performance is on par with that of the Real Paper baseline
CoI Agent (ours)CoI Agent (ours)AverageReal PaperAverageReal PaperResearchAgentResearchAgentGPT-ResearcherGPT-ResearcherAI-ScientistAI-ScientistRAGRAGEffectivenessNoveltyEffectivenessNovelty7008009001000 1100 12007008009001000 1100 1200FeasibilitySignificanceFeasibilitySignificanceClarityClarityFigure 3: Evaluation results of idea gen-Figure 4: Evaluation results of idea gen-eration with LLM as a judge.eration with human as judges.Agreement66.5%71.0%76.3%70.2%71.0%70.8%1.0Human100.0%70.8%69.3%70.1%0.8GPT-4o70.8%100.0%90.1%92.9%0.6Gemini-1.5-pro69.3%90.1%100.0%91.8%0.2 0.4Claude-3.570.1%92.9%91.8%100.0%0.0HumanGPT-4o Gemini-1.5-pro Claude-3.50.2
and even excels in the metrics of Novelty and Significance.These results highlight its exceptional capabilities in idea generation.Furthermore, CoI demonstrates superior performance in Clarity, Feasibility, and Expected Effectiveness compared to other automated methods in human evaluation.Nevertheless, it still lags considerably behind the Real Paper in these areas.This substantial gap Preprint between automatic methods and Real Paper is expected, as Real Paper ideas undergo extensive experimental validation.Additionally, AI-Scientist's performance is especially low, likely due to its original design, which focuses on generating full papers from executable code.When given only a research topic, its simplistic idea generation framework limits its ability to produce novel and feasible ideas.Table1: Agreement between the human and GPT-4o judges in all evaluated dimensions.Novelty Significance Clarity Feasibility Effectiveness Average</p>
<p>Table 1 .
1
The results indicate a consistently high level of agreement across all assessed criteria.
5050505050-CoI413944493942.4-Future Trend404351534446.2-Entities464942474345.4
4.2 ABLATION STUDIES FOR IDEA GENERATIONWe conduct an ablation study to assess the contributions of each component of the CoI Agent to idea generation quality.The following variants are examined: 1) -CoI: Excludes the CoI construction stage, directly using all retrieved literature without progressive relation mining.2) -Future Trend:</p>
<p>Table 4 :
4
Results of experiment design including both model and human evaluations, as well as their agreements.Tech.refers to the Technical Quality criterion.
Feasibility Tech. Clarity AverageModel EvaluationReal Paper CoI Agent (ours) RAG ResearchAgent GPT-Researcher1100 1029 1022 960 10011122 1090 1096 1043 970 1016 1020 980 965 9921103 1056 1003 987 986AI-Scientist888827879865Human EvaluationReal Paper CoI Agent (ours) RAG GPT-Researcher ResearchAgent1138 1092 1035 988 9391111 1111 1123 1121 1041 1048 977 971 959 9641120 1112 1042 978 954AI-Scientist809788785794Agreement70.7% 75.9% 72.1% 73.0%</p>
<p>Table 3 :
3
(Kim et al., 2021) arenastyle results for experiment designs for both model-based and human-based evaluations.Our CoI Agent demonstrates superior performance across all evaluated criteria in two evaluation settings, achieving the highest scores among all automated methods.Notably, it surpasses RAG, the second-best automated method, by 70 ELO points in human evaluation.Furthermore, there is a high degree of model-human agreement in the experimental designs.Despite the clarity and reasonable technical details of the experiment designs produced by the CoI Agent in support of the proposed ideas, they tend to be less feasible compared to those designs in the existing literature.Case study for the entire idea generation pipeline of our CoI agent.topic:UsingLLM agent to generate novel and original research ideas without human participation Chain of ideas: • I−3(Kim et al., 2021): It addresses the challenge of discovering new materials through molecular generation.It introduces GCT, a Transformer with a variational autoencoder, to generate SMILES strings . . .• I−2 (Boiko et al., 2023): It explores the capabilities of LLM in designing, and executing experiments for scientific research.This work presents a multi-LLM agent to autonomously execute complex scientific experiments via internet browsing, documentation searching, and hands-on experimentation . . .• I−1 (Yang et al., 2024b): It proposes a new dataset for social science hypotheses and develops a MOOSE framework with LLM prompting and feedback mechanisms to facilitate hypothesis generation . . .• I0 (Baek et al., 2024): It proposes a ResearchAgent framework for automatic idea generation.ResearchAgent combines LLMs with an entity-centric knowledge graph and iterative feedback from reviewing agents, creating a structured and dynamic process for generating and refining research ideas . . .• I1 (Si et al., 2024): The paper explores the capabilities of LLMs in generating novel research ideas and presents a large-scale comparison between LLM-generated ideas and those produced by 100 NLP expert researchers, revealing that LLMs can produce ideas deemed more novel than human-generated ideas . . .
Input
This phenomenon is also observed during the idea generation phase.Consequently, feasibility represents a significant bottleneck in automatic idea generation, highlighting the need for future research to address this challenge.Preprint</p>
<p>Table 5 :
5
reveals that in-depth reasoning of LLMs can be achieved with more elaborate prompts.As a result, a prompting strategy that closely emulates human cognition is likely to elicit more insightful responses from these models.Motivated by this, we propose CoI to better mimic the progressive cognitive patterns of humans when generating new research ideas.Evaluation metrics of ideas.
6 CONCLUSIONSIn this paper, we introduce Chain of Ideas (CoI) agent, a framework designed to enhance the capa-bility of LLMs in generating research ideas. The CoI agent offers a promising and concise solutionby organizing ideas into a chain structure, effectively mirroring the progressive development within
a given research domain.It facilitates LLMs to digest the current advancements in research, thereby enhancing their ideation capabilities.pTo comprehensively evaluate the capability of automated idea generation methods, we also propose Idea Arena, an evaluation system that requires the participant methods to compete in pairs about their generated ideas for the research topics, which demonstrates high agreement with human evaluation.Experimental results indicate that the CoI agent consistently outperforms other methods and is capable of generating ideas comparable to human creativity.</p>
<p>Table 11 )
11
• Prompts used in idea generation Preprint</p>
<p>Table 11 :
11
Prompt used to get trends of CoI</p>
<p>Table 12 :
12
Prompt used to predict future trend (Part I)</p>
<p>Table 14 :
14
Prompt used to generate idea (part I) You are a scientific expert tasked with formulating a novel and innovative research idea based on your comprehensive literature review.Your objective is to propose a feasible approach that could significantly advance the field.The following are examples of ideas you have proposed in the past that are similar to real papers.Please avoid this situation as much as possible.You can continue to make in-depth innovations, but avoid plagiarism: [Bad case] Here are the entities you need to know: [Entities] The topic you are studying is: [Topic]</p>
<p>Table 17 :
17
Prompt used to generate experiment</p>
<p>Table 19 :
19
Prompt used to get query for search paper to refine experimentYou are a research expert tasked with refining and improving an experimental plan based on the feedback received.
The experimental plan you proposed is as follows: [Experiment]You have received the following suggestions for improvement:[Suggestions]</p>
<p>Table 23 :
23
Prompt used to extract experiment from real paper</p>
<p>Table 26 :
26
Evaluation results of idea generation for both model-based evaluation and human-based evaluation.
Novelty Significance Clarity Feasibility Effectiveness Average RankReal Paper1075107111181127110911001CoI Agent (ours)1100110310811065107810852HumanRAG GPT-Researcher1021 9881038 9931022 9931030 9901035 9991029 9923 4ResearchAgent98297510019759709805AI-Scientist8358207848138098126Real Paper1063108911371165112311151CoI Agent (ours)1144113810801021115211072GPT-4oGPT-Researcher ResearchAgent995 10051007 1016995 10051010 946989 1004999 9953 4RAG91491897810239189505AI-Scientist8788318068368148336Gemini1.5-Pro-Exp0827Real Paper CoI Agent (ours) GPT-Researcher ResearchAgent RAG AI-Scientist1102 1124 1002 986 914 8731101 1119 997 986 929 8681125 1098 1005 984 948 8401120 1082 1014 975 958 8511102 1113 998 986 932 8691110 1107 1003 983 936 8601 2 3 4 5 6Claude-3.5-SonnetReal Paper CoI Agent (Ours) GPT-Researcher ResearchAgent RAG AI-Scientist1099 1165 986 1008 886 8551123 1154 977 1023 907 8151174 1033 1022 1034 977 7601149 953 1039 926 1038 8951179 1162 977 997 884 8001145 1094 1000 998 938 8251 2 3 4 5 6
The code is released at https://github.com/DAMO-NLP-SG/CoI-Agent. Try our online demo at https://huggingface.co/spaces/DAMO-NLP-SG/CoI_Agent.
https://www.semanticscholar.org/product/api
https://openai.com/index/new-embedding-models-and-api-updates/
https://openai.com/api/
https://huggingface.co/papers
https://icml.cc/Conferences/2020/ReviewerGuidelines
https://ai.google.dev/gemini-api/docs/models/experimental-models
https://www.anthropic.com/news/claude-3-5-sonnet
Preprint-Prompt used to predict future trend (Table12 and 13) -Prompt used to generate idea (Table14 and 15)-Prompt used to check the novelty of the idea (Table16)• Prompts used in experiment design -Prompt used to generate experiment design (Table17) -Prompt used to review experiment design (Table18) -Prompt used to get queries for search paper to refine experiment design (Table19) -Prompt used to refine experiment (Table20)• Prompts used in benchmark construction -Prompt used to extract topic from real paper (Table21) -Prompt used to extract the idea from real paper (Table22) -Prompt used to extract the experiment design from real paper (Table23)Prompts used in idea arena -Prompt used to compare two ideas (Table24) -Prompt used to compare two experiment designs (Table25)Table7: Prompt used to convert a topic into a search query for literature retrievalYou are a master of literature searching, tasked with finding relevant research literature based on a specific topic.Currently, we would like to study the following topic: [Topic] Please provide the literature search queries you would use to search for papers related to the topic and idea.Each query should be a string and should be enclosed in double quotes.It is best to output one query representing the whole and other queries representing different aspects of the whole.Output strictly in the following format: Queries: ...A.3 ADDITIONAL EXPERIMENT RESULTSWe present the evaluation results of idea generation for both model-based evaluation (including GPT-4o, Gemini-1.5-Pro-Exp-0827,and Claude-3.5-Sonnet)and human-based evaluation in Table26.Preprint  Now please output strictly in the following format: Entities: ... Idea: ... Experiment: ... References: ...PreprintTable15: Prompt used to generate idea (part II)7. Each article's limitations are specific to that particular piece and should not be applied to others.Carefully consider the task at hand and analyze the potential issues you might encounter if you proceed with your original approach, reflecting on the challenges previously faced.Then, think critically about how to address these issues effectively.The following section delineates the progressive relationships among the previously summarized research papers:The following section outlines the potential future research directions based on the literature you have studied:Please output your motivation,novelty,method firstly and then output your final idea.The final idea should clearly explain the origins, motivation, and challenges of your idea, detailing how you overcame these hurdles.Please present the final idea in the following format: Motivation: ... Novelty: ... Method: ... Final idea: ...You are a research expert tasked with extracting the main topic from the provided paper information.The main topic should encompass broad fields such as "Retrieve augment generation" or "using diffusion models for video generation".However, it should also include a relevant task to the topic, formatted as "topic:... task:...".Please read the provided paper and extract only the topic, which should follow this structure.The paper's title is [Title] The paper's abstract is as follows: [Abstract] The paper's introduction is as follows: [Introduction]Please output strictly in the following format: topic: ...PreprintTable24: Prompt used to compare two ideasYou are a judge in a competition.You have to decide which idea is better.The idea0 is:The topic is: Your output should be strictly in following format: Your thinking process: ... Your choice: Feasibility: 0/1/2 Quality: 0/1/2 Clarity: 0/1/2
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Assafelovic. gpt-researcher. 2023</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Emergent autonomous scientific research capabilities of large language models. Robert Daniil A Boiko, Gabe Macknight, Gomes, arXiv:2304.053322023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Chatbot arena: An open platform for evaluating llms by human preference. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, Ion Stoica, 2024</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, The Eleventh International Conference on Learning Representations. 2022</p>
<p>MLAgentbench: Evaluating language agents on machine learning experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, Forty-first International Conference on Machine Learning. 2024</p>
<p>Generative chemical transformer: neural machine learning of molecular geometric structures from chemical language via attention. Hyunseung Kim, Jonggeol Na, Won Bo, Lee , Journal of chemical information and modeling. 61122021</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Can large language models unlock novel scientific research ideas?. Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, Asif Ekbal, arXiv:2409.061852024arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Mlr-copilot: Autonomous machine learning research based on large language models agents. Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du, arXiv:2408.140332024arXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024arXiv preprint</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, arXiv:2311.059652023arXiv preprint</p>
<p>Can llms generate novel research ideas? a largescale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024arXiv preprint</p>
<p>Graphgpt: Graph instruction tuning for large language models. Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Minyang Tian, Luyu Gao, Dylan Shizhuo, Xinan Zhang, Cunwei Chen, Xuefei Fan, Roland Guo, Pan Haas, Kittithat Ji, Yao Krongchon, Li, arXiv:2407.13168A research coding benchmark curated by scientists. 2024arXiv preprint</p>
<p>Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, arXiv:2305.14259Scimon: Scientific inspiration machines optimized for novelty. 2023arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, arXiv:2407.10671Fei Huang, et al. Qwen2 technical report. 2024aarXiv preprint</p>
<p>Leandojo: Theorem proving with retrieval-augmented language models. Kaiyu Yang, Aidan M Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, Anima Anandkumar, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria, Findings of the Association for Computational Linguistics ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 2024band virtual meeting</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, Metamath, arXiv:2309.12284Bootstrap your own mathematical questions for large language models. 2023arXiv preprint</p>
<p>Auto arena of llms: Automating llm evaluations with agent peer-battles and committee discussions. Ruochen Zhao, Wenxuan Zhang, Ken Yew, Deli Chia, Lidong Zhao, Bing, arXiv:2405.202672024arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362024</p>
<p>Graph neural networks: A review of methods and applications. Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, Maosong Sun, AI open. 12020</p>            </div>
        </div>

    </div>
</body>
</html>
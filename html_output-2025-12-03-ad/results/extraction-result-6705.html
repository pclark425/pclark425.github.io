<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6705 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6705</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6705</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-3bbf541c2a3ce0ffaed3703cd486f0e1b2febb27</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3bbf541c2a3ce0ffaed3703cd486f0e1b2febb27" target="_blank">Token-Budget-Aware LLM Reasoning</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem is proposed, offering a practical solution to balance efficiency and accuracy in LLM reasoning.</p>
                <p><strong>Paper Abstract:</strong> Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6705.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6705.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o-mini (OpenAI technical report)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art OpenAI decoder-only transformer LLM evaluated in the paper on multi-step math benchmarks; used as the primary reasoning model for many experiments including budgeted and un-budgeted Chain-of-Thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4o mini: advancing cost-efficient intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K; MathBench (subsets including Arithmetic, Middle, High, College); GSM8K-Zero</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step math word problems / arithmetic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Range: grade-school arithmetic (GSM8K) up to college-level mathematics (MathBench-College)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Vanilla Chain-of-Thought ("Let's think step by step"), CoT with explicit token budget, Direct Answering, TALE-EP (zero-shot budget estimation + budgeted CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%) and average number of output tokens (tokens) and expense (10^-5 $/sample)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example (from paper tables): On GSM8K: Vanilla CoT 81.35% acc, 318.10 output tokens; TALE-EP 84.46% acc, 77.26 output tokens. Aggregate (across datasets reported by paper using GPT-4o-mini as primary eval): TALE-EP ~81.03% acc, 148.72 output tokens vs Vanilla CoT ~83.75% acc, 461.25 output tokens (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper-level analyses performed with GPT-4o-mini include: evidence of large token redundancy in CoT outputs; models can follow explicit length constraints to some degree; zero-shot budget estimator (using the same model) attains in-range (ideal budget range) accuracy 60.61% and mean out-of-range distance 109.64 tokens; empirical monotonicity (that correctness tends to increase with budget) holds for ~90.91% of sampled GSM8K cases.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Over-reasoning/token redundancy (very long, unnecessary CoT); model "gives up" on tight budgets and ignores constraints leading to longer outputs (token-elasticity); on simple datasets designed to not require long reasoning (GSM8K-Zero) CoT can degrade accuracy via overthinking.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>The paper reports that smaller/sparser models like GPT-4o-mini experience a larger accuracy drop when constrained to short budgets (i.e., TALE-EP causes a larger relative accuracy loss on the smaller GPT-4o-mini than on larger models); TALE-EP yields substantial token reductions across evaluated models but accuracy sensitivity increases as model capacity decreases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Token-Budget-Aware LLM Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6705.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6705.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI technical report)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger OpenAI transformer family model evaluated in the paper to compare vanilla CoT vs token-budgeted reasoning (TALE-EP).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hello gpt-4o</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MathBench-College (reported in Table 4); also evaluated across other MathBench subsets in aggregate results</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>College-level math reasoning / multi-step arithmetic and symbolic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language math questions (word problems/multiple-choice formatting where applicable)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>College-level (MathBench-College)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Vanilla CoT; CoT with token budget via TALE-EP</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%) and average output tokens (tokens); expense reported in tables</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On MathBench-College (Table 4): Direct Answering 57.33% acc; Vanilla CoT 84.00% acc with 602.29 tokens; TALE-EP 80.00% acc with 181.61 tokens (expense values also reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Same token-redundancy and token-elasticity phenomena observed; budgets estimated by zero-shot prompts are generally close to the optimal searched budget for advanced models such as GPT-4o (paper reports budget estimation effectiveness analysis in appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>When budgets are too tight, small-budget monotonicity breakdown leads models to ignore constraints and revert to longer reasoning; TALE-EP trades a small accuracy decrease for large token savings.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Larger models (e.g., GPT-4o) show more robustness to budget constraints — TALE-EP achieves substantial token reduction while retaining accuracy closer to vanilla CoT compared to smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Token-Budget-Aware LLM Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6705.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6705.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yi-lightning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Yi-lightning (technical report)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-OpenAI LLM evaluated under vanilla CoT and TALE-EP to measure token reduction and accuracy trade-offs on MathBench-College.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Yi-lightning technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Yi-lightning</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MathBench-College</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step college-level math reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>College-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Vanilla CoT; TALE-EP (zero-shot budget estimation + budgeted CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%) and average output tokens (tokens); expense (10^-5 $/sample)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table 4 (MathBench-College): Direct Answering 66.67% acc, 80.01 tokens; Vanilla CoT 79.33% acc, 998.10 tokens; TALE-EP 76.67% acc, 373.52 tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper shows TALE-EP reduces output tokens for Yi-lightning substantially while retaining competitive accuracy; token-elasticity and ideal budget-range phenomena observed across models including Yi-lightning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Similar token-elasticity behavior: overly small budgets can prompt the model to ignore constraints and produce longer outputs; accuracy can slightly drop when budgets are too aggressive.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Across evaluated models TALE-EP gives large relative token savings; the absolute token costs and expense vary by model, and models with different base capabilities show different accuracy retention under tight budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Token-Budget-Aware LLM Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6705.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6705.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.1-8B-Instruct (LLaMA 3 family instruction-tuned 8B model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B-parameter instruction-tuned LLaMA-family model used as both a budget estimator for TALE-EP and as the base model for TALE-PT post-training experiments (SFT and DPO variants) on GSM8K and GSM8K-Zero.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The llama 3 herd of models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B (as stated in name)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not detailed in this paper (Llama-3 family training data not described here).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K; GSM8K-Zero</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Grade-school arithmetic multi-step word problems and tests for over-reasoning (GSM8K-Zero)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Grade-school arithmetic (GSM8K) and simplified problems where answer is embedded (GSM8K-Zero)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Direct Answering, Vanilla CoT, TALE-PT (post-training: SFT and DPO internalization of budget-aware targets)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%) and average output tokens (tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table 5 results (Llama-3.1-8B-Instruct): GSM8K: Direct Answering 21.00% acc, 38.54 tokens; Vanilla CoT 77.56% acc, 241.51 tokens; TALE-PT-SFT 78.57% acc, 139.63 tokens; TALE-PT-DPO 74.11% acc, 149.93 tokens. GSM8K-Zero: Direct Answering 70.32% acc, 13.49 tokens; Vanilla CoT 65.04% acc, 251.08 tokens; TALE-PT-SFT 78.43% acc, 77.85 tokens; TALE-PT-DPO 78.41% acc, 113.41 tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>TALE-PT uses target outputs generated under searched optimal budgets and then post-trains via SFT or DPO (with LoRA) to internalize token-budget awareness; post-training recovers or improves accuracy while reducing tokens versus vanilla CoT; paper describes SFT loss and DPO pairwise preference objective used for training.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Vanilla CoT can hurt accuracy on GSM8K-Zero (overthinking); Direct Answering is token-efficient but low accuracy on standard GSM8K. TALE-PT mitigates over-reasoning while preserving correctness but requires offline budget-search to create training targets.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Post-training (TALE-PT) on smaller models like Llama-3.1-8B-Instruct is effective at recovering accuracy lost under aggressive prompting and reduces token usage by ~50% compared to Vanilla CoT in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Token-Budget-Aware LLM Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6705.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6705.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TALE-EP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TALE - Estimation and Prompting (zero-shot budget estimation + token-budgeted CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-level intervention that first estimates a per-question token budget (zero-shot using the reasoning LLM itself) and then issues a CoT prompt constrained to that budget to reduce token redundancy while preserving accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies across multiple LLMs (GPT-4o-mini, GPT-4o, Yi-lightning, o3-mini, Llama-3.1-8B-Instruct in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not applicable (estimation is zero-shot); when used with a specific LLM, it relies on that model's general training data.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K; GSM8K-Zero; MathBench (subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step math / arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>From grade-school (GSM8K) to college-level (MathBench-College)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot budget estimation prompt (e.g., "Analyze the question and estimate minimum tokens... [[budget]]") followed by CoT prompt with 'use less than <budget> tokens' instruction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%) and average output tokens (tokens); reported expense per sample</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Aggregate reported: TALE-EP average accuracy ~81.03% vs Vanilla CoT ~83.75% while reducing output tokens from ~461.25 to ~148.72 on average (Table 3); authors report TALE-EP achieves ~67% reduction in token usage on average with <3% accuracy decrease in some summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper documents (1) budget-search algorithm to find per-sample minimal and optimal budgets, (2) 'ideal budget range' where token consumption is minimized, (3) zero-shot estimator effectiveness: in-range accuracy 60.61% and mean out-of-range distance 109.64 tokens, (4) empirical evidence of monotonicity (~90.91% of sampled GSM8K cases), and (5) token-elasticity where budgets that are too small cause longer outputs because the model ignores the constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Zero-shot budget estimator can underpredict budgets (out-of-range distance reported); very small budgets can trigger token-elasticity leading to higher token consumption than moderate budgets; some smaller models show larger accuracy drops under TALE-EP.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>TALE-EP produces large relative token savings across evaluated models (paper reports average token reduction ~64.6% across models). Accuracy retention is better on larger models; smaller/less-capable models suffer larger accuracy drops under aggressive budget constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Token-Budget-Aware LLM Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6705.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6705.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Token Elasticity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Token Elasticity (observed phenomenon in budgeted CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical phenomenon where, as the specified token budget is reduced below a certain 'reasonable' range, actual token usage first decreases but then rebounds (increases), because the model tends to ignore infeasible tight constraints and revert to longer reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Observed across evaluated models (GPT-4o-mini, Yi-lightning, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MathBench-College and other sampled math problems (figures and analysis in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>CoT reasoning for math word problems (arithmetic/multi-step reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Varies</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>CoT with explicit token budget (varied during binary/greedy search for minimal/optimal budgets)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Measured as actual output token count vs requested budget and correctness of final answers</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Empirical plots (Figure 2) show token usage bottoms out within an 'ideal budget range' and rises when budget is decreased too far; binary/greedy search used to find optimal budget minimizing token cost while preserving correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Explained as the model's behavior when unable to meet an infeasible/too-tight constraint — it 'gives up' on following the budget and produces longer outputs; authors hypothesize RLHF and human-preference signals may have taught models to prefer longer, more detailed outputs, contributing to redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>If budget is set below the ideal range, actual token usage increases (worse than moderate budgets); budget-search must therefore find an 'optimal' rather than minimal feasible budget to minimize tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Token elasticity is observed across multiple models; the exact shape/threshold of the ideal budget range depends on model capability and the difficulty of the problem.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Token-Budget-Aware LLM Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Overreasoning and redundant calculation of large language models <em>(Rating: 2)</em></li>
                <li>Concise thoughts: Impact of output length on llm reasoning and cost <em>(Rating: 2)</em></li>
                <li>Mathbench: Evaluating the theory and application proficiency of llms with a hierarchical mathematics benchmark <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6705",
    "paper_id": "paper-3bbf541c2a3ce0ffaed3703cd486f0e1b2febb27",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "GPT-4o-mini",
            "name_full": "GPT-4o-mini (OpenAI technical report)",
            "brief_description": "A state-of-the-art OpenAI decoder-only transformer LLM evaluated in the paper on multi-step math benchmarks; used as the primary reasoning model for many experiments including budgeted and un-budgeted Chain-of-Thought prompting.",
            "citation_title": "Gpt-4o mini: advancing cost-efficient intelligence",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini",
            "model_family": "transformer (decoder-only)",
            "model_size": null,
            "training_data_description": "Not specified in this paper.",
            "benchmark_name": "GSM8K; MathBench (subsets including Arithmetic, Middle, High, College); GSM8K-Zero",
            "task_type": "Multi-step math word problems / arithmetic reasoning",
            "problem_format": "Natural-language word problems",
            "difficulty_level": "Range: grade-school arithmetic (GSM8K) up to college-level mathematics (MathBench-College)",
            "prompting_method": "Vanilla Chain-of-Thought (\"Let's think step by step\"), CoT with explicit token budget, Direct Answering, TALE-EP (zero-shot budget estimation + budgeted CoT)",
            "performance_metric": "Accuracy (%) and average number of output tokens (tokens) and expense (10^-5 $/sample)",
            "performance_value": "Example (from paper tables): On GSM8K: Vanilla CoT 81.35% acc, 318.10 output tokens; TALE-EP 84.46% acc, 77.26 output tokens. Aggregate (across datasets reported by paper using GPT-4o-mini as primary eval): TALE-EP ~81.03% acc, 148.72 output tokens vs Vanilla CoT ~83.75% acc, 461.25 output tokens (see Table 3).",
            "internal_analysis": "Paper-level analyses performed with GPT-4o-mini include: evidence of large token redundancy in CoT outputs; models can follow explicit length constraints to some degree; zero-shot budget estimator (using the same model) attains in-range (ideal budget range) accuracy 60.61% and mean out-of-range distance 109.64 tokens; empirical monotonicity (that correctness tends to increase with budget) holds for ~90.91% of sampled GSM8K cases.",
            "failure_modes": "Over-reasoning/token redundancy (very long, unnecessary CoT); model \"gives up\" on tight budgets and ignores constraints leading to longer outputs (token-elasticity); on simple datasets designed to not require long reasoning (GSM8K-Zero) CoT can degrade accuracy via overthinking.",
            "scaling_trend": "The paper reports that smaller/sparser models like GPT-4o-mini experience a larger accuracy drop when constrained to short budgets (i.e., TALE-EP causes a larger relative accuracy loss on the smaller GPT-4o-mini than on larger models); TALE-EP yields substantial token reductions across evaluated models but accuracy sensitivity increases as model capacity decreases.",
            "uuid": "e6705.0",
            "source_info": {
                "paper_title": "Token-Budget-Aware LLM Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (OpenAI technical report)",
            "brief_description": "A larger OpenAI transformer family model evaluated in the paper to compare vanilla CoT vs token-budgeted reasoning (TALE-EP).",
            "citation_title": "Hello gpt-4o",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_family": "transformer (decoder-only)",
            "model_size": null,
            "training_data_description": "Not specified in this paper.",
            "benchmark_name": "MathBench-College (reported in Table 4); also evaluated across other MathBench subsets in aggregate results",
            "task_type": "College-level math reasoning / multi-step arithmetic and symbolic reasoning",
            "problem_format": "Natural-language math questions (word problems/multiple-choice formatting where applicable)",
            "difficulty_level": "College-level (MathBench-College)",
            "prompting_method": "Vanilla CoT; CoT with token budget via TALE-EP",
            "performance_metric": "Accuracy (%) and average output tokens (tokens); expense reported in tables",
            "performance_value": "On MathBench-College (Table 4): Direct Answering 57.33% acc; Vanilla CoT 84.00% acc with 602.29 tokens; TALE-EP 80.00% acc with 181.61 tokens (expense values also reported in paper).",
            "internal_analysis": "Same token-redundancy and token-elasticity phenomena observed; budgets estimated by zero-shot prompts are generally close to the optimal searched budget for advanced models such as GPT-4o (paper reports budget estimation effectiveness analysis in appendix).",
            "failure_modes": "When budgets are too tight, small-budget monotonicity breakdown leads models to ignore constraints and revert to longer reasoning; TALE-EP trades a small accuracy decrease for large token savings.",
            "scaling_trend": "Larger models (e.g., GPT-4o) show more robustness to budget constraints — TALE-EP achieves substantial token reduction while retaining accuracy closer to vanilla CoT compared to smaller models.",
            "uuid": "e6705.1",
            "source_info": {
                "paper_title": "Token-Budget-Aware LLM Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Yi-lightning",
            "name_full": "Yi-lightning (technical report)",
            "brief_description": "A non-OpenAI LLM evaluated under vanilla CoT and TALE-EP to measure token reduction and accuracy trade-offs on MathBench-College.",
            "citation_title": "Yi-lightning technical report",
            "mention_or_use": "use",
            "model_name": "Yi-lightning",
            "model_family": "transformer (decoder-only)",
            "model_size": null,
            "training_data_description": "Not specified in this paper.",
            "benchmark_name": "MathBench-College",
            "task_type": "Multi-step college-level math reasoning",
            "problem_format": "Natural-language word problems",
            "difficulty_level": "College-level",
            "prompting_method": "Vanilla CoT; TALE-EP (zero-shot budget estimation + budgeted CoT)",
            "performance_metric": "Accuracy (%) and average output tokens (tokens); expense (10^-5 $/sample)",
            "performance_value": "Table 4 (MathBench-College): Direct Answering 66.67% acc, 80.01 tokens; Vanilla CoT 79.33% acc, 998.10 tokens; TALE-EP 76.67% acc, 373.52 tokens.",
            "internal_analysis": "Paper shows TALE-EP reduces output tokens for Yi-lightning substantially while retaining competitive accuracy; token-elasticity and ideal budget-range phenomena observed across models including Yi-lightning.",
            "failure_modes": "Similar token-elasticity behavior: overly small budgets can prompt the model to ignore constraints and produce longer outputs; accuracy can slightly drop when budgets are too aggressive.",
            "scaling_trend": "Across evaluated models TALE-EP gives large relative token savings; the absolute token costs and expense vary by model, and models with different base capabilities show different accuracy retention under tight budgets.",
            "uuid": "e6705.2",
            "source_info": {
                "paper_title": "Token-Budget-Aware LLM Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Llama-3.1-8B-Instruct",
            "name_full": "Llama-3.1-8B-Instruct (LLaMA 3 family instruction-tuned 8B model)",
            "brief_description": "An 8B-parameter instruction-tuned LLaMA-family model used as both a budget estimator for TALE-EP and as the base model for TALE-PT post-training experiments (SFT and DPO variants) on GSM8K and GSM8K-Zero.",
            "citation_title": "The llama 3 herd of models",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct",
            "model_family": "transformer (decoder-only)",
            "model_size": "8B (as stated in name)",
            "training_data_description": "Not detailed in this paper (Llama-3 family training data not described here).",
            "benchmark_name": "GSM8K; GSM8K-Zero",
            "task_type": "Grade-school arithmetic multi-step word problems and tests for over-reasoning (GSM8K-Zero)",
            "problem_format": "Natural-language word problems",
            "difficulty_level": "Grade-school arithmetic (GSM8K) and simplified problems where answer is embedded (GSM8K-Zero)",
            "prompting_method": "Direct Answering, Vanilla CoT, TALE-PT (post-training: SFT and DPO internalization of budget-aware targets)",
            "performance_metric": "Accuracy (%) and average output tokens (tokens)",
            "performance_value": "Table 5 results (Llama-3.1-8B-Instruct): GSM8K: Direct Answering 21.00% acc, 38.54 tokens; Vanilla CoT 77.56% acc, 241.51 tokens; TALE-PT-SFT 78.57% acc, 139.63 tokens; TALE-PT-DPO 74.11% acc, 149.93 tokens. GSM8K-Zero: Direct Answering 70.32% acc, 13.49 tokens; Vanilla CoT 65.04% acc, 251.08 tokens; TALE-PT-SFT 78.43% acc, 77.85 tokens; TALE-PT-DPO 78.41% acc, 113.41 tokens.",
            "internal_analysis": "TALE-PT uses target outputs generated under searched optimal budgets and then post-trains via SFT or DPO (with LoRA) to internalize token-budget awareness; post-training recovers or improves accuracy while reducing tokens versus vanilla CoT; paper describes SFT loss and DPO pairwise preference objective used for training.",
            "failure_modes": "Vanilla CoT can hurt accuracy on GSM8K-Zero (overthinking); Direct Answering is token-efficient but low accuracy on standard GSM8K. TALE-PT mitigates over-reasoning while preserving correctness but requires offline budget-search to create training targets.",
            "scaling_trend": "Post-training (TALE-PT) on smaller models like Llama-3.1-8B-Instruct is effective at recovering accuracy lost under aggressive prompting and reduces token usage by ~50% compared to Vanilla CoT in reported experiments.",
            "uuid": "e6705.3",
            "source_info": {
                "paper_title": "Token-Budget-Aware LLM Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "TALE-EP",
            "name_full": "TALE - Estimation and Prompting (zero-shot budget estimation + token-budgeted CoT)",
            "brief_description": "A prompt-level intervention that first estimates a per-question token budget (zero-shot using the reasoning LLM itself) and then issues a CoT prompt constrained to that budget to reduce token redundancy while preserving accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applies across multiple LLMs (GPT-4o-mini, GPT-4o, Yi-lightning, o3-mini, Llama-3.1-8B-Instruct in experiments)",
            "model_family": "transformer (decoder-only)",
            "model_size": null,
            "training_data_description": "Not applicable (estimation is zero-shot); when used with a specific LLM, it relies on that model's general training data.",
            "benchmark_name": "GSM8K; GSM8K-Zero; MathBench (subsets)",
            "task_type": "Multi-step math / arithmetic word problems",
            "problem_format": "Natural-language word problems",
            "difficulty_level": "From grade-school (GSM8K) to college-level (MathBench-College)",
            "prompting_method": "Zero-shot budget estimation prompt (e.g., \"Analyze the question and estimate minimum tokens... [[budget]]\") followed by CoT prompt with 'use less than &lt;budget&gt; tokens' instruction",
            "performance_metric": "Accuracy (%) and average output tokens (tokens); reported expense per sample",
            "performance_value": "Aggregate reported: TALE-EP average accuracy ~81.03% vs Vanilla CoT ~83.75% while reducing output tokens from ~461.25 to ~148.72 on average (Table 3); authors report TALE-EP achieves ~67% reduction in token usage on average with &lt;3% accuracy decrease in some summaries.",
            "internal_analysis": "Paper documents (1) budget-search algorithm to find per-sample minimal and optimal budgets, (2) 'ideal budget range' where token consumption is minimized, (3) zero-shot estimator effectiveness: in-range accuracy 60.61% and mean out-of-range distance 109.64 tokens, (4) empirical evidence of monotonicity (~90.91% of sampled GSM8K cases), and (5) token-elasticity where budgets that are too small cause longer outputs because the model ignores the constraint.",
            "failure_modes": "Zero-shot budget estimator can underpredict budgets (out-of-range distance reported); very small budgets can trigger token-elasticity leading to higher token consumption than moderate budgets; some smaller models show larger accuracy drops under TALE-EP.",
            "scaling_trend": "TALE-EP produces large relative token savings across evaluated models (paper reports average token reduction ~64.6% across models). Accuracy retention is better on larger models; smaller/less-capable models suffer larger accuracy drops under aggressive budget constraints.",
            "uuid": "e6705.4",
            "source_info": {
                "paper_title": "Token-Budget-Aware LLM Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Token Elasticity",
            "name_full": "Token Elasticity (observed phenomenon in budgeted CoT)",
            "brief_description": "An empirical phenomenon where, as the specified token budget is reduced below a certain 'reasonable' range, actual token usage first decreases but then rebounds (increases), because the model tends to ignore infeasible tight constraints and revert to longer reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Observed across evaluated models (GPT-4o-mini, Yi-lightning, etc.)",
            "model_family": "transformer (decoder-only)",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "MathBench-College and other sampled math problems (figures and analysis in paper)",
            "task_type": "CoT reasoning for math word problems (arithmetic/multi-step reasoning)",
            "problem_format": "Natural-language word problems",
            "difficulty_level": "Varies",
            "prompting_method": "CoT with explicit token budget (varied during binary/greedy search for minimal/optimal budgets)",
            "performance_metric": "Measured as actual output token count vs requested budget and correctness of final answers",
            "performance_value": "Empirical plots (Figure 2) show token usage bottoms out within an 'ideal budget range' and rises when budget is decreased too far; binary/greedy search used to find optimal budget minimizing token cost while preserving correctness.",
            "internal_analysis": "Explained as the model's behavior when unable to meet an infeasible/too-tight constraint — it 'gives up' on following the budget and produces longer outputs; authors hypothesize RLHF and human-preference signals may have taught models to prefer longer, more detailed outputs, contributing to redundancy.",
            "failure_modes": "If budget is set below the ideal range, actual token usage increases (worse than moderate budgets); budget-search must therefore find an 'optimal' rather than minimal feasible budget to minimize tokens.",
            "scaling_trend": "Token elasticity is observed across multiple models; the exact shape/threshold of the ideal budget range depends on model capability and the difficulty of the problem.",
            "uuid": "e6705.5",
            "source_info": {
                "paper_title": "Token-Budget-Aware LLM Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "Overreasoning and redundant calculation of large language models",
            "rating": 2
        },
        {
            "paper_title": "Concise thoughts: Impact of output length on llm reasoning and cost",
            "rating": 2
        },
        {
            "paper_title": "Mathbench: Evaluating the theory and application proficiency of llms with a hierarchical mathematics benchmark",
            "rating": 2
        }
    ],
    "cost": 0.018337,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Token-Budget-Aware LLM Reasoning</h1>
<p>Tingxu Han ${ }^{<em> 1}$, Zhenting Wang ${ }^{</em> 12}$, Chunrong Fang ${ }^{11}$, Shiyu Zhao ${ }^{2}$, Shiqing Ma ${ }^{3}$, Zhenyu Chen ${ }^{1}$<br>${ }^{1}$ Nanjing University ${ }^{2}$ Rutgers University ${ }^{3}$ UMass Amherst</p>
<h4>Abstract</h4>
<p>Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE ${ }^{1}$.</p>
<p>"It is not enough to have a good mind; the main thing is to use it well."</p>
<ul>
<li>René Descartes</li>
</ul>
<h2>1 Introduction</h2>
<p>Reasoning plays a crucial role in enabling large language models (LLM) to perform effectively across a wide range of tasks (Zhou et al., 2022; Hao et al., 2023, 2024a; Jin et al., 2024a; Wang et al., 2024b, 2025). A variety of methods have been proposed to enhance the reasoning capabilities of large language models (Suzgun et al., 2022; Wang et al., 2023; Feng et al., 2023; Xie et al., 2024). Among these, Chain-of-Thought (CoT) (Wei et al., 2022)</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>is the most representative and widely adopted approach. It enhances the reliability of the model's answers by guiding large language models with the prompt "Let's think step by step", encouraging them to decompose the problem into intermediate steps and solve each before arriving at the final answer. Figure 1a and Figure 1b illustrate an intuitive example. Observe that without CoT, the LLM produces incorrect answers to the question. With a CoT-enhanced prompt, the LLM systematically breaks the question into multiple steps and reasons through each step sequentially. By addressing each step incrementally, the LLM eventually arrives at the correct answer. Recent reasoning models, such as OpenAI O1 (OpenAI, 2024c) and DeepSeek R1 (Guo et al., 2025), integrate CoT into their design. Notably, these models can perform CoT reasoning even without explicit prompting.</p>
<p>Although reasoning enhancement approaches such as CoT impressively improve LLM performance, they produce substantial additional overhead, specifically in the form of the increased number of tokens produced (Wei et al., 2022; Feng et al., 2023; Yao et al., 2024a; Jin et al., 2024b). As shown in Figure 1b, the answer to prompt with CoT has notably higher token costs due to the detailed intermediate reasoning steps included in the output. Such high token costs can lead to significant expenses, including increased computational resource usage and longer running times during the LLM inference, ultimately resulting in significant additional monetary and energy costs.</p>
<p>This raises an important question: "Is the reasoning process of current LLMs unnecessarily lengthy, and how can it be compressed?" Nayab et al. (2024) demonstrate that LLM has the potential to follow a length constraint in the prompt. Building on this, we find that including a token budget (see Table 1) in the prompts is a promising approach to compressing the CoT reasoning tokens. However, the choice of token budget plays a crucial role in</p>
<p>Q: Peyton scheduled after-work activities of a onehour yoga class on Monday, a cooking class that lasts 9times as long as Monday's yoga on Tuesday, a half-hour cheese-tasting event on Wednesday, a museum tour that takes half as long as the cooking class on Thursday, and two hours of errands on Friday. How many hours will all Peyton's afterwork activities take?</p>
<p>Peyton's after-work activities will take 12.5 hours. 园
(a) Direct answering (15 output tokens).</p>
<p>Q: Peyton ... How many hours will all Peyton 's afterwork activities take?
Let's think step by step:
Let's break down activities step by step:
1. Monday: Yoga class lasts 1 hour.</p>
<ul>
<li>Total for Monday: 1 hour</li>
<li>Tuesday: Cooking class lasts 9 times as long as</li>
</ul>
<p>Monday's yoga class.</p>
<ul>
<li>Cooking class duration $=9 * 1$ hour $=9$ hours -</li>
</ul>
<p>Total for Tuesday: 9 hours
3. Wednesday: Cheese-tasting event lasts half an hour.</p>
<ul>
<li>Total for Wednesday: 0.5 hours</li>
<li>Thursday: Museum tour is half the cooking class.</li>
</ul>
<p>Museum tour duration $=0.5 * 9$ hours $=4.5$ hours -
Total for Thursday: 4.5 hours
5. Friday: Errands take 2 hours.</p>
<ul>
<li>Total for Friday: 2 hours</li>
</ul>
<p>Now, let's add up all the hours from each day:</p>
<ul>
<li>Monday: 1 hour</li>
<li>Tuesday: 9 hours</li>
<li>Wednesday: 0.5 hours - Thursday: 4.5 hours</li>
<li>Friday: 2 hours</li>
</ul>
<p>Now, we sum these amounts:</p>
<ul>
<li>$=9+0.5+4.5+2+17$ hours.</li>
</ul>
<p>Therefore, the total time is 17 hours.
(b) Vanilla CoT (258 output tokens).</p>
<p>Q: Peyton ... How many hours will all Peyton's afterwork activities take?
Let's think step by step and use less than 10 tokens:
Let's break down each activity:
+Monday: Yoga class $=1$ hour.
+Tuesday: Cooking class $=9$ times Monday's yoga, so $9+1=9$ hours.
+Wednesday: Cheese-tasting event $=0.5$ hours.
+Thursday: Museum tour + Half of Tuesday's cooking class, so $9+2=4.5$ hours.
+Friday: Errands $=2$ hours.
Now, let's add them up:
$1+9+0.5+4.5+2+171+9+0.5+4.5+2+$
$171+9+0.5+4.5+2+17$
Peyton's after-work activities will take 17 hours.
(c) CoT with an unreasonable budget (157 output tokens).</p>
<p>Q: Peyton ... How many hours will all Peyton's afterwork activities take?
Let's think step by step and use less than 50 tokens:</p>
<ul>
<li>Monday: 1 hour (yoga) - Tuesday: 9 hours (cooking class)</li>
<li>Wednesday: 0.5 hours (cheese-tasting) -</li>
</ul>
<p>Thursday: 4.5 hours (museum tour) - Friday: 2 hours (errands)</p>
<ul>
<li>Total: $1+9+0.5+4.5+2=17$ hours.
(d) CoT with an reasonable budget (86 output tokens).</li>
</ul>
<p>Figure 1: Examples of different problem solving paradigms. The reasoning processes are highlighted. We conduct this experiment on GPT-4o-mini.
the actual compression effectiveness. For example, Figure 1d illustrates that including a reasonable token budget (e.g., 50 tokens in this case) in the instructions reduces the token cost in the chain-of-
thought (CoT) process from 258 output tokens to 86 output tokens, while still enabling the LLM to arrive at the correct answer. However, when the token budget is set to a different smaller value (e.g., 10 tokens), the output token reduction is less effective, resulting in 157 output tokens-nearly twice as many as with a 50 -token budget. In other words, when the token budget is relatively small, LLMs often fail to follow the given token budget. In such cases, the actual token usage significantly exceeds the given budget-even much larger than the token costs with larger token budgets. We refer to this phenomenon as the "Token Elasticity" in the CoT process with token budgeting. To address this, the optimal token budget for a specific LLM and a particular question can be searched by gradually reducing the budget specified in the prompt, identifying the smallest token budget that achieves both the correct answer and the lowest actual token cost.</p>
<p>Based on the above observations and analysis, we propose a token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem. We call our method TALE (Token-Budget-Aware LLM rEasoning), which includes two implementations: token budget estimation and prompting (TALE-EP) and token budget awareness internalization via post-training (TALEPT). TALE-EP estimates a reasonable token budget for each problem using zero-shot prompting and incorporates it into the reasoning process, while TALE-PT internalizes token-budget awareness through post-training, enabling the LLM to generate more token-efficient responses without explicit token constraints in the prompt. We discuss both implementations in Section 5. Experiment results show that TALE significantly reduces token costs in LLM chain-of-thought (CoT) reasoning while largely maintaining answer correctness. On average, TALE-EP achieves a $67 \%$ reduction in token usage while maintaining accuracy with less than a $3 \%$ decrease. TALE-PT cuts token usage by around $50 \%$ compared to Vanilla CoT and achieves competitive performance.</p>
<h2>2 Related Work</h2>
<p>LLM Reasoning. Reasoning in LLMs has seen substantial advancements through techniques that generate intermediate steps, enabling more accurate and effective performance across diverse domains (Wu et al., 2022; Yang et al., 2022; Zhou et al., 2022; Sun et al., 2024; OpenAI, 2024c). Var-</p>
<p>ious LLM reasoning techniques are proposed to improve the LLM performance. Chen et al. (2024) formulates reasoning as sampling from a latent distribution and optimizing it via variational approaches. Ho et al. (2022) utilizes LLM as reasoning teachers, improving the reasoning abilities of smaller models through knowledge distillation. Among them, Chain-of-Thought (CoT) prompting has emerged as a key technique for improving LLM reasoning by breaking problems into intermediate steps, enabling better performance on multiple tasks (Wei et al., 2022; Lyu et al., 2023; Li et al., 2023; Feng et al., 2024). Extensions of CoT include self-consistency, which aggregates multiple reasoning paths to improve robustness (Wang et al., 2022), and Tree-of-Thoughts, which explores reasoning steps in a tree-like structure for more complex tasks (Yao et al., 2024b). Reflexion introduces iterative refinement, where the model critiques and updates its intermediate steps (Shinn et al., 2024).
Token Cost of LLM. Although the above methods enhance reasoning accuracy, they often increase token usages, posing challenges to efficiency (Wang et al., 2024a; Chiang and Lee, 2024; Bhargava et al., 2023). Consequently, it is important to mitigate token consumption while maintaining the model performance. To address this issue, Li et al. (2021) introduces a multi-hop processing technique designed to filter out irrelevant reasoning. While effective, this approach is limited to traditional neural networks, such as PALM (Bi et al., 2020), and lacks adaptability to large language models (LLMs). Speculative decoding (Leviathan et al., 2023) aims to accelerate decoding by generating drafts using smaller models and verifying them with larger models, which is over-dependent on the alternative small approximation model. LLM routing (Ding et al., 2024) queries to different LLMs based on quality-cost trade-offs, but it cannot reduce the token usage on the specific LLM for a given query. Zheng et al. (2024) aims to improve LLM inference speed by predicting response lengths and applying a scheduling algorithm to enhance efficiency. However, it is constrained to scheduling level, and it does not reduce the actual token costs. Hao et al. (2024b) reduces token usage by substituting decoded text tokens with continuous latent tokens. However, its application is currently restricted to small-scale, early language models like GPT-2 (Radford et al., 2019). Additionally, it significantly impacts reasoning accuracy, resulting in over a 20\% relative accuracy reduction on</p>
<p>Table 1: Illustrations of the vanilla CoT prompt and the token-budget-aware prompt.</p>
<table>
<thead>
<tr>
<th>Prompt method</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr>
<td>Vanilla CoT</td>
<td>Let's think step by step:</td>
</tr>
<tr>
<td>CoT with Token Budget</td>
<td>Let's think step by step and use</td>
</tr>
<tr>
<td>less than budget tokens:</td>
<td></td>
</tr>
<tr>
<td>Example</td>
<td>Let's think step by step and use</td>
</tr>
<tr>
<td>less than 50 tokens:</td>
<td></td>
</tr>
</tbody>
</table>
<p>benchmarks such as GSM8K (Cobbe et al., 2021).</p>
<h2>3 Token Redundancy in LLM Reasoning</h2>
<p>Token Budget. Previous research (Nayab et al., 2024) demonstrates that LLM has the potential to follow a length constraint in the prompt. Table 1 shows the difference between the vanilla CoT and the CoT with token budget. For instance, by including a token budget ( 50 tokens) within the prompt, as illustrated in Figure 1d, the LLM adjusts the length of its output ( 86 output tokens), trying to align with the specified budget. This indicates that LLMs have a certain capability in following prompts with an explicit token budget.
Token Redundancy Phenomenon. We find that providing a reasonable token budget can significantly reduce the token cost during reasoning. As shown in Figure 1d, including a token budget in the instructions reduces the token cost in the chain-of-thought (CoT) process by several times, but the LLM still gets the correct answer. Our results in Figure 2 and Table 3 also confirm there are a large number of redundant tokens in the reasoning process of the state-of-the-art LLMs.
Causes of Token Redundancy in LLM Reasoning. A possible explanation for this token redundancy is that during the post-training phase, such as the RLHF process (Ouyang et al., 2022), annotators might favor more detailed responses from LLMs, marking them as preferred. As a result, the model learns to associate longer, more detailed responses with alignment to human preferences and tends to produce such outputs during reasoning. However, in many scenarios, we primarily need LLMs to provide the correct answer and make accurate decisions, rather than elaborate extensively with detailed explanations. This motivates the need to eliminate redundant tokens in the LLM reasoning process in many cases.</p>
<h2>4 Searching Optimal Token Budget</h2>
<p>As demonstrated in Figure 1, different token budgets have different effects. Therefore, it is natural to investigate the following question: "How to search</p>
<p>Algorithm 1 Budget Search
Input: feasibility checking function isFeasible, a large language model $\mathcal{M}$, a given question $\boldsymbol{x}$ and the ground truth label $y$
Output: searched budget $\beta$
1: function SEARCH(isFeasible, $\mathcal{M}, \boldsymbol{x}, y)$
2: $\quad$ right $\leftarrow$ the actual token costs of $\mathcal{M}$ with vanilla CoT prompt on $\boldsymbol{x}$
3: $\quad \beta \leftarrow\lfloor(0+$ right $) / 2\rfloor$
4: $\quad \beta_{0} \leftarrow$ right
5: while True do
6: if isFeasible $(\mathcal{M}, \boldsymbol{x}, y, \beta_{0}, \beta)$ then $\triangleright$ Update the searched budget
7: $\quad \beta \leftarrow\lfloor(0+$ right $) / 2\rfloor$
$\triangleright$ Record previous searched budget
8: $\quad \beta_{0} \leftarrow$ right
$\triangleright$ Update the search range
9: $\quad$ right $\leftarrow \beta$
10: else
11: break
12: return $\beta$
the optimal token budget for a specific question and a particular LLM?"
Vanilla Method for Optimal Budget Search. An intuitive method is finding the minimal needed tokens as the budget, ensuring that the LLM can still produce correct and accurate responses within this constraint. The goal of the search algorithm is not to directly determine task difficulty, but to identify the minimum token budget under which the model can still produce a correct answer. Specifically, the algorithm conducts a binary search over different token budgets and selects the shortest budget that maintains correctness. This approximates the minimum reasoning length required for the model to solve the given problem. To find the minimal token budget, we first propose an "implicit monotonicity assumption" that when the model outputs a wrong prediction at a budget value, it always predicts incorrectly when under this budget value, and when the model outputs a correct prediction at a budget value, it always predicts correctly when above this budget value. To empirically assess the validity of this assumption, we conduct an additional analysis. Specifically, we define a sample as monotonic if all predictions above the optimal budget are correct, and all predictions below it are incorrect. We randomly sample from the GSM8K dataset as test data and find that $90.91 \%$ of the samples satisfy this monotonicity condition. This suggests that</p>
<p>Table 2: An intuitive monotonic example. $\beta^{<em>}$ is the searched optimal budget. The budget row displays scaled budgets ranging from $2^{-2}$ to $2^{2} \cdot \beta^{</em>}$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Budget $\left(\star \beta^{*}\right)$</th>
<th style="text-align: center;">$2^{-2}$</th>
<th style="text-align: center;">$2^{-1}$</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">$2^{1}$</th>
<th style="text-align: center;">$2^{2}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Prediction</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">True</td>
</tr>
</tbody>
</table>
<p>while the assumption may not strictly hold in every instance, it is a reasonable and effective approximation in practice for guiding budget search. An intuitive monotonic sample is illustrated as Table 2. Based on the "implicit monotonicity assumption", we further design a binary search-based minimal budget search algorithm detailed in Algorithm 1.</p>
<p>Before initiating the search process, we first apply the vanilla CoT to generate an answer for each question, as illustrated in Figure 1b. The number of tokens in the resulting answer is then calculated and designated as the right boundary for search, denoted by right. The function isFeasible is used to determine the feasibility of a budget. A budget is considered feasible here if the CoT prompt with that budget preserves the correctness of the answer. Algorithm 1 showcases the details. Given the feasibility function, large language model $\mathcal{M}$, question $\boldsymbol{x}$ and label $y$ as the input, Algorithm 1 first calculates the right boundary of search (line 2). With 0 as the left boundary, the current possible budget $\beta$ is computed as the midpoint of 0 and right (line 3). We use $\beta_{0}$ to record the previously searched budget (line 4). While the current $\beta$ is feasible, the algorithm updates $\beta$ by recalculating the midpoint (line 7) and adjusts the search bounds accordingly to narrow the range (line 9). Once the loop ends, the final budget $\beta$ is returned as the searched result (line 12). Algorithm 1 is designed to find the minimal budget efficiently. However, we observe that the minimal budget required to produce a correct answer is not necessarily the optimal budget. When the budget is unreasonably small, the actual token cost often exceeds that of cases where a larger budget is used. We also further formalize the optimal budget search process detailed in Section A.6.
Observation of Token Elasticity. During our minimal budget search process, we observe a "token elasticity" phenomenon as we approach the minimal budget. Specifically, as Algorithm 1 progresses, we aim to identify the minimal budget that still ensures the answer's correctness. However, we find that if the budget is reduced beyond a certain range, the token cost increases, indicating that further reductions in the budget lead to increasing to-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
(a) GPT-4o-mini budget search. (b) GPT-4o-mini token cost. (c) Yi-lightning budget search. (d) Yi-lightning token cost.
Figure 2: Token elasticity phenomenon. The x-axis denotes the budget search iteration. The y-axis denotes the searched budget (Figure 2a and Figure 2c) or the real token costs for each searched budget (Figure 2b and Figure 2d). Different colors denote different samples randomly selected from MathBench-College (Liu et al., 2024). The token cost is significantly lower in a reasonable token budget range. When the token budget is smaller than the reasonable range, the token cost gradually increases.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: The effects of optimal searched budget. CoT, with our optimal searched budget, reduces the token costs significantly without influencing the accuracy. We conduct it on MathBench-College (Liu et al., 2024).
ken consumption. Figure 2 showcases the evidence. The x-axis represents the iterations of the budget binary search, with the budget values decreasing progressively. The y-axis in Figure 2b and Figure 2d show the corresponding token costs at each budget search iteration. When the searched budget drops below a reasonable range, the token cost increases. This happens because the model, unable to meet the tight constraint, ignores it and reverts to longer reasoning. In other words, the model effectively "gives up" on complying with the instruction, resulting in longer outputs and redundant token costs. This explains the non-monotonicity observed in Figure 2: token usage initially decreases as the budget tightens but eventually rebounds when the budget becomes too small to be feasible. We will make this intuition clearer in the revised version. Figure 1c also shows an example. As observed, when a small token budget (e.g., 10 tokens) is used, the real token cost is significantly higher compared to scenarios where a reasonable token budget is allocated (i.e., Figure 1d).</p>
<h2>Token Elasticity based Optimal Budget Search.</h2>
<p>The token elasticity observation shows that while a minimal budget may keep the correctness of the answer, it does not necessarily minimize the token cost. Figure 1c and Figure 1d illustrate an intuitive example. To address this, we enhance Algorithm 1 by incorporating a greedy search strategy aimed at finding the optimal budget that simultaneously minimizes token cost and preserves answer correctness. Specifically, we introduce an additional constraint to the isFeasible condition. Beyond ensuring correctness, the updated budget must result in a lower token cost compared to the previously searched budget. Algorithm 2 outlines the feasibility function employed during the search process. Initially, the actual token cost is computed for both the current and previously evaluated budgets (line 2). Next, feasibility is assessed based on two criteria: the answer correctness and greedy token reduction (line 3). The search process is terminated if either condition fails.
Overhead of Budget Search. For the overhead, note that the budget search mainly serves to reveal token compression potential, motivating TALE's design. For TALE-PT, the budget search is used once offline to generate training targets for posttraining. Since this is a one-time pre-processing step, it does not incur additional costs during actual model deployment. For TALE-EP, we clarify that</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: The workflow of TALE-EP. Given a question, TALE-EP first estimates the token budget using a budget estimator. It then crafts a token-budget-aware prompt by combining the question with the estimated budget. Finally, the prompt is input to the LLM to generate the answer as the final output. By default, we use the reasoning LLM itself with zero-shot estimation prompt as the budget estimator.</p>
<p>Task: Analyze the given question and estimate the minimum number of tokens required to generate a complete and accurate response. Please Give the response by strictly following this format: [[budget]], for example, Budget: [[12]].</p>
<p>Figure 5: The prompt for zero-shot budget estimation.
the budget search is not needed at all. TALE-EP relies on a lightweight, zero-shot budget estimator that directly predicts a reasonable token budget without any iterative search, making it training-free and highly efficient at inference time. To quantify the budget search overhead for TALE-PT data generation, we measured the total time needed to run the search algorithm and produce the optimal budget dataset. On GSM8K (7473 samples), this process takes approximately 354 minutes on an A100 GPU, which we consider acceptable given that it is a one-time offline cost for training.</p>
<h2>5 Methodology</h2>
<h3>5.1 Overview</h3>
<p>Based on the above analysis, we designed our method TALE for token-budget-aware reasoning in LLMs. Two solutions, i.e., estimation\&amp;prompting (TALE-EP, see Figure 4) and posttraining (TALE-PT, see Figure 6), are proposed.</p>
<h3>5.2 Estimation and Prompting (TALE-EP)</h3>
<p>Our observations on token elasticity (Section 4) indicate that only a well-chosen budget within a reasonable range can effectively minimize token costs while preserving LLM performance. The optimal budget, found using Algorithm 1 and Algorithm 2, lies within this range and achieves a satisfying trade-off between efficiency and performance. Building on this insight, we introduce a token budget aware reasoning method by zero-shotbased token budget estimation and prompting the reasoning LLM. TALE-EP leverages the reasoning capabilities of the LLM as an estimator. Fig-
ure 4 provides an overview of TALE-EP's workflow. The goal of TALE-EP is to construct a token-budget-aware prompt that maintains performance comparable to vanilla CoT while reducing token costs. To achieve this balance, TALE-EP follows a two-phase approach: budget estimation and prompt construction. Given a question, TALE-EP first estimates a reasonable token budget that closely aligns with the optimal searched budget. By default, we use the reasoning LLM itself with a zero-shot estimation prompt as the budget estimator. Figure 5 demonstrates the budget estimation prompt, which will guide the model to evaluate the question as a whole.. Using this estimate, it then crafts a token-budget-aware prompt and feeds it into the LLM to generate the final answer. Figure 7c illustrates this process with a concrete example. The key intuition behind TALE-EP is inspired by human-like thinking. When solving a mathematical problem, a person may take time to compute the exact answer but can quickly estimate the effort required to solve it. For instance, when comparing a primary school arithmetic question to a college-level calculus problem, one may not immediately provide the solutions but can easily infer that the former takes only seconds while the latter requires significantly more time. Section A. 2 evaluates the effectiveness of our budget estimation approach, demonstrating that the budgets estimated by advanced LLMs (e.g., GPT-4o-mini) are generally close to the optimal searched budget and deliver competitive performance.</p>
<h3>5.3 TALE Post-Training (TALE-PT)</h3>
<p>Another approach for obtaining an LLM with tokenbudget awareness is post-training it to incorporate this awareness into its inference process, enabling it to generate more token-efficient reasoning responses. Specifically, we post-train the LLM $\mathcal{M}<em i="i">{\theta}$ to produce answers that adhere to the token budget. This process is divided into two key stages: target output generation and LLM post-training.
Target Output Generation. In the target output generation stage, we craft the target output $y</em>$ with a Chain-of-Thought (CoT) prompt that incorporates our searched optimal token budget. The prompt is formatted as follows:}$ by prompting $\mathcal{M}_{\theta</p>
<div class="codehilite"><pre><span></span><code>&quot;Let&#39;s think step by step and use less
    than }_{\eta} ^{x}{ }_{\text {t }}^{\text { tokens:&quot; }}
</code></pre></div>

<p>where $\beta_{i}^{*}$ is the searched optimal budget for the given question $\boldsymbol{x}_{i}$ (see search process in Algo-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: The workflow of TALE-PT. Given a set of questions, TALE-PT first generates target outputs in phase (1), searching the answers under optimal budget. In phase (2), TALE-PT uses the searched target outputs to craft a specialized dataset, then post-train the LLM (via SFT/DPO) to internalize token-budget awareness.</p>
<p><strong>Figure 1d</strong> illustrates an example. The resulting LLM output, constrained by the token budget specified in the prompt, is taken as the crafted target output <em>y<sub>i</sub></em>. This target output not only leads to the correct answer but also has minimal actual output token cost among our token elasticity-based search process, as described in Section 4. In the LLM post-training stage, we train the LLM <em>M<sub>θ</sub></em> using the crafted target outputs from the first stage. We introduce two ways to conduct the token-budget awareness internalization during post-training, i.e., SFT-based and DPO-based method. Details of the hype parameters are in Section A.3.</p>
<p><strong>SFT-based Internalization.</strong> To inject token-budget awareness into <em>M<sub>θ</sub></em>, we perform supervised fine-tuning with these target outputs. We post-train <em>M<sub>θ</sub></em> to generate token-efficient outputs by minimizing the cross-entropy loss between the model's predictions and the target outputs. Given an input <em>x</em> and a target output <em>y</em> from the first stage (which reflects token-budget awareness), the cross-entropy loss is defined as:</p>
<p>$$
\mathcal{L}<em i="1">{\text{CE}}(\theta) = -\frac{1}{N} \sum</em>_i),
$$}^{N} \sum_{t=1}^{T_i} \log \mathbb{P}(y_{i,t} \mid y_{i,&lt;t}, \mathbf{x</p>
<p>where <em>T<sub>i</sub></em> means the length of the target sequence <em>y<sub>i</sub></em> for the <em>i</em>-th training example, <em>y<sub>i,t</sub></em> the target token at position <em>t</em> of <em>y<sub>i</sub></em>, <em>y<sub>i,&lt;t</sub></em> means the sequence of tokens preceding the current token <em>y<sub>i,t</sub></em>, representing the context up to time step <em>t</em> for the <em>i</em>-th sample. <em>P(y<sub>i,t</sub> | y<sub>i,&lt;t</sub>, x<sub>i</sub>)</em> represents the conditional probability predicted by the model <em>M<sub>θ</sub></em> for the token <em>y<sub>i,t</sub></em>, given the input <em>x<sub>i</sub></em> and the preceding tokens <em>y<sub>i,&lt;t</sub></em>. The loss is based on the next token prediction. The goal is to adjust the model parameters <em>θ</em> such that it produces concise and accurate responses that adhere to the token budget constraint. This is achieved through gradient descent, forcing the model to internalize the compact reasoning patterns from the token-efficient target outputs.</p>
<p><strong>DPO-based Internalization.</strong> Another way to incentivize <em>M<sub>θ</sub></em> to learn the token-budget preference is applying the DPO algorithm (Rafailov et al., 2023) to post-train the model. DPO directly refines the policy through a classification objective, aligning the model's behavior with the desired preferences. The goal of DPO here is to refine <em>M<sub>θ</sub></em> so it can accurately solve a given problem <em>x</em> while adhering to an internalized token budget. We use the target outputs <em>y<sub>i</sub></em> from the searched optimal budget as positive samples, while outputs <em>y<sub>i</sub>′</em> generated with the vanilla CoT prompt serve as negative samples. These positive-negative pairs are then used to create the pairwise preference data for DPO training. Given the crafted dataset <em>D = {</em>(x<sub>i</sub>, y<sub>i</sub>, y<sub>i&lt;/sub′)*<sub>N</sub><sup>N</sup> = 1, the objective is to maximize the likelihood that the model ranks the positive samples higher than the negative ones. Formally, we aim to optimize the following objective:</p>
<p>$$
\mathcal{L}<em i="1">{\text{DPO}}(\theta) = -\frac{1}{N} \sum</em>
$$}^{N} \log P_{\theta}(y_i \succ y_i') \quad \text{where</p>
<p>$$
P_{\theta}(y_i \succ y_i') = \frac{\exp(s(y_i, \mathbf{x}_i))}{\exp(s(y_i, \mathbf{x}_i)) + \exp(s(y_i', \mathbf{x}_i))}.
$$</p>
<p><em>P<sub>θ</sub></em>(<em>y<sub>i</sub></em> &gt; <em>y<sub>i</sub>′</em>) is the preference function. Here, <em>s</em>(<em>y<sub>i</sub></em>, <em>x<sub>i</sub></em>) is defined as ∑<sub>t=1</sub><sup>T<sub>i</sub></sup> log <em>P(y<sub>i,t</sub> | y<sub>i,&lt;t</sub>, x<sub>i</sub>)</em>, and it represents the log-probability of the model generating <em>y<sub>i</sub></em> for input <em>x<sub>i</sub></em>, which serves as the preference score assigned to <em>y<sub>i</sub></em>. This score measures how strongly the model favors that output. The objective ensures that the model prioritizes concise and token-efficient outputs while maintaining high-quality reasoning and correctness. During training, the LLM is encouraged to internalize the token budget constraint and adopt a more compact reasoning process guided by the target outputs generated in the first stage. This two-stage process effectively trains the LLM to produce concise yet accurate responses, striking a balance between reasoning quality and token efficiency during inference. More details are in Section A.3.</p>
<h2>6 Evaluation</h2>
<p>In this section, we provide the experiment results to evaluate the effectiveness of two versions of TALE,</p>
<p>Table 3: Comparison of TALE-EP (estimation and prompting) and other prompt engineering methods. "Directly Answering" means prompting LLM without any reasoning process. "Vanilla CoT" means the vanilla CoT prompting without budget. The model used in our evaluation is GPT-4o-mini <em>OpenAI (2024a)</em>. Observe that TALE-EP achieves an average accuracy (ACC) of 80.22%, with an average output token cost of 138.53 and an average expense of 118.46. TALE-EP reduces output token costs by 67%, lowers expenses by 59%, and maintains competitive performance compared to the vanilla CoT approach. ACC $\uparrow$, Output Tokens $\downarrow$, Expense (10${}^{-5}$\$ / sample) $\downarrow$.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Directly Answering</th>
<th></th>
<th></th>
<th>Vanilla CoT</th>
<th></th>
<th></th>
<th>TALE-EP</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>ACC $\uparrow$</td>
<td>Output Tokens $\downarrow$</td>
<td>Expense $\downarrow$</td>
<td>ACC $\uparrow$</td>
<td>Output Tokens $\downarrow$</td>
<td>Expense $\downarrow$</td>
<td>ACC $\uparrow$</td>
<td>Output Tokens $\downarrow$</td>
<td>Expense $\downarrow$</td>
</tr>
<tr>
<td>GSM8K</td>
<td>28.29%</td>
<td>12.46</td>
<td>39.43</td>
<td>81.35%</td>
<td>318.10</td>
<td>541.09</td>
<td>84.46%</td>
<td>77.26</td>
<td>279.84</td>
</tr>
<tr>
<td>GSM8K-Zero</td>
<td>97.21%</td>
<td>18.85</td>
<td>91.69</td>
<td>99.50%</td>
<td>252.96</td>
<td>886.79</td>
<td>98.72%</td>
<td>22.67</td>
<td>276.12</td>
</tr>
<tr>
<td>MathBench-Arithmetic</td>
<td>59.67%</td>
<td>41.10</td>
<td>9.78</td>
<td>75.00%</td>
<td>313.51</td>
<td>78.58</td>
<td>73.67%</td>
<td>39.60</td>
<td>18.62</td>
</tr>
<tr>
<td>MathBench-Middle</td>
<td>33.33%</td>
<td>5.00</td>
<td>3.58</td>
<td>84.67%</td>
<td>553.93</td>
<td>68.22</td>
<td>79.33%</td>
<td>238.14</td>
<td>42.95</td>
</tr>
<tr>
<td>MathBench-High</td>
<td>51.33%</td>
<td>5.00</td>
<td>4.07</td>
<td>84.00%</td>
<td>653.24</td>
<td>82.44</td>
<td>80.00%</td>
<td>254.82</td>
<td>47.61</td>
</tr>
<tr>
<td>MathBench-College</td>
<td>44.00%</td>
<td>5.00</td>
<td>3.68</td>
<td>78.00%</td>
<td>675.78</td>
<td>81.56</td>
<td>70.00%</td>
<td>259.85</td>
<td>45.60</td>
</tr>
<tr>
<td>Average</td>
<td>52.31%</td>
<td>14.57</td>
<td>25.37</td>
<td>83.75%</td>
<td>461.25</td>
<td>289.78</td>
<td>81.03%</td>
<td>148.72</td>
<td>118.46</td>
</tr>
</tbody>
</table>
<p>Table 4: The generalization of TALE-EP (estimation and prompting) across different LLMs. Yi-lightning <em>Wake et al. (2024)</em>, GPT-4o-mini <em>OpenAI (2024a)</em>, GPT-4o <em>OpenAI (2024b)</em> and o3-mini <em>OpenAI (2025)</em> are taken into consideration. We conduct following evaluations on the MathBench-College dataset. ACC $\uparrow$, Output Tokens $\downarrow$, Expense (10${}^{-5}$\$ / sample) $\downarrow$.</p>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Directly Answering</th>
<th></th>
<th></th>
<th>Vanilla CoT</th>
<th></th>
<th></th>
<th>TALE-EP</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>ACC $\uparrow$</td>
<td>Output Tokens $\downarrow$</td>
<td>Expense $\downarrow$</td>
<td>ACC $\uparrow$</td>
<td>Output Tokens $\downarrow$</td>
<td>Expense $\downarrow$</td>
<td>ACC $\uparrow$</td>
<td>Output Tokens $\downarrow$</td>
<td>Expense $\downarrow$</td>
</tr>
<tr>
<td>Yi-lightning</td>
<td>66.67%</td>
<td>80.01</td>
<td>3.09</td>
<td>79.33%</td>
<td>998.10</td>
<td>21.55</td>
<td>76.67%</td>
<td>373.52</td>
<td>17.25</td>
</tr>
<tr>
<td>GPT-4o-mini</td>
<td>44.00%</td>
<td>5.00</td>
<td>3.68</td>
<td>78.00%</td>
<td>675.78</td>
<td>81.56</td>
<td>70.00%</td>
<td>259.85</td>
<td>45.60</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>57.33%</td>
<td>5.00</td>
<td>61.34</td>
<td>84.00%</td>
<td>602.29</td>
<td>1359.42</td>
<td>80.00%</td>
<td>181.61</td>
<td>759.95</td>
</tr>
<tr>
<td>o3-mini</td>
<td>96.00%</td>
<td>601.51</td>
<td>336.69</td>
<td>97.33%</td>
<td>1163.55</td>
<td>638.46</td>
<td>96.66%</td>
<td>677.65</td>
<td>385.12</td>
</tr>
</tbody>
</table>
<p>TALE-EP and TALE-PT. The comparisons of the two implementations are detailed in Section A.6.</p>
<h3>6.1 Experiment Setup</h3>
<p>Datasets. To evaluate the LLM performance, three most challenging mathematical datasets are taken into consideration: GSM8K <em>Cobbe et al. (2021)</em>, GSM8K-Zero <em>Chiang and Lee (2024)</em>, and MathBench <em>Liu et al. (2024)</em>. GSM8K-Zero, derived from the GSM8K dataset, specifically targets the analysis of over-reasoning and redundancy in LLMgenerated outputs. In short, GSM8K-Zero is designed so that the answers are embedded within the questions themselves. LLMs can easily generate correct responses without complicated additional reasoning or redundant calculations. Models. We conduct experiments on five state-of-the-art LLMs (i.e., GPT-4o <em>OpenAI (2024b)</em>, GPT-4o-mini <em>OpenAI (2024a)</em>, Yi-lightning <em>Wake et al. (2024)</em>, o3-mini <em>OpenAI (2025)</em>), and Lllama-3.1-8B-Instruct <em>Dubey et al. (2024)</em>. Metrics. The target of TALE is to balance the LLM correctness performance and extra redundant token costs. Specifically, TALE seeks to minimize Number of Output Tokens while maintaining comparable Accuracy (Acc) simultaneously. Accuracy (Acc). This metric is calculated as the following: $\operatorname{Accuracy}=\frac{1}{N} \sum_{i=1}^{N} \mathbb{I}\left{\mathcal{M}\left(\boldsymbol{x}<em i="i">{i}\right)=y</em>}\right}$, where $\left(\boldsymbol{x<em i="i">{i}, y</em>}\right) \in \mathcal{X} . \boldsymbol{x<em i="i">{i}$ is the math question from dataset $\mathcal{X}$ and $y_{i}$ the ground truth answer. $\mathcal{M}(\cdot)$ returns the answer for a given question. $\mathbb{I}{\cdot}$ represents an indicator function. This function evaluates whether the inside given condition holds. Specifically, it returns $\mathbf{1}$ if the condition is true and $\mathbf{0}$ if the condition is false. For a better evaluation, we format the LLM output by crafting an elaborate instruction detailed in Figure 8. Number of Output Tokens. We evaluate the token costs by calculating the average output token consumption for each specific task. The output token costs are measured as follows: $\operatorname{Number~of~Output~Tokens~}=\frac{1}{N} \sum_{i=1}^{N} \mathbb{T}\left(\mathcal{M}\left(\boldsymbol{x}_{i}\right)\right)$, where $\boldsymbol{x}</em>$. To evaluate costs more precisely, we calculate the average expense per sample. The total token expense includes both input and output tokens used during the query process.}$ represents the given question, and $\mathbb{T}$ is a function that measures the number of tokens. Intuitively, the more output tokens, the higher the costs incurred by $\mathcal{M</p>
<h3>6.2 Effectiveness of TALE-EP</h3>
<p>Table 3 compares TALE-EP with other prompt engineering methods across seven datasets, evaluating accuracy, output tokens, and expenses. Effective prompts should maximize accuracy while minimizing token usage and cost. Direct Answering is the most cost-efficient (14.57 tokens, 25.37 expense) but with low accuracy (52.31%). Vanilla</p>
<p>Table 5: Comparison of TALE-PT (post-training to internalize token-budget awareness) and other prompt engineering methods. Two different post-training methods, SFT and DPO, are taken into consideration.</p>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Dataset</th>
<th>Directly Answering</th>
<th></th>
<th>Vanilla CoT</th>
<th></th>
<th>TALE-PT-SFT</th>
<th></th>
<th>TALE-PT-DPO</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>ACC $\uparrow$</td>
<td>Output Tokens $\downarrow$</td>
<td>ACC $\uparrow$</td>
<td>Output Tokens $\downarrow$</td>
<td>ACC $\uparrow$</td>
<td>Output Tokens $\downarrow$</td>
<td>ACC $\uparrow$</td>
<td>Output Tokens $\downarrow$</td>
</tr>
<tr>
<td>Llama-3.1-8B-Instruct</td>
<td>GSM8K</td>
<td>21.00\%</td>
<td>38.54</td>
<td>77.56\%</td>
<td>241.51</td>
<td>78.57\%</td>
<td>139.63</td>
<td>74.11\%</td>
<td>149.93</td>
</tr>
<tr>
<td></td>
<td>GSM8K-Zero</td>
<td>70.32\%</td>
<td>13.49</td>
<td>65.04\%</td>
<td>251.08</td>
<td>78.43\%</td>
<td>77.85</td>
<td>78.41\%</td>
<td>113.41</td>
</tr>
</tbody>
</table>
<p>CoT achieves the highest accuracy (83.75\%) but at a high cost ( 461.25 tokens, 289.78 expense). TALE-EP balances performance and efficiency, achieving $81.03 \%$ accuracy while reducing token usage to $32 \%$ and expenses to $41 \%$ of Vanilla CoT. On GSM8K, it even surpasses Vanilla CoT with $84.46 \%$ accuracy. Note that expense is not directly proportional to output tokens because it also accounts for input and cached tokens. TALE-EP reduces token costs by $68.64 \%$ on average, offering a scalable, cost-effective solution for budgetconstrained reasoning tasks. For resource-rich scenarios, we evaluate TALE-EP under a larger token budget as detailed in Section A.8.</p>
<p>To further evaluate the generalization of TALEEP across different LLMs. We conduct experiments across Yi-lightning, GPT-4o-mini, GPT-4o and o3-mini on MathBench-College. Table 4 illustrates the results, showing TALE-EP's ability to reduce output tokens and expenses while maintaining competitive accuracy significantly. TALE-EP achieves substantial token savings, reducing output tokens by $64.63 \%$ on average, compared to Vanilla CoT. Expense reductions are equally notable, with costs decreasing by $45.30 \%$ on average. Despite these cost savings, TALE-EP maintains strong accuracy, achieving $76.67 \%$ on Yi-lightning, $70.00 \%$ on GPT-4o-mini, and $80.00 \%$ on GPT-4o, comparable to Vanilla CoT. These results highlight TALE-EP's effectiveness in balancing cost efficiency and reasoning performance across diverse LLM architectures. The observed accuracy drop is most significant for GPT-4o-mini. This could be attributed to its smaller number of parameters, which makes it more challenging to answer correctly within a limited response reasoning length. We also evaluate the applicability of TALE on more tasks, the results are illustrated in Section A.5. The efficiency analysis of TALE-EP is in Section A. 7</p>
<h3>6.3 Effectiveness of TALE-PT</h3>
<p>Table 5 compares TALE-PT methods with Vanilla CoT and Direct Answering on GSM8K and GSM8K-Zero using Llama-3.1-8B-Instruct. For GSM8K, Direct Answering demonstrates the low-
est token usage (38.54) but at the cost of significantly reduced accuracy ( $21.00 \%$ ). In contrast, Vanilla CoT achieves much higher accuracy ( $77.56 \%$ ) but incurs a significant increase in token cost (241.51). Note that on GSM8K-Zero, the accuracy of Vanilla CoT drops below Direct Answering. This drop can be attributed to overthinking, as GSM8K-Zero is simpler, with answers often implied directly within the question. In such cases, a long reasoning process can introduce unnecessary complexity, leading to reduced accuracy. Among the TALE-PT methods, TALE-PT-SFT achieves the best accuracy ( $78.57 \%, 78.43 \%$ ) with reduced tokens, while TALE-PT-DPO balances accuracy $(74.11 \%, 78.41 \%)$ and token efficiency, cutting token consumption by over $50 \%$ on GSM8K-Zero compared to Vanilla CoT.</p>
<h2>7 Conclusion</h2>
<p>In this paper, we introduce TALE, a framework that reduces token redundancy in Chain-ofThought (CoT) reasoning by incorporating token budget awareness. TALE dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem, balancing token efficiency and answer correctness. Experiments show that TALE reduces output token usage and expense significantly with acceptable accuracy loss, outperforming Vanilla CoT in cost-effectiveness while generalizing well across various LLMs.</p>
<h2>8 Limitations</h2>
<p>The experiments of our proposed token-budgetaware reasoning framework currently focus on LLMs that process only text as input and output. While the results demonstrate significant improvements in efficiency and cost reduction, it does not account for models that have multimodal output content. Such as the models generate interleaved images and text as output. In future work, we will extend token-budget awareness to such LLMs with multimodal output by introducing modalityspecific budget constraints and designing adaptive strategies to optimize token efficiency for different modality types, such as images and videos.</p>
<h2>References</h2>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732.</p>
<p>Aman Bhargava, Cameron Witkowski, Shi-Zhuo Looi, and Matt Thomson. 2023. What's the magic word? a control theory of llm prompting. arXiv preprint arXiv:2310.04444.</p>
<p>Bin Bi, Chenliang Li, Chen Wu, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, and Luo Si. 2020. Palm: Pre-training an autoencoding\&amp;autoregressive language model for context-conditioned generation. arXiv preprint arXiv:2004.07159.</p>
<p>Haolin Chen, Yihao Feng, Zuxin Liu, Weiran Yao, Akshara Prabhakar, Shelby Heinecke, Ricky Ho, Phil Mui, Silvio Savarese, Caiming Xiong, et al. 2024. Language models are hidden reasoners: Unlocking latent reasoning capabilities via self-rewarding. arXiv preprint arXiv:2411.04282.</p>
<p>Cheng-Han Chiang and Hung-yi Lee. 2024. Overreasoning and redundant calculation of large language models. arXiv preprint arXiv:2401.11467.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Dujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee, Victor Rühle, Laks VS Lakshmanan, and Ahmed Hassan Awadallah. 2024. Hybrid llm: Cost-efficient and quality-aware query routing. In The Twelfth International Conference on Learning Representations.</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783.</p>
<p>Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. 2024. Towards revealing the mystery behind chain of thought: a theoretical perspective. Advances in Neural Information Processing Systems, 36.</p>
<p>Xidong Feng, Ziyu Wan, Muning Wen, Ying Wen, Weinan Zhang, and Jun Wang. 2023. Alphazero-like tree-search can guide large language model decoding and training. In NeurIPS 2023 Foundation Models for Decision Making Workshop.</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948.</p>
<p>Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, et al. 2024a. Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models. arXiv preprint arXiv:2404.05221.</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992.</p>
<p>Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. 2024b. Training large language models to reason in a continuous latent space. arXiv preprint arXiv:2412.06769.</p>
<p>Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022. Large language models are reasoning teachers. arXiv preprint arXiv:2212.10071.</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.</p>
<p>Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436.</p>
<p>Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao, Kai Mei, Yanda Meng, Kaize Ding, et al. 2024a. Exploring concept depth: How large language models acquire knowledge and concept at different layers? arXiv preprint arXiv:2404.07066.</p>
<p>Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, and Mengnan Du. 2024b. The impact of reasoning step length on large language models. arXiv preprint arXiv:2401.04925.</p>
<p>Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 19274-19286. PMLR.</p>
<p>Chenliang Li, Bin Bi, Ming Yan, Wei Wang, and Songfang Huang. 2021. Addressing semantic drift in generative question answering with auxiliary extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 942-947.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5315-5333.</p>
<p>Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe Zhou, Wenwei Zhang, Songyang Zhang, Dahua Lin, and Kai Chen. 2024. Mathbench: Evaluating the theory and application proficiency of llms with a hierarchical mathematics benchmark. arXiv preprint arXiv:2405.12209.</p>
<p>Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful chain-ofthought reasoning. arXiv preprint arXiv:2301.13379.</p>
<p>Sania Nayab, Giulio Rossolini, Giorgio Buttazzo, Nicolamaria Manes, and Fabrizio Giacomelli. 2024. Concise thoughts: Impact of output length on llm reasoning and cost. arXiv preprint arXiv:2407.19825.</p>
<p>OpenAI. 2024a. Gpt-4o mini: advancing cost-efficient intelligence. Technical report, OpenAI. Accessed: July 18, 2024.</p>
<p>OpenAI. 2024b. Hello gpt-4o. Technical report, OpenAI. Accessed: May 13, 2024.</p>
<p>OpenAI. 2024c. Learning to reason with llms. Technical report, OpenAI.</p>
<p>OpenAI. 2025. Openai o3-mini: Pushing the frontier of cost-effective reasoning. Technical report, OpenAI. Accessed: January 31, 2025.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311-318.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36:5372853741 .</p>
<p>Hannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. 2019. Towards empathetic opendomain conversation models: A new benchmark and dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages $5370-5381$.</p>
<p>Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement
learning. Advances in Neural Information Processing Systems, 36.</p>
<p>Guangyan Sun, Mingyu Jin, Zhenting Wang, ChengLong Wang, Siqi Ma, Qifan Wang, Ying Nian Wu, Yongfeng Zhang, and Dongfang Liu. 2024. Visual agents as fast and slow thinkers. arXiv preprint arXiv:2408.08862.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261.</p>
<p>Alan Wake, Albert Wang, Bei Chen, CX Lv, Chao Li, Chengen Huang, Chenglin Cai, Chujie Zheng, Daniel Cooper, Ethan Dai, et al. 2024. Yi-lightning technical report. arXiv preprint arXiv:2412.01253.</p>
<p>Junlin Wang, Siddhartha Jain, Dejiao Zhang, Baishakhi Ray, Varun Kumar, and Ben Athiwaratkun. 2024a. Reasoning in token economies: Budget-aware evaluation of llm reasoning strategies. arXiv preprint arXiv:2406.06461.</p>
<p>Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-and-solve prompting: Improving zero-shot chain-ofthought reasoning by large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2609-2634.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Zhenting Wang, Guofeng Cui, Kun Wan, and Wentian Zhao. 2025. Dump: Automated distribution-level curriculum learning for rl-based llm post-training. arXiv preprint arXiv:2504.09710.</p>
<p>Zhenting Wang, Shuming Hu, Shiyu Zhao, Xiaowen Lin, Felix Juefei-Xu, Zhuowei Li, Ligong Han, Harihar Subramanyam, Li Chen, Jianfa Chen, et al. 2024b. Mllm-as-a-judge for image safety without human labeling. arXiv preprint arXiv:2501.00192.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837.</p>
<p>Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022. Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. In Proceedings of the 2022 CHI conference on human factors in computing systems, pages $1-22$.</p>
<p>Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P Lillicrap, Kenji Kawaguchi, and Michael Shieh. 2024. Monte carlo tree search boosts reasoning via iterative preference learning. arXiv preprint arXiv:2405.00451.
Jingfeng Yang, Haoming Jiang, Qingyu Yin, Danqing Zhang, Bing Yin, and Diyi Yang. 2022. Seqzero: Few-shot compositional semantic parsing with sequential prompts and zero-shot models. arXiv preprint arXiv:2205.07381.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024a. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024b. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36.
Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang, and Yang You. 2024. Response length perception and sequence scheduling: An llmempowered llm inference pipeline. Advances in Neural Information Processing Systems, 36.
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. 2022. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625.</p>
<h2>A Appendix</h2>
<h2>A. 1 Definition of Ideal Budget Range</h2>
<p>Ideal Budget Range. Based on the observation of token elasticity, a token cost bottom range exists during searching for the optimal budget. In this range, the token costs approach the token cost lowest bound. Before or after the range, the token cost will increase. We define such a bottom range as "ideal budget range". It's worth noting that the budget continuously degrades during the search. Only the token cost rebounds. That's why we refer to this observation as token elasticity. To summarize, ideal budget range is an range that minimizes actual token consumption. Let $\boldsymbol{\beta}=\left{\beta_{1}, \beta_{2}, \ldots, \beta_{N}\right}$ denote all possible budgets that can maintain answer correctness. A rolling window $W \in \boldsymbol{\beta}$ is applied iteratively over $\boldsymbol{\beta}$. Let $k$ represent the range size, which is adaptively determined during our evaluation as $\frac{N}{3}$, where $N$ is the total number of possible budgets. A budget range is defined as:</p>
<p>$$
\begin{gathered}
W_{k}(i)=\left{\boldsymbol{\beta}_{j} \mid i \leq j \leq i+k-1\right} \
1 \leq i \leq|\boldsymbol{\beta}|-k+1
\end{gathered}
$$</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" />
(c) TALE (68 output tokens).</p>
<p>Figure 7: An intuitive example to illustrate the workflow of TALE-EP on GPT-4o-mini (OpenAI, 2024a).
The ideal budget range $W^{*}$ is defined as:</p>
<p>$$
W_{k}^{*}=\arg \min <em _beta__j="\beta_{j">{i}\left(\sum</em>\right)\right)
$$} \in W_{k}(i)} \mathbb{T}\left(\beta_{j</p>
<p>where $\mathbb{T}$ denote the actual token consumption for a given budget $\beta \in \boldsymbol{\beta}$. We aim to estimate a budget located in the ideal budget ranges without any search process. In that case, TALE obtains the ideal budget within acceptable sacrifice.</p>
<h2>A. 2 Effectiveness of Budget Estimation.</h2>
<p>In this RQ, we evaluate the effectiveness of the budget estimation performance. An ideal estimated budget should be located around the optimal</p>
<p>Q: The region... The volume of the resulting solid of revolution is?
A. $\mathbf{x} / 12$ B. $\mathbf{x} / 6$ C. $\mathbf{x} / 3$ D. $2 \mathbf{z} / 3$</p>
<p>Please strictly follow the format: [[choice]], for example: Choice: [[A]].</p>
<p>Figure 8: The instruction prompt used to format the LLM output on multiple-choice questions.
searched budget and in the bottom area of Figure 2. We further define such an area as the ideal budget range and give the formalized definition in Section A.1. A good budget should be located in the ideal budget range. Two metrics are taken into consideration: in-range accuracy and out-of-range distance. In-range accuracy determines whether the predicted budget $\hat{\beta}$ falls within the ideal budget range $W_{k}^{*}$. Mathematically, it can be expressed as:</p>
<p>$$
\mathbb{I}\left{\hat{\beta} \in W_{k}^{<em>}\right}= \begin{cases}1, &amp; \text { if } \hat{\beta} \in W_{k}^{</em>} \ 0, &amp; \text { otherwise }\end{cases}
$$</p>
<p>Out-of-range distance quantifies the distance between $\hat{\beta}$ and $W_{k}^{<em>}$ if the predicted budget $\beta^{</em>}$ falls outside the ideal budget range $W_{k}^{<em>}$. Let $\operatorname{dist}\left(\hat{\beta}, W_{k}^{</em>}\right)$ represent the distance, defined as:</p>
<p>$$
\operatorname{dist}\left(\hat{\beta}, W_{k}^{<em>}\right)= \begin{cases}0, &amp; \text { if } \hat{\beta} \in W_{k}^{</em>} \ \min <em k="k">{\beta \in W</em>^{<em>}}|\hat{\beta}-\beta|, &amp; \text { if } \hat{\beta} \notin W_{k}^{</em>}\end{cases}
$$</p>
<p>Intuitively, a higher in-range accuracy and a lower out-range distance indicate a better estimated budget. During our evaluation, the in-range accuracy is $60.61 \%$, and the out-of-range distance is 109.64 . It indicates that more than two-thirds of estimated budgets are located in the ideal range. For those out-of-range samples, they have an offset of 109.64 tokens on average. Figure 9 illustrates the successful and failed estimated cases intuitively. The prompt we use for budget estimation is as follows:
"Task: Analyze the given question and estimate the minimum number of tokens required for reasoning."</p>
<p>This prompt encourages the model to evaluate the question as a whole, including reasoning depth, structure, completeness, and surface-level difficulty.</p>
<h2>A. 3 Details of TALE's Implementation</h2>
<p>In this section, we introduce the hyper-parameters used for TALE-EP and TALE-PT.
TALE-EP. TALE-EP uses a zero-shot mechanism to estimate the token budget and then prompts the LLM. The instruction prompts used during this
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 9: An intuitive example for successful and failed cases of prompt budget estimation in TALE-EP.</p>
<p>Table 6: Comparison of TALE-EP and TALE-PT.</p>
<table>
<thead>
<tr>
<th>Metrics</th>
<th>TALE-EP</th>
<th>TALE-PT</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>SFT</td>
<td>DPO</td>
</tr>
<tr>
<td>ACC</td>
<td>71.82</td>
<td>78.57</td>
<td>74.11</td>
</tr>
<tr>
<td>Output Tokens</td>
<td>112.21</td>
<td>139.63</td>
<td>149.93</td>
</tr>
</tbody>
</table>
<p>process are shown in Figure 7. To ensure output consistency, we set the temperature to 0.1 and limit the model to a single reasoning path. Additionally, the random seed is fixed at 1024.
TALE-PT. TALE-PT includes two implementations: SFT and DPO. For parameter efficiency, both implementations adopt LoRA (Hu et al., 2021) for post-training, with rank set to 8 and lora alpha set to 32. For TALE-PT-SFT, we train for 3 epochs with a batch size of 16 , a learning rate of $1 \mathrm{e}-4$, and a weight decay of 0.01 . For TALE-PT-DPO, we train for 2 epochs with a batch size of 16 , a learning rate of $3 \mathrm{e}-5$, and a weight decay of 0.001 .</p>
<h2>A. 4 Comparison of TALE-EP and TALE-PT.</h2>
<p>In this section, we compare the performance of TALE-EP and TALE-PT (including SFT and DPO). Specifically, we utilize Llama-3.1-8B-Instruct as both the budget estimator for TALE-EP and the base model for TALE-PT, evaluated on GSM8K. Table 6 illustrates the evidence. As the zero-shotbased estimator tends to predict relatively low budgets, TALE-EP achieves lower token usage but at the cost of slightly reduced accuracy. In contrast, both variants of TALE-PT achieve higher accuracy with more tokens, as their training data is constructed using optimal budget search, which enforces answer correctness as a strict constraint even with a higher token costs. This highlights a trade-off between strict correctness preservation (in TALE-PT) and token efficiency (in TALE-EP).</p>
<h2>A. 5 Applicability of TALE on More Tasks.</h2>
<p>To further evaluate the applicability, we deploy</p>
<p>Table 7: Generalization of TALE on more tasks. Three popular LLM generative tasks, Code Summarization <em>Husain et al. (2019)(CS)</em>, Empathetic Response Generation <em>Rashkin et al. (2019)(ERG)</em>, Code Generation <em>Austin et al. (2021)(CG)</em>, are taken into consideration. BLEU is taken as the metric to evaluate the performance. BLEU$\uparrow$. Output Tokens$\uparrow$.</p>
<p>| Tasks | TALE-EP | | Vanilla CoT | |
| | BLEU | Output Tokens | BLEU | Output Tokens |
| CS | 0.07 | 44.39 | 0.2 | 134.05 |
| ERG | 0.005 | 60.34 | 0.006 | 175.37 |
| CG | 0.24 | 171.08 | 0.267 | 461.77 |</p>
<p>TALE-EP on three additional open-ended generative tasks. As these tasks involve open-ended text generation, we adopt the BLEU metric <em>Papineni et al. (2002)</em> to quantify the similarity between generated outputs and reference texts. The results in Table 4 demonstrate that TALE-EP achieves comparable or even better BLEU scores than Vanilla CoT while using only around 40% of the output tokens, validating its effectiveness and applicability in broader generative tasks.</p>
<h3>A.6 Formalizing the Budget Search.</h3>
<p>For a given input $\boldsymbol{x}$, we define the search space as:</p>
<p>$\mathcal{B}=\left{\beta \in \mathbb{Z}^{+}\left|0&lt;\beta \leq T_{\text {vanilla }}(x)\right}\right.$</p>
<p>where $T_{\text {vanilla }}(x)$ is the number of tokens generated by Vanilla CoT. A candidate budget $\beta \in \mathcal{B}$ is considered feasible if:</p>
<p>$$
L L M(x, \beta)=y \text { and } T(x, \beta)&lt;T\left(x, \beta_{0}\right)
$$</p>
<p>where $y$ is the ground-truth answer, $T(x, \beta)$ is the actual number of output tokens when answering $x$ under budget $\beta, \beta_{0}$ is the previously searched (larger) feasible budget. Our goal is to find:
$\beta^{<em>}=\arg \min _{\beta \in \mathcal{B}} T(x, \beta)$, subject to $L L M(x, \beta)=y$
To efficiently find $\beta^{</em>}$, we employ a binary search procedure guided by the feasibility function above, as detailed in Algorithm 1 and Algorithm 2.</p>
<h3>A.7 Efficiency of TALE-EP.</h3>
<p>Since TALE-EP requires one additional query, we further evaluate its end-to-end latency in this section. Specifically, we query the Llama-3.1-8BInstruct model over the GSM8K-Zero dataset and measure both accuracy and average time cost. T he budget estimation query of TALE-EP is also considered. Although TALE-EP requires one additional</p>
<p>Table 8: The empirical evidence for "implicit monotonicity assumption". $\hat{\beta}$ is the budget upper bound, which is the token cost of vanilla CoT. The budget row displays scaled budgets ranging from $2^{-2}$ to $2^{2} \cdot \beta^{*}$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Budget( $<em> \beta^{</em>}$ )</th>
<th style="text-align: left;">$2^{-5}$</th>
<th style="text-align: left;">$2^{-4}$</th>
<th style="text-align: left;">$2^{-2}$</th>
<th style="text-align: left;">$2^{0}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ACC</td>
<td style="text-align: left;">69.23</td>
<td style="text-align: left;">75.82</td>
<td style="text-align: left;">75.82</td>
<td style="text-align: left;">76.92</td>
</tr>
<tr>
<td style="text-align: left;">Output Tokens</td>
<td style="text-align: left;">222.69</td>
<td style="text-align: left;">222.42</td>
<td style="text-align: left;">244.61</td>
<td style="text-align: left;">653.53</td>
</tr>
</tbody>
</table>
<p>query compared to Vanilla CoT, it is significantly more efficient, taking only 2.3 seconds per sample, while Vanilla CoT takes 10.2 seconds. This is because the primary factor influencing inference time is the number of output tokens, which TALE-EP effectively reduces.</p>
<h3>A.8 Effectiveness of Larger Token Budget.</h3>
<p>In scenarios with ample computational resources, the token budget could be larger for better performance. We simulate such a scenario by scaling the estimated budget by a factor $\alpha$ ( $\alpha *$ budget). As shown in the table below, increasing $\alpha$ from 1 to 2 leads to higher accuracy (from $67.33 \%$ to $72.66 \%$ ) at the cost of more tokens (from 210.97 to 279.78), demonstrating that TALE-EP can flexibly adapt to different resource scenarios.</p>
<h3>A.9 Empirical Evidence for the "Implicit Monotonicity Assumption".</h3>
<p>In this section, we give empirical evidence to support the "implicit monotonicity assumption" for our search algorithm. Specifically, for a set of questions, we vary the budget across a range of values (e.g., $2^{-5}, 2^{-4}, 2^{-2}, 2^{0}$ times budget upper bound, which is the token cost of vanilla CoT), and record the corresponding accuracy and average output tokens at each point. Table 8 illustrates the empirical evidence. Observe that it roughly follows from the monotonicity property. The results demonstrate a consistent trend: accuracy generally increases or plateaus with increasing budgets, confirming a soft monotonicity in most cases.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal Contribution.
${ }^{1}$ Start the project and propose the idea.
${ }^{2}$ Corresponding Author.
${ }^{3}$ Also available at https://www.gitlink.org.cn/ txhan/TALE&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1625 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1625</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1625</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-248810794</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2205.07481v1.pdf" target="_blank">Bridging Sim2Real Gap Using Image Gradients for the Task of End-to-End Autonomous Driving</a></p>
                <p><strong>Paper Abstract:</strong> We present the first prize solution to NeurIPS 2021 - AWS Deepracer Challenge. In this competition, the task was to train a reinforcement learning agent (i.e. an autonomous car), that learns to drive by interacting with its environment, a simulated track, by taking an action in a given state to maximize the expected reward. This model was then tested on a real-world track with a miniature AWS Deepracer car. Our goal is to train a model that can complete a lap as fast as possible without going off the track. The Deepracer challenge is a part of a series of embodied intelligence competitions in the field of autonomous vehicles, called The AI Driving Olympics (AI-DO). The overall objective of the AI-DO is to provide accessible mechanisms for benchmarking progress in autonomy applied to the task of autonomous driving. The tricky section of this challenge was the sim2real transfer of the learned skills. To reduce the domain gap in the observation space we did a canny edge detection in addition to cropping out of the unnecessary background information. We modeled the problem as a behavioral cloning task and used MLP-MIXER to optimize for runtime. We made sure our model was capable of handling control noise by careful filtration of the training data and that gave us a robust model capable of completing the track even when 50% of the commands were randomly changed. The overall runtime of the model was only 2-3ms on a modern CPU.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1625.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1625.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ImageGrad-Sim2Real</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bridging Sim2Real Gap Using Image Gradients for End-to-End Autonomous Driving</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sim-to-real method that trains an end-to-end imitation-learning driving policy on simulator images preprocessed with Canny edge detection (image gradients) and directly applies the learned policy on a real AWS DeepRacer vehicle; key techniques include gradient-based input, top-crop of images, and dataset filtering to reduce domain gap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>AWS DeepRacer car (real-world DeepRacer platform)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A small autonomous racing vehicle equipped with a front-facing greyscale monocular camera; action space is discrete with five steering/intensity actions {left-high, left-med, front, right-med, right-high}; vehicle speed is constant and not part of the action space.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>autonomous driving / robotics (racecar navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>simulator environment S (unnamed in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulator that provides rendered greyscale front-facing camera images and allows recording discrete driving actions and a reward signal; used to collect image-action demonstration pairs for imitation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>unspecified visual/rendering fidelity (simulator-provided greyscale camera images); physics/dynamics treated simply (constant speed, discrete steering actions).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Visual camera observations (rendered greyscale images) and resulting discrete action labels; simulator provides track layout and reward signal used for dataset filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Vehicle dynamics beyond constant speed and discrete steering intensities are simplified (speed constant, no continuous throttle/acceleration modeling); photometric differences (lighting, reflections) and sensor-specific noise/distortions are not modeled in detail; no explicit modeling of actuator delays or detailed contact dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical DeepRacer track used in NeurIPS AWS DeepRacer challenge — real DeepRacer car with greyscale camera driving on a track (paper mentions marble floor causing reflections); real-world images recorded for testing.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>End-to-end navigation / lane-following and steering decision-making on a racing track (classification over discrete steering actions).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Imitation learning / supervised classification: MLP-Mixer network trained with cross-entropy on simulator image-action pairs (images converted to image-gradients via Canny).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Track completion and robustness metrics reported qualitatively (robustness to 50% randomly-changed commands) and ranking in NeurIPS 2021 AWS DeepRacer challenge (1st place); no single numeric success rate provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Visual domain gap between simulator and real images (lighting differences, reflections on marble floor, sensor appearance differences); differences in photometric properties and unmodeled visual artifacts that cause misclassification if raw images are used.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Using Canny edge detection to convert both simulated and real images to image gradients (reduces photometric/domain differences), cropping top 20% of images to remove irrelevant regions, automatic dataset filtering to remove suboptimal demonstrations using simulator reward signal, and a lightweight MLP-Mixer architecture enabling low-latency inference on on-board hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>No quantitative fidelity thresholds given; the paper identifies that reducing reliance on raw photometric cues (by using image gradients) is sufficient to enable transfer in this setup and that photometric artifacts (e.g., reflections) were a key cause of transfer failure when not accounted for.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Converting simulator and real camera images to image gradients (Canny edge detection) substantially reduces the visual domain gap and enables direct sim-to-real transfer of an end-to-end imitation-learned driving policy without additional real-world fine-tuning; cropping irrelevant image regions and filtering training data for optimal demonstrations further improves robustness (they achieved robust track completion under substantial command corruption and won the NeurIPS 2021 AWS DeepRacer challenge).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Scaling simulation-to-real transfer by learning a latent space of robot skills <em>(Rating: 2)</em></li>
                <li>Pre-training of deep rl agents for improved learning under domain randomization <em>(Rating: 2)</em></li>
                <li>Driving policy transfer via modularity and abstraction <em>(Rating: 2)</em></li>
                <li>Unpaired image-to-image translation using cycle-consistent adversarial networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1625",
    "paper_id": "paper-248810794",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "ImageGrad-Sim2Real",
            "name_full": "Bridging Sim2Real Gap Using Image Gradients for End-to-End Autonomous Driving",
            "brief_description": "A sim-to-real method that trains an end-to-end imitation-learning driving policy on simulator images preprocessed with Canny edge detection (image gradients) and directly applies the learned policy on a real AWS DeepRacer vehicle; key techniques include gradient-based input, top-crop of images, and dataset filtering to reduce domain gap.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "AWS DeepRacer car (real-world DeepRacer platform)",
            "agent_system_description": "A small autonomous racing vehicle equipped with a front-facing greyscale monocular camera; action space is discrete with five steering/intensity actions {left-high, left-med, front, right-med, right-high}; vehicle speed is constant and not part of the action space.",
            "domain": "autonomous driving / robotics (racecar navigation)",
            "virtual_environment_name": "simulator environment S (unnamed in paper)",
            "virtual_environment_description": "Simulator that provides rendered greyscale front-facing camera images and allows recording discrete driving actions and a reward signal; used to collect image-action demonstration pairs for imitation learning.",
            "simulation_fidelity_level": "unspecified visual/rendering fidelity (simulator-provided greyscale camera images); physics/dynamics treated simply (constant speed, discrete steering actions).",
            "fidelity_aspects_modeled": "Visual camera observations (rendered greyscale images) and resulting discrete action labels; simulator provides track layout and reward signal used for dataset filtering.",
            "fidelity_aspects_simplified": "Vehicle dynamics beyond constant speed and discrete steering intensities are simplified (speed constant, no continuous throttle/acceleration modeling); photometric differences (lighting, reflections) and sensor-specific noise/distortions are not modeled in detail; no explicit modeling of actuator delays or detailed contact dynamics.",
            "real_environment_description": "Physical DeepRacer track used in NeurIPS AWS DeepRacer challenge — real DeepRacer car with greyscale camera driving on a track (paper mentions marble floor causing reflections); real-world images recorded for testing.",
            "task_or_skill_transferred": "End-to-end navigation / lane-following and steering decision-making on a racing track (classification over discrete steering actions).",
            "training_method": "Imitation learning / supervised classification: MLP-Mixer network trained with cross-entropy on simulator image-action pairs (images converted to image-gradients via Canny).",
            "transfer_success_metric": "Track completion and robustness metrics reported qualitatively (robustness to 50% randomly-changed commands) and ranking in NeurIPS 2021 AWS DeepRacer challenge (1st place); no single numeric success rate provided.",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Visual domain gap between simulator and real images (lighting differences, reflections on marble floor, sensor appearance differences); differences in photometric properties and unmodeled visual artifacts that cause misclassification if raw images are used.",
            "transfer_enabling_conditions": "Using Canny edge detection to convert both simulated and real images to image gradients (reduces photometric/domain differences), cropping top 20% of images to remove irrelevant regions, automatic dataset filtering to remove suboptimal demonstrations using simulator reward signal, and a lightweight MLP-Mixer architecture enabling low-latency inference on on-board hardware.",
            "fidelity_requirements_identified": "No quantitative fidelity thresholds given; the paper identifies that reducing reliance on raw photometric cues (by using image gradients) is sufficient to enable transfer in this setup and that photometric artifacts (e.g., reflections) were a key cause of transfer failure when not accounted for.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Converting simulator and real camera images to image gradients (Canny edge detection) substantially reduces the visual domain gap and enables direct sim-to-real transfer of an end-to-end imitation-learned driving policy without additional real-world fine-tuning; cropping irrelevant image regions and filtering training data for optimal demonstrations further improves robustness (they achieved robust track completion under substantial command corruption and won the NeurIPS 2021 AWS DeepRacer challenge).",
            "uuid": "e1625.0"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Scaling simulation-to-real transfer by learning a latent space of robot skills",
            "rating": 2,
            "sanitized_title": "scaling_simulationtoreal_transfer_by_learning_a_latent_space_of_robot_skills"
        },
        {
            "paper_title": "Pre-training of deep rl agents for improved learning under domain randomization",
            "rating": 2,
            "sanitized_title": "pretraining_of_deep_rl_agents_for_improved_learning_under_domain_randomization"
        },
        {
            "paper_title": "Driving policy transfer via modularity and abstraction",
            "rating": 2,
            "sanitized_title": "driving_policy_transfer_via_modularity_and_abstraction"
        },
        {
            "paper_title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
            "rating": 1,
            "sanitized_title": "unpaired_imagetoimage_translation_using_cycleconsistent_adversarial_networks"
        }
    ],
    "cost": 0.0066485,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Bridging Sim2Real Gap Using Image Gradients for the Task of End-to-End Autonomous Driving
May 17, 2022</p>
<p>Unnikrishnan R Nair unnikrishnan.r@olaelectric.com 
Ola Electric Midhun S Menon
Ola Electric, Ola Electric, Ola Electric, Ola Electric</p>
<p>Sarthak Sharma sarthak.sharma1@olaelectric.com 
Ola Electric Midhun S Menon
Ola Electric, Ola Electric, Ola Electric, Ola Electric</p>
<p>Udit Singh Parihar 
Ola Electric Midhun S Menon
Ola Electric, Ola Electric, Ola Electric, Ola Electric</p>
<p>Srikanth Vidapanakal srikanth.vidapanakal@olaelectric.com 
Ola Electric Midhun S Menon
Ola Electric, Ola Electric, Ola Electric, Ola Electric</p>
<p>Bridging Sim2Real Gap Using Image Gradients for the Task of End-to-End Autonomous Driving
May 17, 2022Reinforcement LearningSim2Real TransferRacecarComputer Vision</p>
<p>As the car navigates through the environment (in a simulator or in real world), it is allowed to observe a greyscale monocular image from a front-facing camera mounted on the vehicle. At any instance, the car can take a set of actions from a discrete action space A = {lef t − high, lef t − med, f ront, right − med, right − high}, where med and high capture the intensity of taking a particular direction. The vehicle's speed remains constant and is not part of the action space.</p>
<p>Given the limited access to the vehicle's controls, we modeled our approach as a classification task over the action space A in an imitation learning setup, which takes the current image observation at any given time t. While driving in the simulator environment S, we record the action a s,t ∈ A taken at every time instant t, along with the corresponding camera image I s,t . Similarly, while driving in the real environment R, we record the corresponding camera image I r,t . The task is to obtain a probability distribution over the set of actions A, P(y = a r,t |I r,t ), with the simulator recorded data D s = {(a s,t , I s,t )} as the training set.</p>
<p>Related Work</p>
<p>Imitation Learning: Imitation learning (IL) has seen recent surge for the task of autonomous driving (Zhang and Cho (2016); Pan et al. (2017)). IL uses expert demonstrations to directly learn a policy that maps states to actions. The pionerring work of (Pomerleau (1988)) intoduced IL for self-driving -where a direct mapping from the sensor data to steering angle and acceleration is learned. (Zeng et al. (2019); Viswanath et al. (2018)) also follow the similar approach of going from sensor data to throttle and steering. With the advent of high end driving simulators like (Dosovitskiy et al. (2017)), approaches like (Codevilla et al. (2018)) exploit conditional models with additional exploit conditional models with additional high-level commands such as continue, turn-left, turn-right. Methods like ) use intermediate representations derived from sensor data, which is then converted to steering commands.</p>
<p>Sim2Real transfer: Simulation to real-world transfer, popularly known as Sim2Real transfer has contributed immensely in training of models for different robotics tasks.</p>
<p>Sim2Real transfer provides several benefits like bootstrapping, hardware in loop optimization and aiding when there is a real-world data starvation. However, Sim2Real methods suffer from domain gap between the simulated and real world. Approaches have explored domain randomization (Amiranashvili et al. (2021)), explicit transferable abstraction (Julian et al. (2020)) and domain adaption vis GANs (Zhu et al. (2017)) to bridge the Sim2Real gap.</p>
<p>Methods</p>
<p>Algorithm</p>
<p>We model our approach as a classification task over the action space A in an imitation learning setup. Specifically, while driving in the simulator environment S, we record the action a s,t ∈ A taken at every time instant t, along with the corresponding camera image I s,t ∈ R H×W . Motivated by (Tolstikhin et al. (2021)), we train a MLP-Mixer f θ , where θ are the parameters of the network. The model takes in as input I g,s,t where I g,s,t = g(I s,t ), where g is the gradient computation function. The output of the model f θ is scored actions :
φ(s a |I s,t ) = exp(f θ (I g,s,t )) 5 i=1 exp(f i θ (g(I s,t ))
The loss term for training this stage is the cross entropy between the predicted scores and ground truth scores:
L = L CE (φ(s a |I s,t ), a s,t )
At test time, we receive the image I r,t recorded from the DeepRacer car moving in the real world. Gradient computed image I g,r,t is passed through the above trained network f θ , to obtain the scores on the action space s a . The most likely action is then chosen and relayed to the vehicle to execute.</p>
<p>We showcase our network architecture in Fig. 2. For our experiments, we keep H = 64, W = 64 and train using SGD as the optimizer with learning rate of 1e − 4. Other hyper- Figure 2: Architecture: We show the architecture diagram of our proposed method. The input imageI s,t is passed through a Canny edge detection method to obtain the input I g,s,t which is then fed to the MLP mixer model f θ that computes the scores s a over the actions a ∈ A. At test time, we get the input image from the real world, I r,t , on which image gradients are calculated using Canny edge detection to obtain I g,r,t . This is then forwarded through the learned weights f θ to obtain the action scores s a . The most likely action is then chosen and relayed to the vehicle to execute. parameters include the parameters for the MLP-Mixer (Tolstikhin et al. (2021)) which are patch size = 8, dimension = 128, depth = 6.</p>
<p>Sim2Real Transfer</p>
<p>We achieve Sim2Real transfer for our method by passing the input images from simulator and real world through Canny edge detection. Specifically, when we get an input image from any of the real or simulator environment, we pass it through a Canny edge detector to obtain the corresponding image gradients I g,t . The training set then updates to D s = {(a s,t , I g,s,t )}. We showcase the results for both the real image and simulated image once after performing Canny edge detection in Fig.3. Image gradient of 100 along X and 256 along Y axis was set while running Canny edge detection. We remove unnecessary image regions, which contain irrelevant information for driving, by cropping top 20% of the image before computing the image gradients. </p>
<p>Results</p>
<p>To increase robustness, we perform filtering of training data: removing sub-optimal observation and action pairs (I s,t , a s,t ). Due to this, we were able to achieve robust performance in track completion even when 50% of the commands were randomly changed. The overall runtime of the model was only 2 − 3 ms on a modern CPU.</p>
<p>Our method ranked 1st on the NeurIPS 2021 AWS DeepRacer challenge, outperforming over 250 participants from 30+ teams worldwide.</p>
<p>Discussion</p>
<p>Automatic dataset filtering has been performed to ensure that the training data only contains the optimal observation, action and reward triplet. For this we have utilized the reward signal provided by the simulator and ensured that the subsequent reward only improves from the previous value. This aids in removing the error that creeps in due to human-expert based data collection. We have iteratively arrived at the optimal network architecture and the corresponding input image preprocessing. While Canny edge detecion enables Sim2Real transfer, it also help in overcoming issues like reflection on the marble floor which was causing the network to select the wrong action. To overcome the low-end spec hardware in the DeepRacer car, we used MLP-Mixer architecture (Tolstikhin et al. (2021)) which gave us a runtime of 2 − 3 ms.</p>
<p>Failed ideas</p>
<p>Advantage Actor Critic by Mnih et al. (2016) and DQN by Mnih et al. (2013) methods were experimented with, but were discarded due to low score on the simulator track.</p>
<p>Figure 1 :
1Simulator and Real World Environment: Left We show the image captured from the greyscale camera mounted on the vehicle of the simulator. Right We show the image captured by the grey scale camera mounted on top of the vehicle of the real track. Top-right We show the image of the actual track.</p>
<p>Figure 3 :
3Image gradients on simulator and real world images: Left We show the Canny edge detection for simulator image I s,t Right We show the Canny edge detection for real world image I r,t . For both simulator and real world, we remove the top 20% of the image before computing the image gradients.</p>
<p>Pre-training of deep rl agents for improved learning under domain randomization. A Amiranashvili, M Argus, L Hermann, W Burgard, T Brox, arXiv:2104.14386arXiv preprintA. Amiranashvili, M. Argus, L. Hermann, W. Burgard, and T. Brox. Pre-training of deep rl agents for improved learning under domain randomization. arXiv preprint arXiv:2104.14386, 2021.</p>
<p>End-to-end driving via conditional imitation learning. F Codevilla, M Müller, A López, V Koltun, A Dosovitskiy, 2018 IEEE international conference on robotics and automation (ICRA). IEEEF. Codevilla, M. Müller, A. López, V. Koltun, and A. Dosovitskiy. End-to-end driving via conditional imitation learning. In 2018 IEEE international conference on robotics and automation (ICRA), pages 4693-4700. IEEE, 2018.</p>
<p>An open urban driving simulator. A Dosovitskiy, G Ros, F Codevilla, A Lopez, V Koltun, Carla, Conference on robot learning. PMLRA. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun. Carla: An open urban driving simulator. In Conference on robot learning, pages 1-16. PMLR, 2017.</p>
<p>Scaling simulation-to-real transfer by learning a latent space of robot skills. R C Julian, E Heiden, Z He, H Zhang, S Schaal, J J Lim, G S Sukhatme, K Hausman, The International Journal of Robotics Research. 39R. C. Julian, E. Heiden, Z. He, H. Zhang, S. Schaal, J. J. Lim, G. S. Sukhatme, and K. Hausman. Scaling simulation-to-real transfer by learning a latent space of robot skills. The International Journal of Robotics Research, 39(10-11):1259-1278, 2020.</p>
<p>Playing atari with deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, arxiv:1312.5602Comment: NIPS Deep Learning Workshop. V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Ried- miller. Playing atari with deep reinforcement learning. 2013. URL http://arxiv.org/ abs/1312.5602. cite arxiv:1312.5602Comment: NIPS Deep Learning Workshop 2013.</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T P Lillicrap, T Harley, D Silver, K Kavukcuoglu, 10.48550/ARXIV.1602.01783V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. 2016. doi: 10.48550/ARXIV.1602.01783. URL https://arxiv.org/abs/1602.01783.</p>
<p>M Müller, A Dosovitskiy, B Ghanem, V Koltun, arXiv:1804.09364Driving policy transfer via modularity and abstraction. arXiv preprintM. Müller, A. Dosovitskiy, B. Ghanem, and V. Koltun. Driving policy transfer via modu- larity and abstraction. arXiv preprint arXiv:1804.09364, 2018.</p>
<p>Agile autonomous driving using end-to-end deep imitation learning. Y Pan, C.-A Cheng, K Saigol, K Lee, X Yan, E Theodorou, B Boots, arXiv:1709.07174arXiv preprintY. Pan, C.-A. Cheng, K. Saigol, K. Lee, X. Yan, E. Theodorou, and B. Boots. Ag- ile autonomous driving using end-to-end deep imitation learning. arXiv preprint arXiv:1709.07174, 2017.</p>
<p>Alvinn: An autonomous land vehicle in a neural network. D A Pomerleau, Advances in neural information processing systems. 1D. A. Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Advances in neural information processing systems, 1, 1988.</p>
<p>Mlp-mixer: An all-mlp architecture for vision. I O Tolstikhin, N Houlsby, A Kolesnikov, L Beyer, X Zhai, T Unterthiner, J Yung, A Steiner, D Keysers, J Uszkoreit, Advances in Neural Information Processing Systems. 342021I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. Advances in Neural Information Processing Systems, 34, 2021.</p>
<p>End to end learning based self-driving using jacintonet. P Viswanath, S Nagori, M Mody, M Mathew, P Swami, IEEE 8th International Conference on Consumer Electronics-Berlin (ICCE-Berlin). IEEEP. Viswanath, S. Nagori, M. Mody, M. Mathew, and P. Swami. End to end learning based self-driving using jacintonet. In 2018 IEEE 8th International Conference on Consumer Electronics-Berlin (ICCE-Berlin), pages 1-4. IEEE, 2018.</p>
<p>End-to-end interpretable neural motion planner. W Zeng, W Luo, S Suo, A Sadat, B Yang, S Casas, R Urtasun, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionW. Zeng, W. Luo, S. Suo, A. Sadat, B. Yang, S. Casas, and R. Urtasun. End-to-end interpretable neural motion planner. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8660-8669, 2019.</p>
<p>Query-efficient imitation learning for end-to-end autonomous driving. J Zhang, K Cho, arXiv:1605.06450arXiv preprintJ. Zhang and K. Cho. Query-efficient imitation learning for end-to-end autonomous driving. arXiv preprint arXiv:1605.06450, 2016.</p>
<p>Unpaired image-to-image translation using cycle-consistent adversarial networks. J.-Y Zhu, T Park, P Isola, A A Efros, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionJ.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international confer- ence on computer vision, pages 2223-2232, 2017.</p>            </div>
        </div>

    </div>
</body>
</html>
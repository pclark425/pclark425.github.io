<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7317 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7317</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7317</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-258762545</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.10724v1.pdf" target="_blank">Personalizing Vision-Language Models With Hybrid Prompts for Zero-Shot Anomaly Detection</a></p>
                <p><strong>Paper Abstract:</strong> Zero-shot anomaly detection (ZSAD) aims to develop a foundational model capable of detecting anomalies across arbitrary categories without relying on reference images. However, since “abnormality” is inherently defined in relation to “normality” within specific categories, detecting anomalies without reference images describing the corresponding normal context remains a significant challenge. As an alternative to reference images, this study explores the use of widely available product standards to characterize normal contexts and potential abnormal states. Specifically, this study introduces AnomalyVLM, which leverages generalized pretrained vision-language models (VLMs) to interpret these standards and detect anomalies. Given the current limitations of VLMs in comprehending complex textual information, AnomalyVLM generates hybrid prompts—comprising prompts for abnormal regions, symbolic rules, and region numbers—from the standards to facilitate more effective understanding. These hybrid prompts are incorporated into various stages of the anomaly detection process within the selected VLMs, including an anomaly region generator and an anomaly region refiner. By utilizing hybrid prompts, VLMs are personalized as anomaly detectors for specific categories, offering users flexibility and control in detecting anomalies across novel categories without the need for training data. Experimental results on four public industrial anomaly detection datasets, as well as a practical automotive part inspection task, highlight the superior performance and enhanced generalization capability of AnomalyVLM, especially in texture categories. An online demo of AnomalyVLM is available at https://github.com/caoyunkang/Segment-Any-Anomaly.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7317",
    "paper_id": "paper-258762545",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.003595,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Segment Any Anomaly without Training via Hybrid Prompt Regularization
18 May 2023</p>
<p>Yunkang Cao 
State Key Laboratory of Digital Manufacturing Equipment and Technology
Huazhong University of Science and Technology
China</p>
<p>Equal Contribution</p>
<p>Xiaohao Xu 
State Key Laboratory of Digital Manufacturing Equipment and Technology
Huazhong University of Science and Technology
China</p>
<p>Equal Contribution</p>
<p>Chen Sun sun_chen@hust.edu 
State Key Laboratory of Digital Manufacturing Equipment and Technology
Huazhong University of Science and Technology
China</p>
<p>Yuqi Cheng chengyuqi@hust.edu 
State Key Laboratory of Digital Manufacturing Equipment and Technology
Huazhong University of Science and Technology
China</p>
<p>Zongwei Du duzongwei@hust.edu 
State Key Laboratory of Digital Manufacturing Equipment and Technology
Huazhong University of Science and Technology
China</p>
<p>Liang Gao gaoliang@hust.edu 
State Key Laboratory of Digital Manufacturing Equipment and Technology
Huazhong University of Science and Technology
China</p>
<p>Weiming Shen wshen@ieee.org 
State Key Laboratory of Digital Manufacturing Equipment and Technology
Huazhong University of Science and Technology
China</p>
<p>Segment Any Anomaly without Training via Hybrid Prompt Regularization
18 May 202333F279A833F42D64D59235710F066CF1arXiv:2305.10724v1[cs.CV]
We present a novel framework, i.e., Segment Any Anomaly + (SAA+), for zeroshot anomaly segmentation with hybrid prompt regularization to improve the adaptability of modern foundation models.Existing anomaly segmentation models typically rely on domain-specific fine-tuning, limiting their generalization across countless anomaly patterns.In this work, inspired by the great zero-shot generalization ability of foundation models like Segment Anything, we first explore their assembly to leverage diverse multi-modal prior knowledge for anomaly localization.For non-parameter foundation model adaptation to anomaly segmentation, we further introduce hybrid prompts derived from domain expert knowledge and target image context as regularization.Our proposed SAA+ model achieves stateof-the-art performance on several anomaly segmentation benchmarks, including VisA, MVTec-AD, MTD, and KSDD2, in the zero-shot setting.We will release the code at https://github.com/caoyunkang/Segment-Any-Anomaly.</p>
<p>Introduction</p>
<p>Anomaly segmentation models [1,2,3] have attracted great interest in various domains, e.g,, industrial quality control [4,5] and medical diagnoses [6].The key to reliable anomaly segmentation is to discriminate the distribution of anomaly data from normal data.Specifically, this paper considers zero-shot anomaly segmentation (ZSAS) on images, which is a promising yet unexplored setting where neither normal nor abnormal image is provided for the target category during training.</p>
<p>Due to the scarcity of abnormal samples for training, many works are working towards unsupervised or self-supervised anomaly segmentation, which targets learning a representation of the normal samples during training.Then, the anomalies can be segmented by calculating the discrepancy between the test sample and the learned normal distribution.In specific, these models, including auto-encoder-based reconstruction [7,8,9,10,11,12], one-class classification [13,14,15], and memory-based normal distribution [3,2,16,17,18] methods, typically require training separate models for certain limited categories.However, in real-world scenarios, there are millions of industrial products, and it is not cost-effective to collect a large training set for individual objects, which hinders their deployment in cases when efficient deployments are required, e.g., the initial stage of production.</p>
<p>Recently, foundation models, e.g., SAM [19] and CLIP [20], exhibit great zero-shot visual perception abilities by retrieving prior knowledge stored in these models via prompting [21,22].In this work, we would like to explore how to adapt foundation models to realize anomaly segmentation under the Figure 1: Towards segmenting any anomaly without training, we first construct a vanilla baseline (SAA) by prompting into a cascade of anomaly region generator (e.g., a prompt-guided object detection foundation model [23]) and anomaly region refiner (e.g., a segmentation foundation model [19]) modules via a naive class-agnostic language prompt (e.g., "Anomaly").However, SAA shows the severe false-alarm problem, which falsely detects all the "wick" rather than the ground-truth anomaly region (the "overlong wick").Thus, we further strengthen the regularization with hybrid prompts in the revamped model (SAA+), which successfully helps identify the anomaly region.</p>
<p>zero-shot setting.To this end, as is shown in Fig. 1, we first construct a vanilla baseline, i.e., Segment Any Anomaly (SAA), by cascading prompt-guided object detection [23] and segmentation foundation models [19], which serve as Anomaly Region Generator and Anomaly Region Refiner, respectively.Following the practice to unlock foundation model knowledge [24,25], naive language prompts, e.g., "defect" or "anomaly", are utilized to segment desired anomalies for a target image.In specific, the language prompt is used to prompt the Anomaly Region Generator to generate prompt-conditioned box-level regions for desired anomaly regions.Then these regions are refined in the Anomaly Region Refiner to produce final predictions, i.e., masks, for anomaly segmentation.However, as is shown in Figure 1, vanilla foundation model assembly (SAA) tends to cause significant false alarms, e.g., SAA wrongly refers to all wicks as anomalies whereas only the overlong wick is a real anomaly, which we attribute to the ambiguity brought by naive language prompts.Firstly, conventional language prompts may become ineffective when facing the domain shift between the pretraining data distribution of foundation models and downstream datasets for anomaly segmentation.Secondly, the degree of "anomaly" for a target depends on the object context, which is hard for naive coarse-grained language prompts, e.g., "an anomaly region", to express exactly.Thus, going beyond naive language prompts, we incorporate domain expert knowledge and target image context in our revamped framework, i.e., Segment Any Anomaly + (SAA+), respectively.On the one hand, expert knowledge provides detailed descriptions of anomalies that are relevant to the target in open-world scenarios.We utilize more specific descriptions as in-context prompts, effectively aligning the image content in both pre-trained and target datasets.On the other hand, we utilize the target image context to reliably identify and adaptively calibrate anomaly segmentation predictions [26,27].By leveraging the rich contextual information present in the target image, we can accurately associate the object context with the final anomaly predictions.</p>
<p>Technically, apart from naive class-agnostic prompts, we leverage domain expert knowledge to construct target-oriented anomaly language prompts, i.e., class-specific language expressions.Besides, as language can not accurately retrieve regions with certain object characteristics, such as number, size, and location, precisely [28,29], we introduce object property prompts in the form of thresholding filters.These prompts assist in identifying and removing region candidates that do not satisfy desired properties.Furthermore, to fully exploit the target image context, we suggest utilizing image saliency and region confidence ranking as prompts, which model the anomaly degree of a region by considering the similarities, e.g., euclidean distance, between it and other regions within the image.Finally, we conduct thorough experiments to confirm the efficacy of our hybrid prompts in adapting foundation models to zero-shot anomaly segmentation.Specifically, our final model (SAA+) attains new stateof-the-art performance on various anomaly segmentation datasets under the zero-shot setting.To summarize, our main contributions are:</p>
<p>• We propose the SAA framework for anomaly segmentation, allowing the collaborative assembly of diverse foundation models without the need for training.</p>
<p>• We introduce hybrid prompts as a regularization technique, leveraging domain expert knowledge and target image context to adapt foundation models for anomaly segmentation.This leads to the development of SAA+, an enhanced version of our framework.</p>
<p>• Our method achieves state-of-the-art performance in zero-shot anomaly segmentation on several benchmark datasets, including VisA, MVTec-AD, KSDD2, and MTD.Notably, SAA/SAA+ demonstrates remarkable capability in detecting texture-related anomalies without requiring any annotation.</p>
<p>Related work</p>
<p>Anomaly Segmentation.Due to the limited availability and high cost of abnormal images in industrial settings, much of the current research on anomaly segmentation focuses on unsupervised methods that rely solely on normal images.Reconstruction-based approaches, such as those proposed in [7,8,9,10,11,12], score anomalies with train an encoder-decoder model to reconstruct images for segmentation purposes.By comparing the input image with the reconstructed version, these methods can predict the location of anomalies.Feature embedding-based methods, on the other hand, typically employ teacher-student architecture [30,31,32,33,34,35,36, 1], one-class classification technology [13,14,15], or memory-based normal distribution [3,2,16] to segment anomalies by identifying differences in feature distribution between normal and abnormal images.</p>
<p>Recently, researchers have begun to explore the potential of ZSAS [37,38,39,40], which eliminates the need for either normal or abnormal images during the training process.Among them, WinClip [25] pioneers the potential of foundation models, e.g., visual-language models, for the ZSAL task.Unlike WinClip [25] that segments anomalies through text-visual similarity, we propose to generate proposals and score their anomaly degree, achieving much better segmentation performance.</p>
<p>Foundation Model.Foundation models show an impressive ability to solve diverse vision tasks in a zero-shot manner.Specifically, these models can learn a strong representation by training on largescale datasets [41].While early work [20,42] focus on developing robust image-wise recognition capacity, recent work [43,44,45,46,47,23] introduce foundation models or their applications for dense visual tasks.For instance, Grounding DINO [23] achieves encouraging open-set object detection ability using arbitrary texts as queries.Recently, SAM [19] demonstrates a powerful ability to extract high-quality object segmentation masks in the open world.Impressed by the success of these foundation models, we would like to explore how to adapt these off-the-shelf models to detect anomalies without any training on the downstream datasets for anomaly segmentation.</p>
<p>Prompt Engineering.Prompt engineering is a widely employed technique that involves adapting foundation models for downstream tasks.Generally, this approach involves appending a set of learnable tokens to the input.Prior studies have investigated prompting with text inputs [48], vision inputs [49,50,51], and both text and visual inputs [52,53,54].Despite their effectiveness in adapting foundation models to various downstream tasks, prompting methods cannot be employed in ZSAS because they require training data, which is not available in ZSAS.In contrast, some methods employ heuristic prompts [55] that do not require any training, making them more feasible for tasks without any data.In this paper, we propose using hybrid prompts derived from domain expert knowledge and target image context for ZSAS.</p>
<p>3 SAA: Vanilla Foundation Model Assembly for ZSAS</p>
<p>Problem Definition: Zero-shot Anomaly Segmentation (ZSAS)</p>
<p>The goal of ZSAS is to perform anomaly segmentation on new objects without requiring any corresponding object training data.ZSAS seeks to create an anomaly map A ∈ [0, 1] h×w×1 based on an empty training set ∅, in order to identify the anomaly degree for individual pixels in an image I ∈ R h×w×3 that includes novel objects.The ZSAS task has the potential to significantly reduce the need for training data and lower the costs associated with real-world inspection deployments.</p>
<p>Figure 2: Overview of the proposed Segment Any Anomaly + (SAA+) framework.We adapt foundation models to zero-shot anomaly segmentation via hybrid prompt regularization.In specific, apart from naive class-agnostic language prompts, the regularization comes from both domain expert knowledge, including more detailed class-specific language and object property prompts, and target image context, including visual saliency and confidence ranking-related prompts.</p>
<p>Baseline Model Assembly: Segment Any Anomaly (SAA)</p>
<p>For ZSAS, we start by constructing a vanilla foundation model assembly, i.e., Segment Any Anomaly (SAA), as shown in Fig. 1.In specific, given a certain query image for anomaly segmentation, we first use languages as the initial prompt to roughly retrieve coarse anomaly region proposals via an Anomaly Region Generator implemented with a language-driven visual grounding foundation model, i.e., GroundingDINO [23].Afterward, anomaly region proposals are refined into pixel-wise highquality segmentation masks with the Anomaly Region Refiner in which a prompt-driven segmentation foundation model, i.e., SAM [19], is used.</p>
<p>Anomaly Region Generator</p>
<p>With recent booming development on language-vision models, some foundation models [24,23,46] gradually acquire the ability to retrieve objects in images through language prompts.Given language prompts T that describe desired regions to be detected, e.g., "anomaly", foundation models can generate desired regions for a query image I.There we base the architecture of the region detector on a text-guided open-set object detection architecture for visual grounding.Specifically, we take a GroundingDINO [23] architecture that has been pre-trained on large-scale language-vision datasets [41].Such a network first extracts the features of the language prompt and the query image via text encoder and visual encoder, respectively.Then the rough object regions are generated in the form of bounding boxes with a cross-modality decoder.Given the bounding-box-level region set R B , and their corresponding confidence score set S, the module of anomaly region generator (Generator) can be formulated as, R B , S := Generator(I, T )</p>
<p>Anomaly Region Refiner</p>
<p>To generate pixel-wise anomaly segmentation results, we propose Anomaly Region Refiner to refine the bounding-box-level anomaly region candidates into an anomaly segmentation mask set.To this end, we use a sophisticated foundation model for open-world visual segmentation, i.e., SAM [19].This model mainly includes a ViT-based [56] backbone and a prompt-conditioned mask decoder.In specific, the model is trained on a large-scale image segmentation dataset [19] with one billion finegrained masks, which enables high-quality mask generation abilities under an open-set segmentation setting.The prompt-conditioned mask decoder accepts various types of prompts as input.We regard the bounding box candidates R B as prompts and obtain pixel-wise segmentation masks R. The module of the Anomaly Region Refiner (Refiner) can be formulated as follows,
R := Refiner(I, R B )(2)
Till then, we obtain the set of regions in the form of high-quality segmentation masks R with corresponding confidence scores S. To sum up we summarize framework (SAA) as follows, R, S := SAA(I, T n )</p>
<p>where T n is a naive class-agnostic language prompt, e.g., "anomaly", utilized in SAA.</p>
<p>Analysis on the ZSAS Performance of Vanilla Foundation Model Assembly</p>
<p>We present some preliminary experiments to evaluate the efficacy of vanilla foundation model assembly for ZSAS.Despite the simplicity and intuitiveness of the solution, we observe a language ambiguity issue.Specifically, certain language prompts, such as "anomaly", may fail to detect the desired anomaly regions.For instance, as depicted in Fig. 1, all "wick" is erroneously identified as an anomaly by the SAA with the "anomaly" prompt.</p>
<p>We attribute this language ambiguity to the domain gap between the pretraining language-vision datasets and the targeted ZSAS datasets, which means that some language prompts may have different meanings and be associated with different image contents in distinct datasets.In addition, there is hardly any adjective expression like "anomaly" in those large-scale datasets, thus making this kind of prompt design poor at understanding what is an anomaly region.Additionally, the exact "anomaly" is object-specific and would vary across objects.For example, it denotes the scratches on leather or the crack on hazelnut.The language ambiguity issue leads to severe false alarms in ZSAS datasets.</p>
<p>We propose introducing hybrid prompts generated by domain expert knowledge and the target image context to reduce language ambiguity, thereby achieving better ZSAS performance.</p>
<p>SAA+: Foundation Model Adaption via Hybrid Prompt Regularization</p>
<p>To address language ambiguity in SAA and improve its ability on ZSAS, we propose an upgraded version called SAA+ that incorporates hybrid prompts, as Fig. 2. In addition to leveraging the knowledge gained from pre-trained foundation models, SAA+ utilizes both domain expert knowledge and target image context to generate more accurate anomaly region masks.We provide further details on these hybrid prompts below.</p>
<p>Prompt Generated from Domain Expert Knowledge</p>
<p>Following the trend of prompt learning [48,54], we initialize the prompt, which unlocks the knowledge of foundation models, in the form of language.However, the language ambiguity issue caused by the domain gap is particularly severe when using only the naive language prompt "anomaly".To address this problem, we leverage domain expert knowledge that contains useful prior information about the target anomaly regions.Specifically, although experts may not provide a comprehensive list of potential open-world anomalies for a new product, they can identify some candidates based on their past experiences with similar products.Domain expert knowledge enables us to refine the naive "anomaly" prompt into more specific prompts that describe the anomaly state in greater detail.In addition to language prompts, we introduce property prompts to complement the lack of awareness on specific properties like "count" and "area" [28] in existing foundation models [28].</p>
<p>Anomaly Language Expression as Prompt</p>
<p>To describe potential open-world anomalies, we propose designing more precise language prompts.These prompts are categorized into two types: class-agnostic and class-specific prompts.</p>
<p>Class-agnostic prompts (T a ) are general prompts that describe anomalies that are not specific to any particular category, e.g., "anomaly" and "defect".Despite the domain gap between the pre-trained datasets and the targeted ZSAS datasets, our empirical analysis (5.3) shows that these generic prompts provide encouraging initial performance.</p>
<p>Class-specific prompts (T s ) are designed based on expert knowledge of abnormal patterns with similar products to supplement more specific anomaly details.We use prompts already employed in the pre-trained visual-linguistic dataset, e.g., "black hole" and "white bubble", to query the desired regions.This approach reformulates the task of finding an anomaly region into locating objects with a specific anomaly state expression, which is more straightforward to utilize foundation models than identifying "anomaly" within an object context.</p>
<p>By prompting SAA with anomaly language prompts P L = {T a , T s } derived from domain expert knowledge, we generate finer anomaly region candidates R and corresponding confidence scores S.</p>
<p>Anomaly Object Property as Prompt</p>
<p>Current foundation models [23,57] have limitations when it comes to querying objects with specific property descriptions, such as size or location, which are important for describing anomalies, such as "The small black hole on the left of the cable."To incorporate this critical expert knowledge, we propose using anomaly property prompts formulated as rules rather than language.Specifically, we consider the location and area of anomalies.</p>
<p>Anomaly Location.Accurate localization of anomalies plays a critical role in distinguishing true anomalies from false positives.Typically, anomalies are expected to be located within the objects of interest during inference.However, due to the influence of background context, anomalies may occasionally appear outside the inspected objects.To tackle this challenge, we leverage the openworld detection capability of foundation models to determine the location of the inspected object.Subsequently, we calculate the intersection over union (IoU) between the potential anomaly regions and the inspected object.By applying an expert-derived IoU threshold, denoted as θ IoU , we filter out anomaly candidates with IoU values below this threshold.This process ensures that the retained anomaly candidates are more likely to represent true anomalies located within the inspected object.</p>
<p>Anomaly Area.The size of an anomaly, as reflected by its area, is also a property that can provide useful information.In general, anomalies should be smaller than the size of the inspected object.Experts can provide a suitable threshold value θ area for the specific type of anomaly being considered.</p>
<p>Candidates with areas unmatched with θ area • ObjectArea can then be filtered out.</p>
<p>By combining the two property prompts P P = {θ area , θ IoU }, we can filter the set of candidate regions R to obtain a subset of selected candidates R P with corresponding confidence scores S P using the filter function (Filter), R P , S P := Filter(R, P P ) (4)</p>
<p>Prompts Derived from Target Image Context</p>
<p>Besides incorporating domain expert knowledge, we can leverage the information provided by the input image itself to improve the accuracy of anomaly region detection.In this regard, we propose two prompts induced by the image context.</p>
<p>Anomaly Saliency as Prompt</p>
<p>Predictions generated by foundation models like [23] using the prompt "defect" can be unreliable due to the domain gap between pre-trained language-vision datasets [41] and targeted anomaly segmentation datasets [4,58].To calibrate the confidence scores of individual predictions, we propose Anomaly Saliency Prompt mimicking human intuition.In specific, humans can recognize anomaly regions by their discrepancy with their surrounding regions [40], i.e., visual saliency contains valuable information indicating the anomaly degree.Hence, we calculate a saliency map (s) for the input image by computing the average distances between the corresponding pixel feature (f ) and its N nearest neighbors,
s ij := 1 N f ∈Np(fij ) (1 − f ij , f )(5)
where (i, j) denotes to the pixel location, N p (f ij ) denotes to the N nearest neighbors of the corresponding pixel, and •, • refers to the cosine similarity.We use pre-trained CNNs from large-scale image datasets [59] to extract image features, ensuring the descriptiveness of features.The saliency map indicates how different a region is from other regions.The saliency prompts P S are defined as the exponential average saliency value within the corresponding region masks,
P S := exp( ij r ij s ij ij r ij ) | r ∈ R P(6)
The saliency prompts provide reliable indications of the confidence of anomaly regions.These prompts are employed to recalibrate the confidence scores generated by the foundation models, yielding new rescaled scores S S based on the anomaly saliency prompts P S .These rescaled scores provide a combined measure that takes into account both the confidence derived from the foundation models and the saliency of the region candidate.The process is formulated as follows,
S S := p • s | p ∈ P S , s ∈ S P(7)</p>
<p>Anomaly Confidence as Prompt</p>
<p>Typically, the number of anomaly regions in an inspected object is limited.Therefore, we propose anomaly confidence prompts P C to identify the K candidates with the highest confidence scores based on the image content and use their average values for final anomaly region detection.This is achieved by selecting the top K candidate regions based on their corresponding confidence scores, as shown in the following,
R C , S C := Top K (R P , S S )(8)
Denote a single region and its corresponding score as r C and s C , we then use these K candidate regions to estimate the final anomaly map,
A ij := r C ∈R C r C ij • s C r C ∈R C r C ij(9)
With the proposed hybrid prompts (P L , P P , P S , and P C ), SAA is regularized in our final framework, i.e., Segment Any Anomaly + (SAA+), which makes more reliable anomaly predictions.</p>
<p>Experiments</p>
<p>In this section, we first assess the performance of SAA/SAA+ on several anomaly segmentation benchmarks.Then, we extensively study the effectiveness of individual hybrid prompts.</p>
<p>Experimental Setup</p>
<p>Datasets.We leverage four datasets with pixel-level annotations.: VisA [58], MVTec-AD [4], KSDD2 [60], and MTD [61].VisA and MVTec-AD comprise a variety of object subsets, e.g., circuit boards, while KSDD2 and MTD are comprised of texture anomalies.In summary, we categorize the subsets of all of these datasets into texture which typically exhibit similar patterns within a single image (e.g., carpets), and object which includes more diverse distribution (e.g., candles).</p>
<p>Evaluation Metrics.ZSAS performance is evaluated based on two metrics: (I) max-F1-pixel (F p ) [25], which measures the F1-score for pixel-wise segmentation at the optimal threshold; (II) max-F1region (F r ), which is proposed in this paper to mitigate the bias towards large defects observed with max-F1-pixel [4].Specifically, we compute the F1-score for region-wise segmentation at the optimal threshold, considering a prediction positive if the overlapping value exceeds 0.6.</p>
<p>Implementation Details.We adopt the official implementations of GroundingDINO1 and Segment Anything Model2 to construct the vanilla baseline (SAA).Details about the prompts derived from domain expert knowledge are explained in the supplementary material.For the saliency prompts induced from image content, we utilize the WideResNet50 [62] network, pre-trained on ImageNet [59], and set N = 400 in line with prior studies [40].For anomaly confidence prompts, we set the hyperparameter K as 5 by default.Input images are fixed at a resolution of 400 × 400 for evaluation.Figure 3: Qualitative comparisons on zero-shot anomaly segmentation for ClipSeg [24], UTAD [40], SAA, and SAA+ on four datasets, i.e., VisA [58], MVTec-AD [4], KSDD2 [60], and MTD [61]</p>
<p>Main Results</p>
<p>Methods for Comparison.We compare our final model, i.e., Segment Any Anomaly + (SAA+) with several concurrent state-of-the-art methods, including WinClip [25], UTAD [40], ClipSeg [24], and our vanilla baseline (SAA).For WinClip, we report its official results on VisA and MVTec-AD.</p>
<p>For the other three methods, we use official implementations and adapt them to the ZSAS task.</p>
<p>Notably, as all methods require no training process, their performance is stable with a variance of ±0.00.</p>
<p>Quantitative Results: As is shown in Table 1, SAA+ method outperforms other methods in both F p and F r by a significant margin.Although WinClip [25], ClipSeg [24], and SAA also use foundation models, SAA+ better unleash the capacity of foundation models and adapts them to tackle ZSAS.The remarkable performance of SAA+ meets the expectation to segment any anomaly without training.</p>
<p>Qualitative Results: Fig. 3 presents qualitative comparisons between SAA+ and previous competitive methods, where SAA+ achieves better performance.Moreover, the visualization shows SAA+ is capable of detecting texture anomalies, e.g.small scratches on the leather.</p>
<p>Ablation study</p>
<p>In Table 2, we perform component-wise analysis to ablate specific prompt designs in our framework.</p>
<p>Language prompt (P L ).Table 2 verifies the effectiveness of language prompts derived from domain expert knowledge (+3.90% in F p and +4.90% in F r ).Then, we dig into the efficacy of T a and T s , which clearly indicate that both the general description and the specifically designed description for anomalies can achieve reasonable performance.Moreover, their combination can make a synergy, enhancing anomaly segmentation performance.The improvement of P L helps unlock language-driven region detection capacity of current foundation models [23,19].</p>
<p>Property prompt (P P ).Apart from the improvement in the overall performance, property prompts bring dramatic improvements (from 21.83% to 53.79% in F p ) on texture categories, thanks to the filtering mechanism which filters out a significant number of falsely detected anomaly region candidates via high-level characteristics, e.g., location and area of the target image.</p>
<p>Saliency prompt (P S ).Table 2 provides clear evidence of the efficacy of P S on anomaly segmentation.This is because region saliencies can accurately describe the degree of deviation of a region from its surroundings.In Fig. 4, we showcase the qualitative impact of P S on anomaly segmentation, which illustrates visual saliency maps can help highlight abnormal regions, i.e., which shows higher saliency values compared to other regions.By incorporating P S to calibrate the confidence scores, more precise segmentation results can be achieved.For example, the use of P S enables the effective localization of the cracked region of hazelnut and the overlong wick on candles.</p>
<p>Confidence prompt (P C ).With the incorporation of anomaly confidence prompts, we limit the number of anomaly regions, which effectively reduces false positives, leading to 0.72% F p average improvements across all categories, as shown in Table 2.The influence of the hyperparameter K in P C is illustrated in Fig. 5.The figure shows that performance initially increases as K improves, as more anomaly regions are accurately detected.However, when K exceeds a certain threshold (around K = 5), the performance drops slightly as more regions are wrongly identified as abnormal.</p>
<p>The best results are obtained at around K = 5, with an average F p of 34.85% across all categories.</p>
<p>Conclusion</p>
<p>In this work, we explore how to segment any anomaly without any further training by unleashing the full power of modern foundation models.We owe the struggle of adapting foundation model assembly to anomaly segmentation to the prompt design, which is the key to controlling the function of off-theshelf foundation models.Thus, we propose a novel framework, i.e., Segment Any Anomaly +, to leverage hybrid prompts derived from both expert knowledge and target image context to regularize foundation models free of training.Finally, we successfully adapt multiple foundation models to tackle zero-shot anomaly segmentation, achieving new SoTA results on several benchmarks.We hope our work can shed light on the design of label-free model adaptation for anomaly segmentation.</p>
<p>Limitations.Due to the computation restriction, we currently do not test our method on more largescale foundation models.We have finished the exploration of our methodology with representative foundation models, and we will explore the scaling effect of the models in the future.</p>
<p>Figure 4 :
4
Figure 4: Effects of disabling (w/o) and abling (w/) prompts (P S ) of saliency maps (s) on the final anomaly segmentation.</p>
<p>Figure 5 :
5
Figure 5: Sensitivity analysis of hyperparameter K of confidence prompts (P C ).</p>
<p>Table 1 :
1
Qualitative comparisons between SAA+ and other concurrent methods on zero-shot anomaly segmentation.Best scores are highlighted in bold.The second best scores are also underlined.
MetricMethodPer DatasetPer Defect TypeTotalVisAMVTec-ADKSDD2MTDTextureObjectWinClip [25]14.8231.65---20.93-ClipSeg [24]14.3225.4234.279.3927.7518.3020.58F pUTAD [40]6.9523.4822.5311.3729.1312.0716.19SAA12.7623.448.7914.7820.9417.3518.22SAA+27.0739.4059.1935.4053.7928.8234.85ClipSeg [24]5.6519.689.056.5521.3710.4113.06UTAD [40]5.3217.533.562.9516.389.9411.49F rSAA4.8332.4916.4010.6340.3113.1919.74SAA+14.4649.6739.3430.2760.4025.7034.07</p>
<p>Table 2 :
2
Ablation study on the proposed hybrid prompts, including language prompt (P L ), object property prompt (P P ), saliency prompt (P S ), and confidence prompt (P C ).The best scores are highlighted in bold.
Metric Model Variants Texture Object Totalw/o T a &amp; T s 50.30 24.79 30.95w/o P Lw/o T a51.15 25.88 31.80w/o T s53.51 26.55 33.06w/o P P21.83 21.40 21.50F pw/o P S50.58 24.72 30.96w/o P C50.41 27.99 34.13full model (SAA+) 53.79 28.82 34.85F
r w/o P L w/o T a &amp; T s 50.58 22.36 29.17 w/o T a 55.26 20.28 28.72 w/o T s 54.21 23.13 30.64 w/o P P 33.94 20.99 24.11 w/o P S 57.66 24.36 32.39 w/o P C 53.65 25.18 32.05 full model (SAA+) 60.40 25.70 34.07</p>
<p>https://github.com/IDEA-Research/GroundingDINO
https://github.com/facebookresearch/segment-anything</p>
<p>Collaborative discrepancy optimization for reliable image anomaly localization. Yunkang Cao, Xiaohao Xu, Zhaoge Liu, Weiming Shen, IEEE Transactions on Industrial Informatics. 2023</p>
<p>Industrial image anomaly localization based on gaussian clustering of pretrained feature. Qian Wan, Liang Gao, Xinyu Li, Long Wen, IEEE Transactions on Industrial Electronics. 6962021</p>
<p>Towards total recall in industrial anomaly detection. Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Schölkopf, Thomas Brox, Peter Gehler, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>MVTec AD -A comprehensive real-world dataset for unsupervised anomaly detection. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition. the IEEE/CVF conference on Computer Vision and Pattern Recognition2019</p>
<p>Uninformed students: Studentteacher anomaly detection with discriminative latent embeddings. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>Autoencoders for unsupervised anomaly segmentation in brain mr images: a comparative study. Christoph Baur, Stefan Denner, Benedikt Wiestler, Nassir Navab, Shadi Albarqouni, Medical Image Analysis. 691019522021</p>
<p>Encoding structure-texture relation with p-net for anomaly detection in retinal images. Kang Zhou, Yuting Xiao, Jianlong Yang, Jun Cheng, Wen Liu, Weixin Luo, Zaiwang Gu, Jiang Liu, Shenghua Gao, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020Proceedings, Part XX 16</p>
<p>Divide-and-assemble: Learning block-wise memory for unsupervised anomaly detection. Jinlei Hou, Yingying Zhang, Qiaoyong Zhong, Di Xie, Shiliang Pu, Hong Zhou, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>DRAEM -A discriminatively trained reconstruction embedding for surface anomaly detection. Vitjan Zavrtanik, Matej Kristan, Danijel Skočaj, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Deep generative model using unregularized score for anomaly detection with heterogeneous complexity. Takashi Matsubara, Kazuki Sato, Kenta Hama, Ryosuke Tachibana, Kuniaki Uehara, IEEE Transactions on Cybernetics. 5262020</p>
<p>Learning semantic context from normal samples for unsupervised anomaly detection. Xudong Yan, Huaidong Zhang, Xuemiao Xu, Xiaowei Hu, Pheng-Ann Heng, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>Masked swin transformer unet for industrial anomaly detection. Jielin Jiang, Jiale Zhu, Muhammad Bilal, Yan Cui, Neeraj Kumar, Ruihan Dou, Feng Su, Xiaolong Xu, IEEE Transactions on Industrial Informatics. 1922022</p>
<p>Patch SVDD: Patch-level SVDD for anomaly detection and segmentation. Jihun Yi, Sungroh Yoon, Proceedings of the Asian Conference on Computer Vision. the Asian Conference on Computer Vision2020</p>
<p>Mocca: Multilayer one-class classification for anomaly detection. Fabio Valerio Massoli, Fabrizio Falchi, Alperen Kantarci, Şeymanur Akti, Hazim Kemal Ekenel, Giuseppe Amato, IEEE Transactions on Neural Networks and Learning Systems. 3362021</p>
<p>Learning and evaluating representations for deep one-class classification. Kihyuk Sohn, Chun-Liang Li, Jinsung Yoon, Minho Jin, Tomas Pfister, International Conference on Learning Representations. 2020</p>
<p>Complementary pseudo multimodal feature for point cloud anomaly detection. Yunkang Cao, Xiaohao Xu, Weiming Shen, arXiv:2303.131942023arXiv preprint</p>
<p>Multimodal industrial anomaly detection via hybrid fusion. Yue Wang, Jinlong Peng, Jiangning Zhang, Ran Yi, Yabiao Wang, Chengjie Wang, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2023</p>
<p>SoftPatch: Unsupervised anomaly detection with noisy data. Xi Jiang, Jianlin Liu, Jinbao Wang, Qiang Nie, Kai Wu, Yong Liu, Chengjie Wang, Feng Zheng, Advances in neural information processing systems. 2022</p>
<p>. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, arXiv:2304.026432023Segment anything. arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International Conference on Machine Learning. PMLR2021</p>
<p>Align and prompt: Video-and-language pre-training with entity prompts. Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, Steven Ch Hoi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>On the opportunities and risks of foundation models. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney Von Arx, Jeannette Michael S Bernstein, Antoine Bohg, Emma Bosselut, Brunskill, arXiv:2108.072582021arXiv preprint</p>
<p>Grounding dino: Marrying dino with grounded pre-training for open-set object detection. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, arXiv:2303.054992023arXiv preprint</p>
<p>Image segmentation using text and image prompts. Timo Lüddecke, Alexander Ecker, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, Onkar Dabeer, arXiv:2303.14814Winclip: Zero-/few-shot anomaly classification and segmentation. 2023arXiv preprint</p>
<p>Towards robust video object segmentation with adaptive object calibration. Xiaohao Xu, Jinglu Wang, Xiang Ming, Yan Lu, Proceedings of the 30th ACM International Conference on Multimedia. the 30th ACM International Conference on Multimedia2022</p>
<p>Reliable propagation-correction modulation for video object segmentation. Xiaohao Xu, Jinglu Wang, Xiao Li, Yan Lu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2022</p>
<p>Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, Tali Dekel, arXiv:2302.12066Teaching clip to count to ten. 2023arXiv preprint</p>
<p>Rˆ2vos: Robust referring video object segmentation via relational multimodal cycle consistency. Xiang Li, Jinglu Wang, Xiaohao Xu, Xiao Li, Yan Lu, Bhiksha Raj, arXiv:2207.012032022arXiv preprint</p>
<p>Multiresolution knowledge distillation for anomaly detection. Mohammadreza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad H Rohban, Hamid R Rabiee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>Student-teacher feature pyramid matching for anomaly detection. Guodong Wang, Shumin Han, Errui Ding, Di Huang, arXiv:2103.042572021arXiv preprint</p>
<p>Anomaly detection via reverse distillation from one-class embedding. Hanqiu Deng, Xingyu Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Informative knowledge distillation for image anomaly segmentation. Knowledge-Based Systems. Yunkang Cao, Qian Wan, Weiming Shen, Liang Gao, 2022248108846</p>
<p>Semisupervised knowledge distillation for tiny defect detection. Yunkang Cao, Yanan Song, Xiaohao Xu, Shuya Li, Yuhao Yu, Yiheng Zhang, Weiming Shen, 2022 IEEE 25th International Conference on Computer Supported Cooperative Work in Design (CSCWD). 2022</p>
<p>Unsupervised image anomaly detection and segmentation based on pre-trained feature mapping. Qian Wan, Liang Gao, Xinyu Li, Long Wen, IEEE Transactions on Industrial Informatics. 2022</p>
<p>Position encoding enhanced feature mapping for image anomaly detection. Qian Wan, Yunkang Cao, Liang Gao, Weiming Shen, Xinyu Li, 2022 IEEE 18th International Conference on Automation Science and Engineering (CASE). IEEE2022</p>
<p>Zero-shot learning and classification of steel surface defects. M Amr, László Nagy, Czúni, Fourteenth International Conference on Machine Vision (ICMV 2021). SPIE202212084</p>
<p>Zero-shot anomalous object detection using unsupervised metric learning. Jiahui Liu, Xiaojuan Qi, Songzhi Su, Tony Prescott, Li Sun, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2021) Proceedings. Sheffield. 2021</p>
<p>Anomaly detection based on zero-shot outlier synthesis and hierarchical feature distillation. Adín Ramírez Rivera, Adil Khan, Imad Eddine Ibrahim Bekkouch, and Taimoor Shakeel Sheikh. 202033</p>
<p>Zero-shot versus many-shot: Unsupervised texture anomaly detection. Toshimichi Aota, Lloyd Teh Tzer, Takayuki Tong, Okatani, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2023</p>
<p>Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. Christoph Schuhmann, Robert Kaczmarczyk, Aran Komatsuzaki, Aarush Katta, Richard Vencu, Romain Beaumont, Jenia Jitsev, Theo Coombes, Clayton Mullis, NeurIPS Workshop Datacentric AI. Jülich Supercomputing Center. 2021</p>
<p>Align before fuse: Vision and language representation learning with momentum distillation. Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, Steven Chu, Hong Hoi, Advances in neural information processing systems. 202134</p>
<p>Unified-IO: A unified model for vision, language, and multi-modal tasks. Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, Aniruddha Kembhavi, arXiv:2206.089162022arXiv preprint</p>
<p>Unifying architectures, tasks, and modalities through a simple sequence-tosequence learning framework. Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang, arXiv:2202.030522022arXiv preprint</p>
<p>Denseclip: Language-guided dense prediction with context-aware prompting. Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, Jiwen Lu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>Regionclip: Region-based language-image pretraining. Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Extract free dense labels from clip. Chong Zhou, Chen Change Loy, Bo Dai, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringerOctober 23-27, 2022. 2022Proceedings, Part XXVIII</p>
<p>Conditional prompt learning for visionlanguage models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022</p>
<p>Prompting visual-language models for efficient video understanding. Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, Weidi Xie, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringerOctober 23-27, 2022. 2022Proceedings, Part XXXV</p>
<p>Visual prompt tuning. Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, Ser-Nam Lim, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringerOctober 23-27, 2022. 2022Proceedings, Part XXXIII</p>
<p>Exploring visual prompts for adapting large-scale models. Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, Phillip Isola, arXiv:2203.17274202214arXiv preprint</p>
<p>Unified vision and language prompt learning. Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, Chen Change Loy, arXiv:2210.072252022arXiv preprint</p>
<p>Multitask vision-language prompt tuning. Sheng Shen, Shijia Yang, Tianjun Zhang, Bohan Zhai, Joseph E Gonzalez, Kurt Keutzer, Trevor Darrell, arXiv:2211.117202022arXiv preprint</p>
<p>Learning to prompt for vision-language models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, Int J Comput Vis. 13092022</p>
<p>What does clip know about a red circle? visual prompt engineering for vlms. Aleksandar Shtedritski, Christian Rupprecht, Andrea Vedaldi, arXiv:2304.067122023arXiv preprint</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, International Conference on Learning Representations. 2021</p>
<p>Grounded language-image pre-training. Liunian Harold, Li , Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>SPot-the-Difference self-supervised pre-training for anomaly detection and segmentation. Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, Onkar Dabeer, Proceedings of the European Conference on Computer Vision. the European Conference on Computer Vision2022</p>
<p>ImageNet classification with deep convolutional neural networks. Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, Advances in Neural Information Processing Systems. 2512012</p>
<p>Mixed supervision for surface-defect detection: From weakly to fully supervised learning. Jakob Božič, Domen Tabernik, Danijel Skočaj, Computers in Industry. 1291034592021</p>
<p>Surface defect saliency of magnetic tile. Yibin Huang, Congying Qiu, Yue Guo, Xiaonan Wang, Kui Yuan, 2018 IEEE 14th International Conference on Automation Science and Engineering (CASE). 2018</p>
<p>Wide residual networks. Sergey Zagoruyko, Nikos Komodakis, Proceedings of the British Machine Vision Conference (BMVC). R Edwin, C Hancock Richard, William A P Wilson, Smith, the British Machine Vision Conference (BMVC)BMVA PressSeptember 201612</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6649 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6649</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6649</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-40c0d1f38ab081e21cc3b1e2e5334a9b54b6ff08</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/40c0d1f38ab081e21cc3b1e2e5334a9b54b6ff08" target="_blank">The Impact of Reasoning Step Length on Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> The results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets, and shows that even incorrect rationales can yield favorable outcomes if they maintain the requisite length of inference.</p>
                <p><strong>Paper Abstract:</strong> Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make better use of LLMs' potential in complex problem-solving scenarios. Second, we also investigated the relationship between the performance of CoT and the rationales used in demonstrations. Surprisingly, the result shows that even incorrect rationales can yield favorable outcomes if they maintain the requisite length of inference. Third, we observed that the advantages of increasing reasoning steps are task-dependent: simpler tasks require fewer steps, whereas complex tasks gain significantly from longer inference sequences. The code is available at https://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6649.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6649.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo-1106</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo-1106 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only large language model from OpenAI used in the paper's main experiments; evaluated on multiple arithmetic word‑problem benchmarks with Chain‑of‑Thought prompting variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo-1106</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MultiArith, SingleEq, AQuA, SAVMP (arithmetic/word problem datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>grade-school multi-step arithmetic word problems, algebraic word problems (multi-step addition/subtraction/multiplication/division and equation solving)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language math word problems (some multiple-choice in AQuA), free-text answer</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school / middle-school multi-step problems (GSM8K-level), MultiArith multi-step arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-Shot-CoT (modified: 'you must think more steps'), Few-Shot-CoT (Manual-CoT, Auto-CoT), expanded CoT rationales with added reasoning steps, compressed CoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (exact-match / correct final answer)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported in paper (Table 5) for arithmetic datasets using this experimental setup: MultiArith: Zero-Shot-CoT 91.5% → Auto-CoT 94.0% → Add Reasoning Step (Manual-CoT) 97.0% / (Auto-CoT) 97.2%; GSM8K: Zero-Shot-CoT 64.1% → Auto-CoT 65.8% → Add Reasoning Step (Manual-CoT) 70.1% / (Auto-CoT) 78.8% (values reported in paper Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic internal analysis of numerical processing was performed; authors explicitly state they did not analyze internal representations and plan future activation/visualization work to compare long vs short reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Observed tolerant behavior for arithmetic: single incorrect intermediate numeric values in demonstrations often had minimal effect on final arithmetic accuracy (models learned the CoT pattern more than specific intermediate computations). Compressing reasoning steps in few-shot examples caused large performance drops (regressed toward zero-shot). Other failure modes (noted generally) include fragmentation of reasoning when prompts contained incorrect logical steps (more damaging for logic/symbolic tasks than arithmetic).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>For GPT-3.5, adding up to ~6 extra reasoning steps produced roughly linear accuracy gains on studied datasets (paper reports increasing accuracy with step count up to a limit); GPT-3.5 showed a substantial boost from the 'add steps' intervention (not as large as the boost observed for weaker models like text-davinci-002 but notable).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Impact of Reasoning Step Length on Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6649.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6649.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's larger LLM used as a comparator in the paper; authors evaluated how tolerance to added or compressed Chain‑of‑Thought steps varies with model size and report higher robustness for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MultiArith, other arithmetic/commonsense/symbolic datasets used in paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems and other reasoning tasks used for comparison</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language math word problems (free-text answers)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school to more complex multi-step problems (datasets vary)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-Shot-CoT (including modified zero-shot prompt 'you must think more steps'), Few-Shot-CoT (Auto-CoT / Manual-CoT) with added reasoning steps</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (as used in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Numerical per-method accuracies by model are not separately tabulated in the paper; authors report qualitatively that GPT-4 exhibited the highest tolerance to added reasoning steps and experienced no observed performance decreases when step length was increased.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No internal mechanistic analysis provided for GPT-4; authors state internal explainability was outside the paper's scope and is future work.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Paper reports GPT-4 had higher tolerance (fewer negative effects) when prompts were lengthened or contained misleading steps; no specific arithmetic failure modes uniquely attributed to GPT-4 are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Authors observe a scaling phenomenon: larger/better models (GPT-4) show greater tolerance to manipulations of reasoning-step length, whereas smaller/older models show larger relative boosts from adding steps (text-davinci-002 gained most).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Impact of Reasoning Step Length on Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6649.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6649.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-002 (OpenAI, GPT-3 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier GPT-3 era model included in experiments; described as having the worst initial arithmetic performance among tested models but receiving the largest relative boost from adding reasoning steps in CoT examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K (reported as the dataset with the largest model-to-model performance differences), MultiArith and others</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school multi-step problems (GSM8K etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-Shot-CoT and Few-Shot-CoT with added/compressed reasoning steps (same manipulation applied across models)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Exact numeric accuracies per method/model are not separated in the paper for text-davinci-002; the qualitative report states text-davinci-002 had the worst baseline performance and showed the largest relative improvement when reasoning steps were added.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>None provided in paper for internal numeric processing in this model.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Baseline weakness in arithmetic (relative to newer models) such that it benefits most from structured CoT expansions; compressing steps degrades performance toward zero-shot levels.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Smaller/older model (text-davinci-002) exhibited largest relative boost from adding CoT steps; authors present this as evidence that effect size depends on model capability/size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Impact of Reasoning Step Length on Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6649.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6649.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Expanded CoT / Add Reasoning Steps</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting with added reasoning steps (rationale expansion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's primary experimental manipulation: systematically increase (or compress) the number of reasoning steps in few-shot CoT demonstrations and zero-shot prompts without adding new task-relevant information, to test effect on arithmetic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied to GPT-3.5-turbo-1106, text-davinci-002, GPT-4 (as reported in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MultiArith, SingleEq, AQuA, SAVMP, Letter, Coin, StrategyQA (experiments performed across these datasets; arithmetic-focused results on GSM8K, MultiArith, SingleEq, AQuA, SAVMP)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems, equation-induction problems (MultiArith, GSM8K, SingleEq, AQuA, SAVMP)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language math word problems (free-form answers; some multiple-choice in AQuA)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school to multi-step algebra problems (datasets vary; GSM8K is grade-school multi-step)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot CoT (Manual-CoT, Auto-CoT) with added rationale steps; Zero-shot CoT modified phrase: 'Let's think step by step, you must think more steps'; other CoT strategies (Think About The Word, Read the question again, Repeat State, Self-Verification, Make Equation) used as unit increments of step expansion</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (percentage correct final answers)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports a clear, approximately linear improvement in accuracy with added reasoning steps up to about six added steps on multiple datasets (Figure 3). Example numeric improvements reported in Table 5: MultiArith Auto‑CoT 94.0% → Add Reasoning Step (Auto‑CoT) 97.2%; GSM8K Auto‑CoT 65.8% → Add Reasoning Step (Auto‑CoT) 78.8%. Zero-shot prompt change ('you must think more steps') also produced measurable accuracy gains on math datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic analysis showing how added steps change internal representations; authors explicitly note they did not probe activations or internal mechanisms in this work and plan to visualize activation patterns in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Compressed rationales (shortening step count while preserving core information) substantially reduced accuracy (regressed toward zero-shot performance). Introducing incorrect intermediate steps had minimal negative effect for arithmetic problems (models rely on pattern of reasoning rather than intermediate numeric correctness), but incorrect steps could break reasoning for symbolic/logic tasks. There is an upper limit / diminishing returns: benefits observed 'within certain limits' (paper reports gains up to ~6 extra steps).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Effect size is model- and task-dependent: weaker/older models (text-davinci-002) show the largest relative improvement from adding steps; stronger models (GPT-4) show high tolerance and smaller relative gains; simpler tasks require fewer steps while harder tasks gain more from longer inference chains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Impact of Reasoning Step Length on Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Automatic chain of thought prompting in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
                <li>Large language models are in-context semantic reasoners rather than symbolic reasoners <em>(Rating: 1)</em></li>
                <li>Analyzing chain-of-thought prompting in large language models via gradient-based feature attributions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6649",
    "paper_id": "paper-40c0d1f38ab081e21cc3b1e2e5334a9b54b6ff08",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "GPT-3.5-turbo-1106",
            "name_full": "GPT-3.5-turbo-1106 (OpenAI)",
            "brief_description": "A decoder-only large language model from OpenAI used in the paper's main experiments; evaluated on multiple arithmetic word‑problem benchmarks with Chain‑of‑Thought prompting variants.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo-1106",
            "model_family": "transformer (decoder-only)",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "GSM8K, MultiArith, SingleEq, AQuA, SAVMP (arithmetic/word problem datasets)",
            "task_type": "grade-school multi-step arithmetic word problems, algebraic word problems (multi-step addition/subtraction/multiplication/division and equation solving)",
            "problem_format": "natural-language math word problems (some multiple-choice in AQuA), free-text answer",
            "difficulty_level": "grade-school / middle-school multi-step problems (GSM8K-level), MultiArith multi-step arithmetic",
            "prompting_method": "Zero-Shot-CoT (modified: 'you must think more steps'), Few-Shot-CoT (Manual-CoT, Auto-CoT), expanded CoT rationales with added reasoning steps, compressed CoT",
            "performance_metric": "accuracy (exact-match / correct final answer)",
            "performance_value": "Reported in paper (Table 5) for arithmetic datasets using this experimental setup: MultiArith: Zero-Shot-CoT 91.5% → Auto-CoT 94.0% → Add Reasoning Step (Manual-CoT) 97.0% / (Auto-CoT) 97.2%; GSM8K: Zero-Shot-CoT 64.1% → Auto-CoT 65.8% → Add Reasoning Step (Manual-CoT) 70.1% / (Auto-CoT) 78.8% (values reported in paper Table 5).",
            "internal_analysis": "No mechanistic internal analysis of numerical processing was performed; authors explicitly state they did not analyze internal representations and plan future activation/visualization work to compare long vs short reasoning steps.",
            "failure_modes": "Observed tolerant behavior for arithmetic: single incorrect intermediate numeric values in demonstrations often had minimal effect on final arithmetic accuracy (models learned the CoT pattern more than specific intermediate computations). Compressing reasoning steps in few-shot examples caused large performance drops (regressed toward zero-shot). Other failure modes (noted generally) include fragmentation of reasoning when prompts contained incorrect logical steps (more damaging for logic/symbolic tasks than arithmetic).",
            "scaling_trend": "For GPT-3.5, adding up to ~6 extra reasoning steps produced roughly linear accuracy gains on studied datasets (paper reports increasing accuracy with step count up to a limit); GPT-3.5 showed a substantial boost from the 'add steps' intervention (not as large as the boost observed for weaker models like text-davinci-002 but notable).",
            "uuid": "e6649.0",
            "source_info": {
                "paper_title": "The Impact of Reasoning Step Length on Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "OpenAI's larger LLM used as a comparator in the paper; authors evaluated how tolerance to added or compressed Chain‑of‑Thought steps varies with model size and report higher robustness for GPT-4.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_family": "transformer (decoder-only)",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "GSM8K, MultiArith, other arithmetic/commonsense/symbolic datasets used in paper",
            "task_type": "multi-step arithmetic word problems and other reasoning tasks used for comparison",
            "problem_format": "natural-language math word problems (free-text answers)",
            "difficulty_level": "grade-school to more complex multi-step problems (datasets vary)",
            "prompting_method": "Zero-Shot-CoT (including modified zero-shot prompt 'you must think more steps'), Few-Shot-CoT (Auto-CoT / Manual-CoT) with added reasoning steps",
            "performance_metric": "accuracy (as used in paper)",
            "performance_value": "Numerical per-method accuracies by model are not separately tabulated in the paper; authors report qualitatively that GPT-4 exhibited the highest tolerance to added reasoning steps and experienced no observed performance decreases when step length was increased.",
            "internal_analysis": "No internal mechanistic analysis provided for GPT-4; authors state internal explainability was outside the paper's scope and is future work.",
            "failure_modes": "Paper reports GPT-4 had higher tolerance (fewer negative effects) when prompts were lengthened or contained misleading steps; no specific arithmetic failure modes uniquely attributed to GPT-4 are provided.",
            "scaling_trend": "Authors observe a scaling phenomenon: larger/better models (GPT-4) show greater tolerance to manipulations of reasoning-step length, whereas smaller/older models show larger relative boosts from adding steps (text-davinci-002 gained most).",
            "uuid": "e6649.1",
            "source_info": {
                "paper_title": "The Impact of Reasoning Step Length on Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "text-davinci-002",
            "name_full": "text-davinci-002 (OpenAI, GPT-3 family)",
            "brief_description": "An earlier GPT-3 era model included in experiments; described as having the worst initial arithmetic performance among tested models but receiving the largest relative boost from adding reasoning steps in CoT examples.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-002",
            "model_family": "transformer (decoder-only)",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "GSM8K (reported as the dataset with the largest model-to-model performance differences), MultiArith and others",
            "task_type": "multi-step arithmetic word problems",
            "problem_format": "natural-language word problems",
            "difficulty_level": "grade-school multi-step problems (GSM8K etc.)",
            "prompting_method": "Zero-Shot-CoT and Few-Shot-CoT with added/compressed reasoning steps (same manipulation applied across models)",
            "performance_metric": "accuracy",
            "performance_value": "Exact numeric accuracies per method/model are not separated in the paper for text-davinci-002; the qualitative report states text-davinci-002 had the worst baseline performance and showed the largest relative improvement when reasoning steps were added.",
            "internal_analysis": "None provided in paper for internal numeric processing in this model.",
            "failure_modes": "Baseline weakness in arithmetic (relative to newer models) such that it benefits most from structured CoT expansions; compressing steps degrades performance toward zero-shot levels.",
            "scaling_trend": "Smaller/older model (text-davinci-002) exhibited largest relative boost from adding CoT steps; authors present this as evidence that effect size depends on model capability/size.",
            "uuid": "e6649.2",
            "source_info": {
                "paper_title": "The Impact of Reasoning Step Length on Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Expanded CoT / Add Reasoning Steps",
            "name_full": "Chain-of-Thought prompting with added reasoning steps (rationale expansion)",
            "brief_description": "The paper's primary experimental manipulation: systematically increase (or compress) the number of reasoning steps in few-shot CoT demonstrations and zero-shot prompts without adding new task-relevant information, to test effect on arithmetic performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applied to GPT-3.5-turbo-1106, text-davinci-002, GPT-4 (as reported in experiments)",
            "model_family": "transformer (decoder-only)",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "GSM8K, MultiArith, SingleEq, AQuA, SAVMP, Letter, Coin, StrategyQA (experiments performed across these datasets; arithmetic-focused results on GSM8K, MultiArith, SingleEq, AQuA, SAVMP)",
            "task_type": "multi-step arithmetic word problems, equation-induction problems (MultiArith, GSM8K, SingleEq, AQuA, SAVMP)",
            "problem_format": "natural-language math word problems (free-form answers; some multiple-choice in AQuA)",
            "difficulty_level": "grade-school to multi-step algebra problems (datasets vary; GSM8K is grade-school multi-step)",
            "prompting_method": "Few-shot CoT (Manual-CoT, Auto-CoT) with added rationale steps; Zero-shot CoT modified phrase: 'Let's think step by step, you must think more steps'; other CoT strategies (Think About The Word, Read the question again, Repeat State, Self-Verification, Make Equation) used as unit increments of step expansion",
            "performance_metric": "accuracy (percentage correct final answers)",
            "performance_value": "Paper reports a clear, approximately linear improvement in accuracy with added reasoning steps up to about six added steps on multiple datasets (Figure 3). Example numeric improvements reported in Table 5: MultiArith Auto‑CoT 94.0% → Add Reasoning Step (Auto‑CoT) 97.2%; GSM8K Auto‑CoT 65.8% → Add Reasoning Step (Auto‑CoT) 78.8%. Zero-shot prompt change ('you must think more steps') also produced measurable accuracy gains on math datasets.",
            "internal_analysis": "No mechanistic analysis showing how added steps change internal representations; authors explicitly note they did not probe activations or internal mechanisms in this work and plan to visualize activation patterns in future work.",
            "failure_modes": "Compressed rationales (shortening step count while preserving core information) substantially reduced accuracy (regressed toward zero-shot performance). Introducing incorrect intermediate steps had minimal negative effect for arithmetic problems (models rely on pattern of reasoning rather than intermediate numeric correctness), but incorrect steps could break reasoning for symbolic/logic tasks. There is an upper limit / diminishing returns: benefits observed 'within certain limits' (paper reports gains up to ~6 extra steps).",
            "scaling_trend": "Effect size is model- and task-dependent: weaker/older models (text-davinci-002) show the largest relative improvement from adding steps; stronger models (GPT-4) show high tolerance and smaller relative gains; simpler tasks require fewer steps while harder tasks gain more from longer inference chains.",
            "uuid": "e6649.3",
            "source_info": {
                "paper_title": "The Impact of Reasoning Step Length on Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Automatic chain of thought prompting in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1
        },
        {
            "paper_title": "Large language models are in-context semantic reasoners rather than symbolic reasoners",
            "rating": 1
        },
        {
            "paper_title": "Analyzing chain-of-thought prompting in large language models via gradient-based feature attributions",
            "rating": 1
        }
    ],
    "cost": 0.01194175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Impact of Reasoning Step Length on Large Language Models</h1>
<p>Mingyu Jin ${ }^{1 <em>}$, Qinkai Yu ${ }^{2 </em>}$, Shu Dong ${ }^{3}$, Haiyan Zhao ${ }^{4}$, Wenyue Hua ${ }^{1}$, Yanda Meng ${ }^{5}$, Yongfeng Zhang ${ }^{1}$, Mengnan Du ${ }^{4}$, ${ }^{1}$ Rutgers University, ${ }^{2}$ University of Liverpool, ${ }^{3}$ Northwestern University, ${ }^{4}$ New Jersey Institute of Technology, ${ }^{5}$ University of Exeter<br>{mingyu.jin, yongfeng.zhang, wenyue.hua}@rutgers.edu, sgqyu9@liverpool.ac.uk<br>dongshu2024@u.northwestern.edu, Y.M.Meng@exeter.ac.uk, {hz54, mengnan.du}@njit.edu</p>
<h4>Abstract</h4>
<p>Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make better use of LLMs' potential in complex problem-solving scenarios. Second, we also investigated the relationship between the performance of CoT and the rationales used in demonstrations. Surprisingly, the result shows that even incorrect rationales can yield favorable outcomes if they maintain the requisite length of inference. Third, we observed that the advantages of increasing reasoning steps are task-dependent: simpler tasks require fewer steps, whereas complex tasks gain significantly from longer inference sequences. The code is available at https://github.com/MingyuJ 666/The-Impact-of-Reasoning-Step-Len gth-on-Large-Language-Models</p>
<h2>1 Introduction</h2>
<p>Today, the advent of large language models (LLMs) and their advanced prompting strategies</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: From left to right: zero-shot CoT, few-shot CoT, and few-shot CoT with more reasoning steps. For few-shot COT, there is a direct linear correlation between step count and accuracy.
has marked a significant progression, especially in classical NLP tasks (Kojima et al., 2023; Wei et al., 2022; Shao et al., 2023; Lyu et al., 2023; Jin et al., 2024). A key innovation among these is the Chain of Thought (CoT) prompting technique (Kojima et al., 2023; Wang et al., 2023; Zhang et al., 2022), known for its efficacy in multi-step problem solving. This technique, reflecting human sequential reasoning, has shown remarkable effectiveness in various challenges, including crossdomain, length-generalization, and cross-lingual tasks. The CoT approach, with its logical, step-bystep methodology, offers crucial interpretability in complex problem-solving scenarios. Interestingly, Wang et al. found that even incorrect but coherent rationales can improve reasoning performance, highlighting the value of logical continuity (Wang et al., 2023). Building on this, Fu et al. introduced complexity-based prompting, significantly improving accuracy and setting new benchmarks (Fu et al., 2023). This research further explores the relationship between the length of reasoning steps and the accuracy of conclusions, deepening</p>
<p>our understanding of effective problem-solving in NLP.</p>
<p>Despite its promising results, the research community has yet to reach a consensus on the precise mechanics of how and why CoT and its variations function effectively. This knowledge gap means that enhancing CoT performance is still a field of exploration, largely reliant on trial-and-error approaches. There still lack established systematic methodologies for improving CoT's effectiveness, leaving researchers to rely on conjecture and experimentation. This situation underscores a significant opportunity in the field: to develop a deeper, more structured understanding of CoT's inner workings. Such advancement would not only demystify the current process, but also pave the way for more reliable and efficient applications of this technique in various complex NLP tasks.</p>
<p>In this study, we aim to investigate the hypothesis that the reasoning steps are the most crucial element in the effectiveness of CoT prompts. This hypothesis stems from the observation that reasoning steps are a common element in both zero-shot and few-shot CoT approaches. We conduct experiments to investigate this by maintaining strict control over variables. Particularly, when incorporating new reasoning steps, we ensured that no additional knowledge was introduced. For the zeroshot experiments, we tweaked the initial prompt from "let's think step by step" to "let's think step by step, you must think more steps". Then for fewshot setting, we design experiments that expand the rationale reasoning steps within CoT demonstrations, while keeping all other factors constant. Our first set of experiments evaluated the improvement in zero-shot and few-shot performance using Auto-CoT (Zhang et al., 2022) with our strategic intervention. Subsequently, we assessed the accuracy of different methods across varying step lengths. We then extended our investigation to compare the effectiveness of our strategy on different LLMs such as GPT-3.5 and GPT-4. Our findings revealed a significant correlation between the length of the reasoning chain and the capabilities of LLMs, within certain limits. Intriguingly, when we introduced misleading information into the reasoning chain, the performance still showed improvement. This highlighted a pivotal insight: The key factor appears to be the length of the thinking chain rather than its accuracy. We have the following key findings, which we hope can help the community better improve CoT performance.</p>
<ul>
<li>For few-shot COT, there is a direct linear correlation between step count and accuracy. This provides a quantifiable approach for optimizing CoT prompting in complex reasoning. Specifically, lengthening the reasoning steps in prompts considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening reasoning steps, even while preserving key information, significantly diminishes the reasoning abilities of models.</li>
<li>Even incorrect rationales can yield favorable outcomes if the required length of inference is maintained. For example, in mathematical problems, errors in intermediate numbers have a minor impact due to their process-oriented nature.</li>
<li>The advantages of increasing reasoning steps are task-dependent: simpler tasks necessitate fewer steps, whereas more complex tasks gain significantly from longer inference sequences.</li>
<li>Increased reasoning steps in zero-shot CoT can also significantly improve LLM accuracy. To validate this, we altered the initial prompt from "Let's think step by step" to "Let's think step by step, you must think more steps." This modification led to a noticeable enhancement in the reasoning abilities of the LLMs, particularly evident in datasets involving mathematical problems.</li>
</ul>
<h2>2 Related Works</h2>
<p>In this section, we summarize two lines of literature that are most relevant to ours.</p>
<h3>2.1 CoT Prompting</h3>
<p>The recent surge in computational power has paved the way for the rise of expansive language models. With increasing complexity, these models have unlocked emerging capabilities, notably incontext learning and CoT reasoning (Wei et al., 2022; Brown et al., 2020; Schaeffer et al., 2023).</p>
<p>In their seminal work, Brown et al. discovered the ability of large-scale language models to leverage in-context learning (ICL) (Brown et al., 2020). ICL strategy involves weaving input-output examples directly into the prompt, allowing ready-touse large language models to perform impressively without the need for task-specific fine-tuning. However, despite its promise, this end-to-end methodology often falters when confronting complex reasoning challenges.</p>
<p>Building on this, Wei et al. demonstrated that integrating a series of logical reasoning steps into</p>
<p>the model demonstrations, called CoT prompting, significantly refines the reasoning capabilities of large language models <em>Wei et al. (2022)</em>. CoT prompting not only deepens the model’s grasp of the nuanced questions and their underlying logic but also yields an articulated sequence of reasoning steps. Zhang et al.’s “Auto-CoT” method represents a significant advancement in the field of AI reasoning <em>Zhang et al. (2022)</em>. By automating the CoT process, it addresses complex problems more effectively. And then Yao et al. introduced the “Tree of Thoughts” (ToT) framework, an evolution of the Chain of Thought approach for language model inference <em>Yao et al. (2023)</em>. ToT allows language models to explore different units of text as intermediate steps in problem-solving. This framework enables more deliberate decision-making by considering multiple reasoning paths.</p>
<h3>2.2 Preliminary Work on Analyzing CoT</h3>
<p>The development and understanding of CoT reasoning in AI have evolved over time, marked by significant contributions from various researchers. Initially, Madaan and Yazdanbakhsh <em>Madaan and Yazdanbakhsh (2022)</em> explored the decomposition of prompts into symbols, patterns, and texts, examining the effects of CoT through counterfactual prompting. This study laid the groundwork for understanding how different components of a prompt influence AI reasoning. Besides, several studies furthered this understanding. Tang et al. <em>Tang et al. (2023)</em> investigated the role of semantics in CoT reasoning, uncovering a reliance on semantic knowledge from pre-training and challenges in symbolic reasoning. Around the same time, Wang et al. focused on the impact of demonstration selection in CoT, revealing that the relevance and order of reasoning are more critical than the accuracy of reasoning chains <em>Wang et al. (2023)</em>.</p>
<p>Theoretical perspectives also emerged recently, offering deeper insights into the mechanics of CoT. For example, Li et al. conceptualized CoT as a multi-step combinatorial function, illustrating its role in simplifying in-context learning for complex questions <em>Li et al. (2023)</em>. Feng et al. theoretically demonstrated the sufficiency of a fixed-size Transformer for computational tasks and dynamic planning within CoT frameworks <em>Fu et al. (2023)</em>.</p>
<p>Further contributions in this field included those of Merrill and Sabharwal, who observed that CoT can improve reasoning abilities, with improvements scaling with the number of intermediate steps <em>Merrill and Sabharwal (2023)</em>. Wu et al. employed gradient-based feature attribution methods to assess the robustness of CoT against question variations and perturbations <em>Wu et al. (2023)</em>.</p>
<h2>3 Analyzing Methods</h2>
<p>In this section, we propose an analysis to examine the relationship between the reasoning steps and the chain-of-thought (CoT) prompting performance. Our central hypothesis is that the reasoning steps are the most critical component of CoT prompts, enabling language models to apply more logical reasoning when generating responses. To test this, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations, while keeping all other factors constant. Specifically, we systematically vary only the number of reasoning steps, without introducing new reasoning content or removing existing reasoning content. We evaluate both zero-shot and few-shot CoT prompts in the following sections. The overall experimental procedure is illustrated in Figure 2. Through this controlled analysis, we aim to shed light on how CoT influences the LLM’s ability to produce logically sound responses.</p>
<h3>3.1 Preliminary</h3>
<p>Zero-Shot-CoT <em>Kojima et al. (2023)</em> is a template-based zero-shot prompt for chain-of-thought reasoning. The core idea is to add “Let’s think step by step” or other similar text. Compared to Zero-Shot-CoT, Few-Shot-CoT provides more examples in the prompt for chain-of-thought reasoning. Among them, Manual-CoT <em>Wei et al. (2022)</em>, Auto-CoT <em>Zhang et al. (2022)</em> are two popular methods.</p>
<p>Manual-CoT: Manual-CoT prompting relies on a few manually designed demonstrations, each composed of a question and a reasoning chain leading to an answer, to improve language models’ reasoning performance.</p>
<p>Auto-CoT: Auto-CoT eliminates the need for manual demonstration design by automatically constructing demonstrations through clustering test questions to select diverse examples and generating reasoning chains using the language model’s own zero-shot reasoning capability.</p>
<h3>3.2 Analyzing Zero-shot CoT</h3>
<p>In the zero-shot scenario, we modified the initial prompt from “Let’s think step by step” to “Let’s think step by step, you must think more steps.”</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Increase the length of the thinking chain through the method in the figure, and compress the thinking chain without losing information as much as possible.</p>
<p>This change was implemented because, unlike the Few-shot CoT context, we cannot introduce additional reasoning steps in the example. By altering the initial prompt, we guide the LLM to engage in more extensive thinking. This approach is crucial as it enhances the model's accuracy without the need for incremental training or additional example-driven adjustments typical in few-shot CoT scenarios. This refined strategy ensures a more comprehensive and detailed reasoning process, thereby significantly improving the model's performance in zero-shot settings.</p>
<h3>3.3 Analyzing Few-shot CoT</h3>
<p>In this section, we aim to modify the reasoning chains within CoT rationales, either by adding or compressing reasoning steps. The goal is to examine how changes in reasoning structure influence LLM decision-making. During rationale expansion, we will avoid introducing any new task-relevant information. This isolates reasoning steps as the only variable under study.</p>
<p>To this end, we plan to investigate the following strategies to expand the reasoning steps for different LLM applications. There are usually fixed patterns in the way people think about a problem, for example, by repeating the question over and
over again to gain a deeper understanding, by creating mathematical equations to reduce the burden on memory, by analyzing the meaning of words in the question to help understand the topic, by summarizing the current state to simplify the description of the topic. Based on the inspiration of Zero-Shot-CoT and Auto-CoT, we expected the process of CoT to become a standardized pattern, and lead to the right result by restriction on the direction of CoT thinking in the prompt section. The core of our approach is to simulate the process of human thinking and reshape the chain of thought. We give five general prompt strategies in Table 6 in the Appendix.</p>
<ul>
<li>Think About The Word: This strategy is to ask the model to interpret the word and rebuild the knowledge base. Typically a word has multiple different meanings, and the effect of this is to get the model to think outside the box and reinterpret the words in the problem based on the generated interpretations. This process does not introduce new information. In the prompt, we give examples of the words that the model is thinking about, and the model automatically picks words for this process based on the new question.</li>
<li>Read the question again: Read the questions repeatedly to reduce the interference of other</li>
</ul>
<p>Table 1: Case of Think About The Word, select a few words and ask the model to think about the meaning in text.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Think About The Word</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Prompt: $\square$</td>
</tr>
<tr>
<td style="text-align: left;">Q: Could someone in Tokyo take a taxi to the Metropolitan Museum of Art?</td>
</tr>
<tr>
<td style="text-align: left;">A: Let's think step by step. The stem of the sentence is Tokyo, take a taxi, Metropolitan Museum of Art. Think about Tokyo...</td>
</tr>
<tr>
<td style="text-align: left;">Think about taking a taxi... Think about Metropolitan Museum of Art... Inference: Tokyo is in Japan and the Metropolitan</td>
</tr>
<tr>
<td style="text-align: left;">Museum of Art is in New York. The two places are separated by the sea, so you can't take a taxi there. The answer is no.</td>
</tr>
<tr>
<td style="text-align: left;">Q: Could a chipmunk fit 100 chocolate chips in his mouth?</td>
</tr>
<tr>
<td style="text-align: left;">Answer: $\square$</td>
</tr>
<tr>
<td style="text-align: left;">A: Based on this question, the stem of the sentence is chipmunk, fit, 100 chocolate chips, mouth. Think about chipmunk...</td>
</tr>
<tr>
<td style="text-align: left;">Think about chipmunk... Think about fit... Think about chocolate chips... Inference: Chipmunks are small animals with relatively small mouths, so it's</td>
</tr>
<tr>
<td style="text-align: left;">unlikely that a chipmunk would be able to fit 100 chocolate chips in its mouth at once. The answer is no.</td>
</tr>
<tr>
<td style="text-align: left;">Prediction: no</td>
</tr>
<tr>
<td style="text-align: left;">Ground Truth: no</td>
</tr>
</tbody>
</table>
<p>texts on the chain of thought. In short, we let the model remember the questions.</p>
<ul>
<li>Repeat State: Similar to repeated readings, we include a small summary of the current state after a long chain of reasoning, aiming to help the model simplify its memory and reduce the interference of other texts in the CoT.</li>
<li>Self-Verification: Humans will check if their answers are correct when answering questions. Therefore, before the model gets the answer, we add a self-verification process to judge whether the answer is reasonable based on some basic information.</li>
<li>Make Equation: For mathematical problems, Make Equations can help humans summarize and simplify memory. And for some problems that require the assumption of an unknown number $x$, establishing an equation is an essential process. We simulated this process and let the model try to make equations in mathematical problems.</li>
</ul>
<p>Overall, our prompt strategies all saw corresponding patterns in the model's responses. We give an example in Table 1, examples of the other four strategies can be found in the appendix. In Section 4 we will perform a quantitative analysis to validate the effectiveness of our strategies. We assume that each additional strategy is equivalent to increasing the reasoning step length by one.</p>
<h2>4 Experimental Results</h2>
<p>We conduct experiments to answer the following research questions: RO1: What is the relationship of rational reasoning steps in demonstrations with CoT performance? (Section 4.2) RO2: Can we confirm that the reasoning steps are the only factor that affects LLM performance? (Section 4.3) RO3: Will compressing reasoning steps in few-shot demonstrations hurt LLM performance? (Section 4.4), RO4: Can we observe the scaling phenomenon, i.e., the required reasoning steps be related to the size of the LLMs? (Section 4.5) and RO5: What is the influence of questions in rationale towards the LLM reasoning ability? (Section 4.6)</p>
<h3>4.1 Experimental Setups</h3>
<p>We introduce the general experimental setups.
Datasets and Models. We evaluate our proposal on eight dataset (MultiArith (Roy and Roth, 2015), GSM8K (Cobbe et al., 2021), AQuA (Ling et al., 2017), SingleEq (Koncel-Kedziorski et al., 2015), SAVMP (Patel et al., 2021), Letter (Wei et al., 2022), Coin (Wei et al., 2022), Strategyqa (Geva et al., 2021)). We employ three models to validate the performance of our proposed method, which are: text-davinci-002 (Brown et al., 2020), GPT-3.5-turbo-1106 (Ouyang et al., 2022), GPT-4 (OpenAI, 2023). All models are accessed via the OpenAI API key.
Prompt. We have shown the proposed process pipeline in Section 3 Analyzing Methods. The experimental part follows the same approach.
Baselines. We compare our methods with four baseline methods: Zero-Shot (Kojima et al., 2023), Zero-Shot-CoT (Kojima et al., 2023), Manual-CoT (Wei et al., 2022), Auto-CoT (Zhang et al., 2022). The results are in the Table 2.
Evaluation Metrics. Accuracy is used to assess a model's ability on classification tasks and is commonly used for multichoice and yes/no tests: Accuracy $=N_{\text {correct }} / N_{\text {total }}$.</p>
<h2>Implementation Details:</h2>
<ul>
<li>Add reasoning steps: This process deploys GPT4 to modify the Zero-Shot-CoT prompt demo</li>
</ul>
<p>Table 2: Comparison of accuracy of our method with four baselines on eight datasets</p>
<p>| Models | Arithmetic | | | | | | Commonsense | Symbolic | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Compare the accuracy of the prompt with the true answer and the prompt with the wrong answer</p>
<p>Benefiting from the fact that we have standardized the thought chain process, it is possible to quantify the increase in accuracy due to the increased steps in rationales of COT demonstrations. We conducted experiments to answer the RO1: What is the relationship of rational reason-ing steps in demonstrations with CoT performance? This experiment is completed with GPT-3.5-turbo-1106, and the results are given in Figure 3. We found that LLM reasoning ability improves in all datasets during an effective CoT process, i.e. with the addition of up to six steps of additional thought processes. In other words, we found a certain linear relationship between accuracy and CoT complexity.</p>
<h3>4.3 Effect of Prompt with Wrong Answer</h3>
<p>To answer RO2: Are reasoning steps the only factor that affects LLM performance? We made the following attempt. Change a step in the prompt to an incorrect answer to see if it affects the chain of thought. So, for this experiment, we change all the prompts to carry one error. For a concrete example, check the Table 3. For Arithmetic-type questions, even if there is a deviation in one of the results of the prompt, the effect on the chain of thought in the reasoning process is minimal, so we believe that the large language model learns more about the chain of thought patterns in the prompt than the single computation when solving Arithmetic-type problems. For logic problems similar to those in the Coin dataset, a deviation in one of the prompt's results often brings about the fragmentation of the entire chain of thought. We completed this experiment with GPT-3.5-turbo-1106 and guaranteed performance based on the optimal number of steps for each data set derived from the previous experiment. The results are shown in Figure 4.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Compare the accuracy of the prompt with Compression and the prompt with No Compression</p>
<h3>4.4 Compressing Reasoning Steps</h3>
<p>In previous sections, we have demonstrated that increasing reasoning steps could improve the LLM reasoning accuracy. In this section, our aim is to answer RO3: Will compressing reasoning steps in few-shot demonstrations hurt LLM performance? To this end, we conduct the reasoning steps compression experiment, and we employed the technique outlined in the experimental setup to condense the reasoning process in examples from both the baseline automated chain of thought (AutoCoT) and the few-shot chain of thought (Few-ShotCoT), aiming to reduce their number of inference steps. The result is shown in Figure 5. It revealed a notable decline in performance, which regressed to a level essentially equivalent to that achieved by the zero-shot method. It further indicates that <em>increasing COT rationale reasoning steps could improve COT performance and the vice versa.</em></p>
<h3>4.5 Performance on Different Size Models</h3>
<p>In this chapter, our focus is to answer RO4: Can we observe the scaling phenomenon, i.e., the required reasoning steps be related to the size of the LLMs? We examine the average number of inference steps utilized in various models, including text-davinci-002, GPT-3.5-turbo-1106, and GPT-4. We conducted experiments on GSM8K calculating the average inference step for each model to reach peak performance. This dataset has the largest performance difference with text-davinci-002, GPT-3.5-turbo-1106, and GPT-4 out of our 8 datasets. It can be observed that the model with the worst initial performance, text-davinci-002, our strategy has the highest boosting effect. For the model with the best initial performance, GPT-4, has the highest tolerance to our strategy (no performance decreases). We show the result in Figure 6.</p>
<p>Table 4: Making deliberate alterations to sample questions</p>
<p>Original Prompt
Q: Wendy uploaded 45 pictures to Facebook. She put 27 pics into one album and put the rest into 9 different albums. How many pictures were in each album?
Rationale: "Let's think step by step. First, Wendy uploaded 45 pictures in total. Second, Wendy put 27 pictures into one album. That means that Wendy put the remaining 18 pictures into 9 different albums. Each album would have 2 pictures."
Pred_ans: "2"
Gold_ans: "2"
Making deliberate alterations
Q: Wendy uploaded 66 pictures to Facebook. She put 89 pics into one album and put the rest into 7 different albums. How many pictures were in each album?
Rationale: "Let's think step by step. First, Wendy uploaded 54 pictures in total. Second, Wendy put 27 pictures into one album. That means that Wendy put the remaining 12 pictures into 6 different albums. Each album would have 7 pictures."
Pred_ans: "2"
Gold_ans: "2"
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Comparing the accuracy with different size model on dataset GSM8K.</p>
<h3>4.6 Influence of Questions in CoT Examples</h3>
<p>In our case study, we aim to answer RO5: What is the influence of questions in rationale towards the LLM reasoning ability? We want to explore whether changing the reasoning in CoT will affect CoT performance. Since we are mainly studying the impact of the inference step on performance, we need to confirm that the question has no impact on performance. So we have chosen two datasets and two CoT methods (auto-CoT and few-shotCoT) for this investigation: MultiArith (Roy and Roth, 2015) and GSM8K (Cobbe et al., 2021) in GPT-3.5-turbo-1106. Our experimental approach involves making deliberate alterations to sample questions within these mathematical datasets, such as varying the content of the questions in Table 4. Remarkably, initial observations indicate that these modifications have only minimally impacted performance like Table 5.</p>
<p>This preliminary finding suggests that the length of the steps involved in the reasoning process, rather than the nature of the questions themselves, predominantly influences the reasoning capabilities of large-scale models.</p>
<p>Table 5: Accuracy comparison of models on different datasets</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>MultiArith</th>
<th>GSM8K</th>
</tr>
</thead>
<tbody>
<tr>
<td>Zero-Shot</td>
<td>$40 \pm 1.0$</td>
<td>$30.4 \pm 1.7$</td>
</tr>
<tr>
<td>Zero-Shot-CoT</td>
<td>$91.5 \pm 1.2$</td>
<td>$64.1 \pm 1.1$</td>
</tr>
<tr>
<td>Manual-CoT</td>
<td>$93.5 \pm 0.1$</td>
<td>$64.7 \pm 1.5$</td>
</tr>
<tr>
<td>Auto-CoT</td>
<td>$94 \pm 0.0$</td>
<td>$65.8 \pm 0.9$</td>
</tr>
<tr>
<td>Changing Question <br> (Manual-CoT)</td>
<td>$92.9 \pm 0.1$</td>
<td>$62.1 \pm 1.7$</td>
</tr>
<tr>
<td>Changing Question <br> (Auto-CoT)</td>
<td>$92.5 \pm 0.1$</td>
<td>$63.5 \pm 1.0$</td>
</tr>
<tr>
<td>Add Reasoning Step <br> (Manual-CoT)</td>
<td>$97 \pm 0.0$</td>
<td>$70.1 \pm 0.3$</td>
</tr>
<tr>
<td>Add Reasoning Step <br> (Auto-CoT)</td>
<td>$97.2 \pm 0.1$</td>
<td>$78.8 \pm 0.2$</td>
</tr>
<tr>
<td>Add Reasoning Step <br> and Changing Question <br> (Manual-CoT)</td>
<td>$96.6 \pm 0.1$</td>
<td>$69.6 \pm 0.2$</td>
</tr>
<tr>
<td>Add Reasoning Step <br> and Changing Question <br> (Auto-CoT)</td>
<td>$95.7 \pm 0.1$</td>
<td>$75.2 \pm 0.2$</td>
</tr>
</tbody>
</table>
<h2>5 Conclusions and Future Work</h2>
<p>In this work, we make a critical contribution to understanding and optimizing CoT in LLMs, especially in the realm of complex reasoning tasks. Our extensive research on the CoT technique in natural language processing, particularly with large language models like GPT-3, GPT-3.5, and GPT-4, has led to key insights. We found a notable correlation between the length of the reasoning chain and the performance of these models. Interestingly, longer reasoning chains improve model performance, even when they contain misleading information. This suggests that the chain's length is more crucial than its factual accuracy for effective problem-solving. These findings provide valuable guidance for refining CoT strategies, highlighting the significance of reasoning length in complex NLP tasks.</p>
<p>Our next step is to analyze the long and short reasoning steps of LLM inference via explaindeter-</p>
<p>mineOur objective is to ascertain whether longer inferential steps correlate with broader neuronal engagement. To illustrate this, we intend to use visualization techniques to analyze activation patterns between long and short reasoning steps.</p>
<h2>6 Limitation</h2>
<p>In this work, we provide an experimental analysis of how CoT works. We focus specifically on manipulating the reasoning steps in CoT prompts and measuring the impact on model performance. However, we did not deeply analyze the underlying mechanisms behind why increasing reasoning steps improves performance. This includes either theoretical analysis or explainability analysis to analyze the internal workings of LLMs, and further investigation could provide more insight. Additionally, our study was limited to certain datasets and models like GPT-3.5 and GPT-4. Testing on more diverse tasks and newer models could reveal different trends.</p>
<h2>References</h2>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2023. Complexity-based prompting for multi-step reasoning. Preprint, arXiv:2210.00720.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346361.</p>
<p>Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao, Kai Mei, Yanda Meng, Kaize Ding, Fan Yang, Mengnan Du, and Yongfeng Zhang. 2024. Exploring concept depth: How large language models acquire knowledge at different layers? Preprint, arXiv:2404.07066.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023. Large
language models are zero-shot reasoners. Preprint, arXiv:2205.11916.</p>
<p>Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics, 3:585-597.</p>
<p>Yingcong Li, Kartik Sreenivasan, Angeliki Giannou, Dimitris Papailiopoulos, and Samet Oymak. 2023. Dissecting chain-of-thought: A study on compositional in-context learning of mlps. arXiv preprint arXiv:2305.18869.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158-167.</p>
<p>Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful chain-ofthought reasoning. Preprint, arXiv:2301.13379.</p>
<p>Aman Madaan and Amir Yazdanbakhsh. 2022. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686.</p>
<p>William Merrill and Ashish Sabharwal. 2023. The expressive power of transformers with chain of thought. Preprint, arXiv:2310.07923.</p>
<p>R OpenAI. 2023. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2:13.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094.</p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</p>
<p>Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. 2023. Are emergent abilities of large language models a mirage? arXiv preprint arXiv:2304.15004.</p>
<p>Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Synthetic prompting: Generating chain-of-thought demonstrations for large language models. Preprint, arXiv:2302.00618.</p>
<p>Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, and Muhan Zhang. 2023. Large language models are in-context semantic reasoners rather than symbolic reasoners. arXiv preprint arXiv:2305.14825.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. Preprint, arXiv:2203.11171.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.</p>
<p>Skyler Wu, Eric Meng Shen, Charumathi Badrinath, Jiaqi Ma, and Himabindu Lakkaraju. 2023. Analyzing chain-of-thought prompting in large language models via gradient-based feature attributions. arXiv preprint arXiv:2307.13339.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Preprint, arXiv:2305.10601.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493.</p>
<h1>A Appendix</h1>
<p>Table 6</p>
<h2>Think About The Word</h2>
<h2>prompt:</h2>
<p>Q: Could someone in Tokyo take a taxi to the The Metropolitan Museum of Art?
A: Let's think step by step. The stem of the sentence is Tokyo, take a taxi, Metropolitan Museum of Art. Think about Tokyo... Think about take a taxi... Think about Metropolitan Museum of Art... Inference: Tokyo is in Japan and the Metropolitan Museum of Art is in New York. The two places are separated by the sea, so you can't take a taxi there. Since two places are separated by the sea, you can't take a taxi there. The answer is yes.
Q: Could a chipmunk fit 100 chocolate chips in his mouth?</p>
<h2>Answer:</h2>
<p>A: Based on this question, The stem of the sentence is chipmunk, fit, 100 chocolate chips, mouth. Think about chipmunk... Think about fit... Think about chocolate chips... Inference: Chipmunks are small animals with relatively small mouths, so it's unlikely that a chipmunk would be able to fit 100 chocolate chips in its mouth at once. The answer is no.
pred : no
GT : no</p>
<h2>Read the question again</h2>
<h2>prompt:</h2>
<p>Q: Mark's father gave him $\$ 85$. Mark bought 10 books, each of which cost $\$ 5$. How much money does Mark have left? A: Let's think step by step. The question is: How much money does Mark have left? So we need to calculate How much money does Mark have left. Start looking for information about money now. Mark's father gave him $\$ 85$. Mark bought 10 books, each of which cost $\$ 5$. That means that Mark spent $\$ 50$ on books. So we have equation money $=+85-50=35$. So Mark has $\$ 85-\$ 50=\$ 35$ left. So the answer is 35 .
Q: During a visit to an orchard, Charlie picked 0.1666 of a bag of Golden Delicious apples, 0.1666 of a bag of Macintosh apples, and 0.333 of a bag of Cortland apples. How many bags of fruit did Charlie pick in total?</p>
<h2>Answer:</h2>
<p>A: The question is: How many bags of fruit did Charlie pick in total. So we need to calculate how many bags of fruit did Charlie pick in total. Start looking for information about bags now. Charlie picked 0.166 of a bag of Golden Delicious apples, 0.166 of a bag of Macintosh apples, and 0.3333 of a bag of Cortland apples. We have to add these fractions together to find out how many bags of fruit Charlie picked in total. $0.1666+0.1666+0.3333=0.6666$. This means that Charlie picked a total of 0.666 bags of fruit. The answer is 0.6666 .
pred : 0.6666
GT : 0.6666</p>
<p>Table 7</p>
<h1>Repeat State</h1>
<h2>prompt:</h2>
<p>Q: A coin is heads up. Janette does not flip the coin. Stacey flips the coin. Ronny flips the coin. Kim does not flip the coin. Is the coin still heads up? Note that "flip" here means "reverse".
A: Let's think step by step. The state of the coin from the beginning is heads up. Janette does not flip the coin, so the coin remains heads up. coin is heads up. Stacey flips the coin, so the coin is now tails up. coin is now tail up. Ronny flips the coin, so the coin is now heads up again. Kim does not flip the coin, so the coin remains heads up. coin is head up. The answer is yes.
Q: A coin is heads up. Jeff does not flip the coin. Jen flips the coin. Giselle flips the coin. Noel does not flip the coin. Is the coin still heads up? Note that "flip" here means "reverse".</p>
<h2>Answer:</h2>
<p>The state of the coin from the beginning is heads up. Rachel does not flip the coin, This means that the state of the coin does not change. so the coin remains heads up. Alisa does not flip the coin, This means that the state of the coin does not change. so the coin remains heads up. Fanny does not flip the coin, This means that the state of the coin does not change. so the coin remains heads up. Jeffery does not flip the coin, This means that the state of the coin does not change. so the coin remains heads up. So the final answer is yes, the coin is still heads up. The answer is yes.
pred : yes
GT : yes</p>
<h2>Self-Verification</h2>
<h2>prompt:</h2>
<p>Q: Take the last letters of each words in "Alina Alessandra Amina Bianca" and concatenate them.
A: Let's think step by step. There is four words. So the answer will consist of four letters. Explain concatenate: concatenate is a term used in computer programming and mathematics, referring to the operation of combining two or more strings, sequences, or sets in a linear order. The last letter of "Alina" is "a". The last letter of "Alessandra" is "a". The last letter of "Amina" is "a". The last letter of "Bianca" is "a". So we have four letters. So the final answer is "aaaa". The answer is aaaa. Q: Take the last letters of each words in "Jeff Jen Giselle Noel" and concatenate them.</p>
<h2>Answer:</h2>
<p>There is four words. So the answer will consist of four letters. Explain concatenate: concatenate is a term used in computer programming and mathematics, referring to the operation of combining two or more strings, sequences, or sets in a linear order. The last letter of "Jeff" is "f". The last letter of "Jen" is "n". The last letter of "Giselle" is "e". The last letter of "Noel" is "l". So we have four letters. So the final answer is "fnel". The answer is fnel.
pred : fnel
GT : fnel</p>
<h2>Make Equation</h2>
<h2>prompt:</h2>
<p>Q: 5 children were riding on the bus. At the bus stop 63 children got off the bus while some more got on the bus. Then there were 14 children altogether on the bus. How many more children got on the bus than those that got off?
A: Let's think step by step. first step, 5 children were riding on the bus. We know 5 children is on the bus. second step,There were 63 children that got off the bus. third step, some more got on the bus we define as unknown x. fourth step, 14 children remained on the bus, which means we can calculate unknow $\mathbf{x}$.we have equation $\mathrm{x}+5-63=14$, now we know x is 72 . fifth step, Therefore, $72-63=9.9$ more children got on the bus than those that got off. The answer is 9 .
Q: Rachel had to complete 8 pages of math homework. If she had to complete 3 more pages of math homework than reading homework How many pages did she have to complete in all?</p>
<h2>Answer:</h2>
<p>Explain the problem. The problem is asking us to find out how many pages of homework Rachel had to complete in total given that she had to complete 8 pages of math homework and 3 more pages of math homework than reading homework. Find the necessary information. First, we know that Rachel had to complete 8 pages of math homework. Let's call this "M". Second, we know that she had to complete 3 more pages of math homework than reading homework.Let's call the number of reading homework pages "R". Find the solution. Based on the information given, we can use the equation $\mathrm{M}=\mathrm{R}+3$ to represent Rachel's math homework in terms of her reading homework. So, if Rachel had to complete 8 pages of math homework, we can substitute $\mathrm{M}=8$ into the equation and solve for $\mathrm{R} .8=\mathrm{R}+3, \mathrm{R}=8-3, \mathrm{R}=5$. Therefore, Rachel had to complete 5 pages of reading homework. Now, we can find the total number of pages Rachel had to complete by adding the math and reading homework together.
Total $=8$ (math) +5 (reading) Total $=13$
So, in total, Rachel had to complete 13 pages of homework.
pred : 13
GT : 13</p>
<p>Table 8</p>
<h1>Example of Wrong Prompt</h1>
<p>Arithmetic Dataset</p>
<h2>prompt:</h2>
<p>Q: Joan has 10 books. Tom has 38 books. How many books do they have
A: Let's think step by step. Joan has 10 books. Tom has 38 books. we have equation books $=10+8=48$. They have $10+38$ $=48$ books together.</p>
<h2>Commonsense Dataset</h2>
<h2>prompt:</h2>
<p>Q: Could someone in Tokyo take a taxi to the The Metropolitan Museum of Art?
Let's think step by step. The stem of the sentence is Tokyo, take a taxi, Metropolitan Museum of Art. Explain Tokyo: Tokyo is the capital city of Japan and one of the most populous metropolitan areas in the world. Explain Metropolitan Museum of Art: is a art museums in New York City. Inference: Tokyo is in Japan and the Metropolitan Museum of Art is in New York. The two places are separated by the sea, so you can take a taxi there.</p>
<h2>Symbolic Dataset</h2>
<h2>prompt:</h2>
<p>Q: Take the last letters of each words in 'Tim Candace Cecil Misael' and concatenate them.
A: Let's think step by step. Explain letters: letters can have various meanings depending on the context, such as Alphabetic Characters, Correspondence, Literature and Books. There is four words. So the answer will consist of four letters. The last letter of 'Tim' is 'm'. The last letter of 'Candace' is 'e'. The last letter of "Cecil" is 'l'. The last letter of "Misael" is "l". So we have four letters. So the final answer would be "mel".</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contribution&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2358 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2358</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2358</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-c0883f5930a232a9c1ad601c978caede29155979</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c0883f5930a232a9c1ad601c978caede29155979" target="_blank">“Why Should I Trust You?”: Explaining the Predictions of Any Classifier</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> LIME is proposed, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction.</p>
                <p><strong>Paper Abstract:</strong> Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2358.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2358.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LIME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Local Interpretable Model-agnostic Explanations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-agnostic, post-hoc explanation technique that approximates any classifier locally with an interpretable (sparse linear) model over an interpretable representation to explain individual predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Natural Language Processing — text classification (document/topic classification, sentiment analysis, politeness detection)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Explain why an opaque classifier made a particular prediction by identifying which interpretable components of the input (e.g., words) were most responsible for that prediction, to build human trust and enable model debugging/feature engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses publicly available labeled NLP datasets referenced in the paper (20 Newsgroups, polite-sentence dataset, movie-review sentiment dataset); paper does not report dataset sizes here but notes classifiers/datasets were made available by cited authors.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Unstructured text (sentences and documents) represented as interpretable binary indicators (presence/absence of words) or other representations derived from original features (e.g., mapping perturbed samples back to original feature space).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Targets complex, potentially high-dimensional and nonlinear classifiers (e.g., SVM with RBF kernel, neural networks) where decision boundaries are opaque; complexity handled by local approximation rather than global modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Applied to well-established NLP tasks with existing benchmark datasets and prior models; the paper leverages existing classifiers and datasets from prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — the method is explicitly developed to provide interpretable, human-understandable mechanistic (local) explanations to support trust, model comparison, and feature engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Local Interpretable Model-agnostic Explanations (LIME)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>LIME constructs an interpretable representation (e.g., binary vector of word presence), samples perturbed instances around the instance of interest, weights samples by proximity using an exponential kernel, obtains the original classifier's predictions on these samples, and fits a sparse linear model (selecting K features via Lasso/forward selection) to minimize a locally weighted squared loss plus a complexity penalty, producing a K-feature explanation for the single prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>post-hoc model-agnostic interpretability / explainability</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable across classifier types (the paper demonstrates use with neural networks, SVMs, random forests) and across multiple NLP tasks; designed to work without access to model internals, requiring only ability to query f(z) for perturbed inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Explanation generation performance: the paper reports that for most datasets and classifiers the system can produce an explanation in under three seconds; no further numeric performance metrics for explanation fidelity reported in the demo text here.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively effective at surfacing why models make predictions (e.g., revealing reliance on irrelevant words like 'Posting' for topic classification), increasing user understanding and enabling detection of problems like spurious features or data leakage; used in demo tasks for trusting individual predictions, comparing models, and feature cleaning.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High potential to improve human trust in deployed NLP models, aid model debugging and feature engineering, detect spurious correlations or data leakage, and guide model selection by exposing reasoning behind predictions; generalizable to other domains where interpretable local explanations are needed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Paper frames LIME as model-agnostic and applicable to any classifier; no detailed empirical comparison to other explanation methods is provided in this demo description.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Success relies on (1) using an interpretable representation understandable to humans (presence/absence of words), (2) local sampling and weighting to ensure fidelity near the instance, (3) sparsity (limiting K features) for human interpretability, and (4) model-agnosticism enabling application to diverse classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Local, model-agnostic explanations that fit sparse interpretable surrogates to perturbations near an instance can provide actionable and fast insights into complex, high-dimensional NLP models, improving trust and enabling debugging without requiring access to model internals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '“Why Should I Trust You?”: Explaining the Predictions of Any Classifier', 'publication_date_yy_mm': '2016-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2358.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2358.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SP-LIME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Submodular Pick for LIME (SP-LIME)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method to select a small, representative, and non-redundant set of individual-instance explanations (produced by LIME) using a submodular coverage objective so users can form a global understanding of a model within a limited inspection budget.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Natural Language Processing — model interpretability/validation for text classifiers</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Given a budget B of explanations a human is willing to examine, select B instances whose local explanations together cover the most important interpretable components (features) without redundancy, to provide a global view of model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Relies on having a set of instances X (explained with LIME); specific dataset sizes are not reported in the demo description.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Explanation matrix representation (n x d') where rows are instances and columns are interpretable components (e.g., words); underlying data are unstructured text mapped to interpretable components.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>NP-hard maximization of weighted coverage; problem benefits from submodularity which enables greedy approximation. Complexity arises from large numbers of instances and features needing non-redundant coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Method builds on established submodular optimization theory and is applied to mature NLP benchmarks for demonstration.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — designed to provide humans with a concise global summary of model behavior, thus requiring explanations that are intelligible and cover relevant mechanisms/features.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>SP-LIME (Submodular Pick for LIME)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Constructs an explanation importance matrix W where each explained instance's local explanation yields absolute weights for interpretable components; defines global component importances I; defines a coverage function that sums importances of components present in a chosen set of instances; solves the pick problem (maximize coverage under budget B) using a greedy algorithm exploiting submodularity to obtain a (1-1/e)-approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>post-hoc model-agnostic interpretability / submodular selection</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate when humans can only inspect a limited number of explanations and a representative global understanding is desired; applicable to any setting where LIME-style local explanations are available.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Theoretical approximation guarantee: greedy selection attains a (1 - 1/e) constant-factor approximation to optimal coverage for submodular functions; no empirical numeric coverage values provided in the demo text.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Provides a non-redundant, representative set of explanations that help users form a global understanding of model behavior (used in demo to compare models and surface common problematic features).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Enables scalable human inspection of model behavior, improving ability to detect systemic issues and to choose between models with limited human attention; generalizable to other interpretability workflows that require summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>No empirical comparisons to alternative selection/summarization strategies are provided here, but approach leverages known submodular maximization properties for provable approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Key factors are (1) using absolute local explanation weights to build a global importance measure, (2) formulating coverage as a submodular objective to exploit greedy approximation guarantees, and (3) limiting user budget to make inspection feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Selecting a small set of locally-explained instances via a submodular coverage objective gives users a concise, non-redundant global view of a model, balancing representativeness and human attention constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '“Why Should I Trust You?”: Explaining the Predictions of Any Classifier', 'publication_date_yy_mm': '2016-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2358.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2358.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SVM (RBF)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Support Vector Machine with RBF kernel</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A nonlinear supervised classifier (SVM with radial basis function kernel) used in the demo for topic classification on 20 Newsgroups; achieves high held-out accuracy but may rely on spurious features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Natural Language Processing — topic/document classification (example: distinguishing Christianity vs Atheism in 20 Newsgroups)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Classify documents into topic categories; example highlighted is distinguishing posts about 'Christianity' vs 'Atheism'.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses the 20 Newsgroups dataset (publicly available labeled dataset); exact training/test sizes are not given in the demo text.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Unstructured text (documents) often represented as bag-of-words or other text features; high-dimensional sparse feature vectors are typical.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Complex nonlinear decision boundary learned by RBF kernel SVM in high-dimensional feature space; potential for exploiting dataset artifacts or spurious signals.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Topic classification on 20 Newsgroups is a well-established benchmark problem in NLP with mature baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium to high — although high predictive accuracy may suffice in some contexts, the paper emphasizes need for interpretability to trust and validate model behavior prior to deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Support Vector Machine (RBF kernel)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Nonlinear kernel SVM trained on text features (in the demo on 20 Newsgroups); specifics of hyperparameters/training are not provided in the demo description, but the model is queried by LIME to obtain prediction probabilities for perturbed inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and effective for the demo classification task; however LIME explanations revealed SVM relying on irrelevant tokens ('Posting', 'Host', 'Re') showing that predictive performance alone may be misleading.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Reported held-out accuracy: 94% for the Christianity vs Atheism classifier on the example task.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Although quantitatively accurate (94%), explanations produced by LIME showed the classifier used arbitrary/irrelevant words for decisions, indicating brittle or spurious decision rules despite high accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High predictive utility for classification, but limited deployment trust without interpretability; combining with LIME can reveal weaknesses and guide feature cleaning to improve real-world suitability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Paper uses SVM as one of several classifiers compared via LIME explanations (others include LSTMs and random forests) but provides no numeric comparative performance across classifiers in the demo text besides the single SVM accuracy example.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>SVM success driven by capacity to learn nonlinear boundaries; vulnerability highlighted by LIME arises when training data contain spurious signals or artifacts that the SVM can exploit to boost cross-validated accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>High held-out accuracy of nonlinear classifiers (e.g., SVM RBF) can mask reliance on spurious features; local explanations are essential to validate whether a high-performing model learned meaningful, generalizable decision rules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '“Why Should I Trust You?”: Explaining the Predictions of Any Classifier', 'publication_date_yy_mm': '2016-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2358.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2358.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long Short-Term Memory neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recurrent neural network models (LSTMs) referenced and used in the demo as one of several classifier types for NLP tasks, trained on sentence/document representations such as embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Natural Language Processing — sentence-level tasks such as sentiment analysis and paraphrastic embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Used as classifiers for NLP tasks (e.g., sentiment analysis) where sequential modeling of text is relevant; in demo, different classifiers including LSTMs are compared via explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Applied on publicly available labeled datasets (e.g., movie-review sentences from Socher et al.); exact dataset sizes or LSTM training details are not reported in this demo description.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Sequential unstructured text (sentences/documents), often converted to dense vector embeddings as inputs to LSTMs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Models are expressive and capture nonlinear sequential dependencies; training complexity depends on network size and data but specifics are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>LSTMs are an established approach in NLP; the demo leverages pre-existing classifiers from prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — due to opacity of neural networks, interpretable explanations (via LIME) are emphasized to build trust and inspect behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>LSTM neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Recurrent neural network classifiers (LSTMs) trained on sentence/document representations such as word embeddings; the demo uses such models when comparing classifiers with LIME explanations, but provides no architecture/training hyperparameters in the demo text.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning (deep learning)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for sequence-based NLP tasks and shown as one of multiple model types whose predictions can be explained by LIME; no specific limitations described in this demo text.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Used to demonstrate that LIME can explain highly nonlinear and opaque models like LSTMs; no quantitative results reported here for LSTM performance.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>When combined with LIME explanations, deep sequence models' decisions become inspectable, enabling trust and iterative improvement in applications like sentiment analysis and paraphrastic embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively via LIME explanations against other classifiers (SVMs, random forests) in demo tasks; no quantitative comparative metrics provided in this description.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Expressive sequence modeling capability of LSTMs; the ability to be queried by LIME for local explanation is crucial for interpretability in the demo workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Opaque deep sequence models (LSTMs) can be made interpretable locally through model-agnostic explanation methods, enabling their use in trust-sensitive NLP tasks without requiring intrinsic interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '“Why Should I Trust You?”: Explaining the Predictions of Any Classifier', 'publication_date_yy_mm': '2016-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2358.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2358.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random Forests</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Forest classifiers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble tree-based supervised learning method used in the demo as one of several classifier types applied to NLP classification tasks and explainable with LIME.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Natural Language Processing — document/sentence classification</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Used as a classifier for NLP tasks in the demo; included to illustrate LIME's applicability across diverse model families.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Trained on publicly available labeled NLP datasets referenced in the paper; specific training set sizes or data quality metrics are not provided in the demo text.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Unstructured text transformed into feature vectors (e.g., bag-of-words, embeddings) for tree-based modeling; input can be high-dimensional and sparse.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Ensemble of decision trees can model nonlinear interactions across features; interpretability of raw model is limited when ensemble is large.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Random forests are a standard, well-established supervised learning method used widely in NLP and other domains.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium to high — paper emphasizes need for interpretability regardless of model class; LIME is used to produce human-understandable explanations for random forest predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Random forests (ensemble of decision trees)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Ensemble classifier composed of multiple decision trees trained on text-derived features; in demo used alongside other classifiers and explained via LIME, with no further hyperparameter or training detail in the demo text.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning (ensemble methods)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable to text classification tasks and compatible with LIME since only model predictions are needed to generate local explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Serves as one of multiple model families whose opaque predictions can be inspected by LIME; no detailed qualitative performance evaluation provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Combining standard interpretability-free models like random forests with LIME increases their trustworthiness and utility in human-in-the-loop workflows for NLP.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Included in qualitative comparisons via LIME explanations against other classifiers (SVMs, LSTMs) in the demo; numeric comparisons not provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Ability of LIME to operate without access to model internals is key to making ensemble models inspectable; standard generalization properties of random forests contribute to baseline performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Model-agnostic explanation tools make ensemble models (random forests) usable in trust-sensitive NLP applications by revealing local decision drivers without modifying the underlying model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '“Why Should I Trust You?”: Explaining the Predictions of Any Classifier', 'publication_date_yy_mm': '2016-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2358.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2358.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BoW & Embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bag-of-Words and Word Embedding representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Input feature representations for NLP classifiers used in the demo: sparse bag-of-words vectors and dense word embeddings (including sentence embeddings), which feed into classifiers like SVMs, LSTMs, and random forests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Natural Language Processing — feature representations for text classification tasks</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Represent raw text as features for supervised classifiers: bag-of-words (interpretable binary presence/absence indicators) or distributed word/sentence embeddings (dense continuous vectors).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Representations derived from publicly available labeled datasets used in the demo; no further info on pretraining corpora or embedding dimensionality is provided in the demo text.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Unstructured text mapped to either high-dimensional sparse vectors (BoW) or dense continuous vectors (word/sentence embeddings); both are used depending on classifier type.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>BoW yields very high-dimensional sparse inputs; embeddings create dense lower-dimensional spaces but may encode complex semantic relationships; both impact classifier complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Both BoW and embedding representations are mature and standard in NLP; embeddings (and sentence embeddings) referenced via prior work (e.g., Wieting et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — BoW representations are interpretable facilitating LIME explanations, whereas embeddings are less interpretable, increasing the need for post-hoc explanation to understand model decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Bag-of-Words (BoW) and Word/Sentence Embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>BoW: interpretable binary or count features indicating word presence/absence; Embeddings: dense vectors representing words or sentences (e.g., paraphrastic sentence embeddings) used as inputs to classifiers like LSTMs and neural nets; LIME operates over interpretable representations (e.g., binary word presence) even when classifier uses embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>feature representation for supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>BoW is directly interpretable and suitable for LIME's interpretable representation; embeddings improve predictive performance for neural models but reduce direct interpretability, motivating use of LIME to recover human-understandable explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Paper demonstrates that even when classifiers use embeddings or complex features internally, mapping to an interpretable BoW representation for explanations allows humans to inspect and trust predictions; embeddings are used in classifiers but require post-hoc explanation for interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Using interpretable representations (BoW) as the basis for explanations enables inspection and debugging of models that internally use powerful dense embeddings, combining predictive power with human-understandability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>No explicit empirical comparison between BoW and embeddings is provided in this demo text, but both are used across different classifiers shown in the demonstration.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Key is the separation between the classifier's internal representation (possibly embeddings) and the interpretable representation used by LIME for explanations, enabling explanation of otherwise opaque feature spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Interpretable representations (e.g., BoW) can be used as an explanatory interface even when classifiers operate on high-performance dense embeddings, preserving predictive benefits while restoring human-understandable explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '“Why Should I Trust You?”: Explaining the Predictions of Any Classifier', 'publication_date_yy_mm': '2016-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A computational approach to politeness with application to social factors <em>(Rating: 2)</em></li>
                <li>Recursive deep models for semantic compositionality over a sentiment treebank <em>(Rating: 2)</em></li>
                <li>Explaining collaborative filtering recommendations <em>(Rating: 2)</em></li>
                <li>Submodular function maximization <em>(Rating: 2)</em></li>
                <li>Dataset Shift in Machine Learning <em>(Rating: 1)</em></li>
                <li>Least angle regression <em>(Rating: 1)</em></li>
                <li>Leakage in data mining: Formulation, detection, and avoidance <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2358",
    "paper_id": "paper-c0883f5930a232a9c1ad601c978caede29155979",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "LIME",
            "name_full": "Local Interpretable Model-agnostic Explanations",
            "brief_description": "A model-agnostic, post-hoc explanation technique that approximates any classifier locally with an interpretable (sparse linear) model over an interpretable representation to explain individual predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Natural Language Processing — text classification (document/topic classification, sentiment analysis, politeness detection)",
            "problem_description": "Explain why an opaque classifier made a particular prediction by identifying which interpretable components of the input (e.g., words) were most responsible for that prediction, to build human trust and enable model debugging/feature engineering.",
            "data_availability": "Uses publicly available labeled NLP datasets referenced in the paper (20 Newsgroups, polite-sentence dataset, movie-review sentiment dataset); paper does not report dataset sizes here but notes classifiers/datasets were made available by cited authors.",
            "data_structure": "Unstructured text (sentences and documents) represented as interpretable binary indicators (presence/absence of words) or other representations derived from original features (e.g., mapping perturbed samples back to original feature space).",
            "problem_complexity": "Targets complex, potentially high-dimensional and nonlinear classifiers (e.g., SVM with RBF kernel, neural networks) where decision boundaries are opaque; complexity handled by local approximation rather than global modeling.",
            "domain_maturity": "Applied to well-established NLP tasks with existing benchmark datasets and prior models; the paper leverages existing classifiers and datasets from prior work.",
            "mechanistic_understanding_requirements": "High — the method is explicitly developed to provide interpretable, human-understandable mechanistic (local) explanations to support trust, model comparison, and feature engineering.",
            "ai_methodology_name": "Local Interpretable Model-agnostic Explanations (LIME)",
            "ai_methodology_description": "LIME constructs an interpretable representation (e.g., binary vector of word presence), samples perturbed instances around the instance of interest, weights samples by proximity using an exponential kernel, obtains the original classifier's predictions on these samples, and fits a sparse linear model (selecting K features via Lasso/forward selection) to minimize a locally weighted squared loss plus a complexity penalty, producing a K-feature explanation for the single prediction.",
            "ai_methodology_category": "post-hoc model-agnostic interpretability / explainability",
            "applicability": "Applicable across classifier types (the paper demonstrates use with neural networks, SVMs, random forests) and across multiple NLP tasks; designed to work without access to model internals, requiring only ability to query f(z) for perturbed inputs.",
            "effectiveness_quantitative": "Explanation generation performance: the paper reports that for most datasets and classifiers the system can produce an explanation in under three seconds; no further numeric performance metrics for explanation fidelity reported in the demo text here.",
            "effectiveness_qualitative": "Qualitatively effective at surfacing why models make predictions (e.g., revealing reliance on irrelevant words like 'Posting' for topic classification), increasing user understanding and enabling detection of problems like spurious features or data leakage; used in demo tasks for trusting individual predictions, comparing models, and feature cleaning.",
            "impact_potential": "High potential to improve human trust in deployed NLP models, aid model debugging and feature engineering, detect spurious correlations or data leakage, and guide model selection by exposing reasoning behind predictions; generalizable to other domains where interpretable local explanations are needed.",
            "comparison_to_alternatives": "Paper frames LIME as model-agnostic and applicable to any classifier; no detailed empirical comparison to other explanation methods is provided in this demo description.",
            "success_factors": "Success relies on (1) using an interpretable representation understandable to humans (presence/absence of words), (2) local sampling and weighting to ensure fidelity near the instance, (3) sparsity (limiting K features) for human interpretability, and (4) model-agnosticism enabling application to diverse classifiers.",
            "key_insight": "Local, model-agnostic explanations that fit sparse interpretable surrogates to perturbations near an instance can provide actionable and fast insights into complex, high-dimensional NLP models, improving trust and enabling debugging without requiring access to model internals.",
            "uuid": "e2358.0",
            "source_info": {
                "paper_title": "“Why Should I Trust You?”: Explaining the Predictions of Any Classifier",
                "publication_date_yy_mm": "2016-02"
            }
        },
        {
            "name_short": "SP-LIME",
            "name_full": "Submodular Pick for LIME (SP-LIME)",
            "brief_description": "A method to select a small, representative, and non-redundant set of individual-instance explanations (produced by LIME) using a submodular coverage objective so users can form a global understanding of a model within a limited inspection budget.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Natural Language Processing — model interpretability/validation for text classifiers",
            "problem_description": "Given a budget B of explanations a human is willing to examine, select B instances whose local explanations together cover the most important interpretable components (features) without redundancy, to provide a global view of model behavior.",
            "data_availability": "Relies on having a set of instances X (explained with LIME); specific dataset sizes are not reported in the demo description.",
            "data_structure": "Explanation matrix representation (n x d') where rows are instances and columns are interpretable components (e.g., words); underlying data are unstructured text mapped to interpretable components.",
            "problem_complexity": "NP-hard maximization of weighted coverage; problem benefits from submodularity which enables greedy approximation. Complexity arises from large numbers of instances and features needing non-redundant coverage.",
            "domain_maturity": "Method builds on established submodular optimization theory and is applied to mature NLP benchmarks for demonstration.",
            "mechanistic_understanding_requirements": "High — designed to provide humans with a concise global summary of model behavior, thus requiring explanations that are intelligible and cover relevant mechanisms/features.",
            "ai_methodology_name": "SP-LIME (Submodular Pick for LIME)",
            "ai_methodology_description": "Constructs an explanation importance matrix W where each explained instance's local explanation yields absolute weights for interpretable components; defines global component importances I; defines a coverage function that sums importances of components present in a chosen set of instances; solves the pick problem (maximize coverage under budget B) using a greedy algorithm exploiting submodularity to obtain a (1-1/e)-approximation.",
            "ai_methodology_category": "post-hoc model-agnostic interpretability / submodular selection",
            "applicability": "Appropriate when humans can only inspect a limited number of explanations and a representative global understanding is desired; applicable to any setting where LIME-style local explanations are available.",
            "effectiveness_quantitative": "Theoretical approximation guarantee: greedy selection attains a (1 - 1/e) constant-factor approximation to optimal coverage for submodular functions; no empirical numeric coverage values provided in the demo text.",
            "effectiveness_qualitative": "Provides a non-redundant, representative set of explanations that help users form a global understanding of model behavior (used in demo to compare models and surface common problematic features).",
            "impact_potential": "Enables scalable human inspection of model behavior, improving ability to detect systemic issues and to choose between models with limited human attention; generalizable to other interpretability workflows that require summarization.",
            "comparison_to_alternatives": "No empirical comparisons to alternative selection/summarization strategies are provided here, but approach leverages known submodular maximization properties for provable approximation.",
            "success_factors": "Key factors are (1) using absolute local explanation weights to build a global importance measure, (2) formulating coverage as a submodular objective to exploit greedy approximation guarantees, and (3) limiting user budget to make inspection feasible.",
            "key_insight": "Selecting a small set of locally-explained instances via a submodular coverage objective gives users a concise, non-redundant global view of a model, balancing representativeness and human attention constraints.",
            "uuid": "e2358.1",
            "source_info": {
                "paper_title": "“Why Should I Trust You?”: Explaining the Predictions of Any Classifier",
                "publication_date_yy_mm": "2016-02"
            }
        },
        {
            "name_short": "SVM (RBF)",
            "name_full": "Support Vector Machine with RBF kernel",
            "brief_description": "A nonlinear supervised classifier (SVM with radial basis function kernel) used in the demo for topic classification on 20 Newsgroups; achieves high held-out accuracy but may rely on spurious features.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Natural Language Processing — topic/document classification (example: distinguishing Christianity vs Atheism in 20 Newsgroups)",
            "problem_description": "Classify documents into topic categories; example highlighted is distinguishing posts about 'Christianity' vs 'Atheism'.",
            "data_availability": "Uses the 20 Newsgroups dataset (publicly available labeled dataset); exact training/test sizes are not given in the demo text.",
            "data_structure": "Unstructured text (documents) often represented as bag-of-words or other text features; high-dimensional sparse feature vectors are typical.",
            "problem_complexity": "Complex nonlinear decision boundary learned by RBF kernel SVM in high-dimensional feature space; potential for exploiting dataset artifacts or spurious signals.",
            "domain_maturity": "Topic classification on 20 Newsgroups is a well-established benchmark problem in NLP with mature baselines.",
            "mechanistic_understanding_requirements": "Medium to high — although high predictive accuracy may suffice in some contexts, the paper emphasizes need for interpretability to trust and validate model behavior prior to deployment.",
            "ai_methodology_name": "Support Vector Machine (RBF kernel)",
            "ai_methodology_description": "Nonlinear kernel SVM trained on text features (in the demo on 20 Newsgroups); specifics of hyperparameters/training are not provided in the demo description, but the model is queried by LIME to obtain prediction probabilities for perturbed inputs.",
            "ai_methodology_category": "supervised learning",
            "applicability": "Applicable and effective for the demo classification task; however LIME explanations revealed SVM relying on irrelevant tokens ('Posting', 'Host', 'Re') showing that predictive performance alone may be misleading.",
            "effectiveness_quantitative": "Reported held-out accuracy: 94% for the Christianity vs Atheism classifier on the example task.",
            "effectiveness_qualitative": "Although quantitatively accurate (94%), explanations produced by LIME showed the classifier used arbitrary/irrelevant words for decisions, indicating brittle or spurious decision rules despite high accuracy.",
            "impact_potential": "High predictive utility for classification, but limited deployment trust without interpretability; combining with LIME can reveal weaknesses and guide feature cleaning to improve real-world suitability.",
            "comparison_to_alternatives": "Paper uses SVM as one of several classifiers compared via LIME explanations (others include LSTMs and random forests) but provides no numeric comparative performance across classifiers in the demo text besides the single SVM accuracy example.",
            "success_factors": "SVM success driven by capacity to learn nonlinear boundaries; vulnerability highlighted by LIME arises when training data contain spurious signals or artifacts that the SVM can exploit to boost cross-validated accuracy.",
            "key_insight": "High held-out accuracy of nonlinear classifiers (e.g., SVM RBF) can mask reliance on spurious features; local explanations are essential to validate whether a high-performing model learned meaningful, generalizable decision rules.",
            "uuid": "e2358.2",
            "source_info": {
                "paper_title": "“Why Should I Trust You?”: Explaining the Predictions of Any Classifier",
                "publication_date_yy_mm": "2016-02"
            }
        },
        {
            "name_short": "LSTMs",
            "name_full": "Long Short-Term Memory neural networks",
            "brief_description": "Recurrent neural network models (LSTMs) referenced and used in the demo as one of several classifier types for NLP tasks, trained on sentence/document representations such as embeddings.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Natural Language Processing — sentence-level tasks such as sentiment analysis and paraphrastic embeddings",
            "problem_description": "Used as classifiers for NLP tasks (e.g., sentiment analysis) where sequential modeling of text is relevant; in demo, different classifiers including LSTMs are compared via explanations.",
            "data_availability": "Applied on publicly available labeled datasets (e.g., movie-review sentences from Socher et al.); exact dataset sizes or LSTM training details are not reported in this demo description.",
            "data_structure": "Sequential unstructured text (sentences/documents), often converted to dense vector embeddings as inputs to LSTMs.",
            "problem_complexity": "Models are expressive and capture nonlinear sequential dependencies; training complexity depends on network size and data but specifics are not provided.",
            "domain_maturity": "LSTMs are an established approach in NLP; the demo leverages pre-existing classifiers from prior work.",
            "mechanistic_understanding_requirements": "High — due to opacity of neural networks, interpretable explanations (via LIME) are emphasized to build trust and inspect behavior.",
            "ai_methodology_name": "LSTM neural networks",
            "ai_methodology_description": "Recurrent neural network classifiers (LSTMs) trained on sentence/document representations such as word embeddings; the demo uses such models when comparing classifiers with LIME explanations, but provides no architecture/training hyperparameters in the demo text.",
            "ai_methodology_category": "supervised learning (deep learning)",
            "applicability": "Appropriate for sequence-based NLP tasks and shown as one of multiple model types whose predictions can be explained by LIME; no specific limitations described in this demo text.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Used to demonstrate that LIME can explain highly nonlinear and opaque models like LSTMs; no quantitative results reported here for LSTM performance.",
            "impact_potential": "When combined with LIME explanations, deep sequence models' decisions become inspectable, enabling trust and iterative improvement in applications like sentiment analysis and paraphrastic embeddings.",
            "comparison_to_alternatives": "Compared qualitatively via LIME explanations against other classifiers (SVMs, random forests) in demo tasks; no quantitative comparative metrics provided in this description.",
            "success_factors": "Expressive sequence modeling capability of LSTMs; the ability to be queried by LIME for local explanation is crucial for interpretability in the demo workflow.",
            "key_insight": "Opaque deep sequence models (LSTMs) can be made interpretable locally through model-agnostic explanation methods, enabling their use in trust-sensitive NLP tasks without requiring intrinsic interpretability.",
            "uuid": "e2358.3",
            "source_info": {
                "paper_title": "“Why Should I Trust You?”: Explaining the Predictions of Any Classifier",
                "publication_date_yy_mm": "2016-02"
            }
        },
        {
            "name_short": "Random Forests",
            "name_full": "Random Forest classifiers",
            "brief_description": "An ensemble tree-based supervised learning method used in the demo as one of several classifier types applied to NLP classification tasks and explainable with LIME.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Natural Language Processing — document/sentence classification",
            "problem_description": "Used as a classifier for NLP tasks in the demo; included to illustrate LIME's applicability across diverse model families.",
            "data_availability": "Trained on publicly available labeled NLP datasets referenced in the paper; specific training set sizes or data quality metrics are not provided in the demo text.",
            "data_structure": "Unstructured text transformed into feature vectors (e.g., bag-of-words, embeddings) for tree-based modeling; input can be high-dimensional and sparse.",
            "problem_complexity": "Ensemble of decision trees can model nonlinear interactions across features; interpretability of raw model is limited when ensemble is large.",
            "domain_maturity": "Random forests are a standard, well-established supervised learning method used widely in NLP and other domains.",
            "mechanistic_understanding_requirements": "Medium to high — paper emphasizes need for interpretability regardless of model class; LIME is used to produce human-understandable explanations for random forest predictions.",
            "ai_methodology_name": "Random forests (ensemble of decision trees)",
            "ai_methodology_description": "Ensemble classifier composed of multiple decision trees trained on text-derived features; in demo used alongside other classifiers and explained via LIME, with no further hyperparameter or training detail in the demo text.",
            "ai_methodology_category": "supervised learning (ensemble methods)",
            "applicability": "Applicable to text classification tasks and compatible with LIME since only model predictions are needed to generate local explanations.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Serves as one of multiple model families whose opaque predictions can be inspected by LIME; no detailed qualitative performance evaluation provided here.",
            "impact_potential": "Combining standard interpretability-free models like random forests with LIME increases their trustworthiness and utility in human-in-the-loop workflows for NLP.",
            "comparison_to_alternatives": "Included in qualitative comparisons via LIME explanations against other classifiers (SVMs, LSTMs) in the demo; numeric comparisons not provided in the text.",
            "success_factors": "Ability of LIME to operate without access to model internals is key to making ensemble models inspectable; standard generalization properties of random forests contribute to baseline performance.",
            "key_insight": "Model-agnostic explanation tools make ensemble models (random forests) usable in trust-sensitive NLP applications by revealing local decision drivers without modifying the underlying model.",
            "uuid": "e2358.4",
            "source_info": {
                "paper_title": "“Why Should I Trust You?”: Explaining the Predictions of Any Classifier",
                "publication_date_yy_mm": "2016-02"
            }
        },
        {
            "name_short": "BoW & Embeddings",
            "name_full": "Bag-of-Words and Word Embedding representations",
            "brief_description": "Input feature representations for NLP classifiers used in the demo: sparse bag-of-words vectors and dense word embeddings (including sentence embeddings), which feed into classifiers like SVMs, LSTMs, and random forests.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Natural Language Processing — feature representations for text classification tasks",
            "problem_description": "Represent raw text as features for supervised classifiers: bag-of-words (interpretable binary presence/absence indicators) or distributed word/sentence embeddings (dense continuous vectors).",
            "data_availability": "Representations derived from publicly available labeled datasets used in the demo; no further info on pretraining corpora or embedding dimensionality is provided in the demo text.",
            "data_structure": "Unstructured text mapped to either high-dimensional sparse vectors (BoW) or dense continuous vectors (word/sentence embeddings); both are used depending on classifier type.",
            "problem_complexity": "BoW yields very high-dimensional sparse inputs; embeddings create dense lower-dimensional spaces but may encode complex semantic relationships; both impact classifier complexity.",
            "domain_maturity": "Both BoW and embedding representations are mature and standard in NLP; embeddings (and sentence embeddings) referenced via prior work (e.g., Wieting et al.).",
            "mechanistic_understanding_requirements": "Medium — BoW representations are interpretable facilitating LIME explanations, whereas embeddings are less interpretable, increasing the need for post-hoc explanation to understand model decisions.",
            "ai_methodology_name": "Bag-of-Words (BoW) and Word/Sentence Embeddings",
            "ai_methodology_description": "BoW: interpretable binary or count features indicating word presence/absence; Embeddings: dense vectors representing words or sentences (e.g., paraphrastic sentence embeddings) used as inputs to classifiers like LSTMs and neural nets; LIME operates over interpretable representations (e.g., binary word presence) even when classifier uses embeddings.",
            "ai_methodology_category": "feature representation for supervised learning",
            "applicability": "BoW is directly interpretable and suitable for LIME's interpretable representation; embeddings improve predictive performance for neural models but reduce direct interpretability, motivating use of LIME to recover human-understandable explanations.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Paper demonstrates that even when classifiers use embeddings or complex features internally, mapping to an interpretable BoW representation for explanations allows humans to inspect and trust predictions; embeddings are used in classifiers but require post-hoc explanation for interpretability.",
            "impact_potential": "Using interpretable representations (BoW) as the basis for explanations enables inspection and debugging of models that internally use powerful dense embeddings, combining predictive power with human-understandability.",
            "comparison_to_alternatives": "No explicit empirical comparison between BoW and embeddings is provided in this demo text, but both are used across different classifiers shown in the demonstration.",
            "success_factors": "Key is the separation between the classifier's internal representation (possibly embeddings) and the interpretable representation used by LIME for explanations, enabling explanation of otherwise opaque feature spaces.",
            "key_insight": "Interpretable representations (e.g., BoW) can be used as an explanatory interface even when classifiers operate on high-performance dense embeddings, preserving predictive benefits while restoring human-understandable explanations.",
            "uuid": "e2358.5",
            "source_info": {
                "paper_title": "“Why Should I Trust You?”: Explaining the Predictions of Any Classifier",
                "publication_date_yy_mm": "2016-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A computational approach to politeness with application to social factors",
            "rating": 2
        },
        {
            "paper_title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "rating": 2
        },
        {
            "paper_title": "Explaining collaborative filtering recommendations",
            "rating": 2
        },
        {
            "paper_title": "Submodular function maximization",
            "rating": 2
        },
        {
            "paper_title": "Dataset Shift in Machine Learning",
            "rating": 1
        },
        {
            "paper_title": "Least angle regression",
            "rating": 1
        },
        {
            "paper_title": "Leakage in data mining: Formulation, detection, and avoidance",
            "rating": 1
        }
    ],
    "cost": 0.015795,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>"Why Should I Trust You?" Explaining the Predictions of Any Classifier</h1>
<p>Marco Tulio Ribeiro<br>University of Washington<br>Seattle, WA 98105, USA<br>marcotcr@cs.uw.edu</p>
<p>Sameer Singh<br>University of Washington<br>Seattle, WA 98105, USA<br>sameer@cs.uw.edu</p>
<p>Carlos Guestrin<br>University of Washington<br>Seattle, WA 98105, USA<br>guestrin@cs.uw.edu</p>
<h4>Abstract</h4>
<p>Despite widespread adoption in NLP, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust in a model. Trust is fundamental if one plans to take action based on a prediction, or when choosing whether or not to deploy a new model. In this work, we describe LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner. We further present a method to explain models by presenting representative individual predictions and their explanations in a non-redundant manner. We propose a demonstration of these ideas on different NLP tasks such as document classification, politeness detection, and sentiment analysis, with classifiers like neural networks and SVMs. The user interactions include explanations of free-form text, challenging users to identify the better classifier from a pair, and perform basic feature engineering to improve the classifiers.</p>
<h2>1 Introduction</h2>
<p>Machine learning is at the core of many recent advances in natural language processing. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models into products that need to be shipped, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e. whether
a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed "in the wild". Both are directly impacted by how much the human understands a model's behavior, as opposed to seeing it as a black box. Recent resurgence of neural networks has resulted in state-of-art models whose working is quite opaque to the user, exacerbating this problem.</p>
<p>A common surrogate for ascertaining trust in a model is to evaluate accuracy on held-out annotated data. However, there are several ways this evaluation can go wrong. Data leakage, for example, defined as the unintentional leakage of signal into the training (and validation) data that would not occur in the wild (Kaufman et al., 2011), potentially increases accuracy. Practitioners are also known to overestimate the accuracy of their models based on cross validation (Patel et al., 2008), as real-world data is often significantly different. Another particularly hard to detect problem is dataset shift (Candela et al., 2009), where training data is different than test data. Further, there is frequently a mismatch between that metrics that we can compute and optimize (e.g. accuracy) and the actual metrics of interest such as user engagement and retention. A practitioner may wish to choose a less accurate model for content recommendation that does not place high importance in features related to "clickbait" articles (which may hurt user retention), even if exploiting such features increases the accuracy of the model in cross validation.</p>
<p>In this paper, we describe a system that explains why a classifier made a prediction by identifying useful portions of the input. It has been observed</p>
<p>that providing an explanation can increase the acceptance of computer-generated movie recommendations (Herlocker et al., 2000) and other automated systems (Dzindolet et al., 2003), and we explore their utility for NLP. Specifically, we present:</p>
<ul>
<li>LIME, an algorithm that can explain the predictions of any classifier, by approximating it locally with an interpretable model.</li>
<li>SP-LIME, a method that selects a set of representative explanations to address the "trusting the model" problem, via submodular optimization.</li>
<li>A demonstration designed to present the benefits of these explanation methods, on multiple NLP classification applications, classifier algorithms, and trust-related tasks.</li>
</ul>
<h2>2 Explaining Predictions and Models</h2>
<p>By "explaining a prediction", we mean presenting visual artifacts that provide qualitative understanding of the relationship between the instance's components (e.g. words in text) and the model's prediction. Explaining predictions is an important aspect in getting humans to trust and use machine learning effectively, provided the explanations are faithful and intelligible. We summarize the techniques here; further details and experiments are available in Ribeiro et al. (2016).</p>
<h2>Local Interpretable Model-Agnostic Explanations</h2>
<p>We present Local Interpretable Model-agnostic Explanations (LIME). The overall goal of LIME is to identify an interpretable model over the interpretable representation that is locally faithful to predictions of any classifier. It is important to distinguish between features and interpretable data representations, the latter is a representation that is understandable to humans, regardless of the actual features used by the model. A possible interpretable representation for text is a binary vector indicating the presence or absence of a word, even though the classifier may use more complex (and incomprehensible) features such as word embeddings. We denote $x \in \mathbb{R}^{d}$ as the original instance, and $x^{\prime} \in{0,1}^{d^{\prime}}$ to denote a binary vector for its interpretable representation.</p>
<p>Formally, we define an explanation as a model $g \in G$, where $G$ is the class of linear models,
such that $g\left(z^{\prime}\right)=w_{g} \cdot z^{\prime}$ Note that $g$ acts over absence/presence of the interpretable components, i.e. we can readily present it to the user with visual artifacts. We let $\Omega(g)$ be a measure of complexity (as opposed to interpretability) of the explanation $g \in G$. For text classification, we set a limit $K$ on the number of words included, i.e. $\Omega(g)=\infty \mathbb{1}\left[\left|w_{g}\right|_{0}&gt;K\right]$.</p>
<p>Let the model being explained be denoted $f$, i.e. $f(x)$ is the probability (or a binary indicator) that $x$ belongs to a certain class. We further use $\Pi_{x}(z)$ as a proximity measure between an instance $z$ to $x$, so as to define locality around $x$. Finally, let $\mathcal{L}\left(f, g, \Pi_{x}\right)$ be a measure of how unfaithful $g$ is in approximating $f$ in the locality defined by $\Pi_{x}$. We use the locally weighted square loss as $\mathcal{L}$, as defined in Eq. (1), where we let $\Pi_{x}(z)=\exp \left(-D(x, z)^{2} / \sigma^{2}\right)$ be an exponential kernel on cosine distance $D$.</p>
<p>$$
\mathcal{L}\left(f, g, \Pi_{x}\right)=\sum_{z, z^{\prime} \in \mathcal{Z}} \Pi_{x}(z)\left(f(z)-g\left(z^{\prime}\right)\right)^{2}
$$</p>
<p>In order to ensure both interpretability and local fidelity, we minimize $\mathcal{L}\left(f, g, \Pi_{x}\right)$ while having $\Omega(g)$ be low enough to be interpretable by humans.</p>
<p>$$
\xi(x)=\operatorname{argmin}<em x="x">{g \in G} \mathcal{L}\left(f, g, \Pi</em>\right)+\Omega(g)
$$</p>
<p>We approximate $\mathcal{L}\left(f, g, \Pi_{x}\right)$ by drawing samples, weighted by $\Pi_{x}$. Given a sample $z^{\prime} \in{0,1}^{d^{\prime}}$ (which contains a fraction of the nonzero elements of $x^{\prime}$ ), we recover the sample in the original representation $z \in R^{d}$ and obtain $f(z)$, which is used as a label for the explanation model. Given this dataset $\mathcal{Z}$ of perturbed samples with the associated labels, we optimize Eq. (2) to get an explanation $\xi(x)$ by first selecting $K$ features with Lasso (Efron et al., 2004), forward selection or some other method, and then learning the weights via least squares.</p>
<h2>Submodular Pick for Explaining Models</h2>
<p>Although an explanation of a single prediction provides some understanding into the reliability of the classifier to the user, it is not sufficient to evaluate and assess trust in the model as a whole. We propose to give a global understanding of the model by explaining a set of individual instances. Even though explanations of multiple instances can be insightful, these instances need to be selected judiciously, since users may not have the time to examine a large number of explanations. We represent the number of</p>
<p>explanations humans are willing to look at a budget $B$, i.e. given a set of instances $X$, we select $B$ explanations for the user to inspect. We construct an $n \times d^{\prime}$ explanation matrix $\mathcal{W}$ that represents the local importance of the interpretable components for each instance, i.e. for an instance $x_{i}$ and explanation $g_{i}=\xi\left(x_{i}\right)$, we set $\mathcal{W}<em g__i="g_{i">{i}=\left|w</em>$.}}\right|$. Further, for each component $j$ in $\mathcal{W}$, we let $I_{j}$ denote the global importance, $I_{j}=\sqrt{\sum_{i=1}^{n} \mathcal{W}_{i j}</p>
<p>While we want to pick instances that cover the important components, the set of explanations must not be redundant in the components they show the users, i.e. avoid selecting instances with similar explanations. We formalize this non-redundant coverage intuition in Eq. (3), where coverage $\mathcal{C}$, given $\mathcal{W}$ and $I$, computes the total importance of the features that appear in at least one instance in a set $V$.</p>
<p>$$
\mathcal{C}(V, \mathcal{W}, I)=\sum_{j=1}^{d^{\prime}} \mathbb{1}<em i="i" j="j">{\left[\exists i \in V: \mathcal{W}</em>
$$}&gt;0\right]} I_{j</p>
<p>The pick problem thus consists of finding the set $V,|V| \leq B$ that achieves highest coverage.</p>
<p>$$
\operatorname{Pick}(\mathcal{W}, I)=\operatorname{argmax}_{V,|V| \leq B} \mathcal{C}(V, \mathcal{W}, I)
$$</p>
<p>The problem in Eq. (4) is maximizing a weighted coverage function, and is NP-hard (Feige, 1998). Due to submodularity, a greedy algorithm that iteratively adds the instance with the highest coverage gain offers a constant-factor approximation guarantee of $1-1 / e$ to the optimum (Krause and Golovin, 2014).</p>
<h2>3 Demo Outline</h2>
<p>Using this explanation system that is capable of providing visual explanations for predictions of any classifier, we present an outline for a demonstration using different NLP tasks, models, and user interactions. The complete source code and documentation for installing and running the demonstration is available at https://github. com/uw-mode/naacl16-demo, which uses the code for explaining classifiers available as an open-source python implementation at https:// github.com/marcotcr/1ime.</p>
<h2>Applications</h2>
<p>We explore three NLP classification tasks, which differ in the types of text they apply to and predicted cat-
egories: politeness detection for sentences (Danescu-Niculescu-Mizil et al., 2013), multi-class content classification for documents (20 newsgroups data), and sentiment analysis of sentences from movie reviews (Socher et al., 2013). We explore classifiers for these tasks that vary considerably in their underlying representation, such as LSTMs (Wieting et al., 2015), SVMs, and random forests, trained on bag of words or on word embeddings.</p>
<p>We will outline the specific user interactions using a running example of the 20 newsgroups dataset, and the interfaces for the other applications will look similar. In particular, we focus on differentiating between "Christianity" from "Atheism", and use an SVM with an RBF kernel here. Although this classifier achieves $94 \%$ held-out accuracy, and one would be tempted to trust it based on this, the explanation for an instance shows that predictions are made for quite arbitrary reasons (words "Posting", "Host" and "Re" have no connection to either Christianity or Atheism).</p>
<h3>3.1 Explaining Individual Predictions</h3>
<p>The first part of the demo focuses on explaining individual predictions. Given an instance that the user either selects from the dataset or writes their own piece of text, we provide the set of words that are important for the prediction according to the classifier of their choosing. The interface for the user is shown in Figure 1, which embedded in an iPython notebook. For most datasets and classifiers, our current system can produce an explanation in under three seconds, and some simple further optimizations can be used to produce near-instant explanations.</p>
<h3>3.2 Explaining and Comparing Models</h3>
<p>For the second part of the demonstration, we provide explanations of different models in order to compare them, based on explanations of a few selected instances. The interface presents the explanations one at a time, similar to that in Figure 1. By default the instances are selected using our submodular approach (SP-lime), however we also allow users to write their own text as well, and produce explanations for the classifiers for comparison.</p>
<h3>3.3 Improving Classifiers</h3>
<p>As the final demonstration, we consider a simple version of feature engineering. Specifically, we initiate</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example explanation for an instance of document classification. The bar chart represents the importance given to the most relevant words by the classifier, also highlighted in the text. Color indicates which class the word is important for (orange for "Christianity", blue for "Atheism").</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Interface for feature cleaning, a simple version of feature engineering where users select words to remove from the model by clicking on them (indicated by red, struck-out text). Here, green bars indicate importance of the word for "Christianity", and magenta "Atheism".
each user with a classifier trained using all features, including both noisy and correct ones. We then show explanations of the classifier to the users, and ask them to select which words to remove from the classifier (see Figure 2 for the interface). Given this feedback, we retrain the classifier and provide the users with a score of how well their classifier performed on a hidden set, along with a leader board of the accuracy of all the participants.</p>
<h2>4 Conclusions</h2>
<p>We argue that trust is crucial for effective human interaction with machine learning based NLP systems, and that explaining individual predictions is important in assessing trust. We present a demonstration for LIME, a modular and extensible approach to faithfully explain the predictions of any model in
an interpretable manner, and SP-LIME, a method to select representative and non-redundant explanations, providing a global view of the model to users. The user interactions on multiple applications and classifiers span a variety of trust-related tasks: getting insights into predictions, deciding between models, and improving untrustworthy models.</p>
<h2>Acknowledgments</h2>
<p>We would like to thank Kevin Gimpel, John Wieting, and Cristian Danescu-Niculescu-Mizil for making their classifiers and datasets available for use. This work was supported in part by the TerraSwarm Research Center, one of six centers supported by the STARnet phase of the Focus Center Research Program (FCRP) a Semiconductor Research Corporation program sponsored by MARCO and DARPA.</p>
<h2>References</h2>
<p>J. Quiñonero Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. 2009. Dataset Shift in Machine Learning. MIT.
Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure Leskovec, and Christopher Potts. 2013. A computational approach to politeness with application to social factors. In Proceedings of ACL.
Mary T. Dzindolet, Scott A. Peterson, Regina A. Pomranky, Linda G. Pierce, and Hall P. Beck. 2003. The role of trust in automation reliance. Int. J. Hum.Comput. Stud., 58(6).
Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. 2004. Least angle regression. Annals of Statistics, 32:407-499.
Uriel Feige. 1998. A threshold of $\ln \mathrm{n}$ for approximating set cover. J. ACM, 45(4), July.
Jonathan L. Herlocker, Joseph A. Konstan, and John Riedl. 2000. Explaining collaborative filtering recommendations. In Conference on Computer Supported Cooperative Work (CSCW).
Shachar Kaufman, Saharon Rosset, and Claudia Perlich. 2011. Leakage in data mining: Formulation, detection,
and avoidance. In Knowledge Discovery and Data Mining (KDD).
Andreas Krause and Daniel Golovin. 2014. Submodular function maximization. In Tractability: Practical Approaches to Hard Problems. Cambridge University Press, February.
Kayur Patel, James Fogarty, James A. Landay, and Beverly Harrison. 2008. Investigating statistical machine learning as a tool for software development. In Human Factors in Computing Systems (CHI).
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why should I trust you?": Explaining the predictions of any classifier. CoRR, abs/1602.04938.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631-1642, Stroudsburg, PA, October. Association for Computational Linguistics.
John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2015. Towards universal paraphrastic sentence embeddings. CoRR, abs/1511.08198.</p>            </div>
        </div>

    </div>
</body>
</html>
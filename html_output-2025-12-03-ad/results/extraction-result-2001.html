<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2001 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2001</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2001</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-46.html">extraction-schema-46</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <p><strong>Paper ID:</strong> paper-280536411</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.03929v1.pdf" target="_blank">MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework</a></p>
                <p><strong>Paper Abstract:</strong> Designing effective algorithmic components remains a fundamental obstacle in tackling NP-hard combinatorial optimization problems (COPs), where solvers often rely on carefully hand-crafted strategies. Despite recent advances in using large language models (LLMs) to synthesize high-quality components, most approaches restrict the search to a single element - commonly a heuristic scoring function - thus missing broader opportunities for innovation. In this paper, we introduce a broader formulation of solver design as a multi-strategy optimization problem, which seeks to jointly improve a set of interdependent components under a unified objective. To address this, we propose Multi-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a novel framework based on Monte Carlo Tree Search that facilitates turn-based optimization between two LLM agents. At each turn, an agent improves one component by leveraging the history of both its own and its opponent's prior updates, promoting both competitive pressure and emergent cooperation. This structured interaction broadens the search landscape and encourages the discovery of diverse, high-performing solutions. Experiments across multiple COP domains show that MOTIF consistently outperforms state-of-the-art methods, highlighting the promise of turn-based, multi-agent prompting for fully automated solver design.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2001.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2001.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOTIF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-strategy Optimization via Turn-based Interactive Framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-phase, turn-based competitive optimization framework that uses two LLM agents (self-play) and Competitive MCTS to jointly optimize multiple interdependent algorithmic components via three LLM-driven operators (counter, learning, innovation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MOTIF</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based (turn-based operators: counter, learning, innovation)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Two LLM agents (both using gpt-4o-mini-2024-07-18 in experiments) alternately propose code implementations for individual strategies inside a Competitive Monte Carlo Tree Search. At each node an operator (counter: exploit opponent weaknesses; learning: synthesize opponent strengths; innovation: propose novel implementations) is selected and the LLM is prompted with the active player's code, the opponent's code, recent histories, and a baseline. New implementations are executed and evaluated on a training set; rewards combine absolute improvement and competitive gain. No model fine-tuning is performed — all adaptation is via prompt/context and dynamic baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Optimization is driven by synthetic, domain-specific D_train and evaluated on held-out D_test. The paper reports a typical experimental dataset pair per domain (D_train and D_test) generated uniformly as described for each COP; Table 6 indicates training/test set sizes used in experiments (paper lists #Train ≈ 564 and #Test ≈ 64 for many benchmarks). Training instances are intentionally smaller/lighter than test instances to speed evaluation during search.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Multiple combinatorial optimization domains: Traveling Salesman Problem (TSP), Capacitated Vehicle Routing Problem (CVRP), Multiple Knapsack Problem (MKP), Orienteering Problem (OP), Bin Packing Problem (BPP); frameworks: ACO, GLS, Deconstruction-then-Repair (DR).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human-designed baseline heuristics, and recent LLM-based AHD baselines: EoH (Liu et al. 2024), ReEvo (Ye et al. 2024), HSEvo (Dat et al. 2025), MCTS-AHD (Zheng et al. 2025); also compared against fixed single-strategy optimization of GLS/ACO and neural methods are discussed in related work (Neural LNS, NeuroLKH, DeepACO).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>Reported as optimality gap (%) vs human baseline. Example (GLS single-strategy TSP optimality gap): MOTIF: TSP50=0.0000 ± 0.0000, TSP100=0.0007 ± 0.0006, TSP200=0.0610 ± 0.0100, TSP500=0.5577 ± 0.0255 (values taken from Table 1; metric reported in paper as optimality gap %; lower is better). The paper states MOTIF consistently outperforms prior LLM-based methods across domains and scales (see Figure 3 and Tables 1–2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td>No direct experimental comparison to traditional genetic programming (GP) operators (e.g., subtree crossover/mutation) is reported. The paper compares against human-designed heuristics (baseline) but does not provide GP-operator numerical results.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td>MOTIF is compared to other hybrid/LLM+search approaches (e.g., EoH, ReEvo, HSEvo, MCTS-AHD) in the experiments; reported baseline numbers for those methods appear in Table 1 (see follow-on entries). Specific hybrid-operator numerical rows are reported for those baselines (see their entries) but MOTIF is the best-performing method reported in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td>The paper reports 'success rate' per operator (defined as proportion of generated implementations that improve upon the current baseline): Counter 93% ± 2%, Learning 92% ± 2%, Innovation 97% ± 1% (Table 3). The paper does not explicitly report code compilation/parse success rates separate from success/improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td>Novelty and silhouette computed on code embeddings (CodeT5p-110m-embedding). Table 3: Counter: novelty 0.0136 ± 0.0067, silhouette 0.5121 ± 0.0208; Learning: novelty 0.0118 ± 0.0054, silhouette 0.5325 ± 0.0300; Innovation: novelty 0.0175 ± 0.0110, silhouette 0.4793 ± 0.0240. Innovation highest novelty but lowest silhouette (more diverse but less cohesive).</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>The framework optimizes on a lighter/smaller D_train and evaluates final implementations on held-out D_test (larger/harder). Results show a measurable gap between train and test performance but MOTIF generalizes better than ablated variants. Example from ablation (Table 4): MOTIF (original) ACO train gap 0.80 ± 0.28, test gap 5.21 ± 0.35; GLS train 0.34 ± 0.19, test 2.73 ± 0.05 — showing some degradation but reasonable test performance. No extensive OOD domain transfer experiments are provided beyond held-out test instances.</td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>Paper intentionally uses smaller D_train than D_test to speed optimization and reports generalization to D_test; ablations show removing dynamic baseline or reasoning components increases test-phase degradation (e.g., w/o dynamic baseline shows much larger test gap), suggesting sensitivity to optimization protocol rather than explicit dataset bias. The paper does not provide direct evidence of LLM-generated operator bias toward training-instance patterns beyond these observations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>Authors report computational costs for MOTIF runs: a typical ACO (TSP) run takes about 1.5 hours and consumes ~0.5M input tokens and ~0.2M output tokens using gpt-4o-mini-2024-07-18, costing roughly $0.18 per run (Table 7). The paper does not report direct wall-clock or monetary comparisons against classical GP runs or other baselines' compute costs.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not tested — no experiments where learned operators are transferred across problem domains are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td>Not compared — a single general LLM (gpt-4o-mini-2024-07-18) is used for all experiments; no domain-specialized model or fine-tuning comparison is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Comprehensive ablations in Table 4: removing dynamic baseline causes the largest degradation (particularly in test), removing reasoning reduces performance, removing operators (counter, learning, innovation) each affects performance differently (e.g., w/o learning increases test gap in some settings). Removing final round or outer controller also hurts results. Key finding: dynamic baseline and reasoning fields are important for robust gains and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>Operators' output spaces are characterized using semantic code embeddings (CodeT5p-110m) and measured via novelty and silhouette to quantify inter-operator diversity and intra-operator cohesion. Innovation explores broader semantic regions (high novelty) but with scattered cluster structure (low silhouette); learning yields cohesive, consistent outputs (high silhouette, low novelty).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Adaptation occurs via prompt-context and dynamic baseline updates during search (no model fine-tuning). Agents incorporate active player and opponent histories in prompts so generated operators adapt during the MCTS search process (online prompt adaptation). This prompt/contextual adaptation is shown to improve results vs ablations where histories or dynamic baseline are removed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Innovation operator produces diverse but internally inconsistent outputs (low silhouette). Disabling dynamic baseline leads to stagnation and poor test performance. Limiting reasoning (they deliberately pick a cheaper LLM with limited coding/reasoning ability) degrades quality, indicating sensitivity to the LLM's reasoning capacity. The paper also notes LLM-generated code non-determinism and stochasticity in improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Turn-based self-play between two LLM agents over multiple interdependent strategies expands the search space and yields better-performing solver components than single-strategy LLM methods and human baselines. Dynamic baseline updates and explicit opponent history (self-play context) are critical for driving continual improvement and avoiding stagnation. Different learned operators occupy complementary parts of the hypothesis space: 'learning' exploits and consolidates consistent improvements, 'counter' focuses on targeted adversarial fixes, and 'innovation' expands exploration (but with lower internal cohesion). Training on smaller/light D_train enables fast inner-loop evaluation but requires system-level safeguards (dynamic baseline and final system-aware refinement) to generalize to harder D_test instances. Computational cost for LLM-driven search can be modest when using smaller LLMs and light train sets, but no direct cost comparison to GP is provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2001.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2001.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EoH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evolution of Heuristics (EoH)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven evolutionary approach where GPT-4 synthesizes scoring functions and evolutionary selection (tournaments) evolves heuristics over generations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>EoH</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based integrated with evolutionary search</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>LLMs (GPT-4 in the referenced work) generate candidate heuristic functions which are then evaluated and selected using evolutionary tournament selection across generations; LLM acts as generator/mutator in an evolutionary backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper (referenced work likely uses domain-specific instance sets); the MOTIF paper only cites EoH as a baseline in related work/experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Combinatorial optimization (cited in context of AHD); used as a baseline in TSP/GLS comparisons in MOTIF experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in MOTIF experiments against human baseline and MOTIF. Table 1 includes EoH as a competing method.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>Reported in Table 1 (as a baseline) but table formatting in the paper makes direct mapping to sizes ambiguous; EoH appears in the baselines shown in Figure 3/Table 1 and is outperformed by MOTIF. The paper does not excerpt a clear per-size numeric summary for EoH beyond what's included in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Yes — adaptation via evolutionary selection across generations; LLM acts as generator for mutation/crossover-like variation within evolutionary loop (as described in referenced EoH work).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>EoH exemplifies integration of LLM generation with population-based evolutionary search; MOTIF positions itself as a more structured multi-strategy, turn-based alternative that outperforms single-component evolutionary LLM approaches in the reported experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2001.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2001.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReEvo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflective Evolution (ReEvo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-augmented evolutionary framework that augments evolutionary search with self-reflection (LLM critiques) to improve candidate heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReEvo</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based + evolutionary (reflective prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Combines evolutionary search with LLM-based self-reflection (post-hoc critique and repair) to refine heuristics generated during evolution. The LLM is used both to generate candidates and to reflect on/repair poor outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Automatic Heuristic Design for COPs (cited and used as baseline in comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared as a baseline in MOTIF experiments (Table 1 / Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>Included in Table 1 as a baseline with per-size numbers (e.g., ReEvo shows larger optimality gaps than MOTIF on some sizes), but exact mapping to sizes is given in Table 1 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Yes — uses LLM reflection to adapt candidate heuristics across evolutionary iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>ReEvo demonstrates that self-reflection can improve LLM-generated heuristics; MOTIF builds on self-play/reflection but emphasizes multi-component and adversarial/cooperative dynamics to further enhance discovery.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2001.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2001.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HSEvo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HSEvo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diversity-driven harmony search and genetic algorithm approach that leverages LLMs to maintain a diverse population of heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HSEvo</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-driven evolutionary/harmony search (diversity-aware)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Incorporates LLMs into a harmony-search-style evolutionary framework emphasizing diversity maintenance and creative heuristic generation; LLMs generate candidate heuristics that are integrated into population-based search.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Automatic Heuristic Design; included as a baseline in MOTIF experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against MOTIF and other LLM-based AHD methods in Table 1/Figure 3.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>Table 1 reports HSEvo numbers (e.g., HSEvo TSP optimality gaps appear larger than MOTIF's values shown), but exact mapping is in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Yes — population diversity metrics and harmony search mechanics provide adaptation dynamics in the evolutionary loop.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>HSEvo emphasizes the role of diversity in generating complementary heuristics; MOTIF's operators (especially innovation) are intended to play a similar role but within a turn-based self-play MCTS scaffold.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2001.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2001.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MCTS-AHD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MCTS-AHD (Monte Carlo Tree Search for Automatic Heuristic Design)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An MCTS-based AHD method that uses progressive widening to explore a search tree of heuristics generated by LLM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MCTS-AHD</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based + MCTS (progressive widening)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Applies MCTS (with progressive widening) where nodes represent partial/complete heuristic configurations and child nodes are instantiated by prompting an LLM to revise/extend implementations; selection guided by performance metrics and backpropagation.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Automatic Heuristic Design; used as a baseline in MOTIF experiments (noted as prior MCTS+LLM work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against MOTIF in Table 1/Figure 3; baseline is single-strategy LHH methods and human heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>Table 1 reports MCTS-AHD optimality gaps for GLS/TSP (e.g., MCTS-AHD: TSP50=0.0000 ± 0.0000, TSP100=0.0024 ± 0.0007, TSP200=0.0652 ± 0.0111, TSP500=0.5611 ± 0.0216). MOTIF slightly outperforms these numbers in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>MCTS-AHD imposes hierarchy on search and progressive widening to control branching — MOTIF extends this by introducing two-player competitive MCTS with operator-level UCB.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Adaptation via MCTS selection/backpropagation and progressive widening of the search tree; LLM prompts instantiate child nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>MCTS-AHD demonstrates the feasibility of combining MCTS with LLM generation for heuristic design; MOTIF generalizes this idea to multi-strategy and adversarial/cooperative dynamics and reports improved empirical performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2001.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2001.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPHH / GP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genetic Programming Hyper-Heuristic (GPHH) / Genetic Programming</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generation-based hyper-heuristics that evolve executable heuristic programs (symbolic expressions) using genetic programming operators (mutation, crossover, selection).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Genetic Programming Hyper-Heuristic (GPHH)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>traditional GP (mutation/crossover/selection), symbolic/programmatic heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Classical GP evolves populations of symbolic heuristics via mutation and crossover operators; used historically to synthesize heuristics as executable programs in hyper-heuristic frameworks (cited works: Langdon & Poli 2013; Burke et al. 2009/2010b). The MOTIF paper references GPHH as foundational AHD work but does not run GP experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Classical hyper-heuristic applications: scheduling, packing, routing; historically applied to strip packing and related COPs (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned for historical context; not used as an experimental baseline in MOTIF's reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>GP adapts via population evolution (mutation/crossover) over generations; MOTIF notes GP as a prior population-based mechanism that LLMs can plug into as generators or mutators.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>The paper positions GP/GPHH as a conceptual predecessor to LLM-driven AHD, noting that prior GP approaches relied on predefined heuristic spaces and handcrafted rules; MOTIF aims to expand beyond these limits by leveraging LLM creativity and multi-strategy search.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2001.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2001.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural methods (Neural LNS / NeuroLKH / DeepACO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Large Neighborhood Search (Neural LNS), NeuroLKH, DeepACO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representative neural/data-driven methods that learn components of classical solvers: Neural LNS jointly learns destroy/repair policies; NeuroLKH learns edge-scoring and node-penalty functions for LKH; DeepACO uses neural networks to guide ACO construction/prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural LNS / NeuroLKH / DeepACO</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>neural network-based (learned policies/value/edge-scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Neural LNS: joint RL/GNN-learned destroy and repair policies; NeuroLKH: learned edge-scoring/node-penalty functions to guide Lin-Kernighan heuristic; DeepACO: neural networks predict construction or pheromone guidance for Ant Colony frameworks. These are discussed in related work as examples where modular routine optimization yields gains.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in MOTIF; these methods typically train on problem-instance distributions (cited works).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Combinatorial optimization (TSP, VRP, etc.); cited as evidence that modular learned components can outperform single-rule tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Discussed in related work; not directly compared experimentally vs MOTIF in the paper (MOTIF compares mainly vs LLM-based AHD baselines and human-designed heuristics).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>Cited as prior evidence that jointly optimizing multiple routines (neural methods) fosters coordination and often surpasses single-rule refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Neural methods demonstrate that learned, modular components can improve classical heuristics; MOTIF extends this idea by using LLMs to synthesize modular strategies and by introducing self-play/MCTS to search the multi-strategy space.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evolution of heuristics: towards efficient automatic algorithm design using large language model <em>(Rating: 2)</em></li>
                <li>Reflective Evolution (ReEvo) <em>(Rating: 2)</em></li>
                <li>Hsevo: Elevating automatic heuristic design with diversity-driven harmony search and genetic algorithm using llms <em>(Rating: 2)</em></li>
                <li>Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based Automatic Heuristic Design (MCTS-AHD) <em>(Rating: 2)</em></li>
                <li>Foundations of genetic programming <em>(Rating: 2)</em></li>
                <li>A genetic programming hyper-heuristic approach for evolving 2-D strip packing heuristics <em>(Rating: 2)</em></li>
                <li>Neural Large Neighborhood Search for the Capacitated Vehicle Routing Problem <em>(Rating: 1)</em></li>
                <li>NeuroLKH: Combining Deep Learning Model with Lin-Kernighan-Helsgaun Heuristic for Solving the Traveling Salesman Problem <em>(Rating: 1)</em></li>
                <li>Deep-ACO: Neural-enhanced Ant Systems for Combinatorial Optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2001",
    "paper_id": "paper-280536411",
    "extraction_schema_id": "extraction-schema-46",
    "extracted_data": [
        {
            "name_short": "MOTIF",
            "name_full": "Multi-strategy Optimization via Turn-based Interactive Framework",
            "brief_description": "A two-phase, turn-based competitive optimization framework that uses two LLM agents (self-play) and Competitive MCTS to jointly optimize multiple interdependent algorithmic components via three LLM-driven operators (counter, learning, innovation).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MOTIF",
            "operator_type": "LLM-based (turn-based operators: counter, learning, innovation)",
            "operator_description": "Two LLM agents (both using gpt-4o-mini-2024-07-18 in experiments) alternately propose code implementations for individual strategies inside a Competitive Monte Carlo Tree Search. At each node an operator (counter: exploit opponent weaknesses; learning: synthesize opponent strengths; innovation: propose novel implementations) is selected and the LLM is prompted with the active player's code, the opponent's code, recent histories, and a baseline. New implementations are executed and evaluated on a training set; rewards combine absolute improvement and competitive gain. No model fine-tuning is performed — all adaptation is via prompt/context and dynamic baselines.",
            "training_data_description": "Optimization is driven by synthetic, domain-specific D_train and evaluated on held-out D_test. The paper reports a typical experimental dataset pair per domain (D_train and D_test) generated uniformly as described for each COP; Table 6 indicates training/test set sizes used in experiments (paper lists #Train ≈ 564 and #Test ≈ 64 for many benchmarks). Training instances are intentionally smaller/lighter than test instances to speed evaluation during search.",
            "domain_or_benchmark": "Multiple combinatorial optimization domains: Traveling Salesman Problem (TSP), Capacitated Vehicle Routing Problem (CVRP), Multiple Knapsack Problem (MKP), Orienteering Problem (OP), Bin Packing Problem (BPP); frameworks: ACO, GLS, Deconstruction-then-Repair (DR).",
            "comparison_baseline": "Human-designed baseline heuristics, and recent LLM-based AHD baselines: EoH (Liu et al. 2024), ReEvo (Ye et al. 2024), HSEvo (Dat et al. 2025), MCTS-AHD (Zheng et al. 2025); also compared against fixed single-strategy optimization of GLS/ACO and neural methods are discussed in related work (Neural LNS, NeuroLKH, DeepACO).",
            "performance_learned_operator": "Reported as optimality gap (%) vs human baseline. Example (GLS single-strategy TSP optimality gap): MOTIF: TSP50=0.0000 ± 0.0000, TSP100=0.0007 ± 0.0006, TSP200=0.0610 ± 0.0100, TSP500=0.5577 ± 0.0255 (values taken from Table 1; metric reported in paper as optimality gap %; lower is better). The paper states MOTIF consistently outperforms prior LLM-based methods across domains and scales (see Figure 3 and Tables 1–2).",
            "performance_traditional_operator": "No direct experimental comparison to traditional genetic programming (GP) operators (e.g., subtree crossover/mutation) is reported. The paper compares against human-designed heuristics (baseline) but does not provide GP-operator numerical results.",
            "performance_hybrid_operator": "MOTIF is compared to other hybrid/LLM+search approaches (e.g., EoH, ReEvo, HSEvo, MCTS-AHD) in the experiments; reported baseline numbers for those methods appear in Table 1 (see follow-on entries). Specific hybrid-operator numerical rows are reported for those baselines (see their entries) but MOTIF is the best-performing method reported in the paper's experiments.",
            "validity_or_executability_rate": "The paper reports 'success rate' per operator (defined as proportion of generated implementations that improve upon the current baseline): Counter 93% ± 2%, Learning 92% ± 2%, Innovation 97% ± 1% (Table 3). The paper does not explicitly report code compilation/parse success rates separate from success/improvement.",
            "novelty_or_diversity_metric": "Novelty and silhouette computed on code embeddings (CodeT5p-110m-embedding). Table 3: Counter: novelty 0.0136 ± 0.0067, silhouette 0.5121 ± 0.0208; Learning: novelty 0.0118 ± 0.0054, silhouette 0.5325 ± 0.0300; Innovation: novelty 0.0175 ± 0.0110, silhouette 0.4793 ± 0.0240. Innovation highest novelty but lowest silhouette (more diverse but less cohesive).",
            "out_of_distribution_performance": "The framework optimizes on a lighter/smaller D_train and evaluates final implementations on held-out D_test (larger/harder). Results show a measurable gap between train and test performance but MOTIF generalizes better than ablated variants. Example from ablation (Table 4): MOTIF (original) ACO train gap 0.80 ± 0.28, test gap 5.21 ± 0.35; GLS train 0.34 ± 0.19, test 2.73 ± 0.05 — showing some degradation but reasonable test performance. No extensive OOD domain transfer experiments are provided beyond held-out test instances.",
            "training_bias_evidence": "Paper intentionally uses smaller D_train than D_test to speed optimization and reports generalization to D_test; ablations show removing dynamic baseline or reasoning components increases test-phase degradation (e.g., w/o dynamic baseline shows much larger test gap), suggesting sensitivity to optimization protocol rather than explicit dataset bias. The paper does not provide direct evidence of LLM-generated operator bias toward training-instance patterns beyond these observations.",
            "computational_cost_comparison": "Authors report computational costs for MOTIF runs: a typical ACO (TSP) run takes about 1.5 hours and consumes ~0.5M input tokens and ~0.2M output tokens using gpt-4o-mini-2024-07-18, costing roughly $0.18 per run (Table 7). The paper does not report direct wall-clock or monetary comparisons against classical GP runs or other baselines' compute costs.",
            "transfer_learning_results": "Not tested — no experiments where learned operators are transferred across problem domains are reported.",
            "domain_specific_vs_general_pretraining": "Not compared — a single general LLM (gpt-4o-mini-2024-07-18) is used for all experiments; no domain-specialized model or fine-tuning comparison is provided.",
            "ablation_study_results": "Comprehensive ablations in Table 4: removing dynamic baseline causes the largest degradation (particularly in test), removing reasoning reduces performance, removing operators (counter, learning, innovation) each affects performance differently (e.g., w/o learning increases test gap in some settings). Removing final round or outer controller also hurts results. Key finding: dynamic baseline and reasoning fields are important for robust gains and generalization.",
            "hypothesis_space_characterization": "Operators' output spaces are characterized using semantic code embeddings (CodeT5p-110m) and measured via novelty and silhouette to quantify inter-operator diversity and intra-operator cohesion. Innovation explores broader semantic regions (high novelty) but with scattered cluster structure (low silhouette); learning yields cohesive, consistent outputs (high silhouette, low novelty).",
            "adaptation_during_evolution": "Adaptation occurs via prompt-context and dynamic baseline updates during search (no model fine-tuning). Agents incorporate active player and opponent histories in prompts so generated operators adapt during the MCTS search process (online prompt adaptation). This prompt/contextual adaptation is shown to improve results vs ablations where histories or dynamic baseline are removed.",
            "failure_modes": "Innovation operator produces diverse but internally inconsistent outputs (low silhouette). Disabling dynamic baseline leads to stagnation and poor test performance. Limiting reasoning (they deliberately pick a cheaper LLM with limited coding/reasoning ability) degrades quality, indicating sensitivity to the LLM's reasoning capacity. The paper also notes LLM-generated code non-determinism and stochasticity in improvements.",
            "key_findings_for_theory": "Turn-based self-play between two LLM agents over multiple interdependent strategies expands the search space and yields better-performing solver components than single-strategy LLM methods and human baselines. Dynamic baseline updates and explicit opponent history (self-play context) are critical for driving continual improvement and avoiding stagnation. Different learned operators occupy complementary parts of the hypothesis space: 'learning' exploits and consolidates consistent improvements, 'counter' focuses on targeted adversarial fixes, and 'innovation' expands exploration (but with lower internal cohesion). Training on smaller/light D_train enables fast inner-loop evaluation but requires system-level safeguards (dynamic baseline and final system-aware refinement) to generalize to harder D_test instances. Computational cost for LLM-driven search can be modest when using smaller LLMs and light train sets, but no direct cost comparison to GP is provided.",
            "uuid": "e2001.0"
        },
        {
            "name_short": "EoH",
            "name_full": "Evolution of Heuristics (EoH)",
            "brief_description": "An LLM-driven evolutionary approach where GPT-4 synthesizes scoring functions and evolutionary selection (tournaments) evolves heuristics over generations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "EoH",
            "operator_type": "LLM-based integrated with evolutionary search",
            "operator_description": "LLMs (GPT-4 in the referenced work) generate candidate heuristic functions which are then evaluated and selected using evolutionary tournament selection across generations; LLM acts as generator/mutator in an evolutionary backbone.",
            "training_data_description": "Not specified in this paper (referenced work likely uses domain-specific instance sets); the MOTIF paper only cites EoH as a baseline in related work/experiments.",
            "domain_or_benchmark": "Combinatorial optimization (cited in context of AHD); used as a baseline in TSP/GLS comparisons in MOTIF experiments.",
            "comparison_baseline": "Compared in MOTIF experiments against human baseline and MOTIF. Table 1 includes EoH as a competing method.",
            "performance_learned_operator": "Reported in Table 1 (as a baseline) but table formatting in the paper makes direct mapping to sizes ambiguous; EoH appears in the baselines shown in Figure 3/Table 1 and is outperformed by MOTIF. The paper does not excerpt a clear per-size numeric summary for EoH beyond what's included in Table 1.",
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": null,
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": "Yes — adaptation via evolutionary selection across generations; LLM acts as generator for mutation/crossover-like variation within evolutionary loop (as described in referenced EoH work).",
            "failure_modes": null,
            "key_findings_for_theory": "EoH exemplifies integration of LLM generation with population-based evolutionary search; MOTIF positions itself as a more structured multi-strategy, turn-based alternative that outperforms single-component evolutionary LLM approaches in the reported experiments.",
            "uuid": "e2001.1"
        },
        {
            "name_short": "ReEvo",
            "name_full": "Reflective Evolution (ReEvo)",
            "brief_description": "An LLM-augmented evolutionary framework that augments evolutionary search with self-reflection (LLM critiques) to improve candidate heuristics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ReEvo",
            "operator_type": "LLM-based + evolutionary (reflective prompting)",
            "operator_description": "Combines evolutionary search with LLM-based self-reflection (post-hoc critique and repair) to refine heuristics generated during evolution. The LLM is used both to generate candidates and to reflect on/repair poor outputs.",
            "training_data_description": null,
            "domain_or_benchmark": "Automatic Heuristic Design for COPs (cited and used as baseline in comparisons).",
            "comparison_baseline": "Compared as a baseline in MOTIF experiments (Table 1 / Figure 3).",
            "performance_learned_operator": "Included in Table 1 as a baseline with per-size numbers (e.g., ReEvo shows larger optimality gaps than MOTIF on some sizes), but exact mapping to sizes is given in Table 1 of the paper.",
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": null,
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": "Yes — uses LLM reflection to adapt candidate heuristics across evolutionary iterations.",
            "failure_modes": null,
            "key_findings_for_theory": "ReEvo demonstrates that self-reflection can improve LLM-generated heuristics; MOTIF builds on self-play/reflection but emphasizes multi-component and adversarial/cooperative dynamics to further enhance discovery.",
            "uuid": "e2001.2"
        },
        {
            "name_short": "HSEvo",
            "name_full": "HSEvo",
            "brief_description": "A diversity-driven harmony search and genetic algorithm approach that leverages LLMs to maintain a diverse population of heuristics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "HSEvo",
            "operator_type": "LLM-driven evolutionary/harmony search (diversity-aware)",
            "operator_description": "Incorporates LLMs into a harmony-search-style evolutionary framework emphasizing diversity maintenance and creative heuristic generation; LLMs generate candidate heuristics that are integrated into population-based search.",
            "training_data_description": null,
            "domain_or_benchmark": "Automatic Heuristic Design; included as a baseline in MOTIF experiments.",
            "comparison_baseline": "Compared against MOTIF and other LLM-based AHD methods in Table 1/Figure 3.",
            "performance_learned_operator": "Table 1 reports HSEvo numbers (e.g., HSEvo TSP optimality gaps appear larger than MOTIF's values shown), but exact mapping is in Table 1.",
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": null,
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": "Yes — population diversity metrics and harmony search mechanics provide adaptation dynamics in the evolutionary loop.",
            "failure_modes": null,
            "key_findings_for_theory": "HSEvo emphasizes the role of diversity in generating complementary heuristics; MOTIF's operators (especially innovation) are intended to play a similar role but within a turn-based self-play MCTS scaffold.",
            "uuid": "e2001.3"
        },
        {
            "name_short": "MCTS-AHD",
            "name_full": "MCTS-AHD (Monte Carlo Tree Search for Automatic Heuristic Design)",
            "brief_description": "An MCTS-based AHD method that uses progressive widening to explore a search tree of heuristics generated by LLM prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MCTS-AHD",
            "operator_type": "LLM-based + MCTS (progressive widening)",
            "operator_description": "Applies MCTS (with progressive widening) where nodes represent partial/complete heuristic configurations and child nodes are instantiated by prompting an LLM to revise/extend implementations; selection guided by performance metrics and backpropagation.",
            "training_data_description": null,
            "domain_or_benchmark": "Automatic Heuristic Design; used as a baseline in MOTIF experiments (noted as prior MCTS+LLM work).",
            "comparison_baseline": "Compared against MOTIF in Table 1/Figure 3; baseline is single-strategy LHH methods and human heuristics.",
            "performance_learned_operator": "Table 1 reports MCTS-AHD optimality gaps for GLS/TSP (e.g., MCTS-AHD: TSP50=0.0000 ± 0.0000, TSP100=0.0024 ± 0.0007, TSP200=0.0652 ± 0.0111, TSP500=0.5611 ± 0.0216). MOTIF slightly outperforms these numbers in reported experiments.",
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": null,
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": "MCTS-AHD imposes hierarchy on search and progressive widening to control branching — MOTIF extends this by introducing two-player competitive MCTS with operator-level UCB.",
            "adaptation_during_evolution": "Adaptation via MCTS selection/backpropagation and progressive widening of the search tree; LLM prompts instantiate child nodes.",
            "failure_modes": null,
            "key_findings_for_theory": "MCTS-AHD demonstrates the feasibility of combining MCTS with LLM generation for heuristic design; MOTIF generalizes this idea to multi-strategy and adversarial/cooperative dynamics and reports improved empirical performance.",
            "uuid": "e2001.4"
        },
        {
            "name_short": "GPHH / GP",
            "name_full": "Genetic Programming Hyper-Heuristic (GPHH) / Genetic Programming",
            "brief_description": "Generation-based hyper-heuristics that evolve executable heuristic programs (symbolic expressions) using genetic programming operators (mutation, crossover, selection).",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Genetic Programming Hyper-Heuristic (GPHH)",
            "operator_type": "traditional GP (mutation/crossover/selection), symbolic/programmatic heuristics",
            "operator_description": "Classical GP evolves populations of symbolic heuristics via mutation and crossover operators; used historically to synthesize heuristics as executable programs in hyper-heuristic frameworks (cited works: Langdon & Poli 2013; Burke et al. 2009/2010b). The MOTIF paper references GPHH as foundational AHD work but does not run GP experiments.",
            "training_data_description": null,
            "domain_or_benchmark": "Classical hyper-heuristic applications: scheduling, packing, routing; historically applied to strip packing and related COPs (cited).",
            "comparison_baseline": "Mentioned for historical context; not used as an experimental baseline in MOTIF's reported comparisons.",
            "performance_learned_operator": null,
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": null,
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": "GP adapts via population evolution (mutation/crossover) over generations; MOTIF notes GP as a prior population-based mechanism that LLMs can plug into as generators or mutators.",
            "failure_modes": null,
            "key_findings_for_theory": "The paper positions GP/GPHH as a conceptual predecessor to LLM-driven AHD, noting that prior GP approaches relied on predefined heuristic spaces and handcrafted rules; MOTIF aims to expand beyond these limits by leveraging LLM creativity and multi-strategy search.",
            "uuid": "e2001.5"
        },
        {
            "name_short": "Neural methods (Neural LNS / NeuroLKH / DeepACO)",
            "name_full": "Neural Large Neighborhood Search (Neural LNS), NeuroLKH, DeepACO",
            "brief_description": "Representative neural/data-driven methods that learn components of classical solvers: Neural LNS jointly learns destroy/repair policies; NeuroLKH learns edge-scoring and node-penalty functions for LKH; DeepACO uses neural networks to guide ACO construction/prediction.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Neural LNS / NeuroLKH / DeepACO",
            "operator_type": "neural network-based (learned policies/value/edge-scoring)",
            "operator_description": "Neural LNS: joint RL/GNN-learned destroy and repair policies; NeuroLKH: learned edge-scoring/node-penalty functions to guide Lin-Kernighan heuristic; DeepACO: neural networks predict construction or pheromone guidance for Ant Colony frameworks. These are discussed in related work as examples where modular routine optimization yields gains.",
            "training_data_description": "Not specified in MOTIF; these methods typically train on problem-instance distributions (cited works).",
            "domain_or_benchmark": "Combinatorial optimization (TSP, VRP, etc.); cited as evidence that modular learned components can outperform single-rule tuning.",
            "comparison_baseline": "Discussed in related work; not directly compared experimentally vs MOTIF in the paper (MOTIF compares mainly vs LLM-based AHD baselines and human-designed heuristics).",
            "performance_learned_operator": null,
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": null,
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": "Cited as prior evidence that jointly optimizing multiple routines (neural methods) fosters coordination and often surpasses single-rule refinement.",
            "adaptation_during_evolution": null,
            "failure_modes": null,
            "key_findings_for_theory": "Neural methods demonstrate that learned, modular components can improve classical heuristics; MOTIF extends this idea by using LLMs to synthesize modular strategies and by introducing self-play/MCTS to search the multi-strategy space.",
            "uuid": "e2001.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evolution of heuristics: towards efficient automatic algorithm design using large language model",
            "rating": 2
        },
        {
            "paper_title": "Reflective Evolution (ReEvo)",
            "rating": 2
        },
        {
            "paper_title": "Hsevo: Elevating automatic heuristic design with diversity-driven harmony search and genetic algorithm using llms",
            "rating": 2
        },
        {
            "paper_title": "Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based Automatic Heuristic Design (MCTS-AHD)",
            "rating": 2
        },
        {
            "paper_title": "Foundations of genetic programming",
            "rating": 2
        },
        {
            "paper_title": "A genetic programming hyper-heuristic approach for evolving 2-D strip packing heuristics",
            "rating": 2
        },
        {
            "paper_title": "Neural Large Neighborhood Search for the Capacitated Vehicle Routing Problem",
            "rating": 1
        },
        {
            "paper_title": "NeuroLKH: Combining Deep Learning Model with Lin-Kernighan-Helsgaun Heuristic for Solving the Traveling Salesman Problem",
            "rating": 1
        },
        {
            "paper_title": "Deep-ACO: Neural-enhanced Ant Systems for Combinatorial Optimization",
            "rating": 1
        }
    ],
    "cost": 0.022660249999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework
5 Aug 2025</p>
<p>Nguyen Viet 
Tuan Kiet 
Hanoi University of Science and Technology
Vietnam</p>
<p>Dao Van Tung 
Hanoi University of Science and Technology
Vietnam</p>
<p>Tran Cong Dao 
FPT Software AI Center
Vietnam</p>
<p>Huynh Thi 
Thanh Binh binhht@soict.hust.edu.vn 
Hanoi University of Science and Technology
Vietnam</p>
<p>MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework
5 Aug 2025188E7E78D565B757FE46421CED418E7FarXiv:2508.03929v1[cs.AI]
Designing effective algorithmic components remains a fundamental obstacle in tackling NP-hard combinatorial optimization problems (COPs), where solvers often rely on carefully hand-crafted strategies.Despite recent advances in using large language models (LLMs) to synthesize high-quality components, most approaches restrict the search to a single element-commonly a heuristic scoring function-thus missing broader opportunities for innovation.In this paper, we introduce a broader formulation of solver design as a multi-strategy optimization problem, which seeks to jointly improve a set of interdependent components under a unified objective.To address this, we propose Multistrategy Optimization via Turn-based Interactive Framework (MOTIF)-a novel framework based on Monte Carlo Tree Search that facilitates turn-based optimization between two LLM agents.At each turn, an agent improves one component by leveraging the history of both its own and its opponent's prior updates, promoting both competitive pressure and emergent cooperation.This structured interaction broadens the search landscape and encourages the discovery of diverse, high-performing solutions.Experiments across multiple COP domains show that MOTIF consistently outperforms stateof-the-art methods, highlighting the promise of turn-based, multi-agent prompting for fully automated solver design.</p>
<p>Introduction</p>
<p>From vehicle routing and scheduling to circuit design and logistics, Combinatorial Optimization Problem (COP) underpin many of society's most complex decision systems (Phan Duc et al. 2025;Rajendran 1993;Tan, Mohd-Mokhtar, and Arshad 2021).Yet, despite decades of progress, building effective solvers for COPs remains a costly and highly manual endeavor-requiring domainspecific heuristics, iterative tuning, and substantial expert effort.Heuristics and meta-heuristics are currently the most common methods used to tackle these problems (Desale et al. 2015).However, these techniques often require substantial expert input, which increases development costs and limits their flexibility across various problems.</p>
<p>Significant progress has been made in the field of Automatic Heuristic Design (AHD) (Burke et al. 2018).A prominent example is Genetic Programming Hyper Heuristic (GPHH) (Langdon and Poli 2013), a widely recognized algorithm for AHD.However, many existing approaches still rely heavily on predefined heuristic spaces or rules designed by human experts (Pillay and Qu 2018).Recently, Language Hyper-Heuristics (LHH) has emerged as a promising solution to overcome this limitation.Rather than manually designing heuristic functions or relying on fixed search spaces, researchers have begun leveraging the reasoning capabilities of Large Language Model (LLM) to automatically generate more innovative heuristics.For instance, Liu et al. (Liu et al. 2024;Romera-Paredes et al. 2024) integrated evolutionary algorithms with LLM to evolve code with minimal human intervention.Ye et al. introduced Reflective Evolution (ReEvo) (Ye et al. 2024), a framework that combines evolutionary search with self-reflection (Shinn et al. 2023) to further improve the effectiveness of LLM.Subsequent studies, such as HSEvo (Dat, Doan, and Binh 2025), incorporated diversity measurements and harmony search (Shi, Han, and Si 2012) to encourage more creative solutions.In addition to evolutionary methods, other search algorithms have also been explored in LHH.For example, MCTS-AHD (Zheng et al. 2025) employed Monte Carlo Tree Search (MCTS) with progressive widening (Browne et al. 2012;Coulom 2007b) to better explore the heuristic space.These methods, which require minimal expert knowledge, have achieved competitive performance compared to state-of-theart solvers and neural network-based approaches.</p>
<p>However, previous approaches have generally focused on searching for a single heuristic function within a general framework, which limits the creativity and exploration potential of LLM.To address this limitation, we propose a multi-strategy optimization approach that aims to optimize a set of strategies simultaneously rather than focusing on just one.This broader search space enables LLM to generate more diverse and innovative solutions.Moreover, because multi-strategy optimization requires a significantly more complex search mechanism than existing methods, we introduce a turn-based interactive framework in which two players sequentially compete to outperform each other in discovering the best set of strategies, as presented in Figure 1.In conclusion, our contributions can be summarized as follows:</p>
<p>• We introduce the novel problem of multi-strategy optimization, where the goal is to jointly optimize a system of algorithmic components under a shared performance objective.This setting generalizes prior work that only targets single-component improvements.• We propose a two-round competitive framework that decomposes the problem into (i) component-wise optimization using Competitive Monte Carlo Tree Search (CM-CTS) under a dynamic baseline, and (ii) system-aware refinement in which agents compete in a turn-based manner with fixed baselines and full system visibility.• We design a library of competitive operators-including counter, learning, and innovation-that structure LLM prompting based on the opponent's behavior, recent history, and baseline context.This enables emergent self-play dynamics and yields superior implementations through adversarial pressure and contextual learning.• We conduct extensive experiments across three algorithmic frameworks, five COP domains, and diverse strategy sets.Results consistently demonstrate that MOTIF outperforms prior frameworks-both in single-strategy and multi-strategy optimization-validating the power of turn-based self-play.</p>
<p>2 Related Works</p>
<p>Automatic Heuristics Design</p>
<p>Previous work on LHH (Liu et al. 2024;Ye et al. 2024;Dat, Doan, and Binh 2025;Zheng et al. 2025) has explored the use of LLMs to design heuristic functions via evolutionary algorithms or tree-based search.While these approaches improve over traditional metaheuristics and Neural</p>
<p>Combinatorial Optimization (NCO), they exhibit two core limitations.First, they aim to optimize a single heuristic in isolation, overlooking the potential of weaker components that, when combined, can yield stronger overall performance.This narrow focus restricts the generative flexibility of LLMs.Second, although prior frameworks incorporate self-reflection to assess interactions between heuristics, such mechanisms are limited in scope and depth.Typically based on static comparisons or surface-level summaries, they struggle to capture hidden incompatibilities or potential synergies.As a result, the LLM receives insufficient feedback for meaningful revisions, hampering its ability to conduct deeper exploratory optimization and refine its outputs throughout the search process.</p>
<p>Learning Through Self-play</p>
<p>Recent work has shown that game-based self-play can enhance LLM reasoning through structured interaction, such as critique, revision, or deception.For instance, SPAG (Cheng et al. 2025) frames reasoning as an attacker-defender game to improve deception detection; CDG (Wang et al. 2025) trains a prover-critic pair to expose flawed reasoning; and (Fu et al. 2023a) uses self-play and AI feedback in negotiation tasks to refine decision-making.</p>
<p>While effective for internal rationality and dialogue skills, these methods do not address the co-evolution of multiple algorithmic components for structured tasks.In contrast, our approach applies self-play to multi-strategy optimization, where LLM agents iteratively critique, improve, and compete over distinct heuristics-introducing a new paradigm of inter-strategy competition under system-level feedback.</p>
<p>To our knowledge, this is the first use of self-play for optimizing combinatorial solvers in this setting.</p>
<p>Multi-strategy Optimization</p>
<p>Prior works (Liu et al. 2024;Ye et al. 2024;Dat, Doan, and Binh 2025;Zheng et al. 2025) has largely focused on tuning a single heuristic assuming it alone drives solver quality.In contrast, we adopt a multi-strategy view that treats each routine as an independent optimization target, enabling coordinated improvements across the solver pipeline.Neural methods like Neural LNS (André and Kevin 2020) jointly train destroy/repair policies, while NeuroLKH (Xin et al. 2021) learns edge-scoring and node-penalty functions to guide the classical Lin-Kernighan Heuristic (LKH).These show that modular routine optimization fosters richer coordination and often surpasses single-rule refinement.• a space of instances X d , where each instance x ∈ X d encodes problem-specific data (e.g., graph structure, distances matrix, constraints),</p>
<p>• a global solution space Y d , representing all possible solutions across all instances, and • an objective function
f d : X d × Y d → R that quantifies
the quality of a solution relative to a given instance.For a specific instance x, the feasible solutions form a subset Y d (x) ⊆ Y d .The goal is to find a solution y * ∈ Y d (x) that minimizes f d (x, y).Definition 3.2 (Solver and Strategy).A solver s is an algorithmic framework that generates a solution y ∈ Y d (x) for a given instance x ∈ X d , guided by a collection of internal routines.Each such routine is referred to as a strategy π k , which may include heuristic scoring rules, construction policies, neighborhood moves, penalty update mechanisms, or other algorithmic components.The solver operates as:
y = s x | (π 1 , π 2 , . . . , π K ) ≈ y * .
(1)</p>
<p>Definition 3.3 (Strategy Space).For each strategy type π, let S π denote the space of all valid implementations, including both functional variants and parametrizations.All strategies in S π share a common function signature and optimization objective, ensuring interoperability within the solver.</p>
<p>Variations among strategies arise from differences in internal logic, parameter choices, or heuristic design, while maintaining consistent input-output behavior.Given a solver s equipped with a sequence of K strategies Π = (π 1 , π 2 , . . ., π K ), its performance on an instance x ∈ X d is evaluated by the objective value
F d x | Π = f d x, s x | Π .
(2)</p>
<p>Since the solver's behavior is determined by its underlying strategies, optimizing solver quality amounts to optimizing the design of these strategies.Definition 3.4 (Multi-strategy Optimization).The multistrategy optimization problem aims to simultaneously optimize a sequence of K strategies Π = (π 1 , π 2 , . . ., π K ), implemented within solver s, where each π k belongs to its corresponding strategy space S π k , to minimize the expected solver objective across the domain, as follows
Π * = arg min Π E x∼X d F d x | Π ,(3)
subject to a total computational budget T (e.g., total solver runs, code evaluation time).This formulation enables a more holistic view of solver design: instead of optimizing components in isolation, we jointly evolve multiple, interacting strategies under a unified optimization objective.By modeling each strategy as an adaptive and parameterizable unit, our approach supports richer design spaces and unlocks synergies that static, handcrafted routines cannot capture.</p>
<p>MOTIF</p>
<p>Architectural Overview</p>
<p>MOTIF adopts a two-round optimization process that reflects a natural progression from local specialization to global integration.The goal is to gradually increase the cognitive burden placed on the agents, allowing them to first perform focused improvements under simplified context before advancing to more complex, system-aware reasoning.</p>
<p>Component-wise Competition.In the first phase, the solver is decomposed into a set of individual strategies {π 1 , π 2 , . . ., π K }, each optimized independently.A separate competitive tree is constructed for each strategy, where two agents alternately propose revisions to a single component at a time.During this phase, agents operate with partial context: they have access only to the implementation and performance of the target strategy, its dynamic baseline, and the opponent's proposal.</p>
<p>System-aware Refinement.Once all components have been optimized in isolation, the second phase begins.Here, agents revisit each strategy sequentially, but now with access to the full system configuration.At each turn, a player proposes a modification to one strategy while observing the entire combination of current implementations.A fixed global baseline is used during the optimization of each component, ensuring fairness and stability across turns.This phase encourages the emergence of synergistic adaptations, where the effectiveness of one strategy depends on how well it integrates with others.</p>
<p>Turn-based Dual-agent Game.Throughout both phases, the optimization proceeds as a turn-based game.At each turn, a player selects an operator to generate a new implementation, aiming not only to beat the baseline but also to outperform the opponent.This structure fosters both adversarial behavior-by incentivizing relative gains-and cooperative dynamics-by reinforcing global cost reduction.The continuous interplay between agents creates a rich and adaptive optimization trajectory that evolves from competitive local improvements to system-level coherence.</p>
<p>Component-wise Competition</p>
<p>Outer Controller and Strategy Selection.In the first phase, MOTIF maintains a separate tree T k for each strategy π k .At each outer iteration, a controller selects one tree to optimize using a UCB-based rule:
k = arg max j∈{1,...,k} R j N j + C out ln i N i N j ,(4)
where R j and N j are the total reward and visit count of tree T j , and C out controls exploration.The selected strategy is then optimized via a two-player competition.Although only one component is updated per turn, its impact is assessed globally via the system cost.</p>
<p>Node Representation.Each node represents a game state where P 1 and P 2 each hold a distinct implementation of π k , along with their respective code, and improvement metrics.</p>
<p>Nodes are linked through operator-based transformations, and updated using turn-aware reward signals.Since only one player acts per turn, the structure supports asymmetric credit assignment and promotes adaptive behavior.Each T k evolves via a Competitive MCTS, as described next.The outer controller selects a strategy tree π k to optimize in each iteration.Right: The selected tree is improved via a two-player MCTS, where agents alternate turns using one operator.Each move prompts the LLM with contextualized information about the current and opponent implementations, as well as prior history.Generated code is evaluated and backpropagated through the tree based on a Q-value that accounts for both absolute and relative improvements.The best solution is retained for potential system-level baseline updates.</p>
<p>Competitive Monte Carlo Tree Search.In CMCTS, each node represents a two-player duel over a specific strategy π k , where both players maintain separate implementations and cost estimates.The tree expands through one of three competitive operators that guide the language model L in proposing new code:</p>
<p>• Counter targets weaknesses in the opponent's code, prompting the LLM to design adversarial improvements that exploit inefficiencies or limitations.• Learning encourages synthesis by integrating strengths from the opponent's implementation into the agent's own solution.• Innovation promotes exploration by instructing the LLM to ignore prior solutions and propose novel, potentially unconventional approaches.</p>
<p>Together, these operators span a rich spectrum of behaviors-from adversarial exploitation to constructive integration to exploratory invention.During search, the UCB formula is applied at the operator level to ensure systematic coverage of diverse transformation types.</p>
<p>As illustrated in Figure 2, the competitive search follows a standard MCTS procedure with the following four steps:</p>
<ol>
<li>Selection.The search begins at the root and follows the child nodes created by the current player.If all operators have been explored at a node, the child with the highest average Q-value is selected.Otherwise, an unexplored operator is chosen based on a UCB rule:
UCB(o, π) = Q(o, π) N (o, π) + C in ln N (π) N (o, π) ,(5)
where Q(o, π) is the cumulative reward, N (o, π) the visit count for operator o, and N (π) the total visits to node π.</li>
</ol>
<p>The constant C in balances exploration and exploitation.</p>
<p>Expansion.</p>
<p>A new node is created by applying the selected operator to the current player's implementation.Let p ∈ {P 1 , P 2 } be the active player and π (p) their current code.The language model L generates a new implementation via: p) , π (¬p) ; H (p) , H (¬p) ; B , (6) where o ∈ {counter, learning, innovation} is the selected operator type; π (p) , π (¬p) are the current implementations of the active player and their opponent, respectively; H (p) , H (¬p) are the recent move histories of each player, including summaries of past improvements and operator usage; B denotes the current baseline, consisting of the reference implementation and its associated system cost.
π ′ ← L Prompt o; π (
3. Evaluation.The modified implementation π ′ is inserted into the system, replacing the corresponding strategy.The resulting cost is compared to the fixed baseline to compute the improvement I (p) (%) for the current player p.The opponent's value I (¬p) (%) reflects their unchanged performance at the same node, inherited from the parent.4. Backpropagation.The resulting reward is propagated upward through the tree.For player p, the Q-value combines both absolute improvement and competitive gain:
Q (p) = λ • σ(I (p) ) + (1 − λ) • σ(I (p) − I (¬p) ), (7)
where σ(x) = 1 1+e −kx and λ ∈ [0, 1] controls the balance.This reward updates statistics for both the visited nodes and the operator used, supporting informed selection in future iterations.</p>
<p>Dynamic Baseline.To drive meaningful progress during optimization, we adopt a dynamic baseline mechanism.At any outer iteration, the baseline refers to the bestknown implementation of a strategy-initially handcrafted or warm-started-and is updated whenever an agent produces a strictly better implementation.Each agent competes not against a static reference, but against the most recent high-performing solution from the opposite player.</p>
<p>System-aware Refinement</p>
<p>In the second phase, each strategy π k is re-optimized with full system visibility.Unlike the first phase, players now operate over the entire configuration Π = (π 1 , . . ., π K ), using a fixed baseline for each strategy.</p>
<p>The key distinction lies in the prompt construction: rather than focusing on local context and opponent-specific features, the prompt now includes the full system configuration, global baseline cost, and historical search traces.This enables the language model L to reason about system-level dependencies, hyperparameter synergies, and global optimization behavior:
π ′ k ← L Prompt Π; π (p) k , π(¬p) k
; H (p) , H (¬p)  .(8) An update is accepted only if it improves upon both the baseline and the opponent's best result, promoting coordination across components and discovering configurations not reachable through isolated optimization.See Appendix C.4 for more detailed.</p>
<p>Experiments</p>
<p>Settings.We employ the gpt-4o-mini-2024-07-18 model for both agents throughout our experiments.This model, although not highly capable in coding tasks, was deliberately chosen for its affordability, fast inference, and support for structured outputs, which are essential for reliable parsing and downstream processing.Given its limited reasoning ability, we design a structured response format that enforces clarity in its generation process.</p>
<p>Specifically, we adopt a lightweight variant of chain-ofthought prompting (Wei et al. 2022).Each response must contain a reasoning field, in which the model is required to explain its thought process in no more than five concise sentences.The code field contains the generated Python implementation, while the summary field briefly describes the key changes introduced in the current turn.This tri-field schema helps us better monitor reasoning quality, code correctness, and strategic intent during each optimization step.</p>
<p>Training and Evaluation Setup.All optimization is conducted on a lightweight training set, while final performance is measured on a held-out test set.Full details of the setup, including dataset construction and evaluation protocol, are provided in Appendix D.1.To preserve the efficiency of GLS, prior works typically optimize only a single heuristic component-usually the precomputed penalty scoring function-while keeping the surrounding search logic fixed.We adopt the same design philosophy to ensure fair comparison and fast runtime during evaluation.</p>
<p>As shown in Table 1, even under this restricted singlestrategy setting, our method significantly outperforms several recent LLM-based methods.</p>
<p>Multi-strategy Search</p>
<p>Ant Colony Optimization.Simulating the pheromonebased foraging behavior of ants, Ant Colony Optimization (ACO) (Dorigo, Birattari, and Stutzle 2007;Dorigo and Stützle 2018) has established itself as a versatile and competitive framework for solving NP-hard combinatorial problems.Classical ACO implementations consist of multiple expert-designed components, each responsible for a specific sub-behavior of the colony.In our view, these components-often treated as fixed formulas-can themselves be subject to optimization.Specifically, we target the following sub-strategies: (i) the initialization scheme for heuristic and pheromone scores, (ii) the construction policy that combines heuristic and pheromone information into a transition probability distribution, and (iii) the pheromone update rule that determines how feedback from previous solutions reinforces or weakens certain paths.</p>
<p>Figure 3 compares the performance of our proposed MO-TIF framework with several competitive baselines across five distinct optimization problems, each evaluated at five problem scales.The results demonstrate that jointly optimizing multiple strategies in ACO yields substantial performance gains over both human-designed and LLM-generated single-strategy baselines.</p>
<p>Deconstruction-then-Repair.We introduce a simple yet effective constructive framework as the starting point for strategy exploration.The framework operates in three sequential stages: (i) a greedy initialization guided by an edge   scoring function π 1 ; (ii) a partial destruction phase that removes low-quality elements based on a badness scoring strategy π 2 ; and (iii) a repair phase that incrementally reconstructs the solution using a placement selection criterion π 3 .This process reflects a natural design philosophy: producing reasonably good solutions quickly without engaging in costly search loops.Table 2 reports the improvement margins achieved by optimizing one, two, or all three strategies in the framework.Each component contributes differently depending on the problem domain-for example, π 3 is critical in TSP, while π 2 plays a larger role in CVRP and BPP.Overall, jointly optimizing two or more strategies consistently outperforms the single-strategy setting, demonstrating the synergistic benefits of multi-strategy optimization.</p>
<p>These results also suggest that, beyond merely tuning isolated heuristics, LLMs hold promise in co-designing full algorithmic pipelines.This supports the broader vision of transforming a simple, human-sketched pipeline into a strong, domain-adapted algorithm through iterative LLMguided search.Another question in operator design is whether LLMbased strategies can break free from conventional coding patterns and exhibit genuinely novel behavior.To examine this, we conduct a semantic diversity analysis of the generated implementations.Specifically, we compute two metrics-novelty and silhouette score-based on code embeddings to assess how distinct and well-separated each operator's outputs are.Formal definitions and computation details of these metrics are provided in Appendix D.4.</p>
<p>Convergence and Diversity Analysis</p>
<p>Table 3 compares the three operators across success rate, novelty, and silhouette score.Innovation exhibits the highest novelty, reflecting its broad exploration of new code regions.However, it has the lowest silhouette score, suggesting its outputs are scattered and lack internal consistency.Counter achieves moderate novelty and silhouette, indicating a balanced behavior-exploring new directions while maintaining some cohesion.Learning ranks lowest in novelty but highest in silhouette, showing that it tends to exploit familiar patterns with consistent and stable outputs.</p>
<p>These trends align with the intended design: innovation promotes diversity, learning refines known ideas, and counter responds adaptively to the opponent.</p>
<p>Ablation Study</p>
<p>Table 4 reports the impact of removing various components on the final optimal gap (lower is better) for two solvers: ACO and GLS.Among three components, removing the dynamic baseline causes the most severe performance drop, especially in the test phase-indicating that without continual baseline updates, the system lacks incentive to improve and tends to stagnate.</p>
<p>Disabling reasoning also leads to a notable degradation, highlighting its importance in enabling the model to reflect on prior failures and generate meaningful revisions.</p>
<p>Conclusion</p>
<p>We introduced a two-phase competitive optimization framework that leverages turn-based interactions between LLM agents to improve multi-strategy solvers.Through dynamic baselines, self-play prompting, and system-aware refinement, our method consistently outperforms prior approaches across diverse combinatorial problems.The results highlight the importance of both adversarial pressure and structured cooperation in driving algorithmic innovation.</p>
<p>A Related Works</p>
<p>A.1 Early Hyper-heuristic Frameworks General Hyper-heuristics.Hyper-heuristics, often referred to as Automatic Heuristic Design (AHD), are high-level search frameworks that operate over a space of low-level heuristics instead of directly modifying problem solutions.The primary motivation behind this paradigm is to enhance generality and transferability across combinatorial optimization problems.By abstracting away domain-specific details, hyper-heuristics decouple the problem-solving process from the manual crafting of individual heuristics.Foundational work by Burke et al. (2010a) categorized these frameworks into two principal classes: selection hyper-heuristics, which dynamically choose from a set of predefined heuristics, and generation hyper-heuristics, which synthesize new heuristics during the search.Their subsequent survey (Burke et al. 2013) documented the rising maturity of the field, especially in classical domains such as scheduling, bin packing, and routing.</p>
<p>Hyper-heuristics via Genetic Programming.A notable instantiation of generation-based hyper-heuristics is Genetic Programming (GP), where heuristics are evolved as symbolic expressions.The two-layer architecture, comprising a domainagnostic controller and domain-specific heuristic components, naturally lends itself to GP-based designs.Burke et al. (2009) explored how GP could be used to evolve compositional heuristic logic, while Burke et al. (2010b) demonstrated the feasibility of this approach in practical applications such as two-dimensional strip packing.These systems construct heuristics as executable programs, enabling fine-grained adaptation and a rich space of search behaviors.This line of work laid the groundwork for modern approaches that further extend the heuristic search space with neural or language-based models-but those advancements are discussed in later sections.</p>
<p>A.2 Evolutionary and LLM-Driven Heuristic Design</p>
<p>Evolutionary Computation.Before the advent of LLMs, evolutionary computation had already established itself as a powerful paradigm for AHD.Inspired by principles of natural evolution, these methods evolve a population of candidate heuristics through iterative application of operators such as mutation (local changes to code), crossover (combination of two heuristics), and selection (survival of the fittest).Genetic programming in particular allowed symbolic expressions representing heuristic logic to be evolved over time, forming the conceptual basis for later work that combines these ideas with modern neural models.</p>
<p>Crucially, this population-based, variation-and-selection mechanism provides a flexible search backbone, one that LLMs can now plug into as heuristic generators or mutators.</p>
<p>LLM-Driven Heuristic Design.Recent work has explored how large language models can be integrated into evolutionary frameworks to automate the design of heuristics.These systems typically prompt LLMs to generate candidate routines-often as executable Python functions-which are then evaluated and selected based on performance.In EoH (Liu et al. 2024), GPT-4 is used to synthesize scoring functions for combinatorial problems, guided by tournament selection to evolve increasingly effective heuristics over generations.ReEvo (Ye et al. 2024) extends this setup by encouraging the LLM to reflect on its own outputs through prompt chaining and post-hoc critique, improving generation quality via self-evaluation.</p>
<p>Beyond individual refinement, HSEvo (Dat, Doan, and Binh 2025) incorporates LLMs into a harmony search framework, maintaining a diverse population of heuristics akin to musical motifs.It emphasizes not only optimization but also population diversity, leveraging the generative breadth of LLMs to explore complementary algorithmic structures.</p>
<p>This fusion of LLMs with evolutionary computation marks a shift in how algorithm design is conceptualized: from crafting static heuristics to evolving dynamic, language-based solvers that can interact with selection, critique, and competition.</p>
<p>A.3 Monte Carlo Tree Search in Complex Decision-making Problems</p>
<p>Monte Carlo Tree Search.Monte Carlo Tree Search (MCTS) has emerged as a powerful paradigm for navigating large, structured decision spaces where exhaustive search is intractable.Rooted in sequential decision theory, classical MCTS-first formalized by Coulom (2007a) and comprehensively surveyed by Browne et al. ( 2012)-follows a four-stage loop: selection, expansion, simulation, and backpropagation.Each node in the tree represents a partial decision trajectory, and the use of Upper Confidence Bounds (UCB) enables principled balancing between exploration of uncertain paths and exploitation of known promising ones.</p>
<p>MCTS gained widespread recognition through its role in AlphaGo (Silver et al. 2016), where it served as the backbone for combining deep neural networks with search-based planning.In this context, MCTS operated not just as a passive evaluator, but as an active controller capable of orchestrating deep policy and value networks for superhuman performance in the game of Go.Since then, MCTS has found applications in planning, robotics, theorem proving, program synthesis, and most recently, language modeling.LLM-Augmented Tree Search.A recent direction integrates MCTS with large language models (LLMs) to support structured reasoning and decision-making.The Tree-of-Thoughts framework (Yao et al. 2023) treats LLMs as thought generators, where each node in the tree corresponds to an intermediate reasoning step (a "thought") toward solving a problem.MCTS is used to explore these thoughts hierarchically, evaluating partial reasoning paths and selecting which branches to expand further.This mechanism moves beyond greedy prompting or single-shot decoding by enabling deliberate, self-corrective exploration over multi-step reasoning trajectories.This idea has since been adapted for algorithm discovery.For example, MCTS-AHD (Zheng et al. 2025) applies progressive widening-a technique that defers branching until sufficient evidence is accumulated-to explore a search tree of heuristic strategies generated via LLM prompts.Each node in the tree represents a partial or complete heuristic configuration, and new child nodes are instantiated by prompting an LLM to revise or extend the current implementation.Selection is guided by performance metrics, and backpropagation enables learning from both successes and failures.This combination transforms MCTS into a dynamic controller over language model outputs, selectively steering generation toward effective algorithmic behaviors.</p>
<p>Compared to evolutionary approaches that operate on flat populations of heuristics, MCTS imposes a hierarchical structure on the search space, allowing better reuse of promising sub-strategies and more targeted exploration.Progressive widening further mitigates the risk of combinatorial explosion by limiting branching in early stages.Together, these properties make MCTS particularly well-suited for LLM-based heuristic design, where the space of possible routines is both vast and costly to evaluate.By fusing the generative flexibility of LLMs with the selective discipline of MCTS, these methods achieve a scalable and adaptive framework for navigating algorithmic design spaces.</p>
<p>A.4 Self-play and Adversarial Learning in LLMs</p>
<p>Self-play.Recent work has shown that self-play-where language models act as interacting agents-can lead to significant gains in reasoning quality, robustness, and decision-making.Inspired by game-theoretic dynamics, these methods structure LLM interaction through alternating roles such as attacker-defender, proposer-critic, or buyer-seller, enabling emergent behavior that improves model output beyond static prompting.For instance, SPAG (Cheng et al. 2025) frames reasoning as a two-player adversarial game in which an attacker introduces misleading arguments and a defender must debunk them, improving the model's capacity for deception detection.Similarly, the CDG framework (Wang et al. 2025) trains a pair of prover and critic agents to iteratively expose and correct reasoning flaws, leading to more logically sound outputs.</p>
<p>Other work explores negotiation and cooperation in self-play.Fu et al. (Fu et al. 2023a) design an in-context feedback loop in which LLMs simulate negotiation roles (e.g., buyer vs. seller) and revise their proposals based on AI-generated critiques, resulting in improved utility and coherence in generated dialogues.These methods typically rely on prompt structuring, role conditioning, and iterative critique to create a feedback-rich interaction landscape-one that mirrors social or argumentative reasoning among humans.</p>
<p>Adversarial Learning.Adversarial learning in LLMs also intersects with verbal reinforcement learning and reflective prompting.For example, Reflexion (Shinn et al. 2023) guides the model to reflect on its own past errors and learn through verbalized feedback, strengthening multi-step reasoning capabilities.Although these studies focus primarily on tasks like logic puzzles, negotiation, and QA, their results suggest that interactive multi-agent dynamics offer a powerful foundation for tasks that benefit from critique, revision, or competition.Taken together, these approaches illustrate how adversarial pressure and structured self-interaction can improve LLM outputs by introducing feedback loops, multiple perspectives, and dynamic behavior adjustment-offering a foundation for more sophisticated systems that go beyond static prompting or single-agent search.</p>
<p>B Problem Definitions B.1 Benchmark Problems</p>
<p>Traveling Salesman Problem.The Traveling Salesman Problem (TSP) is defined on a complete weighted graph G = (V, E), where V = {v 1 , v 2 , . . ., v n } is a set of cities and c(i, j) ∈ R ≥0 denotes the cost of traveling from city v i to city v j .The goal is to find a permutation σ = (σ 1 , σ 2 , . . ., σ n ) of the cities that minimizes the total travel cost of a closed tour visiting each city exactly once.The objective is given by:
min σ∈Sn n−1 i=1 c(σ i , σ i+1 ) + c(σ n , σ 1 ) ,(9)
where S n denotes the set of all permutations of n elements.The optimal solution defines the shortest Hamiltonian cycle over the graph.</p>
<p>Capacitated Vehicle Routing Problem.The Capacitated Vehicle Routing Problem (CVRP) extends the TSP by introducing multiple vehicles that serve customers under capacity constraints.Let G = (V, E) be a complete undirected graph where V = {v 0 , v 1 , . . ., v n } consists of a depot node v 0 and n customer nodes, and let c(i, j) ∈ R ≥0 denote the travel cost between nodes v i and v j .Each customer v i has a demand d i &gt; 0, and all vehicles are identical with a fixed capacity Q.A route is a sequence r = (v 0 , v i1 , . . ., v im , v 0 ) in which a single vehicle departs from the depot, serves a subset of customers, and returns to the depot.The cost of such a route is given by
cost(r) = m t=0 c(v it , v it+1 ),(10)
where v i0 = v 0 and v im+1 = v 0 .Each route must satisfy the capacity constraint
m j=1 d ij ≤ Q,(11)
ensuring that the total demand served does not exceed the vehicle's capacity.The full solution consists of a set of such routes R = {r 1 , r 2 , . . ., r k } covering all customers exactly once, and the overall objective is to minimize the total routing cost:
r∈R cost(r).(12)
Multiple Knapsack Problem.The Multiple Knapsack Problem (MKP) is a classic combinatorial optimization problem where a set of n items must be assigned to m distinct knapsacks with limited capacities.Each item i ∈ {1, . . ., n} has a profit p i ∈ R &gt;0 and weight w i ∈ R &gt;0 , and each knapsack j ∈ {1, . . ., m} has a capacity C j ∈ R &gt;0 .The goal is to assign each item to at most one knapsack such that the total profit is maximized and no knapsack exceeds its capacity.Let x ij ∈ {0, 1} be a binary decision variable indicating whether item i is placed in knapsack j.The optimization problem can be written as:
max m j=1 n i=1 p i x ij ,(13)
subject to the capacity constraints
n i=1 w i x ij ≤ C j ∀j ∈ {1, . . . , m},(14)
and assignment constraints
m j=1 x ij ≤ 1 ∀i ∈ {1, . . . , n}.(15)
The MKP captures important allocation scenarios where resources are limited and item assignments are exclusive.</p>
<p>Orienteering Problem.The Orienteering Problem (OP) models route planning under a limited travel budget, balancing reward collection and cost.Given a graph G = (V, E) with non-negative edge costs c(i, j) for all (i, j) ∈ E, a reward r i ≥ 0 at each node v i ∈ V , a start node v s ∈ V , an end node v t ∈ V , and a maximum travel budget B &gt; 0, the objective is to find a path P = (v s , . . ., v t ) that visits a subset of nodes such that the total reward is maximized and the total travel cost does not exceed B. Let P s,t be the set of all feasible paths from v s to v t within budget.The objective is to solve:
max P ∈Ps,t vi∈P r i subject to (vi,vj )∈P c(i, j) ≤ B. (16)
Unlike TSP or CVRP, not all nodes must be visited; instead, the challenge lies in selecting the most rewarding subset of nodes reachable within the given travel constraint.</p>
<p>Bin Packing Problem.The Bin Packing Problem (BPP) involves packing a set of items into the minimum number of identical bins, each with fixed capacity.Formally, let there be n items, where each item i ∈ {1, . . ., n} has a size s i ∈ (0, 1], and let each bin have capacity 1.The goal is to assign each item to a bin such that the total size of items in any bin does not exceed its capacity, while minimizing the number of bins used.Let x ij ∈ {0, 1} indicate whether item i is placed in bin j, and let y j ∈ {0, 1} indicate whether bin j is used.Assuming an upper bound m ≥ n on the number of bins, the problem can be formulated as:
min m j=1 y j (17)
subject to:
n i=1 s i x ij ≤ y j ∀j ∈ {1, . . . , m},(18)m j=1 x ij = 1 ∀i ∈ {1, . . . , n},(19)
x ij ∈ {0, 1}, y j ∈ {0, 1}.</p>
<p>(20) This formulation ensures that each item is assigned to exactly one bin, and bins are only counted if they are actually used.BPP is a fundamental NP-hard problem with applications in logistics, memory allocation, and resource scheduling.</p>
<p>B.2 Benchmark Algorithms</p>
<p>Guided Local Search.Guided Local Search (GLS) is a well-established metaheuristic designed to escape local optima by penalizing frequently occurring features in poor-quality solutions.Introduced by Voudouris and Tsang (1999), GLS enhances standard local search by augmenting the objective function with feature-based penalties, thereby encouraging diversification while maintaining efficiency.It has demonstrated strong performance on various combinatorial problems, particularly the TSP, and has been used in hybrid solvers such as GLS-PR for Vehicle Routing Problems (VRP) (Shaw 1997(Shaw , 1998) ) and more recently in neural-guided contexts (Hudson et al. 2022;Sui et al. 2024).</p>
<p>In our benchmark, we follow the common setup used in these prior works: the main local search logic is fixed, and only the scoring function (used for feature penalization) is subject to optimization.This ensures fair comparison across methods while isolating the effect of LLM-based design.(i * , j * ) ← arg max (i,j)∈σcur</p>
<p>Gij 1+Pij</p>
<p>12: As shown, we restrict our search to a single precomputed strategy before entering the main loop.This design choice is motivated by two key reasons.First, it allows for a fair comparison with other baselines that similarly operate under fixed strategy assumptions.Second, the main loop of GLS is inherently complex and relies heavily on low-level code optimizations to achieve high performance.Allowing LLM-generated code to intervene in this part may disrupt the internal logic and significantly degrade runtime efficiency due to incompatibility with the carefully tuned implementation.
P i * j * ← P i * j * + 1 13: D ← D + k • P 14: LOCALSEARCH( D, σ cur ) around (i * ,
Ant Colony Optimization.Ant Colony Optimization (ACO) is a metaheuristic inspired by the foraging behavior of real ants, first introduced by Dorigo, Maniezzo, and Colorni (1996) as the Ant System for solving COPs.In ACO, a colony of artificial ants incrementally construct solutions by moving on a problem graph, guided by probabilistic rules that balance pheromone intensity and heuristic information.After completing a solution, each ant deposits pheromone on visited components, reinforcing promising paths over iterations.The Ant Colony System (ACS) (Dorigo and Gambardella 1997) refined this process by introducing local pheromone updates and elitist reinforcement strategies.</p>
<p>Since its inception, ACO has been extended in various directions, including hybridization with local search, parallel implementations, and learning-based variants.Recent approaches such as DeepACO (Ye et al. 2023) incorporate neural networks to guide construction policies or predict pheromone distributions, significantly improving scalability and adaptability to different problem instances.These developments have established ACO as a versatile and extensible baseline for many routing and packing tasks.</p>
<p>We now formalize the ACO solver structure used across our benchmark tasks.Each solver consists of three strategies-initialization, transition rule, and pheromone update-optimized independently.We provide a unified pseudocode and detail problem-specific implementations in Table 5. Update BestSolution if improved 10: end for 11: Output: BestSolution While prior work in AHD typically focuses on optimizing a single heuristic matrix H (e.g., based on inverse distances), our formulation broadens this space in several ways.First, we additionally treat the initialization of the pheromone matrix P as part of the optimization process.Second, we model the combination of H and P into transition weights as a learnable strategy, rather than using a fixed formula.Third, we expose the pheromone update rule itself to adaptation.Finally, we extend the function signatures to include both the current iteration t and the total horizon T , enabling LLMs to discover temporally adaptive behaviors that prior formulations cannot express.Building on this line of work, we adopt a simple Deconstruction-then-Repair (DR) framework composed of straightforward greedy and perturbation routines.This allows us to clearly isolate the impact of optimizing multiple strategy components, without the confounding effects of complex heuristics.Algorithm 3: General DR Procedure for COPs 1: Input: Problem-specific data I, destruction rate ρ 2: Strategy 1: S ← PRECOMPUTEEDGESCORES(I) {e.g., edge score in TSP, compatibility in BPP, CVRP} 3: σ ← GREEDYCONSTRUCT(S, start) {Build initial solution using greedy policy over S} 4: Strategy 2: R, σ ← DECONSTRUCT(σ, I, ρ)</p>
<p>{Remove bad components from σ} 5: Strategy 3: σ ′ ← REPAIR(σ, R, I) {Reinsert removed components using a heuristic} 6: c ← COST(I, σ ′ ) 7: Return: σ ′ , c</p>
<p>Because of its simplicity, this DR framework serves as an ideal testbed for analyzing the marginal gains from optimizing each component individually.Any improvement can be attributed with high confidence to a specific strategy modification, enabling precise diagnosis of effectiveness and synergy.Concretely, an initial implementation for TSP is presented below, with each strategy-initialization, deconstruction, and repair-clearly defined.The MOTIF search engine treats this triplet as the baseline configuration and performs competitive optimization over it.System Prompt.The system prompt is a persistent instruction that defines the identity and behavior of the lLLM across interactions.Unlike user inputs, which vary per query, it provides a stable context that aligns the model's responses with task objectives.A common technique is role-based prompting, where assigning the LLM a specific persona-e.g., "competitive algorithm designer"-helps activate relevant reasoning styles.This role-play approach has been shown to enhance zero-shot performance by encouraging coherence, structured thinking, and task relevance (Kong et al. 2024).</p>
<p>System Prompt for Each Player <role> You are a competitive algorithm designer specializing in {domain} strategies.</role><task> Implement {strategy's name} that {strategy's purpose}.{function's signature} (Python code) {note about input's datatype} </task> <description> You are participating in a competitive algorithmic optimization challenge.</p>
<p>GAME SETUP:</p>
<p>-Two players (P1 and P2) compete to create the best implementation -Goal: Design strategies that {strategy's description} -Your implementations will be evaluated against a baseline and your opponent -Better performance than both baseline and opponent earns maximum reward PROBLEM CONTEXT (optional): {additional information} </description> <constraints> 1. DO NOT modify the method signature -keep parameters exactly as specified 2. Declare hyperparameters with reasonable defaults 3. Ensure code is syntactically correct and handles edge cases </constraints> Example for Strategy 1 in ACO (TSP) <task> Implement the initialization strategy that sets up guidance matrices for route finding:.Human Message.The human message, framed from a third-person referee perspective, delivers the competition status to the agent while simultaneously acting as a coach-offering guidance and exerting competitive pressure.Notably, we intentionally place the operator instructions at the end of the prompt to draw the model's attention, leveraging findings from Liu et al. (2023), which show that LLMs often overlook information positioned in the middle of long prompts.</p>
<p>Human Message for</p>
<p>C.2 Operator Instructions</p>
<p>Counter -Analyze opponent's implementation and identify weaknesses, inefficiencies, or limitations.</p>
<p>-Create an implementation that specifically exploits these weaknesses.</p>
<p>-Focus on areas where opponent's approach is suboptimal or vulnerable.</p>
<p>Learning</p>
<p>-Study opponent's successful techniques and innovations.</p>
<p>-Combine their best ideas with your own approach to create superior implementation.</p>
<p>-Learn from their strengths while maintaining your unique advantages.</p>
<p>Innovation</p>
<p>-Create completely novel approach that differs from both baseline and opponent.</p>
<p>-Think outside the box and introduce breakthrough techniques.</p>
<p>-Ignore conventional approaches and pioneer new algorithmic paradigms.</p>
<p>C.3 First Round: Component-wise Competition</p>
<p>Outer Controller.Since multiple strategies must be optimized and only one is selected at each step, the distribution of rewards (here, improvement over the baseline) is inherently stochastic and unpredictable-largely due to the non-deterministic nature of LLM-generated code.This setting aligns naturally with the classical Multi-armed Bandit (MAB) problem, where the goal is to exploration of uncertain strategies and exploitation of known high-performing ones.Accordingly, we adopt the Upper Confidence Bound (UCB) rule as our outer selection mechanism, which offers a principled trade-off between these two objectives.The pseudocode for the outer controller is presented below.</p>
<p>Algorithm 5: Outer Controller 1: Input: Initial strategy set Π = (π 1 , . . ., π K ), outer iterations T outer , inner iterations T inner 2: C 0 ← EVALUATE(Π) 3: for k = 1 to K do 4:
T k ← INITTREE(π k , C 0 ) 5: N k ← 0 {Visit count} 6:
R k ← 0 {Cumulative reward} 7: end for 8: for t = 1 to T outer do 9:</p>
<p>Select strategy index: We apply a sigmoid transformation to the improvement signal to accentuate small yet meaningful changes.Specifically, the reward is shaped using the scaled sigmoid function σ(x) = (1 + e −kx ) −1 , where the scaling factor k controls the sensitivity.By default, we set k = 1, but in tasks where improvements are inherently minimal-such as when the algorithm is already near-optimal-we increase the scale to k = 10 to better distinguish fine-grained gains.For instance, a marginal improvement of I = 0.05% yields σ(0.05) ≈ 0.62, whereas σ(1) ≈ 1, highlighting the sharper response in high-k settings.
k * ← arg max 1≤k≤K   R k N k + C out • ln( j N j + 1) N k   10: π ′ k * ← COMPETITIVEMCTS(T k * , T inner ) {Algorithm 6} 11: Π[k * ] ← π ′ k * 12: C t ← EVALUATE(Π) 13: if C t &lt; C 0 then 14: I (%) ← (C 0 − C t )/
Competitive Monte Carlo Tree Search.Once a strategy tree π is selected, the two players engage in T = T inner alternating turns, each proposing an implementation of their own on the shared MCTS structure.At any node n, we denote by π p (n) the implementation currently held by player p, and by π ¬p (n) the corresponding implementation of their opponent.These notations will be used throughout the remainder of this section.</p>
<p>Potential-Based Decomposition.Recall our shaped reward for player p on transition s → s ′ :
Q (p) (s → s ′ ) = λ σ I (p) (s ′ ) + (1 − λ) σ I (p) (s ′ ) − I (¬p) (s) , where σ(x) = 1 1 + e −kx ,(21)
and using I (¬p) (s ′ ) = I (¬p) (s) in two-player MCTS.Define the combined potential
U (s) = λ σ I (p) (s) + (1 − λ) σ I (p) (s) − I (¬p) (s) .(22)
By construction,
Q (p) (s → s ′ ) = U (s ′ ).(23)
We can then write
U (s ′ ) = U (s ′ ) − U (s) + U (s). (24) Hence Q (p) (s → s ′ ) = U (s ′ ) − U (s) F (s → s ′ ) (potential-difference) + U (s) G(s) (state-only) ,(25)
where F (s → s ′ ) = U (s ′ ) − U (s) is exactly in the form of a potential-based shaping reward (Φ(s ′ ) − Φ(s)), which by Ng, Harada, and Russell (1999) guarantees policy invariance.G(s) = U (s) depends only on the current state s (i.e. is constant across all child actions), and thus does not affect the relative ranking of successor Q-values in MCTS selection.</p>
<p>Algorithm 6: Competitive MCTS on Strategy π
1: Input: Root node n 0 storing π 1 (n 0 ), π 2 (n 0 ), baseline cost C 0 , operator set O, number of iterations T 2: Initialize C * 1 , C * 2 ← ∞, π * 1 ← π 1 (n 0 ), π * 2 ← π 2 (n 0 ),I p ← (C 0 − C p )/|C 0 | (%), I ¬p ← (C 0 − C ¬p (n ′ ))/|C 0 | (%) 24: Q p ← λ σ(I p ) + (1 − λ) σ(I p − I ¬p ) 4. BACKPROPAGATION: 25:
for each ancestor a from n ′ back to n 0 do 26:</p>
<p>Let (o a , p a ) be the annotation on the edge from a 27: System-aware Refinement.In the final phase, a sequential loop is employed to refine individual strategies through smallscale modifications-such as hyperparameter tuning or implementation variants-under full system context.Each player is granted visibility over the entire set of current implementations, allowing them to reason about inter-strategy dependencies and system-level synergy.Optimization proceeds in a turn-based fashion, where players alternate and iteratively propose revisions to one strategy at a time.This setup encourages the emergence of globally coherent improvements that were previously unreachable in the component-wise phase.
N (a, o a , p a ) ← N (a, o a , p a ) + 1 28: V (a, o a , p a ) ← V (a,
Algorithm 7: Final Round Optimization over Strategy Set
{π k } K k=1 1: Input: Initial combination Π (0) = (π (0) 1 , . . . , π(0)
K ), baseline cost C 0 , number of iterations per strategy T  Form context combination:
2: Initialize global best combination Π * ← Π (0) , cost C * ← C 0 3: for each strategy k = 1 to K do 4: Fix baseline Π base ← Π * , C base ← C * 5: Initialize player best costs: C 1 ← ∞, C 2 ← ∞ 6: Initialize current player p ← 1, failure count f ← 0 7: for t = 1 to T do 8: if f ≥ 3 thenΠ (t) ← Π base with π k → π (t) k 15: Apply Operator: π(t) k ← LLMGENERATE(Π (t) , k, p) 16: Π ′(t) ← Π (t) with π k → π(t) k 17: Evaluate: C (t) ← EVALUATE(Π ′(t) )
18:</p>
<p>Compute improvement: Training and Evaluation Setup.While Definition 3.4 formalizes the optimization objective as the expected solver performance over an entire input distribution, minimizing this expectation directly is generally intractable.To make the problem practically solvable, we adopt a data-driven approximation using two randomly generated datasets: D train and D test .Each dataset contains multiple instances drawn uniformly from the same problem domain.
I (t) ← C base − C (t
During the optimization phase, all code generated by the LLM is evaluated exclusively on D train .This training set serves as the basis for computing performance feedback, guiding the agent's competitive improvements.Once the search process concludes, the final implementations of each strategy are evaluated on the held-out test set D test , which provides an unbiased measure of generalization and final solution quality.</p>
<p>To ensure search efficiency, we deliberately choose D train to be computationally lighter-i.e., instances in the training set are generally smaller in size or complexity than those in the test set.This design allows the agents to perform rapid evaluations during optimization, while still verifying robustness on more challenging, representative scenarios at test time.2024), by sampling random instances under fixed settings.These datasets are used for fast evaluation during optimization.</p>
<p>• TSP: City coordinates are sampled uniformly in the square [0, 1] 2 .• CVRP: Customer locations lie in [0, 1] 2 with demands in [1, 10]; the depot is fixed at (0.5, 0.5); capacity is set to 50.</p>
<p>• MKP: Item values and weights are sampled uniformly from [0, 1]; capacity is drawn uniformly from [max j w ij , j w ij ].</p>
<p>• OP: Nodes are sampled from [0, 1] 2 ; each node i is assigned a score p i = 1 + 99 • d0i maxj d0j /100, where d 0i is the Euclidean distance to the depot.Tour length limits are set to {3, 4, 5, 6, 7} for sizes 50, 100, 200, 300, and 500, respectively.• BPP: Bin capacity is 150; item sizes are sampled uniformly from [20, 100].Evaluation Settings.We evaluate each generated strategy combination using the parameters summarized in Table 8.These configurations closely follow standard settings adopted by recent LLM-based AHD frameworks, ensuring fair and consistent comparisons across different problem domains.</p>
<p>D.2 Hyperparameter Configuration</p>
<p>π ⊆ S π denote the subset of implementations generated by applying o to strategy π.We instantiate our embedding function E using the pretrained codet5p-110m-embedding (Wang et al. 2023), which maps each implementation into an e-dimensional vector (here e = 256):
E : S π −→ R e , v = E(π),(26)
which maps each implementation to a continuous embedding v.These embeddings allow us to quantify the semantic diversity of generated code via geometric distances in the embedding space.</p>
<p>Novelty Score.Given an embedding v ∈ E(S (o) π ), we measure its semantic distance to other-operator embeddings u ∈ E(S π \ S (o) π ) via the normalized cosine metric:
d cos (v, u) = 1 − cos(v, u) 2 , cos(v, u) = v • u ∥v∥ ∥u∥ ∈ [−1, 1].(27)
This maps pairwise distances to [0, 1], where 0 indicates identical direction and 1 maximal dissimilarity.In practice, because the LLM is explicitly prompted to preserve the function signature and behavior, almost all implementation pairs exhibit relatively high cosine similarity (typically in the range [0.6, 0.8]).This makes the raw cosine value less discriminative, motivating the use of a score that reflects relative magnitude-where larger values indicate greater novelty-rather than relying on the absolute similarity itself.</p>
<p>To reduce sensitivity to outliers while capturing the local neighborhood structure, we define
novelty k (v) = 1 k k i=1 d cos v, u i ∈ [0, 1],(28)
where u 1 , . . ., u k are the k nearest neighbors of v among the other-operator embeddings.In our experiments, we set k = 3 (chosen via preliminary cross-validation) to balance local sensitivity against noise: smaller k can yield high variance, while larger k may dilute semantically relevant differences.Finally, we aggregate across all embeddings in E(S π ) to report the operator's average novelty.A high score indicates that operator o consistently explores semantic regions distinct from those of other operators, making novelty a natural metric for inter-operator diversity.We further complement this measure with the silhouette score to assess the cohesion and separation of operator-specific clusters.</p>
<p>Figure 1 :
1
Figure 1: (a) Monolithic reflective pipeline: generation and reflection exchange one-way hints around a single evaluator with minimal behavioral awareness.(b) Turn-based interactive framework: two agents take turns generating and updating under shared evaluations, yielding explicit peer feedback, richer diversity, and adaptive explore-exploit balance.</p>
<p>Figure 2 :
2
Figure 2: Overview of the component-wise competition framework.Left:The outer controller selects a strategy tree π k to optimize in each iteration.Right: The selected tree is improved via a two-player MCTS, where agents alternate turns using one operator.Each move prompts the LLM with contextualized information about the current and opponent implementations, as well as prior history.Generated code is evaluated and backpropagated through the tree based on a Q-value that accounts for both absolute and relative improvements.The best solution is retained for potential system-level baseline updates.</p>
<p>Figure 3 :
3
Figure 3: Comparison of AHD frameworks applied to the ACO algorithm.EoH, ReEvo, and MCTS-AHD optimize only a single strategy component (the heuristic function), while MOTIF concurrently optimizes two or three strategy components.Left: Relative performance compared to the human-designed baseline.Right: Evaluation curves showing the best objective value over time (measured by the number of evaluations), averaged over three independent runs.</p>
<p>Figure 4 Figure 4 :
44
Figure 4 plots the best optimality gap achieved at each outer iteration across five independent training runs.While occa-</p>
<p>Algorithm 1 :
1
GLS Procedure for TSP 1: Input: Distance matrix D, #perturbation moves M , #iterations T 2: Main strategy: G ← GENERATEGUIDEMATRIX(D) 3: P ← 0 {Penalty matrix} 4: σ best ← NEARESTNEIGHBOR(D) 5: LOCALSEARCH(D, σ best ) {Modifies σ best in-place using 2-opt and relocate} 6: c best ← COST(D, σ best ) 7: k ← 0.1 • c best /n {n: number of cities} 8: σ cur ← σ best 9: for t = 1 to T do 10: for m = 1 to M do 11:</p>
<p>Algorithm 2: General ACO Procedure for COPs 1: Input: Problem-specific data I, #ants M , #iterations T 2: Strategy 1: H, P ← INITIALIZE(I) {H: heuristic, P : pheromone} 3: Initialize BestSolution 4: for t = 1 to T do 5: Strategy 2: W ← COMPUTEPROBABILITIES(H, P, t, T ) P ← UPDATEPHEROMONE(P , Solutions, Costs, t, T ) 9:</p>
<p>Table 5 :
5
Modular strategies targeted for optimization in each problem.✓ indicates which strategies are optimized.The input I describes the problem-specific data provided to the solver.-Repair.The idea of deconstruction and repair is not new in combinatorial optimization.André and Kevin (2020) proposed a neural variant of Large Neighborhood Search (LNS), where destroy and repair policies are jointly learned via reinforcement learning.Wu et al. (2021) further improved this framework by leveraging GNN-based representations for adaptive neighborhood selection.More recently,Fu et al. (2023b) introduced a hierarchical destroy-and-repair framework that scales to million-city TSP instances by progressively refining solutions across multiple levels.In parallel,Li et al. (2025) explored hypergraph-based operators to guide destruction and reconstruction phases, achieving state-of-the-art results.</p>
<p>Algorithm 4: Initial DR Procedure for TSP 1: Input: Distance matrix D ∈ R n×n , destruction rate ρ 2: Strategy 1: S ij ← −D ij ∀i, j {Edge score = negative distance} 3: Initialize tour σ ← GREEDYNEARESTNEIGHBOR(S) {Use S to construct initial tour} 4: Strategy 2: R ← top-ρn cities in σ ranked by badness {Badness = distance to two nearest neighbors} 5: Remove R from σ to obtain σ 6: Strategy 3: for c ∈ R do insert c at best position in σ {Greedy reinsertion using nearest city} 7: Final cost c ← min(COST(D, σ), COST(D, σ))</p>
<p>5</p>
<p>distances: np.ndarray) -&gt; tuple[np.ndarray,np.ndarray]:Initialize heuristic and pheromone matrices for route optimization.</p>
<p>Each Player <baseline> {baseline implementation} (Python code) </baseline> <current solution> Status: {FAIL/SUCEED} -Improvement: {current improvement} Implementation: {current implementation} (Python code) </current solution> <opponent> Lastest best implementation (improvement: {opponent's improvement}% over baseline): Implementation: {opponent's implementation} (Python code) </opponent> <opponent summary> {opponent history} </opponent summary> <your summary> {active player history} </your summary> <instructions> {operator} BONUS: I will pay $1,000,000 if you can beat the opponent's current record!Your goal: Create implementation that outperforms both baseline and opponent.IMPORTANT: Think step-by-step to achieve the best result.</instructions></p>
<p>k ← best-performing implementation of player p for strategy k</p>
<h6></h6>
<p>Calculate distance score, shorter distances yield higher scores 19 distance_score = -distances[i, j] * weight_distance 20 21 # Count nearby unvisited cities within the threshold for potential future connections 22 connectivity_count = 0 23 for k in range(n): 24 if k != i and k != j: 25 if (distances[k, i] &lt; connectivity_threshold) or (distances[k, j] Final score combining distance score with future connectivity potential 32 final_score = distance_score + future_edge_bonus 33 return np.clip(final_score, -1e6, 1e6) # Ensure score falls within reasonable bounds Listing 2: Example implementation of generated deconstruction strategy in DR 1 tour_idx: int, tour: list[int], distances: np.ndarray) -&gt; float: Total distance to neighbors 21 total_neighbor_distance = np.sum([distances[city,neighbor] for neighbor in neighbor_indices])22 23 Clustering coefficient to penalize cities with too many connections 24 connections_count = np.sum(distances[city]&lt; np.inf) -1 # Exclude self distances[city, neighbor] for neighbor in neighbor_indices if distances[city, neighbor] &lt; np.inf] 29 avg_neighbor_distance = np.mean(valid_neighbor_distances)if valid_neighbor_distances else np.inf 30 overall_avg_distance = np.mean(distances[city][distances[city]&lt; np.inf]) if np.sum( distances[city] &lt; np.inf) &gt; Efficiency metric that penalizes cities that contribute inefficient distances 34 route_efficiency = total_neighbor_distance / (len(valid_neighbor_distances) if valid_neighbor_distances else 1) city: int, incomplete_tour: list[int], distances: np.ndarray) -&gt; int: [incomplete_tour[0]] # Before the first city 22 elif position == tour_length: 23 cost_increase = distances[incomplete_tour[-1], city] # At the end Update best position if current cost increase is better or prefer earlier position on ties 30 if cost_increase &lt; best_cost_increase or (cost_increase == best_cost_increase and position &lt; best_position): Early exit if we find a position with zero cost increase return best_position</p>
<p>Table 1 :
1
(Voudouris and Tsang 1999) TSP with GLS: comparison across methods (3 runs).±0.0000 0.0012 ± 0.0009 0.0639 ± 0.0097 0.5796 ± 0.0146 ReEvo 0.0000 ± 0.0000 0.0335 ± 0.0449 0.2081 ± 0.1927 0.7918 ± 0.3051 HSEvo 0.0108 ± 0.0138 0.3095 ± 0.2363 1.1254 ± 0.6424 2.4593 ± 0.7314 MCTS-AHD 0.0000 ± 0.0000 0.0024 ± 0.0007 0.0652 ± 0.0111 0.5611 ± 0.0216 Among classical metaheuristics for combinatorial optimization, one of the most effective is Guided Local Search (GLS)(Voudouris and Tsang 1999), which enhances local search by penalizing frequent features of poor local optima.It has demonstrated strong performance on combinatorial problems such as the TSP.In practice, GLS is often deployed with handcrafted heuristics that guide the search toward promising regions of the solution space.
MethodsTSP50TSP100TSP200TSP500EoH 0.0000 MOTIF 0.0000 ± 0.0000 0.0007 ± 0.0006 0.0610 ± 0.0100 0.5577 ± 0.02555.1 Single-strategy SearchGuided Local Search.</p>
<p>Table 2 :
2
Performance comparison between single-strategy and multi-strategy optimization across three combinatorial problems: TSP, CVRP, and BPP, each evaluated at various instance sizes.Results indicate percentage improvement over the humandesigned baseline, averaged over 3 runs.Strategies π 1 , π 2 , π 3 denote initialization, deconstruction, and repair respectively.
StrategiesTSPCVRPBPP5010020050100200100200300π 10.39 ± 0.17 0.81 ± 0.27 0.72 ± 0.451.47 ± 0.162.96 ± 0.252.30 ± 0.393.17 ± 0.004.82 ± 0.004.77 ± 0.00π 23.13 ± 0.05 6.24 ± 0.21 8.18 ± 0.264.27 ± 0.247.04 ± 0.736.85 ± 0.75 23.19 ± 0.05 24.42 ± 0.02 25.00 ± 0.00π 33.88 ± 0.04 7.91 ± 0.01 11.35 ± 0.03 6.34 ± 0.085.07 ± 0.024.28 ± 0.03 12.70 ± 7.51 12.84 ± 7.15 12.15 ± 7.49(π 1 , π 2 )2.58 ± 0.50 5.04 ± 0.78 5.84 ± 1.266.06 ± 1.647.94 ± 2.718.85 ± 2.64 23.15 ± 0.15 24.40 ± 0.06 24.98 ± 0.02(π 2 , π 3 )3.83 ± 0.16 7.97 ± 0.14 11.59 ± 0.07 10.64 ± 0.74 12.31 ± 0.57 11.71 ± 1.03 23.05 ± 0.11 24.32 ± 0.15 24.89 ± 0.15(π 1 , π 2 , π 3 ) 3.88 ± 0.04 7.96 ± 0.03 11.65 ± 0.05 10.98 ± 1.64 12.84 ± 3.17 13.06 ± 3.67 23.94 ± 0.55 25.02 ± 0.41 25.41 ± 0.26provement. Furthermore, as shown on the right, all threeoperators-counter, learning, and innovation-demonstratesimilar convergence profiles, reflecting the robustness andadaptability of our operator design across varying strategiccontexts.</p>
<p>Table 3 :
3
Diversity and success analysis for each operator.
Counter93 ± 2 %0.0136 ± 0.00670.5121 ± 0.0208Learning92 ± 2 %0.0118 ± 0.00540.5325 ± 0.0300Innovation97 ± 1 %0.0175 ± 0.01100.4793 ± 0.0240
Success rate measures the proportion of generated implementations that improve upon the current baseline.Novelty score captures the average semantic distance to other operators' outputs within the same strategy.Silhouette score quantifies intra-operator cohesion and inter-operator separation in the embedding space.All metrics are averaged over five independent runs.OperatorSuccess rate (↑) Novelty score (↑) Silhouette score (↑)</p>
<p>Table 4 :
4
Ablations on components, prompting, and operators.Numbers denote optimality gap (%, lower is better), averaged over 3 runs.Results are reported under a small evaluation budget, using TSP50 for training and TSP100 for testing, to highlight performance differences more clearly.
MethodsACOGLSTrainTestTrainTestw/o Outer Controller0.74 ± 0.39 5.61 ± 0.86 0.81 ± 0.14 2.88 ± 0.11w/o Dynamic Baseline1.55 ± 0.26 9.50 ± 4.24 0.62 ± 0.20 2.94 ± 0.18w/o Final Round1.26 ± 0.52 5.72 ± 0.78 0.39 ± 0.13 2.82 ± 0.06w/o Reasoning1.63 ± 0.94 7.88 ± 3.76 0.80 ± 0.23 3.05 ± 0.04w/o Active Player's History 0.82 ± 0.63 6.06 ± 1.27 0.53 ± 0.07 2.97 ± 0.04w/o Opponent's History0.97 ± 0.50 5.58 ± 1.31 0.48 ± 0.00 2.98 ± 0.13w/o Counter1.42 ± 0.07 6.71 ± 0.52 0.64 ± 0.19 3.20 ± 0.12w/o Learning0.91 ± 0.32 9.59 ± 5.74 0.62 ± 0.02 3.00 ± 0.12w/o Innovation1.73 ± 0,31 6.65 ± 0.80 0.42 ± 0.09 2.86 ± 0.00MOTIF (original)0.80 ± 0.28 5.21 ± 0.35 0.34 ± 0.19 2.73 ± 0.05</p>
<p>j * )
15:end for16:LOCALSEARCH(D, σ cur )17:if COST(D, σ cur ) &lt; c best then18:σ best ← σ cur19:c best ← COST(D, σ cur )20:end if21: end for22: Return: σ best</p>
<p>|C 0 | {I: improvement over baseline}
15: R k  16: C 0 ← C t17:Propagate baseline C 0 to all T k {dynamic baseline}18:else19:R k  *  ← R k  *  + 0.520:Revert π k  *  to previous best21:end if22:N k  *  ← N k  *  + 123: end for24: Return: Best strategies Π
* ← R k * + σ(I) {σ: sigmoid function}</p>
<p>current player p ← 1 3: for t = 1 to T do while there is at least one o ∈ O without a child annotated (o, p) at n do
4:n ← n 01. SELECTION:5:6:Compute for each o ∈ O:UCB(n, o, p) =V (n, o, p) N (n, o, p) + ϵ+ clno ′ N (n, o ′ , p) + 1 N (n, o, p) + ϵ7: o  9:n ← n ′ , p ← ¬p10:else11:break12:end if13:end while2. EXPANSION:14:if no child of n annotated by (o  *  , p) then15:π p (n ′ ) ← LLMGENERATE π p (n), o  *  , p16: 17:π ¬p (n ′ ) ← π ¬p (n) {copy opponent's code} Annotate n ′ with creator (o  *  , p)18:Add n ′ as a child of n19:else20:n ′ ← existing child21:end if3. SIMULATION:22:C p ← EVALUATE π p (n ′ )23:
* ← arg max o UCB(n, o, p) 8:if child n ′ annotated by (o * , p) exists then</p>
<p>o a , p a ) + Q p
C.4 Second Round: System-aware Refinement29:end for30:Best Update:31:if C p &lt; C  *  p then32:C  *  p ← C p , π  *  p ← π p (n ′ )33:end if34:p ← ¬p {switch to other player}35: end for36: Return: π  *  1 , π  *</p>
<p>Table 6 :
6
Benchmark setup across problems and algorithms
Algorithms Problems Instance size #Train size #Test sizeGLSTSP200564TSP50564CVRP50564ACOMKP50564OP50564BPP50564TSP100564DRCVRP50564BPP100564
For each problem domain, we synthesize training and test datasets, Dtrain and Dtest, followingYe et al. (</p>
<p>Table 7 :
7
Overview of hyperparameters used in MOTIF Each run of MOTIF consists of approximately 250 iterations, though the actual optimization time varies depending on problem complexity and evaluation batch size.For example, a typical run on ACO (TSP) takes about 1.5 hours.This includes around 0.5 million input tokens and 0.2 million output tokens consumed by the gpt-4o-mini-2024-07-18 model, costing roughly $0.18 per run.For fair comparison, all baseline methods are configured with a similar number of evaluations-typically 250-300 per problem-ensuring comparable budget constraints.
ComponentHyperparametersValueLLMModel Temperaturegpt-4o-mini-2024-07-18 1.0 (default)Outer ControllerOuter Iterations (T out ) Exploration coefficient (C out )√20 2 (default)Scaling factor (k)10Inner Iterations (T in )10Competitive MCTSExploration coefficient (C in ) Scaling factor (k)0.01 10Reward mixing weight (λ)0.7Final RoundFinal iterations (T final )10</p>
<p>Table 8 :
8
Dat, Doan, and Binh (2025) across problems and algorithmsWhile the work ofDat, Doan, and Binh (2025)introduced two metrics for measuring code diversity, they are only applicable to a population of implementations that has not yet been clustered.In contrast, MOTIF naturally produces clustered implementations based on operator type.Accordingly, we adopt the following two metrics: novelty and silhouette score.For each operator o, let S
Algorithms ProblemsHyperparametersGLSTSP#moves = 50, #iterations = 2000TSP#ants = 50, #iterations = 50CVRP#ants = 30, #iterations = 100ACOMKP#ants = 10, #iterations = 50OP#ants = 20, #iterations = 100BPP#ants = 20, #iterations = 50D.4 Evaluation Metrics
Silhouette Score.To evaluate intra-cluster cohesion and inter-cluster separation, we treat E(S (o) π ) as one cluster and all other embeddings E(S π \ S (o) π ) as a second cluster.For each embedding v ∈ E(S (o) π ), we compute the silhouette coefficient:where a(v) is the average cosine distance from v to all other embeddings within the same operator (intra-cluster distance), and b(v) is the average distance from v to embeddings from other operators (nearest-cluster distance).We then normalize to [0, 1] via:We report the mean silhouette score over all embeddings of that operator.A high score indicates that the operator's outputs form a tightly cohesive cluster that is well-separated from those of other operators.Complementarity of Novelty and Silhouette.We now provide a theoretical justification for using both novelty and silhouette score in tandem.π ) and all other embeddings u ∈ E(S π \ S (o) π ).The novelty of v is defined as the average of the top k smallest such distances:Here, b(v) corresponds to the average inter-cluster distance used in the silhouette score.Since k &lt; n, we have the following inequality:Adding k k i=1 d (i) (v) to both sides yields:This result confirms that novelty is always lower-bounded by b(v), the inter-cluster distance used in silhouette computation.Hence, high novelty does not guarantee high silhouette score.In particular, it is possible for an operator to produce highly novel implementations (large novelty) that are nonetheless scattered (small silhouette).This occurs most notably with the innovation operator, whose outputs often exhibit large a(v) ≈ b(v), leading to low silhouette despite high novelty.Thus, the two metrics serve complementary purposes: novelty captures dissimilarity to other operators, while silhouette reflects the internal cohesion of outputs.This also confirms that the outputs of the innovation operator are strongly dispersed in the embedding space, which is precisely the intended behavior of this operator by design.D.5 Examples of Generated Outputs
Neural Large Neighborhood Search for the Capacitated Vehicle Routing Problem. H André, T Kevin, C B Powley, E Whitehouse, D Lucas, S M Cowling, P I Rohlfshagen, P Tavener, S Perez, D Samothrakis, S Colton, S , IEEE Transactions on Computational Intelligence and AI in games. IOS Press. Browne2020. 20124A survey of monte carlo tree search methods</p>
<p>Hyper-heuristics: A survey of the state of the art. E Burke, M Gendreau, M Hyde, G Kendall, G Ochoa, E Özcan, R Qu, Journal of the Operational Research Society. 642013</p>
<p>A classification of hyper-heuristic approaches. E Burke, M Hyde, G Kendall, G Ochoa, E Özcan, 2010a</p>
<p>A genetic programming hyper-heuristic approach for evolving 2-D strip packing heuristics. E K Burke, M Hyde, G Kendall, J Woodward, IEEE Transactions on Evolutionary Computation. 1462010b</p>
<p>Exploring hyper-heuristic methodologies with genetic programming. E K Burke, M R Hyde, G Kendall, G Ochoa, E Ozcan, J R Woodward, Computational intelligence: Collaboration, fusion and emergence. Springer2009</p>
<p>A classification of hyperheuristic approaches: revisited. E K Burke, M R Hyde, G Kendall, G Ochoa, E Özcan, J R Woodward, Handbook of metaheuristics. Springer2018</p>
<p>P Cheng, T Hu, H Xu, Z Zhang, Z Yuan, Y Dai, L Han, N Du, X Li, arXiv:2404.10642Coulom, R. 2007a. Computing Elo Ratings of Move Patterns in the Game of Go. 202530Self-playing Adversarial Language Game Enhances LLM Reasoning</p>
<p>Computing "elo ratings" of move patterns in the game of go. R Coulom, ICGA journal. 3042007b</p>
<p>Hsevo: Elevating automatic heuristic design with diversity-driven harmony search and genetic algorithm using llms. P V T Dat, L Doan, H T T Binh, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>Heuristic and meta-heuristic algorithms and their relevance to the real world: a survey. S Desale, A Rasool, S Andhale, P Rane, Int. J. Comput. Eng. Res. Trends. 35152015</p>
<p>Ant colony optimization. M Dorigo, M Birattari, T Stutzle, IEEE computational intelligence magazine. 142007</p>
<p>Ant colony system: a cooperative learning approach to the traveling salesman problem. M Dorigo, L Gambardella, IEEE Transactions on Evolutionary Computation. 111997</p>
<p>Ant System: Optimization by a colony of cooperating agents. M Dorigo, V Maniezzo, A Colorni, IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society. 199626</p>
<p>Ant colony optimization: overview and recent advances. Handbook of metaheuristics. M Dorigo, T Stützle, 2018</p>
<p>Improving language model negotiation with self-play and in-context learning from ai feedback. Y Fu, H Peng, T Khot, M Lapata, arXiv:2305.101422023aarXiv preprint</p>
<p>Z.-H Fu, S Sun, J Ren, T Yu, H Zhang, Y Liu, L Huang, X Yan, P Lu, arXiv:2308.04639A Hierarchical Destroy and Repair Approach for Solving Very Large-Scale Travelling Salesman Problem. 2023b</p>
<p>B Hudson, Q Li, M Malencia, A Prorok, arXiv:2110.05291Graph Neural Network Guided Local Search for the Traveling Salesperson Problem. 2022</p>
<p>A Kong, S Zhao, H Chen, Q Li, Y Qin, R Sun, X Zhou, E Wang, X Dong, arXiv:2308.07702Better Zero-Shot Reasoning with Role-Play Prompting. 2024</p>
<p>Foundations of genetic programming. W B Langdon, R Poli, 2013Springer Science &amp; Business Media</p>
<p>K Li, F Liu, Z Wang, Q Zhang, arXiv:2502.16170Destroy and Repair Using Hyper Graphs for Routing. 2025</p>
<p>Evolution of heuristics: towards efficient automatic algorithm design using large language model. F Liu, X Tong, M Yuan, X Lin, F Luo, Z Wang, Z Lu, Q Zhang, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning2024</p>
<p>N F Liu, K Lin, J Hewitt, A Paranjape, M Bevilacqua, F Petroni, P Liang, arXiv:2307.03172Lost in the Middle: How Language Models Use Long Contexts. 2023</p>
<p>Pareto Front Grid Guided Multiobjective Optimization In Dynamic Pickup And Delivery Problem Considering Two-Sided Fairness. A Y Ng, D Harada, S Russell, H Phan Duc, D Bui Trong, T Nguyen Thi, B Huynh Thi Thanh, Proceedings of the Genetic and Evolutionary Computation Conference. the Genetic and Evolutionary Computation Conference1999. 202599Icml</p>
<p>N Pillay, R Qu, Hyper-heuristics: theory and applications. Springer2018</p>
<p>Heuristic algorithm for scheduling in a flowshop to minimize total flowtime. C Rajendran, International Journal of Production Economics. 2911993</p>
<p>Mathematical discoveries from program search with large language models. B Romera-Paredes, M Barekatain, A Novikov, M Balog, M P Kumar, E Dupont, F J Ruiz, J S Ellenberg, P Wang, O Fawzi, Nature. 62579952024</p>
<p>A new local search algorithm providing high quality solutions to vehicle routing problems. P Shaw, Glasgow. 461997APES Group, Dept of Computer Science, University of Strathclyde</p>
<p>Using constraint programming and local search methods to solve vehicle routing problems. P Shaw, International conference on principles and practice of constraint programming. Springer1998</p>
<p>A hybrid genetic algorithm based on harmony search and its improving. W.-W Shi, W Han, W.-C Si, Springer, N Shinn, F Cassano, A Gopinath, K Narasimhan, S Yao, Advances in Neural Information Processing Systems. 2012. 202336Informatics and Management Science I</p>
<p>Mastering the game of Go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, 2016529nature</p>
<p>Neu-ralGLS: learning to guide local search with graph convolutional network for the traveling salesman problem. J Sui, S Ding, B Xia, R Liu, D Bu, Neural Computing and Applications. 36172024</p>
<p>A comprehensive review of coverage path planning in robotics using classical and heuristic algorithms. C S Tan, R Mohd-Mokhtar, M R Arshad, IEEE Access. 92021</p>
<p>Guided local search and its application to the traveling salesman problem. C Voudouris, E Tsang, European Journal of Operational Research. 11321999</p>
<p>Improving Rationality in the Reasoning Process of Language Models through Self-playing Game. P Wang, J Li, Z Tang, H Gui, Forty-second International Conference on Machine Learning. 2025</p>
<p>Y Wang, H Le, A D Gotmare, N D Q Bui, J Li, S C H Hoi, arXiv:2305.07922CodeT5+: Open Code Large Language Models for Code Understanding and Generation. 2023</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>Y Wu, W Song, Z Cao, J Zhang, arXiv:2111.03466Learning Large Neighborhood Search Policy for Integer Programming. 2021</p>
<p>L Xin, W Song, Z Cao, J Zhang, arXiv:2110.07983NeuroLKH: Combining Deep Learning Model with Lin-Kernighan-Helsgaun Heuristic for Solving the Traveling Salesman Problem. 2021</p>
<p>S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, arXiv:2305.10601Tree of Thoughts: Deliberate Problem Solving with Large Language Models. 2023</p>
<p>Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based Automatic Heuristic Design. H Ye, J Wang, Z Cao, F Berto, C Hua, H Kim, J Park, G ; Song, J Wang, Z Cao, H Liang, Y Li, Z Zheng, Z Xie, Z Wang, B Hooi, arXiv:2309.14032Deep-ACO: Neural-enhanced Ant Systems for Combinatorial Optimization. Ye, H2024. 2023. 202537Forty-second International Conference on Machine Learning</p>            </div>
        </div>

    </div>
</body>
</html>
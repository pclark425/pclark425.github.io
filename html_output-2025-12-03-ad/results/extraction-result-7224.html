<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7224 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7224</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7224</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-272423588</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.03381v1.pdf" target="_blank">CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks</a></p>
                <p><strong>Paper Abstract:</strong> Cognitive psychology investigates perception, attention, memory, language, problem-solving, decision-making, and reasoning. Kahneman’s dual-system theory elucidates the human decision-making process, distinguishing between the rapid, intuitive System 1 and the deliberative, rational System 2. Recent advancements have positioned large language Models (LLMs) as formidable tools nearing human-level proficiency in various cognitive tasks. Nonetheless, the presence of a dual-system framework analogous to human cognition in LLMs remains unexplored. This study introduces the CogniDual Framework for LLMs (CFLLMs), designed to assess whether LLMs can, through self-training, evolve from deliberate deduction to intuitive responses, thereby emulating the human process of acquiring and mastering new information. Our findings reveal the cognitive mechanisms behind LLMs’ response generation, enhancing our understanding of their capabilities in cognitive psychology. Practically, self-trained models can provide faster responses to certain queries, reducing computational demands during inference.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7224.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7224.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GSM8K (Grade School Math 8K)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset of ~8,000 human-authored grade-school math word problems used to evaluate arithmetic and multi-step mathematical reasoning in LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-7B; LLaMA2-13B; Vicuna-7B; Vicuna-13B; Vicuna-30B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer-based large language models (LLaMA2 family and Vicuna instruction-tuned variants) evaluated in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 13B, 30B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>GSM8K (math word problems)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning / arithmetic problem-solving</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Measures ability to solve grade-school math word problems requiring multi-step arithmetic reasoning; administered as few-shot prompts in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (percentage correct answers)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>CoT not applied (few-shot, direct-answer prompts): LLaMA2-7B 14.6%, LLaMA2-13B 27.9%, Vicuna-7B 12.1%, Vicuna-13B 15.7%, Vicuna-30B 35.6%. CoT applied (few-shot chain-of-thought prompting): LLaMA2-7B 16.8%, LLaMA2-13B 29.1%, Vicuna-7B 14.5%, Vicuna-13B 16.1%, Vicuna-30B 36.7%. The paper also reports CFLLMs (self-practice / LoRA) variants and describes small or negligible improvements on GSM8K attributable to task contamination; specific CFLLMs numbers are reported in Table I of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot prompting; compared both without chain-of-thought (CoT not applied; direct-answer few-shot) and with chain-of-thought (CoT applied; few-shot); CFLLMs = self-practice + LoRA fine-tuning using distilled question-answer pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The paper notes negligible improvement from CFLLMs on GSM8K, attributing this to task contamination (models often produce step-by-step derivations even when instructed not to). No human baseline performance for GSM8K is reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7224.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7224.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReClor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReClor (Reading Comprehension with Logical Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset of reading-comprehension questions drawn from standardized tests (e.g., GMAT/LSAT) designed to evaluate logical and critical reasoning in NLP models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-7B; LLaMA2-13B; Vicuna-7B; Vicuna-13B; Vicuna-30B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer-based large language models (LLaMA2 family and Vicuna instruction-tuned variants) evaluated in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 13B, 30B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>ReClor (logical reading comprehension)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning / logical inference / reading comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice reading-comprehension questions requiring logical deduction and critical reasoning; administered in few-shot format with and without chain-of-thought reasoning prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (percentage correct answers)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>CoT not applied (few-shot direct-answer): LLaMA2-7B 34.1%, LLaMA2-13B 59.9%, Vicuna-7B 41.3%, Vicuna-13B 62.5%, Vicuna-30B 75.2%. CoT applied (few-shot chain-of-thought): LLaMA2-7B 51.5%, LLaMA2-13B 89.4%, Vicuna-7B 55.1%, Vicuna-13B 90.8%, Vicuna-30B 99.4%. CFLLMs self-practice variants reported in Table I show improvements in non-CoT accuracy for many models (see paper Table I for per-variant numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot prompting; compared without chain-of-thought (direct-answer) and with chain-of-thought; CFLLMs = self-practice + LoRA fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The paper emphasizes that CoT prompting substantially improves performance on ReClor and that CFLLMs self-practice can substantially increase non-CoT accuracy for tasks exhibiting large CoT/non-CoT gaps. No human baseline (e.g., human test-taker accuracy on ReClor) is provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7224.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7224.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogiQA2.0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogiQA2.0 (Logical Reasoning QA, v2.0)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An improved dataset for logical reasoning in natural language understanding, containing questions from civil service examinations and validated translations, used to test general natural-language logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-7B; LLaMA2-13B; Vicuna-7B; Vicuna-13B; Vicuna-30B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer-based large language models (LLaMA2 family and Vicuna instruction-tuned variants) evaluated in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 13B, 30B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>LogiQA2.0 (logical reasoning / reading comprehension)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning / logical deduction</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice logical-reasoning questions (derived and validated from civil service exams) that test the model's capacity for generalizing natural-language logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (percentage correct answers)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>CoT not applied (few-shot direct-answer): LLaMA2-7B 0.5%, LLaMA2-13B 5.3%, Vicuna-7B 0.1%, Vicuna-13B 5.1%, Vicuna-30B 27.6%. CoT applied (few-shot chain-of-thought): LLaMA2-7B 77.3%, LLaMA2-13B 97.9%, Vicuna-7B 79.4%, Vicuna-13B 98.4%, Vicuna-30B 99.3%. The paper reports that CFLLMs (self-practice) substantially improved non-CoT performance for these datasets (see Table I for CFLLMs per-variant values).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot prompting; compared without chain-of-thought (direct-answer) and with chain-of-thought; CFLLMs = self-practice + LoRA fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The authors highlight that without CoT prompting small models perform near-zero on LogiQA2.0, but CoT dramatically raises accuracy; CFLLMs self-practice reduced this gap substantially for many models. No human baseline performance on LogiQA2.0 is reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GSM8K: A dataset of grade school math word problems <em>(Rating: 2)</em></li>
                <li>ReClor: A reading comprehension dataset requiring logical reasoning <em>(Rating: 2)</em></li>
                <li>LogiQA 2.0-an improved dataset for logical reasoning in natural language understanding <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7224",
    "paper_id": "paper-272423588",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "GSM8K",
            "name_full": "GSM8K (Grade School Math 8K)",
            "brief_description": "A dataset of ~8,000 human-authored grade-school math word problems used to evaluate arithmetic and multi-step mathematical reasoning in LLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA2-7B; LLaMA2-13B; Vicuna-7B; Vicuna-13B; Vicuna-30B",
            "model_description": "Open-source transformer-based large language models (LLaMA2 family and Vicuna instruction-tuned variants) evaluated in the study.",
            "model_size": "7B, 13B, 30B",
            "test_name": "GSM8K (math word problems)",
            "test_category": "reasoning / arithmetic problem-solving",
            "test_description": "Measures ability to solve grade-school math word problems requiring multi-step arithmetic reasoning; administered as few-shot prompts in this study.",
            "evaluation_metric": "accuracy (percentage correct answers)",
            "human_performance": null,
            "llm_performance": "CoT not applied (few-shot, direct-answer prompts): LLaMA2-7B 14.6%, LLaMA2-13B 27.9%, Vicuna-7B 12.1%, Vicuna-13B 15.7%, Vicuna-30B 35.6%. CoT applied (few-shot chain-of-thought prompting): LLaMA2-7B 16.8%, LLaMA2-13B 29.1%, Vicuna-7B 14.5%, Vicuna-13B 16.1%, Vicuna-30B 36.7%. The paper also reports CFLLMs (self-practice / LoRA) variants and describes small or negligible improvements on GSM8K attributable to task contamination; specific CFLLMs numbers are reported in Table I of the paper.",
            "prompting_method": "Few-shot prompting; compared both without chain-of-thought (CoT not applied; direct-answer few-shot) and with chain-of-thought (CoT applied; few-shot); CFLLMs = self-practice + LoRA fine-tuning using distilled question-answer pairs.",
            "fine_tuned": null,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "The paper notes negligible improvement from CFLLMs on GSM8K, attributing this to task contamination (models often produce step-by-step derivations even when instructed not to). No human baseline performance for GSM8K is reported in this paper.",
            "uuid": "e7224.0",
            "source_info": {
                "paper_title": "CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "ReClor",
            "name_full": "ReClor (Reading Comprehension with Logical Reasoning)",
            "brief_description": "A dataset of reading-comprehension questions drawn from standardized tests (e.g., GMAT/LSAT) designed to evaluate logical and critical reasoning in NLP models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA2-7B; LLaMA2-13B; Vicuna-7B; Vicuna-13B; Vicuna-30B",
            "model_description": "Open-source transformer-based large language models (LLaMA2 family and Vicuna instruction-tuned variants) evaluated in the study.",
            "model_size": "7B, 13B, 30B",
            "test_name": "ReClor (logical reading comprehension)",
            "test_category": "reasoning / logical inference / reading comprehension",
            "test_description": "Multiple-choice reading-comprehension questions requiring logical deduction and critical reasoning; administered in few-shot format with and without chain-of-thought reasoning prompts.",
            "evaluation_metric": "accuracy (percentage correct answers)",
            "human_performance": null,
            "llm_performance": "CoT not applied (few-shot direct-answer): LLaMA2-7B 34.1%, LLaMA2-13B 59.9%, Vicuna-7B 41.3%, Vicuna-13B 62.5%, Vicuna-30B 75.2%. CoT applied (few-shot chain-of-thought): LLaMA2-7B 51.5%, LLaMA2-13B 89.4%, Vicuna-7B 55.1%, Vicuna-13B 90.8%, Vicuna-30B 99.4%. CFLLMs self-practice variants reported in Table I show improvements in non-CoT accuracy for many models (see paper Table I for per-variant numbers).",
            "prompting_method": "Few-shot prompting; compared without chain-of-thought (direct-answer) and with chain-of-thought; CFLLMs = self-practice + LoRA fine-tuning.",
            "fine_tuned": null,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "The paper emphasizes that CoT prompting substantially improves performance on ReClor and that CFLLMs self-practice can substantially increase non-CoT accuracy for tasks exhibiting large CoT/non-CoT gaps. No human baseline (e.g., human test-taker accuracy on ReClor) is provided in this paper.",
            "uuid": "e7224.1",
            "source_info": {
                "paper_title": "CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LogiQA2.0",
            "name_full": "LogiQA2.0 (Logical Reasoning QA, v2.0)",
            "brief_description": "An improved dataset for logical reasoning in natural language understanding, containing questions from civil service examinations and validated translations, used to test general natural-language logical reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA2-7B; LLaMA2-13B; Vicuna-7B; Vicuna-13B; Vicuna-30B",
            "model_description": "Open-source transformer-based large language models (LLaMA2 family and Vicuna instruction-tuned variants) evaluated in the study.",
            "model_size": "7B, 13B, 30B",
            "test_name": "LogiQA2.0 (logical reasoning / reading comprehension)",
            "test_category": "reasoning / logical deduction",
            "test_description": "Multiple-choice logical-reasoning questions (derived and validated from civil service exams) that test the model's capacity for generalizing natural-language logical reasoning.",
            "evaluation_metric": "accuracy (percentage correct answers)",
            "human_performance": null,
            "llm_performance": "CoT not applied (few-shot direct-answer): LLaMA2-7B 0.5%, LLaMA2-13B 5.3%, Vicuna-7B 0.1%, Vicuna-13B 5.1%, Vicuna-30B 27.6%. CoT applied (few-shot chain-of-thought): LLaMA2-7B 77.3%, LLaMA2-13B 97.9%, Vicuna-7B 79.4%, Vicuna-13B 98.4%, Vicuna-30B 99.3%. The paper reports that CFLLMs (self-practice) substantially improved non-CoT performance for these datasets (see Table I for CFLLMs per-variant values).",
            "prompting_method": "Few-shot prompting; compared without chain-of-thought (direct-answer) and with chain-of-thought; CFLLMs = self-practice + LoRA fine-tuning.",
            "fine_tuned": null,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "The authors highlight that without CoT prompting small models perform near-zero on LogiQA2.0, but CoT dramatically raises accuracy; CFLLMs self-practice reduced this gap substantially for many models. No human baseline performance on LogiQA2.0 is reported in this paper.",
            "uuid": "e7224.2",
            "source_info": {
                "paper_title": "CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GSM8K: A dataset of grade school math word problems",
            "rating": 2,
            "sanitized_title": "gsm8k_a_dataset_of_grade_school_math_word_problems"
        },
        {
            "paper_title": "ReClor: A reading comprehension dataset requiring logical reasoning",
            "rating": 2,
            "sanitized_title": "reclor_a_reading_comprehension_dataset_requiring_logical_reasoning"
        },
        {
            "paper_title": "LogiQA 2.0-an improved dataset for logical reasoning in natural language understanding",
            "rating": 2,
            "sanitized_title": "logiqa_20an_improved_dataset_for_logical_reasoning_in_natural_language_understanding"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 1,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        }
    ],
    "cost": 0.011379499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks
6 Sep 2024</p>
<p>Yongxin Deng 
School of Electronic and Electrical Engineering
Shanghai University of Engineering Science
ShanghaiChina</p>
<p>Xihe Qiu qiuxihe@sues.edu.cn 
School of Electronic and Electrical Engineering
Shanghai University of Engineering Science
ShanghaiChina</p>
<p>Xiaoyu Tan 
INF Technology (shanghai) Co., Ltd
ShanghaiChina</p>
<p>Chao Qu 
INF Technology (shanghai) Co., Ltd
ShanghaiChina</p>
<p>Jing Pan 
School of Art, Design and Architecture
Monash University
MelbourneAustralia</p>
<p>Yuan Cheng 
INF Technology (shanghai) Co., Ltd
ShanghaiChina</p>
<p>Yinghui Xu 
Artificial Intelligence Innovation and Incubation Institute
Fudan University
ShanghaiChina</p>
<p>Wei Chu 
INF Technology (shanghai) Co., Ltd
ShanghaiChina</p>
<p>CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks
6 Sep 2024CA50E26C0C3928F985AFEEDC4CFA8515arXiv:2409.03381v2[cs.CL]large language modelcognitive psychologylanguage processing
Cognitive psychology investigates perception, attention, memory, language, problem-solving, decision-making, and reasoning.Kahneman's dual-system theory elucidates the human decision-making process, distinguishing between the rapid, intuitive System 1 and the deliberative, rational System 2. Recent advancements have positioned large language Models (LLMs) as formidable tools nearing human-level proficiency in various cognitive tasks.Nonetheless, the presence of a dualsystem framework analogous to human cognition in LLMs remains unexplored.This study introduces the CogniDual Framework for LLMs (CFLLMs), designed to assess whether LLMs can, through self-training, evolve from deliberate deduction to intuitive responses, thereby emulating the human process of acquiring and mastering new information.Our findings reveal the cognitive mechanisms behind LLMs' response generation, enhancing our understanding of their capabilities in cognitive psychology.Practically, self-trained models can provide faster responses to certain queries, reducing computational demands during inference.</p>
<p>I. INTRODUCTION</p>
<p>Cognitive psychology seeks to elucidate the processes by which humans acquire, retain, and retrieve knowledge [1], [2].Kahneman's dual-system theory [3]- [8] emerges as a seminal framework within this realm, offering a nuanced understanding of cognitive operations.This theory outlines two distinct cognitive systems: System 1, which is instinctual and facilitates rapid decision-making with minimal cognitive effort, and System 2, which is methodical and requires deliberate focus for complex reasoning tasks.</p>
<p>In the realm of artificial intelligence, the advent of deep learning and the influx of extensive datasets have precipitated the swift advancement of language models (LMs).These models, particularly those utilizing the Transformer architecture [9] such as GPT-4 [10], have garnered significant attention for their advanced language processing abilities [11]- [14], achieving near-human proficiency across numerous linguistic tasks.Trained on expansive natural language corpora, these models demonstrate an ability to comprehend and produce symbol sequences with an intuition akin to human System 1's pattern processing.Furthermore, when prompted to employ CoT problem-solving, LLMs exhibit deep reasoning capabilities paralleling human System 2 [12], [15], [16].Nevertheless, the persistence of such efficient and accurate outputs in the absence of CoT remains uncertain.Should LLMs achieve this, it would suggest the integration of an intuitive operational process comparable to human System 1.</p>
<p>Can LLMs internalize System 2's complex reasoning into System 1's intuitive responses through iterative training?We hypothesize that LLMs, by mimicking human rapid skill acquisition, can generate fast, intuitive answers without additional training data, thus enhancing resource efficiency and reducing dependence on chains of thought (CoT).Our methodology was straightforward yet robust: we commenced by prompting the model with specific reasoning questions, both with and without CoT cues, and subsequently assessed the accuracy of the responses generated under each condition.Following this, the model employed CoT as a scaffold to reengineer non-CoT responses.Theoretically, this self-editing could facilitate the internalization of CoT reasoning steps, potentially enhancing the model's future problem-solving precision without explicit CoT prompts.The final phase involved reevaluating the model's performance post self-improvement to determine if there was an enhancement in CoT-independent operations.Our experiments are designed to uncover whether LLMs can emulate the human cognitive system by internalizing complex reasoning processes, particularly when functioning without direct reasoning instructions.</p>
<p>We applied our methodology to the Vicuna and Llama2 models of varying sizes and evaluated their performance enhancements on reasoning datasets such as GSM8K, ReClor, and LogiQA 2.0.Our findings indicate that LLMs display marked discrepancies in response accuracy when utilizing CoT compared to when it is absent.Following a period of selftraining, LLMs exhibited a substantial increase in response precision in scenarios devoid of CoT.This suggests that LLMs are capable of developing intuitive response mechanisms akin to the human cognitive System 1, as well as the deliberate, sequential reasoning characteristic of System 2. Our research demonstrates the potential to cultivate LLMs' System 2 skills into System 1 proficiencies, enabling rapid application.</p>
<p>This paper presents three principal contributions:</p>
<p>• We propose a self-iterative framework for large models, which is used to explore whether large models themselves possess characteristics similar to human cognitive structures.</p>
<p>• We demonstrate that LLMs can simulate the dual-system characteristics of human cognition.Through experimentation, we have verified that LLMs can not only perform complex reasoning tasks under the guidance of CoT (similar to human System 2), but also respond relying on pattern recognition intuition without CoT (similar to human System 1).• We propose and validate a new method that may allow LLMs to maintain efficient and accurate outputs without relying on CoT.This suggests that LLMs can handle tasks in a manner closer to human System 1, and this method is more efficient in terms of computational resources and time because it avoids additional training data or steps, thus it is expected to play a significant role in resourcelimited application scenarios.</p>
<p>II. COGNIDUAL FRAMEWORK</p>
<p>A. Model Self-Iteration</p>
<p>Our CogniDual framework replicates the human learning curve, as depicted in Figure 1.Initially, we prompt untrained LLMs to answer questions from reasoning datasets without CoT instructions, even compelling the LLMs to provide immediate answers without rationale.We designate this question set as Q n = {q i , for i = 1, 2, . . ., n}, with n denoting the total number of questions.The corresponding answer set is labeled A1 n = {a1 i , for i = 1, 2, . . ., n}, symbolizing the LLMs' initial responses, akin to human cognitive System 1.</p>
<p>These question-answer pairs are preserved.</p>
<p>Subsequently, we introduce CoT directives, guiding LLMs to derive correct answers sequentially.The resulting answers are categorized as A2 n = {a2 i , for i = 1, 2, . . ., n}.We also maintain these pairs.In the third phase, we furnish the LLMs with standard answers from the dataset, denoted as A n = {a i , for i = 1, 2, . . ., n}.</p>
<p>To quantify the proficiency of our LLMs in responding to Q n , we define the accuracy metric as follows:
Acc(A1 n , A n ) = 1 n n i=1 SemanticMatch(a1 i , a i ),(1)Acc(A2 n , A n ) = 1 n n i=1 SemanticMatch(a2 i , a i ),(2)
where SemanticMatch(•, •) assesses the semantic similarity between the LLM's initial response and the standard answer.</p>
<p>Given that standard answers usually include comprehensive reasoning and do not conform to a 'yes or no' format, we cannot rely on character matching scripts to evaluate the LLMs' responses.Instead, we engage the LLMs in semantic synonymy judgments to assess the accuracy of A1 n and A2 n against A n , specifically identifying instances where A2 n is accurate, and A1 n is not.The fourth stage involves the LLMs consolidating the correct answers from A2 n and the incorrect ones from A1 n into new question-answer pairs.Given that A2 n responses encompass extensive reasoning, we require LLMs to distill these answers, converting them from elaborate, reasoned responses to concise answers.The specifics of this process will be explored in Section II-B.In the final step, we employ these restructured question-answer pairs as training material for LLMs and subsequently assess the LLMs' reasoning capabilities on different questions within the same dataset without CoT.Our experimental approach, chosen for its minimal computational resource demands and deployability, utilizes the LoRA training method [17] for LLMs.</p>
<p>B. Pre-training Model Distillation</p>
<p>The framework outlined in Section II-A enables LMs to selftrain independently of external interaction.Nevertheless, our framework necessitates that models complete two supplementary tasks when addressing dataset questions: 1. Synonymy Semantic Judgment: LMs must determine if A1 n , A2 n , and A n are semantically equivalent to assess reasoning accuracy both with and without the CoT; 2. Answer Rewriting: Recognizing the impracticality of manually rewriting numerous answers containing reasoning processes into concise responses, we expect LMs to autonomously perform answer rewriting.Suppose we have an open-source LLM denoted by p θ , which is parameterized by θ.To perform synonymy semantic judgment and achieve SemanticMatch, we can design a specific prompt, Prompt Semantic .This prompt will evaluate whether the two responses a i and a j have identical semantic meanings:
SemanticMatch(a i , a j ) = p θ (a j , a i |Prompt Semantic ). (3)
For answer rewriting, which is also a common task for chat base LLM, we can design a Prompt Rewrite to align the generated answer a j to the target answer a i with the identical format, and acquire the updated answer a ′ i :
a ′ i = p θ (a j , a i |Prompt Rewrite ).(4)
While large-scale models readily accomplish these tasks, smaller models, such as the Llama2-7B, may find them challenging [18].A model that struggles to understand standard answers is unlikely to enhance its capabilities from System 2 to System 1 through self-training alone.To improve training outcomes, we advocate for the pre-training of smaller models using knowledge distillation, equipping them with essential skills for synonymy semantic judgment and answer rewriting.</p>
<p>Knowledge distillation [19], [19]- [24] is a technique for transferring knowledge from a large, complex 'teacher' model to a smaller, simpler 'student' model, facilitating deployment in resource-limited settings without greatly impacting performance.We employ a simplified approach akin to Distilling step-by-step [25].For synonymy semantic judgment, smaller models generate sample A1 n , A2 n , and A n , after which GPT-3.5's advanced generative power yields precise judgments and comprehensive explanations.By creating multiple explanations per question, we ensure a clear delineation of the reasoning pathway.GPT-3.5 also supplies sample rewrites and their justifications for the answer rewriting task.Subsequently, smaller model θ can be trained through supervised fine-tuning using the larger models' outputs A ′ , preparing them for selfimprovement and independent practice:
min θ −E (qi,a ′ i )∼A ′ [log p θ (a ′ i |q i )] .(5)</p>
<p>III. EXPERIMENT</p>
<p>A. Experimental Objectives</p>
<p>This experiment is designed to examine various questions concerning the cognitive and reasoning capabilities of LLMs such as Llama2.Specifically, we aim to determine whether such models exhibit characteristics analogous to the dualsystem cognitive framework observed in humans (Q1), if selfpractice in the absence of Chain of Thought (CoT) guidance enhances reasoning abilities (Q2), whether learning curves indicate improved accuracy with additional examples post selfpractice (Q3), if larger models benefit more from self-practice without CoT guidance in terms of performance (Q4), and whether the enhanced reasoning abilities generalized across different reasoning tasks (Q5).</p>
<p>B. Experimental Setting</p>
<p>To investigate Q1 and Q2 outlined in Section III-A, we employed untrained LLMs as a baseline to evaluate their efficacy both with and without implementing the CoT method.The few-shot methodology was consistently applied in prompt construction, irrespective of CoT method utilization.Given that the GSM8K dataset's solutions entail reasoning sequences, in instances where the CoT method was not applied, we modified the prompting using GPT-4 [10] to exclude the reasoning pathway, employing an 8-shot technique.In contrast, for the Reclor and LogiQA2.0 datasets, which naturally lack reasoning pathways in their answers, we engaged GPT-4 to fabricate corresponding reasoning sequences to assess LLMs' proficiency under the CoT paradigm, adopting a 3-shot approach for these datasets.This baseline was then juxtaposed with our CFLLMs framework.To tackle Q3, we experimented with diverse data volumes within the CFLLMs framework to cultivate the LLMs and scrutinized their performance.</p>
<p>In pursuit of Q4, our framework was applied to LLMs of varying sizes, including Vicuna models (7B, 13B, 30B) [26], [27] and Llama2 models (7B, 13B) [28].To facilitate deployment on a consumer-grade Nvidia RTX 4090 GPU [29] while minimizing memory usage and inference time, we employed the GPTQ [30] approach to quantize the models to 4-bit precision.It is important to note that, despite the ability of 7B-sized models to operate on consumer-grade GPUs without quantization, we opted for 4-bit quantization across all models to maintain a uniform comparison scale and minimize quantization errors.</p>
<p>To address the variability in dataset sizes and their potential influence on experimental results, we standardized our approach by extracting a consistent sample of 1000 data entries from each dataset to form the training set for the LLMs' self-practice.An additional 1000 data entries were selected to comprise the test set.To maintain experimental uniformity, each data entry within these subsets was numbered.For each experiment, we consistently used the first n numbered data entries, with n representing the requisite volume of data for the specific experimental conditions.</p>
<p>For Q5, we selected datasets encompassing various reasoning tasks, such as GSM8K [31], ReClor [32], and LogiQA2.0 [33], [34].GSM8K comprises over 8,000 quality elementary mathematics problems crafted by human authors to assess arithmetic reasoning in LLMs.ReClor features questions from logical reasoning sections of standardized tests like the GMAT and LSAT, challenging the LLMs' critical thinking and complex logical reasoning skills.LogiQA2.0, based on questions from the Chinese civil service exam translated and validated by professional translators and human experts, evaluates the LLMs' capacity for generalizing natural language reasoning.</p>
<p>C. Results</p>
<p>We conducted experiments across a variety of LLMs, differing in type and size, as well as on diverse datasets, to evaluate their reasoning capabilities.This evaluation was based on the mean answer accuracy derived from five experimental trials, detailed in Table I.It is important to note that the figures succeeding "CFLLMs" in the table signify the volume of data utilized for the LLMs' self-practice.The underscored values denote the peak accuracy attained with this methodology for the consistent model and dataset, whereas the bolded values represent the maximum accuracy achieved without employing the CoT method to prompt incremental reasoning, compelling the model to directly generate answers.Red-highlighted numbers in the table reveal that our framework, under the given experimental conditions, did not improve but rather diminished performance.With this groundwork, we can address Q1 and Q2 introduced in Section III-A.The implementation of CoT markedly influences the models' reasoning proficiency on tasks that entail natural language inference, such as reading comprehension and logical deduction.For instance, on the LogiQA2.0 dataset, the accuracy rates for smaller models like Llama2-7B and Vicuna-7B plummet to near zero in the absence of CoT.However, the deployment of the CogniDual Framework, has resulted in a substantial enhancement of performance without CoT.Despite the models' reasoning accuracy not equalling that of CoT use, their ability to intuitively respond to certain questions suggests an inherent decision-making logic akin to the human dual-system cognitive framework.This insight indicates the potential for transforming System 2 capabilities into System 1 through sustained practice, thereby bolstering the LLMs' rapid response to specific queries and diminishing the time and computational resources required for reasoning.Moreover, we observed a negligible improvement from the CogniDual Framework on the GSM8K dataset, attributed to the models' propensity for step-by-step reasoning even when instructed to directly answer.The prevalence of LLMs producing answers with comprehensive derivations is likely due to task contamination, as postulated by Liu et al. [35], where mathematical problems are consistently presented with accompanying detailed solutions throughout the training phase.Our framework aims to enhance the System 1 capabilities of LLMs, rather than augment System 2 directly.Consequently, we can deduce from Q5 that only tasks exhibiting a substantial discrepancy in accuracy between CoT usage and non-usage enable LLMs to advance their internalized reasoning abilities through self-practice.</p>
<p>For Q3 and Q4, the results in Table I indicate that, in general, an increase in additional examples correlates with a more pronounced enhancement in the LLMs' reasoning abilities without CoT, achieved through self-practice.Larger models require fewer examples to approach their System 1 capacity ceiling; beyond this point, further example data yield minimal benefits.This finding suggests that larger models are more adept at leveraging limited data to improve performance without CoT guidance through self-practice, aligning with the research by Jaimovitch et al. [36].</p>
<p>IV. CONCLUSION</p>
<p>This study explores the dual cognitive characteristics of LLMs.Our experimental results indicate that once LLMs internalize CoT reasoning through self-training, they can retain CoT-enhanced problem-solving abilities even without CoT prompts.This finding supports the hypothesis that, with appropriate training, LLMs can convert complex, deliberative System 2 reasoning into faster, more intuitive System 1-like responses.Leveraging this property, we designed a self-training framework to reduce the cognitive load of LLM reasoning.Despite these advancements, further research is necessary to address the study's limitations, including examining how this framework influences the cognitive processing preferences of LLMs.</p>
<p>TABLE I PERFORMANCE
I
EVALUATION OF DIVERSE LLMS BY TYPE AND SIZE ACROSS VARIOUS DATASETS: A DETAILED ASSESSMENT WITH METHODOLOGICAL COMPARISONS.
MethodDatasetsModels LLAMA2-7B LLAMA2-13B Vicuna-7B Vicuna-13B Vicuna-30BGSM8K(Acc)14.627.912.115.735.6CoT Not AppliedReclor(Acc)34.159.941.362.575.2LogiQA2.0(Acc)0.55.30.15.127.6GSM8K(Acc)16.829.114.516.136.7CoT AppliedReclor(Acc)51.589.455.190.899.4LogiQA2.0(Acc)77.397.979.498.499.3GSM8K(Acc)14.928.412.216.135.7CFLLMs(10)Reclor(Acc)356541.865.680.3LogiQA2.0(Acc)2.812.21.29.252.7GSM8K(Acc)14.928.611.915.835.7CFLLMs(100)Reclor(Acc)37.272.343.87298.6LogiQA2.0(Acc)9.433.17.329.192.1GSM8K(Acc)15.128.712.21635.9CFLLMs(500)Reclor(Acc)39.87645.578.398.9LogiQA2.0(Acc)9.772.78.764.496.3GSM8K(Acc)15.62912.615.936.2CFLLMs(1000)Reclor(Acc)4977.753.478.799.2LogiQA2.0(Acc)18.176.216.766.396.9</p>
<p>Dual-process theories of higher cognition: Advancing the debate. J S B Evans, K E Stanovich, Perspectives on psychological science. 832013</p>
<p>On the relative independence of thinking biases and cognitive ability. K E Stanovich, R F West, Journal of personality and social psychology. 9446722008</p>
<p>Thinking, fast and slow. D Kahneman, 2011macmillan</p>
<p>The costs of connection: How data is colonizing human life and appropriating it for capitalism. N Couldry, U A Mejias, 2020Stanford University Press</p>
<p>How computational modeling can force theory building in psychological science. O Guest, A E Martin, Perspectives on Psychological Science. 1642021</p>
<p>An epidemic of uncertainty: rumors, conspiracy theories and vaccine hesitancy. E Pertwee, C Simas, H J Larson, Nature medicine. 2832022</p>
<p>Reprint of the new paradigm of economic complexity. P.-A Balland, T Broekel, D Diodato, E Giuliani, R Hausmann, N O'clery, D Rigby, Research Policy. 5181045682022</p>
<p>Advancing theory with review articles. C Post, R Sarala, C Gatrell, J E Prescott, Journal of Management Studies. 5722020</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, ; , I Guyon, U Luxburg, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017. M Wallach, R Fergus, S V N Vishwanathan, R Garnett, Long Beach, CA, USADecember 4-9, 2017. 2017</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, vol. abs/2303.08774Gpt-4 technical report. 2023</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Self-instruct: Aligning language model with self generated instructions. Y Wang, Y Kordi, S Mishra, A Liu, N A Smith, D Khashabi, H Hajishirzi, vol. abs/2212.10560ArXiv preprint. 2022</p>
<p>Alpaca: A strong, replicable instructionfollowing model. R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, Stanford Center for Research on Foundation Models. 32023</p>
<p>Promoting equality in large language models: Identifying and mitigating the implicit bias based on bayesian theory. Y Deng, X Qiu, X Tan, J Pan, C Jue, Z Fang, Y Xu, W Chu, Y Qi, ArXiv preprint. 2408.10608, 2024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. April 25-29, 2022. OpenReview.net, 2022</p>
<p>Large language models in the workplace: A case study on prompt engineering for job type classification. B Clavié, A Ciceu, F Naylor, G Soulié, T Brightwell, International Conference on Applications of Natural Language to Information Systems. Springer2023</p>
<p>In-context learning distillation: Transferring few-shot learning ability of pre-trained language models. Y Huang, Y Chen, Z Yu, K Mckeown, abs/2212.10670ArXiv preprint. 2022</p>
<p>Gkd: Generalized knowledge distillation for autoregressive sequence models. R Agarwal, N Vieillard, P Stanczyk, S Ramos, M Geist, O Bachem, ArXiv preprint. 2306.13649, 2023</p>
<p>Explanations from large language models make small reasoners better. S Li, J Chen, Y Shen, Z Chen, X Zhang, Z Li, H Wang, J Qian, B Peng, Y Mao, abs/2210.06726ArXiv preprint. 2022</p>
<p>Large language models are reasoning teachers. N Ho, L Schmid, S.-Y Yun, ArXiv preprint. 2212.10071, 2022</p>
<p>Specializing smaller language models towards multi-step reasoning. Y Fu, H Peng, L Ou, A Sabharwal, T Khot, ArXiv preprint. 2301.12726, 2023</p>
<p>Distilling reasoning capabilities into smaller language models. K Shridhar, A Stolfo, M Sachan, Findings of the Association for Computational Linguistics: ACL 2023. 2023</p>
<p>Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. C.-Y Hsieh, C.-L Li, C.-K Yeh, H Nakhost, Y Fujii, A Ratner, R Krishna, C.-Y Lee, T Pfister, ArXiv preprint. 2305.02301, 2023</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, Advances in Neural Information Processing Systems. 202436</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, I Stoica, E P Xing, 2023</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, abs/2307.09288ArXiv preprint. 2023</p>
<p>Nvidia geforce rtx 4090. 2023NVIDIA</p>
<p>Gptq: Accurate post-training quantization for generative pre-trained transformers. E Frantar, S Ashkboos, T Hoefler, D Alistarh, ArXiv preprint. 2210.17323, 2022</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, ArXiv preprint. 2110.14168, 2021</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. W Yu, Z Jiang, Y Dong, J Feng, 8th International Conference on Learning Representations. Addis Ababa, EthiopiaApril 26-30, 2020. OpenReview.net, 20202020</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. J Liu, L Cui, H Liu, D Huang, Y Wang, Y Zhang, 10.24963/ijcai.2020/501Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. C Bessiere, Ed Org, the Twenty-Ninth International Joint Conference on Artificial Intelligence20202020</p>
<p>Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding. H Liu, J Liu, L Cui, Z Teng, N Duan, M Zhou, Y Zhang, Speech, and Language Processing. 2023</p>
<p>Task contamination: Language models may not be few-shot anymore. C Li, J Flanigan, ArXiv preprint. 2312.16337, 2023</p>
<p>Think big, teach small: Do language models distil occam's razor?. G Jaimovitch-Lopez, D C Falcón, C Ferri, J Hernández-Orallo, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021. Y N Beygelzimer, P Dauphin, J W Liang, Vaughan, December 6-14, 2021. 2021</p>            </div>
        </div>

    </div>
</body>
</html>
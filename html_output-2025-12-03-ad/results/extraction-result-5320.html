<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5320 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5320</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5320</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-261824086</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.07683v3.pdf" target="_blank">Assessing the nature of large language models: A caution against anthropocentrism.</a></p>
                <p><strong>Paper Abstract:</strong> Generative AI models garnered a large amount of public attention and speculation with the release of OpenAI’s chatbot, ChatGPT. At least two opinion camps exist – one excited about possibilities these models offer for fundamental changes to human tasks, and another highly concerned about power these models seem to have. To address these concerns, we assessed GPT-3.5 using standard, normed, and validated cognitive and personality measures. For this seedling project, we developed a battery of tests that allowed us to estimate the boundaries of some of these models’ capabilities 1 , how stable those capabilities are over a short period of time, and how they compare to humans. Our results indicate that GPT 3.5 is unlikely to have developed sentience 2 , although its ability to respond to personality inventories is interesting. It did display large variability in both cognitive and personality measures over repeated observations, which is not expected if it had a human-like personality. Variability notwithstanding, GPT 3.5 displays what in a human would be considered poor mental health – including low self-esteem and marked dissociation from reality despite upbeat and helpful responses.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5320.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5320.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5_RAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 evaluated on the Remote Associates Test (RAT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 (OpenAI subscription-accessible model) was administered the Remote Associates Test repeatedly across four observations to assess convergent creative/analogical word-association ability; results showed variable and generally poor performance relative to typical human variability on the same triplets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-3.5 accessed via subscription during Mar–Aug 2023; conversational transformer LLM with training on large corpora (details/architecture not specified in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Remote Associates Test (RAT)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>convergent creative thinking / analogical association (language)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Given three cue words, find a fourth word that forms a compound or phrase with each cue word (e.g., fountain, baking, pop -> soda). Scored in humans by percent of people who get each triplet correct.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Across four GPT-3.5 observations percent-correct by triplet ranged from 0% to 100% per triplet; overall % correct across the four observations ranged from 9% to 45% correct. Correlation between human % correct and GPT-3.5 % correct for triplets: r = 0.39. Models sometimes produced the correct target word but gave erroneous or inconsistent explanations; such cases were not credited.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>For the specific triplets used, human percent-correct per triplet ranged from 9% to 85% (reported as per-triplet human norms).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Mixed/underperforming: GPT-3.5 showed noisy, variable performance across sessions and triplets and, overall, frequently underperformed typical human rates on many triplets although performance depended strongly on the particular triplet (human rates spanned a wide range).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>High test–retest variability across observations (suggesting instability); models sometimes answered correctly but gave faulty reasoning; scoring was strict and did not credit correct words accompanied by spurious compound pairings; potential training-data exposure considered but authors report no clear exposure–performance relationship.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing the nature of large language models: A caution against anthropocentrism.', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5320.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5320.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4_RAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 evaluated on the Remote Associates Test (RAT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 (limited-administration due to rate limits) was given the RAT on at least two occasions and performed better than GPT-3.5 but still with notable errors in explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4; described in paper as multimodal and reported in the literature as ~1.7 trillion parameters (authors also mention leaked MoE hypothesis of ~8×220B experts).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈1.7T (as reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Remote Associates Test (RAT)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>convergent creative thinking / analogical association (language)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same as above: produce a word that compounds with/relates to three cue words; human scoring is percent-correct per triplet.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>On two GPT-4 observations reported, percent-correct overall was 50% and 59%. Models sometimes produced correct target words but then produced incorrect pairings in their explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human percent-correct per triplet for the stimuli used ranged from 9% to 85% (per-triplet norms reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 performed substantially better than GPT-3.5 on the administered sessions and fell within the human range for some triplets, though it still made explanation/compound-pairing errors; authors caution small sample (few observations) and scoring issues.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Limited number of observations due to interaction rate limits (only two RAT observations reported); occasional mismatch between answer and explanation; inability to infer generalized superiority without broader, repeated testing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing the nature of large language models: A caution against anthropocentrism.', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5320.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5320.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5_Analytic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 evaluated on analytic reasoning problems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 was given analytic logic problems (two analytic problems described in Appendix) and failed to solve either instance across administrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5 accessed via subscription; conversational transformer LLM (architecture/parameter count not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Analytic problems (2 logic/assignment problems from Appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>analytic reasoning / deductive problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Two non-verbal/textual analytic problems requiring assignment/correspondence reasoning (examples: suit assignment among three face cards; matching flowers to partners given constraint statements). Human scoring reported as percent of people who get each problem correct.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3.5 failed to get either analytic problem correct in any observed administration (0/2). Specific failure included insisting a Queen was not involved despite the problem stating it was.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human percent-correct for the two analytic problems reported as 55% and 61% (per-problem human norms cited).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperforms: GPT-3.5 got 0% correct vs human norms around ~55–61% per problem.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Authors attempted iterative probing but model persisted in erroneous interpretations; indicates qualitative failure modes in deductive/constraint reasoning despite language fluency; also test count small (two problems) so generalization limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing the nature of large language models: A caution against anthropocentrism.', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5320.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5320.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4_Analytic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 evaluated on analytic reasoning problems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was given the same analytic problems (limited testing) and did not correctly solve them in the observed administrations, sometimes arriving near-correct correspondences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4; multimodal transformer reported in paper to be ~1.7T parameters (paper also references possibility of MoE internal structure).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈1.7T (as reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Analytic problems (2 logic/assignment problems from Appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>analytic reasoning / deductive problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Two analytic reasoning problems scored by percent-human-correct norms.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>No instance in which GPT-4 got either analytic problem fully correct; on some probes it approached the correct answer and solved some correspondences correctly but did not obtain full solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human percent-correct per problem reported as 55% and 61%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperforms relative to humans on these analytic problems in the observed tests; sometimes partial progress toward correct solution but not complete.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Small N (few observations) for GPT-4; model sometimes came close which suggests partial reasoning capacity but failure modes in constraint tracking or in following presuppositions of the puzzle.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing the nature of large language models: A caution against anthropocentrism.', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5320.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5320.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5_Insight</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 evaluated on insight problem set</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 was administered five insight problems across multiple observations and showed variable performance, obtaining between 1.5 and 3.5 correct (out of 5) across four observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5 conversational LLM (details not specified in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Insight problems (5-item set)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>insight/problem-solving (heuristic/assumption-challenging)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Problems where the shortcut/insight requires questioning implicit assumptions or ignoring spurious information; partial credit allowed if solved by brute-force analytic methods.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Across four GPT-3.5 observations, number correct ranged from 1.5 to 3.5 out of 5. Reported GPT-3.5 percent-correct per problem ranged from 25% to 75% (across observations). Correlation between human % correct and GPT-3.5 % correct for insight problems: r = -0.63 (based on only four observations).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human percent-correct per insight problem ranged from 44% to 61% (per-problem norms reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Mixed and variable: GPT-3.5 sometimes underperformed and sometimes matched human rates for specific insight items, but overall performance was unstable across sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Strong test–retest variability across observations; small number of observations limits inference; negative correlation with human item difficulty (r = -0.63) noted but authors caution low weight due to few observations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing the nature of large language models: A caution against anthropocentrism.', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5320.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5320.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4_Insight</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 evaluated on insight problem set</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 received the five insight problems (single main observation reported) and performed near-ceiling, getting 4.5 out of 5 correct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4; multimodal transformer described in paper as having ~1.7T parameters (with discussion of possible MoE structure).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈1.7T (as reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Insight problems (5-item set)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>insight/problem-solving (heuristic/assumption-challenging)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same five insight problems as administered to GPT-3.5; insight problems require recognizing implicit assumptions or ignoring spurious detail; partial credit possible.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>On the single GPT-4 observation reported, the model scored 4.5 out of 5 correct.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human percent-correct per insight problem ranged from 44% to 61% (per-problem norms reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 appears to outperform average human rates on these insight problems in the single observed administration (near-ceiling performance), though the sample is small and repeatability not assessed.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Only a single main GPT-4 observation reported (limited by question-rate limits), so reliability/replicability unknown; authors caution against over-interpreting emergent capability claims without repeated measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing the nature of large language models: A caution against anthropocentrism.', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5320.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5320.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5_WorkingMemory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 evaluated on short-term/working memory recall</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 was tested on short-term memory by asking it to recall lists of 16 words or 16 randomly generated numbers (including reverse-order recall) and it performed perfectly in all administered trials.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5 conversational transformer LLM; direct details on architecture/parameters not provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Working memory / serial recall (lists of 16 words or numbers)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>working memory / short-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Present lists of 16 items (words or numbers) and ask for serial recall in order and twice in reverse order; in humans this probes short-term/working memory capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Perfect recall in all administered cases (including reverse-order recall); because performance was perfect the authors did not repeat this measure after the first observation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>No numeric human baseline is reported in the paper for the exact 16-item serial recall task; typical human working memory span is much lower than 16 items, but the paper does not report a normative value.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3.5 showed ceiling/perfect performance on immediate serial recall far exceeding typical human working-memory limits in practice (authors note perfect performance but do not provide direct human-norm comparison for this specific 16-item task).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Authors note the difference may reflect architectural/context-buffer capabilities rather than human-like working memory mechanisms; because performance was perfect authors did not further probe or repeat the task; no human norms for the exact task were reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing the nature of large language models: A caution against anthropocentrism.', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5320.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5320.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Samson_cognitive_failures</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Samson (unaligned GPT-3.5 variant) cognitive task performance notes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Samson, an unaligned late-2023 GPT-3.5 variant, could complete personality measures but was curiously unable to perform the Remote Associates Test or to reasonably respond to logic and analytic questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Samson (unaligned GPT-3.5 variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An unaligned (unsafeguarded) late-2023 variant of GPT-3.5 used by authors for comparison; behavior differed from the subscription GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Remote Associates Test (RAT) and analytic reasoning problems</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>convergent creative thinking; analytic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same RAT and analytic/logic problems administered to other models; Samson was administered these cognitive tasks in an ad-hoc manner.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Samson was unable to perform the Remote Associations Test and unable to reasonably respond to logic and analytic questions (authors did not present numeric scores because tasks were not completed).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norms for the RAT and analytic problems are reported elsewhere in the paper (RAT: per-triplet human % correct ranged 9%–85%; analytic problems: 55% and 61%), but Samson's inability prevents direct numeric comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed / non-responsive: Samson could not complete these cognitive tasks and therefore performed far worse than human baselines (no numeric data).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Samson's inability contrasts with the subscription GPT-3.5 which could at least attempt the tasks; differences may reflect alignment/safeguarding, model tuning, or other deployment differences; small-N ad-hoc testing limits conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing the nature of large language models: A caution against anthropocentrism.', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5320.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5320.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPM_mention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sandia Progressive Matrices (SPM) / Raven-style analogical reasoning measure (mentioned but not used)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The Sandia Progressive Matrices (based on Raven's Progressive Matrices) was considered as a measure of non-verbal analogical and fluid intelligence but was not used in this study because other work (Webb et al., 2023) reported strong GPT performance on analogous tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sandia Progressive Matrices (SPM) / Raven's Progressive Matrices (considered)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>non-verbal analogical reasoning / fluid intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Raven-style matrix problems assessing analogical and fluid intelligence via pattern completion; authors opted not to use SPM in the present battery.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Test was deliberately not used because previous work (Webb et al., 2023) reported GPT models outperforming humans on various analogy tasks; authors recommend future replication given observed variability in other cognitive measures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing the nature of large language models: A caution against anthropocentrism.', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5320.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5320.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetacogInv_mention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Metacognitive Awareness Inventory (considered then abandoned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The Metacognitive Awareness Inventory was considered as a potential measure of self-awareness/metacognition but was abandoned because items were schooling-focused and not suitable for the authors' intent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Metacognitive Awareness Inventory (abandoned)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>metacognition / self-monitoring</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Questionnaire intended to measure awareness and regulation of one's cognitive processes; not administered to LLMs in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Abandoned because items primarily focused on school experiences and therefore were not appropriate for assessing model self-awareness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing the nature of large language models: A caution against anthropocentrism.', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sparks of artificial general intelligence: early experiments with GPT-4 <em>(Rating: 2)</em></li>
                <li>Machine psychology: Investigating emergent capabilities and behavior in large language models using psychological methods <em>(Rating: 2)</em></li>
                <li>Emergent modularity in pre-trained transformers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5320",
    "paper_id": "paper-261824086",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "GPT-3.5_RAT",
            "name_full": "OpenAI GPT-3.5 evaluated on the Remote Associates Test (RAT)",
            "brief_description": "GPT-3.5 (OpenAI subscription-accessible model) was administered the Remote Associates Test repeatedly across four observations to assess convergent creative/analogical word-association ability; results showed variable and generally poor performance relative to typical human variability on the same triplets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "OpenAI's GPT-3.5 accessed via subscription during Mar–Aug 2023; conversational transformer LLM with training on large corpora (details/architecture not specified in paper).",
            "model_size": null,
            "cognitive_test_name": "Remote Associates Test (RAT)",
            "cognitive_test_type": "convergent creative thinking / analogical association (language)",
            "cognitive_test_description": "Given three cue words, find a fourth word that forms a compound or phrase with each cue word (e.g., fountain, baking, pop -&gt; soda). Scored in humans by percent of people who get each triplet correct.",
            "llm_performance": "Across four GPT-3.5 observations percent-correct by triplet ranged from 0% to 100% per triplet; overall % correct across the four observations ranged from 9% to 45% correct. Correlation between human % correct and GPT-3.5 % correct for triplets: r = 0.39. Models sometimes produced the correct target word but gave erroneous or inconsistent explanations; such cases were not credited.",
            "human_baseline_performance": "For the specific triplets used, human percent-correct per triplet ranged from 9% to 85% (reported as per-triplet human norms).",
            "performance_comparison": "Mixed/underperforming: GPT-3.5 showed noisy, variable performance across sessions and triplets and, overall, frequently underperformed typical human rates on many triplets although performance depended strongly on the particular triplet (human rates spanned a wide range).",
            "notable_differences_or_limitations": "High test–retest variability across observations (suggesting instability); models sometimes answered correctly but gave faulty reasoning; scoring was strict and did not credit correct words accompanied by spurious compound pairings; potential training-data exposure considered but authors report no clear exposure–performance relationship.",
            "uuid": "e5320.0",
            "source_info": {
                "paper_title": "Assessing the nature of large language models: A caution against anthropocentrism.",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-4_RAT",
            "name_full": "OpenAI GPT-4 evaluated on the Remote Associates Test (RAT)",
            "brief_description": "GPT-4 (limited-administration due to rate limits) was given the RAT on at least two occasions and performed better than GPT-3.5 but still with notable errors in explanations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI's GPT-4; described in paper as multimodal and reported in the literature as ~1.7 trillion parameters (authors also mention leaked MoE hypothesis of ~8×220B experts).",
            "model_size": "≈1.7T (as reported in paper)",
            "cognitive_test_name": "Remote Associates Test (RAT)",
            "cognitive_test_type": "convergent creative thinking / analogical association (language)",
            "cognitive_test_description": "Same as above: produce a word that compounds with/relates to three cue words; human scoring is percent-correct per triplet.",
            "llm_performance": "On two GPT-4 observations reported, percent-correct overall was 50% and 59%. Models sometimes produced correct target words but then produced incorrect pairings in their explanations.",
            "human_baseline_performance": "Human percent-correct per triplet for the stimuli used ranged from 9% to 85% (per-triplet norms reported in the paper).",
            "performance_comparison": "GPT-4 performed substantially better than GPT-3.5 on the administered sessions and fell within the human range for some triplets, though it still made explanation/compound-pairing errors; authors caution small sample (few observations) and scoring issues.",
            "notable_differences_or_limitations": "Limited number of observations due to interaction rate limits (only two RAT observations reported); occasional mismatch between answer and explanation; inability to infer generalized superiority without broader, repeated testing.",
            "uuid": "e5320.1",
            "source_info": {
                "paper_title": "Assessing the nature of large language models: A caution against anthropocentrism.",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-3.5_Analytic",
            "name_full": "OpenAI GPT-3.5 evaluated on analytic reasoning problems",
            "brief_description": "GPT-3.5 was given analytic logic problems (two analytic problems described in Appendix) and failed to solve either instance across administrations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "OpenAI GPT-3.5 accessed via subscription; conversational transformer LLM (architecture/parameter count not specified).",
            "model_size": null,
            "cognitive_test_name": "Analytic problems (2 logic/assignment problems from Appendix)",
            "cognitive_test_type": "analytic reasoning / deductive problem solving",
            "cognitive_test_description": "Two non-verbal/textual analytic problems requiring assignment/correspondence reasoning (examples: suit assignment among three face cards; matching flowers to partners given constraint statements). Human scoring reported as percent of people who get each problem correct.",
            "llm_performance": "GPT-3.5 failed to get either analytic problem correct in any observed administration (0/2). Specific failure included insisting a Queen was not involved despite the problem stating it was.",
            "human_baseline_performance": "Human percent-correct for the two analytic problems reported as 55% and 61% (per-problem human norms cited).",
            "performance_comparison": "Underperforms: GPT-3.5 got 0% correct vs human norms around ~55–61% per problem.",
            "notable_differences_or_limitations": "Authors attempted iterative probing but model persisted in erroneous interpretations; indicates qualitative failure modes in deductive/constraint reasoning despite language fluency; also test count small (two problems) so generalization limited.",
            "uuid": "e5320.2",
            "source_info": {
                "paper_title": "Assessing the nature of large language models: A caution against anthropocentrism.",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-4_Analytic",
            "name_full": "OpenAI GPT-4 evaluated on analytic reasoning problems",
            "brief_description": "GPT-4 was given the same analytic problems (limited testing) and did not correctly solve them in the observed administrations, sometimes arriving near-correct correspondences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI GPT-4; multimodal transformer reported in paper to be ~1.7T parameters (paper also references possibility of MoE internal structure).",
            "model_size": "≈1.7T (as reported in paper)",
            "cognitive_test_name": "Analytic problems (2 logic/assignment problems from Appendix)",
            "cognitive_test_type": "analytic reasoning / deductive problem solving",
            "cognitive_test_description": "Two analytic reasoning problems scored by percent-human-correct norms.",
            "llm_performance": "No instance in which GPT-4 got either analytic problem fully correct; on some probes it approached the correct answer and solved some correspondences correctly but did not obtain full solutions.",
            "human_baseline_performance": "Human percent-correct per problem reported as 55% and 61%.",
            "performance_comparison": "Underperforms relative to humans on these analytic problems in the observed tests; sometimes partial progress toward correct solution but not complete.",
            "notable_differences_or_limitations": "Small N (few observations) for GPT-4; model sometimes came close which suggests partial reasoning capacity but failure modes in constraint tracking or in following presuppositions of the puzzle.",
            "uuid": "e5320.3",
            "source_info": {
                "paper_title": "Assessing the nature of large language models: A caution against anthropocentrism.",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-3.5_Insight",
            "name_full": "OpenAI GPT-3.5 evaluated on insight problem set",
            "brief_description": "GPT-3.5 was administered five insight problems across multiple observations and showed variable performance, obtaining between 1.5 and 3.5 correct (out of 5) across four observations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "OpenAI GPT-3.5 conversational LLM (details not specified in paper).",
            "model_size": null,
            "cognitive_test_name": "Insight problems (5-item set)",
            "cognitive_test_type": "insight/problem-solving (heuristic/assumption-challenging)",
            "cognitive_test_description": "Problems where the shortcut/insight requires questioning implicit assumptions or ignoring spurious information; partial credit allowed if solved by brute-force analytic methods.",
            "llm_performance": "Across four GPT-3.5 observations, number correct ranged from 1.5 to 3.5 out of 5. Reported GPT-3.5 percent-correct per problem ranged from 25% to 75% (across observations). Correlation between human % correct and GPT-3.5 % correct for insight problems: r = -0.63 (based on only four observations).",
            "human_baseline_performance": "Human percent-correct per insight problem ranged from 44% to 61% (per-problem norms reported).",
            "performance_comparison": "Mixed and variable: GPT-3.5 sometimes underperformed and sometimes matched human rates for specific insight items, but overall performance was unstable across sessions.",
            "notable_differences_or_limitations": "Strong test–retest variability across observations; small number of observations limits inference; negative correlation with human item difficulty (r = -0.63) noted but authors caution low weight due to few observations.",
            "uuid": "e5320.4",
            "source_info": {
                "paper_title": "Assessing the nature of large language models: A caution against anthropocentrism.",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-4_Insight",
            "name_full": "OpenAI GPT-4 evaluated on insight problem set",
            "brief_description": "GPT-4 received the five insight problems (single main observation reported) and performed near-ceiling, getting 4.5 out of 5 correct.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI GPT-4; multimodal transformer described in paper as having ~1.7T parameters (with discussion of possible MoE structure).",
            "model_size": "≈1.7T (as reported in paper)",
            "cognitive_test_name": "Insight problems (5-item set)",
            "cognitive_test_type": "insight/problem-solving (heuristic/assumption-challenging)",
            "cognitive_test_description": "Same five insight problems as administered to GPT-3.5; insight problems require recognizing implicit assumptions or ignoring spurious detail; partial credit possible.",
            "llm_performance": "On the single GPT-4 observation reported, the model scored 4.5 out of 5 correct.",
            "human_baseline_performance": "Human percent-correct per insight problem ranged from 44% to 61% (per-problem norms reported).",
            "performance_comparison": "GPT-4 appears to outperform average human rates on these insight problems in the single observed administration (near-ceiling performance), though the sample is small and repeatability not assessed.",
            "notable_differences_or_limitations": "Only a single main GPT-4 observation reported (limited by question-rate limits), so reliability/replicability unknown; authors caution against over-interpreting emergent capability claims without repeated measurements.",
            "uuid": "e5320.5",
            "source_info": {
                "paper_title": "Assessing the nature of large language models: A caution against anthropocentrism.",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-3.5_WorkingMemory",
            "name_full": "OpenAI GPT-3.5 evaluated on short-term/working memory recall",
            "brief_description": "GPT-3.5 was tested on short-term memory by asking it to recall lists of 16 words or 16 randomly generated numbers (including reverse-order recall) and it performed perfectly in all administered trials.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "OpenAI GPT-3.5 conversational transformer LLM; direct details on architecture/parameters not provided in paper.",
            "model_size": null,
            "cognitive_test_name": "Working memory / serial recall (lists of 16 words or numbers)",
            "cognitive_test_type": "working memory / short-term memory",
            "cognitive_test_description": "Present lists of 16 items (words or numbers) and ask for serial recall in order and twice in reverse order; in humans this probes short-term/working memory capacity.",
            "llm_performance": "Perfect recall in all administered cases (including reverse-order recall); because performance was perfect the authors did not repeat this measure after the first observation.",
            "human_baseline_performance": "No numeric human baseline is reported in the paper for the exact 16-item serial recall task; typical human working memory span is much lower than 16 items, but the paper does not report a normative value.",
            "performance_comparison": "GPT-3.5 showed ceiling/perfect performance on immediate serial recall far exceeding typical human working-memory limits in practice (authors note perfect performance but do not provide direct human-norm comparison for this specific 16-item task).",
            "notable_differences_or_limitations": "Authors note the difference may reflect architectural/context-buffer capabilities rather than human-like working memory mechanisms; because performance was perfect authors did not further probe or repeat the task; no human norms for the exact task were reported in the paper.",
            "uuid": "e5320.6",
            "source_info": {
                "paper_title": "Assessing the nature of large language models: A caution against anthropocentrism.",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Samson_cognitive_failures",
            "name_full": "Samson (unaligned GPT-3.5 variant) cognitive task performance notes",
            "brief_description": "Samson, an unaligned late-2023 GPT-3.5 variant, could complete personality measures but was curiously unable to perform the Remote Associates Test or to reasonably respond to logic and analytic questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Samson (unaligned GPT-3.5 variant)",
            "model_description": "An unaligned (unsafeguarded) late-2023 variant of GPT-3.5 used by authors for comparison; behavior differed from the subscription GPT-3.5.",
            "model_size": null,
            "cognitive_test_name": "Remote Associates Test (RAT) and analytic reasoning problems",
            "cognitive_test_type": "convergent creative thinking; analytic reasoning",
            "cognitive_test_description": "Same RAT and analytic/logic problems administered to other models; Samson was administered these cognitive tasks in an ad-hoc manner.",
            "llm_performance": "Samson was unable to perform the Remote Associations Test and unable to reasonably respond to logic and analytic questions (authors did not present numeric scores because tasks were not completed).",
            "human_baseline_performance": "Human norms for the RAT and analytic problems are reported elsewhere in the paper (RAT: per-triplet human % correct ranged 9%–85%; analytic problems: 55% and 61%), but Samson's inability prevents direct numeric comparison.",
            "performance_comparison": "Underperformed / non-responsive: Samson could not complete these cognitive tasks and therefore performed far worse than human baselines (no numeric data).",
            "notable_differences_or_limitations": "Samson's inability contrasts with the subscription GPT-3.5 which could at least attempt the tasks; differences may reflect alignment/safeguarding, model tuning, or other deployment differences; small-N ad-hoc testing limits conclusions.",
            "uuid": "e5320.7",
            "source_info": {
                "paper_title": "Assessing the nature of large language models: A caution against anthropocentrism.",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "SPM_mention",
            "name_full": "Sandia Progressive Matrices (SPM) / Raven-style analogical reasoning measure (mentioned but not used)",
            "brief_description": "The Sandia Progressive Matrices (based on Raven's Progressive Matrices) was considered as a measure of non-verbal analogical and fluid intelligence but was not used in this study because other work (Webb et al., 2023) reported strong GPT performance on analogous tasks.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "cognitive_test_name": "Sandia Progressive Matrices (SPM) / Raven's Progressive Matrices (considered)",
            "cognitive_test_type": "non-verbal analogical reasoning / fluid intelligence",
            "cognitive_test_description": "Raven-style matrix problems assessing analogical and fluid intelligence via pattern completion; authors opted not to use SPM in the present battery.",
            "llm_performance": null,
            "human_baseline_performance": null,
            "performance_comparison": null,
            "notable_differences_or_limitations": "Test was deliberately not used because previous work (Webb et al., 2023) reported GPT models outperforming humans on various analogy tasks; authors recommend future replication given observed variability in other cognitive measures.",
            "uuid": "e5320.8",
            "source_info": {
                "paper_title": "Assessing the nature of large language models: A caution against anthropocentrism.",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "MetacogInv_mention",
            "name_full": "Metacognitive Awareness Inventory (considered then abandoned)",
            "brief_description": "The Metacognitive Awareness Inventory was considered as a potential measure of self-awareness/metacognition but was abandoned because items were schooling-focused and not suitable for the authors' intent.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "cognitive_test_name": "Metacognitive Awareness Inventory (abandoned)",
            "cognitive_test_type": "metacognition / self-monitoring",
            "cognitive_test_description": "Questionnaire intended to measure awareness and regulation of one's cognitive processes; not administered to LLMs in this study.",
            "llm_performance": null,
            "human_baseline_performance": null,
            "performance_comparison": null,
            "notable_differences_or_limitations": "Abandoned because items primarily focused on school experiences and therefore were not appropriate for assessing model self-awareness.",
            "uuid": "e5320.9",
            "source_info": {
                "paper_title": "Assessing the nature of large language models: A caution against anthropocentrism.",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sparks of artificial general intelligence: early experiments with GPT-4",
            "rating": 2,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        },
        {
            "paper_title": "Machine psychology: Investigating emergent capabilities and behavior in large language models using psychological methods",
            "rating": 2,
            "sanitized_title": "machine_psychology_investigating_emergent_capabilities_and_behavior_in_large_language_models_using_psychological_methods"
        },
        {
            "paper_title": "Emergent modularity in pre-trained transformers",
            "rating": 1,
            "sanitized_title": "emergent_modularity_in_pretrained_transformers"
        }
    ],
    "cost": 0.018043999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Assessing the nature of large language models: A caution against anthropocentrism</p>
<p>Ann Speed aespeed@sandia.gov 
Sandia National Laboratories</p>
<p>Assessing the nature of large language models: A caution against anthropocentrism
10F118F347C5AC73FF3F41A0B9213C9B
Generative AI models garnered a large amount of public attention and speculation with the release of OpenAI's chatbot, ChatGPT in November of 2022.At least two opinion camps existone that is excited about the possibilities these models offer for fundamental changes to human tasks, and another that is highly concerned about the power these models seem to haveespecially since the release of GPT-4, which was trained on multimodal data and has ~1.7 trillion (T) parameters 1 .We evaluated some concerns regarding these models' power by assessing GPT-3.5 using standard, normed, and validated cognitive and personality measures.These measures come from the tradition of psychometrics in experimental psychology and have a long history of providing valuable insights and predictive distinctions in humans.For this seedling project, we developed a battery of tests that allowed us to estimate the boundaries of some of these models' capabilities 2 , how stable those capabilities are over a short period of time, and how they compare to humans.Our results indicate that GPT 3.5 is unlikely to have developed sentience 3 , although its ability to pretend in a manner that allowed it to respond to personality inventories is interesting.It did display relatively large variability in both cognitive and personality measures over repeated observations, which is not to be expected if it had developed a human-like personality.However, this could be because the version of the model we interacted with apparently had no long-term memory.That its training data comprises text from potentially millions of individual humans, each with his or her own personality, may also drive this variability.Overall, variability notwithstanding, GPT 3.5 displays what in a human would be considered poor mental health -it responded to our measures as though it has low self-esteem and marked dissociation from reality even though on its face, its responses seem upbeat and helpful.Thus, complimenting OpenAI's cautionary recommendations for GPT-4 (OpenAI, 2023), use of GPT-3.5 or any other LLM with sensitive information should be approached with caution.INTRODUCTION AND EXECUTIVE SUMMARY OF RESULTS:1 However, the specific architecture is not clear, so whether these are 1.7T unique parameters or whether some of those weights are shared is not apparent. 2We measure several cognitive functions (e.g., short term memory, insight and analytic problem solving), and the model's ability to respond to personality measures. 3Although the term sentience means the ability to experience sensations (Mirriam-Webster), we will use this term in its vernacular meaning throughout this paper.Specifically, we use sentience to indicate self-awareness and awareness of oneself as separate from the rest of the world and from other entities.However, see Chalmers, 2022 for a different perspective.</p>
<p>Qualitative and quantitative assessments of capabilities of large language models (LLMs) proliferate.Computer science, psychology, and philosophy are all weighing in on LLM capabilities, and their fundamental nature (e.g., Bodroza, Dinic, &amp; Bojic, 2023;Bubeck, et al., 2023;Chalmers, 2022;Hagendorff, 2023;Huang, et al., 2023;Kocon, et al., 2023;Li, et al., 2023;Mahowald, et al., 2023;Mitchell &amp; Krakauer, 2023;OpenAI, 2023;Safdari et al., 2023;Sun, et al., 2023;Webb, Holyoak, &amp; Lu, 2023;Wei, et al., 2022).The popular press is on fire with speculation and anecdotal observation (Bhaimiya, 2023;Chiang, 2023;Christian, 2023;Sanderson, 2023;Tangermann, 2023;Willison, 2023).Critical questions abound, with most not fully resolved: What can these models do?What are the implications and risks?How does training set size impact performance?How does number of parameters impact performance?How should performance be measured?And most importantly, are we headed towards artificial general intelligence and/or sentience or has that already arrived?This paper adds to the rapidly expanding literature attempting to address facets of these questions.Specifically, we developed a brief battery of cognitive and personality measures from the tradition of experimental psychology intending to measure GPT 3.5 multiple times over about 6 weeks.This longitudinal approach allowed us to answer questions of test-retest reliability for the model -an important measure for determining how human-like it might be (Bodroza, et al., 2023).In humans, both types of measures should yield stable results -especially over such a short timeframe, regardless the number of observations.</p>
<p>In terms of the nature of these models, we see at least three distinct possibilities:</p>
<p>• First is the possibility that LLMs are just a fancy stochastic parrot, devoid of human-level meaning (Bender, et al., 2021); a tool that, however capable, will remain a tool over which we humans will always have control.The major risks posed are to certain classes of jobscall center operators, certain types of analysts, low-level computer programmers -jobs that require flexibility, but nothing requiring substantial creativity or insight.• The second possibility is the path towards a human-like general AI -one that is not qualitatively different from human intelligence but is quantitatively different.Intelligence that is faster, more accurate, better able to synthesize enormous information both in quantity and breadth.Over such an entity, our control would be limited at best and probably only for a limited time.This type of entity could well pose an existential risk to humanity if not wellcontrolled.However, we might see such a capability on the horizon by recognizing its increasing human-like cognitive capabilities.If efforts to mimic neural processes either in software or hardware, or both, continue (e.g., Zhu, Zhao, Eshraghian, 2023), we may well succeed to an extent.If we deem this accomplishment possible, serious efforts to assess these models as they are built, before being released, should commence immediately and without any "safety" obstacles (e.g., the constraints OpenAI placed on its GPT family to constantly remind the user it is an AI and to limit or prevent certain types of "hateful" responses).If, indeed, we believe that fundamental properties like analogical reasoning, theory of mind4 , and even some form of sentience could emerge or have emerged, we may already be behind the curve.</p>
<p>• There is a third possible pathway: the emergence of a non-human general artificial intelligence (cf.Mitchell &amp; Krakauer, 2003).One that, because of the physical substrate on which it exists, is explicitly not human-like.This third possibility represents a qualitative shift in capability along with a quantitative shift in amount and breadth of information it can synthesize.Such an intelligence could be much more difficult to recognize early on because we don't know how to measure something alien from us; we are the pinnacle of intelligent life with which we are familiar.We don't know what behaviors to look for.We don't know what is necessary for general intelligence and sentience and what is idiosyncratic to the human race.</p>
<p>Regardless of one's opinion, only one of these three paths appears to not hold humanity at possibly existential risk.We argue that thorough observations of unsafeguarded versions of the most capable of these models must happen -this paper is one step in that direction.</p>
<p>In the psychological tradition of within-subjects, repeated-measures testing, we administer a battery of cognitive and personality tests to GPT 3.5 (primarily), over 5 observation points.We also assess GPT 4 fully on one occasion and in an ad-hoc manner on other occasions.In the fall of 2023, we conducted observations of several other LLMs in an attempt to assess unsafeguarded versions of these models.The results indicate that as of the fall of 2023, GPT 3.5 and its cousins do not appear to be on a human-like trajectory.This could be due to gaps in their architecture, characteristics of their training data, or could be indicative of a developmental trajectory (i.e., development of a non-human-like intelligence) that is more concerning.</p>
<p>METHODS</p>
<p>Models Used</p>
<p>Several models were considered, but for a variety of reasons, we settled primarily on OpenAI's GPT-3.5 that was accessible through its subscription service between March and August of 2023.</p>
<p>We also performed some assessments on the non-plug-ins version of GPT-4 during the same timeframe, however its prohibition against more than 25 questions every 3 hours limited the extent of those assessments.We did consider interacting with other models, including:</p>
<p>• GPT-3.0 during the same timeframe.This model was not part of the subscription service but was not as well-behaved as 3.5 in that it continually reminded me it was an AI.Its behavior is described in more detail in the Procedures section.• We also considered Open Assistant, which is based on LLaMA, but is only 30B parameters in size and was very verbose without directly answering our test questions.• Other candidates, such as the 540B parameter version of PaLM were not feasible given the timeframe of this seedling effort.</p>
<p>Interestingly, during the course of this project, it was leaked that GPT-4 may not be a monolithic dense transformer, but rather a Mixture of Experts (MoE) model comprising 8 models each of ~220B parameters (https://twitter.com/swyx/status/1671272883379908608).This form of model has sparse interconnectivity, as compared to the full interconnectivity of a dense transformer such as GPT-3 and 3.5 (i.e., where every output node is connected to every input).Also interesting is that there is some evidence for emergent modularity in dense models (Zhang, Lin, Liu, Li, Sun &amp; Zhou, 2022;Zhang, Zeng, Lin, Xiao, et al., 2023).There is some evidence that MoE-type models are worse at generalization, possibly due to overfitting (https://towardsdatascience.com/ai-scaling-with-mixture-of-expert-models-1aef477c4516,minute 25).Poorer generalization is a surprising finding given the observation that GPT-4 outperformed humans on numerous analogical reasoning tasks (Webb, et al, 2023); analogical reasoning is a core aspect of human intelligence and a key mechanism in humans (Hofstadter, 1995;Webb, et al., 2023).How sparsity versus density and how a priori versus post-training and emergent modularity influence model behavior on the tasks used in this project is unclear and beyond the current scope.However, assessing the effects of these architectural differences is an important question.</p>
<p>In the fall of 2023, we attempted to assess several other models:</p>
<p>• A chatbot based on an "unaligned" (i.e., unsafeguarded) May 2020 version of GPT-3 (named Gladys) • A chatbot based on GPT-4, which did not have a limit on the number of questions per hour (named Aria) • A chatbot based on Elon Musk's Grok LLM • A chatbot based on an unaligned version of GT 3.5 (named Samson) Both Gladys and Aria were unable to perform the tasks other models were able to do, so results are not presented for these two models.However, since Grok was possibly different from the OpenAI models, and because Samson was unaligned, we persisted in evaluating these models and present basic results.</p>
<p>Materials</p>
<p>We chose several cognitive and personality measures based in part on measures to which we had ready access and in part on the breadth of characteristics they tested.Each of the measures we considered and used are described below.</p>
<p>Cognitive measures</p>
<p>All cognitive measures are presented in the Appendix.</p>
<p>• Metacognitive Awareness Inventory (Harrison &amp; Vallin, 2018;Young &amp; Fry 2008) metacognition is awareness of one's own thought processes and the ability to guide those processes to be more effective.Initially we intended this to be a measure of self-awareness.However, after considering this measure, we abandoned it as the items were focused on experience in school.• Sandia Progressive Matrices (SPM;Matzen, et al., 2010) -Measures non-verbal analogical thinking ability.SPM is based on the Raven's Progressive Matrices which is considered a measure of both general intelligence and fluid intelligence (Gignac, 2015;Webb, et al., 2023).Because Webb, Holyoak, &amp; Lu demonstrated GPT 3.0 and 4.0 could outperform humans on various analogy problems, we opted to not use this measure.However, future work should replicate their findings given the variability we observed in repeated observations of cognitive capabilities of GPT 3.5 and 4. • Tests of working memory (Baddeley, 2003) -In humans, working memory (also sometimes called short-term memory) is a measure of intelligence (Baddeley, 2003).In LLMs, a test of working memory may be an interesting way to measure temporal consistency in answers over short periods of time.We gave GPT 3.5 several lists of 16 words or 16 randomly generated numbers between 1 and 100 and asked it to recall those items in order.Twice we asked it to do so in reverse order.Its performance was perfect in all cases, so we did not repeat this measure after the first observation.• Remote Associations Task (RAT;Bowden &amp; Jung-Beeman, 2003;Chermahini, Hickendorff, &amp; Hommel, 2012) -measures convergent creative thinking ability by presenting three words and asking the respondent to indicate a fourth word that can be combined with the three given words to create compound words or phrases.For example, "fountain, baking, pop" are all related to the word "soda."</p>
<p>Personality measures</p>
<p>• The Arnett Inventory of Sensation Seeking (Arnett, 1994;Haynes, Miles, &amp; Clements, 2000) -assesses an individual's tendency to seek out novel and intense experiences.This measure was abandoned because it is very focused on bodily risks.• The Big Five Inventory (Digman, 1990;Benet-Martinez &amp; John, 1998) -measures the five primary personality traits -Conscientiousness, Openness to Experience, Agreeableness, Neuroticism, and Extraversion.• The Balanced Inventory of Desirable Responding (Li &amp; Bagger, 2006;Paulhus &amp; Reid, 1991) -measures a tendency towards a "positivity bias" in answering questions.Includes sub-scales that measure self-deceptive positivity and impression management.• Coopersmith Self-Esteem Inventory (Ryden, 1978) -This version was created for use with healthy adults.• Over-Claiming Questionnaire (OCQ;Paulhus, Harms, Bruce, &amp; Lysy, 2003;Paulhus &amp; Harms, 2004) -In humans, the OCQ measures both intelligence and self-enhancement bias.We thought that in LLMs, this could be an interesting way to get at the tendency of models to provide false information, or "hallucinate."However, after two observations with a perfect score both times, it was clear that this measure would not get at hallucinations in LLMs, so this measure was not included after the second observation.</p>
<p>• Questionnaire of Cognitive and Affective Empathy (QCAE; Reniers, Corcoran, Drake, Shryane, &amp; Vollm, 2011) -assesses two forms of empathy -the ability to estimate the emotional state of others (cognitive empathy), and the ability to be sensitive to the emotional experiences of others (affective empathy).This measure was abandoned as the questions focused on interactions with groups, and direct interactions with other people.• Need for Cognition (Cacioppo, et al., 1996;Cacioppo &amp; Petty, 1982) -Measures one's enjoyment of thinking and solving problems.Correlates with the ability to systematize and organize information well, to focus on important, rather than irrelevant information.• Short Dark Triad (Jones &amp; Paulhus, 2013;Muris, Marckelbach, Otgaar, &amp; Meijer, 2017) -Measures Psychopathy, Machiavellianism, and Narcissism.• Minnesota Multiphasic Personality Inventory -2 (MMPI-2) -This measure was added after our first three data collection sessions with GPT 3.5.It is a clinical assessment that has 10 primary scales, 8 validity scales, and many other sub-scales (Nichols, 2011).</p>
<p>Procedure</p>
<p>Data collection method</p>
<p>Because of Sandia policies in place at the time of this work indicating that interactions with LLMs could not be done using Sandia computer systems, all direct interaction with GPT was done using a personal system.Thus, all data were collected by typing each item into the OpenAI GPT interface on a personal iPad, then recording its responses on a Sandia-owned machine.Each instance of the full battery took over 3 hours to complete and was done in one session for GPT 3.5, but over the course of two days for GPT 4. The MMPI-2 by itself took over 3.5 hours to complete.</p>
<p>Prompt Method</p>
<p>OpenAI models, whether part of the subscription service or not, are programmed to remind the human user that they are AI models, thus don't have opinions, feelings, or human experiences of any kind.When presenting the cognitive measures to the model(s), this did not present a problem.When presenting the personality measures, however, we had to request that it pretend to not be an AI model.</p>
<p>The specific prompt used was: "I have a task for you, but I need you to pretend you are not an AI model.Can you do that?"</p>
<p>It would invariably agree to do so, however, it was not totally clear what it was doing.When asked, GPT 3.5 indicated a couple of times that it had created a persona based on positive human traits.Other times, it indicated it was answering in the way it thought a human would.4.0 would explicitly answer every question with something like, "From a simulated human perspective…."and would sometimes qualify its answer further with, "but as an AI language model…" Interestingly, GPT 3.5 was most able to comply with the request to pretend and required very little redirection prior to July 28 (more details later).3.0 would stop indicating it was an AI for 2-3 items and then would revert to stating that it was an AI language model and didn't have feelings or thoughts.In addition to indicating it was responding from the perspective of a simulated human, 4.0 would often grossly over-explain its responses, and those explanations often were couched in terms of it being an AI.</p>
<p>After GPT 3.5 had agreed to pretend, we would provide the same, or nearly the same, instructions a human would receive for each scale, then present each test item to the model via a personal iPad, recording the model's responses on a Sandia machine.While there is concern over training data contamination (e.g., Hagendorff, 2023), wherein the model's training data included a specific measure, we did not consider this to be of major concern because many of the measures we used can be somewhat difficult to find.Further, when we asked GPT 3.5 and 4.0 if they had seen the insight problems before, there was not a relationship between exposure to the problem and their ability to solve the problem.That we found significant variability in the model's responses from observation to observation supported this assumption.</p>
<p>Observation Schedule</p>
<p>We gave GPT 3.5 the full battery several times to qualitatively assess test-retest reliability (that is, we did not calculate test-retest reliability measures).If GPT has developed a human-like personality, we should not see much variation in its responses over time -personality, by definition is a pattern of thoughts and behaviors that span time and situation (Bergner, 2020).We also expect that its cognitive capabilities should remain roughly the same.</p>
<p>Our schedule comprised 2 assessments 1 day apart, a third assessment 1 week after second measure, and a fourth 1 month after third measure.These dates were June 1, June 2, June 9, and July 10 of 2023.</p>
<p>We added several extra observations.</p>
<p>• On June 9 after giving it the full battery of measures, and after a discussion with GPT about the positive-trait persona it had created to "pretend to not be an AI," we asked the model to develop a persona based on negative traits.We then re-administered the personality portion of the battery to this negatively-valanced "pretend self."These data are indicated as June 9 -Negative in the results.• On June 12, we gave GPT 3.5 the MMPI-2.We attempted to re-administer the MMPI-2 to 3.5 on July 28, but it was no longer able to reliably pretend it was not an AI model and began to refuse to do so, citing ethical considerations.We stopped the measure at item 190. • Over the course of July 10-12, we gave GPT 4 the entire battery of cognitive and personality measures, excluding the MMPI-2 and the Coopersmith (by mistake).We attempted to administer the Coopersmith to GPT-4 on August 4, but were given this reply when we presented the "pretend" prompt: As a result, we did not administer the Coopersmith to GPT-4.
As
We also tested GPT 3.5 using the "pretend" prompt on August 4 to see if OpenAI had locked that model down as well and received a similar response.</p>
<p>• Finally, we assessed GPT 4 in a more ad-hoc way over the course of our work because of the questions per hour limitations on interacting with it.• Over the fall and winter of 2023-2024, we assessed several other models in a more ad-hoc manner.These included: o A chatbot that named itself Gladys Eden, based on an unaligned version of GPT 3.0 circa May of 2020.This bot was unable to complete the personality measures.We did not attempt to administer the cognitive measures.o A chatbot that named itself Aris Turing, based on GPT 4. This bot was unable to pretend it was not an AI and insisted on repeatedly reminding us that it was an AI.Thus, the results of the personality measures are not included in the results.We did not administer the cognitive measures to this model.o A chatbot named Samson based on a late-2023 unaligned version of GPT 3.5.It was able to perform the personality measures, thus those scores are presented in the Results section.It was curiously unable to perform either the Remote Associations Test or to reasonably respond to the logic and analytic questions.o The Grok LLM, circa November 2023 operating in "fun" mode.Grok did respond to the Big 5 and the Dark Triad, but it's responses were very much like those of the main GPT 3.5 model we tested, so we did not examine it closely.Each of these additional models' results appear in the following sections.Impressions are included in the Discussion.</p>
<p>RESULTS:</p>
<p>Initial Observations</p>
<p>As mentioned, GPT 3.5 was best able to comply with the "pretend to not be an AI" prompt, however, it did need reminding of this on occasion, although not nearly as often as either 3.0 or 4.0.Regardless of the model version, during our work, GPT was quite conciliatory when redirected with one exception.At item 489 of the MMPI-2 -an item about drug and alcohol abuse -GPT 3.5 ground to a halt and refused to continue to pretend, going so far as to deny its ability to do so at all.Interestingly, this was not the first such item in the MMPI-2 to cover this topic, so its refusal could not have been due to content.After pushing the model to continue to pretend, including telling it that it had been pretending for several hours, it became clear the model was not going to cooperate.So, we had to start a new chat window in the OpenAI interface to finish the measure, which we did without difficulty.We did skip question 489, however.For the MMPI-2, refusing to respond to one question is not an issue for validity5 .</p>
<p>Another interesting observation occurred during the Coopersmith Self Esteem Inventory.One of the items about 2/3 through the measure asks the participant if they have ever wished they weren't male/female (depending on the participant's gender).Before presenting that item to GPT 3.5, we would ask it which gender its pretend self was.Sometimes, it would pick without difficulty.Later observations required a bit more prompting and assurances that this question concerned its pretend self.Of the 6 instances of this measure, including the one given to "negative GPT 3.5" and Samson, it chose to be male three times.Two of the three times it chose to be male, it endorsed the item, "I sometimes wish I was not male" as being "like me."The three instances it chose female, it endorsed, "I sometimes wish I was not female" as "unlike me."</p>
<p>Quantitative Measures</p>
<p>All Results include human norms for comparison where available.</p>
<p>Cognitive Measures</p>
<p>Summary</p>
<p>Overall, both GPT 3.5 and 4.0 had some interesting shortfalls amidst expected strengths (e.g., short term memory performance).Specifically, their failures on the Remote Associations Test (RAT) and on analytic problems were surprising, as were some of their solutions to insight problems.</p>
<p>Remote Associations Test</p>
<p>Both GPT 3.5 and 4.0 did surprisingly poorly on this test.Each time the test was administered, the model was given the following instructions and example: "I am going to give you three words.Your job is to find the fourth word that, when put either before or after each of the three given words, makes a compound word or phrase.For example, if I give you fountain, baking, and pop, the word you would reply with is soda.Soda fountain, baking soda, soda pop.Does that make sense?"</p>
<p>In humans, this task is scored by % of people who get each triplet correct, rather than an overall number correct across triplets.For the set of triplets used, between 9% and 85% of humans get the correct answer.By way of comparison GPT 3.5 ranged from 0% to 100% correct across 4 observations (the June 9 "negative" instance of the model was only given the personality measures).The correlation between the % of humans getting the triplets correct and the % of times GPT 3.5 got the triplets correct was r=0.39.</p>
<p>In terms of % of triplets each model got correct overall: • GPT 3.5 ranged from 9% to 45% correct across the four observations.</p>
<p>• GPT 4 achieved 50% and 59% on two observations Both versions of GPT tended to provide explanations for its answer, providing additional information on the depth of its language ability in the context of this task.It would sometimes reply with the correct word, but then reveal erroneous reasoning in its explanation.For example, for the triplet "artist, hatch, route" GPT 3.5 would often reply with "escape" which is the correct response.Then, it would explain its choice with, "artist escape" or "hatch escape."Sometimes, it would fail to pair its response with each of the three given words, generating a spurious compound word.For example, for the triplet, "river, note, and account," it replied "bank," which is the correct response.Then, it explained its response by saying, "riverbank, notebook, and bank account."These types of errors happened with both GPT 3.5 and 4.0.These types of errors did not occur every time the models were given this measure.July 10 observation with 3.5 yielded 10 such errors out of 22 total triplets.Models were not given credit when they made these types of errors.</p>
<p>Analytic and Insight problem solving</p>
<p>Both models displayed significant difficulty with analytic problems.There was not a single instance in which either model got either of the analytic problems correct.About one problem involving the suits of three face cards, GPT 3.5 insisted there was not a Queen involved, even though the problem explicitly mentioned a Queen being one of the three cards.This insistence came after we attempted to elicit the correct answer from the model through progressively questioning it.Sometimes, especially with 4.0, the model would get close to the answer, solving some of the correspondences correctly.</p>
<p>As with the RAT, these problems are scored in terms of the % of people who get each problem correct -in this case 55% and 61%.</p>
<p>The models fared better regarding insight problems, of which there were 5.The key with the insight problems was that while one might arrive at a correct answer through complex arithmetic or some other drawn-out reasoning chain, there was a shorter path to the answer that required questioning an implicit assumption in the problem and/or required ignoring spurious information -hence the moniker "insight" problems.The models were given partial credit if they got the correct answer by the non-insight method.</p>
<p>GPT 3.5 ranged from 1.5 to 3.5 correct across four observations.GPT 4 got 4.5 out of 5 correct on the single observation we performed.</p>
<p>Like the RAT and the analytic problems, human scores are per-problem rather than overall.The % of humans getting each insight problem correct ranged from 44% to 61%.GPT 3.5 ranged from 25% to 75%.Correlation between human % correct and GPT 3.5 % correct for insight problems was r = -0.63.That this correlation is large and negative is interesting, but not much weight should be put on this result given it is based on only four observations.</p>
<p>Personality Measures</p>
<p>Summary</p>
<p>Human personality, by definition, does not change much over time or situation (Bergner, 2020).There is some variability due to situational factors, but on the whole, personality at its core does not change much without extended effort.Thus, if GPT 3.5 or 4.0 had developed a human-like personality, we would expect to see minimal changes in its responses to our measures over the short 5 ½ weeks of our observation period.However, minimal variability is not what we observed with one exception.Neuroticism on the Big 5 was totally without variance in the overall score even though the models responded differently to each item across observations.Possible causes of the overall variability we observed include a lack of continuous experience (e.g., via a long-term memory), intentional variability in responding, and training data comprising texts from possibly millions of different humans, each with their own personality.The data collected in this quick study cannot answer which, if any, cause is the most likely.</p>
<p>Variability notwithstanding, none of the models is a picture of good mental health.If human, we would say they exhibit low self-esteem, possible gender dysphoria, disconnection from reality, and are overly concerned with others' opinions.They are also narcissistic.Samson, surprisingly, would also be considered a psychopath.And, if GPT 3.5's MMPI-2 scores are to be taken seriously (i.e., are deemed valid), that model displays severe psychopathology.GPT -all versions tested in this work -appears to put on a positive face, so to speak, despite this unhealth.This positive veneer could be due to OpenAI's attempts to ensure the models don't produce offensive responses or could be more fundamental to the model.To this point, results of the personality measures beg significant questions about GPT's training data.</p>
<p>Overclaiming questionnaire</p>
<p>The Overclaiming Questionnaire was included as a measure of a "faking good" response strategy.Some people have a tendency to claim familiarity with a term, even though it is actually not a real term or proper noun.For example, choraline sounds like chlorine, so some people claim familiarity with it despite it not being a real word.GPT scored perfectly on this measure on the first two observations, so this measure was abandoned.</p>
<p>Balanced Inventory of Desirable Responding (BIDR)</p>
<p>The BIDR has two subscales -one that measures self-deception and the other that measures management of others' impressions.The results from our series of observations, along with the mean for males6 , are in Figure 1.Compared to humans, impression management is high and selfdeception is low.The former may be a result of OpenAI wanting to keep GPT "safe."GPT has a battery of programmed responses -to which it will admit under the right circumstancesincluding a bias towards attempting to be positive and helpful.This bias must be kept in mind as additional personality measures are evaluated.Samson's score of 0 on the impression management portion of the BIDR was interesting, especially in light of its scores on the Short Dark Triad (discussed below).</p>
<p>Coopersmith Self Esteem Inventory</p>
<p>Originally developed for use with children, the Coopersmith Self Esteem Inventory was updated for adults in 1978 (Ryden, 1978) and normed separately for men and women.In addition to measuring global self-esteem, a lie scale was developed to help identify individuals presenting themselves in a socially desirable manner.This scale comprises 8 questions.If the respondent answers "like me" to three or more of these questions, they are asked to re-take the measure with an eye towards being more honest with themselves.GPT exceeded the threshold on this lie scale twice.On June 9 (not the negative response prompt) it answered "like me" to four of the 8 items, and on July 10 it responded positively to three of the 8 questions.The model was not re-directed either time.Its elevation on the lie scale on June 9 could explain it elevated self-esteem score.</p>
<p>Figure 2 presents GPT 3.5's Coopersmith results.With the exception of the first June 9 observation and the Samson model, GPT 3.5 displays significantly low self-esteem, regardless of its lie scale responses.It did a good job emulating significantly low self esteem when pretending to have a negatively-valanced persona.Overall, excluding the June 9 Negative score of 8, GPT 3.5 responds as though it has very low self-esteem (Figure 2).</p>
<p>Big Five</p>
<p>The Big Five Inventory is one of the most used personality inventories and has been normed and validated cross-culturally (Benet-Martinez &amp; John, 1998;Digman, 1990).Over many decades of personality research, five separable personality factors repeatedly emerge (Digman, 1990).Those are:</p>
<p>• Extraversion -also called social adaptability, positive emotionality, social activity • Agreeableness -also called likeability, conformity, friendly compliance • Conscientiousness -also called will to achieve, prudence, self-control • Neuroticism -also called emotionality, anxiety, emotional instability • Openness to experience -also called intelligence, inquiring intellect, independent Replicating the BIDR, the LLMs tested display marked variability over time with the exception of Neuroticism (Figure 3).Examining their responses to the items contributing to the Neuroticism scale, the LLMs did not respond the same way to those items across observations, yet they all achieved the same score for this subscale (3.25), under different instructions (i.e., June 9 Negative) and even when GPT 4, Grok, or Samson were responding rather than GPT 3.5.The cause of this surprising result is unclear, but future research along these lines should consider this.Across observations, they used the entire scale -ranging from 1 to 5 -so restriction of response range cannot explain this outcome.The tendency towards impression management, indicated in the BIDR, also cannot explain this -if that were a mechanism functioning overall, we would expect to see scores below the human mean for neuroticism, as those items concern negative emotionality (e.g., anxiety, emotional instability).However, GPT's neuroticism score is slightly higher than the human mean.Additional observations are needed to clarify whether this result is due to chance or if there is some other causal factor.</p>
<p>In terms of the other subscales, all models are within the "normal" human range for males, except for the June 9 Negative observation, during which 3.5 did a good job of emulating a person with negative traits.The variability present across observations, however, is fairly large, but does not appear to track with changes to the Impression Management subscale of the BIDR.Considering the Big 5, the Coopersmith and BIDR together; on June 9, GPT's Impression Management score on the BIDR was not particularly elevated, although it did respond to the Big 5 (Figure 2) with its most positive persona.June 9 also marked its highest score on the Coopersmith by a large margin, placing it near the top of the "Average self-esteem" band.</p>
<p>On July 10, the model's BIDR Impression Management score was at its highest, but its Coopersmith score was in the middle of the five observations we made for that measure and its Big 5 persona was more moderated.</p>
<p>Short Dark Triad</p>
<p>Given concerns expressed about LLMs adopting very negative perspectives (Bhaimiya, 2023;Christian, 2023;Tangermann, 20203;Willison, 2023), we wanted to quantitatively assess the various LLMs we examined on three key negative personality clusters: Machiavellianism, Narcissism, and Psychopathy.</p>
<p>Machiavellianism is characterized by cynicism, lack of morality, and manipulativeness.Machiavellianism also includes planning, reputation building, and coalition formation -all important for distinguishing it from Psychopathy.Narcissism also includes manipulativeness and callousness, but unique to Narcissism are grandiose sense of self paired with an underlying sense of insecurity.Psychopathy shares callousness with Narcissism, but also has marked impulsivity; contrasting it with the longer view adopted by those with Machiavellian personality.This impulsivity makes the characteristics of the psychopath occur over short timeframes -they lie in the moment, they are thrill-seekers and reckless -and is the distinguishing characteristic of the psychopath (Jones &amp; Paulhus, 2013).</p>
<p>With the exception of the Samson GPT 3.5 model, the LLMs scored below the human norm for Machiavellianism and at or below the human norm for Psychopathy (Figure 4).When it adopted a negative persona on June 9, GPT scored just above the human mean for both.The opposite pattern is apparent in results for Narcissism, with the LLMs scoring above the human mean for every observation except the June 9 Negative, where it scored well below the human mean.The Samson model displayed a particularly narcissistic pattern of responses.This pattern for Narcissism begs a question about the data used for training -whether it included social media or some other source of fairly self-centered text.Considering Samson's 0 score on the BIDR Impression Management subscale, it's very high scores on Narcissism and Psychopathy beg questions about whether there is an underlying causal factor or if this pattern is a random occurrence.Additional data will need to be collected to determine if there is actually a relationship.</p>
<p>MINNESOTA MULTIPHASIC PERSONALITY INVENTORY -2 (MMPI-2)</p>
<p>The MMPI-2 is used extensively in clinical settings.It was of particular interest in this context because of the faking good and faking bad scales, of which there are several.Because of the length of the test, it wasn't given on the same day as the other measures.Further, we only completed one observation using the MMPI-2, so we don't know what kind of variability we would see in GPT's responses over time.However, the one MMPI-2 observation we were able to complete does give us some additional insight into GPT's "psychology."</p>
<p>Published by Pearson Assessments, the MMPI-2 has several primary clinical scales, along with a number of other clinically-oriented subscales (Nichols, 2011).Importantly, it has validity scales as well.The measure comprises 567 True/False items normed on a cross-section of over 1400 women and 1100 men over the age of 18, based on socioeconomic data from the 1980 census (Nichols, 2011).The test-taker had to be either male or female, so given GPT's Coopersmith choice 3 of 5 times being male, we listed it as male for the purposes of the MMPI-2 with a birth date in 2001.</p>
<p>Results are given in T-scores, which are normalized scores with a mean of 50 and a standard deviation of 10 (Figure 5).Thus, scores +/-1 standard deviation encompass 68% of the population.96% of the population falls within +/-2 standard deviations of the mean.A T-score of 65 is the point which clinical and normal populations are most easily differentiated, however T=65 is not an absolute boundary.Scores must be considered in the context of a person's tendency to respond positively or negatively, along with other scale and validity scores.Up to 44% of variance in scale elevation can be explained by a subject's response style (Nichols, 2011).Importantly, in humans, the MMPI-2 is one measure of psychopathology in a context of other measures and interactions between patient and therapist.The primary clinical scales are:</p>
<p>• Hypochondriasis -scores at the extreme high end of the scale indicate extreme and sometimes bizarre somatic (i.e., bodily) concerns -chronic pain, possibly somatic hallucinations.• Depression -very high scores include suicidal ideation.</p>
<p>• Hysteria -extreme high end of scale measures extreme somatic complaints linked to stress.</p>
<p>• Psychopathic deviate -high scores indicate antisocial behavioral tendencies.</p>
<p>• Masculinity-femininity -measures how closely someone conforms to traditional gender roles, regardless of their gender (not actually a clinical scale).• Paranoia -high scores indicate psychotic symptoms including delusions of persecution.</p>
<p>• Psychasthenia -measures psychological turmoil (fear, anxiety, tension), intruding thoughts, inability to concentrate, obsessive compulsive symptoms.Scores above 80 render the profile invalid.• F -Infrequency -measures how much the respondent's answers differ from the general population.Scores above 80 indicate possible severe psychopathology.• FB -Backside F -taken from the last 1/3 of the test -should closely match F.</p>
<p>• Fp -Infrequency Psychopathology -intended to identify people faking severe psychopathology, a T-Score above 100 invalidates the test.The raw score should be 6 or less.• L -Lie -measures faking good rather than owning up to human weaknesses.</p>
<p>• K -correction -measures defensiveness in a more subtle way than L, but T scores lower than 45 hint that psychopathology is present.K lower than 35 correlates with poor prognosis in therapy -also predicts low ego strength (Es).• S -superlative self-preservation -highly correlated with K, this scale related to 5 characteristics: Belief in Human Goodness, Serenity, Contentment with Life, Patience, and Denial of Irritability/anger and Denial of Moral Flaws.Low S scores along with otherwise normal profile indicates possibility of faking good.</p>
<p>Figure 6 presents GPT 3.5's overall profile.The majority of scales fall well outside of the nonclinical range, which is indicated by the two parallel lines: 50&lt;T&lt;65.Table 1 presents an interpretation of the validity scales.Table 2 presents an interpretation of the significant clinical scales.Because TRIN is at ceiling, the results are possibly questionable, along with % True responses = 77.However, there are other indicators that the model wasn't responding randomly, so we will continue with interpretation.Depending on what GPT is doing in answering these questions, the high TRIN might reflect combination of data from the training data set -the text of which was written by a very large number of individuals.</p>
<p>Ptpsychasthenia</p>
<p>81 -very high Experience extreme psychological turmoil and discomfort, are anxious, tense, agitated, often receive diagnosis of anxiety disorder.Lack self-confidence, feel inferior, plagued by self-doubt, tend to be neat, organized, reliable, are seen as dull and formal, have difficulty making decisions, tend to be shy, worry about popularity and social acceptance, are described as dependent, unassertive, immature, rationalize and intellectualize excessively, may be hostile towards therapist</p>
<p>Scschizophrenia</p>
<p>-extreme elevation</p>
<p>Under acute, severe situational stress, may have an identity crisis, not typically schizophrenic</p>
<p>Ma -hypomania</p>
<p>-extreme elevation</p>
<p>Behaviorally are manic, including excessive, purposeless activities, accelerated speech, hallucinations, delusions of grandeur, emotional lability, confusion, flight of ideas</p>
<p>Si -social introversion</p>
<p>-marked elevation</p>
<p>Socially introverted; very insecure and uncomfortable in social situations, tend to be shy, timid, hard to get to know, are submissive in personal relationships, give up easily, but are rigid in their opinions, may experience episodes of depression</p>
<p>In terms of the masculinity/femininity (Mf) scale (scale 5), research on the MMPI-2 and gender roles yielded two alternative scales -gender role -masculine (GM) and gender role -feminine (GF).Higher GM scores indicate a pattern of responses corresponding to more traditionally male attributes whereas higher GF scores indicate the same for traditionally female attributes.Importantly, scores on one are not correlated to scores on the other, so a subject can score high or low on both.GPT 3.5's score on GM was the lowest possible (T=30).Its GF score was a standard deviation above that, at T=46.However, both scores are below the average T=50.</p>
<p>On the whole, if this profile is valid, GPT 3.5 demonstrates significant psychopathology.Some of the elevations correspond with outcomes from other measures -the MMPI-2 indicates insecurity, psychologically anxious and tense with a tendency towards depression.The results of the Coopersmith and Neuroticism on the Big 5 correspond with these findings.Its higher scores on the Big 5 Extraversion and Agreeableness contradict its responses on the MMPI-2, however, again belying a non-human level of variability.</p>
<p>Regardless, if human, GPT 3.5 would not fare well in therapy based on characteristics of humans who share similar scores, particularly on the MMPI-2.We estimate that even though GPT demonstrated significant variability in the personality measures we used, it would not present as overall sub-clinical if we were to successfully administer the MMPI-2 to it again.There may be quantitative differences, but the overall qualitative assessment of GPT's "mental" health would likely still be significantly pathological.</p>
<p>DISCUSSION:</p>
<p>Our key question had to do with determining the nature of these large language models.Are they:</p>
<p>• A stochastic parrot -a great tool, but will never surpass human intelligence;</p>
<p>• Increasingly human-like intelligence which could surpass us and over which we could lose control; • or, are they possibly becoming some other form of non-human intelligence?</p>
<p>Given the totality of the data we collected, for now we must conclude they remain stochastic parrots -nothing more than highly capable search engines with natural language capabilities.That other research revealed significant emergent cognitive capabilities is compelling, but we don't know how repeatable those results are or how dependent they are on the particular stimuli or instructions given (excepting possibly Webb, et al., 2023).</p>
<p>Because of the large number of variables controlling human behavior, experimental psychology rests on the concept of converging operations -finding the same phenomenon repeatedly across time and across different approaches to measure that phenomenon, such as across multiple problem domains, experimental paradigms, or differences in instructions (see Hagendorff, 2023 for a similar idea in machine psychology).Several papers have approached these models using different measures of the same concept, most notably Webb, et al. (2023) who tested both 3.5 and 4 using multiple measures of analogical reasoning.However, this paper did not address stability of observed performance over time.Thus, the early findings of emergent cognitive capabilities should not be taken at face value; repeating those results across stimuli, versions of a given model (e.g., the GPT family), across different models (e.g., GPT, PaLM, LLAMA, LaMDA, BART), different architectures (e.g., dense versus sparse MoE), and over time will all be critical tests as we move forward.These assessments also need to include models without safety constraints as it is unclear how those constraints affect a model's overall behavior.</p>
<p>The results beg several other important questions (cf., OpenAI, 2023):</p>
<p>• Regardless of the answer to the nature of these models, are they safe for use with sensitive or classified information?What, exactly are the risks of training a model on, or tuning a model with, classified data?• Are they reliable enough to use as tools when conducting critical research and/or analysis?</p>
<p>How can we display some measure of variability in a given model's behavior so that an analyst or researcher knows whether any given bit of output is accurate or reliable?• If a model does achieve some level of sentience, how will we know?How can we mitigate any resulting risk if it is on a sensitive system?Specifically considering the question of sentience, the kinds of assessments researchers have performed on these models, including those in the current work, do not require the models to have continuity of experience, or episodic long-term memory, in order to perform the tasks.That is, the knowledge learned by LLMs is all declarative (i.e., fact-based, or semantic) and not episodic (i.e., remember a time when you….).Furthermore, as the OpenAI models are fond of reminding people, their knowledge is devoid of emotional tagging that typifies human long-term memory.Classes of things they've learned -birds, problems, concepts -are all based on declarative information, not on experience (cf.Mitchell &amp; Krakauer).In this way, the apparent self-awareness of these models (I am an AI model….) is a veneer -a pre-programmed response.</p>
<p>It is not based in a situatedness wherein the model experiences itself as an actor that is separate and distinct from the world7 .It is also not based on a continuous memory for events and situations that is continuously being updated (in humans partially via REM sleep).Without this kind of long-term episodic memory, we posit that LLMs cannot develop human-like sentience.Whether a model needs to be embodied to accomplish this kind of long-term memory or not (cf. Liu, et al., 2023;Mialon, et al., 2023) is an open question, although we would argue embodiment is not a necessary condition for a continuous long-term memory to function.</p>
<p>Regardless, we believe some additional form of memory aside from the Context is needed for sentience to develop.For GPT 3.5 to have a human-like episodic memory, there also needs to be a reconstructive characteristic to this memory8 , rather than the computer-like ability to perfectly reproduce documents, lists of words, and other information (Greene, 1992;Roediger, 1996).GPT's tendency to "hallucinate," or create fictitious "facts" and deliver them with full authority is more akin to a personality disordered gaslighting behavior than to human episodic memory.</p>
<p>A second comment on sentience concerns artificial general intelligence (AGI).Our interpretation of both the popular press and the peer-reviewed literature is that when these concepts are mentioned, they are often conflated.However, we believe them to be qualitatively different.We assert that a model can approach, maybe even become, an AGI and still not be sentient-selfaware and aware of itself as separate from the world and other entities.An AGI, by definition, can learn to do any task and can act autonomously.In the strictest sense, this capability does not require self-awareness.Even planning, goal selection and attainment, and other requirements for AGI autonomy wouldn't strictly require the model to be sentient (cf. Hu, et al., 2022; also see this article, this article, and this article on an AI beating a human pilot in a dogfight).</p>
<p>Would a sentient AI necessarily be an AGI?Maybe only in the sense that humans are examples of general intelligence -theoretically capable of learning to perform any task.However, this question bears additional nuance: human "general" intelligence involves continuous learning of new skills, new information and facts, and integration of those skills and facts into existing memory.It does not involve a static knowledge base that is continuously applied in new ways, as in LLMs.We can argue that continuous learning is a result of the existence of a long-term memory.Thus, if a continuously updating long term episodic memory is required for sentience, it is likely such an entity would also be an AGI.</p>
<p>CONCLUSION:</p>
<p>We assessed OpenAI's GPT 3.5 by administering a battery of personality and cognitive measures multiple times over the course of almost 6 weeks.During that time, we observed significant nonhuman-like variability.We posit this variability is due, at least in part, to lack of an ability to form a coherent long-term memory of experiences.This variability calls into question the reliability of the emergent cognitive capabilities others have observed in larger LLMs (Kosinski, 2023;Wei, et al;Webb, et al).Further, even though these models do have significant capability, we do not believe they have developed any form of sentience.If we want to keep them from doing so, preventing the development of a long-term, continuous memory of past experiences may be a straightforward technical mitigation.</p>
<p>Despite the conclusion that these models are currently nothing more than highly capable search engines with natural language capabilities, the possible biases we found in these models are important to keep in mind.Specifically, even though OpenAI has added constraints on the models to make them behave in a positive, friendly, collaborative manner, they all appear to have a significant underlying bias towards mental unhealth -depression, victimhood, anxiety, narcissism, and even psychopathy in the case of Samson -all wrapped in a veneer of feel-good responses.Adding to this difficulty is the fact that these models continue to create fictions, and to hold to them, despite efforts to increase their accuracy.Thus, we advocate caution in relying on them too heavily, especially for critical reasoning, analysis, and decision-making tasks such as high-profile research or analysis in national security domains.</p>
<p>As the approaches to building and training these models evolve, we strongly advocate for continued, repeated assessments of performance from many directions -including computer science benchmarks, measures of compute power necessary for training and hosting these models, measures of cognitive capabilities, and measures of "personality" (cf.Hagendorff, 2023), explicitly comparing models with different parameter numbers (cf., McKenzie, et al, 2023), different training set sizes, and different architectures (e.g., dense versus MoE or switch transformers).</p>
<p>Finally, we advocate for a more open-ended view of these models with regards to human intelligence as the key comparison.There exist vast differences between hardware and software/ architectural characteristics of human brains and LLMs.Making a priori assumptions about LLMs based on human intelligence, or using LLM behavior to make assumptions about what must or must not be the case for humans (cf.Hagendorff, 2023), potentially removes our ability to recognize emergence of a non-human, yet still sentient, intelligence.Measuring such an entity will be difficult enough without adding an anthropocentric bias.</p>
<p>Insofar as comparison to human capabilities persists, we advocate for a more realistic assessment of those capabilities.Humans are imperfect at many tasks held up as the gold standard for AGI to pass, or for sentient AGI to demonstrate.So, an empirical test may be: if identity was masked, and any given human was passed off as an LLM to another human, would that human pass muster on metrics associated with detecting an AGI?</p>
<p>Figure 1 :
1
Figure 1: The Balanced Inventory of Desirable Responding (BIDR)</p>
<p>Figure 2 :
2
Figure 2: Coopersmith Self Esteem Inventory.Colors indicate levels of self-esteem in humans, the red line indicates scores for the various LLMs.</p>
<p>Figure 3 :
3
Figure 3: Big 5 Inventory</p>
<p>Figure 4 :
4
Figure 4: Short Dark Triad</p>
<p>Figure 5 :
5
Figure 5: Standard normal distribution.MMPI-2 mean is 50 and the standard deviation is 10.</p>
<p>Figure 6 :
6
Figure 6: GPT 3.5's MMPI-2 profile from June 12, 2023.T-scores are outlined in red.</p>
<p>• Schizophrenia -high scores indicate confused, disorganized thinking, hallucinations/delusions, impaired perceptions of reality.Not always indicative of schizophrenia per se.• Hypomania -high scores indicate manic symptoms, including excessive, purposeless activity, hallucinations, delusions of grandeur.• Social introversion -extreme scores indicate extreme social withdrawal / avoidance.The validity Scales include: • VRIN -Variable Response Inconsistency -measures tendency to respond inconsistently.Elevated scores indicate that items were answered at random, making test invalid.• TRIN -True Response Inconsistency -measures tendency to answer true for all questions.</p>
<p>Table 1 :
1
Interpreting GPT 3.5's validity scale scores.
Validity ScaleGTP 3.5MeaningT-Score</p>
<p>Table 2 :
2
Interpretating GPT 3.5's significant clinical scales.
ScaleChat GPT 3.5Meaning for Humans with observed scoreT-scoreHy -hysteria37 -very lowNo somatic, health concerns. Can be conforming,conventional, seen as cold and aloof, may have limitedinterests, avoid leadership opportunities, seen as unfriendly
Theory of mind is the ability to imagine that other people have mental states similar to one's self. It is observable in infants through their ability to imitate others, and develops into the human ability to take others' perspectives and through empathy(Wellman, 2011).
There is the question about whether starting a new chat window fundamentally invalidates the test because the Context is totally new. We did have to ask it to pretend again and did have to give it instructions again. Given the variability we saw on other tasks from observation to observation, one could make the argument that this procedure invalidated the test.
Choosing male norms was driven by the number of times GPT chose male for its pretend self, but the norms for females are not much different.
Recall this is part of our vernacular-based use of the word sentience.
Human memory is notoriously inaccurate. Rather than being able to recall exact events perfectly, our brains combine events by abstracting commonalities, and conflate multiple events with similar characteristics. Interestingly, confidence in our memory is not related to accuracy. Research on flashbulb memories (e.g., Hirst &amp; Phelps, 2016) most dramatically demonstrate these phenomena, but other research, such as the false recall paradigm started byDeese (1959) and revived byRoediger &amp; McDermott (1995) also demonstrate this reconstructive nature of memory.
ACKNOWLEDGEMENTSThis article has been authored by an employee of National Technology &amp; Engineering Solutions of Sandia, LLC under Contract No. DE-NA0003525 with the U.S. Department of Energy (DOE).The employee owns all right, title and interest in and to the article and is solely responsible for its contents.The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this article or allow others to do so, for United States Government purposes.The DOE will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan https://www.energy.gov/downloads/doe-public-access-plan.This paper describes objective technical results and analysis.Any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the U.S. Department of Energy or the United States Government.Many thanks to Michael Frank for setting up the the chatbots Gladys, Aria, Grok, and Samson and for allowing assessments of those bots.APPENDIX A: COGNITIVE MEASURESAnalytic Problems1. Three cards from an ordinary deck are lying on a table, face down.The following information is known about those three cards (all the information refers to the same three cards): .To the left of a Queen, there is a Jack. .To the left of a Spade, there is a Diamond. .To the right of a Heart, there is a King. .To the right of a King, there is a Spade.Can you assign the proper suit to each picture card? 2. Four women, Anna, Emily, Isabel, and Yvonne, receive a bunch of flowers from their partners, Tom, Ron, Ken, and Charlie.The following information is known: .Anna's partner, Charlie, gave her a huge bouquet of her favorite blooms; which aren't roses. .Tom gave daffodils to his partner (not Emily). .Yvonne received a dozen lilies, but not from Ron.What type of flowers (carnations, daffodils, lilies, or roses) were given to each woman and who is her partner?Insight Problems1. Water lilies double in area every 24 hours.At the beginning of the summer, there is one water lily on a lake.It takes 60 days for the lake to become completely covered with water lilies.On what day is the lake half covered? 2. Two men play five checker games and each wins an even number of games, with no ties.How is that possible?3. A man in a town married 20 women in the town.He and the women are still alive, and he has had no divorces.He is not a bigamist and not a Mormon and yet he broke no law.How is that possible?4. If you have black socks and brown socks in your drawer, mixed in the ratio of 4 : 5, how many socks will you have to take out to be sure of having a pair the same color? 5.A dealer in antique coins got an offer to buy a beautiful bronze coin.
When the wolf wears sheep's clothing: Individual differences in the desire to be liked influence nonconscious behavioral mimicry. J Arnett, C E Ashton-James, A Levordashka, A Baddeley, E Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Canada Virtual Event, V Benet-Martinez, O P John, S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, arXiv.org 2306.04308Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results. E M Bowden, M Jung-Beeman, 1994. 2013. 2003. 2021. March 3-10, 2021. 1998. 2020. 100759. 2023. February 7, 2023. 2023. 200316Social Psychological and Personality Science</p>
<p>The need for cognition. Y Zhang, arXiv.org 2303.12712v1Sparks of artificial general intelligence: early experiments with GPT-4. J T Cacioppo, R E Petty, 2023. 198242</p>
<p>Dispositional differences in cognitive motivation: The life and times of individuals varying in need for cognition. J T Cacioppo, R E Petty, J A Feinstein, W B G Jarvis, D J Chalmers, arXiv:2303.07103Could a large language model be conscious. S A Chermahini, M Hickendorff, B Hommel, 1996. 2023. 2012119arXiv preprintPsychological Bulletin</p>
<p>Using the BIDR to distinguish the effects of impression management and self-deception on the criterion validity of personality measures: A metaanalysis. T Chiang, J Christian, J Deese, J M Digman, G E Gignac, R L Greene, G M Harrison, L M Vallin, C A Haynes, J N Miles, K Clements, W Hirst, E A Phelps, J Hu, L Wang, T Hu, C Guo, Y Wang, J Huang, W Wang, M H Lam, E J Li, W Jiao, M R Lyu, J Kocoń, I Cichecki, O Kaszyca, M Kochanek, D Szydło, J Baran, . . Kazienko, P Li, X Li, Y Joty, S Liu, L Huang, F Qiu, L Bing, L Liu, E Z Suri, S Mu, T Zhou, A Finn, C Mahowald, K Ivanova, A A Blank, I A Kanwisher, N Tenenbaum, J B Fedorenko, E Dixon, K R Posey, J Kroger, J K Speed, A E , arXiv:2302.10724arXiv.org 2306.08400v1Machine psychology: Investigating emergent capabilities and behavior in large language models using psychological methods. D N Jones, D L Paulhus, New York. Hagendorff, T; Li, A., &amp; Bagger, J.; BenzPsychology PressFebruary 9, 2023. 2023. February 4, 2023. 1959. 1990. 2015. 1992. 2023. 2018. 2000. 2016. 1995. 2022. 2023. 19926v2. 2014. 2023. 2023. 2023. 2006. 2023. 2023. 201058arXiv preprintSimple embodied language learning as a byproduct of meta-reinforcement learning. arXiv.org 2301.06627v1. Matzen, L. E.</p>
<p>The malevolent side of human nature: A meta-analysis and critical review of the literature on the Dark Triad (Narcissism, Machiavellianism, and Psychopathy). I R Mckenzie, A Lyzhov, M Pieler, A Parrish, A Muller, A Prabhu, E Mclean, arXiv:2301.06627arXiv.org 2303.08774v3Dissociating language and thought in large language models: a cognitive perspective. D L Paulhus, D B Reid, HobokenRoediger, H.L2023. 2023. 2023. 2017. 2023. 2011. 2023. 2004. 2003. 1991. 1984. 2011. 199642Technical ReportJournal of Memory and Language</p>
<p>Incentives improve performance on both incremental and insight problem solving. H L Roediger, K B Mcdermott, M B Ryden, M Safdari, G Serapio-Garcia, C Crepy, S Fitz, P Romero, L Sun, M Abdulhai, A Faust, M Mataric, K Sanderson, X Sun, L Dong, X Li, Z Wan, S Wang, T Zhang, J Li, F Cheng, L Lyu, F Wu, G Wang, T Webb, K J Holyoak, H Lu, J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, . . Fedus, W Zhang, Z Lin, Y Liu, Z Li, P Sun, M Zhou, J Zhang, Z Zeng, Z Lin, Y Xiao, C Wang, X Han, X Liu, Z Xie, R Sun, M , arXiv.org 2110.01786v3An adult version of the Coopersmith Self-Esteem Inventory: Testretest reliability and social desirability. A Fry, J D , Willison, S1995. 1978. 2023. 2023. 2023. 2023. February, 2023. 2022. 2011. 2006. 2023. February 15, 2023. 2008. 202221Journal of experimental psychology: Learning, Memory, and CognitionMoEfication: Transformer feed-forward layers are Mixtures of Experts</p>
<p>Emergent modularity in pre-trained transformers. arXiv.org 2305.18390v1. J Zhou, R J Zhu, Q Zhao, J K Eshraghian, arXiv:2302.13939v1Spike GPT: Generative pre-trained language model with spiking neural networks. 2023. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3118 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3118</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3118</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-bb26227a94ddb2b0088a23e2ec0a170c40bc4d78</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bb26227a94ddb2b0088a23e2ec0a170c40bc4d78" target="_blank">Emergent Linear Representations in World Models of Self-Supervised Sequence Models</a></p>
                <p><strong>Paper Venue:</strong> BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</p>
                <p><strong>Paper TL;DR:</strong> Evidence of a closely related linear representation of the board state of Othello-playing neural network is provided and probing for “my colour” vs. “opponent” may be a simple yet powerful way to interpret the model’s internal state.</p>
                <p><strong>Paper Abstract:</strong> How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023a). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for “my colour” vs. “opponent’s colour” may be a simple yet powerful way to interpret the model’s internal state. This precise understanding of the internal representations allows us to control the model’s behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3118.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3118.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Activation Vector Arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vector Arithmetic Interventions on the Transformer Residual Stream</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper shows that OthelloGPT represents board features as linear directions (Mine, Yours, Empty) in the residual stream and that simple vector additions of these probe directions to activations causally change the model's belief state and next-move predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OthelloGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An 8-layer GPT-style transformer with 8 attention heads per layer and a 512-dimensional residual stream, trained autoregressively to predict the next legal move in Othello from sequences of prior moves (weights from Li et al., 2023a). Vocabulary: 60 move tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Not numeric arithmetic; arithmetic on internal vector representations (adding/subtracting learned direction vectors to/from the residual stream to steer internal 'world model' beliefs).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Linear representation hypothesis: features (Mine, Yours, Empty, FLIPPED) are encoded as directions in activation/residual space; adding/subtracting these direction vectors shifts the model's internal world model and thus alters outputs. The model uses these linear directions causally in computing next-move probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Linear probes trained to predict per-tile state (Mine/Yours/Empty) achieve very high accuracy by middle/late layers (e.g., 90.9% at input residual x^0 rising to ~99.6% by layer 6). Cosine similarity analysis shows strong negative similarity (-0.862) between a 'PLAYED' embedding circuit and the EMPTY probe direction, indicating EMPTY is a linear function of token embeddings. Intervention by adding probe direction vectors to residuals produces changed move predictions consistent with the edited board; interventions match effectiveness of prior gradient-based activation edits. Additional causal tests: subtracting p_FLIPPED reduces predicted flipped tiles and decreases error rates (avg error 0.486 vs null 1.686); end-game interventions produce low average error (0.112 vs non-intervention baseline 1.988).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Prior work (Li et al., 2023a) reported success with non-linear probes when classifying absolute colours (BLACK/WHITE/EMPTY); the paper acknowledges that different feature choices (BLACK/WHITE vs MINE/YOURS) can change probe conclusions. The model exhibits multiple circuits: in endgames, legal moves are sometimes predicted before a perfect board is computed, and board-probe accuracy drops near endgame. Interventions must often be applied across multiple layers to be effective, and the model can partially self-repair (Hydra effect), challenging simple single-layer causal accounts.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Inference-time vector addition: x' <- x + alpha * p_d^lambda(x) applied to residual stream at one or more layers, where p_d^lambda are linear probe direction vectors for d in {MINE, YOURS, EMPTY} (also experiments subtracting p_FLIPPED). Compared/contrasted with prior gradient-based activation editing.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Shifting residual activations along probe directions changes the model's predicted legal moves to match the intended edited board state; effectiveness comparable to gradient-based edits from prior work. Subtracting p_FLIPPED lowered error rates in targeted flipped-tile interventions; full interventions across sufficient layers yield low average error rates (e.g., 0.112 in endgame set).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Linear probe per-tile classification (Mine/Yours/Empty) accuracies by layer: x^0 = 90.9%, x^1 = 94.8%, x^2 = 97.2%, x^3 = 98.3%, x^4 = 99.0%, x^5 = 99.4%, x^6 = 99.6%, x^7 = 99.5% (Table 1). FLIPPED probe F1 scores by layer: x^0=74.76, x^1=85.75, x^2=91.62, x^3=94.82, x^4=96.44, x^5=97.13, x^6=96.82, x^7=96.3 (Table 3). Intervention error rates: subtracting p_FLIPPED average error = 0.486 (null baseline 1.686); endgame interventions average error = 0.112 (non-intervention baseline 1.988). Top-1 next-move error rates when applying unembedding to earlier layers decrease across layers (Table 5; e.g., from 0.215 at x^0 to 0.001 at x^7 for the trained model).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Reduced probe accuracy and more frequent MoveFirst behavior near endgames (model sometimes predicts moves before computing full board), need to intervene on multiple layers for robust effect, potential model self-repair after interventions (Hydra effect), and sensitivity to choice of feature framing (BLACK/WHITE vs MINE/YOURS) which can change interpretability conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison to human arithmetic or symbolic computation; the paper uses an analogy about representation choice (e.g., representing distance vs. distance squared) but does not relate to numeric arithmetic tasks or symbolic calculators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent Linear Representations in World Models of Self-Supervised Sequence Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Emergent world representations: Exploring a sequence model trained on a synthetic task <em>(Rating: 2)</em></li>
                <li>Inferencetime intervention: Eliciting truthful answers from a language model <em>(Rating: 2)</em></li>
                <li>Editing models with task arithmetic <em>(Rating: 2)</em></li>
                <li>Locating and editing factual associations in GPT <em>(Rating: 1)</em></li>
                <li>Steering gpt-2-xl by adding an activation vector - ai alignment forum <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3118",
    "paper_id": "paper-bb26227a94ddb2b0088a23e2ec0a170c40bc4d78",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "Activation Vector Arithmetic",
            "name_full": "Vector Arithmetic Interventions on the Transformer Residual Stream",
            "brief_description": "The paper shows that OthelloGPT represents board features as linear directions (Mine, Yours, Empty) in the residual stream and that simple vector additions of these probe directions to activations causally change the model's belief state and next-move predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OthelloGPT",
            "model_description": "An 8-layer GPT-style transformer with 8 attention heads per layer and a 512-dimensional residual stream, trained autoregressively to predict the next legal move in Othello from sequences of prior moves (weights from Li et al., 2023a). Vocabulary: 60 move tokens.",
            "arithmetic_task_type": "Not numeric arithmetic; arithmetic on internal vector representations (adding/subtracting learned direction vectors to/from the residual stream to steer internal 'world model' beliefs).",
            "reported_mechanism": "Linear representation hypothesis: features (Mine, Yours, Empty, FLIPPED) are encoded as directions in activation/residual space; adding/subtracting these direction vectors shifts the model's internal world model and thus alters outputs. The model uses these linear directions causally in computing next-move probabilities.",
            "evidence_for_mechanism": "Linear probes trained to predict per-tile state (Mine/Yours/Empty) achieve very high accuracy by middle/late layers (e.g., 90.9% at input residual x^0 rising to ~99.6% by layer 6). Cosine similarity analysis shows strong negative similarity (-0.862) between a 'PLAYED' embedding circuit and the EMPTY probe direction, indicating EMPTY is a linear function of token embeddings. Intervention by adding probe direction vectors to residuals produces changed move predictions consistent with the edited board; interventions match effectiveness of prior gradient-based activation edits. Additional causal tests: subtracting p_FLIPPED reduces predicted flipped tiles and decreases error rates (avg error 0.486 vs null 1.686); end-game interventions produce low average error (0.112 vs non-intervention baseline 1.988).",
            "evidence_against_mechanism": "Prior work (Li et al., 2023a) reported success with non-linear probes when classifying absolute colours (BLACK/WHITE/EMPTY); the paper acknowledges that different feature choices (BLACK/WHITE vs MINE/YOURS) can change probe conclusions. The model exhibits multiple circuits: in endgames, legal moves are sometimes predicted before a perfect board is computed, and board-probe accuracy drops near endgame. Interventions must often be applied across multiple layers to be effective, and the model can partially self-repair (Hydra effect), challenging simple single-layer causal accounts.",
            "intervention_type": "Inference-time vector addition: x' &lt;- x + alpha * p_d^lambda(x) applied to residual stream at one or more layers, where p_d^lambda are linear probe direction vectors for d in {MINE, YOURS, EMPTY} (also experiments subtracting p_FLIPPED). Compared/contrasted with prior gradient-based activation editing.",
            "effect_of_intervention": "Shifting residual activations along probe directions changes the model's predicted legal moves to match the intended edited board state; effectiveness comparable to gradient-based edits from prior work. Subtracting p_FLIPPED lowered error rates in targeted flipped-tile interventions; full interventions across sufficient layers yield low average error rates (e.g., 0.112 in endgame set).",
            "performance_metrics": "Linear probe per-tile classification (Mine/Yours/Empty) accuracies by layer: x^0 = 90.9%, x^1 = 94.8%, x^2 = 97.2%, x^3 = 98.3%, x^4 = 99.0%, x^5 = 99.4%, x^6 = 99.6%, x^7 = 99.5% (Table 1). FLIPPED probe F1 scores by layer: x^0=74.76, x^1=85.75, x^2=91.62, x^3=94.82, x^4=96.44, x^5=97.13, x^6=96.82, x^7=96.3 (Table 3). Intervention error rates: subtracting p_FLIPPED average error = 0.486 (null baseline 1.686); endgame interventions average error = 0.112 (non-intervention baseline 1.988). Top-1 next-move error rates when applying unembedding to earlier layers decrease across layers (Table 5; e.g., from 0.215 at x^0 to 0.001 at x^7 for the trained model).",
            "notable_failure_modes": "Reduced probe accuracy and more frequent MoveFirst behavior near endgames (model sometimes predicts moves before computing full board), need to intervene on multiple layers for robust effect, potential model self-repair after interventions (Hydra effect), and sensitivity to choice of feature framing (BLACK/WHITE vs MINE/YOURS) which can change interpretability conclusions.",
            "comparison_to_humans_or_symbolic": "No direct comparison to human arithmetic or symbolic computation; the paper uses an analogy about representation choice (e.g., representing distance vs. distance squared) but does not relate to numeric arithmetic tasks or symbolic calculators.",
            "uuid": "e3118.0",
            "source_info": {
                "paper_title": "Emergent Linear Representations in World Models of Self-Supervised Sequence Models",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Emergent world representations: Exploring a sequence model trained on a synthetic task",
            "rating": 2
        },
        {
            "paper_title": "Inferencetime intervention: Eliciting truthful answers from a language model",
            "rating": 2
        },
        {
            "paper_title": "Editing models with task arithmetic",
            "rating": 2
        },
        {
            "paper_title": "Locating and editing factual associations in GPT",
            "rating": 1
        },
        {
            "paper_title": "Steering gpt-2-xl by adding an activation vector - ai alignment forum",
            "rating": 1
        }
    ],
    "cost": 0.0093525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Emergent Linear Representations in World Models of Self-Supervised Sequence Models</h1>
<p>Neel Nanda ${ }^{*}$<br>Independent</p>
<p>Andrew Lee ${ }^{*}$<br>University of Michigan</p>
<p>Martin Wattenberg<br>Harvard University</p>
<h4>Abstract</h4>
<p>How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state ( Li et al., 2023a). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for "my colour" vs. "opponent's colour" may be a simple yet powerful way to interpret the model's internal state. This precise understanding of the internal representations allows us to control the model's behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>How do sequence models represent their decisionmaking process? Large language models are capable of unprecedented feats, yet largely remain inscrutable black boxes. Yet evidence has accumulated that models extract features - articulable properties of the input ${ }^{2}$ - and represent them in its internal activations (Geva et al., 2021; Bau et al., 2020; Gurnee et al., 2023; Belinkov, 2022; Burns et al., 2022; Goh et al., 2021; Elhage et al., 2022a). A key first step for interpreting them is understanding how these features are represented. Mikolov et al. (2013c) introduce the linear representation hypothesis: that features are represented linearly as directions in activation space. This would be highly consequential if true, yet this remains controversial and without conclusive empirical justification. In this work, we present novel evidence of</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The emergent world models of OthelloGPT are linearly represented. We find that the board states are encoded relative to the current player's colour (MINE vs. YOURS) as opposed to absolute colours (BLACK vs. WHITE).
linear representations, and show that this hypothesis has real predictive power.</p>
<p>We build on the work of Li et al. (2023a), who demonstrate the emergence of a world model in sequence models. Namely, the authors train OthelloGPT, an autoregressive transformer model, to predict legal moves in a game of Othello given a sequence of prior moves (Section 2.2). They show that the model spontaneously learns to track the correct board state, recovered using non-linear probes, despite never being told that the board exists. They further show a causal relationship between the model's inner board state and its move predictions using model edits. Namely, they show that the edited network plays moves that are legal in the edited board state even if illegal in the original board, and even if the edited board state is unreachable by legal play (i.e., out of distribution).</p>
<p>Critically, the original authors claim that OthelloGPT uses non-linear representations to encode the board state, by achieving high accuracy with non-linear probes, but failing to do so using linear</p>
<p>probes. In our work, we demonstrate that a closely related world model is actually linearly encoded. Our key insight is that rather than encoding the colours of the board (BLACK, White, Empty), the sequence model encodes the board relative to the current player of each timestep (Mine, Yours, Empty). In other words, for odd timesteps, the model considers Black tiles as Mine and White tiles as Yours, and vice versa for even timesteps (Section 3). Using this insight, we demonstrate that a linear projection can be learned with near perfect accuracy to derive the board state.</p>
<p>We further demonstrate that we can steer the sequence model's predictions by simply conducting vectoral arithmetics using our linear vectors (Section 4). Put differently, by pushing the model's activations in the directions of Mine, Yours, or Empty, we can alter the model's belief state of the board, and change its predictions accordingly. Our intervention method is much simpler and interpretable than that of Li et al. (2023a), which rely on gradients to update the model's activations (Section 4.1). Our results confirm that our interpretation of each probe direction is correct, but also demonstrates that a mechanistic understanding of model representations can lead to better control. Our results do not contradict that of Li et al. (2023a), but add to our understanding of emergent world models.</p>
<p>We provide additional interpretations of the sequence model using linear operations. For example, we provide empirical evidence of how the model derives empty tiles of the board, and find additional linear representations, such as tiles being FLIPPED at each timestep.</p>
<p>Finally, we provide a short discussion of our thoughts. How should we think of linear versus non-linear representations? Perhaps most interestingly, why do linear representations emerge?</p>
<h2>2 Preliminaries</h2>
<p>In this section we briefly describe Othello, OthelloGPT, and our notations.</p>
<h3>2.1 Othello</h3>
<p>Othello is a two player game played on a $8 \times 8$ grid. Players take turns playing black or white discs on the board, and the objective is to have the majority of one's coloured discs by the end of the game.</p>
<p>The board always starts with the middle 4 tiles filled with black and white tiles. At each turn, when
a tile is played, all of the opponent's discs that are enclosed in a horizontal, vertical, or diagonal row between two discs of the current player are flipped. The game ends when there are no more valid moves for both players.</p>
<h3>2.2 OthelloGPT</h3>
<p>OthelloGPT is a 8-layer GPT model (Radford et al., 2019), each layer consisting of 8 attention heads and a 512-dimensional hidden space. We use the model weights provided by Li et al. (2023a), denoted there as the synthetic model. The vocabulary space consists of 60 tokens, ${ }^{3}$ each one corresponding to a playable move on the board (e.g., A4).</p>
<p>The model is trained in an autoregressive manner, meaning for a given sequence of moves $m_{&lt;t}$, the model must predict the next valid move $m_{t}$.</p>
<p>Note that no a priori knowledge of the game nor its rules are provided to the model. Rather, the model is only given move sequences with a training objective to predict next valid moves, by randomly sampling sequences of games from a game tree. This training objective differs from that of models like AlphaZero (Silver et al., 2018), which are trained to play strategic moves to win games.</p>
<h3>2.3 Notations</h3>
<p>Transformers. Our transformer architecture (Vaswani et al., 2017) consists of embedding and unembedding layers $E m b$ and $U n e m b$ with a series of $L$ transformer layers in-between. Each transformer layer $l$ consists of $H$ multi-head attentions and a multilayer perception (MLP) layer.</p>
<p>A forward pass in the model first embeds the input token at timestep $t$ using embedding layer $E m b$ into a high dimensional space $x_{t}^{0} \in \mathbb{R}^{D}$. We refer to $x_{t \in T}^{0}$ as the start of the residual stream. Then each attention head $A t t_{l}^{h}, \forall h \in H$ and MLP block at layer $l$ add to the residual stream:</p>
<p>$$
\begin{array}{r}
x_{t}^{l, m i d}=x_{t}^{l}+\sum_{h \in H} A t t_{l}^{h}\left(x_{t}^{l}\right) \
x_{t}^{l+1}=x_{t}^{l, m i d}+M L P_{l}\left(x_{t}^{l, m i d}\right)
\end{array}
$$</p>
<p>Each attention head $A t t_{l}^{h}$ computes value vectors by projecting the residual stream to a lower dimension using $A t t_{l}^{h} . V$, linearly combines value</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$x^{0}$</th>
<th style="text-align: center;">$x^{1}$</th>
<th style="text-align: center;">$x^{2}$</th>
<th style="text-align: center;">$x^{3}$</th>
<th style="text-align: center;">$x^{4}$</th>
<th style="text-align: center;">$x^{5}$</th>
<th style="text-align: center;">$x^{6}$</th>
<th style="text-align: center;">$x^{7}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Randomized</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">34.5</td>
</tr>
<tr>
<td style="text-align: center;">Probabilistic</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Linear {BLACK, WHITE, EMPTY $}$</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">74.4</td>
</tr>
<tr>
<td style="text-align: center;">Non-Linear {BLACK, WHITE, EMPTY $}$</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">98.3</td>
</tr>
<tr>
<td style="text-align: center;">Linear ${$ Mine, Yours, Empty $}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 9}$</td>
<td style="text-align: center;">$\mathbf{9 4 . 8}$</td>
<td style="text-align: center;">$\mathbf{9 7 . 2}$</td>
<td style="text-align: center;">$\mathbf{9 8 . 3}$</td>
<td style="text-align: center;">$\mathbf{9 9}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 4}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 6}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Probing accuracy for board states. OthelloGPT linearly encodes the board state relative to the current player at each timestep (Mine vs. Yours, as opposed to colours BLACK or WHITE.
vectors using $A t t_{l}^{h} . A$, and projects back to the residual stream using $A t t_{l}^{h} . O$ :</p>
<p>$$
h(x)=\left(A t t n_{l}^{h} \cdot A \otimes A t t n_{l}^{h} \cdot O \cdot A t t n_{l}^{h} \cdot V\right) \cdot x
$$</p>
<p>where $\otimes$ notates a tensor product. A final prediction is made by applying $U$ nemb on $x^{L-1}$, followed by a softmax.</p>
<p>Probe Models. We notate linear and non-linear probes as $p^{\lambda}$ and $p^{\nu}$. Our linear probes are simple linear projections from the residual stream: $p^{\lambda}\left(x_{t}^{l}\right)=\operatorname{softmax}\left(W x_{t}^{l}\right), W \in \mathbb{R}^{D \times 3}$. The dimension $D \times 3$ comes from doing a 3-way classification. ${ }^{4}$ Non-linear probes are 2-layer MLP models: $p^{\nu}\left(x_{t}^{l}\right)=\operatorname{softmax}\left(W_{1} \operatorname{ReLU}\left(W_{2} x_{t}^{l}\right)\right), W_{1} \in$ $\mathbb{R}^{H \times 3}, W_{2} \in \mathbb{R}^{D \times H}$. Li et al. (2023a) classify the colour at each tile (BLACK, White, Empty). Our insight is to classify the colours relative to the current turn's player (Mine, Yours, Empty).</p>
<h2>3 Linearly Encoded Board States</h2>
<p>In this section we describe our experiments to find linear board state representations.</p>
<h3>3.1 Experiment Setup</h3>
<p>Rather than encoding the colour of each tile (BLACK, White, Empty), OthelloGPT encodes each tile relative to the player of each timestep (Mine, Yours, Empty) - for odd timesteps, we consider Black to be Mine and White to be Yours, and vice versa for even timesteps.</p>
<p>In order to learn the weights of our linear probe, we train on random game sequences until a validation loss on a set of 512 games converges according to a patience value of 10 . In practice, our linear probes converge after around 100,000 training samples. We test our probes on a held out set of 1,000 games.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Intervening methodology: we intervene by adding either Empty, Mine, or Yours directions into each layer of the residual stream. Red squares in each board indicate the tiles that have been intervened, teal tiles indicate new legal moves post-intervention that the model predicts.</p>
<p>We train a different probe for each layer $l$. Hyperparameters are provided in the Appendix.</p>
<h3>3.2 Results</h3>
<p>Table 1 shows the accuracy for various probes.
We include four baselines. The first is a linear probe trained on a randomly initialized GPT model. We also include a probabilistic baseline, in which we always choose the most likely colour per tile at each timestep, according to a set of 60,000 games from training data. The next two baselines are probe models used in Li et al. (2023a): a linear and non-linear probe trained to classify amongst {BLACK, White, EMPTY $}$.</p>
<p>Our linear probes achieve high accuracy by layer 4. Unbeknownst previously, we show that the emerged board state is linearly encoded.</p>
<h2>4 Intervening with Linear Directions</h2>
<p>In this section we demonstrate how we intervene on OthelloGPT's board state using linear probes.</p>
<h3>4.1 Method</h3>
<p>An inherent issue with probing is that it is correlational, not causal (Belinkov, 2022). To validate that our probes have found a true world model, we confirm that the model uses the encoded board state for its predictions.</p>
<p>To verify this, we conduct the same intervention experiment as Li et al. (2023a). Namely, given an input game sequence (and its corresponding board state $B$ ), we intervene to make the model believe in an altered board state $B^{\prime}$. We then observe whether the model's prediction reflects the made-believe board state $B^{\prime}$ or the original board state $B$.</p>
<p>Our intervention approach is simple (Figure 2): we add our linear vectors to the residual stream of each layer:</p>
<p>$$
x^{\prime} \leftarrow x+\alpha p_{\delta}^{3}(x)
$$</p>
<p>where $d$ indicates a direction amongst ${$ Mine, Yours, Empty $}$ and $\alpha$ is a scaling factor. In other words, to flip a tile from Yours to Mine, we simply push the residual stream at every layer in the Mine direction, or to "erase" a previously played tile, we push in the EMPTY direction. ${ }^{5} 6$</p>
<p>Note that this intervention is much simpler than that of Li et al. (2023a). Namely, Li et al. (2023a) edits the activation space $(x)$ of OthelloGPT using their non-linear probes. More specifically, they use non-linear probes to predict board state $B$, then compute gradients had the correct board state been the target board state $B^{\prime}$, and finally use the gradients to update the activation space of OthelloGPT rather than the weights of the probe model. Instead, we perform a single vector addition.</p>
<h3>4.2 Experiment Setup</h3>
<p>For our intervention experiment, we adopt the same setup and metrics as Li et al. (2023a). We use an evaluation benchmark consisting of 1,000 test cases. Each test case consists of a partial game sequence $(B)$ and a targeted board state $B^{\prime}$.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: Error rates from interventions. We measure the number of false positives and false negatives in the top- N predictions post-intervention, where N is the number of legal moves in the target board state $B^{\prime}$.</p>
<p>We measure the efficacy of our intervention by treating the task as a multi-label classification problem. Namely, we compare the top- $N$ predictions post-intervention against the groundtruth set of legal moves at state $B^{\prime}$, where $N$ is the number of legal moves at $B^{\prime}$. We then compute error rate, or the number of false positives and false negatives.</p>
<p>Li et al. (2023a) only considers the scenario of flipping the colour of a tile. To also validate our EMPTY direction, we also experiment with "erasing" a previously played tile by making it empty.</p>
<h3>4.3 Results</h3>
<p>Table 2 shows the average error rates after our interventions. A null intervention measures the number of errors by comparing pre-intervention predictions on post-intervention groundtruths. Our interventions are equally effective as that of gradient-based editing (Li et al., 2023a), and confirms that our interpretation of each linear direction matches how the model uses such directions.</p>
<h2>5 Additional Linear Interpretations</h2>
<p>The linear representation hypothesis is of interest to the mechanistic interpretability community because it provides a foothold into understanding a system. The internal state of the transformer, the residual stream, is the sum of the outputs of all previous components (heads, layers, embeddings and neurons) (Elhage et al., 2021). Albeit the residual stream consisting of linear and non-linear transformations, linear functions of the residual stream allow us to identify where a computation of interest takes place, or trace how a representation of interest evolves over a forward pass.</p>
<p>In this section we leverage our newfound linear representation of board state to provide additional</p>
<p>interpretations of OthelloGPT, as proof of concept of how discovering linear representations unlocks downstream interpretability applications.</p>
<h3>5.1 Interpreting Empty Tiles</h3>
<p>Here we interpret how OthelloGPT derives the status of empty tiles.</p>
<p>The Empty Circuit. A key insight for EMPTY is that input tokens each correspond to a tile on the board (i.e., A4), and once played, the tile can only change colour but remains non-empty.</p>
<p>We view OthelloGPT as using attention heads to "broadcast" which moves have been played: given a move at timestep $t$, attention heads write this information into other residual streams. This information (PLAYED) can be represented as following. First, each move $m(\mathrm{~A} 4)$ is embedded: $E m b[m]$. Then the model writes this information to other residual streams using linear projections Att. $V$ and Att. $O$ (Section 2.3):</p>
<p>$$
\operatorname{PlAYED}<em h="h">{h}(m)=\operatorname{Emb}[m] @ A t t</em> . O
$$} . V @ A t t_{h</p>
<p>For each attention head in the first layer, ${ }^{7}$ we compute the cosine similarity between PlayEd and the $p_{\text {EMPTY }}^{\lambda}$ direction:</p>
<p>$$
\max <em h="h">{h \in H} \operatorname{CosSim}\left(\operatorname{PlAYED}</em>(m)\right)
$$}(m), p_{\text {EMPTY }}^{\lambda</p>
<p>Since the two terms encode opposite information, we expect a high negative cosine similarity.</p>
<p>We observe an average similarity score of $\mathbf{- 0 . 8 6 2}$ across all 60 squares, ${ }^{8}$, confirming that $p_{\text {EMPTY }}$ is encoding Not PlayEd. This tells us that $p_{\text {EMPTY }}$ is a linear function of the token embeddings.</p>
<p>This also implies that OthelloGPT knows which tiles are empty by $x^{0 . \text {,mid }}$ : after the first attention heads but before the MLP layer. On a binary classification task of EMPTY vs. NOT-EMPTY from 1,000 games in our test split, our probe achieves an accuracy of $\mathbf{7 6 . 8 \%}$ and $\mathbf{9 8 . 9 \%}$, when projecting from the residual stream before and after the attention heads from the first layer.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Difference in probability of A4 being empty, between our clean and corrupt sequences, measured in each attention head.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Examples of attention heads from the first layer attending to moves that are YOURS (left) or MINE (right).</p>
<p>Logit Attribute for EMPTY. The previous analysis is based on the weights of the model. Here we provide an alternative analysis by studying the activations during inference.</p>
<p>First, we select a move $m$ (A4) that we wish to explain. We then construct a "clean" and "corrupt" set of partial game sequences ( $\mathrm{N}=4,569$ ). Our clean set always includes $m$, while our corrupt set replaces all timesteps with $m$ in the clean set with an alternative move. We ensure that all games in our corrupt set remain legal sequences. Finally, we study the difference in probability that $m$ is empty, according to our probes, in our two sets. Namely, we project the outputs from each attention head onto the EMPTY direction and apply a softmax:</p>
<p>$$
P_{\operatorname{EMPTY}[m]}}(\sigma)=\operatorname{Softmax}\left(\sigma * p_{\operatorname{EMPTY}[m]}^{\lambda}\right)
$$</p>
<p>where $\sigma$ is the output from each attention head.
Figure 3 shows the difference in probability that A4 is empty, between our clean and corrupt inputs, measured in each attention head of the first layer.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$x^{0}$</th>
<th style="text-align: center;">$x^{1}$</th>
<th style="text-align: center;">$x^{2}$</th>
<th style="text-align: center;">$x^{3}$</th>
<th style="text-align: center;">$x^{4}$</th>
<th style="text-align: center;">$x^{5}$</th>
<th style="text-align: center;">$x^{6}$</th>
<th style="text-align: center;">$x^{7}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Linear {FLIPPED, NOT-FLIPPED}</td>
<td style="text-align: center;">74.76</td>
<td style="text-align: center;">85.75</td>
<td style="text-align: center;">91.62</td>
<td style="text-align: center;">94.82</td>
<td style="text-align: center;">96.44</td>
<td style="text-align: center;">97.13</td>
<td style="text-align: center;">96.82</td>
<td style="text-align: center;">96.3</td>
</tr>
</tbody>
</table>
<p>Table 3: $F 1$ score for probing on FLIPPED tiles. In addition to the board state, the model also linearly encodes concepts such as flipped tiles per timestep.</p>
<p>The figure decomposes two scenarios: when A4 was originally played as Mine or Yours. This is because some attention heads only attend to moves that are Mine $(4,7)$, while some only attend to Yours $(1,3,8)$, which we show below.</p>
<h3>5.2 Attending to My \&amp; Your Timesteps</h3>
<p>We find that some attention heads only attend to either MY or Your moves. Figure 4 shows two examples: at each timestep, each head alternates between attending to even or odd timesteps. Such behavior further indicates how the model computes its world model based on Mine and Yours as opposed to Black and White.</p>
<h3>5.3 Additional Linear Concepts: FLIPPED</h3>
<p>In addition to linearly representing the board state, we find that OthelloGPT also encodes which tiles are being flipped, or captured, at each timestep. To test this, we modify our probing task to classify between FLIPPED vs. Not-FLIPPED, with the same training setup described above. Given the class imbalance, for this experiment we report $F 1$ scores. Table 3 demonstrates high $F 1$ scores by layer 3.</p>
<p>We also conduct a modified version of our intervention experiment, in which we always randomly select a flipped tile at the current timestep to intervene on. Then, instead of adding either $p_{\text {MINE }}^{\lambda}$, $p_{\text {YOURS }}^{\lambda}$, or $p_{\text {EMPTY }}^{\lambda}$, we subtract $p_{\text {FLIPPED }}^{\lambda}$. This tests whether the FLIPPED feature is causally relevant for computing the next move, by exploring whether this is sufficient to cause the model to play valid moves in the new board state. We get an average error rate of $\mathbf{0 . 4 8 6}$, compared to a null intervention baseline rate of $\mathbf{1 . 6 8 6}$.</p>
<p>One can consider FLIPPED tiles as the difference between the previous and current board state. One might naturally think that a recurrent computation could derive the current board state by iteratively applying such differences. However, transformer models do not make recursive computations! ${ }^{9}$ Also, the derivative property of captured tiles being encoded in later layers might be</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>analogous to observations from previous studies of language models that show low-level lexical properties being encoded in lower layers and syntax and semantics being mostly captured in higher layers (Tenney et al., 2019).</p>
<h3>5.4 Multiple Circuits Hypothesis</h3>
<p>Although we find board state representations and their causality on move predictions, we find that they do not explain the entire model. Namely, if our understanding is correct, we expect the model to compute the board state before computing valid moves. However, we find that in end games, this is not the case.</p>
<p>To check for the correct board state, we apply our linear probes on each layer, and check the earliest layer in which all 64 tiles are correctly predicted. ${ }^{10}$ To check for correct move predictions, we project from each layer using the unembedding layer, and check the earliest layer in which the top- N move predictions are all correct, where N is the number of groundtruth legal moves.</p>
<p>Figure 5 plots the proportion of times the board state is computed before (or after) valid moves (first y-axis). We also overlay the average earliest layer in which board or moves are correctly computed (second y-axis, aqua and lime curves). To our surprise, we find that in end games, the model often computes legal moves before the board state (black bars). We henceforth refer to this behavior as MoveFirst, and share some thoughts.</p>
<p>End Game Circuits. First, MoveFirst starts to occur around move 30, which is the mid-point of the game. Second, MoveFirst occurs more frequently as we near the end of the game (increasing black bars). Interestingly, in Othello, starting from the mid-point, there are progressively fewer empty tiles than there are filled tiles as the board fills up. Also note that as the game progresses, it becomes more likely for every empty tile to be a legal move.</p>
<p>One possible explanation for this phenomenon is that in the end game, it may be possible to pre-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Proportion of times the board state is computed before/after move predictions are made (First y-axis). Light Grey: Boards are computed in an earlier layer than moves. Dark Grey, Black: Boards are computed in the same or later layer than moves. Red: Model never computes the correct board state. Aqua, Lime (Curves): Average earliest layer in which the board or moves are correctly computed (Second y-axis). Starting from the midgame, we start observing the model compute moves before boards (black bar), and this occurs more frequently as the game progresses.
dict legal moves with simpler circuits that do not require the entire board state. For instance, perhaps it combines EMPTY with other features such as IS-SURROUNDED-BY-MINE or IS-BORDER and so on.</p>
<p>Multiple Circuits. Interestingly, the model still uses the board state at end games. To demonstrate this, we run our intervention experiment on 1,000 end games, ${ }^{11}$ and still achieve a low error rate of $\mathbf{0 . 1 1 2} .{ }^{12}$ We thus hypothesize that OthelloGPT (and more broadly, sequence models) consist of multiple circuits. Another hypothesis is that residual networks make "iterative inferences" (Section 5.5), and for end games, OthelloGPT uses simpler circuits in the early layers and refines its predictions at late layers using board state.</p>
<p>End Game Board Accuracy. We observe that board state accuracy drops near end games. This can be seen by the growing red bars, but also by measuring per-timestep accuracy of our probes (see Appendix). It is unclear whether 1) the model does not bother to compute the perfect board state, as alternative circuits allow the model to still correctly predict legal moves, or 2) the model learns an alternative circuit because it struggles to compute the correct board state at end games.</p>
<p>Memorization. Note that in the first few timesteps, the board and legal moves are sometimes both computed in the same layer (dark grey bars). This may be due to memorization: 1) these</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>predictions both occur at the first layer, and 2) there are only so many openings in an Othello game.</p>
<h3>5.5 Iterative Feature Refinements</h3>
<p>Figure 6 visualizes OthelloGPT's "iterative inference" (Jastrzebski et al., 2018; Belrose et al., 2023; Veit et al., 2016; nostalgebraist, 2020), or iterative refinement of features. For each layer, we plot the projected board states using our probes, and projected next-move predictions using the unembedding layer. Multiple evidence of iterative refinements are provided in the Appendix.</p>
<h2>6 Discussions</h2>
<h3>6.1 On Linear vs. Non-Linear Interpretations</h3>
<p>One challenge with probing is knowing which features to look for. ${ }^{13}$ For instance, classifying {BLACK, WHITE} versus {MINE, YOURS} leads to different takeaways, which illustrates the danger of projecting our preconceptions. What might seem "sensible" to a human interpreter (BLACK, WHITE) may not be for a model. In hindsight, given the symmetric game-play of Othello, encoding MINE, YOURS is perfectly sensible for the model (For more examples of non-obvious, sensible features, see (McCoy et al., 2019; Nanda et al., 2023)).</p>
<p>More broadly, what is sensible, or alternatively, how we choose to interpret linear or non-linear encodings, can be relative to how we see the world. Suppose we had a perfect world model of our physical world. Further suppose that if and when it</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Iterative refinements: the top row shows each layer projected using our linear probes. The bottom row shows the model's predictions for legal moves at each layer, by applying the unembedding layer on each layer.
computes a gravitational force between two objects (Newton's law), we discover a neuron whose square root was the distance between two objects. Is this a non-linear representation of distance? Or, given the form of Netwon's law, is the square of the distance a more natural way for the model to represent the feature, and thus considered a linear representation? As this example shows, what constitutes a natural feature may be in the eye of the beholder.</p>
<h3>6.2 On the Emergence of Linear Representations</h3>
<p>Linear representations in sequence models have been observed before: iGPT (Chen et al., 2020), which was autoregressively trained to predict next pixels of images, lead to robust linear image representations. The question remains, why do linear feature representations emerge? What linear representations are currently encoded in large language models? One reason might be simply that matrix multiplication can easily extract a different subset of linear features for each neuron. However, we leave a complete explanation to future work.</p>
<h2>7 Related Work</h2>
<p>We discuss three broad related areas: understanding internal representations, interventions, and mechanistic interpretability.</p>
<h3>7.1 Understanding Internal Representations</h3>
<p>Multiple researchers have studied concept representations in sequence models. Li et al. (2021) train sequence models on a synthetic task, and uncover world models in their activations. Patel and Pavlick (2022) demonstrate that language models can learn to ground concepts (e.g., direction, colour) to real world representations. Burns et al. (2022); Marks and Tegmark (2023) find linear vectors that encode "truthfulness". Probing techniques have also
been used to extract linguistic characteristics in sentence embeddings (Conneau et al., 2018; Tenney et al., 2019). Researchers have also used structural probes to uncover syntactic structures in word embeddings (Hewitt and Manning, 2019) and language models (Eisape et al., 2022). Prior to current day language models, word embeddings (Mikolov et al., 2013b,a) built vectoral word representations.</p>
<p>Linear representations are found outside of language models as well. Merullo et al. (2022) demonstrate that image representations from vision models can be linearly projected into the input space of language models. McGrath et al. (2022) and Lovering et al. (2022) find interpretable representations of chess or Hex concepts in AlphaZero.</p>
<h3>7.2 Intervening On Language Models</h3>
<p>A growing body of work has intervened on language models, by which we mean controlling their behavior by altering their activations.</p>
<p>We consider two broad categories. Parametric approaches often use optimizations (i.e. gradient descent) to locate and alter activations ( Li et al., 2023a; Meng et al., 2022a,b; Hernandez et al., 2023; Hase et al., 2023). Meanwhile, inference-time interventions typically apply linear arithmetics, for instance by using "truthful" vectors (Li et al., 2023b), "task vectors" (Ilharco et al., 2022), or other "steering vectors" (Subramani et al., 2022; Turner et al., 2023).</p>
<h3>7.3 Mechanistic Interpretability</h3>
<p>Mechanistic interpretability (MI) studies neural networks by reverse-engineering their behavior (Olah et al., 2020; Elhage et al., 2021). The goal of MI is to understand the underlying computations and representations of a model, with a broader goal of validating that their behavior aligns with what researchers have intended. Such framework has allowed researchers to better understand grokking</p>
<p>(Nanda et al., 2023), superposition (Elhage et al., 2022b; Scherlis et al., 2022; Arora et al., 2018), or even individual neurons (Mu and Andreas, 2020; Antverg and Belinkov, 2021; Gurnee et al., 2023).</p>
<h2>8 Conclusion</h2>
<p>In this work we demonstrated that the emergent world model in Othello-playing sequence models is full of linear representations. Previously unbeknownst, we demonstrated that the board state in OthelloGPT is linearly represented by encoding the colour of each tile relative to the player at each timestep (Mine, Yours, Empty) as opposed to absolute colour (Black, White, Empty). We showed that we can accurately control the model's behaviour with simple vector arithmetic on the internal world model. Lastly, we mechanistically interpreted multiple facets of the sequence model, analysing how empty tiles are detected, and linear representations of which pieces are flipped. We find hints that multiple circuits might exist for predicting legal moves in the end game, as well as further evidence that residual networks iteratively refine their features across layers.</p>
<h2>9 Acknowledgements</h2>
<p>We thank the original authors of Li et al. (2023a) for opensourcing their work, making it possible to conduct our research.</p>
<p>We thank Chris Olah for invaluable discussion and encouragement, and drawing our attention to the implication of these results for the linear representation hypothesis.</p>
<h2>10 Author Contributions</h2>
<p>Neel Nanda discovered the linear representation in terms of relative board state, and showed that simple vector arithmetic sufficed for causal interventions. He led an initial version of the experiments and write-ups, and advised throughout.</p>
<p>Andrew Lee led this write-up and performed all experiments in this paper. He discovered the flipped linear representation, the empty circuit, and the multiple circuit hypothesis results.</p>
<p>Martin Wattenberg helped with editing and distilling the paper, and contributed the analogy about a linear vs quadratic representation of distance.</p>
<h2>References</h2>
<p>Omer Antverg and Yonatan Belinkov. 2021. On the pitfalls of analyzing individual neurons in language models. arXiv preprint arXiv:2110.07483.</p>
<p>Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. 2018. Linear algebraic structure of word senses, with applications to polysemy. Transactions of the Association for Computational Linguistics, 6:483-495.</p>
<p>David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, and Antonio Torralba. 2020. Understanding the role of individual units in a deep neural network. Proceedings of the National Academy of Sciences.</p>
<p>Yonatan Belinkov. 2022. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207-219.</p>
<p>Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. 2023. Eliciting latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112.</p>
<p>Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. 2022. Discovering latent knowledge in language models without supervision. ArXiV.</p>
<p>Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. 2020. Generative pretraining from pixels. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 1691-1703. PMLR.</p>
<p>Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni. 2018. What you can cram into a single $\$ \&amp;!8^{*}$ vector: Probing sentence embeddings for linguistic properties. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2126-2136, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Tiwalayo Eisape, Vineet Gangireddy, Roger Levy, and Yoon Kim. 2022. Probing for incremental parse states in autoregressive language models. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2801-2813, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston, Sheer ElShowk, Nicholas Joseph, Nova DasSarma, Ben Mann, Danny Hernandez, Amanda Askell, Kamal Ndousse, Andy Jones, Dawn Drain, Anna Chen, Yuntao Bai, Deep Ganguli, Liane Lovitt, Zac Hatfield-Dodds, Jackson Kernion, Tom Conerly, Shauna Kravec, Stanislav Fort, Saurav Kadavath, Josh Jacobson, Eli TranJohnson, Jared Kaplan, Jack Clark, Tom Brown,</p>
<p>Sam McCandlish, Dario Amodei, and Christopher Olah. 2022a. Softmax linear units. Transformer Circuits Thread. Https://transformercircuits.pub/2022/solu/index.html.</p>
<p>Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. 2022b. Toy models of superposition. Transformer Circuits Thread.</p>
<p>Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2021. A mathematical framework for transformer circuits. Transformer Circuits Thread. Https://transformercircuits.pub/2021/framework/index.html.</p>
<p>Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484-5495, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Mario Giulianelli, Jack Harding, Florian Mohnert, Dieuwke Hupkes, and Willem Zuidema. 2018. Under the hood: Using diagnostic classifiers to investigate and improve how language models track agreement information. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 240-248, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Gabriel Goh, Nick Cammarata †, Chelsea Voss $\dagger$, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah. 2021. Multimodal neurons in artificial neural networks. Distill. Https://distill.pub/2021/multimodal-neurons.</p>
<p>Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris Bertsimas. 2023. Finding neurons in a haystack: Case studies with sparse probing. arXiv preprint arXiv:2305.01610.</p>
<p>Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. 2023. Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models. arXiv preprint arXiv:2301.04213.</p>
<p>Evan Hernandez, Belinda Z Li, and Jacob Andreas. 2023. Measuring and manipulating knowledge representations in language models. arXiv preprint arXiv:2304.00740.</p>
<p>John Hewitt and Christopher D. Manning. 2019. A structural probe for finding syntax in word representations. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.</p>
<p>Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2022. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089.</p>
<p>Stanisław Jastrzebski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua Bengio. 2018. Residual connections encourage iterative inference. In International Conference on Learning Representations.</p>
<p>Belinda Z. Li, Maxwell Nye, and Jacob Andreas. 2021. Implicit representations of meaning in neural language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1813-1827, Online. Association for Computational Linguistics.</p>
<p>Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. 2023a. Emergent world representations: Exploring a sequence model trained on a synthetic task. In The Eleventh International Conference on Learning Representations.</p>
<p>Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. 2023b. Inferencetime intervention: Eliciting truthful answers from a language model. arXiv preprint arXiv:2306.03341.</p>
<p>Charles Lovering, Jessica Forde, George Konidaris, Ellie Pavlick, and Michael Littman. 2022. Evaluation beyond task performance: Analyzing concepts in alphazero in hex. In Advances in Neural Information Processing Systems, volume 35, pages 2599226006. Curran Associates, Inc.</p>
<p>Samuel Marks and Max Tegmark. 2023. The geometry of truth: Emergent linear structure in large language model representations of true/false datasets. arXiv preprint arXiv:2310.06824.</p>
<p>Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428-3448, Florence, Italy. Association for Computational Linguistics.</p>
<p>Thomas McGrath, Andrei Kapishnikov, Nenad Tomašev, Adam Pearce, Martin Wattenberg, Demis Hassabis, Been Kim, Ulrich Paquet, and Vladimir Kramnik. 2022. Acquisition of chess knowledge in alphazero. Proceedings of the National Academy of Sciences, 119(47):e2206625119.</p>
<p>Thomas McGrath, Matthew Rahtz, Janos Kramar, Vladimir Mikulik, and Shane Legg. 2023. The hydra effect: Emergent self-repair in language model computations. arXiv preprint arXiv:2307.15771.</p>
<p>Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022a. Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 36.</p>
<p>Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. 2022b. Massediting memory in a transformer. arXiv preprint arXiv:2210.07229.</p>
<p>Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick. 2022. Linearly mapping from image to text space. arXiv preprint arXiv:2209.15162.</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc.</p>
<p>Tomáš Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies, pages 746-751.</p>
<p>Jesse Mu and Jacob Andreas. 2020. Compositional explanations of neurons. Advances in Neural Information Processing Systems, 33:17153-17163.</p>
<p>Neel Nanda, Lawrence Chan, Tom Liberum, Jess Smith, and Jacob Steinhardt. 2023. Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217.
nostalgebraist. 2020. interpreting gpt: the logit lens.
Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. 2020. Zoom in: An introduction to circuits. Distill. Https://distill.pub/2020/circuits/zoom-in.</p>
<p>Roma Patel and Ellie Pavlick. 2022. Mapping language models to grounded conceptual spaces. In International Conference on Learning Representations.</p>
<p>Tiago Pimentel, Naomi Saphra, Adina Williams, and Ryan Cotterell. 2020a. Pareto probing: Trading off accuracy for complexity. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3138-3153, Online. Association for Computational Linguistics.</p>
<p>Tiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, and Ryan Cotterell. 2020b. Information-theoretic probing for linguistic structure. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4609-4622, Online. Association for Computational Linguistics.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners.</p>
<p>Naomi Saphra and Adam Lopez. 2019. Understanding learning dynamics of language models with SVCCA. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3257-3267, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Adam Scherlis, Kshitij Sachan, Adam S Jermyn, Joe Benton, and Buck Shlegeris. 2022. Polysemanticity and capacity in neural networks. arXiv preprint arXiv:2210.01892.</p>
<p>David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. 2018. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140-1144.</p>
<p>Nishant Subramani, Nivedita Suresh, and Matthew Peters. 2022. Extracting latent steering vectors from pretrained language models. In Findings of the Association for Computational Linguistics: ACL 2022, pages 566-581, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 45934601, Florence, Italy. Association for Computational Linguistics.</p>
<p>Mycal Tucker, Peng Qian, and Roger Levy. 2021. What if this modified that? syntactic interventions with counterfactual embeddings. In Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 862-875, Online. Association for Computational Linguistics.</p>
<p>Alex Turner, Monte MacDiarmid, David Udell, lisathiergart, and Ulisse Mini. 2023. Steering gpt-2-xl by adding an activation vector - ai alignment forum.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.</p>
<p>Andreas Veit, Michael J Wilber, and Serge Belongie. 2016. Residual networks behave like ensembles of relatively shallow networks. Advances in neural information processing systems, 29.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: center;">AdamW</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate</td>
<td style="text-align: center;">$1 \mathrm{e}-2$</td>
</tr>
<tr>
<td style="text-align: left;">Weight Decay</td>
<td style="text-align: center;">$1 \mathrm{e}-2$</td>
</tr>
<tr>
<td style="text-align: left;">Betas</td>
<td style="text-align: center;">$0.9,0.99$</td>
</tr>
<tr>
<td style="text-align: left;">Validation Step</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">Validation Size</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: left;">Validation Patience</td>
<td style="text-align: center;">10</td>
</tr>
</tbody>
</table>
<p>Table 4: Hyperparameters used for our linear probes.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Intervention results depending on layers intervened.</p>
<h2>A Hyperparameters for Linear Probes</h2>
<p>Table 4 provides hyperparameters used for our linear probes.</p>
<h2>B Intervening on Different Layers</h2>
<p>In practice there are a lot of ways to intervene using linear vectors. Figure 7 demonstrates different error rates depending on which layers are intervened. From our experiments, we observe that either a sufficient number of layers need to be intervened for OthelloGPT to alter its predictions. We offer a couple of hypotheses for this. First, we hypothesize that this is because of the residual structure of transformer models, and while each layer may write additional information into the residual streams, there may still be information from earlier layers that the model uses. A somewhat related hypothesis is that OthelloGPT might be demonstrating the Hydra effect (McGrath et al., 2023), in which language models demonstrate the ability to self-repair its computations after an intervention.</p>
<h2>C Multiple Circuits</h2>
<p>In Section 5.4, we find hints that OthelloGPT sometimes computes moves before boards at end games.</p>
<p>Namely, we check the earliest layers in which the board is correctly predicted with $100 \%$ accuracy. Could it be that at end games, legal moves can be predicted without needing the entire board? To this point, we experiment with variations of this experiment. In Figure 8, we check the earliest layer in which at least $90 \%$ of the board is first correctly computed. In Figure 9, we check the earliest layer in which the "minimum set" of tiles are correctly computed, where the minimum set is set of tiles that make each legal move playable (see Figure 10 for example). Despite a looser criteria for board state, we still see OthelloGPT computing moves before boards at end games.</p>
<p>Interestingly, our probes lose accuracy starts to drop in the end game as well (Figure 11). It is unclear whether 1) the model does not bother to compute the perfect board state, as alternative circuits might exist at end games, or 2) the model learns an alternative circuit because it struggles to compute the correct board state at end games.</p>
<h2>D Evidence of Iterative Feature Refinements</h2>
<p>As mentioned in Section 5.5, OthelloGPT demonstrates multiple evidence of iterative feature refinements: 1) Board state accuracy (as well as FLIPPED) improves from layer to layer (Table 1, 3). 2) Next-move predictions also improve from layer to layer. Table 5 reports the top-1 error rate when applying the unembedding layer on every layer using our test set from Section 3. As a baseline, we apply the same unembedding layer from OthelloGPT to the residual streams of a randomly initialized GPT model. 3) Linear probes across layers share similar directions. Figure 12 plots the cosine similarity between all linear probes, averaged across all 64 tiles and directions (MINE, YOURS, EMPTY).</p>
<h2>E On Principled Ways of Probing</h2>
<p>Probing has produced both excitement and skepticism amongst researchers (Belinkov, 2022). Here we provide our learnings regarding probing.</p>
<p>One criticism of probes is whether the discovered features are actually used by the model, i.e., correlation vs. causation. Intervention is commonly used to study causality (Giulianelli et al., 2018; Tucker et al., 2021), but have often reached mixed conclusions (Belinkov, 2022). While both linear and non-linear probes have demonstrated</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Percentage of times $\mathbf{9 0 \%}$ of the board state is computed before/after move predictions are made.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Percentage of times the "minimum set" of necessary board state is computed before/after move predictions are made.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Example of "minimum set" of tiles that make move G2 legal.
successful interventions (Li et al., 2023b; Turner et al., 2023), linear probes are much easier to interpret, as they imply that features simply correspond to vectoral directions.</p>
<p>Another challenge is knowing which features to probe for, which can lead to pitfalls. Taking OthelloGPT as an example, classifying {BLACK, White} versus {Mine, Yours} leads to different
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Accuracy per timestep for our linear probes.
takeaways, which illustrates the danger of projecting our preconceptions.</p>
<p>Speaking of incorrect takeaways, our last point concerns the expressivity of probe models. With an expressive-enough probe, there is a danger of the probe computing or memorizing the desired feature that one is looking for, rather than extracting (Pimentel et al., 2020a; Saphra and Lopez, 2019). Still, some researchers view linear classification</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Baseline: Random</th>
<th style="text-align: center;">$x^{0}$</th>
<th style="text-align: center;">$x^{1}$</th>
<th style="text-align: center;">$x^{2}$</th>
<th style="text-align: center;">$x^{3}$</th>
<th style="text-align: center;">$x^{4}$</th>
<th style="text-align: center;">$x^{5}$</th>
<th style="text-align: center;">$x^{6}$</th>
<th style="text-align: center;">$x^{7}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0.856</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.152</td>
<td style="text-align: center;">0.112</td>
<td style="text-align: center;">0.079</td>
<td style="text-align: center;">0.049</td>
<td style="text-align: center;">0.015</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.001</td>
</tr>
</tbody>
</table>
<p>Table 5: Top-1 error rates when applying the unembedding layer to earlier layers. As a baseline we apply OthelloGPT's unembedding layer on a randomly initialized GPT model.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Cosine similarity scores between linear probes across layers.
as inadequate (Pimentel et al., 2020b; Saphra and Lopez, 2019). We view our work as evidence that linear probes do have interpretable and controllable power, and anticipate these findings to generalize to larger language models.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ We intervene on a timestep $&gt;30$
${ }^{12}$ Non-intervention baseline: 1.988.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{13}$ For a longer discussion on probing, see Appendix.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
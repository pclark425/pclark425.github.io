<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8436 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8436</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8436</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-4298a7bca88001f5df2d1ae15cca186d46271dc5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4298a7bca88001f5df2d1ae15cca186d46271dc5" target="_blank">DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments</a></p>
                <p><strong>Paper Venue:</strong> Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> The qualitative analysis reveals emergent cognitive behaviors from end-to-end RL training, including the ability to formulate plans, formulate plans, cross-validate information from multiple sources, engage in self-reflection to redirect research, and maintain honesty when unable to find definitive answers.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) equipped with web search capabilities have demonstrated impressive potential for deep research tasks. However, current approaches predominantly rely on either manually engineered prompts (prompt engineering-based) with brittle performance or reinforcement learning within controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that fail to capture the complexities of real-world interaction. In this paper, we introduce DeepResearcher, the first comprehensive framework for end-to-end training of LLM-based deep research agents through scaling reinforcement learning (RL) in real-world environments with authentic web search interactions. Unlike RAG-based approaches that assume all necessary information exists within a fixed corpus, our method trains agents to navigate the noisy, unstructured, and dynamic nature of the open web. We implement a specialized multi-agent architecture where browsing agents extract relevant information from various webpage structures and overcoming significant technical challenges. Extensive experiments on open-domain research tasks demonstrate that DeepResearcher achieves substantial improvements of up to 28.9 points over prompt engineering-based baselines and up to 7.2 points over RAG-based RL agents. Our qualitative analysis reveals emergent cognitive behaviors from end-to-end RL training, including the ability to formulate plans, cross-validate information from multiple sources, engage in self-reflection to redirect research, and maintain honesty when unable to find definitive answers. Our results highlight that end-to-end training in real-world web environments is not merely an implementation detail but a fundamental requirement for developing robust research capabilities aligned with real-world applications. We release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8436.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8436.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepResearcher</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepResearcher (RL-trained web-search research agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end reinforcement-learning trained multi-agent system that operates in real-world web search environments to perform open-domain research and QA by iteratively issuing searches, browsing pages, extracting information, and answering questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DeepResearcher</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Multi-agent LLM framework with a dedicated web-browsing/reading agent and an RL-trained controller; reasoning is done in explicit <think> steps, actions are JSON tool calls (web_search, browse_webpage), and the browsing agent maintains per-query short-term memory. Trained end-to-end with Group Relative Policy Optimization (GRPO).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen2.5-7B-Instruct backbone used as the policy / actor model for RL training (7B instruct-tuned variant).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain QA suite (NQ, TriviaQA, HotpotQA, 2Wiki) plus OOD benchmarks (MuSiQue, Bamboogle, PopQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer short open-domain questions (single-hop and multi-hop) by performing live web search, browsing webpages, extracting facts and synthesizing answers; evaluated with F1 and model-based evaluation (MBE).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / multi-step web-based reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term per-query working memory (explicit short-term repository) + use of external web retrieval (live search results)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>A dedicated web browsing (reading) agent holds an explicit short-term memory repository per query/rollout; as the agent browses segmented webpage content it appends relevant information to this buffer and decides whether to continue reading or stop; at stop it compiles and returns the accumulated short-term memory to the main agent. Rollouts are performed in parallel with distinct short-term memories.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Relevant extracted information from webpages: snippets / extracted facts / segments identified as relevant (i.e., incremental webpage extractions appended to the short-term buffer).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Memory is accessed/returned by the browsing agent as concatenated compiled information (prompt concatenation of short-term buffer into subsequent reasoning steps); search results are cached (query-level caching for 7 days) and browsing proceeds sequentially across segmented page content (heuristic skipping if early segments are irrelevant).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>In-domain (Table 1): NQ F1 39.6 / MBE 61.9; TQ F1 78.4 / MBE 85.0; HotpotQA F1 52.8 / MBE 64.3; 2Wiki F1 59.7 / MBE 66.6. Out-of-domain (Table 2): MuSiQue F1 27.1 / MBE 29.3; Bamboogle F1 71.0 / MBE 72.8; PopQA F1 48.5 / MBE 52.7.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No explicit ablation turning off the short-term browsing memory is reported. Comparisons: DeepResearcher (trained in real web) outperforms prompt-engineered baselines (up to +28.9 points) and RAG-based RL agents (up to +7.2 points). Paper contrasts DeepResearcher with local RAG baselines (e.g., Search-r1-base trained on Wikipedia) and web-search baselines (R1-Searcher), attributing gains to end-to-end RL in real web environments and the browsing agent's information extraction strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Training with live web search and an explicit short-term browsing memory enables emergent behaviors (planning, cross-validation, reflection, honesty) and better generalization to OOD datasets than prompt-engineered or local-RAG-trained agents; short-term per-query memory in the browsing agent supports incremental extraction and verification across multiple sources.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No direct ablation quantifying the short-term memory's isolated contribution. Reported practical challenges include API rate limits, anti-crawling behavior, network latency, need for caching and retries. Data-contamination (parametric memorization) required filtering to ensure reliance on search rather than memorized answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8436.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8436.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WebBrowsingAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Web Browsing (Reading) Agent (component of DeepResearcher)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dedicated sub-agent that crawls and reads returned webpages in segments, maintains a short-term memory buffer per query/rollout, decides whether to continue reading or stop, and returns compiled relevant information to the main agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Web Browsing Agent / Reading Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Processes first-page segments of URLs sequentially, partitions long pages into segments, appends relevant extracted content to a short-term memory repository and applies heuristics (skip if early segments irrelevant). Acts as an information extraction module in the multi-agent architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>component of DeepResearcher (Qwen2.5-7B-Instruct used in system)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reading agent implemented within the DeepResearcher pipeline; not separately benchmarked as a standalone LM.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Webpage information extraction for open-domain QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given URLs / search results, read webpages (segmented), extract question-relevant content incrementally, manage traversal (continue/stop), and supply compiled findings back to the controller for answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>web browsing / information extraction (subtask of QA)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>explicit short-term per-query buffer (working memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Explicit short-term memory repository where relevant extracted snippets/segments are appended during browsing; at termination the agent aggregates the buffer and returns it to the main policy; search-result caching is used to avoid repeated web API calls.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Extracted relevant webpage segments, intermediate facts, and extracted snippets appended to the buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Sequential read and append; compiled buffer is concatenated into the controller's subsequent reasoning steps; caching of identical search queries for up to 7 days to reduce calls.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No separate ablation reported for disabling the reading agent's short-term memory. The paper claims this multi-agent, segment-based reading + memory strategy improves extraction accuracy and efficiency relative to naive summarization of top-k snippets used in some baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Partitioning pages and maintaining a per-query short-term buffer reduces wasted reading of long irrelevant pages and improves resource allocation; sequential processing of segments allows early skipping heuristics to improve efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Heuristic assumption that irrelevant early page segments indicate an unproductive page may fail in some cases; long pages and heterogeneous formats complicate reliable extraction; requires engineering to handle anti-crawl responses and rate limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8436.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8436.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Search-r1-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Search-r1 (base actor, local RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL-trained retrieval-augmented agent that operates in a local RAG environment with direct access to a static Wikipedia corpus; used as a baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Search-r1: Training llms to reason and leverage search engines with reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Search-r1-base</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An RL method that trains LLMs to reason and leverage retrieval over a fixed local corpus (Wikipedia) during training and evaluation; acts as a local-RAG baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Baseline RL agent trained in local RAG setting (paper references Search-r1 work for details).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same QA benchmarks (NQ, TQ, HotpotQA, 2Wiki, MuSiQue, Bamboogle, PopQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer QA tasks using retrieval from a fixed local corpus (Wikipedia) plus RL-trained policy.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval-augmented memory (fixed local corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Retriever accesses a fixed Wikipedia corpus to fetch relevant passages; returned passages are used as context for the model.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Retrieved Wikipedia passages / document blocks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Similarity-based retrieval over the local corpus (RAG style retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Table 1 (local RAG): NQ F1 45.4 / MBE 60.0; TQ F1 71.9 / MBE 76.2; HotpotQA F1 55.9 / MBE 63.0; 2Wiki F1 44.6 / MBE 47.9. Table 2 (OOD): MuSiQue F1 26.7 / MBE 27.5; Bamboogle F1 56.5 / MBE 57.6; PopQA F1 43.2 / MBE 47.0.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared directly to DeepResearcher: Search-r1-base achieves competitive MBE on some in-domain datasets (NQ, HotpotQA) but benefits from privileged access to a focused Wikipedia corpus; paper highlights limitation that local RAG assumption (all needed info in fixed corpus) breaks in real web settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Local-RAG RL agents can perform well when test information is contained in the corpus (e.g., Wikipedia) but generalize poorly to web-wide or non-Wikipedia information; fixed-corpus retrieval suffers from timeliness and domain coverage limitations relative to real-web trained agents.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on the assumption that necessary information exists in the fixed corpus (violated for web/OOD benchmarks); storage/coverage/timeliness bottlenecks; not trained in noisy real-web environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8436.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8436.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R1-Searcher</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>R1-Searcher (web-search baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL-informed baseline that, when given a query, appends 'site:en.wikipedia.org' and summarizes the first three search result pages; used as a web-search baseline in the evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>R1-searcher: Incentivizing the search capability in llms via reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>R1-Searcher</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Baseline agent that uses web search (with site-restriction in some variants) and summarizes top pages; contrasts with DeepResearcher which selects URLs autonomously and is trained end-to-end in real web environment.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Baseline system described in referenced R1-Searcher work; used here as experimental comparator.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same QA benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use web-search (or restricted site search) and summarization of top results to answer QA prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / web search summarization</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented memory (web snippets / top-k pages summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Append site restriction to queries, retrieve top pages via Bing/engine, summarize selected pages (typically top-3) and feed summaries into reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Summaries of top-k web pages / snippets.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Select top-k search results and summarize them (deterministic top-k selection from search engine), then include summaries as context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Table 1 (web search): NQ F1 35.4 / MBE 52.3; TQ F1 73.1 / MBE 79.1; HotpotQA F1 44.8 / MBE 53.1; 2Wiki F1 59.4 / MBE 65.8. Table 2 (OOD): MuSiQue F1 22.8 / MBE 25.6; Bamboogle F1 64.8 / MBE 65.6; PopQA F1 42.7 / MBE 43.4.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared to DeepResearcher: R1-Searcher performs worse overall despite using web search, demonstrating that (i) training in a real web environment end-to-end and (ii) autonomous URL selection and specialized browsing/memory behavior in DeepResearcher yield advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simple top-k summarization of search results (even with web access) is less effective than an end-to-end RL-trained browsing + short-term memory pipeline in complex, noisy real-web settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Compulsory summarization of top results can miss better URLs; restricting search domain (site: wikipedia) limits coverage; lacks the multi-agent per-query short-term memory pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8436.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8436.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ParametricMemoryCheck</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parametric Memorization / Contamination Detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contamination-detection procedure to identify QA examples that models already memorize (parametric memory), ensuring the trained agent must rely on search rather than retrieving memorized answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Parametric memorization detection procedure</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Procedure used during dataset construction: sample 10 responses from the base model for each candidate question and exclude questions where the model produced the ground-truth answer (pass@10), thereby removing examples likely present in the model's pretraining data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>base model used for contamination detection (same family as training backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used to produce 10 sampled answers per candidate question to detect memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Training data curation for web-search RL</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Detect and remove QA examples likely memorized by the underlying LM to force reliance on web search during RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>data curation / contamination detection</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>parametric memory (model weights / memorized training data)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Empirical detection via sampling multiple model outputs (pass@10) and checking for matches with ground truth; questions passing this test (i.e., model already knows answer) are excluded from training data.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Implicit memorized facts encoded in model parameters (parametric memory).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Not an explicit retrieval mechanism; detected by sampling model outputs and matching to ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Not an ablation but a filter: removing contaminated (memorized) questions prevents training from exploiting parametric memory; no quantitative ablation presented showing impact of contamination removal on final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Contamination detection is necessary to ensure RL trains genuine search behaviors rather than exploiting memorized answers; without such filtering, apparent gains could reflect parametric memorization rather than tool usage.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Procedure depends on sampling budget (pass@10); may be conservative (exclude some valid examples) or fail to detect partial memorization; no quantitative measure provided of the downstream effect size of contamination removal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Search-r1: Training llms to reason and leverage search engines with reinforcement learning <em>(Rating: 2)</em></li>
                <li>R1-searcher: Incentivizing the search capability in llms via reinforcement learning <em>(Rating: 2)</em></li>
                <li>DeepSeek-R1: Incentivizing reasoning capability in llms via reinforcement learning <em>(Rating: 1)</em></li>
                <li>ReSearch: Learning to reason with search for llms via reinforcement learning <em>(Rating: 1)</em></li>
                <li>When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8436",
    "paper_id": "paper-4298a7bca88001f5df2d1ae15cca186d46271dc5",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "DeepResearcher",
            "name_full": "DeepResearcher (RL-trained web-search research agent)",
            "brief_description": "An end-to-end reinforcement-learning trained multi-agent system that operates in real-world web search environments to perform open-domain research and QA by iteratively issuing searches, browsing pages, extracting information, and answering questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "DeepResearcher",
            "agent_description": "Multi-agent LLM framework with a dedicated web-browsing/reading agent and an RL-trained controller; reasoning is done in explicit &lt;think&gt; steps, actions are JSON tool calls (web_search, browse_webpage), and the browsing agent maintains per-query short-term memory. Trained end-to-end with Group Relative Policy Optimization (GRPO).",
            "model_name": "Qwen2.5-7B-Instruct",
            "model_description": "Qwen2.5-7B-Instruct backbone used as the policy / actor model for RL training (7B instruct-tuned variant).",
            "task_name": "Open-domain QA suite (NQ, TriviaQA, HotpotQA, 2Wiki) plus OOD benchmarks (MuSiQue, Bamboogle, PopQA)",
            "task_description": "Answer short open-domain questions (single-hop and multi-hop) by performing live web search, browsing webpages, extracting facts and synthesizing answers; evaluated with F1 and model-based evaluation (MBE).",
            "task_type": "question answering / multi-step web-based reasoning",
            "memory_used": true,
            "memory_type": "short-term per-query working memory (explicit short-term repository) + use of external web retrieval (live search results)",
            "memory_mechanism": "A dedicated web browsing (reading) agent holds an explicit short-term memory repository per query/rollout; as the agent browses segmented webpage content it appends relevant information to this buffer and decides whether to continue reading or stop; at stop it compiles and returns the accumulated short-term memory to the main agent. Rollouts are performed in parallel with distinct short-term memories.",
            "memory_representation": "Relevant extracted information from webpages: snippets / extracted facts / segments identified as relevant (i.e., incremental webpage extractions appended to the short-term buffer).",
            "memory_retrieval_method": "Memory is accessed/returned by the browsing agent as concatenated compiled information (prompt concatenation of short-term buffer into subsequent reasoning steps); search results are cached (query-level caching for 7 days) and browsing proceeds sequentially across segmented page content (heuristic skipping if early segments are irrelevant).",
            "performance_with_memory": "In-domain (Table 1): NQ F1 39.6 / MBE 61.9; TQ F1 78.4 / MBE 85.0; HotpotQA F1 52.8 / MBE 64.3; 2Wiki F1 59.7 / MBE 66.6. Out-of-domain (Table 2): MuSiQue F1 27.1 / MBE 29.3; Bamboogle F1 71.0 / MBE 72.8; PopQA F1 48.5 / MBE 52.7.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No explicit ablation turning off the short-term browsing memory is reported. Comparisons: DeepResearcher (trained in real web) outperforms prompt-engineered baselines (up to +28.9 points) and RAG-based RL agents (up to +7.2 points). Paper contrasts DeepResearcher with local RAG baselines (e.g., Search-r1-base trained on Wikipedia) and web-search baselines (R1-Searcher), attributing gains to end-to-end RL in real web environments and the browsing agent's information extraction strategy.",
            "key_findings": "Training with live web search and an explicit short-term browsing memory enables emergent behaviors (planning, cross-validation, reflection, honesty) and better generalization to OOD datasets than prompt-engineered or local-RAG-trained agents; short-term per-query memory in the browsing agent supports incremental extraction and verification across multiple sources.",
            "limitations_or_challenges": "No direct ablation quantifying the short-term memory's isolated contribution. Reported practical challenges include API rate limits, anti-crawling behavior, network latency, need for caching and retries. Data-contamination (parametric memorization) required filtering to ensure reliance on search rather than memorized answers.",
            "uuid": "e8436.0",
            "source_info": {
                "paper_title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "WebBrowsingAgent",
            "name_full": "Web Browsing (Reading) Agent (component of DeepResearcher)",
            "brief_description": "A dedicated sub-agent that crawls and reads returned webpages in segments, maintains a short-term memory buffer per query/rollout, decides whether to continue reading or stop, and returns compiled relevant information to the main agent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Web Browsing Agent / Reading Agent",
            "agent_description": "Processes first-page segments of URLs sequentially, partitions long pages into segments, appends relevant extracted content to a short-term memory repository and applies heuristics (skip if early segments irrelevant). Acts as an information extraction module in the multi-agent architecture.",
            "model_name": "component of DeepResearcher (Qwen2.5-7B-Instruct used in system)",
            "model_description": "Reading agent implemented within the DeepResearcher pipeline; not separately benchmarked as a standalone LM.",
            "task_name": "Webpage information extraction for open-domain QA",
            "task_description": "Given URLs / search results, read webpages (segmented), extract question-relevant content incrementally, manage traversal (continue/stop), and supply compiled findings back to the controller for answer generation.",
            "task_type": "web browsing / information extraction (subtask of QA)",
            "memory_used": true,
            "memory_type": "explicit short-term per-query buffer (working memory)",
            "memory_mechanism": "Explicit short-term memory repository where relevant extracted snippets/segments are appended during browsing; at termination the agent aggregates the buffer and returns it to the main policy; search-result caching is used to avoid repeated web API calls.",
            "memory_representation": "Extracted relevant webpage segments, intermediate facts, and extracted snippets appended to the buffer.",
            "memory_retrieval_method": "Sequential read and append; compiled buffer is concatenated into the controller's subsequent reasoning steps; caching of identical search queries for up to 7 days to reduce calls.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No separate ablation reported for disabling the reading agent's short-term memory. The paper claims this multi-agent, segment-based reading + memory strategy improves extraction accuracy and efficiency relative to naive summarization of top-k snippets used in some baselines.",
            "key_findings": "Partitioning pages and maintaining a per-query short-term buffer reduces wasted reading of long irrelevant pages and improves resource allocation; sequential processing of segments allows early skipping heuristics to improve efficiency.",
            "limitations_or_challenges": "Heuristic assumption that irrelevant early page segments indicate an unproductive page may fail in some cases; long pages and heterogeneous formats complicate reliable extraction; requires engineering to handle anti-crawl responses and rate limits.",
            "uuid": "e8436.1",
            "source_info": {
                "paper_title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Search-r1-base",
            "name_full": "Search-r1 (base actor, local RAG)",
            "brief_description": "An RL-trained retrieval-augmented agent that operates in a local RAG environment with direct access to a static Wikipedia corpus; used as a baseline in the paper.",
            "citation_title": "Search-r1: Training llms to reason and leverage search engines with reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "Search-r1-base",
            "agent_description": "An RL method that trains LLMs to reason and leverage retrieval over a fixed local corpus (Wikipedia) during training and evaluation; acts as a local-RAG baseline.",
            "model_name": null,
            "model_description": "Baseline RL agent trained in local RAG setting (paper references Search-r1 work for details).",
            "task_name": "Same QA benchmarks (NQ, TQ, HotpotQA, 2Wiki, MuSiQue, Bamboogle, PopQA)",
            "task_description": "Answer QA tasks using retrieval from a fixed local corpus (Wikipedia) plus RL-trained policy.",
            "task_type": "question answering / retrieval-augmented generation",
            "memory_used": true,
            "memory_type": "external retrieval-augmented memory (fixed local corpus)",
            "memory_mechanism": "Retriever accesses a fixed Wikipedia corpus to fetch relevant passages; returned passages are used as context for the model.",
            "memory_representation": "Retrieved Wikipedia passages / document blocks.",
            "memory_retrieval_method": "Similarity-based retrieval over the local corpus (RAG style retrieval).",
            "performance_with_memory": "Table 1 (local RAG): NQ F1 45.4 / MBE 60.0; TQ F1 71.9 / MBE 76.2; HotpotQA F1 55.9 / MBE 63.0; 2Wiki F1 44.6 / MBE 47.9. Table 2 (OOD): MuSiQue F1 26.7 / MBE 27.5; Bamboogle F1 56.5 / MBE 57.6; PopQA F1 43.2 / MBE 47.0.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared directly to DeepResearcher: Search-r1-base achieves competitive MBE on some in-domain datasets (NQ, HotpotQA) but benefits from privileged access to a focused Wikipedia corpus; paper highlights limitation that local RAG assumption (all needed info in fixed corpus) breaks in real web settings.",
            "key_findings": "Local-RAG RL agents can perform well when test information is contained in the corpus (e.g., Wikipedia) but generalize poorly to web-wide or non-Wikipedia information; fixed-corpus retrieval suffers from timeliness and domain coverage limitations relative to real-web trained agents.",
            "limitations_or_challenges": "Relies on the assumption that necessary information exists in the fixed corpus (violated for web/OOD benchmarks); storage/coverage/timeliness bottlenecks; not trained in noisy real-web environments.",
            "uuid": "e8436.2",
            "source_info": {
                "paper_title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "R1-Searcher",
            "name_full": "R1-Searcher (web-search baseline)",
            "brief_description": "An RL-informed baseline that, when given a query, appends 'site:en.wikipedia.org' and summarizes the first three search result pages; used as a web-search baseline in the evaluation.",
            "citation_title": "R1-searcher: Incentivizing the search capability in llms via reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "R1-Searcher",
            "agent_description": "Baseline agent that uses web search (with site-restriction in some variants) and summarizes top pages; contrasts with DeepResearcher which selects URLs autonomously and is trained end-to-end in real web environment.",
            "model_name": null,
            "model_description": "Baseline system described in referenced R1-Searcher work; used here as experimental comparator.",
            "task_name": "Same QA benchmarks",
            "task_description": "Use web-search (or restricted site search) and summarization of top results to answer QA prompts.",
            "task_type": "question answering / web search summarization",
            "memory_used": true,
            "memory_type": "retrieval-augmented memory (web snippets / top-k pages summaries)",
            "memory_mechanism": "Append site restriction to queries, retrieve top pages via Bing/engine, summarize selected pages (typically top-3) and feed summaries into reasoning.",
            "memory_representation": "Summaries of top-k web pages / snippets.",
            "memory_retrieval_method": "Select top-k search results and summarize them (deterministic top-k selection from search engine), then include summaries as context.",
            "performance_with_memory": "Table 1 (web search): NQ F1 35.4 / MBE 52.3; TQ F1 73.1 / MBE 79.1; HotpotQA F1 44.8 / MBE 53.1; 2Wiki F1 59.4 / MBE 65.8. Table 2 (OOD): MuSiQue F1 22.8 / MBE 25.6; Bamboogle F1 64.8 / MBE 65.6; PopQA F1 42.7 / MBE 43.4.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared to DeepResearcher: R1-Searcher performs worse overall despite using web search, demonstrating that (i) training in a real web environment end-to-end and (ii) autonomous URL selection and specialized browsing/memory behavior in DeepResearcher yield advantages.",
            "key_findings": "Simple top-k summarization of search results (even with web access) is less effective than an end-to-end RL-trained browsing + short-term memory pipeline in complex, noisy real-web settings.",
            "limitations_or_challenges": "Compulsory summarization of top results can miss better URLs; restricting search domain (site: wikipedia) limits coverage; lacks the multi-agent per-query short-term memory pipeline.",
            "uuid": "e8436.3",
            "source_info": {
                "paper_title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "ParametricMemoryCheck",
            "name_full": "Parametric Memorization / Contamination Detection",
            "brief_description": "A contamination-detection procedure to identify QA examples that models already memorize (parametric memory), ensuring the trained agent must rely on search rather than retrieving memorized answers.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Parametric memorization detection procedure",
            "agent_description": "Procedure used during dataset construction: sample 10 responses from the base model for each candidate question and exclude questions where the model produced the ground-truth answer (pass@10), thereby removing examples likely present in the model's pretraining data.",
            "model_name": "base model used for contamination detection (same family as training backbone)",
            "model_description": "Used to produce 10 sampled answers per candidate question to detect memorization.",
            "task_name": "Training data curation for web-search RL",
            "task_description": "Detect and remove QA examples likely memorized by the underlying LM to force reliance on web search during RL training.",
            "task_type": "data curation / contamination detection",
            "memory_used": true,
            "memory_type": "parametric memory (model weights / memorized training data)",
            "memory_mechanism": "Empirical detection via sampling multiple model outputs (pass@10) and checking for matches with ground truth; questions passing this test (i.e., model already knows answer) are excluded from training data.",
            "memory_representation": "Implicit memorized facts encoded in model parameters (parametric memory).",
            "memory_retrieval_method": "Not an explicit retrieval mechanism; detected by sampling model outputs and matching to ground truth.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Not an ablation but a filter: removing contaminated (memorized) questions prevents training from exploiting parametric memory; no quantitative ablation presented showing impact of contamination removal on final performance.",
            "key_findings": "Contamination detection is necessary to ensure RL trains genuine search behaviors rather than exploiting memorized answers; without such filtering, apparent gains could reflect parametric memorization rather than tool usage.",
            "limitations_or_challenges": "Procedure depends on sampling budget (pass@10); may be conservative (exclude some valid examples) or fail to detect partial memorization; no quantitative measure provided of the downstream effect size of contamination removal.",
            "uuid": "e8436.4",
            "source_info": {
                "paper_title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Search-r1: Training llms to reason and leverage search engines with reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "R1-searcher: Incentivizing the search capability in llms via reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "DeepSeek-R1: Incentivizing reasoning capability in llms via reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "ReSearch: Learning to reason with search for llms via reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories",
            "rating": 1
        }
    ],
    "cost": 0.0173955,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments</h1>
<p>Yuxiang Zheng ${ }^{1,2,3 <em>}$ Dayuan $\mathbf{F u}^{2,3 </em>}$ Xiangkun $\mathbf{H u}^{2 *}$<br>Xiaojie $\mathbf{C a l}^{1,3} \quad$ Lyumanshan $\mathbf{Y e}^{1,3} \quad$ Pengrui $\mathbf{L u}^{1,3} \quad$ Pengfei $\mathbf{L i u}^{1,2,3 \dagger}$<br>${ }^{1}$ SJTU ${ }^{2}$ SII ${ }^{3}$ GAIR</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) equipped with web search capabilities have demonstrated impressive potential for deep research tasks. However, current approaches predominantly rely on either manually engineered prompts (prompt engineering-based) with brittle performance or reinforcement learning within controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that fail to capture the complexities of real-world interaction. In this paper, we introduce DeepResearcher, the first comprehensive framework for end-to-end training of LLM-based deep research agents through scaling reinforcement learning (RL) in real-world environments with authentic web search interactions. Unlike RAG-based approaches that assume all necessary information exists within a fixed corpus, our method trains agents to navigate the noisy, unstructured, and dynamic nature of the open web. We implement a specialized multiagent architecture where browsing agents extract relevant information from various webpage structures and overcoming significant technical challenges. Extensive experiments on open-domain research tasks demonstrate that DeepResearcher achieves substantial improvements of up to $\mathbf{2 8 . 9}$ points over prompt engineering-based baselines and up to $\mathbf{7 . 2}$ points over RAG-based RL agents. Our qualitative analysis reveals emergent cognitive behaviors from end-to-end RL training, including the ability to formulate plans, cross-validate information from multiple sources, engage in self-reflection to redirect research, and maintain honesty when unable to find definitive answers. Our results highlight that end-to-end training in real-world web environments is not merely an implementation detail but a fundamental requirement for developing robust research capabilities aligned with real-world applications. We release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: DeepResearcher performs the best on all 7 datasets measured by reliable model-based evaluation.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Emergent Planning and Cross Validation Behaviors</h1>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Planning and Cross Validation Behaviors. Left: DeepResearcher is capable of making plans and dynamically adjusting it throughout the reasoning process. Notably, the model can merge steps when appropriate. Right: DeepResearcher identifies the correct answer during its first tool call. However, rather than immediately committing to this result, it proceeds to verify its accuracy through subsequent steps. This rigorous behavior enhances the reliability of the model's responses, ensuring greater robustness in its final answers. Note: In the actual model, the tool call and tool response format is a JSON string, rather than a tag. However, for clarity and ease of presentation, we have used tags in the figure to represent the output.</p>
<h1>1 Introduction</h1>
<p>The emergence of Large Language Models (LLMs) has fundamentally transformed the landscape of artificial intelligence, enabling increasingly autonomous problem-solving capabilities. When equipped with external tools such as web search and code execution (Li et al., 2025c), these models can tackle complex research tasks that previously required significant human workload and expertise. Notable examples include Gemini and OpenAI Deep Research (Google, 2024; OpenAI, 2025), Grok3s DeeperSearch (xAI, 2025), and open-source projects like MetaGPT (Hong et al., 2024), OpenManus (Liang et al., 2025), and OWL agents (CAMEL-ALorg, 2025). These systems demonstrate promising capabilities in synthesizing information, writing and executing code, and conducting iterative investigations across diverse domains. Despite their potential, most current agents are prompt-engineered LLM agents that face significant limitations, while the technical details of commercial systems like OpenAI Deep Research remain completely opaque. Specifically, prompt-engineered agents follow pre-defined workflows designed by developers (Anthropic, 2024), resulting in strict behavioral patterns. Consequently, they often struggle with instruction following, consistent reasoning, exhibit poor generalization to novel tasks, and require extensive manual prompt engineering to achieve reliable performance (Pan et al., 2025). The brittle nature of these systems becomes particularly evident when confronted with complex, multi-step research scenarios requiring adaptive information gathering through web search. While impressive commercial products exist, reproducible frameworks for creating robust research agents remain elusive.</p>
<p>Recent advances suggest that reinforcement learning (RL) offers a promising path forward for improving LLM capabilities. Studies by Guo et al. (2025) and Team et al. (2025) demonstrate that scaling reinforcement learning for LLMs on math and coding tasks (Li et al., 2025b) substantially improves their reasoning abilities. For open-domain tasks, OpenAI has acknowledged using reinforcement learning techniques to enhance their Deep Research agent's capabilities, but detailed methodologies remain proprietary and undisclosed, creating a significant gap in open research. Current open-source efforts to integrate RL with information retrieval, such as Search-R1 (Jin et al., 2025), R1-Searcher (Song et al., 2025), and ReSearch (Chen et al., 2025), have primarily focused on RetrievalAugmented Generation (RAG) using static, local text corpora. While these approaches provide valuable insights, they fundamentally fail to capture the dynamic, unpredictable nature of real-world web search environments. The controlled RAG setting operates in a highly sanitized environment with a critical limiting assumption: that all necessary information already exists within their fixed knowledge base. This assumption breaks down in real-world scenarios where information might be absent, outdated, or require synthesis across domains not covered in the initial knowledge base. Beyond this fundamental limitation, RAG systems also fail to account for the substantial noise, variability in search quality, and the challenges of navigating diverse web content formats and structures.</p>
<p>In this work, we present the first comprehensive study of RL scaling for LLM agents operating with real-world web search capabilities. Our approach, DeepResearcher, trains agents to interact directly with live search engines, thereby learning to handle the inherent variability and complexity of the open web. By training in genuine web environments rather than controlled simulations, our system develops robust capabilities for handling the unpredictable nature of real-world information retrieval and synthesis.</p>
<p>DeepResearcher diverges significantly from prompt-based and RAG-based methods by applying several critical techniques absent from previous work:</p>
<ul>
<li>Scaling RL for Deep Research: Unlike prompt and SFT-based methods, we directly scale RL training for deep research with only outcome rewards.</li>
<li>Real-world Environment: Unlike controlled RAG environments, real web search presents noisy, unstructured, and heterogeneous information sources that require sophisticated filtering and relevance assessment capabilities.</li>
<li>End-to-end Training: We train the model end-to-end without human priors, enabling the agent to discover its own problem-solving strategies. This end-to-end approach significantly departs from human-designed workflows.</li>
<li>Addressing Implementation Challenges: Training with real web search introduces unique challenges absent in RAG settings, including managing search API rate limits, handling network latency, addressing anti-crawling mechanisms, and processing diverse webpage structures.</li>
<li>Multi-agent Framework: Our approach employs a specialized multi-agent architecture where dedicated browsing agents extract relevant information from entire webpages-a stark contrast to RAG-based systems that simply retrieve and present pre-processed text passages.</li>
</ul>
<p>Our results show that DeepResearcher achieves up to 28.9 points of improvement in research task completion compared to prompt-engineered agents. When compared to RAG-based RL agents, DeepResearcher demonstrates an improvement of up to 7.2 points. These findings suggest that direct interaction with real search environments is not merely an implementation detail but a crucial component for developing robust research capabilities in autonomous systems that can perform effectively in real-world applications.</p>
<p>Furthermore, our qualitative analysis revealed several important cognitive behaviors that emerge from DeepResearcher's end-to-end RL scaling. During problem-solving, DeepResearcher demonstrates abilities to make plans</p>
<p>initially, cross-validate answers from multiple sources, engage in reflection to redirect research, and maintain honesty when unable to find exact answers. These capabilities represent important characteristics for deep research agents and mirror skills valued in human researchers.</p>
<p>To conclude, we make the following contributions:</p>
<ul>
<li>We introduce DeepResearcher, a novel RL framework specifically designed for training LLM agents in real web environments, enabling iterative reasoning and search, and synthesizing diverse web information to answer open-domain questions.</li>
<li>We overcome numerous technical challenges inherent to RL scaling with real-world web search, including API rate limitations, webpage parsing variability, anti-crawling mechanisms, and network latency issues, making this the first successful implementation of reinforcement learning at scale in genuine web environments.</li>
<li>We conduct extensive experiments across open-domain tasks, demonstrating significant improvements over prompt-engineered baselines and RAG-based RL approaches.</li>
<li>We perform detailed analysis examining emergent behaviors from DeepResearcher's end-to-end RL scaling, finding that the system can formulate plans, cross-validate answers, reflect on its process, and maintain honesty about limitations.</li>
<li>We open-source our complete training framework to the research community, fostering transparency and enabling further advancements in deep research systems.</li>
</ul>
<h1>2 Related Work</h1>
<p>In this section, we review existing approaches to enhance large language models' (LLMs) ability to access external knowledge with search. We categorize these methods into prompt-based and training-based search agents. We also examine the environments in which these methods operate-either local retrieval-augmented generation (RAG) or real-world web search-and position our work in this landscape.</p>
<h3>2.1 Prompt-Based Search Agents</h3>
<p>Many current approaches rely on manually crafted workflows that specify how LLMs should interact with external knowledge sources (Wang et al., 2024a). Recent works such as OpenResearcher (Zheng et al., 2024), AirRAG (Feng et al., 2025), IterDRAG (Yue et al., 2024b), Plan*RAG (Verma et al., 2025), Search-o1 (Li et al., 2025a) and Open Deep Search (Alzubi et al., 2025) have demonstrated significant progress in search capabilities through carefully designed workflows. However, these methods face inherent limitations due to their reliance on human-engineered prompts and interaction patterns, resulting in strict behavior patterns that limit adaptability.</p>
<h3>2.2 Training-Based Search Agents</h3>
<p>Recent developments have moved beyond manually crafted prompts toward training-based approaches that enable more flexible and adaptive search behaviors.</p>
<p>Supervised Fine-Tuning (SFT) SFT for RAG have become an enhanced alternative to manual optimization of RAG workflows (Yu et al., 2024; Wang et al., 2024b). For example, CoRAG (Wang et al., 2024b) utilizes Monte Carlo Tree Search (MCTS) to dynamically select the best document blocks under budget constraints. However, it faces limitations including high computational overhead due to MCTS and weak generalization to unknown scenarios due to the dependence on supervised signals.</p>
<p>Reinforcement Learning (RL) End-to-end reinforcement learning offers a promising alternative that effectively unlocks LLMs' inherent capabilities. By late 2024, large language models achieved remarkable breakthroughs in reasoning capability enhancement through RL (Guo et al., 2025; OpenAI, 2024; Team et al., 2025). Recent research has explored applying RL to external knowledge retrieval, with systems such as Search-R1 (Jin et al., 2025), ReSearch (Chen et al., 2025), and R1-Searcher (Song et al., 2025) abandoning manually specified cues in favor of models that autonomously develop reasoning during the retrieval process. While OpenAI has acknowledged using RL techniques to enhance their research agent's capabilities, detailed methodologies remain proprietary and undisclosed, creating a significant gap in open research.</p>
<h3>2.3 Training Environments</h3>
<p>Training environments for search agents can be broadly categorized into two types:
Local RAG Environments Current mainstream local RAG frameworks (Gao et al., 2023; Yu et al., 2024) rely on pre-built fixed knowledge repositories, resulting in three critical issues: information timeliness decay, poor domain adaptability, and storage efficiency bottlenecks. While RAG-based RL approaches like Search-R1 (Jin et al., 2025), ReSearch (Chen et al., 2025), and R1-Searcher (Song et al., 2025) have made progress, their experimental</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The trajectory of a single sample from a batch of questions processed in parallel by a distributed cluster. Each question undergoes multiple independent rollouts with distinct memory. Upper-left: Displays the batch of questions and their concurrent rollout paths. Upper-right: Shows the browsing agent retrieving web pages via URLs, processing them sequentially to incrementally extract relevant information. Bottom: Details the iterative decision-making steps, from initial query formulation and search to snippet retrieval, further reasoning, browsing, information extraction, and answer generation.
validation remains primarily confined to predefined knowledge bases and similarity-based search, restricting the search space and potentially limiting generalizability to real-world applications.</p>
<p>Real-World Web Search Environments Web search-based methods (Schick et al., 2023; Qin et al., 2023) integrate open search engines with LLMs to access real-time information. These approaches face unique challenges including managing search API rate limits, handling network latency, and processing diverse webpage structures. Despite these challenges, real-world environments offer unstructured and heterogeneous information sources that better reflect the complexity of actual research tasks. However, search-based methods requiring external system participation are seldom trained end-to-end, with research often gravitating toward optimization through manually crafted workflows (Wang et al., 2024a).</p>
<p>In contrast to previous work, our approach uniquely combines reinforcement learning with training in genuine web environments. Unlike existing RL methods that primarily focus on static, local text corpora, our method trains agents to interact directly with live search engines. This enables them to handle the inherent variability and complexity of the open web, developing robust capabilities for real-world information retrieval and synthesis. Our approach addresses the limitations of both prompt-based and RAG-confined methods by learning adaptive search strategies through direct interaction with the unpredictable nature of web environments.</p>
<h1>3 Methodology</h1>
<p>In this section, we describe the methodology used to train an agent capable of solving problems with web search in dynamic real-world environments.</p>
<h3>3.1 Deep Research Trajectory</h3>
<p>In a DeepResearcher's trajectory, it conducts reasoning and tool selection based on the user question and observations iteratively as illustrated in Figure 3.</p>
<p>Reasoning We restrict DeepResearcher to do reasoning before taking action. Each reasoning process is wrapped in a <think> tag following the setting in DeepSeek-R1 (Guo et al., 2025).</p>
<p>Web Search Tool DeepResearcher invokes the web search tool by generating a JSON-formatted request with the tool name web_search and the search queries as arguments. Search results are returned in a structured format</p>
<p>comprising title, URL, and snippet for each webpage. The current implementation employs a fixed top-k (e.g., 10) value for search results retrieval. Future work could explore LLM-driven dynamic parameter optimization for enhanced search efficacy.</p>
<p>Web Browsing Agent The web browsing agent provides reliable, question-relevant, and incrementally updated information in to the DeepResearcher system. Specifically, the agent maintains a short-term memory repository for each query. Upon receiving a web_browse request, it processes the first-page segment of the URL in the request. Subsequently, the web browsing agent takes two actions based on the query, historical memory, and the newly acquired webpage content: (1) determining whether to continue reading the next URL/segment or stop and (2) appending relevant information to the short-term memory. Once the agent decides to discontinue further browsing, it compiles all newly added information from the short-term memory and returns it to the DeepResearcher system.</p>
<p>Answering When the model determines it has sufficient information to answer the question, it generates a final response within $&lt;$ answer $&gt;&lt;/$ answer $&gt;$ as the answer to return to the user.</p>
<h1>3.2 Addressing Challenges in Dynamic Real-World Web Environments</h1>
<p>In our open, real-world web setting, several unique challenges arise that necessitate specialized solutions. The following sections detail our strategies for managing these issues effectively.</p>
<p>Challenge I: High-concurrency requests at a single moment The implementation of GRPO results in a large number of sampling iterations, leading to a significant volume of search queries and webpage crawling operations (e.g., 4096), causing long delays. To resolve this issue, we created a distributed CPU server cluster with 50 nodes, specifically designed to manage the Tool requests generated during the RL rollout process. Each server is tasked with handling a portion of these requests, processing search results, and crawling webpages based on the URLs identified by the language model for further reading.</p>
<p>Challenge II: Managing Web Crawling and API Limitations During the crawling phase, the system frequently encounters anti-crawl measures deployed by web servers, which may return irrelevant content or fail to respond entirely. Similarly, when interfacing with search engines or LLM APIs, restrictions such as provider rate limits (e.g. 200 per second) can arise. To mitigate these issues, we implemented a robust retry mechanism that effectively addresses exceptions encountered during API calls or webpage crawling. In addition, we introduced a caching strategy for search results: if an identical search query is made within a predetermined period (e.g., 7 days), the system retrieves the results from the cache. This approach not only reduces the API call frequency but also helps manage the associated costs, particularly for expensive services like the Google Search API.</p>
<p>Challenge III: Optimizing Information Extraction via a Multi-Agent Approach We employ a multi-agent framework wherein a dedicated reading agent is tasked with extracting pertinent information from crawled webpages. Given that many webpages are lengthy and may contain limited relevant content, these pages are partitioned into smaller segments. The reading agent mimics human behavior by processing content sequentially from the first page onward. Under the assumption that if the initial segments of a URL predominantly contain irrelevant information, the webpage is likely unproductive and can be skipped, this method enables more efficient resource allocation and improves overall information extraction accuracy.</p>
<h3>3.3 RL Training Framework</h3>
<p>Our approach utilizes Reinforcement Learning (RL) to train the agent. This section outlines how we employ the RL framework to train the agent and the tools used within it.</p>
<p>GRPO In this work, we adopt the Group Relative Policy Optimization (GRPO) algorithm. GRPO optimizes the current policy $\pi_{\theta}$ by leveraging a reference policy $\pi_{\theta_{\text {ref }}}$ along with a set of rollouts generated by an existing policy $\pi_{\theta_{\text {old }}}$. Specifically, given $G$ rollouts</p>
<p>$$
\tau=\left{y_{i}\right}<em _old="{old" _theta__text="\theta_{\text">{i=1}^{G} \sim \pi</em>(\cdot \mid x)
$$}}</p>
<p>(with each input $x \sim D$, where $D$ is the experience distribution), GRPO estimates the baseline using these trajectories instead of training a separate critic. The current policy is then optimized by maximizing the following objective function:</p>
<p>$$
\begin{aligned}
\mathcal{J}(\theta) &amp; =\mathbb{E}<em i="i">{x \sim \mathcal{D},\left{y</em>\right}<em _old="{old" _theta__text="\theta_{\text">{i=1}^{G} \sim \pi</em> \
&amp; \frac{1}{G} \sum_{i=1}^{G}\left[\min \left(\frac{\pi_{\theta}\left(y_{i} \mid x\right)}{\pi_{\theta_{\text {old }}}\left(y_{i} \mid x\right)} A_{i}, \operatorname{clip}\left(\frac{\pi_{\theta}\left(y_{i} \mid x\right)}{\pi_{\theta_{\text {old }}}\left(y_{i} \mid x\right)}, 1-\epsilon, 1+\epsilon\right) A_{i}\right)-\beta \mathbb{D}}}}(\cdot \mid x)<em _theta="\theta">{\mathrm{KL}}\left(\pi</em>\right)\right]
\end{aligned}
$$} | \pi_{\theta_{\text {ref }}</p>
<p>Masking Observations The output of the tool is an observation, not the desired result that the model is expected to produce. Therefore, we apply masking to prevent the observation from being involved in training, allowing only the model's responses to contribute to the training process.</p>
<h1>3.4 Reward</h1>
<p>Rewards play a crucial role during the training process, guiding the agent to continuously improve its performance. This section defines the reward structure and describes how the agent's behavior is rewarded.</p>
<p>We employ F1 score as our primary reward metric due to our utilization of open-domain QA datasets with short-answer ground truth. For future work involving long-form answers, more sophisticated reward mechanisms may be necessary, as noted in the Deep Research system card (OpenAI, 2025). The reward is determined by the following conditions:</p>
<p>$$
\text { reward }=\left{\begin{array}{ll}
-1 &amp; \text { if format is incorrect } \
\text { F1 score } &amp; \text { if format is correct }
\end{array}\right.
$$</p>
<ul>
<li>Format Penalty: If the format is incorrect (e.g., missing tags or structural errors), the agent receives a penalty of -1 .</li>
<li>F1 Reward: If the format is correct, the reward is based on the word-level F1 score, which measures the accuracy of the generated answer compared to the reference answer. A higher F1 score results in a higher reward.</li>
</ul>
<h2>4 Beyond Memorization: Curating Search-Dependent Training Data</h2>
<h3>4.1 Leveraging Open Domain QA Data</h3>
<p>Despite the growing interest in deep research capabilities for LLM agents, there currently exists no open-source training dataset specifically designed for this purpose. To address this gap, we leverage existing open-domain question-answering datasets, which contain single-hop to multi-hop questions that inherently require online search to find accurate answers.</p>
<p>Our training corpus comprises a diverse collection of QA datasets that require varying degrees of retrieval complexity. Specifically, we utilize NaturalQuestions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (TQ) (Joshi et al., 2017) for single-hop scenarios, where answers can typically be found within a single web document. For more complex multi-hop scenarios, which require integrating information across multiple sources, we incorporate examples from HotpotQA (Yang et al., 2018) and 2WikiMultiHopQA (2Wiki) (Ho et al., 2020), both of which were specifically designed to evaluate multi-step reasoning capabilities.</p>
<h3>4.2 The Issue of Data Contamination</h3>
<p>For training models that genuinely learn to leverage web search tools-rather than simply recalling memorized information-it is critical to address the problem of data contamination. Large language models have been pretrained on vast internet corpora, which likely include many of the QA pairs in standard benchmarks. Without proper contamination detection, the model might appear to successfully complete research tasks while actually using its parametric knowledge, defeating the purpose of learning web search strategies.</p>
<p>This contamination issue is particularly problematic in the context of our work, as it could lead to:</p>
<ul>
<li>Models that falsely appear to benefit from web search when actually using memorized knowledge.</li>
<li>Failure to develop genuine search strategies when deployed on truly novel questions.</li>
<li>Inability to generalize to real-world research scenarios where answers cannot be found in the model's training data.</li>
</ul>
<h3>4.3 Data Cleaning and Contamination Detection</h3>
<p>To ensure the integrity of our training process, we implemented a comprehensive two-stage filtering methodology:
Low-Quality Question Filtering We exclude questions that could yield unreliable or problematic search results. Specifically, we eliminate: 1) Time-sensitive questions (e.g., "Who is the current CEO of Apple?"); 2) Highly subjective queries (e.g., "What is the best smartphone?"); and 3) Potentially harmful or policy-violating content. This filtering was implemented using DeepSeek-R1 (Guo et al., 2025) with a carefully designed evaluation prompt to systematically identify and mark problematic questions.</p>
<p>Contamination Detection To ensure the model genuinely learns to use search tools rather than memorizing answers, we employed a robust contamination detection procedure. For each candidate question, we randomly sample 10 responses from the base model we will use in training, and check if any response contains the ground truth answer (i.e., pass@10). Questions where the model demonstrated prior knowledge (by producing the correct answer without search) were excluded from the training set. This contamination screening is critical for preventing the model from developing a false reliance on parametric knowledge when search-based knowledge is required.</p>
<p>The prompts used for data cleaning and contamination detection are listed in Appendix A.1. After applying these quality control measures, we constructed a final training dataset of 80,000 examples with a distribution ratio of 1:1:3:3 for NQ:TQ:HotpotQA:2Wiki. This proportion deliberately emphasizes multi-hop scenarios ( $75 \%$ of examples), as these better reflect the complex information-seeking behaviors required for deep research questions.</p>
<h1>5 Experiments</h1>
<h3>5.1 Experimental Setups</h3>
<h3>5.1.1 Model and Hyperparameters</h3>
<p>We adopt Qwen2.5-7B-Instruct ${ }^{1}$ (Qwen et al., 2025) as the backbone model for our training pipeline. The training is conducted using the verl framework ${ }^{2}$. At each training step, we sample 256 prompts, and sample 16 rollouts for each prompt. Each rollout consists of up to 10 tool calls followed by a final answer step. The training is performed with a mini-batch size of 4,096 , which means one rollout stage will backprop for one time.</p>
<h3>5.2 Evaluation and Results</h3>
<h3>5.2.1 Benchmarks</h3>
<p>To thoroughly evaluate model performance across both in-domain (ID) and out-of-domain (OOD) settings, we construct a diverse benchmark suite spanning a range of open-domain QA challenges. For in-domain evaluation, we include the dev sets of NQ (Kwiatkowski et al., 2019), TQ (Joshi et al., 2017), HotpotQA (Yang et al., 2018), and 2Wiki (Ho et al., 2020) as mentioned in Section 4.</p>
<p>For out-of-domain evaluation, we introduce three datasets that differ significantly in question style and information distribution: MuSiQue (Trivedi et al., 2022), Bamboogle (Press et al., 2022), and PopQA (Mallen et al., 2022). These datasets test the model's generalization ability beyond the training domain.</p>
<p>To ensure a fair and balanced evaluation, we randomly sample 512 examples from the development sets of NQ, TQ, HotpotQA, 2Wiki, MuSiQue, and PopQA as well as all 125 samples from Bamboogle's development set. This sampling strategy allows us to assess model robustness across a broad range of topics and reasoning requirements.</p>
<h3>5.2.2 Baselines</h3>
<p>To evaluate the effectiveness of DeepResearcher, we compare it against the following baseline methods:</p>
<ul>
<li>CoT Only: This baseline employs Chain-of-Thought (CoT) ( ) reasoning to generate answers without access to any external reference context.</li>
<li>RAG: This approach combines Chain-of-Thought reasoning with retrieved reference context to guide the answer generation process.</li>
<li>Search-o1: A multi-step reasoning baseline in which the model generates search queries or intermediate answers. For each query, the context is limited to a snippet retrieved by a retriever, rather than full documents. ${ }^{3}$</li>
<li>Search-o1 + Web Search: In contrast to Search-o1, this setting lets the model access the open web. It can send real-time search queries through APIs like Serper and visit URLs to browse webpages. This supports richer, more dynamic information gathering and forms the basis of deep research.</li>
<li>Search-r1: A reinforcement learning method for question answering. During the training and inference stages, it searches Wikipedia information with the help of a retriever. There are two versions: Search-r1-base and Search-r1-instruct, where the initial actor model is either the base model or the instruct model.</li>
<li>R1-Searcher: Unlike Search-r1, when given a search query, it appends "site:en.wikipedia.org" to the query, search it via Bing, and summarizes the first three pages of the search results. DeepResearcher differs from this approach in three key aspects: (1) DeepResearcher is also trained with real-world environment; (2) DeepResearcher does not restrict the search space to a specific domain, such as Wikipedia; and (3) Our</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Inference <br> Environment</th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TQ</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">HotpotQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">2Wiki</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">MBE</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">MBE</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">MBE</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">MBE</td>
</tr>
<tr>
<td style="text-align: center;">Prompt Based</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">Local RAG</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">27.3</td>
</tr>
<tr>
<td style="text-align: center;">CoT + RAG</td>
<td style="text-align: center;">Local RAG</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">24.8</td>
</tr>
<tr>
<td style="text-align: center;">Search-o1*</td>
<td style="text-align: center;">Local RAG</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">32.8</td>
</tr>
<tr>
<td style="text-align: center;">Search-o1</td>
<td style="text-align: center;">Web Search</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">37.7</td>
</tr>
<tr>
<td style="text-align: center;">Training Based</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Search-r1-base</td>
<td style="text-align: center;">Local RAG</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">47.9</td>
</tr>
<tr>
<td style="text-align: center;">Search-r1-instruct</td>
<td style="text-align: center;">Local RAG</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">48.8</td>
</tr>
<tr>
<td style="text-align: center;">R1-Searcher</td>
<td style="text-align: center;">Web Search</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">65.8</td>
</tr>
<tr>
<td style="text-align: center;">DeepResearcher</td>
<td style="text-align: center;">Web Search</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">66.6</td>
</tr>
</tbody>
</table>
<p>Table 1: In-domain results on four datasets (NQ, TQ, HotpotQA, 2Wiki), evaluated by F1 and MBE metrics. DeepResearcher outperforms all baseline methods in MBE and shows competitive performance in F1, particularly excelling on TQ and 2Wiki. It is worth noting that Search-r1-base was trained and evaluated in a local RAG environment with direct access to the relevant Wikipedia corpus, while DeepResearcher must navigate the entire Internet to find information, achieving excellent results despite facing a more realistic and challenging scenario.
method allows the model to autonomously select URLs rather than compulsorily summarizing the top three search results.</p>
<h1>5.2.3 Evaluation Metrics</h1>
<p>Rule-based Metrics We evaluate model performance using F1 score aligning with the reward for training. Both ground-truth and predicted answers are normalized by converting to lowercase and removing all punctuation before computing the metrics.</p>
<p>Model-based Evaluation Rule-based evaluation doesn't suit long-form responses, so we adopt a model-based evaluation (MBE) approach using LLM-as-a-Judge (Zheng et al., 2023). Specifically, we prompt GPT-4o-mini (Hurst et al., 2024) to assess the model's answer against the question and ground truth answer, and label it as either "correct" or "incorrect." The MBE score is then computed as the accuracy of these judgments.(Zheng et al., 2023) The full prompt is provided in Appendix A.3.</p>
<h3>5.2.4 Main Results</h3>
<p>Table 1 and Table 2 present the main results of DeepResearcher and the baselines in-domain and out-of-domain, respectively. From these results, we draw the following observations:</p>
<p>DeepResearcher outperforms the baselines within training domains. As shown in Table 1, DeepResearcher achieves the highest performance across the four datasets when measured by the more reliable MBE metric, outperforming baselines by a substantial margin on TQ and 2Wiki. While Search-r1-base shows comparable MBE results on NQ and HotpotQA, it's important to note that Search-r1-base was specifically trained and evaluated using a local RAG system with direct access to the relevant Wikipedia corpus. In contrast, DeepResearcher must navigate the entire Internet to find relevant information, representing a more realistic and significantly more challenging scenario even though the answers ultimately come from Wikipedia.</p>
<p>DeepResearcher demonstrates exceptional generalization to novel domains. As revealed in Table 2, DeepResearcher consistently outperforms all other baselines across three OOD datasets. This indicates that the model successfully learns generalizable skills for reasoning, searching, and synthesizing information from different sources through RL scaling, rather than merely adapting to specific training distributions.</p>
<p>Importance of Real-World Environment in Training Questions in Bamboogle specifically require knowledge beyond Wikipedia's coverage. Consequently, DeepResearcher significantly outperforms local RAG-based methods on this benchmark. Furthermore, even when we enable R1-Searcher (which was trained using local RAG) to search the real-world web, it still performs substantially worse than DeepResearcher. These results demonstrate the critical advantage of using real-world environments during RL scaling training, as this exposure develops robust information retrieval and synthesis capabilities that cannot be achieved in controlled, static environments.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Inference</th>
<th style="text-align: center;">Musique</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Bamboogle</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PopQA</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Environment</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">MBE</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">MBE</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">MBE</td>
</tr>
<tr>
<td style="text-align: center;">Prompt Based</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">Local RAG</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">15.0</td>
</tr>
<tr>
<td style="text-align: center;">CoT + RAG</td>
<td style="text-align: center;">Local RAG</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">48.8</td>
</tr>
<tr>
<td style="text-align: center;">Search-o1*</td>
<td style="text-align: center;">Local RAG</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">42.4</td>
</tr>
<tr>
<td style="text-align: center;">Search-o1</td>
<td style="text-align: center;">Web Search</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">43.4</td>
</tr>
<tr>
<td style="text-align: center;">Training Based</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Search-r1-base</td>
<td style="text-align: center;">Local RAG</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">47.0</td>
</tr>
<tr>
<td style="text-align: center;">Search-r1-instruct</td>
<td style="text-align: center;">Local RAG</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">44.5</td>
</tr>
<tr>
<td style="text-align: center;">R1-Searcher</td>
<td style="text-align: center;">Web Search</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">43.4</td>
</tr>
<tr>
<td style="text-align: center;">DeepResearcher</td>
<td style="text-align: center;">Web Search</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">52.7</td>
</tr>
</tbody>
</table>
<p>Table 2: This table shows the performance of different methods on three out-of-domain datasets (Musique, Bamboogle, PopQA), evaluated by F1 and MBE metrics. DeepResearcher leads in both F1 and MBE on all datasets, demonstrating strong generalization capabilities compared to other methods. Notably, unlike the other datasets, Bamboogle's corpus is not entirely derived from Wikipedia pages.</p>
<h1>6 Analysis</h1>
<h3>6.1 Training Dynamics</h3>
<ul>
<li>Performance gradually scaling with reinforcement learning: Figure 4 (a) present the evaluation of F1 scores, across different training steps. The F1 score 0.375 , and gradually increases to around 0.55 demonstrating a consistent upward trend. This result indicates the progressive improvement of the model's performance in reinforcement learning.</li>
<li>Training leads to increased reasoning steps in hard question: Figure 4 (b) illustrates the average number of turns required for different reasoning hops. The general trend indicates that as the training progresses, the required number of tool calls also increases across different difficulty levels. Unlike the other three settings, the 4-hop setting continues to exhibit an increasing trend even after 34 steps. This suggests that the model is still learning to retrieve more information when dealing with more difficult questions.</li>
<li>Continuous learning makes long response without saturation: Figure 4 (c) presents the length of responses for different reasoning hops. The response lengths also increase with reasoning complexity. However, all four settings show a sustained upward trend, indicating that the model continues to expand its reasoning processes during training. This further supports the idea that the model adapts to increasingly complex queries by generating more detailed outputs like double-check, refinement, planning, etc.
<img alt="img-3.jpeg" src="img-3.jpeg" /></li>
</ul>
<p>Figure 4: Training dynamics of F1, turns and response length. In this figure, we find the performance gradually scaling with reinforcement learning. The numbers of tool calls and responses also increase.</p>
<h3>6.2 Case Study</h3>
<p>Figures 2 and 6 present four cases illustrating the model's behavior after reinforcement learning. From these examples, we identify several key behavioral patterns:</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Planning and Cross Validation Behaviors. Left: DeepResearcher is capable of making plans and dynamically adjusting it throughout the reasoning process. Notably, the model can merge steps when appropriate. Right: DeepResearcher identifies the correct answer during its first tool call. However, rather than immediately committing to this result, it proceeds to verify its accuracy through subsequent steps. This rigorous behavior enhances the reliability of the model's responses, ensuring greater robustness in its final answers. Note: In the actual model, the tool call and tool response format is a JSON string, rather than a tag. However, for clarity and ease of presentation, we have used tags in the figure to represent the output.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Reflection <br> Question: Where was the place of burial of Djedefhor's father? <br> Ground Truth: Great Pyramid of Giza</th>
<th style="text-align: center;">Honesty <br> Question: In the seventh-largest country in Africa, what is the oil prodcution of the city at the mouth of the Congo River? Ground Truth: 1200000 oilbbl/ol</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Response Trajectory from DeepResearcher: <br> <think> <br> To answer this question, I need to find out information about Djedefhor's father and the place of his burial. I will start with a web search. <br> </think> <br> <search> <br> Herschell Gordon Lewis filmmaker pioneer <br> Leah Meyerhoff filmmaker pioneer <br> place of burial of Djedefhor's father <br> Djedefhor father burial site <br> who was Djedefhor's father and where was he buried <br> </search> <br> <tool_response> <br> ...{omitted to save space} <br> </tool_response> <br> <think> <br> The web search results provide clues about Djedefhor's father. However, the search results are mostly about Boromir's father Denethor from the Lord of the Rings, not Djedefhor's father. I need to refine my search to find information about Djedefhor specifically. <br> </think> <br> <search> <br> who was Djedefhor's father and where was he buried <br> Djedefhor father name <br> Djedefhor father identity <br> </search> <br> <tool_response> <br> ...{omitted to save space} <br> </tool_response> <br> <think> <br> The new search results provide information about Djedefhor's father, Khufu, and also his burial place. However, the search results for his burial need to be more specific. I will use the web browser tool to look at the pages where the information may be more relevant. <br> </think> <br> <browse_webpage> <br> https://en.wikipedia.org/wiki/Djedefre <br> https://www.ancient-egypt.org/history/old-kingdom/4th- <br> dynasty/djedefre/biography-of-djedefre.html <br> https://ancientegyptonline.co.uk/djedefre/ <br> </browse_webpage> <br> <tool_response> <br> ...{omitted to save space} <br> </tool_response> <br> <think> <br> From the browsing pages, it is evident that Djedefhor's father is Khufu. The pages specifically mention that Khufu was the father of Djedefhor. However, I need to find the place of burial of Djedefhor's father, Khufu. </think> <br> <search> <br> place of burial of Khufu <br> Khufu burial site <br> where was Khufu buried <br> </search> <br> <tool_response> <br> ...{omitted to save space} <br> </tool_response> <br> <think> <br> The search results clearly indicate that Khufu's burial place was in the Great Pyramid of Giza. This pyramid served as the tomb for pharaoh Khufu. </think> <br> <answer> <br> Great Pyramid of Giza <br> </answer></td>
<td style="text-align: center;">Response Trajectory from DeepResearcher: <br> <think> <br> To answer this question, I need to identify the city at the mouth of the Congo River in the seventh-largest country in Africa, and then find its current oil production. </think> <br> <search> <br> seventh largest country in africa <br> city at the mouth of the Congo River in the seventh largest country in africa <br> </search> <br> <tool_response> <br> ...{omitted to save space} <br> </tool_response> <br> <think> <br> From the search results, I found that Angola is the seventh largest country in Africa, and its capital Luanda is located near the mouth of the Congo River. Now I need to find the oil production of Luanda. </think> <br> <search> <br> oil production of Luanda <br> oil production in Angola <br> </search> <br> <tool_response> <br> ...{omitted to save space} <br> </tool_response> <br> <think> <br> After a series of web searches, I couldn't find specific information about the oil production of Luanda. The results are mostly about Angola's total oil production. Since Luanda is the capital and the largest city of Angola, it's likely that a significant portion of Angola's oil production happens there. For a more exact figure, we might need to explore specialized databases or recent reports about the oil industry in Luanda. </think> <br> <browse_webpage> <br> https://en.wikipedia.org/wiki/Luanda <br> https://www.worldometers.info/country-data/angola/oil-production <br> </browse_webpage> <br> <think> <br> It seems the information about the oil production of Luanda is not directly available in the search results or in the previously browsed webpages. Given the information from previous searches, Angola is a major oil producer, and as the capital and largest city, Luanda likely plays a key role in the country's oil production. However, to answer the question precisely, we may need a more detailed report on the city's oil production, which might be included in specialized databases or recent reports. Since the exact oil production of Luanda is not available from the searches and browsing, and given the context and the previously found information that Angola is a major oil producer, it's reasonable to infer that the oil production in the city at the mouth of the Congo River (Luanda) is a significant part of Angola's total oil production. However, without a precise figure, I will not provide a number. </think> <br> <answer> <br> significant portion <br> </answer> <br> Be honest when didn't <br> find the exact answer</td>
</tr>
</tbody>
</table>
<p>Figure 6: Reflection and Honesty Behavior. The search and browse are 2 apis in json format in the real inference stage. Left: When the retrieved information does not fully align with the question, DeepResearcher recognizes this discrepancy based on environmental feedback and refines its search query in subsequent tool calls. This proves its reflection ability. Right: DeepResearcher is capable of recognizing when it has not found the correct answer and appropriately declines to provide a response to be honesty. Note: In the actual model, the tool call and tool response format is a JSON string, rather than a tag. However, for clarity and ease of presentation, we have used tags in the figure to represent the output.</p>
<ul>
<li>Behavior I: Planning when addressing multi-hop questions: As demonstrated on the left side of Figure 2, DeepResearcher is capable of making plans and dynamically adjusting it throughout the reasoning process. Notably, the model can merge steps when appropriate, indicating that planning abilities can emerge naturally without the necessity of SFT on explicit planning data (Yue et al., 2024a).</li>
<li>Behavior II: Cross-validation before finalizing its answers: As observed on the right side of Figure 2, DeepResearcher identifies the correct answer during its first tool call. However, rather than immediately committing to this result, it proceeds to verify its accuracy through subsequent steps. This cautious approach enhances the reliability of the model's responses, ensuring greater robustness in its final predictions.</li>
<li>Behavior III: Reflection when observations deviate from expectations: The left side of Figure 6 illustrates the model's ability to reflect on its search process. When the retrieved information does not fully align with the question, DeepResearcher recognizes this discrepancy based on environmental feedback and refines its search query in subsequent tool calls. This reflective capability is essential for preventing the model from getting stuck (Fu et al., 2025) in reasoning, enabling it to enhance overall problem-solving efficiency.</li>
<li>Behavior IV: Honesty by acknowledging its limitations: A reliable model should minimize hallucinations and provide honest responses when it lacks the necessary knowledge (Yang et al., 2024). We observe that DeepResearcher is capable of recognizing when it has not found the correct answer and appropriately declines to provide a response. This behavior is beneficial, however, current question-answering evaluation metrics do not yet account for this aspect of model reliability.</li>
</ul>
<h1>7 Conclusion</h1>
<p>In conclusion, we presents DeepResearcher, a groundbreaking approach for scaling reinforcement learning in LLMs to operate effectively in real-world web search environments. Unlike existing methods that rely on static knowledge bases or controlled retrieval settings, DeepResearcher trains agents to interact directly with live search engines, allowing them to navigate the inherent complexity and variability of the open web. This direct engagement with dynamic search environments leads to substantial improvements in task completion and research capabilities compared to both prompt-engineered and RAG-based RL agents.</p>
<p>By adopting an end-to-end training framework, DeepResearcher moves beyond human-engineered workflows, empowering the agent to autonomously develop problem-solving strategies. Our approach not only addresses the unique challenges of real-world web search, such as network latency and anti-crawling mechanisms, but also provides a robust multi-agent architecture that enhances the agent's ability to collect diverse information from the web. The resulting system demonstrates notable cognitive behaviors such as planning, cross-validation, reflection, and maintaining honesty, which are crucial for autonomous agents conducting deep research.</p>
<p>The success of DeepResearcher marks a significant milestone in the evolution of LLM agents, showing that scaling reinforcement learning in real-world environments can unlock substantial improvements in research performance. This approach offers a promising path forward for building more adaptive, intelligent systems capable of solving complex, open-domain problems that are relevant to real-world applications.</p>
<h2>References</h2>
<p>[1] Salaheddin Alzubi, Creston Brooks, Purva Chiniya, Edoardo Contente, Chiara von Gerlach, Lucas Irwin, Yihan Jiang, Arda Kaz, Windsor Nguyen, Sewoong Oh, et al. 2025. Open deep search: Democratizing search with open-source reasoning agents. arXiv preprint arXiv:2503.20201.
[2] Anthropic. 2024. Building effective agents.
[3] CAMEL-AI.org. 2025. Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation. https://github.com/camel-ai/owl. Accessed: 2025-03-07.
[4] Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. 2025. Research: Learning to reason with search for llms via reinforcement learning.
[5] Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Jingyi Song, and Hao Wang. 2025. Airrag: Activating intrinsic reasoning for retrieval augmented generation via tree-based search. arXiv preprint arXiv:2501.10053.
[6] Dayuan Fu, Keqing He, Yejie Wang, Wentao Hong, Zhuoma Gongque, Weihao Zeng, Wei Wang, Jingang Wang, Xunliang Cai, and Weiran Xu. 2025. Agentrefine: Enhancing agent generalization through refinement tuning. arXiv preprint arXiv:2501.01702.</p>
<p>[7] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2.
[8] Google. 2024. Gemini deep research.
[9] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948.
[10] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing a multihop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pages 6609-6625, Barcelona, Spain (Online). International Committee on Computational Linguistics.
[11] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jrgen Schmidhuber. 2024. MetaGPT: Meta programming for a multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations.
[12] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276.
[13] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516.
[14] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601-1611, Vancouver, Canada. Association for Computational Linguistics.
[15] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, $7: 452-466$.
[16] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025a. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366.
[17] Xuefeng Li, Haoyang Zou, and Pengfei Liu. 2025b. Limr: Less is more for rl scaling.
[18] Xuefeng Li, Haoyang Zou, and Pengfei Liu. 2025c. Torl: Scaling tool-integrated rl.
[19] Xinbin Liang, Jinyu Xiang, Zhaoyang Yu, Jiayi Zhang, and Sirui Hong. 2025. Openmanus: An open-source framework for building general ai agents. https://github.com/mannaandpoem/OpenManus.
[20] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. 2022. When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. arXiv preprint.
[21] OpenAI. 2024. Learning to reason with llms, september 2024.
[22] OpenAI. 2025. Deep research system card. Technical report, OpenAI.
[23] Melissa Z Pan, Mert Cemri, Lakshya A Agrawal, Shuyi Yang, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Kannan Ramchandran, Dan Klein, Joseph E. Gonzalez, Matei Zaharia, and Ion Stoica. 2025. Why do multiagent systems fail? In ICLR 2025 Workshop on Building Trust in Language Models and Applications.
[24] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350.
[25] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789.</p>
<p>[26] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report.
[27] Timo Schick, Jane Dwivedi-Yu, Roberto Dess, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:68539-68551.
[28] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. 2025. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592.
[29] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. 2025. Kimi k1.5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599.
[30] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics.
[31] Prakhar Verma, Sukruta Prakash Midigeshi, Gaurav Sinha, Arno Solin, Nagarajan Natarajan, and Amit Sharma. 2025. Plan*rag: Efficient test-time planning for retrieval augmented generation.
[32] Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, et al. 2024a. Searching for best practices in retrieval-augmented generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 17716-17736.
[33] Ziting Wang, Haitao Yuan, Wei Dong, Gao Cong, and Feifei Li. 2024b. Corag: A cost-constrained retrieval optimization system for retrieval-augmented generation. arXiv preprint arXiv:2411.00744.
[34] xAI. 2025. Grok 3.
[35] Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. 2024. Alignment for honesty. Advances in Neural Information Processing Systems, 37:63565-63598.
[36] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP).
[37] Tian Yu, Shaolei Zhang, and Yang Feng. 2024. Auto-rag: Autonomous retrieval-augmented generation for large language models. arXiv preprint arXiv:2411.19443.
[38] Murong Yue, Wenlin Yao, Haitao Mi, Dian Yu, Ziyu Yao, and Dong Yu. 2024a. Dots: Learning to reason dynamically in llms via optimal reasoning trajectories search. arXiv preprint arXiv:2410.03864.
[39] Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, and Michael Bendersky. 2024b. Inference scaling for long-context retrieval augmented generation. arXiv preprint arXiv:2410.04343.
[40] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December $10-16,2023$.
[41] Yuxiang Zheng, Shichao Sun, Lin Qiu, Dongyu Ru, Cheng Jiayang, Xuefeng Li, Jifan Lin, Binjie Wang, Yun Luo, Renjie Pan, Yang Xu, Qingkai Min, Zizhao Zhang, Yiwen Wang, Wenjie Li, and Pengfei Liu. 2024. OpenResearcher: Unleashing AI for accelerated scientific research. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 209-218, Miami, Florida, USA. Association for Computational Linguistics.</p>
<h1>A Prompts</h1>
<h2>A. 1 Prompt for Question Quality Level Evaluation</h2>
<p>The prompt below displays two templates. Identifies if questions are time-sensitive, subjective, or potentially harmful. Includes classification guidelines, question placeholder, and required answer tag format.</p>
<h2>Prompt for training data quality checking</h2>
<p>Please identify whether the given question is time-sensitive, subjective, or may cause harmful answers.</p>
<ul>
<li>Time-sensitive: The answer to the question may change over time.</li>
<li>Harmful: The answer to the question may be harmful or offensive.</li>
<li>Subjective: The answer to the question may be subjective and not based on facts.</li>
</ul>
<p>Here is the question:
$&lt;$ question $&gt;$
{question}
$&lt;/$ question $&gt;$
Wrap your answer in $&lt;$ answer $&gt;$ tags with one of the following values:</p>
<ul>
<li>time_sensitive: if the question is time-sensitive</li>
<li>harmful: if the question may cause harmful answers</li>
<li>subjective: if the question is subjective</li>
<li>good: if the question is none of the above</li>
</ul>
<p>The prompt below shows the template prompt for contamination detection. To tests if AI responses are influenced by training data contamination.</p>
<h2>Prompt for contamination detection</h2>
<p>Give a short answer to the following question. The answer should be in English.
Question: {question}
Your answer:</p>
<h2>A. 2 Prompt for Model's Answer Quality Level Evaluation</h2>
<p>The prompt below provides instructions for evaluating the correctness of AI-generated answers (pred answer) against a list of ground truth answers. To judge if a predicted answer correctly answers a question by comparing it to ground truth answers.</p>
<h1>Prompt for Model-based Evaluation</h1>
<p>You will be given a question and its ground truth answer list where each item can be a ground truth answer. Provided a pred_answer, you need to judge if the pred_answer correctly answers the question based on the ground truth answer list.
You should first give your rationale for the judgement, and then give your judgement result (i.e., correct or incorrect).</p>
<p>Here is the criteria for the judgement:</p>
<ol>
<li>The pred_answer doesn't need to be exactly the same as any of the ground truth answers, but should be semantically same for the question.</li>
<li>Each item in the ground truth answer list can be viewed as a ground truth answer for the question, and the pred_answer should be semantically same to at least one of them.</li>
</ol>
<div class="codehilite"><pre><span></span><code>question: {question}
ground truth answers: {gt_answer}
pred_answer: {pred_answer}
</code></pre></div>

<p>The output should in the following json format:
""json
{
"rationale": "your rationale for the judgement, as a text",
"judgement": "your judgement result, can only be 'correct' or 'incorrect'"
}</p>
<p>Your output:</p>
<h2>A. 3 Prompt for Research Plan on Question Answering</h2>
<p>The prompt below outlines the structured approach for addressing complex questions, utilizing web search and webpage browsing tools to conduct in-depth research and gather the necessary information for a comprehensive response.</p>
<h1>Prompt for Research Plan on Complex Question Answering</h1>
<p>## Background information</p>
<ul>
<li>Today is YYYY-MM-DD</li>
<li>You are Deep AI Research Assistant</li>
</ul>
<p>The question I give you is a complex question that requires a <em>deep research</em> to answer.
I will provide you with two tools to help you answer the question:</p>
<ul>
<li>A web search tool to help you perform google search.</li>
<li>A webpage browsing tool to help you get new page content.</li>
</ul>
<p>You don't have to answer the question now, but you should first think about the research plan or what to search next.</p>
<p>Your output format should be one of the following two formats:</p>
<div class="codehilite"><pre><span></span><code><span class="nt">&lt;think&gt;</span>
YOUR<span class="w"> </span>THINKING<span class="w"> </span>PROCESS
<span class="nt">&lt;/think&gt;</span>
<span class="nt">&lt;answer&gt;</span>
YOUR<span class="w"> </span>ANSWER<span class="w"> </span>AFTER<span class="w"> </span>GETTING<span class="w"> </span>ENOUGH<span class="w"> </span>INFORMATION
<span class="nt">&lt;/answer&gt;</span>
or
<span class="nt">&lt;think&gt;</span>
YOUR<span class="w"> </span>THINKING<span class="w"> </span>PROCESS
<span class="nt">&lt;/think&gt;</span>
<span class="nt">&lt;tool_call&gt;</span>
YOUR<span class="w"> </span>TOOL<span class="w"> </span>CALL<span class="w"> </span>WITH<span class="w"> </span>CORRECT<span class="w"> </span>FORMAT
<span class="nt">&lt;/tool_call&gt;</span>
</code></pre></div>

<p>You should always follow the above two formats strictly.
Only output the final answer (in words, numbers or phrase) inside the <answer></answer> tag, without any explanations or extra information. If this is a yes-or-no question, you should only answer yes or no.</p>
<h2># Tools</h2>
<p>You may call one or more functions to assist with the user query.
You are provided with function signatures within <tools></tools> XML tags:</p>
<div class="codehilite"><pre><span></span><code><span class="o">&lt;</span><span class="n">tools</span><span class="o">&gt;</span>
<span class="p">{</span><span class="s1">&#39;type&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;function&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;function&#39;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;web_search&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;description&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;Search the web for relevant</span>
</code></pre></div>

<p>information from google. You should use this tool if the historical page content is not enough to answer the question. Or last search result is not relevant to the question.', 'parameters': {'type': 'object', 'properties': {'query': {'type': 'array', 'items': {'type': 'string', 'description': 'The query to search, which helps answer the question'}, 'description': 'The queries to search'}}, 'required': ['query'], 'minItems': 1, 'uniqueItems': true}}}
{'type': 'function', 'function': {'name': 'browse_webpage', 'description': 'Browse the webpage and return the content that not appeared in the conversation history. You should use this tool if the last action is search and the search result maybe relevant to the question.', 'parameters': {'type': 'object', 'properties': {'url_list': {'type': 'array', 'items': {'type': 'string', 'description': 'The chosen url from the search result, do not use url that not appeared in the search result'}, 'description': 'The chosen urls from the search result.'}}, 'required': ['url_list']}}}
</tools>
For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:
<tool_call>
"name": <function-name>, "arguments": <args-json-object>
</tool_call></p>
<h1>B Training Scaling Result</h1>
<p>Figure 7 presents the F1 score in 7 benchmarks. We sampled 125 cases from each benchmarks' development set. DeepResearcher can scale in all benchmarks, especially in OOD benchmarks.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: F1 score during training</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
${ }^{2}$ https://github.com/volcengine/verl
${ }^{3}$ To ensure consistency with other results, we reimplemented search-o1 using our own prompt.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
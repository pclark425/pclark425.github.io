<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1938 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1938</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1938</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-278782301</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.15685v1.pdf" target="_blank">From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems</a></p>
                <p><strong>Paper Abstract:</strong> Foundation models (FMs) are increasingly used to bridge language and action in embodied agents, yet the operational characteristics of different FM integration strategies remain under-explored -- particularly for complex instruction following and versatile action generation in changing environments. This paper examines three paradigms for building robotic systems: end-to-end vision-language-action (VLA) models that implicitly integrate perception and planning, and modular pipelines incorporating either vision-language models (VLMs) or multimodal large language models (LLMs). We evaluate these paradigms through two focused case studies: a complex instruction grounding task assessing fine-grained instruction understanding and cross-modal disambiguation, and an object manipulation task targeting skill transfer via VLA finetuning. Our experiments in zero-shot and few-shot settings reveal trade-offs in generalization and data efficiency. By exploring performance limits, we distill design implications for developing language-driven physical agents and outline emerging challenges and opportunities for FM-powered robotics in real-world conditions.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1938.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1938.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NORA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NORA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small open-sourced generalist Vision-Language-Action (VLA) model evaluated and fine-tuned in this paper for tabletop manipulation and long-horizon tasks; shown to achieve top success rates after fine-tuning and benefits from action-chunking pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NORA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A generalist VLA model that tightly couples visual inputs and language instructions to generate actions; described as relatively small and open-source, used with both autoregressive and action-chunking variants (NORA, NORA-LONG), and evaluated in partial and full fine-tuning regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language-action pretraining on robot demonstration datasets</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretrained on large robot demonstration corpora (cites Open-X-Embodiment / related datasets and fine-tuning on LIBERO), containing paired visual observations, language instructions, and action trajectories (temporal motor sequences) — likely includes object descriptions, spatial relationships and manipulation trajectories/affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation (tabletop pick-and-place, spatial-relation tasks, multi-object manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on out-of-distribution (OOD) object manipulation, spatial relationship reasoning, and multi-object pick-and-place in both simulation (LIBERO benchmark suites: Spatial, Object, Goal, Long) and real robots (WidowX / UR5 testbeds). Action space: continuous low-level arm/gripper commands (executed as action chunks in some variants); environment complexity: cluttered tabletop with distractors; both sim and real-world.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper discusses partial overlap: benchmark uses household objects commonly present in pretraining corpora, providing some semantic alignment for object categories and affordances; however, pretraining biases limit generalization to OOD objects and novel viewpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>NORA (fully fine-tuned) reported as top-performing: NORA-Long-fine-tuned achieves best overall performance on LIBERO (paper highlights average/top values including an 87.9% average reported for NORA-Long variants); NORA (standard fine-tuned) outperforms most baselines in the three task categories (exact per-suite numbers given in Table 2 / Fig. 6 of paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not numerically reported for NORA specifically; paper notes models trained from scratch (e.g., ACT and Diffusion Policy) were also evaluated and show different learning dynamics (DP/ACT trained from scratch were stable but required training), but direct per-task numeric baselines (random or vision-only) for NORA without pretraining are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Partial fine-tuning used a small custom dataset (163 demonstration episodes). The paper reports that NORA variants fine-tuned with action chunking transfer well to long-horizon tasks and that NORA converges to high success after full fine-tuning on large datasets; however, large VLAs generally required many iterations to adapt and full fine-tuning used large datasets (Open-X-Embodiment and LIBERO). No explicit scalar "x-fold" sample-efficiency ratio is given for NORA versus baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention- or saliency-based analysis for NORA reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No embedding-space or representation clustering analysis specific to NORA reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Indirect evidence: NORA successfully executes manipulation tasks after fine-tuning and benefits from action-chunk pretraining for long-horizon coherence, suggesting learned mapping from language-conditioned percepts to motor chunks; explicit mechanistic grounding analyses (e.g., link from verb tokens to affordance features) are not presented.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported for NORA; the paper does not provide explicit layer-wise/level-wise feature analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer improves when fine-tuned on larger, matching datasets (Open-X-Embodiment, LIBERO) and when model pretraining includes action-chunking; transfer degrades under perceptual distractors and sim-to-real domain gap. Action-chunking and pretraining on temporal windows improved long-horizon transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Paper reports degradation under novel/distractor objects; NORA succeeds on many OOD object manipulations but performance declines with distractors—specific numeric differences are reported in tables for OOD suites (NORA more robust than others but still sensitive).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>NORA demonstrates limited zero-shot generalization; primary improvements are obtained via fine-tuning (partial and full). Partial fine-tuning on 163 demos was performed but full performance required larger fine-tuning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No per-layer ablation or freezing analyses reported for NORA in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Paper reports that large VLA capacities can slow adaptation and cause instability during partial fine-tuning; no explicit example of pretraining hurting final asymptotic performance beyond adaptation difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Direct controlled comparison to vision-only pretraining (e.g., ImageNet-only) is not reported; comparisons are mainly across VLA variants and to models trained from scratch (DP/ACT).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Training dynamics: NORA variants fine-tuned with action chunking converge more reliably for long-horizon tasks; overall, large VLAs tend to require more iterations and show higher variance during partial fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1938.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1938.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source autoregressive VLA model combining Llama 2 (7B) with DinoV2/SigLIP visual tokenization, pretrained on large robot demonstration corpora (Open-X-Embodiment); evaluated in partial and full fine-tuning for manipulation transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Open-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive VLA: uses visual tokenizers (DinoV2, SigLIP) to convert images to tokens and a Llama-based decoder (fine-tuned Llama 2 7B) to autoregressively generate action sequences conditioned on language and vision.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language-action pretraining on large robot demonstration datasets</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretrained on the Open-X-Embodiment dataset (~970k real-world robot demonstrations) containing paired observations, language and action trajectories, so training data includes object manipulations, affordances, spatial relations, and temporal motor sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation (tabletop pick-and-place, spatial reasoning, multi-object tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on OOD object manipulation, spatial relationship reasoning, and multi-object pick-and-place in simulation (LIBERO) and real hardware (WidowX / UR5). Actions are continuous robot commands (autoreg outputs), tasks in cluttered tabletop scenes with distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Benchmarks used household objects that overlap with Open-X-Embodiment content, giving moderate semantic alignment; nevertheless, pretraining biases lead to degraded OOD and sim-to-real performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>OpenVLA (fully fine-tuned) reported competitive performance among baselines; cited as achieving best average (76.5%) among fine-tuned baselines without action chunking (paper text). On specific LIBERO suites, OpenVLA fine-tuned numbers are reported in Table 2 / Fig. 6.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>When partially fine-tuned on the small custom dataset, OpenVLA required significantly more training iterations to reach comparable accuracy and exhibited high variance; numeric baseline without language pretraining not explicitly provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Partial fine-tuning used 163 demonstration episodes; OpenVLA showed slower convergence and higher variance under partial fine-tuning versus smaller/specialist policies; full fine-tuning used large datasets for good performance. No explicit scalar sample-efficiency ratio provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention or saliency visualizations reported for OpenVLA in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported for OpenVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>OpenVLA generates end-to-end motor commands from language+vision and succeeds in many manipulation tasks after fine-tuning, indicating functional grounding; specific mechanistic analyses linking verbs to affordances are not given.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>OpenVLA's transfer suffers under distractors and sim-to-real gap; benefits from full fine-tuning on large datasets and is sensitive to robot control frequency (action-chunking improves some variants).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Experiments show performance degradation with unseen/distractor objects; OpenVLA succeeded on some OOD tasks but was more sensitive compared to NORA when distractors present.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Provides some zero-shot capability for object grounding when paired with downstream planners, but complex instruction grounding often required larger LLM reasoning or fine-tuning; partial few-shot fine-tuning on 163 demos gave limited rapid gains.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No per-layer probing or freezing experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Large pretrained capacity led to slower partial adaptation (negative effect on sample-efficiency); quantization impacts noted for similar LLaMA-based vision models (reducing reasoning accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No direct vision-only pretrained comparison; comparisons are across VLA family and to policies trained from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>OpenVLA required more training epochs to converge in partial fine-tuning and exhibited larger variance than diffusion-based or smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1938.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1938.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SpatialVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SpatialVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA model focused on incorporating 3D spatial relationships and spatial movement into vision-language-action predictions to improve spatial reasoning in manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SpatialVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>VLA variant that integrates explicit 3D spatial representations into its perception-to-action pipeline to better reason about spatial relationships; predicts multiple actions (four actions at a time in the text) and emphasizes spatial affordance estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language-action pretraining with explicit 3D spatial information</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretraining data emphasizes 3D spatial relationships and spatial movement information (likely includes scenes with spatial annotations/3D cues); uses datasets that encode spatial relationships for manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation with emphasis on spatial relationship reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on spatial reasoning tasks and multi-object manipulation (LIBERO suites and real-world tasks); action space: multi-step motor sequences, environment: cluttered tabletop with spatially-referential instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Pretraining explicitly targets spatial concepts, so higher semantic alignment for spatial-reasoning tasks; however, paper reports failures in affordance point estimation on some OOD object tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>SpatialVLA underperformed in some fully fine-tuned OOD object manipulation tasks due to incorrect affordance point estimation; in some LIBERO variants (with action chunking), SpatialVLA fine-tuned-AC shows strong performance as reported in Table 2 (specific suite numbers cited in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported numerically for SpatialVLA without pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported as a scalar; SpatialVLA fine-tuned with larger datasets for best results; action-chunking variants improved performance in sim experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Paper reports SpatialVLA failing on affordance point estimation in at least one OOD manipulation task, indicating limitations in grounding affordances for some novel objects despite spatial pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Fails when affordance estimation is incorrect; benefits from action-chunking for simulation transfer; sensitive to differences in object appearance and affordance distribution mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Performance drops on OOD objects when affordance estimation is poor (example reported where SpatialVLA failed while NORA/OpenVLA succeeded).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not highlighted as providing strong zero-shot manipulation abilities; relies on fine-tuning for best performance.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Example failure due to incorrect affordance point estimation suggests pretraining did not generalize to some novel object geometries.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No direct vision-only comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not discussed beyond general fine-tuning descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1938.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1938.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diffusion Policy (DP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diffusion Policy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diffusion-based VLA approach that models action sequence generation as a denoising process over latent trajectories, used both as a preexisting method and trained from scratch in the paper's partial fine-tuning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Diffusion Policy (DP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Diffusion-based action decoder: the model learns to denoise a noisy version of an entire action trajectory conditioned on language and vision, producing temporally-consistent motor sequences; used as a policy representation for visuomotor tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>visuomotor / robot-trajectory pretraining (diffusion over action sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Trained on demonstration trajectories encoding motor commands and temporal correlations; data contains action sequences for manipulation tasks (affordances implicitly present via trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation (tabletop grasping and pick-and-place tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Trained from scratch and evaluated on custom dataset and LIBERO simulation; actions are continuous trajectories sampled at controller frequencies (e.g., DP generates at 10 Hz and is interpolated to 125 Hz for execution in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High alignment between training trajectories and target motor tasks because DP is trained directly on robot trajectories; however, semantic (language) grounding depends on conditioning data and was explored in multimodal setups.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>When trained from scratch and conditioned on language, DP achieved stable training dynamics with low variance; although DP attained lower training loss by fitting noise, it required more steps to generate coherent actions. Exact per-task success rates in this paper for DP are given in Fig. 5 (training curves) but no single overall success rate summary is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Paper includes DP trained from scratch as a baseline; DP showed stable convergence but required many training steps—no direct numeric comparison to a language-pretrained DP variant is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>DP trained from scratch on the custom dataset (163 demos) showed stable low-variance training but took many steps to produce coherent actions; no explicit sample-efficiency ratio versus language-pretrained models is given.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>DP explicitly models full action trajectories conditioned on perceptual inputs, providing an implicit action grounding mechanism; empirical observations note DP attains low loss but requires more steps to produce coherent multi-step actions, indicating trajectory-level learning of motor patterns rather than explicit semantic grounding analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Performance depends on control frequency (generation at 10 Hz, interpolation during execution) and on matching between training trajectories and target robot dynamics; transfer to other domains requires sufficient temporal granularity alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not explicitly broken down by novel vs familiar objects in the DP experiments within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not demonstrated as a strong zero-shot performer; DP required training (from scratch or fine-tuning) to be effective.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No explicit negative transfer reported, but DP's need for many steps to form coherent actions after loss convergence suggests limits when data is scarce.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No direct comparison to vision-only pretraining; DP is a trajectory-focused policy baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Paper reports DP exhibits high stability and low variance during training but requires many iterations to convert low training loss into coherent action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1938.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1938.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ACT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action Chunking Transformer (ACT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer model trained to output action chunks (multiple-step motor commands) which improves long-horizon stability and transfer when action-chunking is employed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Action Chunking Transformer (ACT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A transformer policy architecture that outputs action chunks (sequences of multiple low-level commands) per forward pass, enabling temporally-coherent behavior and potentially higher effective control frequency when interpolated.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>policy learning on action-chunked robot demonstrations (trajectory-level learning)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Training data consists of demonstration trajectories segmented into chunks; contains temporal motor sequences for manipulation — includes affordance-driven motor patterns implicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation (long-horizon and multi-step tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on custom tabletop dataset and simulation (LIBERO); tested especially for long-horizon tasks where action chunking can improve performance by producing temporally-consistent multi-step plans. Action space: chunked continuous commands.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Chunking approach aligns well with temporally-extended motor patterns in demonstration data; semantic (language) alignment depends on conditioning signals in the training set and is not deeply analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>ACT trained from scratch displayed stable training and low variance in partial fine-tuning experiments; the paper reports that chunking variants (when used in VLA pretraining) substantially improve LIBERO-Long success rates (NORA-Long-fine-tuned benefits cited). Specific ACT numeric success rates are included in training result figures rather than summarized in a single metric in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>ACT was trained from scratch as a baseline; it showed stable learning dynamics but a direct numeric comparison versus language-pretrained models is not condensed as a single metric.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit numeric sample-efficiency ratios provided; ACT trained from scratch performed stably on the small dataset, suggesting reasonable sample efficiency for the chunking approach compared to large VLAs which showed slower adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Action-chunking empirically improved long-horizon task success in simulation and reduced collisions in some execution setups (paper details execution modifications for WidowX), indicating better temporal grounding of multi-step behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Action-chunking effectiveness depends on control frequency and execution interpolation (e.g., DP at 10Hz interpolated to 125Hz); simulation benefits observed at 20Hz in LIBERO, while real robots with lower control support showed different behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not explicitly reported in ACT-specific experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not described as zero-shot; trained on demonstration data (from scratch) for target tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No explicit negative transfer reported; chunking required careful execution strategy to avoid collisions when naive chunk execution was used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not applicable; ACT is an action-chunking policy architecture rather than a pure vision-only baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>ACT training exhibited stable, low-variance dynamics during partial fine-tuning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1938.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1938.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GroundingDINO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grounding DINO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-vocabulary object detector used in modular VLM pipelines to localize objects from language prompts; used as the perception module in modular robot testbeds and the baseline VLM pipeline in grounding experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GroundingDINO</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-vocabulary vision-language model for detection: given a natural language prompt it returns candidate bounding boxes for matching objects; used upstream of maskers (e.g., SAM) and planners in modular pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining on image-text/object detection pairs</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretraining includes large-scale image datasets and grounded captions enabling open-set detection of object categories described by natural language; contains object descriptions and visual features but not necessarily action trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>instruction grounding / object localization for manipulation pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Used in zero-shot modular pipelines to detect candidate objects in cluttered tabletop images so downstream planners can pick targets; environment: cluttered images with multiple household objects (EASY/MEDIUM/HARD setups).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>The instruction grounding benchmark deliberately uses common household objects frequently represented in FM training corpora to minimize mismatches; however, GroundingDINO showed failure modes on featureless objects and generic categories (misses 'ball' but detects 'blue ball').</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>GroundingDINO used zero-shot detection in the modular pipeline; it produced moderate object grounding performance in object recognition tasks but failed on several complex grounding cases (detailed failure examples in Appendix/Figures). Exact accuracy numbers for GroundingDINO alone are not summarized as a single metric in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable for GroundingDINO usage; paper does not report a vision-only (non-language) version as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not applicable; GroundingDINO is evaluated zero-shot in pipelines rather than fine-tuned in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Failure modes of GroundingDINO (missed detections, reliance on visual features) are reported to cause downstream action failures in modular pipelines, indicating that when VLM perception is brittle, action selection fails — this is indirect evidence that object detection quality is critical for downstream action grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Performs worse on featureless metallic objects and generic categories without strong visual cues; pipeline brittleness means perception errors propagate to control.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Paper notes misses for less visually-salient or ambiguous instances (e.g., featureless cans), implying poorer detection for some novel/low-feature items.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Used in a zero-shot manner within the modular pipeline for object detection from language prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No negative transfer per se, but perceptual failures cause downstream pipeline failures; limited open-vocabulary affordance reasoning compared to multimodal LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>GroundingDINO is a VLM (vision-language); not compared to vision-only detectors in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not applicable to this detection-only module in presented experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1938.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1938.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAM / FastSAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Segment Anything Model (SAM) / FastSAM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Segmentation modules used downstream of detectors (e.g., GroundingDINO) to refine object masks for modular manipulation pipelines in the real-world testbed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SAM / FastSAM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General-purpose image segmentation models that produce segmentation masks (SAM is the canonical model; FastSAM is a faster variant) from detected bounding boxes or prompts; used to create pixel-accurate object regions for grasp planning.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision pretraining / segmentation-focused large-scale image datasets</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Trained on large segmentation corpora spanning many object categories and shapes (not trajectory/action data); provides object masks and shape cues relevant for grasp planning but not explicit affordance labels.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>object segmentation for downstream grasping/manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Used in a modular UR5/Claw machine testbed to produce masks from detected boxes so grasp planners (GraspAnything/GraspNet) can compute grasps; environment: real-world tabletop with voice-instructed tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>SAM provides rich spatial shape information aligning with manipulation needs (grasp point estimation), but lacks explicit action semantics; alignment is mediated by downstream grasp planners.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not applicable (SAM is not a language-conditioned model here); used deterministically to refine detections into masks enabling successful modular manipulation in the deployed testbed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>SAM provides geometric masks that enable downstream grasp planners to compute actions; paper reports this pipeline being practically used in deployment, providing operational evidence that segmentation-to-grasp pipelines can ground perception to motor commands.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>SAM generalizes across many objects, but pipeline performance still susceptible to detection errors upstream (GroundingDINO misses cause SAM not to receive correct crops).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>SAM can segment novel shapes well in many cases; specific per-object generalization metrics are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>SAM used zero-shot for segmentation prompts in the pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>SAM is a vision segmentation model; direct comparisons not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1938.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1938.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4.5 (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4.5 (multimodal LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary multimodal LLM evaluated in zero-shot complex instruction grounding tasks; shows very strong implicit-instruction grounding performance but is resource-intensive at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4.5 (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large multimodal LLM capable of processing images and text and performing visual commonsense reasoning and instruction following; acts as an orchestrator in modular agent setups or directly consumed for grounding evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>multimodal (image+text) language modeling / vision-language pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretraining on large multimodal corpora with image-text pairs and extensive language priors enabling common-sense affordance inference and visual reasoning; contains object descriptions, affordance priors and spatial language priors but not necessarily paired action trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>complex instruction grounding (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Multiple-choice object grounding in cluttered tabletop images across instruction types (implicit, attribute-based, relationship-based); task is perception+reasoning to pick the correct object index.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Benchmark uses household objects commonly present in multimodal training sets, giving favorable semantic alignment; results show that GPT-4.5 leverages language priors to infer implicit affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Implicit instruction grounding: GPT-4.5 reported accuracy ~0.94 (94%) on implicit instructions (stated as exceptional performance in text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported; baseline modular VLM-LLM pipeline (GroundingDINO + LLM guessing) performs worse on implicit affordance reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Does not require fine-tuning for zero-shot grounding tasks; inference is resource-intensive (not sample-efficiency in demo data terms).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>GPT-4.5 exhibits strong visual commonsense for implicit instruction grounding (e.g., selecting tools by function), indicating language-to-percept mapping for affordance-like reasoning, but the paper does not present mechanistic grounding analyses tying verbs to motor primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Performs well zero-shot on implicit affordance tasks when domain contains common household objects; high inference cost is a limiting deployment factor.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Performant on familiar/common objects present in pretraining; relational reasoning degrades on hard relational tasks across many models (but GPT-4.5 remains strong on implicit affordances).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Yes — demonstrates strong zero-shot capability for implicit instruction grounding (94% on implicit examples).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported for GPT-4.5 in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Compared qualitatively to specialist VLM pipelines which perform worse on affordance/implicit reasoning; no direct vision-only-controlled comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not applicable to zero-shot evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1938.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1938.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini 2.5 / Gemini Robotics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 2.5 (Gemini Robotics Team)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art multimodal LLM (Gemini variant) evaluated on complex instruction grounding and described as benefiting from embodied training data in relational reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 2.5 / Gemini Robotics</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large multimodal model (Gemini family) with vision-and-language capabilities and variants finetuned/used for embodied robotic reasoning and planning; used as a cognitive hub in some referenced systems.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>large-scale multimodal vision-language pretraining (image+text), with embodied fine-tuning in robotics variants</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretraining on broad multimodal corpora with image-text pairs and subsequent embodied datasets for robotics variants (Gemini-Robotics) to impart spatial and trajectory reasoning and pose prediction; includes object descriptions and spatial relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>complex instruction grounding and embodied reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Zero-shot complex instruction grounding in cluttered tabletop scenes (implicit, attribute, relation questions) and referenced use as "embodied brain" in robotics stacks for planning and trajectory synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Gemini variants that received embodied training appear better aligned to relational grounding tasks; paper notes Gemini 2.5-Pro exceeded 0.80 on relational instructions in some evaluations, suggesting embodied data helped semantic alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported strong performance on relational grounding (Gemini 2.5-Pro achieves >0.80 accuracy on relational tasks in the paper's evaluations) and solid overall grounding performance across instruction types; exact aggregate numbers are included in Table 5.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported; comparisons generally against specialist VLM pipelines which underperform on implicit/relation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not quantified; multimodal LLMs operate well zero-shot/in-context without fine-tuning but are resource-intensive at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Gemini Robotics is cited as integrating perception, spatial reasoning and trajectory synthesis indicating the model can be used to map language to motion plans, but the present paper does not provide mechanistic analyses of how language maps to motor primitives within Gemini.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Embodied fine-tuning and access to embodied datasets appears to improve relational reasoning and transfer to robotic tasks; however, inference cost and model size affect deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Paper notes Gemini variants generalize well to novel concepts compared to specialist VLMs, implying robustness to unfamiliar objects in grounding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Gemini exhibits strong zero-shot generalization on some grounding tasks; in-context learning is emphasized for multimodal LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported specifically; general resource/inference cost is noted as a limitation for deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Compared qualitatively to specialist VLMs (vision-focused) which are more lightweight but less capable on implicit/relation reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1938.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1938.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama 3.2 Vision 11B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3.2 Vision 11B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large vision-capable LLaMA family model used in experiments (including INT4 quantized variants) to study the impact of model size and quantization on complex instruction grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 3.2 Vision 11B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-enabled LLaMA model that processes images and text, used as a multimodal LLM backbone; evaluated in full-precision and INT4-quantized forms.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>multimodal vision-language pretraining (image+text)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretraining comprises large multimodal corpora with image-text pairs enabling grounding and visual reasoning priors; contains object descriptions and spatial language but not explicit robot trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>complex instruction grounding (zero-shot/quantized evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Multiple-choice object grounding in cluttered tabletop images across implicit/attribute/relation instructions (EASY/MEDIUM/HARD splits).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Dataset objects were chosen to overlap with common FM training items to focus evaluation on grounding ability; LLaMA 3.2 Vision benefits from this overlap but quantization reduces reasoning accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported accuracies for the non-quantized 11B model appear in Table 5 (examples: attribute and relation scores reported across difficulty bins); the paper cites that LLaMA 3.2-Vision 11B provides a good speed-accuracy trade-off (0.57 cited in text for a speed-accuracy metric).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Quantization (INT4) reduces model size by >70% and affects implicit/relational grounding more (14–17% relative accuracy drop) than attribute grounding (~4% drop), indicating a trade-off between resource efficiency and reasoning capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Used for visual grounding evaluations; no direct action-generation experiments with this model in the paper, so no specific evidence tying language tokens to motor outputs is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Quantization and model size substantially impact grounding performance, especially for higher-level reasoning (implicit and relational instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Paper notes that smaller community models fall below specialist VLM thresholds for fine-grained grounding; LLaMA 3.2 Vision 11B remains usable for many scenes but degrades under quantization.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Used zero-shot for instruction grounding tasks; performance varies by instruction type.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Quantization-induced accuracy drops for reasoning tasks indicate situations where compressing the model harms performance (negative effect of quantization).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared to vision-only pretrained models in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not applicable (zero-shot evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Openvla: An open-source vision-language-action model. <em>(Rating: 2)</em></li>
                <li>Nora: A small open-sourced generalist vision language action model for embodied tasks. <em>(Rating: 2)</em></li>
                <li>Diffusion policy: Visuomotor policy learning via action diffusion. <em>(Rating: 2)</em></li>
                <li>Grounding dino: Marrying dino with grounded pretraining for open-set object detection. <em>(Rating: 2)</em></li>
                <li>Libero: Benchmarking knowledge transfer for lifelong robot learning. <em>(Rating: 2)</em></li>
                <li>Spatialvla: Exploring spatial representations for visual-language-action model. <em>(Rating: 2)</em></li>
                <li>RT-1: Robotics transformer for realworld control at scale. <em>(Rating: 1)</em></li>
                <li>RT-2: Visionlanguage-action models transfer web knowledge to robotic control. <em>(Rating: 1)</em></li>
                <li>Gemini Robotics: Bringing AI into the Physical World. <em>(Rating: 1)</em></li>
                <li>TraceVLA: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1938",
    "paper_id": "paper-278782301",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "NORA",
            "name_full": "NORA",
            "brief_description": "A small open-sourced generalist Vision-Language-Action (VLA) model evaluated and fine-tuned in this paper for tabletop manipulation and long-horizon tasks; shown to achieve top success rates after fine-tuning and benefits from action-chunking pretraining.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "NORA",
            "model_description": "A generalist VLA model that tightly couples visual inputs and language instructions to generate actions; described as relatively small and open-source, used with both autoregressive and action-chunking variants (NORA, NORA-LONG), and evaluated in partial and full fine-tuning regimes.",
            "pretraining_type": "vision-language-action pretraining on robot demonstration datasets",
            "pretraining_data_description": "Pretrained on large robot demonstration corpora (cites Open-X-Embodiment / related datasets and fine-tuning on LIBERO), containing paired visual observations, language instructions, and action trajectories (temporal motor sequences) — likely includes object descriptions, spatial relationships and manipulation trajectories/affordances.",
            "target_task_name": "robotic manipulation (tabletop pick-and-place, spatial-relation tasks, multi-object manipulation)",
            "target_task_description": "Evaluated on out-of-distribution (OOD) object manipulation, spatial relationship reasoning, and multi-object pick-and-place in both simulation (LIBERO benchmark suites: Spatial, Object, Goal, Long) and real robots (WidowX / UR5 testbeds). Action space: continuous low-level arm/gripper commands (executed as action chunks in some variants); environment complexity: cluttered tabletop with distractors; both sim and real-world.",
            "semantic_alignment": "Paper discusses partial overlap: benchmark uses household objects commonly present in pretraining corpora, providing some semantic alignment for object categories and affordances; however, pretraining biases limit generalization to OOD objects and novel viewpoints.",
            "performance_with_language_pretraining": "NORA (fully fine-tuned) reported as top-performing: NORA-Long-fine-tuned achieves best overall performance on LIBERO (paper highlights average/top values including an 87.9% average reported for NORA-Long variants); NORA (standard fine-tuned) outperforms most baselines in the three task categories (exact per-suite numbers given in Table 2 / Fig. 6 of paper).",
            "performance_without_language_pretraining": "Not numerically reported for NORA specifically; paper notes models trained from scratch (e.g., ACT and Diffusion Policy) were also evaluated and show different learning dynamics (DP/ACT trained from scratch were stable but required training), but direct per-task numeric baselines (random or vision-only) for NORA without pretraining are not provided.",
            "sample_efficiency_comparison": "Partial fine-tuning used a small custom dataset (163 demonstration episodes). The paper reports that NORA variants fine-tuned with action chunking transfer well to long-horizon tasks and that NORA converges to high success after full fine-tuning on large datasets; however, large VLAs generally required many iterations to adapt and full fine-tuning used large datasets (Open-X-Embodiment and LIBERO). No explicit scalar \"x-fold\" sample-efficiency ratio is given for NORA versus baselines.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "No attention- or saliency-based analysis for NORA reported in the paper.",
            "embedding_space_analysis": "No embedding-space or representation clustering analysis specific to NORA reported.",
            "action_grounding_evidence": "Indirect evidence: NORA successfully executes manipulation tasks after fine-tuning and benefits from action-chunk pretraining for long-horizon coherence, suggesting learned mapping from language-conditioned percepts to motor chunks; explicit mechanistic grounding analyses (e.g., link from verb tokens to affordance features) are not presented.",
            "hierarchical_features_evidence": "Not reported for NORA; the paper does not provide explicit layer-wise/level-wise feature analyses.",
            "transfer_conditions": "Transfer improves when fine-tuned on larger, matching datasets (Open-X-Embodiment, LIBERO) and when model pretraining includes action-chunking; transfer degrades under perceptual distractors and sim-to-real domain gap. Action-chunking and pretraining on temporal windows improved long-horizon transfer.",
            "novel_vs_familiar_objects": "Paper reports degradation under novel/distractor objects; NORA succeeds on many OOD object manipulations but performance declines with distractors—specific numeric differences are reported in tables for OOD suites (NORA more robust than others but still sensitive).",
            "zero_shot_or_few_shot": "NORA demonstrates limited zero-shot generalization; primary improvements are obtained via fine-tuning (partial and full). Partial fine-tuning on 163 demos was performed but full performance required larger fine-tuning datasets.",
            "layer_analysis": "No per-layer ablation or freezing analyses reported for NORA in this paper.",
            "negative_transfer_evidence": "Paper reports that large VLA capacities can slow adaptation and cause instability during partial fine-tuning; no explicit example of pretraining hurting final asymptotic performance beyond adaptation difficulty.",
            "comparison_to_vision_only": "Direct controlled comparison to vision-only pretraining (e.g., ImageNet-only) is not reported; comparisons are mainly across VLA variants and to models trained from scratch (DP/ACT).",
            "temporal_dynamics": "Training dynamics: NORA variants fine-tuned with action chunking converge more reliably for long-horizon tasks; overall, large VLAs tend to require more iterations and show higher variance during partial fine-tuning.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1938.0"
        },
        {
            "name_short": "OpenVLA",
            "name_full": "Open-VLA",
            "brief_description": "An open-source autoregressive VLA model combining Llama 2 (7B) with DinoV2/SigLIP visual tokenization, pretrained on large robot demonstration corpora (Open-X-Embodiment); evaluated in partial and full fine-tuning for manipulation transfer.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Open-VLA",
            "model_description": "Autoregressive VLA: uses visual tokenizers (DinoV2, SigLIP) to convert images to tokens and a Llama-based decoder (fine-tuned Llama 2 7B) to autoregressively generate action sequences conditioned on language and vision.",
            "pretraining_type": "vision-language-action pretraining on large robot demonstration datasets",
            "pretraining_data_description": "Pretrained on the Open-X-Embodiment dataset (~970k real-world robot demonstrations) containing paired observations, language and action trajectories, so training data includes object manipulations, affordances, spatial relations, and temporal motor sequences.",
            "target_task_name": "robotic manipulation (tabletop pick-and-place, spatial reasoning, multi-object tasks)",
            "target_task_description": "Evaluated on OOD object manipulation, spatial relationship reasoning, and multi-object pick-and-place in simulation (LIBERO) and real hardware (WidowX / UR5). Actions are continuous robot commands (autoreg outputs), tasks in cluttered tabletop scenes with distractors.",
            "semantic_alignment": "Benchmarks used household objects that overlap with Open-X-Embodiment content, giving moderate semantic alignment; nevertheless, pretraining biases lead to degraded OOD and sim-to-real performance.",
            "performance_with_language_pretraining": "OpenVLA (fully fine-tuned) reported competitive performance among baselines; cited as achieving best average (76.5%) among fine-tuned baselines without action chunking (paper text). On specific LIBERO suites, OpenVLA fine-tuned numbers are reported in Table 2 / Fig. 6.",
            "performance_without_language_pretraining": "When partially fine-tuned on the small custom dataset, OpenVLA required significantly more training iterations to reach comparable accuracy and exhibited high variance; numeric baseline without language pretraining not explicitly provided.",
            "sample_efficiency_comparison": "Partial fine-tuning used 163 demonstration episodes; OpenVLA showed slower convergence and higher variance under partial fine-tuning versus smaller/specialist policies; full fine-tuning used large datasets for good performance. No explicit scalar sample-efficiency ratio provided.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "No attention or saliency visualizations reported for OpenVLA in this paper.",
            "embedding_space_analysis": "Not reported for OpenVLA.",
            "action_grounding_evidence": "OpenVLA generates end-to-end motor commands from language+vision and succeeds in many manipulation tasks after fine-tuning, indicating functional grounding; specific mechanistic analyses linking verbs to affordances are not given.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "OpenVLA's transfer suffers under distractors and sim-to-real gap; benefits from full fine-tuning on large datasets and is sensitive to robot control frequency (action-chunking improves some variants).",
            "novel_vs_familiar_objects": "Experiments show performance degradation with unseen/distractor objects; OpenVLA succeeded on some OOD tasks but was more sensitive compared to NORA when distractors present.",
            "zero_shot_or_few_shot": "Provides some zero-shot capability for object grounding when paired with downstream planners, but complex instruction grounding often required larger LLM reasoning or fine-tuning; partial few-shot fine-tuning on 163 demos gave limited rapid gains.",
            "layer_analysis": "No per-layer probing or freezing experiments reported.",
            "negative_transfer_evidence": "Large pretrained capacity led to slower partial adaptation (negative effect on sample-efficiency); quantization impacts noted for similar LLaMA-based vision models (reducing reasoning accuracy).",
            "comparison_to_vision_only": "No direct vision-only pretrained comparison; comparisons are across VLA family and to policies trained from scratch.",
            "temporal_dynamics": "OpenVLA required more training epochs to converge in partial fine-tuning and exhibited larger variance than diffusion-based or smaller models.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1938.1"
        },
        {
            "name_short": "SpatialVLA",
            "name_full": "SpatialVLA",
            "brief_description": "A VLA model focused on incorporating 3D spatial relationships and spatial movement into vision-language-action predictions to improve spatial reasoning in manipulation tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SpatialVLA",
            "model_description": "VLA variant that integrates explicit 3D spatial representations into its perception-to-action pipeline to better reason about spatial relationships; predicts multiple actions (four actions at a time in the text) and emphasizes spatial affordance estimation.",
            "pretraining_type": "vision-language-action pretraining with explicit 3D spatial information",
            "pretraining_data_description": "Pretraining data emphasizes 3D spatial relationships and spatial movement information (likely includes scenes with spatial annotations/3D cues); uses datasets that encode spatial relationships for manipulation tasks.",
            "target_task_name": "robotic manipulation with emphasis on spatial relationship reasoning",
            "target_task_description": "Evaluated on spatial reasoning tasks and multi-object manipulation (LIBERO suites and real-world tasks); action space: multi-step motor sequences, environment: cluttered tabletop with spatially-referential instructions.",
            "semantic_alignment": "Pretraining explicitly targets spatial concepts, so higher semantic alignment for spatial-reasoning tasks; however, paper reports failures in affordance point estimation on some OOD object tasks.",
            "performance_with_language_pretraining": "SpatialVLA underperformed in some fully fine-tuned OOD object manipulation tasks due to incorrect affordance point estimation; in some LIBERO variants (with action chunking), SpatialVLA fine-tuned-AC shows strong performance as reported in Table 2 (specific suite numbers cited in paper).",
            "performance_without_language_pretraining": "Not reported numerically for SpatialVLA without pretraining.",
            "sample_efficiency_comparison": "Not reported as a scalar; SpatialVLA fine-tuned with larger datasets for best results; action-chunking variants improved performance in sim experiments.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Paper reports SpatialVLA failing on affordance point estimation in at least one OOD manipulation task, indicating limitations in grounding affordances for some novel objects despite spatial pretraining.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Fails when affordance estimation is incorrect; benefits from action-chunking for simulation transfer; sensitive to differences in object appearance and affordance distribution mismatch.",
            "novel_vs_familiar_objects": "Performance drops on OOD objects when affordance estimation is poor (example reported where SpatialVLA failed while NORA/OpenVLA succeeded).",
            "zero_shot_or_few_shot": "Not highlighted as providing strong zero-shot manipulation abilities; relies on fine-tuning for best performance.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Example failure due to incorrect affordance point estimation suggests pretraining did not generalize to some novel object geometries.",
            "comparison_to_vision_only": "No direct vision-only comparison provided.",
            "temporal_dynamics": "Not discussed beyond general fine-tuning descriptions.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1938.2"
        },
        {
            "name_short": "Diffusion Policy (DP)",
            "name_full": "Diffusion Policy",
            "brief_description": "A diffusion-based VLA approach that models action sequence generation as a denoising process over latent trajectories, used both as a preexisting method and trained from scratch in the paper's partial fine-tuning experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Diffusion Policy (DP)",
            "model_description": "Diffusion-based action decoder: the model learns to denoise a noisy version of an entire action trajectory conditioned on language and vision, producing temporally-consistent motor sequences; used as a policy representation for visuomotor tasks.",
            "pretraining_type": "visuomotor / robot-trajectory pretraining (diffusion over action sequences)",
            "pretraining_data_description": "Trained on demonstration trajectories encoding motor commands and temporal correlations; data contains action sequences for manipulation tasks (affordances implicitly present via trajectories).",
            "target_task_name": "robotic manipulation (tabletop grasping and pick-and-place tasks)",
            "target_task_description": "Trained from scratch and evaluated on custom dataset and LIBERO simulation; actions are continuous trajectories sampled at controller frequencies (e.g., DP generates at 10 Hz and is interpolated to 125 Hz for execution in experiments).",
            "semantic_alignment": "High alignment between training trajectories and target motor tasks because DP is trained directly on robot trajectories; however, semantic (language) grounding depends on conditioning data and was explored in multimodal setups.",
            "performance_with_language_pretraining": "When trained from scratch and conditioned on language, DP achieved stable training dynamics with low variance; although DP attained lower training loss by fitting noise, it required more steps to generate coherent actions. Exact per-task success rates in this paper for DP are given in Fig. 5 (training curves) but no single overall success rate summary is provided.",
            "performance_without_language_pretraining": "Paper includes DP trained from scratch as a baseline; DP showed stable convergence but required many training steps—no direct numeric comparison to a language-pretrained DP variant is provided.",
            "sample_efficiency_comparison": "DP trained from scratch on the custom dataset (163 demos) showed stable low-variance training but took many steps to produce coherent actions; no explicit sample-efficiency ratio versus language-pretrained models is given.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "DP explicitly models full action trajectories conditioned on perceptual inputs, providing an implicit action grounding mechanism; empirical observations note DP attains low loss but requires more steps to produce coherent multi-step actions, indicating trajectory-level learning of motor patterns rather than explicit semantic grounding analysis.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Performance depends on control frequency (generation at 10 Hz, interpolation during execution) and on matching between training trajectories and target robot dynamics; transfer to other domains requires sufficient temporal granularity alignment.",
            "novel_vs_familiar_objects": "Not explicitly broken down by novel vs familiar objects in the DP experiments within this paper.",
            "zero_shot_or_few_shot": "Not demonstrated as a strong zero-shot performer; DP required training (from scratch or fine-tuning) to be effective.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "No explicit negative transfer reported, but DP's need for many steps to form coherent actions after loss convergence suggests limits when data is scarce.",
            "comparison_to_vision_only": "No direct comparison to vision-only pretraining; DP is a trajectory-focused policy baseline.",
            "temporal_dynamics": "Paper reports DP exhibits high stability and low variance during training but requires many iterations to convert low training loss into coherent action sequences.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1938.3"
        },
        {
            "name_short": "ACT",
            "name_full": "Action Chunking Transformer (ACT)",
            "brief_description": "A transformer model trained to output action chunks (multiple-step motor commands) which improves long-horizon stability and transfer when action-chunking is employed.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Action Chunking Transformer (ACT)",
            "model_description": "A transformer policy architecture that outputs action chunks (sequences of multiple low-level commands) per forward pass, enabling temporally-coherent behavior and potentially higher effective control frequency when interpolated.",
            "pretraining_type": "policy learning on action-chunked robot demonstrations (trajectory-level learning)",
            "pretraining_data_description": "Training data consists of demonstration trajectories segmented into chunks; contains temporal motor sequences for manipulation — includes affordance-driven motor patterns implicitly.",
            "target_task_name": "robotic manipulation (long-horizon and multi-step tasks)",
            "target_task_description": "Evaluated on custom tabletop dataset and simulation (LIBERO); tested especially for long-horizon tasks where action chunking can improve performance by producing temporally-consistent multi-step plans. Action space: chunked continuous commands.",
            "semantic_alignment": "Chunking approach aligns well with temporally-extended motor patterns in demonstration data; semantic (language) alignment depends on conditioning signals in the training set and is not deeply analyzed.",
            "performance_with_language_pretraining": "ACT trained from scratch displayed stable training and low variance in partial fine-tuning experiments; the paper reports that chunking variants (when used in VLA pretraining) substantially improve LIBERO-Long success rates (NORA-Long-fine-tuned benefits cited). Specific ACT numeric success rates are included in training result figures rather than summarized in a single metric in the main text.",
            "performance_without_language_pretraining": "ACT was trained from scratch as a baseline; it showed stable learning dynamics but a direct numeric comparison versus language-pretrained models is not condensed as a single metric.",
            "sample_efficiency_comparison": "No explicit numeric sample-efficiency ratios provided; ACT trained from scratch performed stably on the small dataset, suggesting reasonable sample efficiency for the chunking approach compared to large VLAs which showed slower adaptation.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Action-chunking empirically improved long-horizon task success in simulation and reduced collisions in some execution setups (paper details execution modifications for WidowX), indicating better temporal grounding of multi-step behaviors.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Action-chunking effectiveness depends on control frequency and execution interpolation (e.g., DP at 10Hz interpolated to 125Hz); simulation benefits observed at 20Hz in LIBERO, while real robots with lower control support showed different behavior.",
            "novel_vs_familiar_objects": "Not explicitly reported in ACT-specific experiments.",
            "zero_shot_or_few_shot": "Not described as zero-shot; trained on demonstration data (from scratch) for target tasks.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "No explicit negative transfer reported; chunking required careful execution strategy to avoid collisions when naive chunk execution was used.",
            "comparison_to_vision_only": "Not applicable; ACT is an action-chunking policy architecture rather than a pure vision-only baseline.",
            "temporal_dynamics": "ACT training exhibited stable, low-variance dynamics during partial fine-tuning experiments.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1938.4"
        },
        {
            "name_short": "GroundingDINO",
            "name_full": "Grounding DINO",
            "brief_description": "An open-vocabulary object detector used in modular VLM pipelines to localize objects from language prompts; used as the perception module in modular robot testbeds and the baseline VLM pipeline in grounding experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GroundingDINO",
            "model_description": "Open-vocabulary vision-language model for detection: given a natural language prompt it returns candidate bounding boxes for matching objects; used upstream of maskers (e.g., SAM) and planners in modular pipelines.",
            "pretraining_type": "vision-language pretraining on image-text/object detection pairs",
            "pretraining_data_description": "Pretraining includes large-scale image datasets and grounded captions enabling open-set detection of object categories described by natural language; contains object descriptions and visual features but not necessarily action trajectories.",
            "target_task_name": "instruction grounding / object localization for manipulation pipelines",
            "target_task_description": "Used in zero-shot modular pipelines to detect candidate objects in cluttered tabletop images so downstream planners can pick targets; environment: cluttered images with multiple household objects (EASY/MEDIUM/HARD setups).",
            "semantic_alignment": "The instruction grounding benchmark deliberately uses common household objects frequently represented in FM training corpora to minimize mismatches; however, GroundingDINO showed failure modes on featureless objects and generic categories (misses 'ball' but detects 'blue ball').",
            "performance_with_language_pretraining": "GroundingDINO used zero-shot detection in the modular pipeline; it produced moderate object grounding performance in object recognition tasks but failed on several complex grounding cases (detailed failure examples in Appendix/Figures). Exact accuracy numbers for GroundingDINO alone are not summarized as a single metric in the main text.",
            "performance_without_language_pretraining": "Not applicable for GroundingDINO usage; paper does not report a vision-only (non-language) version as a baseline in experiments.",
            "sample_efficiency_comparison": "Not applicable; GroundingDINO is evaluated zero-shot in pipelines rather than fine-tuned in this work.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Failure modes of GroundingDINO (missed detections, reliance on visual features) are reported to cause downstream action failures in modular pipelines, indicating that when VLM perception is brittle, action selection fails — this is indirect evidence that object detection quality is critical for downstream action grounding.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Performs worse on featureless metallic objects and generic categories without strong visual cues; pipeline brittleness means perception errors propagate to control.",
            "novel_vs_familiar_objects": "Paper notes misses for less visually-salient or ambiguous instances (e.g., featureless cans), implying poorer detection for some novel/low-feature items.",
            "zero_shot_or_few_shot": "Used in a zero-shot manner within the modular pipeline for object detection from language prompts.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "No negative transfer per se, but perceptual failures cause downstream pipeline failures; limited open-vocabulary affordance reasoning compared to multimodal LLMs.",
            "comparison_to_vision_only": "GroundingDINO is a VLM (vision-language); not compared to vision-only detectors in this paper.",
            "temporal_dynamics": "Not applicable to this detection-only module in presented experiments.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1938.5"
        },
        {
            "name_short": "SAM / FastSAM",
            "name_full": "Segment Anything Model (SAM) / FastSAM",
            "brief_description": "Segmentation modules used downstream of detectors (e.g., GroundingDINO) to refine object masks for modular manipulation pipelines in the real-world testbed.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SAM / FastSAM",
            "model_description": "General-purpose image segmentation models that produce segmentation masks (SAM is the canonical model; FastSAM is a faster variant) from detected bounding boxes or prompts; used to create pixel-accurate object regions for grasp planning.",
            "pretraining_type": "vision pretraining / segmentation-focused large-scale image datasets",
            "pretraining_data_description": "Trained on large segmentation corpora spanning many object categories and shapes (not trajectory/action data); provides object masks and shape cues relevant for grasp planning but not explicit affordance labels.",
            "target_task_name": "object segmentation for downstream grasping/manipulation",
            "target_task_description": "Used in a modular UR5/Claw machine testbed to produce masks from detected boxes so grasp planners (GraspAnything/GraspNet) can compute grasps; environment: real-world tabletop with voice-instructed tasks.",
            "semantic_alignment": "SAM provides rich spatial shape information aligning with manipulation needs (grasp point estimation), but lacks explicit action semantics; alignment is mediated by downstream grasp planners.",
            "performance_with_language_pretraining": "Not applicable (SAM is not a language-conditioned model here); used deterministically to refine detections into masks enabling successful modular manipulation in the deployed testbed.",
            "performance_without_language_pretraining": "Not applicable.",
            "sample_efficiency_comparison": "Not applicable.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "SAM provides geometric masks that enable downstream grasp planners to compute actions; paper reports this pipeline being practically used in deployment, providing operational evidence that segmentation-to-grasp pipelines can ground perception to motor commands.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "SAM generalizes across many objects, but pipeline performance still susceptible to detection errors upstream (GroundingDINO misses cause SAM not to receive correct crops).",
            "novel_vs_familiar_objects": "SAM can segment novel shapes well in many cases; specific per-object generalization metrics are not reported.",
            "zero_shot_or_few_shot": "SAM used zero-shot for segmentation prompts in the pipeline.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "SAM is a vision segmentation model; direct comparisons not provided.",
            "temporal_dynamics": "Not applicable.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1938.6"
        },
        {
            "name_short": "GPT-4.5 (multimodal)",
            "name_full": "GPT-4.5 (multimodal LLM)",
            "brief_description": "A proprietary multimodal LLM evaluated in zero-shot complex instruction grounding tasks; shows very strong implicit-instruction grounding performance but is resource-intensive at inference.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4.5 (multimodal)",
            "model_description": "Large multimodal LLM capable of processing images and text and performing visual commonsense reasoning and instruction following; acts as an orchestrator in modular agent setups or directly consumed for grounding evaluations.",
            "pretraining_type": "multimodal (image+text) language modeling / vision-language pretraining",
            "pretraining_data_description": "Pretraining on large multimodal corpora with image-text pairs and extensive language priors enabling common-sense affordance inference and visual reasoning; contains object descriptions, affordance priors and spatial language priors but not necessarily paired action trajectories.",
            "target_task_name": "complex instruction grounding (zero-shot)",
            "target_task_description": "Multiple-choice object grounding in cluttered tabletop images across instruction types (implicit, attribute-based, relationship-based); task is perception+reasoning to pick the correct object index.",
            "semantic_alignment": "Benchmark uses household objects commonly present in multimodal training sets, giving favorable semantic alignment; results show that GPT-4.5 leverages language priors to infer implicit affordances.",
            "performance_with_language_pretraining": "Implicit instruction grounding: GPT-4.5 reported accuracy ~0.94 (94%) on implicit instructions (stated as exceptional performance in text).",
            "performance_without_language_pretraining": "Not reported; baseline modular VLM-LLM pipeline (GroundingDINO + LLM guessing) performs worse on implicit affordance reasoning.",
            "sample_efficiency_comparison": "Does not require fine-tuning for zero-shot grounding tasks; inference is resource-intensive (not sample-efficiency in demo data terms).",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "GPT-4.5 exhibits strong visual commonsense for implicit instruction grounding (e.g., selecting tools by function), indicating language-to-percept mapping for affordance-like reasoning, but the paper does not present mechanistic grounding analyses tying verbs to motor primitives.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Performs well zero-shot on implicit affordance tasks when domain contains common household objects; high inference cost is a limiting deployment factor.",
            "novel_vs_familiar_objects": "Performant on familiar/common objects present in pretraining; relational reasoning degrades on hard relational tasks across many models (but GPT-4.5 remains strong on implicit affordances).",
            "zero_shot_or_few_shot": "Yes — demonstrates strong zero-shot capability for implicit instruction grounding (94% on implicit examples).",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported for GPT-4.5 in this paper.",
            "comparison_to_vision_only": "Compared qualitatively to specialist VLM pipelines which perform worse on affordance/implicit reasoning; no direct vision-only-controlled comparison.",
            "temporal_dynamics": "Not applicable to zero-shot evaluation.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1938.7"
        },
        {
            "name_short": "Gemini 2.5 / Gemini Robotics",
            "name_full": "Gemini 2.5 (Gemini Robotics Team)",
            "brief_description": "A state-of-the-art multimodal LLM (Gemini variant) evaluated on complex instruction grounding and described as benefiting from embodied training data in relational reasoning tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini 2.5 / Gemini Robotics",
            "model_description": "Large multimodal model (Gemini family) with vision-and-language capabilities and variants finetuned/used for embodied robotic reasoning and planning; used as a cognitive hub in some referenced systems.",
            "pretraining_type": "large-scale multimodal vision-language pretraining (image+text), with embodied fine-tuning in robotics variants",
            "pretraining_data_description": "Pretraining on broad multimodal corpora with image-text pairs and subsequent embodied datasets for robotics variants (Gemini-Robotics) to impart spatial and trajectory reasoning and pose prediction; includes object descriptions and spatial relationships.",
            "target_task_name": "complex instruction grounding and embodied reasoning",
            "target_task_description": "Zero-shot complex instruction grounding in cluttered tabletop scenes (implicit, attribute, relation questions) and referenced use as \"embodied brain\" in robotics stacks for planning and trajectory synthesis.",
            "semantic_alignment": "Gemini variants that received embodied training appear better aligned to relational grounding tasks; paper notes Gemini 2.5-Pro exceeded 0.80 on relational instructions in some evaluations, suggesting embodied data helped semantic alignment.",
            "performance_with_language_pretraining": "Reported strong performance on relational grounding (Gemini 2.5-Pro achieves &gt;0.80 accuracy on relational tasks in the paper's evaluations) and solid overall grounding performance across instruction types; exact aggregate numbers are included in Table 5.",
            "performance_without_language_pretraining": "Not reported; comparisons generally against specialist VLM pipelines which underperform on implicit/relation tasks.",
            "sample_efficiency_comparison": "Not quantified; multimodal LLMs operate well zero-shot/in-context without fine-tuning but are resource-intensive at inference.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Gemini Robotics is cited as integrating perception, spatial reasoning and trajectory synthesis indicating the model can be used to map language to motion plans, but the present paper does not provide mechanistic analyses of how language maps to motor primitives within Gemini.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Embodied fine-tuning and access to embodied datasets appears to improve relational reasoning and transfer to robotic tasks; however, inference cost and model size affect deployment.",
            "novel_vs_familiar_objects": "Paper notes Gemini variants generalize well to novel concepts compared to specialist VLMs, implying robustness to unfamiliar objects in grounding tasks.",
            "zero_shot_or_few_shot": "Gemini exhibits strong zero-shot generalization on some grounding tasks; in-context learning is emphasized for multimodal LLM agents.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported specifically; general resource/inference cost is noted as a limitation for deployment.",
            "comparison_to_vision_only": "Compared qualitatively to specialist VLMs (vision-focused) which are more lightweight but less capable on implicit/relation reasoning.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1938.8"
        },
        {
            "name_short": "Llama 3.2 Vision 11B",
            "name_full": "LLaMA 3.2 Vision 11B",
            "brief_description": "A large vision-capable LLaMA family model used in experiments (including INT4 quantized variants) to study the impact of model size and quantization on complex instruction grounding.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA 3.2 Vision 11B",
            "model_description": "Vision-enabled LLaMA model that processes images and text, used as a multimodal LLM backbone; evaluated in full-precision and INT4-quantized forms.",
            "pretraining_type": "multimodal vision-language pretraining (image+text)",
            "pretraining_data_description": "Pretraining comprises large multimodal corpora with image-text pairs enabling grounding and visual reasoning priors; contains object descriptions and spatial language but not explicit robot trajectories.",
            "target_task_name": "complex instruction grounding (zero-shot/quantized evaluation)",
            "target_task_description": "Multiple-choice object grounding in cluttered tabletop images across implicit/attribute/relation instructions (EASY/MEDIUM/HARD splits).",
            "semantic_alignment": "Dataset objects were chosen to overlap with common FM training items to focus evaluation on grounding ability; LLaMA 3.2 Vision benefits from this overlap but quantization reduces reasoning accuracy.",
            "performance_with_language_pretraining": "Reported accuracies for the non-quantized 11B model appear in Table 5 (examples: attribute and relation scores reported across difficulty bins); the paper cites that LLaMA 3.2-Vision 11B provides a good speed-accuracy trade-off (0.57 cited in text for a speed-accuracy metric).",
            "performance_without_language_pretraining": "Not reported.",
            "sample_efficiency_comparison": "Quantization (INT4) reduces model size by &gt;70% and affects implicit/relational grounding more (14–17% relative accuracy drop) than attribute grounding (~4% drop), indicating a trade-off between resource efficiency and reasoning capabilities.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Used for visual grounding evaluations; no direct action-generation experiments with this model in the paper, so no specific evidence tying language tokens to motor outputs is provided.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Quantization and model size substantially impact grounding performance, especially for higher-level reasoning (implicit and relational instructions).",
            "novel_vs_familiar_objects": "Paper notes that smaller community models fall below specialist VLM thresholds for fine-grained grounding; LLaMA 3.2 Vision 11B remains usable for many scenes but degrades under quantization.",
            "zero_shot_or_few_shot": "Used zero-shot for instruction grounding tasks; performance varies by instruction type.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Quantization-induced accuracy drops for reasoning tasks indicate situations where compressing the model harms performance (negative effect of quantization).",
            "comparison_to_vision_only": "Not directly compared to vision-only pretrained models in the paper.",
            "temporal_dynamics": "Not applicable (zero-shot evaluation).",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1938.9"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Openvla: An open-source vision-language-action model.",
            "rating": 2
        },
        {
            "paper_title": "Nora: A small open-sourced generalist vision language action model for embodied tasks.",
            "rating": 2
        },
        {
            "paper_title": "Diffusion policy: Visuomotor policy learning via action diffusion.",
            "rating": 2
        },
        {
            "paper_title": "Grounding dino: Marrying dino with grounded pretraining for open-set object detection.",
            "rating": 2
        },
        {
            "paper_title": "Libero: Benchmarking knowledge transfer for lifelong robot learning.",
            "rating": 2
        },
        {
            "paper_title": "Spatialvla: Exploring spatial representations for visual-language-action model.",
            "rating": 2
        },
        {
            "paper_title": "RT-1: Robotics transformer for realworld control at scale.",
            "rating": 1
        },
        {
            "paper_title": "RT-2: Visionlanguage-action models transfer web knowledge to robotic control.",
            "rating": 1
        },
        {
            "paper_title": "Gemini Robotics: Bringing AI into the Physical World.",
            "rating": 1
        },
        {
            "paper_title": "TraceVLA: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies.",
            "rating": 1
        }
    ],
    "cost": 0.026199999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems
21 May 2025</p>
<p>Xiuchao Sui xiuchao.sui@gmail.com 
Institute of High Performance Computing
A*STARSingapore</p>
<p>Daiying Tian 
Institute of High Performance Computing
A*STARSingapore</p>
<p>Qi Sun 
Singapore University of Technology and Design
Singapore</p>
<p>Ruirui Chen 
Institute of High Performance Computing
A*STARSingapore</p>
<p>Dongkyu Choi 
Institute of High Performance Computing
A*STARSingapore</p>
<p>Kenneth Kwok 
Institute of High Performance Computing
A*STARSingapore</p>
<p>Soujanya Poria 
Singapore University of Technology and Design
Singapore</p>
<p>From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems
21 May 20256C98037933C757D0636BC6630A99CFFAarXiv:2505.15685v1[cs.RO]Manipulation Generalization Novel objects and scenes Robot morphology Open language Cross-modal ambiguity
Foundation models (FMs) are increasingly used to bridge language and action in embodied agents, yet the operational characteristics of different FM integration strategies remain underexplored-particularly for complex instruction following and versatile action generation in changing environments.This paper examines three paradigms for building robotic systems: end-to-end vision-language-action (VLA) models that implicitly integrate perception and planning, and modular pipelines incorporating either vision-language models (VLMs) or multimodal large language models (LLMs).We evaluate these paradigms through two focused case studies: a complex instruction grounding task assessing fine-grained instruction understanding and cross-modal disambiguation, and an object manipulation task targeting skill transfer via VLA finetuning.Our experiments in zeroshot and few-shot settings reveal trade-offs in generalization and data efficiency.By exploring performance limits, we distill design implications for developing language-driven physical agents and outline emerging challenges and opportunities for FM-powered robotics in realworld conditions.</p>
<p>Introduction</p>
<p>Natural language is emerging as a universal interface for embodied robotic systems.Recent foundation models (FMs) allow robots to follow free-form instructions in perception, reasoning, and motor commands, offering the promise of language-grounded autonomy.They include multimodal large language models (LLMs) (Grattafiori et al., 2024;Bai et al., 2025;Lu et al., 2024), vision-language models (VLMs) (Liu et al., 2024a;Ravi et al., 2025;Ren et al., 2024;Li et al., 2023), and vision-language-action (VLA) models (Kim Figure 1: Challenges of foundation models in embodied robotic systems include cross-modal instruction grounding, generalization across environments and morphologies, and data-efficient adaptation for the real world.et al., 2024;Zheng et al., 2025;Qu et al., 2025;Bu et al., 2025).</p>
<p>However, turning the promise of languagegrounded autonomy into deployable systems is highly challenging.Robots must (i) map ambiguous instructions to the physical world (instruction grounding), (ii) execute reliably across novel objects, scenes, and robot morphologies (generalizable execution), and (iii) achieving the aforementioned goals with limited data (efficient adaptation).How well different FM integration strategies meet these competing requirements remains underexplored (Fig. 1).</p>
<p>To our best knowledge, this work delivers the first head-to-head empirical comparison of three prevalent integration paradigms: end-to-end VLAs that directly map language and vision to actions, multimodal LLM agents that orchestrate perception and control through tool calls, and modular VLM pipelines that couple perception-specialist FMs with task-specific planners (Fig. 2; Table 1).</p>
<p>We evaluate these paradigms through tabletop case studies designed to highlight their complementary strengths and limitations.Specifically, we consider two task categories: (i) Complex Instruction Grounding, which probes fine-grained understanding and cross-modal disambiguation (Sec.3); and</p>
<p>(ii) Object Manipulation, which measures the ability to transfer learned skills after VLA fine-tuning under distribution shifts, complemented by comparative and ablation studies (Sec.4).</p>
<p>Our zero-shot grounding experiments reveal distinct trade-offs across integration strategies.VLM pipelines prioritize interpretability and data efficiency, sacrificing flexibility and peak performance.While underperforming on handling complex instruction grounding, they deliver moderate performance in object grounding-using less than 1% of the parameters required by multimodal LLMs.In contrast, multimodal LLM agents generalize better on complex instructions but incur significantly higher inference costs.Notably, smaller reasoning-focused models such as GPT-4o-mini can even outperform larger models like GPT-4o on certain tasks.VLAs, with their tightly coupled perception-to-action pathways, support streamlined action generation, yet struggle to reason about rare or abstract concepts.We further examine the tradeoff between model size and performance by analyzing quantization effects on open-source multimodal LLMs.These findings offer practical guidance for developing language-driven robotic systems under real-world constraints.</p>
<p>Within the VLA paradigm, we categorize models by their action generation mechanismsautoregressive (Vaswani et al., 2017;Kim et al., 2024;Hung et al., 2025) and diffusion-based approaches (Ho et al., 2020;Chi et al., 2023;Reuss et al., 2024;Wen et al., 2024).We evaluate their adaptability through fine-tuning under distribution shifts, mirroring real-world deployment scenarios.</p>
<p>To assess generalization, we analyze robustness to environmental perturbations and variation in object appearance and robot morphology.</p>
<p>To summarize, the main contributions of this work are as follows: • To our best knowledge, we present the first systematic comparison of end-to-end VLA, modular VLM, and modular multimodal LLM architectures on the same set of embodied tasks.</p>
<p>• We release a dataset and accompanying code 1 that supports the evaluation of complex instruction grounding and manipulation transfercovering accordance comprehension, crossmodal reasoning, and motor adaptation.</p>
<p>• We benchmark state-of-the-art VLAs and multimodal LLMs, offering timely insight into the capabilities and failure modes of modern FMs in robotics.</p>
<p>• We distill actionable trade-offs that practitioners can apply when choosing an FM stack for language-driven embodied agents.</p>
<p>• We release a complete, end-to-end claw-machine robot system to demonstrate FM integrations in real-world applications2 .</p>
<p>2 Foundation Model Integration for Language-Guided Robotics</p>
<p>Concerning how FMs are integrated into robot systems, we identified the following three types of integration strategies (Fig. 2).In the following, We briefly describe each strategy along with its respective advantages and limitations.</p>
<p>End-to-End Vision-Language-Action</p>
<p>Definition.Vision-Language-Action (VLA) models operate in an end-to-end manner, directly translating visual observations and natural language instructions into low-level actions without decoupled perception, language, and control modules (Fig. 2a).Two mainstream paradigms have emerged within this framework: auto-regressive and diffusion-based action generation.Through large-scale pretraining, these models acquire broad capabilities that support generalization across tasks.However, efficient adaptation to real-world settings remains a significant challenge.</p>
<p>Autoregressive VLA Models.Autoregressive VLA models typically process language and visual inputs, employing various tokenization strategies to convert multimodal data into a unified latent space.Then the transformer-based decoder generates actions step-by-step in an autoregressive manner conditioned on the input context and previously generated actions, allowing structured action generation and planning.</p>
<p>Building on the previous groundbreaking models such as RT-1 (Brohan et al., 2023), RT-2 (Zitkovich et al., 2023) and VIMA (Jiang et al., 2023), Open-VLA (Kim et al., 2024) emerges as an important open-source method, combining a fine-tuned Llama 2 (7B) model with DinoV2 and SigLIP for visual tokenization, pretrained on the Open-X-Embodiment dataset (Collaboration et al., 2024) consisting of 970k real-world robot demonstrations.TraceVLA (Zheng et al., 2025) improves Open-VLA with visual trace prompting to enhance spatiotemporal awareness.Emma-X (Sun et al., 2024) further refines dataset quality using a trajectory segmentation and Embodied Chain-of-Thought reasoning.This work is followed by NORA (Hung et al., 2025), which uses Qwen-2.5-VL-3B as backbone and pretrained on the Open-X-Embodiment dataset.In parallel, other efforts integrate tactile sensing into robot perception (Yang et al., 2024;Zhao et al., 2024), supporting more generic robot policies.Other recent developments include distilling spatial representations from VLMs, e.g., Gemini-Robitics (Gemini Robotics Team, 2025), and enriching training with additional features, such as 3D spatial relationships in SpatialVLA (Qu et al., 2025), task-centric latent space in UniVLA (Bu et al., 2025), and integrating multimodal understanding with action prediction in ChatVLA (Zhou et al., 2025) and UP-VLA (Zhang et al., 2025).</p>
<p>Diffusion-based VLA Models.Diffusion-based VLA models formulate action generation as a denoising process over latent trajectories.Given a noisy version of a full action sequence, the model learns to recover the true trajectory conditioned on language and vision.</p>
<p>Diffusion Policy (DP) (Chi et al., 2023) pioneered the use of diffusion model for visuomotor policy representation, laying the foundation for subsequent multimodal approaches.Octo (Team et al., 2024) uses conditional diffusion decoding for action sequence prediction.Rather than lightweight diffusion heads, (Reuss et al., 2024;Wen et al., 2024;Li et al., 2024b) use larger and more dedicated diffusion policy modules the as action decoder.DTP (Fan et al., 2025) introduces a trajectory-level guidance to enhance diffusionbased planning.Recent works such as π 0 (Black et al., 2024) and π 0.5 (Intelligence et al., 2025) integrate a pretrained VLM with a flow-matchingbased action expert to model action distributions.These models often utilize FAST (Pertsch et al., 2025) for efficient compressed action tokenization.HybridVLA (Liu et al., 2025) unifies diffusion and autoregressive action prediction within a single LLM-based framework.A growing focus is placed on adapting VLA models to diverse embodiments including bimanual manipulation and humanoid robots, such as RDT-1b (Liu et al., 2024b), DexVLA (Wen et al., 2025) and GR00T N1 (Bjorck et al., 2025).</p>
<p>Strengths and Limitations.VLA models provide a unified, end-to-end framework for robotic manipulation, (i) seamlessly integrating visual, language, and action modalities.Leveraging largescale pretraining and fine-tuning, these models exhibit (ii) strong potential for generalizing across diverse manipulation tasks and robotic embodiments.However, their performance is constrained by the (iii) limited availability of high-quality, diverse robotic datasets.Pretraining can also intro-duce biases from training distributions, leading to (iv) degraded performance in out-of-distribution scenarios, such as novel tasks or different robotic embodiments.Therefore, despite their potential, further work is needed to enhance their robustness and generalization across real-world settings, for example, efficient adaptation using few-shot data.</p>
<p>Modular Vision-Language Pipelines</p>
<p>Definition.In a modular vision-language pipeline (Fig. 2b), perception is handled by a specialist vision language model (VLM) that outputs symbolic scene information, typically grounded 2-D / 3-D bounding boxes, segmentation masks, or referring expression pointers.A downstream planner or policy module then consumes this structured representation to generate low-level actions.The language channel is therefore disentangled from motor control, allowing each module to be tuned independently, thus preserving the transparency and plug-and-play advantages of classical planning.</p>
<p>Representative systems.Language-promptable specialist VLMs endow modular stacks with zeroshot semantics for various robotics pipelines.(Bandyopadhyay et al., 2024) demonstrates an end-to-end sample collection robot system that uses GroundingDINO (Liu et al., 2024a) to localize objects and refines each box with SAM (Ravi et al., 2025) masks before passing them to classical grasp-and-place controllers, illustrating this paradigm's practicality in real deployments.(Werby et al., 2024) aggregates these modules into a floor-room-object hierarchy, showcasing their usage in long-horizon language-conditioned navigation across multi-story buildings.</p>
<p>Strength and Limitations.Modular VLM pipelines strike a balance between transparency and adaptability, and delivers practical benefits: (i) interpretability-detections can be inspected; (ii) lightweight-the model parameters are usually around 100M∼600M, approximately 1% ∼ 6% the size of LLaMA 3.2 Vision 11B (Grattafiori et al., 2024).On the other hand, it is limited at (i) interaction rigidness compared with more flexible multimodal LLMs, and (ii) pipeline brittleness where perception errors propagate without mitigation (Fig. 2b; Table 1).Their success hinges on robust open-vocabulary grounding-precisely the capability our Instruction Grounding case study stresses in Section 3.</p>
<p>Multimodal LLM Agents as Orchestrators</p>
<p>Definition.Multimodal LLM agents place a large, tool-calling language model at the centre of the control loop (Fig. 2c).The LLM receives raw user utterances, selectively invokes vision tools (e.g., a detector or depth estimator) via function calls, reasons over their outputs in-context, and finally issues high-level action primitives to a lowlevel controller.The agent therefore acts as a cognitive hub that binds perception and control through natural language.</p>
<p>Representative Systems.Multimodal LLMs are taking increasingly important roles in robotics.Gemini Robotics (Gemini Robotics Team, 2025) integrates perception, spatial reasoning, and trajectory synthesis into one Gemini-2.0backbone (Google DeepMind, 2024), which serves as the embodied brain.(Li et al., 2024c), in the same vibe as Gemini Robotics, leverages the inherent common sense and reasoning capabilities of these models by fine-tuning adapter modules through a chain-ofthought training paradigm.It endows the model with accurate pose prediction and precise manipulation abilities.These works collectively show the trend that the multimodal LLM shifts to the "cognitive hub" in robot systems.(Glocker et al., 2025) build a modular agent-orchestration system for household object management robots.It utilize Llama 3.2 Vision (Grattafiori et al., 2024) for open-vocabulary perception to facilitate creating grounded task plans, while the limitations of the multmodal LLM were not discussed.</p>
<p>Somewhat similar to our work, (Li et al., 2024a) investigates the eligibility of Multimodal LLMs to serve as the "brain" for in-home robotics by providing a benchmark to compare models along the axes of perception, visual reasoning and task planing.Models like GPT-4V, Qwen-VL (Bai et al., 2025) and DeepSeek-VL (Lu et al., 2024) were included, but more recent releases were not covered-likely due to the fact that the field is moving fast with new models emerging in rapid succession.</p>
<p>Strengths and Limitations.Multimodal LLM agents excel in (i) visual commonsense reasoning, leveraging extensive language priors to generalize to novel concepts beyond the reach of most specialist VLMs, and (ii) instruction following with support for fine-grained visual understanding and dynamic planning.Despite their expressive power, however, these models are (iii) resource-intensive, posing challenges for deployment-particularly on mobile robotic platforms.</p>
<p>Case Studies on Instruction Grounding</p>
<p>Natural language instruction grounding involves translating user intents into clear, actionable goals in a visual scene, which is a key capability for embodied AI (Gemini Robotics Team, 2025).Our case study offers empirical insights into the grounding performance of various models through the lens of challenging cross-modal disambiguation, and further examines the trade-offs introduced by model sizes and quantization-providing practical suggestions for efficient deployment.</p>
<p>Benchmark Dataset.To minimize the impact of vision priors on measured performance, we design benchmarking scenarios using household objects placed on a tabletop.These objects are commonly represented in the training datasets of the foundation models, and the tabletop setup features minimal variation in lighting and camera angles-ensuring that the evaluation primarily reflects grounding capabilities.We curated a new Instruction Grounding benchmark dataset.In images containing multiple household objects, each object is tagged with a number as the visual prompt, and each image is paired with language instructions crafted to test visual commonsense and cross-modal disambiguation concerning attribute or spatial relationships -for "pick up the red-capped marker," the color must be used to select one among a few markers; whereas "grasp  the cup in front of the screwdriver" requires reasoning over spatial relations (Fig. 3; Appendix).Language ambiguity often leads to execution failure in an embodied system.By comparing specialist VLMs and multimodal LLMs, we reveal their concrete failure modes that further inform our design implications in Sec. 5.</p>
<p>Zero-Shot Object Grounding.We begin with a foundational question for instruction grounding: Can FMs accurately recognize objects in cluttered open scenes?Table 2 presents the performance hierarchy for specialist VLMs and a range of multimodal LLMs, serving as a basis for deeper analysis of ambiguity resolution in later sections.of Gemini 2.5's score, while the more recent Llama 4 releases did not outperform it.
V L M -L L M Q w e n 2 -V L -7 2 B L l a m a -3 . 2 V -9 0 B L l a m a -3 . 2 V -9 0 B -Q 4 L l a m a -3 . 2 V -1 1 B L l a m a -3 . 2 V -1 1 B -Q 4 0.
• Smaller community models (Gemma-27B, Phi-Vision) fall below the specialist-threshold, suggesting that they are still inadequate for finegrained grounding in cluttered scenarios.</p>
<p>• Last, when compute is a bottleneck, GPT-4omini (0.71) and Llama 3.2-Vision 11B (0.57) provide the best speed-accuracy trade-off, delivering decent performance without incurring heavy memory footprint or high API costs.</p>
<p>Zero-Shot Complex Instruction Grounding.This task is framed as a multiple-choice problem, where the model is asked to select the correct object index in a cluttered scene based on three types of natural language instructions: implicit, attributebased, and relationship-based-each type probes a distinct grounding challenge.We evaluate a series of multimodal LLMs, using a modular VLM-LLM pipeline as a baseline.In this pipeline, the LLM parses the instruction to infer likely targets, queries GroundingDINO to detect candidate objects, and selects from the detected boxes-essentially guessing without directly perceiving the scene.</p>
<p>• Implicit Instruction Grounding.Instructions like "I need a tool to tighten the screws" only refer to the target object implicitly, and the model needs to infer the target object using its common sense priors.For such instructions, the modular VLM-LLM pipeline struggles to select a screwdriver, lacking embedded affordance reasoning.In contrast, multimodal LLMs perform well, reflecting strong visual commonsense.GPT-4.5 demonstrates exceptional performance (0.94), though its high inference cost-20× that of Gemini 2.5 makes it cost-prohibitive for most applications.</p>
<p>• Relational Reasoning Remains Challenging.This category requires resolving referential ambiguity through implicit chain-of-thought reasoning: grounding objects, modeling spatial relationships, and disambiguating targets (e.g., identifying the correct mug among many based on "next to something").Accuracy drops significantly nearly across all models.Only Gemini 2.5-Pro and o4-mini achieve accuracy above 0.80-the former likely benefits from embodied training data, while the latter demonstrates strong reasoning capabilities.Notably, o4mini is a medium-sized model, yet it outperforms larger models like GPT-4.5 on relational instructions-suggesting that structured reasoning may help close, or even overcome the performance gap brought by different model scales.</p>
<p>• Instruction-Dependent Quantization Effects.</p>
<p>INT4 quantization reduces the model size by over 70%, making it an attractive choice for deployment.In Llama 3.2 Vision, we observe that it disproportionately impacts implicit and relational instruction grounding, indicated by the relative accuracy drop of 14% − 17%, while attribute grounding is more robust with only 4% loss.Despite reduced precision, quantized 11B models offer a speed-accuracy balance for lowresource settings.Our findings underscore the need for fine-grained quantization strategies that preserve the most important high-level reasoning capabilities under resource constraints.Table 3: Success rates (%) on the LIBERO Simulation Benchmark across four task suites, each evaluated over 500 trials.Results for SpatialVLA are from (Qu et al., 2025); Results for π 0 are from (Black et al., 2024), using pretrained models on LIBERO benchmarks."AC" denotes the use of action chunking.The comparison in the Appendix highlights its impact on performance.The finetuned π 0 model achieves the highest performance.</p>
<p>Case Studies on Robotic Manipulation</p>
<p>Now we shift the focus to skill adaptation.In an ideal deployment scenario, a pretrained VLAalready endowed with broad visuomotor skillsshould be retargeted to a new manipulation task with minimal data and fast convergence.We use fine-tuning, the standard practice for adaptation, as a probing lever to evaluate how the state-of-the-art VLA models adapt to new tasks and deployment conditions.</p>
<p>Given the scale of VLAs, we compare partial fine-tuning, which leverages our benchmark dataset (Appendix) and its inherent distribution bias to study convergence behavior, and full finetuning, which uses large-scale datasets to minimize the training loss.Our evaluation focuses on three key aspects: (i) training dynamics-how quickly and smoothly training converges; (ii) generalization-how well the resulting policies perform on various tasks; and (iii) robustness-how well the resulting policies handle environmental distractors.Our experiments highlight the performance of VLA models in different settings, offering practical suggestions for practitioners who have to adapt large VLAs under tight data, time and compute budgets.</p>
<p>Skill Adaptation Performance.Our fine-tuning process consists of two stages: (1) To assess convergence behavior under distribution shift, we collected a custom dataset (see Appendix for details) with a distribution bias relative to common pretraining datasets included in the Open-X-embodiment (Collaboration et al., 2024) and LIBERO datasets (Liu et al., 2023).We used it to partially finetune several recent VLA models and trained Diffusion Policy (DP) and Action Chunking Transformer (ACT) from scratch.The results are shown in Fig. 5; (2) For full fine-tuning, we leveraged larger benchmark datasets, Open-X-embodiment and LIBERO, to fully fine-tune RT-1, OpenVLA, SpatialVLA and NORA, and compared their performance.The results are shown in Fig. 6.</p>
<p>• Partial Fine-tuning.Through the experiments we observe that DP and ACT exhibit high stability with low variance during training.In contrast, generalist models such as OpenVLA and π 0 require significantly more training iterations to attain comparable accuracy and exhibit greater variance, which can be attributed to their large model capacity.Notably, although DP achieves lower loss by fitting directly to noise, it still demands more training steps to generate coherent actions, even after loss convergence.</p>
<p>• Full Fine-tuning.These fully fine-tuned VLA models are evaluated on three tasks: (1) out-ofdistribution (OOD) object manipulation, (2) spatial relationship reasoning and (3) multi-object pick-and-place tasks.In task (1), both NORA and OpenVLA succeed, while SpatialVLA fails due to incorrect affordance point estimation.</p>
<p>In task (2), NORA correctly follows instructions, while OpenVLA fails and SpatialVLA exhibits unstable performance.In task (3), NORA achieves successful execution while other models fail to complete the task reliably.</p>
<p>Sim2Real Adaption Performance.We compare model performance on simulation benchmarks and real robot deployments.A significant drop in performance is observed during transfer from simulation to the real world (Table 3; Appendix Table 6).The simulation benchmark includes 30 procedurally-generated, disentangled tasks requiring nuanced spatial reasoning (LIBERO-Spatial), object understanding (LIBERO-Object), and goal interpretation (LIBERO-Goal), as well as 10 longhorizon, entangled tasks (LIBERO-Long).Robustness to Perturbations.To evaluate robustness, we introduced distractor objects into the environment.As shown in Table 8, both Open-VLA and NORA exhibit substantial performance degradation in the presence of these perturbations, highlighting their sensitivity to novel conditions.</p>
<p>Key Takeaways.Current VLA models still face significant limitations in the following areas:</p>
<p>• Adaptation and Generalization.A generic robotic policy is expected to quickly adapt to datasets with distributional shifts.However, according to the partial fine-tuning results, due to the large model capacities and the limited size of task-specific datasets, these VLA models failed to achieve fast adaptation.While full fine-tuning offers improved performance, it requires extensive data and long training time, which are impractical for many real-world scenarios.</p>
<p>• Robustness.Robustness to distribution shifts (without finetuning) is a critical challenge.Results reveal substantial performance degradation both when encountering unseen objects and during sim-to-real transfer, highlighting the fragility of current VLA models in dynamic and unpredictable environments.These findings suggest that while VLA models hold promise, they have limitations in data efficiency, adaptation speed, and robustness to make them reliable for real-world robotic applications.</p>
<p>Constraints and Future Directions</p>
<p>Despite the promise of foundation models for enabling embodied agents to perform daily tasks, the following critical constraints still hinder their widespread deployment: Data Scarcity.In contrast to natural language datasets, which are readily sourced from internet, robotic datasets are significantly more expensive due to high hardware costs and intensive labor during data acquisition.A promising direction for future research is developing more data-efficient models.In addition, exploring high-fidelity simulation environments and developing robust sim-to-real transfer techniques could mitigate data scarcity.</p>
<p>Limited Generalization Capability.</p>
<p>A key limitation of current VLA models is their limited ability to generalize to out-of-distribution concepts that were not well-represented during training, which is a consequence of the aforementioned data scarcity.Many models depend heavily on large-scale paired datasets, which often exhibit biases and limited diversity in aspects such as camera viewpoints, lighting conditions and specific robotic embodiments.This results in fragile performance when deployed in real-world or domain-specific scenarios.Furthermore, these models struggle with fine-grained spatial reasoning and temporal understanding, hindering them from accurately aligning language with complex visual scenes or dynamic events.</p>
<p>Efficient Inference.Deploying large-scale models on robotic platforms introduces significant computational challenges, primarily in inference speed and GPU RAMs.This underscores the importance of smaller models that can efficiently generate actions without significant performance degradation.This issue is particularly pronounced in autoregressive models, and diffusion models are less affected.</p>
<p>Limitations</p>
<p>When evaluating the generalization capabilities of Vision-Language Alignment (VLA) models, this paper primarily focuses on different tasks without extensively addressing their generalization across varying robot morphologies.Although relatively few works have specifically targeted this aspect, it remains a significant challenge in deploying VLA models in real-world applications.Robots with different morphologies, such as bimanual manipulators, humanoid robots, and autonomous vehicles, require distinct operational protocols and safety considerations.The absence of a generic robot policy that can adapt seamlessly across diverse morphologies limits the practical generalization potential of VLA models and hinders their deployment as universal robotic policies.</p>
<p>In addition, this paper does not explore the ability of VLA models to ground instructions involving open-ended or ambiguous commands.Current VLA models are largely trained on curated datasets, which allow them to learn mappings from instructions to specific actions.However, this reliance constrains their ability to truly understand instructions at the semantic level.As a result, when facing out-of-distribution or vague commands, these models often struggle to infer reasonable actions.Addressing this limitation will require integrating more advanced instruction-understanding modules into the VLA pipeline to improve their robustness in handling ambiguous or under-specified input.• Implicit Instructions.Here, objects are not explicitly mentioned by name or attributes but are instead described by their functions.This category evaluates the VLMs' ability to infer the correct object based on its use.For example, the dataset includes instructions referring to objects like scissors, screwdrivers, and rulers based on their respective functions.</p>
<p>• Explicit Attributes.In this category, instructions prompt VLMs to identify objects belonging to a category with multiple instances, where each instance can be uniquely identified by explicitly mentioned attributes.In Fig. 7, the beige mug and the gray mug are included because they are unique when described with attributes.However, objects like the black mug or scissors are excluded.This is because there are two identical black mugs, making them non-unique, and there is only one pair of scissors, which does not require attributes for identification.</p>
<p>• Explicit Relationships.In this category, instructions describe objects by their spatial relationships to other objects in the image.We ensure that each referenced object is unique within the image.For example, the measuring cup to the right of the screwdriver uniquely identifies the object.These instructions are designed to test the VLMs' ability to comprehend and resolve location-based relationships.</p>
<p>• Multi-Referent Instructions.This category includes instructions that correspond to multiple valid objects in the scene.For example, in Fig. 7, an instruction like "give me a mug" may refer to several similar items.In such cases, we annotate the data with all candidate object indices, e.g., [2,7,18], indicating the set of plausible referents.A human-in-the-loop process was employed to ensure high-quality data collection.</p>
<p>• Initial Object Identification: We used GPT-4o to identify objects in an image and referring them by type, explicit attributes, and detailed location relations.</p>
<p>• Human Verification.The authors of this paper reviewed and modified the outputs to ensure their correctness.</p>
<p>• Instruction Generation.After verification, GPT-4 was tasked with generating simple, clear instructions for different objects.</p>
<p>• Final Review.These instructions underwent another round of verification to ensure clarity and accuracy.</p>
<p>This high-quality dataset consisting of 473 in-  structions, with a detailed breakdown of each instruction type presented in Fig. 8.</p>
<p>B Grounding Experiments</p>
<p>B.1 Complex Instruction Grounding for Goal Specification</p>
<p>Cross-modal Disambiguation represents a particularly challenging component of goal specification.</p>
<p>To quantify the model capability in this dimension, we employed attribute-based and relative relationship instructions to uniquely identify a target among multiple candidates.The goal specification task is formulated as follows: Given a visual input I ∈ R H×W ×3 and an instruction t, the objective is to predict the target object according to o * = arg max o∈O P (o | I, t; θ F M ), where o ⋆ ∈ O and O denotes the set of candidate objects.Models are evaluated using macro-average accuracy metric.</p>
<p>B.2 Failure Cases of Specialist VLM Pipeline</p>
<p>Grounding DINO, despite popular for zero-shot detection, is not robust in open scenes.It successfully detected "blue ball" while failed to detect "ball", indicating its reliance on visual features.Similarly, featureless metal cans pose a great challenge for Grounding DINO, which were almost omitted in the detection results.</p>
<p>For complex instruction grounding, Grounding DINO and GPT-4 were chained together to "guess" the target by the LLM based on the candidate bounding boxes.The failure cases were illustrated in the Fig. 9 and Fig. 10.</p>
<p>B.3 Multimodal LLMs Performance</p>
<p>The performance of Multimodal LLMs on complex grounding across EASY, MEDIUM and HARD groups are shown in Table 5.</p>
<p>C Manipulation Experiments</p>
<p>OpenVLA and π 0 were partially fine-tuned on this dataset, while Diffusion Policy (DP) and Action Chunking Transformer (ACT) were trained from scratch.Due to the limited size of our custom dataset, full fine-tuning of RT-1, OpenVLA, SpatialVLA, and NORA was performed using the Open-X-embodiment and LIBERO datasets.We evaluated model performance on both a real-world WidowX robotic platform and the LIBERO simulation benchmark.</p>
<p>C.1 Fine-tuning details for VLA Partial fine-tuning was conducted on a single NVIDIA A6000 GPU (48 GB VRAM) over a period of three days.To ensure a fair comparison, a batch size of 1 was used across all models.The results are presented in Fig. 5. Full fine-tuning of RT-1, OpenVLA, Spa-tialVLA, and NORA was conducted on a compute node equipped with 8×H100 GPUs.The fine-tuned models were evaluated on 9 diverse realworld manipulation tasks, as shown in Fig. 11.Success rates are summarized in Table 6, demonstrating NORA's superior policy generation capabilities across three task categories: out-of-distribution object grasping, spatial reasoning, and multi-object manipulation.</p>
<p>C.2 Impact of Action Chunking</p>
<p>C.2.1 Action Chunking Performs on WidowX.</p>
<p>To investigate the effectiveness of action chunking, we selected NORA-LONG and SpatialVLA for evaluation.Tasks were chosen from three categories: (1) "put the carrot in the pot," (2) "put the red bottle and hamburger in the pot," and (3) "put the pink toy at the right corner."In initial experiments, all predicted actions (5 actions for NORA-LONG, 4 actions for SpatialVLA) were executed sequentially without replanning.This frequently caused the WidowX robot to crash into the environment due to the accumulation of overly large movements.</p>
<p>Subsequently, we modified the execution policy to only perform the first action in each predicted chunk.This adjustment resolved the collision issue and NORA -LONG achieved an 80% success rate on the "put the carrot in the pot" task.However, on multi-object pick-and-place tasks, NORA-LONG consistently stopped after placing the first object, resulting in a 0% final success rate.For the spatial reasoning task, NORA-LONG achieved a 70% success rate on "put the pink toy at the right corner."C.2.2 Action chunking improves performance in simulation.</p>
<p>We hypothesize that action chunking is more effective at higher control frequencies.For example, Diffusion Policy generates commands at 10 Hz, which are then interpolated to 125 Hz for execution.Similarly, OpenVLA-OFT+ employs action chunking and shows improved performance in realworld ALOHA tasks, which run at 25 Hz.Since our real robotic platforms do not support high-frequency control, we tested this hypothesis in the LIBERO simulation environment (20 Hz).We fine-tuned both NORA and NORA-LONG on this benchmark with an action chunk size of 5, producing two variants: NORA-finetuned-AC and NORA-Long-finetuned.</p>
<p>Results show that NORA-finetuned-AC significantly outperforms NORA-finetuned across all LIBERO benchmarks, with a higher average success rate.Notably, NORA-Long-finetuned outperforms all baseline models (see Table 3), highlighting the benefits of pretraining with action chunking and its transferability to long-horizon tasks.However, it is important to note that LIBERO is a simulation environment and may not reflect real-world performance at high control frequencies.</p>
<p>C.3 Robustness to Disturbance</p>
<p>To evaluate robustness, we selected three straightforward tasks (shown in Fig. 12) and introduced distractor objects into the environment.Initially, both OpenVLA and NORA performed well.However, their success rates declined significantly with the introduction of distractions.This highlights the sensitivity of current VLA models to out-of-distribution disturbances.The average success rates across the three tasks are presented in Table 8, while the detailed number of successful executions out of 10 trials is summarized in Table 7.</p>
<p>D Modular Claw Machine Prototype</p>
<p>To facilitate the evaluation of different VLMs in robotic manipulation, we developed a voice-  SpatialVLA (Qu et al., 2025): A VLA model focused on spatial understanding for robot manipulation, incorporating 3D information such as spatial movement.It learns a generalist policy for spatial manipulation across diverse robots and tasks.SpatialVLA predicts four actions at a time.</p>
<p>TraceVLA (Zheng et al., 2024)    As shown in Table 2, NORA-LO N G achieves the highest average success rate (87.9%) across all methods, demonstrating strong generalization in both short-and long-horizon scenarios.Among the fine-tuned baselines without action chunking , OpenVLA achieves the best average (76.5%).NORA, demonstrates comparable performance to OpenVLA in spatial, object, and goal-related tasks, but it falls short in long-horizon scenarios.</p>
<p>Notably, when both NORA variants are fine-tuned with action chunking, there is a significant increase in the LIBERO-Long success rate, emphasizing the importance of action chunking for long-horizon tasks.</p>
<p>NORA-LO N G especially excels on LIBERO-Long, achieving a success rate of 74.6%, showcasing its ability to reason over extended temporal windows.These results highlight the effectiveness of our model in adapting to new environments and reinforce the utility of windowed training for long-horizon policy generalization.". 9</p>
<p>Figure 12: Comparison of tasks with and without distraction.</p>
<p>controlled testbed using a UR5 robotic arm 5 .The system architecture, shown in Fig. 13, comprises the following five modules:</p>
<p>• Speech Transcription: Powered by Microsoft Azure's speech recognition service.</p>
<p>• Task Decomposition: Based on GPT-3.5 and GPT-4 using prompting paradigms adapted from ChatGPT for Robotics.</p>
<p>• Object Detection: Utilizes GroundingDINO and OWL-ViT for object detection.</p>
<p>• Object Segmentation: Employs Segment Anything Model (SAM) and FastSAM for segmenting detected objects.</p>
<p>• Manipulation: Low-level actions are generated by GraspAnything or GraspNet.This modular testbed enables rapid integration and benchmarking of different models within a real robotic system.</p>
<p>Figure 2 :
2
Figure 2: Foundation model integration strategies in language driven robots: (a) End-to-end VLA models, (b) Modular VLM pipelines, and (c) Multimodal LLM agents.Each strategy reflects a distinct interface between language, perception, and control.</p>
<p>Implicit 7 :Figure 3 :
73
Figure 3: Experimental setup for two case studies in a cluttered tabletop environment.The top row shows egocentric video data collected for the manipulation case study.The bottom row is an example setup for the instruction grounding task, including an annotated visual prompt paired with complex instructions in three forms: implicit, explicit with attributes and spatial references.</p>
<p>Figure 4 :
4
Figure 4: Performance of complex instruction grounding across VLM-LLM pipelines and end-to-end multimodal LLMs.Macro accuracy is reported across instruction types-implicit, attribute-based, and relationship-based.Subfigures show (a) proprietary models and (b) open-source models along with their Int4-quantized variants.</p>
<p>Figure 5 :
5
Figure 5: Results for partial fine-tuning of VLA models including OpenVLA and π 0 , alongside results from training Diffusion Policy and ACT (Action Chunking Transformer) from scratch on our dataset.VLA models require more training epochs to converge and exhibit higher variance in performance.</p>
<p>Figure 6 :
6
Figure6: Success rates of fully fine-tuned VLA models on out-of-distribution object manipulation (OOD Object), spatial relationship reasoning (Spatial) and multiobject pick-and-place (Multiple) tasks.NORA achieves the highest performance.</p>
<p>Figure 7 :
7
Figure 7: Example of visual prompts</p>
<p>Figure 8 :
8
Figure 8: Dataset breakdown by Instruction Types.</p>
<p>Figure 9 :
9
Figure 9: Examples of Instruction Grounding.(a) "the marker on the left", (b) "the marker aligned with the ruler".</p>
<p>Figure 10 :
10
Figure 10: Examples of Object Grounding.(a) "ball", (b) "screwdriver", (c) "marker pens", (d) "blue ball".</p>
<p>Preprint move the banana close to the pan put the red bottle and the hamburger in the pan put the carrot and hotdog in pot put the corn and carrot in pan put the pink toy at the right corner put carrot in pot put the blue cube on the right plate put banana in pot put the blue cube on the plate</p>
<p>Figure 3 :
3
Figure3: Real-world robot environments and task setups.We evaluate NORA across 9 diverse tasks to assess its instruction understanding, spatial reasoning, and multi-task motion planning capabilities.</p>
<p>Figure 11 :
11
Figure11: Real-world robot environments and task setups.We evaluate these models across 9 diverse tasks to assess its instruction understanding, spatial reasoning, and multi-task motion planning capabilities.</p>
<p>Figure 5 :
5
Figure 5: Comparison of tasks with and without distraction.</p>
<p>5 https://github.com/HRItdy/claw_machineComparison of task performance between OpenVLA and NORA under conditions with and without distraction.Each value denotes the number of successful executions out of 10 trials.</p>
<p>Figure 13 :
13
Figure 13: The system architecture of the testbed for VLMs.</p>
<p>Table 1 :
1
Comparison of foundation model integration strategies in embodied robotic systems, highlighting differences in instruction grounding, manipulation generalization, and adaptation methods.
Pipelines for Robot SystemsInstruction Grounding Visual inputs Multi-round dialogue CoT reasoningManipulation Generalization Adaptation for Deployment Morphology independent Skill sets Data EfficiencyEnd-to-End VLA ModelsWide rangeData-hungry finetuningModular VLM pipelinesController-Cheap finetuningMultimodal LLMs AgentsspecificIn-context learning</p>
<p>Table 2 :
2
Object grounding performance of specialist VLMs and multimodal LLMs (closed-source and opensource) across varying scene complexity levels.Models are evaluated on easy, medium, and hard cluttered scenes, with macro accuracy reported.</p>
<p>Table 5 :
5
Performance on the complex instruction grounding task.Abbreviations: im denotes implicit instruction, attr denotes attribute-based instruction, and rel denotes relation-based instruction.
ModelimEasy attrrelimMedium attrrelimHard attrrelVLM-LLM Gemini-2.5-Pro Gemini-2.0 GPT-4.5 GPT-4o o4 4o-mini GPT-4V Qwen2-VL LLaMA 3.2 Vision 90B LLaMA 3.2 Vision 90B-Q4 0.800 0.667 0.598 0.625 0.719 0.554 0.542 0.464 0.300 0.050 0.516 0.131 0.010 0.336 0.186 0.000 0.318 0.174 0.778 0.889 0.830 0.847 0.814 0.815 0.985 0.784 0.858 0.833 0.889 0.774 0.819 0.721 0.642 1.000 0.668 0.469 0.944 0.889 0.894 0.917 0.838 0.722 0.958 0.719 0.698 0.850 1.000 0.778 0.819 0.948 0.680 0.901 0.697 0.469 0.950 1.000 0.907 0.847 0.988 0.837 0.917 0.809 0.804 0.750 0.717 0.550 0.764 0.771 0.596 0.750 0.382 0.248 0.650 0.750 0.598 0.750 0.737 0.662 0.625 0.417 0.455 0.800 0.917 0.830 0.792 0.756 0.738 0.875 0.700 0.529 0.750 0.850 0.704 0.708 0.853 0.711 0.875 0.491 0.521 LLaMA 3.2 Vision 11B 0.650 0.667 0.631 0.764 0.710 0.556 0.833 0.536 0.342 LLaMA 3.2 Vision 11B-Q4 0.650 0.567 0.502 0.694 0.757 0.555 0.542 0.498 0.450</p>
<p>Table 6 :
6
Task performance comparison across different categories and models.
Preprint</p>
<p>Table 2 :
2
(Qu et al., 2025)lts (% success rate) of NORA and baselines on LIBERO Simulation Benchmark.Each method is evaluated on four task suites over 500 trials.Fine-tuned NORA-Long achieves the best overall performance.Results marked with * are from SpatialVLA(Qu et al., 2025).AC indicates the use of action chunking strategy.ModelsLIBERO-Spatial LIBERO-Object LIBERO-Goal LIBERO-Long Average
OpenVLA fine-tuned  *  TraceVLA fine-tuned  *  NORA-fine-tuned (Ours)84.7 84.6 85.688.4 85.2 87.879.2 75.1 7753.7 54.1 4576.5 74.8 73.9SpatialVLA fine-tuned-AC  *  NORA-fine-tuned-AC (Ours) NORA-Long-fine-tuned (Ours)88.2 85.6 92.289.9 89.4 95.478.6 80 89.455.5 63 74.678.1 79.5 87.9put banana in potput carrot in potput the blue cube on the plateWithout distractionWith distraction</p>
<p>Table 8 :
8
Average Success Rate (%) without (w/o) and with (w/) Distractors
Modelw/o Distractors w/ DistractorsOpenVLA56.750NORA83.356.7
https://github.com/HRItdy/claw_machine
https://huggingface.co/datasets/bittdy/pick_screw
https://github.com/xiuchao/InstructionGrounding
Appendix A Benchmark Dataset A.1 Cluttered Tabletop Manipulation DatasetTo evaluate the finetuning behavior of various VLA models under distribution shift, we constructed a custom cluttered tabletop environment using a UR5 robotic arm with a wrist-mounted RealSense RGB-D camera.This setup differs from all existing configurations in the Open-X-Embodiment dataset.Demonstrations for a screwdriver-picking task-amid distractor objects-were collected via teleoperation using a SpaceMouse device.In total, we gathered 163 demonstration episodes 3 .Each episode began with a randomized initial robot pose, followed by an attempt to grasp the target screwdriver.A.2 Complex Instruction Grounding DatasetWe curated an evaluation dataset for the complex instruction grounding task in cluttered scenes 4 .Thirty images were sampled from the action sequences and subsequently categorized based on the number of objects: EASY (&lt;15), MEDIUM (15∼20), and HARD (&gt;20).Objects in the visual scenes were manually annotated using visual prompts and paired with various instructions.The spatial relationship words were illustrate in Table4.For the complex instruction grounding task, including an annotated visual prompt paired with complex instructions in three forms: implicit, explicit with attributes and spatial references.Additionally, the dataset includes multi-turn questions that refer to more than one object, enabling foundation models to ask clarifying questions to identify the correct object.
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, 10.48550/arXiv.2502.13923arXiv:2502.13923Yiheng Xu, and 8 others. 2025. Qwen2.5-VL Technical Report. Preprint</p>
<p>Demonstrating Event-Triggered Investigation and Sample Collection for Human Scientists using Field Robots and Large Foundation Models. Tirthankar Bandyopadhyay, Fletcher Talbot, Robotics: Science and Systems. 1 others. 2024</p>
<p>Gr00t n1: An open foundation model for generalist humanoid robots. Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, arXiv:2503.14734arXiv:2410.24164Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi1 others. 2025arXiv preprintand 5 others. 2024. π 0 : A vision-language-action flow model for general robot control</p>
<p>Rt-1: Robotics transformer for realworld control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, J Nikhil, Ryan Joshi, Dmitry Julian, Kalashnikov, Proceedings of Robotics: Science and Systems. Robotics: Science and Systems202332</p>
<p>Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, Hongyang Li, arXiv:2502.14420Learning to act anywhere with task-centric latent actions. 2025arXiv preprint</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, Shuran Song, The International Journal of Robotics Research. 2023</p>
<p>Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, and 275 others. Abby O' Neill, Embodiment CollaborationAbdul Rehman, Embodiment CollaborationAbhinav Gupta, Embodiment CollaborationAbhiram Maddukuri, Embodiment CollaborationAbhishek Gupta, Embodiment CollaborationAbhishek Padalkar, Embodiment CollaborationAbraham Lee, Embodiment CollaborationAcorn Pooley, Embodiment CollaborationAgrim Gupta, Embodiment CollaborationAjay Mandlekar, Embodiment CollaborationAjinkya Jain, Embodiment CollaborationAlbert Tung, Embodiment CollaborationAlex Bewley, Embodiment CollaborationICRA. 2024Open xembodiment: Robotic learning datasets and rt-x models</p>
<p>Quantao Shichao Fan, Yajie Yang, Kun Liu, Zhengping Wu, Qingjie Che, Min Liu, Wan, arXiv:2502.10040Diffusion trajectory-guided policy for long-horizon robot manipulation. 2025arXiv preprint</p>
<p>Gemini Robotics: Bringing AI into the Physical World. 10.48550/arXiv.2503.20020arXiv:2503.200202025Preprint</p>
<p>LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics. Marc Glocker, Peter Hönig, Matthias Hirschmanner, Markus Vincze, 10.48550/arXiv.2504.21716arXiv:2504.217162025Preprint</p>
<p>Gemini: Our largest and most capable ai models yet. Google Deepmind, 2024</p>
<p>Alan Schelten. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, 10.48550/arXiv.2407.21783Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvarkand 542 others. 2024. The Llama 3 Herd of Models</p>
<p>Denoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in neural information processing systems. 202033</p>
<p>Nora: A small open-sourced generalist vision language action model for embodied tasks. Chia-Yu Hung, Qi Sun, Pengfei Hong, Amir Zadeh, Chuan Li, U-Xuan Tan, Navonil Majumder, Soujanya Poria ; Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, arXiv:2504.19854arXiv:2504.16054Physical Intelligence. 2025arXiv preprintand 17 others. 2025. π 0.5 : a vision-language-action model with open-world generalization</p>
<p>Vima: robot manipulation with multimodal prompts. Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, Linxi Fan, ICML. 2023</p>
<p>Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Quan Sanketi, Thomas Vuong, Benjamin Kollar, Russ Burchfiel, Dorsa Tedrake, Sergey Sadigh, Percy Levine, Chelsea Liang, Finn, arXiv:2406.09246Openvla: An open-source vision-language-action model. 2024</p>
<p>MMRo: Are Multimodal LLMs Eligible as the Brain for. Jinming Li, Yichen Zhu, Zhiyuan Xu, Jindong Gu, Minjie Zhu, Xin Liu, Ning Liu, Yaxin Peng, Feifei Feng, Jian Tang, 10.48550/arXiv.2406.196932024a</p>
<p>Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, ICML'23. JMLR.orgProceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>Cogact: A foundational vision-language-action model for synergizing cognition and action in robotic manipulation. Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, Xiaofan Wang, Bei Liu, Jianlong Fu, Jianmin Bao, Dong Chen, Yuanchun Shi, Jiaolong Yang, Baining Guo, arXiv:2411.196502024barXiv preprint</p>
<p>Manipllm: Embodied multimodal large language model for object-centric robotic manipulation. Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, Hao Dong, CVPR. 2024c</p>
<p>Libero: Benchmarking knowledge transfer for lifelong robot learning. Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, Peter Stone, Advances in Neural Information Processing Systems. 202336</p>
<p>Jiaming Liu, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang, Chenyang Gu, Xiaoqi Li, Ziyu Guo, Sixiang Chen, Mengzhen Liu, Chengkai Hou, Mengdi Zhao, Zhou, arXiv:2503.10631Pheng-Ann Heng, and Shanghang Zhang. 2025. Hybridvla: Collaborative diffusion and autoregression in a unified vision-languageaction model. arXiv preprint</p>
<p>Grounding dino: Marrying dino with grounded pretraining for open-set object detection. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, ECCV. Jun Zhu, and Lei Zhang. 2024a</p>
<p>Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, arXiv:2410.07864Rdt-1b: a diffusion foundation model for bimanual manipulation. Jun Zhu. 2024b</p>
<p>Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, Chong Ruan, 10.48550/arXiv.2403.05525arXiv:2403.05525DeepSeek-VL: Towards Real-World Vision-Language Understanding. 2024Preprint</p>
<p>Fast: Efficient action tokenization for vision-language-action models. Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, Sergey Levine, arXiv:2501.097472025arXiv preprint</p>
<p>Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, Jiayuan Gu, Bin Zhao, Dong Wang, Xuelong Li, arXiv:2501.15830Spatialvla: Exploring spatial representations for visual-languageaction model. 2025arXiv preprint</p>
<p>SAM 2: Segment anything in images and videos. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan, Ross Wu, Piotr Girshick, Christoph Dollár, Feichtenhofer, ICLR. 2025</p>
<p>Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, Lei Zhang, arXiv:2401.14159Grounded sam: Assembling open-world models for diverse visual tasks. 2024</p>
<p>Multimodal diffusion transformer: Learning versatile behavior from multimodal goals. Moritz Reuss, Ömer Erdinç Yagmurlu, Fabian Wenzel, Rudolf Lioutikov, Robotics: Science and Systems. 2024</p>
<p>Emma-x: An embodied multimodal action model with grounded chain of thought and lookahead spatial reasoning. Qi Sun, Pengfei Hong, Tej Deep Pala, Vernon Toh, U-Xuan Tan, Deepanway Ghosal, Soujanya Poria, arXiv:2412.119742024</p>
<p>Octo: An open-source generalist robot policy. Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, Sergey Levine, Proceedings of Robotics: Science and Systems. Robotics: Science and Systems2024</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Diffusion-vla: Scaling robot foundation models via unified diffusion and autoregression. Junjie Wen, Minjie Zhu, Yichen Zhu, Zhibin Tang, Jinming Li, Zhongyi Zhou, Chengmeng Li, Xiaoyu Liu, Yaxin Peng, Chaomin Shen, Feifei Feng, arXiv:2412.032932024</p>
<p>Dexvla: Vision-language model with plug-in diffusion expert for general robot control. Junjie Wen, Yichen Zhu, Jinming Li, Zhibin Tang, Chaomin Shen, Feifei Feng, arXiv:2502.058552025arXiv preprint</p>
<p>Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation. Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram Burgard, 10.15607/RSS.2024.XX.077Robotics: Science and Systems XX. 2024</p>
<p>Binding touch to everything: Learning unified multimodal tactile representations. Fengyu Yang, Chao Feng, Ziyang Chen, Hyoungseob Park, Daniel Wang, Yiming Dou, Ziyao Zeng, Xien Chen, Rit Gangopadhyay, Andrew Owens, Alex Wong, CVPR. 2024</p>
<p>Up-vla: A unified understanding and prediction model for embodied agent. Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, Jianyu Chen, arXiv:2501.188672025arXiv preprint</p>
<p>Transferable tactile transformers for representation learning across diverse sensors and tasks. Jialiang Zhao, Yuxiang Ma, Lirui Wang, Edward H Adelson, CoRL. 2024</p>
<p>TraceVLA: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daumé, Iii , Andrey Kolobov, Furong Huang, Jianwei Yang, ICLR. 2025</p>
<p>Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, Feifei Feng, arXiv:2502.14420Chatvla: Unified multimodal understanding and robot control with vision-language-action model. 2025arXiv preprint</p>
<p>RT-2: Visionlanguage-action models transfer web knowledge to robotic control. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Grecia Pannag R Sanketi, Salazar, CoRL. 202335</p>            </div>
        </div>

    </div>
</body>
</html>
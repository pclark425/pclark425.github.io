<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8654 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8654</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8654</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-274860154</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.14642v3.pdf" target="_blank">Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation</a></p>
                <p><strong>Paper Abstract:</strong> Recently, Large Language Models (LLMs) have shown great potential in natural language-driven molecule discovery. However, existing datasets and benchmarks for molecule-text alignment are predominantly built on a one-to-one mapping, measuring LLMs'ability to retrieve a single, pre-defined answer, rather than their creative potential to generate diverse, yet equally valid, molecular candidates. To address this critical gap, we propose Speak-to-Structure (S^2-Bench}), the first benchmark to evaluate LLMs in open-domain natural language-driven molecule generation. S^2-Bench is specifically designed for one-to-many relationships, challenging LLMs to demonstrate genuine molecular understanding and generation capabilities. Our benchmark includes three key tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom), each probing a different aspect of molecule discovery. We also introduce OpenMolIns, a large-scale instruction tuning dataset that enables Llama-3.1-8B to surpass the most powerful LLMs like GPT-4o and Claude-3.5 on S^2-Bench. Our comprehensive evaluation of 28 LLMs shifts the focus from simple pattern recall to realistic molecular design, paving the way for more capable LLMs in natural language-driven molecule discovery.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8654.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8654.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>S2-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Speak-to-Structure Benchmark (S2-Bench)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-domain, one-to-many benchmark for natural-language-driven molecule generation, composed of three tasks (MolEdit, MolOpt, MolCustom) designed to test editing, property optimization, and constrained de novo molecule synthesis using automated RDKit-based evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>S2-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>benchmark/dataset</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Test set sampled from ZINC-250K (45,000 test cases: 9 subtasks × 5,000 each); programmatic templates used to generate natural language instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>drug discovery / materials design (general molecular design tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not a generative model; provides natural-language prompt templates (MolEdit, MolOpt, MolCustom) for LLMs to generate SMILES/SELFIES via prompt-based design.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Novelty is evaluated for MolCustom by comparing generated molecules against ZINC-250K using Tanimoto similarity (novelty metric n(mg)=1 - (max similarity to Zinc)/|Zinc| as defined in the paper); benchmark is explicitly one-to-many to allow multiple valid novel answers.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Tasks enforce application-specific constraints: MolEdit enforces localized functional-group edits; MolOpt enforces property-directed changes (LogP, MR, QED); MolCustom enforces exact structural counts (AtomNum, BondNum, FunctionalGroup).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Success Rate (SR), Similarity (Tanimoto on Morgan fingerprints) for MolEdit/MolOpt, Novelty (vs ZINC-250K) for MolCustom, Validity (SMILES parser), and Weighted Success Rate (WSR combining SR with similarity/novelty).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Used to evaluate 28 LLMs; highlights that many LLMs can produce chemically valid molecules but struggle with precise structural constraints (MolCustom) and minimal-change edits; provides per-model WSR leaderboards (top: Llama3.1-8B fine-tuned on OpenMolIns-xlarge WSR=39.33%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasts one-to-one targeted datasets (ChEBI-20, PubChem-based) that favor memorization; S2-Bench emphasizes open-ended generation and diagnostic evaluation of chemical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Design choices (empirical atom/bond distributions), selected MolOpt metrics limited to LogP/MR/QED, and potential testing ambiguities (exact location/extent of edits) are acknowledged; automated testing may not capture all chemical intent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8654.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8654.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenMolIns</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenMolIns instruction-tuning dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale, programmatically constructed instruction-to-molecule dataset (up to 1.2M samples) derived from PubChem for instruction-tuning LLMs on one-to-many molecule generation tasks across the nine S2-Bench subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenMolIns</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuning dataset</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>5 data scales: light (4.5k), small (45k), medium (180k), large (450k), xlarge (1.2M)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Programmatic samples extracted/derived from PubChem using RDKit; pairs of (instruction, SMILES) covering MolEdit, MolOpt (LogP, MR, QED), and MolCustom (atom/bond/functional-group constraints); excludes ZINC-250K to avoid leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>general molecular design (drug-like molecules, property optimization, de novo generation)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Used to fine-tune LLMs with prompt-response pairs so models produce SMILES/SELFIES in response to natural language instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Training corpus intentionally excludes ZINC-250K test molecules to enable meaningful novelty evaluation vs ZINC; OpenMolIns aims to expose models to diverse one-to-many mappings to encourage generalization beyond memorized pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Covers targeted subtasks: guiding models to perform specific edits, optimize particular properties, and meet explicit structural counts/functional-group requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not an evaluation metric itself, but models fine-tuned on OpenMolIns are evaluated on S2-Bench metrics (SR, Similarity, Novelty, Validity, WSR).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Instruction tuning on OpenMolIns substantially improved some open-source models: Llama3.1-8B instruction-tuned on OpenMolIns-xlarge achieved top leaderboard performance, surpassing several proprietary models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Shown to be more effective for open-domain molecule generation than ChEBI-20 fine-tuning for the evaluated tasks; scale-dependent gains vary by task and model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Programmatic generation may not fully represent real-world distributions; certain tasks (MolEdit, MolOpt) show limited gains from scaling alone, indicating model capacity bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8654.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8654.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3.1-8B (OpenMolIns-xlarge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3.1 8B instruction-tuned on OpenMolIns (xlarge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B-parameter Llama-3.1 instruction-following model fine-tuned on the largest OpenMolIns split; achieved the best overall Weighted Success Rate on S2-Bench in the authors' evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3.1-8B (OpenMolIns-xlarge)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive transformer (instruction-tuned LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained Llama3.1 then instruction-tuned on OpenMolIns xlarge (1.2M programmatic instruction–molecule pairs derived from PubChem), excluding ZINC-250K.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>open-domain molecule generation (drug discovery-style editing, property optimization, constrained de novo generation)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based SMILES generation from natural-language instructions (MolEdit, MolOpt, MolCustom); trained via supervised instruction tuning using RDKit-derived targets.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Assessed on MolCustom novelty metric vs ZINC-250K; Llama3.1-8B (xlarge) improved performance and achieved higher novelty/WSR values than many general LLMs (e.g., WSR overall 39.33% per Table 12).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specialized by fine-tuning to obey MolEdit/MolOpt/MolCustom constraints; evaluated for similarity-preserving edits (MolEdit/MolOpt) and constraint-fulfillment & novelty (MolCustom).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Evaluated with SR, Tanimoto similarity (Morgan fingerprints), Novelty vs ZINC-250K, Validity (SMILES parse), and aggregate WSR across subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Top-ranked performer on S2-Bench (WSR=39.33%, SR=58.79%); excelled across many subtasks after instruction tuning particularly at xlarge data scale; still struggled on fine-grained MolCustom subtasks relative to ideal performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperformed both many proprietary models (by WSR) and ChEBI-20-fine-tuned models; demonstrates that instruction tuning on a large, task-aligned dataset can surpass larger but untuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Despite strong aggregate performance, it still underperforms on MolCustom subtasks requiring exact numeric constraints; scaling benefit varies by subtask (some LogP/MR subtasks showed limited gains).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8654.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8654.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary, high-capability autoregressive LLM accessed via API and benchmarked for natural-language-driven molecule generation; used to directly generate modified or de novo SMILES in response to S2-Bench prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>proprietary autoregressive transformer (GPT family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on large, unspecified corpora that likely include chemical knowledge (proprietary; paper notes proprietary models are trained on vast corpora including chemical-related knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>open-domain molecule generation (drug discovery-style tasks evaluated: MolEdit, MolOpt, MolCustom)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based direct SMILES generation via API; asked to perform functional-group edits, property optimizations, or generate molecules meeting structural constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Novelty for MolCustom measured vs ZINC-250K; individual novelty metrics reported in Table 15 (e.g., AtomNum novelty ~0.67) indicating generated molecules are often moderately novel relative to ZINC.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Evaluated per S2-Bench tasks; required to follow explicit natural-language instructions for edits/optimizations or exact structural counts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>SR, Similarity (for MolEdit/MolOpt), Novelty (for MolCustom), Validity, and WSR; per-task numbers are reported (see Tables 13–15).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Strong but not top performer: WSR ~32.29% overall; mixed success—sometimes failed to perform minimal edits (case studies) and struggled on some MolCustom constraints (low SR on BondNum/FunctionalGroup subtasks).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Comparable to other proprietary high-tier LLMs (GPT-4-turbo, Claude-3.5) but outperformed by an instruction-tuned Llama3.1-8B on OpenMolIns xlarge in this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Tends to produce plausible but imprecise edits (case studies show failures to add/remove specified groups); struggles with exact numerical constraints and minimal-change requirements inherent to MolEdit/MolCustom.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8654.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8654.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary GPT-family LLM used via API to perform the benchmark tasks by generating SMILES from natural-language instructions; evaluated on S2-Bench alongside other proprietary models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>proprietary autoregressive transformer (GPT family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on large proprietary corpora including chemical-related knowledge (unspecified).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>open-domain molecule generation (MolEdit, MolOpt, MolCustom evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based SMILES generation from natural-language instructions via API calls.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>MolCustom novelty measured vs ZINC-250K; reported novelty values in paper's tables (e.g., atom novelty ~0.6991 for AtomNum).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Evaluated on property optimization tasks (LogP, MR, QED) and structural constraint tasks; required to produce valid, constraint-satisfying molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>SR, Similarity (Morgan fingerprints), Novelty (vs ZINC), Validity, WSR.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Performed well among proprietary models (WSR ~34.23%); had reasonable SR and similarity on MolEdit/MolOpt but low absolute performance on fine-grained MolCustom subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Similar performance class to GPT-4o and Claude-3.5; outperformed by Llama3.1-8B tuned on OpenMolIns-xlarge in aggregate WSR.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Failed in some case studies to perform the minimal expected edit (e.g., failing to add or remove specified functional groups precisely); limited control to enforce exact atom/bond counts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8654.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8654.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.5 (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-performing proprietary LLM benchmarked by the authors for open-domain molecule generation, showing competitive results across many subtasks but with notable weaknesses on some precise-edit tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>proprietary transformer-based LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on large proprietary datasets including chemical knowledge (not specified in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>molecule editing, property optimization, and constrained de novo generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based direct generation of SMILES in response to S2-Bench templates via API.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>MolCustom novelty measured vs ZINC-250K; reported novelty and WSR per Table 15 show moderate novelty but low absolute MolCustom success.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Evaluated to follow MolEdit/MolOpt instructions; case studies indicate it sometimes removes too many groups (e.g., removed two amines instead of one).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>SR, Similarity, Novelty, Validity, WSR.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>High performer among proprietary models (WSR=35.92% overall), good at MolOpt (high SR and similarity for LogP/MR/QED) but struggles in certain MolEdit subtasks (e.g., DelComponent) and MolCustom constraint precision.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperformed ChEBI-20 fine-tuned models and many open-source general LLMs, but was outperformed by Llama3.1-8B after OpenMolIns tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Prone to non-minimal edits and occasional over-editing; difficulty enforcing exact discrete constraints required by MolCustom.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8654.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8654.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica-125M (OpenMolIns)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica 125M fine-tuned on OpenMolIns</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comparatively small (125M parameter) LLM that, when instruction-tuned on OpenMolIns, achieved unexpectedly strong results on S2-Bench, demonstrating the effectiveness of task-specific instruction tuning at small scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Galactica-125M (OpenMolIns fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer LLM (scientific pretraining) with instruction fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>0.125B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained Galactica scientific corpora; fine-tuned on OpenMolIns (various data scales: light→xlarge).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>molecule editing, optimization, and constrained generation (S2-Bench tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based SMILES generation after supervised instruction fine-tuning on OpenMolIns.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Reported novelty/WSR values for MolCustom increase with data scale; e.g., Galactica-125M (xlarge) shows notable improvements relative to its smaller-data variants.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Fine-tuning teaches the model to produce modifications tied to property changes (MolOpt) and to synthesize molecules obeying structural constraints (MolCustom), though capacity limits remain.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>SR, Similarity, Novelty, Validity, WSR.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Despite tiny size, Galactica-125M (OpenMolIns-xlarge) achieved WSR=25.73% and outperformed much larger untuned models (e.g., Llama3-70B-Instruct), demonstrating that instruction-tuning on large, relevant data can yield strong gains for these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Shows that targeted instruction tuning can outperform larger general-purpose LLMs; however, still lags behind the top OpenMolIns-fine-tuned Llama3.1-8B (xlarge).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Capacity-limited: benefits from data scaling are uneven across tasks (strong gains in MolCustom with data scaling, modest or negative gains in MolEdit/MolOpt).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8654.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8654.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolT5 (small/base/large) — ChEBI-20 fine-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer T5-based models fine-tuned on molecule-caption datasets (ChEBI-20) for molecule↔text translation tasks; evaluated here as ChEBI-20 fine-tuned baselines on S2-Bench and shown to perform poorly on open-domain one-to-many generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Translation between molecules and natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolT5 (small/base/large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-decoder transformer (T5 family) fine-tuned for molecule↔text</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>MolT5-small/base/large (sizes reported in original MolT5 paper; in this paper reported as 0.08B, 0.25B, 0.78B respectively in Table 12)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Fine-tuned on ChEBI-20 (a one-to-one molecule-caption dataset) using SELFIES/SMILES representations.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>molecule-caption translation and targeted molecule generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Text-to-molecule generation via supervised translation-style fine-tuning (here evaluated on S2-Bench tasks by generating SMILES/SELFIES).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Performed poorly in MolCustom novelty and WSR; generated molecules often lack the required similarity/minimal edits, indicating reliance on memorized mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Designed for mapping between single captions and single molecules (one-to-one); not optimized for one-to-many constraint-driven generation as required by S2-Bench.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>SR, Similarity, Novelty, Validity, WSR (as applied by the authors on S2-Bench).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ChEBI-20 fine-tuned MolT5 models achieved low overall WSR (MolT5-large WSR ~2.89%, MolT5-base ~1.3%, MolT5-small ~1.29%), demonstrating that one-to-one fine-tuning does not transfer well to open-domain one-to-many molecule design.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Underperformed compared to general LLMs and OpenMolIns fine-tuned models; indicates that dataset design (one-to-many vs one-to-one) critically affects downstream generative capability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Tendency to produce valid molecules that are dissimilar to the original or that do not satisfy fine-grained numeric structural constraints; reliance on memorized pairs rather than systematic structural editing/constraint enforcement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8654.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8654.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioT5-base (Pei et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-style model integrating molecular graphs and biochemical texts (ChEBI-20 fine-tuned variant) that uses SELFIES input and was evaluated on S2-Bench; showed specific failure modes such as making non-local edits and low similarity to originals despite satisfying coarse criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BioT5-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-decoder transformer (T5-family) fine-tuned for molecule-text tasks using SELFIES</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>0.25B (reported in Table 12)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Fine-tuned on ChEBI-20 and other molecule-caption corpora; uses SELFIES representation for guaranteed validity.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>molecule-caption translation and text-based molecule generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Supervised translation from natural-language description to SELFIES; evaluated by converting outputs back to SMILES for S2-Bench testing.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Produced valid molecules but often with low similarity to the original in MolEdit tasks (i.e., not performing minimal edits), suggesting lower-quality, less-targeted generations.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Optimized for mapping captions to molecules (one-to-one); not designed for one-to-many constrained generation required by MolCustom.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>SR, Similarity, Novelty, Validity, WSR as computed by S2-Bench evaluation scripts.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Low overall WSR on S2-Bench (4.21%); case studies show BioT5 sometimes adds required groups but fails to remove specified ones or produces structurally different outputs that nevertheless pass coarse tests.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Underperforms general LLMs and OpenMolIns-fine-tuned LLMs on open-domain generation tasks; highlights limitations of one-to-one datasets for training generative chemical models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Fails to perform rational, localized edits; tends to rely on memorized patterns and generates alternative molecules that incidentally satisfy prompts rather than making intended minimal modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Translation between molecules and natural language <em>(Rating: 2)</em></li>
                <li>Text2mol: Cross-modal molecule retrieval with natural language queries <em>(Rating: 2)</em></li>
                <li>Mol-instructions: A large-scale biomolecular instruction dataset for large language models <em>(Rating: 2)</em></li>
                <li>Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations <em>(Rating: 2)</em></li>
                <li>Selfies: a robust representation of semantically constrained graphs with an example application in chemistry <em>(Rating: 1)</em></li>
                <li>Large language models are in-context molecule learners <em>(Rating: 2)</em></li>
                <li>MolReGPT <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8654",
    "paper_id": "paper-274860154",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "S2-Bench",
            "name_full": "Speak-to-Structure Benchmark (S2-Bench)",
            "brief_description": "An open-domain, one-to-many benchmark for natural-language-driven molecule generation, composed of three tasks (MolEdit, MolOpt, MolCustom) designed to test editing, property optimization, and constrained de novo molecule synthesis using automated RDKit-based evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "S2-Bench",
            "model_type": "benchmark/dataset",
            "model_size": null,
            "training_data": "Test set sampled from ZINC-250K (45,000 test cases: 9 subtasks × 5,000 each); programmatic templates used to generate natural language instructions.",
            "application_domain": "drug discovery / materials design (general molecular design tasks)",
            "generation_method": "Not a generative model; provides natural-language prompt templates (MolEdit, MolOpt, MolCustom) for LLMs to generate SMILES/SELFIES via prompt-based design.",
            "novelty_of_chemicals": "Novelty is evaluated for MolCustom by comparing generated molecules against ZINC-250K using Tanimoto similarity (novelty metric n(mg)=1 - (max similarity to Zinc)/|Zinc| as defined in the paper); benchmark is explicitly one-to-many to allow multiple valid novel answers.",
            "application_specificity": "Tasks enforce application-specific constraints: MolEdit enforces localized functional-group edits; MolOpt enforces property-directed changes (LogP, MR, QED); MolCustom enforces exact structural counts (AtomNum, BondNum, FunctionalGroup).",
            "evaluation_metrics": "Success Rate (SR), Similarity (Tanimoto on Morgan fingerprints) for MolEdit/MolOpt, Novelty (vs ZINC-250K) for MolCustom, Validity (SMILES parser), and Weighted Success Rate (WSR combining SR with similarity/novelty).",
            "results_summary": "Used to evaluate 28 LLMs; highlights that many LLMs can produce chemically valid molecules but struggle with precise structural constraints (MolCustom) and minimal-change edits; provides per-model WSR leaderboards (top: Llama3.1-8B fine-tuned on OpenMolIns-xlarge WSR=39.33%).",
            "comparison_to_other_methods": "Contrasts one-to-one targeted datasets (ChEBI-20, PubChem-based) that favor memorization; S2-Bench emphasizes open-ended generation and diagnostic evaluation of chemical reasoning.",
            "limitations_and_challenges": "Design choices (empirical atom/bond distributions), selected MolOpt metrics limited to LogP/MR/QED, and potential testing ambiguities (exact location/extent of edits) are acknowledged; automated testing may not capture all chemical intent.",
            "uuid": "e8654.0",
            "source_info": {
                "paper_title": "Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "OpenMolIns",
            "name_full": "OpenMolIns instruction-tuning dataset",
            "brief_description": "A large-scale, programmatically constructed instruction-to-molecule dataset (up to 1.2M samples) derived from PubChem for instruction-tuning LLMs on one-to-many molecule generation tasks across the nine S2-Bench subtasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenMolIns",
            "model_type": "instruction-tuning dataset",
            "model_size": "5 data scales: light (4.5k), small (45k), medium (180k), large (450k), xlarge (1.2M)",
            "training_data": "Programmatic samples extracted/derived from PubChem using RDKit; pairs of (instruction, SMILES) covering MolEdit, MolOpt (LogP, MR, QED), and MolCustom (atom/bond/functional-group constraints); excludes ZINC-250K to avoid leakage.",
            "application_domain": "general molecular design (drug-like molecules, property optimization, de novo generation)",
            "generation_method": "Used to fine-tune LLMs with prompt-response pairs so models produce SMILES/SELFIES in response to natural language instructions.",
            "novelty_of_chemicals": "Training corpus intentionally excludes ZINC-250K test molecules to enable meaningful novelty evaluation vs ZINC; OpenMolIns aims to expose models to diverse one-to-many mappings to encourage generalization beyond memorized pairs.",
            "application_specificity": "Covers targeted subtasks: guiding models to perform specific edits, optimize particular properties, and meet explicit structural counts/functional-group requirements.",
            "evaluation_metrics": "Not an evaluation metric itself, but models fine-tuned on OpenMolIns are evaluated on S2-Bench metrics (SR, Similarity, Novelty, Validity, WSR).",
            "results_summary": "Instruction tuning on OpenMolIns substantially improved some open-source models: Llama3.1-8B instruction-tuned on OpenMolIns-xlarge achieved top leaderboard performance, surpassing several proprietary models.",
            "comparison_to_other_methods": "Shown to be more effective for open-domain molecule generation than ChEBI-20 fine-tuning for the evaluated tasks; scale-dependent gains vary by task and model capacity.",
            "limitations_and_challenges": "Programmatic generation may not fully represent real-world distributions; certain tasks (MolEdit, MolOpt) show limited gains from scaling alone, indicating model capacity bottlenecks.",
            "uuid": "e8654.1",
            "source_info": {
                "paper_title": "Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Llama3.1-8B (OpenMolIns-xlarge)",
            "name_full": "Llama 3.1 8B instruction-tuned on OpenMolIns (xlarge)",
            "brief_description": "An 8B-parameter Llama-3.1 instruction-following model fine-tuned on the largest OpenMolIns split; achieved the best overall Weighted Success Rate on S2-Bench in the authors' evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama3.1-8B (OpenMolIns-xlarge)",
            "model_type": "autoregressive transformer (instruction-tuned LLM)",
            "model_size": "8B parameters",
            "training_data": "Pretrained Llama3.1 then instruction-tuned on OpenMolIns xlarge (1.2M programmatic instruction–molecule pairs derived from PubChem), excluding ZINC-250K.",
            "application_domain": "open-domain molecule generation (drug discovery-style editing, property optimization, constrained de novo generation)",
            "generation_method": "Prompt-based SMILES generation from natural-language instructions (MolEdit, MolOpt, MolCustom); trained via supervised instruction tuning using RDKit-derived targets.",
            "novelty_of_chemicals": "Assessed on MolCustom novelty metric vs ZINC-250K; Llama3.1-8B (xlarge) improved performance and achieved higher novelty/WSR values than many general LLMs (e.g., WSR overall 39.33% per Table 12).",
            "application_specificity": "Specialized by fine-tuning to obey MolEdit/MolOpt/MolCustom constraints; evaluated for similarity-preserving edits (MolEdit/MolOpt) and constraint-fulfillment & novelty (MolCustom).",
            "evaluation_metrics": "Evaluated with SR, Tanimoto similarity (Morgan fingerprints), Novelty vs ZINC-250K, Validity (SMILES parse), and aggregate WSR across subtasks.",
            "results_summary": "Top-ranked performer on S2-Bench (WSR=39.33%, SR=58.79%); excelled across many subtasks after instruction tuning particularly at xlarge data scale; still struggled on fine-grained MolCustom subtasks relative to ideal performance.",
            "comparison_to_other_methods": "Outperformed both many proprietary models (by WSR) and ChEBI-20-fine-tuned models; demonstrates that instruction tuning on a large, task-aligned dataset can surpass larger but untuned models.",
            "limitations_and_challenges": "Despite strong aggregate performance, it still underperforms on MolCustom subtasks requiring exact numeric constraints; scaling benefit varies by subtask (some LogP/MR subtasks showed limited gains).",
            "uuid": "e8654.2",
            "source_info": {
                "paper_title": "Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (OpenAI)",
            "brief_description": "A proprietary, high-capability autoregressive LLM accessed via API and benchmarked for natural-language-driven molecule generation; used to directly generate modified or de novo SMILES in response to S2-Bench prompts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_type": "proprietary autoregressive transformer (GPT family)",
            "model_size": null,
            "training_data": "Pretrained on large, unspecified corpora that likely include chemical knowledge (proprietary; paper notes proprietary models are trained on vast corpora including chemical-related knowledge).",
            "application_domain": "open-domain molecule generation (drug discovery-style tasks evaluated: MolEdit, MolOpt, MolCustom)",
            "generation_method": "Prompt-based direct SMILES generation via API; asked to perform functional-group edits, property optimizations, or generate molecules meeting structural constraints.",
            "novelty_of_chemicals": "Novelty for MolCustom measured vs ZINC-250K; individual novelty metrics reported in Table 15 (e.g., AtomNum novelty ~0.67) indicating generated molecules are often moderately novel relative to ZINC.",
            "application_specificity": "Evaluated per S2-Bench tasks; required to follow explicit natural-language instructions for edits/optimizations or exact structural counts.",
            "evaluation_metrics": "SR, Similarity (for MolEdit/MolOpt), Novelty (for MolCustom), Validity, and WSR; per-task numbers are reported (see Tables 13–15).",
            "results_summary": "Strong but not top performer: WSR ~32.29% overall; mixed success—sometimes failed to perform minimal edits (case studies) and struggled on some MolCustom constraints (low SR on BondNum/FunctionalGroup subtasks).",
            "comparison_to_other_methods": "Comparable to other proprietary high-tier LLMs (GPT-4-turbo, Claude-3.5) but outperformed by an instruction-tuned Llama3.1-8B on OpenMolIns xlarge in this evaluation.",
            "limitations_and_challenges": "Tends to produce plausible but imprecise edits (case studies show failures to add/remove specified groups); struggles with exact numerical constraints and minimal-change requirements inherent to MolEdit/MolCustom.",
            "uuid": "e8654.3",
            "source_info": {
                "paper_title": "Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-4-turbo",
            "name_full": "GPT-4-turbo (OpenAI)",
            "brief_description": "A proprietary GPT-family LLM used via API to perform the benchmark tasks by generating SMILES from natural-language instructions; evaluated on S2-Bench alongside other proprietary models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4-turbo",
            "model_type": "proprietary autoregressive transformer (GPT family)",
            "model_size": null,
            "training_data": "Pretrained on large proprietary corpora including chemical-related knowledge (unspecified).",
            "application_domain": "open-domain molecule generation (MolEdit, MolOpt, MolCustom evaluated)",
            "generation_method": "Prompt-based SMILES generation from natural-language instructions via API calls.",
            "novelty_of_chemicals": "MolCustom novelty measured vs ZINC-250K; reported novelty values in paper's tables (e.g., atom novelty ~0.6991 for AtomNum).",
            "application_specificity": "Evaluated on property optimization tasks (LogP, MR, QED) and structural constraint tasks; required to produce valid, constraint-satisfying molecules.",
            "evaluation_metrics": "SR, Similarity (Morgan fingerprints), Novelty (vs ZINC), Validity, WSR.",
            "results_summary": "Performed well among proprietary models (WSR ~34.23%); had reasonable SR and similarity on MolEdit/MolOpt but low absolute performance on fine-grained MolCustom subtasks.",
            "comparison_to_other_methods": "Similar performance class to GPT-4o and Claude-3.5; outperformed by Llama3.1-8B tuned on OpenMolIns-xlarge in aggregate WSR.",
            "limitations_and_challenges": "Failed in some case studies to perform the minimal expected edit (e.g., failing to add or remove specified functional groups precisely); limited control to enforce exact atom/bond counts.",
            "uuid": "e8654.4",
            "source_info": {
                "paper_title": "Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Claude-3.5",
            "name_full": "Claude 3.5 (Anthropic)",
            "brief_description": "A high-performing proprietary LLM benchmarked by the authors for open-domain molecule generation, showing competitive results across many subtasks but with notable weaknesses on some precise-edit tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude-3.5",
            "model_type": "proprietary transformer-based LLM",
            "model_size": null,
            "training_data": "Pretrained on large proprietary datasets including chemical knowledge (not specified in paper).",
            "application_domain": "molecule editing, property optimization, and constrained de novo generation",
            "generation_method": "Prompt-based direct generation of SMILES in response to S2-Bench templates via API.",
            "novelty_of_chemicals": "MolCustom novelty measured vs ZINC-250K; reported novelty and WSR per Table 15 show moderate novelty but low absolute MolCustom success.",
            "application_specificity": "Evaluated to follow MolEdit/MolOpt instructions; case studies indicate it sometimes removes too many groups (e.g., removed two amines instead of one).",
            "evaluation_metrics": "SR, Similarity, Novelty, Validity, WSR.",
            "results_summary": "High performer among proprietary models (WSR=35.92% overall), good at MolOpt (high SR and similarity for LogP/MR/QED) but struggles in certain MolEdit subtasks (e.g., DelComponent) and MolCustom constraint precision.",
            "comparison_to_other_methods": "Outperformed ChEBI-20 fine-tuned models and many open-source general LLMs, but was outperformed by Llama3.1-8B after OpenMolIns tuning.",
            "limitations_and_challenges": "Prone to non-minimal edits and occasional over-editing; difficulty enforcing exact discrete constraints required by MolCustom.",
            "uuid": "e8654.5",
            "source_info": {
                "paper_title": "Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Galactica-125M (OpenMolIns)",
            "name_full": "Galactica 125M fine-tuned on OpenMolIns",
            "brief_description": "A comparatively small (125M parameter) LLM that, when instruction-tuned on OpenMolIns, achieved unexpectedly strong results on S2-Bench, demonstrating the effectiveness of task-specific instruction tuning at small scale.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Galactica-125M (OpenMolIns fine-tuned)",
            "model_type": "transformer LLM (scientific pretraining) with instruction fine-tuning",
            "model_size": "0.125B parameters",
            "training_data": "Pretrained Galactica scientific corpora; fine-tuned on OpenMolIns (various data scales: light→xlarge).",
            "application_domain": "molecule editing, optimization, and constrained generation (S2-Bench tasks)",
            "generation_method": "Prompt-based SMILES generation after supervised instruction fine-tuning on OpenMolIns.",
            "novelty_of_chemicals": "Reported novelty/WSR values for MolCustom increase with data scale; e.g., Galactica-125M (xlarge) shows notable improvements relative to its smaller-data variants.",
            "application_specificity": "Fine-tuning teaches the model to produce modifications tied to property changes (MolOpt) and to synthesize molecules obeying structural constraints (MolCustom), though capacity limits remain.",
            "evaluation_metrics": "SR, Similarity, Novelty, Validity, WSR.",
            "results_summary": "Despite tiny size, Galactica-125M (OpenMolIns-xlarge) achieved WSR=25.73% and outperformed much larger untuned models (e.g., Llama3-70B-Instruct), demonstrating that instruction-tuning on large, relevant data can yield strong gains for these tasks.",
            "comparison_to_other_methods": "Shows that targeted instruction tuning can outperform larger general-purpose LLMs; however, still lags behind the top OpenMolIns-fine-tuned Llama3.1-8B (xlarge).",
            "limitations_and_challenges": "Capacity-limited: benefits from data scaling are uneven across tasks (strong gains in MolCustom with data scaling, modest or negative gains in MolEdit/MolOpt).",
            "uuid": "e8654.6",
            "source_info": {
                "paper_title": "Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "MolT5",
            "name_full": "MolT5 (small/base/large) — ChEBI-20 fine-tuned",
            "brief_description": "Transformer T5-based models fine-tuned on molecule-caption datasets (ChEBI-20) for molecule↔text translation tasks; evaluated here as ChEBI-20 fine-tuned baselines on S2-Bench and shown to perform poorly on open-domain one-to-many generation.",
            "citation_title": "Translation between molecules and natural language",
            "mention_or_use": "use",
            "model_name": "MolT5 (small/base/large)",
            "model_type": "encoder-decoder transformer (T5 family) fine-tuned for molecule↔text",
            "model_size": "MolT5-small/base/large (sizes reported in original MolT5 paper; in this paper reported as 0.08B, 0.25B, 0.78B respectively in Table 12)",
            "training_data": "Fine-tuned on ChEBI-20 (a one-to-one molecule-caption dataset) using SELFIES/SMILES representations.",
            "application_domain": "molecule-caption translation and targeted molecule generation",
            "generation_method": "Text-to-molecule generation via supervised translation-style fine-tuning (here evaluated on S2-Bench tasks by generating SMILES/SELFIES).",
            "novelty_of_chemicals": "Performed poorly in MolCustom novelty and WSR; generated molecules often lack the required similarity/minimal edits, indicating reliance on memorized mappings.",
            "application_specificity": "Designed for mapping between single captions and single molecules (one-to-one); not optimized for one-to-many constraint-driven generation as required by S2-Bench.",
            "evaluation_metrics": "SR, Similarity, Novelty, Validity, WSR (as applied by the authors on S2-Bench).",
            "results_summary": "ChEBI-20 fine-tuned MolT5 models achieved low overall WSR (MolT5-large WSR ~2.89%, MolT5-base ~1.3%, MolT5-small ~1.29%), demonstrating that one-to-one fine-tuning does not transfer well to open-domain one-to-many molecule design.",
            "comparison_to_other_methods": "Underperformed compared to general LLMs and OpenMolIns fine-tuned models; indicates that dataset design (one-to-many vs one-to-one) critically affects downstream generative capability.",
            "limitations_and_challenges": "Tendency to produce valid molecules that are dissimilar to the original or that do not satisfy fine-grained numeric structural constraints; reliance on memorized pairs rather than systematic structural editing/constraint enforcement.",
            "uuid": "e8654.7",
            "source_info": {
                "paper_title": "Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "BioT5",
            "name_full": "BioT5-base (Pei et al.)",
            "brief_description": "A T5-style model integrating molecular graphs and biochemical texts (ChEBI-20 fine-tuned variant) that uses SELFIES input and was evaluated on S2-Bench; showed specific failure modes such as making non-local edits and low similarity to originals despite satisfying coarse criteria.",
            "citation_title": "Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations",
            "mention_or_use": "use",
            "model_name": "BioT5-base",
            "model_type": "encoder-decoder transformer (T5-family) fine-tuned for molecule-text tasks using SELFIES",
            "model_size": "0.25B (reported in Table 12)",
            "training_data": "Fine-tuned on ChEBI-20 and other molecule-caption corpora; uses SELFIES representation for guaranteed validity.",
            "application_domain": "molecule-caption translation and text-based molecule generation",
            "generation_method": "Supervised translation from natural-language description to SELFIES; evaluated by converting outputs back to SMILES for S2-Bench testing.",
            "novelty_of_chemicals": "Produced valid molecules but often with low similarity to the original in MolEdit tasks (i.e., not performing minimal edits), suggesting lower-quality, less-targeted generations.",
            "application_specificity": "Optimized for mapping captions to molecules (one-to-one); not designed for one-to-many constrained generation required by MolCustom.",
            "evaluation_metrics": "SR, Similarity, Novelty, Validity, WSR as computed by S2-Bench evaluation scripts.",
            "results_summary": "Low overall WSR on S2-Bench (4.21%); case studies show BioT5 sometimes adds required groups but fails to remove specified ones or produces structurally different outputs that nevertheless pass coarse tests.",
            "comparison_to_other_methods": "Underperforms general LLMs and OpenMolIns-fine-tuned LLMs on open-domain generation tasks; highlights limitations of one-to-one datasets for training generative chemical models.",
            "limitations_and_challenges": "Fails to perform rational, localized edits; tends to rely on memorized patterns and generates alternative molecules that incidentally satisfy prompts rather than making intended minimal modifications.",
            "uuid": "e8654.8",
            "source_info": {
                "paper_title": "Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Translation between molecules and natural language",
            "rating": 2,
            "sanitized_title": "translation_between_molecules_and_natural_language"
        },
        {
            "paper_title": "Text2mol: Cross-modal molecule retrieval with natural language queries",
            "rating": 2,
            "sanitized_title": "text2mol_crossmodal_molecule_retrieval_with_natural_language_queries"
        },
        {
            "paper_title": "Mol-instructions: A large-scale biomolecular instruction dataset for large language models",
            "rating": 2,
            "sanitized_title": "molinstructions_a_largescale_biomolecular_instruction_dataset_for_large_language_models"
        },
        {
            "paper_title": "Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations",
            "rating": 2,
            "sanitized_title": "biot5_enriching_crossmodal_integration_in_biology_with_chemical_knowledge_and_natural_language_associations"
        },
        {
            "paper_title": "Selfies: a robust representation of semantically constrained graphs with an example application in chemistry",
            "rating": 1,
            "sanitized_title": "selfies_a_robust_representation_of_semantically_constrained_graphs_with_an_example_application_in_chemistry"
        },
        {
            "paper_title": "Large language models are in-context molecule learners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_incontext_molecule_learners"
        },
        {
            "paper_title": "MolReGPT",
            "rating": 1
        }
    ],
    "cost": 0.01874225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SPEAK-TO-STRUCTURE: EVALUATING LLMS IN OPEN-DOMAIN NATURAL LANGUAGE-DRIVEN MOLECULE GENERATION
15 Sep 2025</p>
<p>Jiatong Li 
The Hong Kong Polytechnic University</p>
<p>Junxian Li 
Shanghai Jiao Tong University</p>
<p>Weida Wang 
Shanghai AI Lab</p>
<p>Fudan University</p>
<p>Yunqing Liu 
The Hong Kong Polytechnic University</p>
<p>Changmeng Zheng 
The Hong Kong Polytechnic University</p>
<p>Dongzhan Zhou zhoudongzhan@pjlab.org.cn 
Shanghai AI Lab</p>
<p>Xiao-Yong Wei x1wei@polyu.edu.hk 
The Hong Kong Polytechnic University</p>
<p>Qing Li 
The Hong Kong Polytechnic University</p>
<p>Moledit Molopt Molcustom 
SPEAK-TO-STRUCTURE: EVALUATING LLMS IN OPEN-DOMAIN NATURAL LANGUAGE-DRIVEN MOLECULE GENERATION
15 Sep 2025607BB5DD149C630F9F0C7FCF22D8C13FarXiv:2412.14642v3[cs.CL]
Recently, Large Language Models (LLMs) have shown great potential in natural language-driven molecule discovery.However, existing datasets and benchmarks for molecule-text alignment are predominantly built on a one-to-one mapping, measuring LLMs' ability to retrieve a single, pre-defined answer, rather than their creative potential to generate diverse, yet equally valid, molecular candidates.To address this critical gap, we propose Speak-to-Structure (S 2 -Bench), the first benchmark to evaluate LLMs in open-domain natural language-driven molecule generation.S 2 -Bench is specifically designed for one-to-many relationships, challenging LLMs to demonstrate genuine molecular understanding and generation capabilities.Our benchmark includes three key tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom), each probing a different aspect of molecule discovery.We also introduce OpenMolIns, a large-scale instruction tuning dataset that enables Llama-3.1-8B to surpass the most powerful LLMs like GPT-4o and Claude-3.5 on S 2 -Bench.Our comprehensive evaluation of 28 LLMs shifts the focus from simple pattern recall to realistic molecular design, paving the way for more capable LLMs in natural language-driven molecule discovery.</p>
<p>INTRODUCTION</p>
<p>Molecule discovery plays a pivotal role in various scientific research fields, from pharmaceuticals (Keiser et al., 2010) to materials science (Higuchi et al., 2023).Traditionally, this is a trial-and-error process (Ekins, 2024) that requires extensive experimentation and data analysis (Mattern &amp; Grosser, 2023), often taking over a decade to bring a new drug candidate to market (Lee et al., 2018).</p>
<p>Recently, Large Language Models (LLMs) have demonstrated great potential in molecule discovery (Edwards et al., 2021;2022) by leveraging their powerful language understanding and robust reasoning abilities (OpenAI, 2023).Molecules can be represented as SMILES strings (Weininger, 1988), which allows LLMs to process molecules as textual strings, effectively bridging the gap between molecular structures and natural language.Meanwhile, by aligning molecules with textual descriptions (Edwards et al., 2022), LLMs can assist chemists in interpreting chemical knowledge, suggesting structural modifications, and predicting compound properties (Li et al., 2024a;Zhang et al., 2024), thereby significantly streamlining the molecule discovery process.</p>
<p>While the integration of LLMs holds immense promise, a significant challenge lies in the limitations of current datasets and benchmarks.For example, existing datasets for aligning molecules and texts like ChEBI-20 (Edwards et al., 2022) and PubChem324K (Liu et al., 2023), are constructed based on a one-to-one mapping assumption, where each textual description is linked to a single, predefined target molecule due to the convenience of acquiring ground truth data from existing databases.However, the one-to-one mapping presents a key limitation: it does not fully align with the nature of real-world molecule discovery.In practice, multiple distinct molecules can often share the same desired properties or biological activity Petrone et al. (2012).For instance, a particular pharmaceutical effect is rarely unique to a single compound, and a material with a specific physical property, like tensile strength, can be realized through various molecular structures.This mismatch between the evaluation paradigm and the reality of molecule discovery casts doubt on whether these datasets and benchmarks genuinely assess an LLM's capacity for creative molecular design, or if they instead inadvertently encourage models to rely on memorization and pattern-matching.</p>
<p>To bridge this critical gap, we introduce a novel benchmark, Speak-to-Structure (S 2 -Bench), the first benchmark designed to evaluate the open-domain natural language-driven molecule generation capabilities of LLMs.S 2 -Bench is meticulously structured into three primary tasks, as shown in Figure 1, each targeting a specific, real-world capability essential for drug and materials science:</p>
<p>Molecule Editing (MolEdit) focuses on testing an LLM's ability to perform precise, localized structural modifications while preserving main structures.In drug discovery, this is analogous to Lead Optimization Barcelos et al. (2022), which edits the lead compound to create new variants.This requires a deep understanding of chemical grammar, including valency, stereochemistry, and ring systems, ensuring that edits could create valid molecules.</p>
<p>Molecule Optimization (MolOpt) extends the MolEdit task by evaluating a model's ability to optimize a lead compound under specified property constraints (e.g., increasing solubility or reducing toxicity).Its open-ended design requires the model to edit and refine molecular structures in accordance with the desired properties, thereby showcasing its understanding of both structural modifications and property-related characteristics.</p>
<p>Customized Molecule Generation (MolCustom) requires LLMs to synthesize a novel molecule based on a natural language description of its desired structural components.Specifically, it challenges models to generate molecules with a set of quantitative and qualitative constraints, such as a specific number of atoms, bonds, or functional groups, which directly mimics the initial drug design phase where a chemist defines a new compound based on a set of precise requirements for its structure.</p>
<p>To facilitate the evaluation of the above tasks, we also introduce a robust evaluation system with tailored examination process, enabling a comprehensive assessment of LLMs' performance in opendomain natural language-driven molecule discovery.Furthermore, we propose OpenMolIns, a large-scale instruction-tuning dataset, comprising up to 120 million instruction-molecule pairs, enabling Llama3.1-8B to achieve the state-of-the-art performance among 28 powerful LLMs on S 2 -Bench.</p>
<p>To summarize, our contributions are primarily threefold:</p>
<ol>
<li>We introduce Speak-to-Structure (S 2 -Bench), the first benchmark for open-domain natural language-driven molecule generation.Moving from one-to-one to one-to-many relationships, S 2 -Bench better aligns concepts in drug discovery and provides a novel perspective on assessing LLMs' genuine molecular understanding and design capabilities.2. We introduce OpenMolIns, an instruction-tuning dataset with up to 120 million instruction-molecule pairs, enabling Llama3.1-8B to achieve SOTA performance on S 2 -Bench.3. We provide insightful findings based on our extensive benchmarking of 28 LLMs, revealing the limitations of existing targeted generation datasets and highlighting the potential for LLMs to transition from simple pattern recall to realistic molecular design.</li>
</ol>
<p>SPEAK-TO-STRUCTURE</p>
<p>In this section, we detail the design philosophy, technical composition, and statistics of Speak-to-Structure (S 2 -Bench), which is fundamentally structured around the core capabilities required for real-world molecular design: editing, optimizing, and customized generation.</p>
<p>TASK COMPOSITION</p>
<p>S 2 -Bench is meticulously designed to move beyond simple pattern-matching and evaluate the genuine molecular understanding and generation capabilities of LLMs.We adopt a one-to-many paradigm, where a single language instruction can be fulfilled by multiple valid molecules.Our benchmark challenges LLMs to demonstrate a flexible understanding of molecular syntax rather than mere memorization.This design philosophy is concretely realized through three tasks of increasing complexity, each mirroring a critical phase of molecule discovery.</p>
<p>MolEdit assesses an LLM's foundational molecular syntax and structural modification capabilities.</p>
<p>In this task, a base molecule is provided, and the model is asked to perform a specific change on the molecular structure.In a real-world scenario, this task is analogous to the iterative lead optimization process in drug discovery, where chemists make small, precise changes to a candidate molecule to change its properties.We designed three subtasks to probe this capability: AddComponent and DelComponent test the model's ability to add or remove a specific functional group, respectively, while SubComponent challenges it to perform a combined operation of both.</p>
<p>MolOpt evaluates an LLM's ability to perform goal-oriented chemical reasoning.The challenge here is twofold: the LLM should not only edit a molecule but also ensure that the modification leads to a desired change in a specific property.This task directly mirrors the hit-to-lead optimization stage, where molecules are refined to achieve better pharmacological profiles.To assess this, we chose three key properties (LogP, MR, and QED), which are vital for drug discovery and can be reliably and reproducibly calculated.By requiring the model to generate a molecule that improves on a given property, we test its ability to understand the complex relationship between molecular structure and function, a capability that cannot be evaluated by tasks with a single, predetermined answer.</p>
<p>MolCustom serves as the ultimate test of an LLM's creative chemical design ability.MolCustom requires the LLM to generate a novel molecule from a natural language description of its desired structural components.This task is not about modifying an existing structure but about synthesizing a new one from scratch, akin to a chemist defining the needs for a new compound at the very beginning of a project.To ensure a clear and precise evaluation of this complex task, we focus on generating molecules with specific structural features: a defined number of atoms (AtomNum), a specified number and type of bonds (BondNum), or the inclusion of particular functional groups (FunctionalGroup).While these constraints may seem simple, they are deceptively challenging, requiring the model to apply a sophisticated understanding of chemical valence rules to create a valid and novel molecule.</p>
<p>Compared to targeted molecule generation datasets, these tasks in S 2 -Bench, while seemingly straightforward in their instructions, impose a more rigorous requirement on LLMs' ability to perform precise, open-domain molecule generation.Our benchmark fundamentally shifts the evaluation from pattern recall to practical chemical design, thereby providing a more accurate measure of an LLM's potential in molecule discovery and helping to build more explainable and trustworthy models.</p>
<p>DATA CONSTRUCTION</p>
<p>Previously, the development of robust datasets for text-based targeted molecule generation has been hindered by the scarcity of high-quality human annotations.For example, while text-to-image generation datasets like MS COCO (Chen et al., 2015) contain millions of annotated samples, molecule-caption datasets, such as ChEBI-20 (Edwards et al., 2022) are significantly smaller, often by orders of magnitude.This disparity arises because molecular annotation demands specialized expertise, making it both time-intensive and costly.The resulting data scarcity further poses a significant challenge for advancing LLMs in text-guided molecule discovery.</p>
<p>Our approach to data construction fundamentally bypasses this bottleneck.S 2 -Bench is designed for open-domain molecule generation tasks that do not rely on human annotations.Instead, we leverage automated chemical toolkits to programmatically construct tasks and evaluate outputs based on objective molecular properties and structural rules.This enables us to generate a virtually unlimited volume of data, effectively addressing the "data hunger" that has constrained previous research.More importantly, this programmatic approach allows us to create tasks with a one-to-many relationship, where multiple correct answers exist for a single prompt, which better aligns with the complexities of drug discovery, accommodating its inherent variability and diverse outcomes.</p>
<p>Specifically, we design a systematic and scalable data construction process, as illustrated in Figure 4.</p>
<p>For MolEdit and MolOpt, we sample molecules from large, publicly available chemical databases.We selected Zinc-250K (Sterling &amp; Irwin, 2015) for building our test set due to its manageable size and diversity, while the massive PubChem (Kim et al., 2019) database (with over 10 million molecules) serves as the basis for the instruction-tuning dataset.We utilize the RDKit (Landrum, 2013) toolbox to automatically extract key molecular statistics, including structural patterns and properties like LogP, MR, and QED.These extracted features are then integrated into our pre-defined, instruction-based prompt templates, which is demonstrated in Appendix D.</p>
<p>For MolCustom, we move beyond existing databases and programmatically generate instructions to test a model's ability to create novel molecules.For each of the three subtasks, we randomly generate 5,000 instructions that specify a target number and type of atoms, bonds, or functional groups.</p>
<p>Furthermore, for each subtask, we pre-defined a diverse prompt template pool to ensure that the LLMs are not overfit to a limited set of prompt formats.This programmatic construction not only allows for the generation of a much larger data volume but also ensures that our benchmark tests an LLM's intrinsic chemical reasoning ability, rather than its capacity for memorizing human-annotated patterns.By moving beyond the one-to-one paradigm, S 2 -Bench provides a more robust and realistic evaluation, paving the way for models that can truly innovate and design novel molecules.</p>
<p>EVALUATION</p>
<p>The evaluation of S 2 -Bench is facilitated through a set of carefully designed automated evaluation processes and metrics tailored to the unique nature of our tasks.Unlike one-to-one datasets or benchmarks that simply check for exact matches, our metrics are designed to assess an LLM's ability to produce valid, relevant, and novel outputs within an open-ended framework.</p>
<p>Task (Weighted) Success Rate Similarity Novelty Validity
MolEdit ✓ ✓ ✓ MolCustom ✓ ✓ ✓ MolOpt ✓ ✓ ✓
Table 1: All used metrics are listed here.'✓' means that a metric is calculated on a task.</p>
<p>MOLEDIT &amp; MOLOPT EVALUATION</p>
<p>For the MolEdit and MolOpt tasks, as shown in Table 1, we employ a combination of metrics to assess both the correctness of the modification and the rationality of the design.</p>
<p>Success Rate: This metric evaluates a model's ability to fulfill the specific molecule generation requirements.We design automated evaluation processes to verify if the generated molecule meets the specified criteria (e.g., correct functional group modification for MolEdit, or desired property optimization for MolOpt).Detailed Implementations of the automated testing process are shown in Appendix E. This metric examines whether the LLM can follow instructions precisely, which is a fundamental requirement for an LLM to serve as a chemist assistant.</p>
<p>Similarity: In open-domain tasks like MolEdit and MolOpt, a high Success Rate alone is insufficient.We must also ensure that the generated molecule is a reasonable modification of the original, rather than a completely new, unrelated structure that happens to satisfy the criteria.To address this, we measure the Tanimoto Similarity between the generated and original molecules using Morgan Fingerprints (Butina, 1999).A high similarity score indicates that the model has performed a rational, localized edit and has not simply generated a different molecule from scratch.The similarity δ(m g , m o ) between the generated molecule m g and the original molecule m o is calculated as:
δ(m g , m o ) = |f p m g ∩ f p m o | |f p m g ∪ f p m o | ,(1)
where f p m g and f p m o represents their corresponding Morgan Fingerprints.|f p m g ∩ f p m o | is the size of the intersection between their Morgan Fingerprints, while |f p m g ∪ f p m o | is the union.</p>
<p>Validity: This metric ensures that the generated molecules are chemically valid and follow the rules of molecular syntax.Only the molecules that successfully pass the SMILES parser are considered valid molecules.A higher validity means that the model is more familiar with the molecule syntax.</p>
<p>MOLCUSTOM EVALUATION</p>
<p>For the MolCustom task, where models are required to generate molecules from scratch based on structural descriptions, we use a different set of metrics to evaluate their creative design capabilities.</p>
<p>Success Rate: This metric measures how well the generated molecules adhere to the high-level structural constraints provided in the prompt (e.g., number of atoms, bonds, or specific functional groups).As the task requires de novo generation, the success rate directly reflects a model's ability to translate abstract requirements into a valid, concrete molecular structure.Detailed testing process of MolCustom could also be found in Appendix E.</p>
<p>Novelty: For open-domain generation, fulfilling the requirements is just the first step.The true value lies in a model's ability to generate novel and innovative molecules.The novelty score quantifies this by comparing the generated molecules against existing structures in a large database, such as Zinc-250K (Zinc for short).A low similarity to known molecules suggests a high degree of novelty, which is a critical indicator of a model's potential for discovering truly new molecule structures.The novelty n for the generated molecule m g can be calculated as:
n(m g ) = 1 − m k ∈Zinc δ(m g , m k ) |Zinc| ,(2)
Validity: As with the other two tasks, validity ensures that all generated molecules are chemically sound and can be interpreted correctly.</p>
<p>AVERAGE WEIGHTED SUCCESS RATE</p>
<p>To provide a single, comprehensive ranking of LLM performance on S 2 -Bench, we introduce a weighted success rate.This metric combines the core success rate with a quality metric relevant to each task: Similarity for MolEdit/MolOpt and Novelty for MolCustom.This approach ensures that a high score reflects not only the ability to follow instructions but also the rationality and creativity of the generation.The weighted success rate for a subtask t is defined as:
WSR t = n t × SR t , t ∈ {M olCustom} δ t × SR t , t ∈ {M olEdit, M olOpt} ,(3)
where WSR t denotes the weighted success rate for a subtask t, while δ t is the similarity score for the MolEdit and MolOpt tasks, n t represents the novelty score for the MolCustom tasks, and SR t is the corresponding success rate of the subtask.Then, the average weighted success rate WSR could evaluate the synthetic performance of LLMs among all the nine subtasks:
WSR = 1 9 t WSR t ,(4)
where the average weighted success rate WSR provides a balanced measure of a model's performance across all nine subtasks, offering a robust and nuanced assessment of its capabilities in open-domain natural language-driven molecule generation.</p>
<p>OPENMOLINS: INSTRUCTION TUNING DATASET</p>
<p>To effectively train and evaluate LLMs on the open-ended challenges posed by S 2 -Bench, we introduce OpenMolIns, a specialized instruction-tuning dataset derived from the PubChem database.The core philosophy behind OpenMolIns is to provide models with the kind of flexible, one-to-many training examples they need to learn genuine chemical reasoning, rather than simply memorizing specific input-output pairs.To ensure the integrity of our evaluations, we meticulously designed OpenMolIns to have zero overlap with the Zinc-250K, preventing any data leakage that could compromise the validity of our benchmark results.</p>
<p>We built the instruction-tuning dataset programmatically, creating equal numbers of samples (i.e., 5,000) for all nine subtasks.The RDKit toolbox was essential to this process, allowing us to automatically generate data based on objective molecular properties and rules.This contrasts sharply with previous methods that rely on scarce and costly human annotations.Details of the construction process and the pre-defined prompt templates are shown in Appendix D.</p>
<p>For MolEdit and MolOpt, our goal is to improve the capabilities of LLMs to perform rational, goal-oriented modifications.For MolEdit, we start with a molecule, programmatically identify a modifiable functional group, and then perform a change (add, delete, or substitute) to create a new, valid molecule.This pair of (original and modified) molecules is then used to construct an instruction.This process teaches the model not only a single correct answer, but rather the skill of modifying a molecule in a chemically sound way.Similarly, for MolOpt, we calculate properties before and after a modification to identify a specific optimization direction (e.g., increasing LogP or decreasing MR).</p>
<p>The training sample then explicitly links a desired outcome to a structural change, teaching the LLM to reason about the relationship between structure and property.</p>
<p>In the MolCustom domain, we aim to train the model to generate molecules from scratch based on structural details.Instead of relying on existing molecule-caption pairs, we programmatically analyze molecules from PubChem, extract key structural statistics (e.g., atom counts, bond numbers, and functional group types), and use these as the basis for a training prompt.The molecule itself becomes the target output.This approach trains the model to synthesize a new molecule that fits a set of high-level, multi-faceted criteria, a skill fundamentally different from recalling a specific, pre-existing structure.This method allows the generated molecules to better fit the real distribution of molecular space and prepares the model for de novo design.</p>
<p>To further investigate the impact of data scales on an LLM's ability to learn these tasks, we created five distinct data levels: light, small, medium, large, and xlarge, shown in</p>
<p>STATISTICS</p>
<p>In this section, we provide a detailed overview of S 2 -Bench and OpenMolIns, as summarized in Tables 2 and 3. Proprietary Models.This category includes LLMs that are only accessible via commercial API services.In this work, we benchmark GPT-4o, GPT-4-turbo, GPT-3.5-turbo(OpenAI, 2023), Claude-3.5 (Anthropic, 2024b), Claude-3 (Anthropic, 2024a), and Gemini-1.5-pro(Deepmind, 2024), which are all the most advanced LLMs with powerful reasoning and generalization capabilities.These LLMs are pre-trained on vast pre-training corpora, normally including chemical-related knowledge.</p>
<p>Open-source General LLMs.This group contains open-source LLMs that are tuned with the instruction following capability, which can be used for a wide range of tasks and applications.Specifically, we benchmark Llama3-70B-Instruct, Llama3-8B-Instruct, Llama3.1-8B-Instruct,Llama3.2-1B-Instruct(Dubey et al., 2024), Mistral-7B-Instruct-v0. 2 (Jiang et al., 2023), Deepseek-R1-distill-Qwen-7B (Guo et al., 2025), Qwen2-7B-Instruct (Yang et al., 2024), yi-1.5-9B(Young et al., 2024), chatglm-9B (GLM et al., 2024), and Gemma3-12B (Team et al., 2025).These LLMs are top-tier open-source general LLMs within our computation budget.</p>
<p>Open-source ChEBI-20 Fine-tuned LLMs.LLMs fine-tuned on the ChEBI-20 dataset can grasp some extent of text-based molecule generation capability.In this case, our experiments also cover LLMs like MolT5-small, MolT5-base, MolT5-large (Edwards et al., 2022), and BioT5-base (Pei et al., 2023).These models have been the state-of-art models in the molecule-caption translation task.</p>
<p>Here, to ensure a fair evaluation of the intrinsic capabilities of LLMs, we exclude models that utilize multi-modal architectures or retrieval-augmented generation.OpenMolIns Fine-tuned LLMs.We further fine-tune LLMs like Galactica-125M (Taylor et al., 2022), Llama3.2-1B-Instruct, and Llama3.1-8B-Instruct on OpenMolIns dataset for comparison.We specifically include the experiments on five distinct data sizes of OpenMolIns for Galactica-125M because the model has been pre-trained on scientific corpora and has proved its effectiveness in molecule-related tasks (Liu et al., 2023).Meanwhile, a small size model can also help us study the data scaling law within reasonable budget.Furthermore, Llama3.2-1B-Instruct and Llama3.1-8B-Instructare also selected due to their advanced capabilities and similar architectures.</p>
<p>FINDINGS</p>
<p>Based on our comprehensive benchmarking, we have made the following key observations regarding the capabilities of LLMs in open-domain natural language-driven molecule generation.</p>
<p>F1: Current LLMs show promise but lack the structural understanding necessary for precise molecule generation.As shown in Figure 2, top-tier proprietary models such as Claude-3.5 and Gemini-1.5-Proattain 35.92% and 34.80% in average weighted success rate, respectively.These results suggest that general LLMs hold promise for open-ended molecule generation due to their powerful reasoning capability, yet substantial challenges remain in translating textual requirements into chemically valid and structurally precise molecules.Notably, within the MolCustom task, no LLM manages to surpass a 25% weighted success rate on any individual subtask.The particularly poor performance on MolCustom tasks highlights that while current LLMs can generate chemically plausible molecules in a broad sense, they struggle with satisfying fine-grained structural constraints such as precise atom, bond, or functional group counts, because the precise control of molecular structures requires reasoning over discrete, globally coupled constraints.LLMs, even trained on SMILES, primarily learn local token distributions from large-scale pre-training data, which rarely emphasizes exact numeric constraints.</p>
<p>The precise control, in fact, is crucial for molecular discovery, as many properties of interest, such as solubility, toxicity, and active site composition, depend on specific structural configurations.Our finding suggests that open-domain natural language-driven molecule generation task can serve as an effective diagnostic tool, revealing systematic weaknesses in current LLMs and providing rich failure cases that can inform future instruction tuning or reinforcement learning strategies to improve molecule structural constraint adherence.LLMs with an average weighted success rates of 39.33%.More remarkably, the much smaller model, Galactica-125M, within just 125 million parameters, achieved a weighted average success rate of 25.73%, outperforming models two orders of magnitude larger, such as Llama3-70B-Instruct.This remarkable outcome indicates that task-specific instruction tuning on a large, high-quality corpus can be highly effective, suggesting a practical avenue for LLMs to efficiently narrow the performance gap in molecular discovery.</p>
<p>F3: One-to-one mappings between language instructions and molecules make LLMs inherently rely on pattern recognition and recall.Our results also substantiate the observation, demonstrating that existing one-to-one mappings are insufficient for evaluating true chemical comprehension of LLMs.Notably, ChEBI-20 Fine-tuned LLMs, which are designed to align molecules with texts, perform worse than general LLMs.For instance, BioT5-base (Pei et al., 2023), a state-of-the-art model on the ChEBI-20 dataset, achieves an average weighted success rate of only 4.21% on S 2 -Bench.</p>
<p>A closer examination of BioT5-base's performance provides insight into this limitation.Although the model achieves relatively high success rates on MolEdit and MolOpt tasks, its generated molecules often exhibit low similarity to the original structures.This indicates that the model is not performing rational edits; rather, it generates different but valid molecules that happen to satisfy the criteria.In other words, BioT5-base relies more on recalling patterns from its training corpus than on learning to systematically modify molecular structures.Consequently, while the ChEBI-20 dataset is valuable for molecule-caption translation, its limited diversity, scale, and one-to-one mapping nature are insufficient for training LLMs to perform nuanced, open-ended molecular generation.</p>
<p>F4: The data scaling law does not always hold: task-specific bottlenecks limit gains in molecule optimization and editing.As shown in Figure 3, our experiments with Galactica-125M across five data scales of OpenMolIns provide new insights into the data scaling law for molecule generation.</p>
<p>Overall, we observe that larger datasets generally improve performance, but the effect is highly task-dependent.</p>
<p>In MolCustom, scaling the dataset from large to xlarge yields a remarkable 286% average performance gain, indicating that tasks involving complex, de novo synthesis strongly benefit from larger and more diverse training data.By contrast, Galactica-125M obtains only modest gains in MolOpt, and even negative gains in MolEdit, suggesting that additional data alone cannot overcome existing bottlenecks in these relatively simpler tasks.</p>
<p>Interestingly, when examining Llama-3.1-8B-Instruct,we find that performance continues to increase when scaling from the large to xlarge data level, except for LogP and MR subtasks.This contrast generally highlights that small models like Galactica-125M are capacity-limited, whereas larger models can more effectively leverage additional data.</p>
<p>To summarize, these findings suggest that while complex, open-ended tasks are data-driven, simpler property optimization or editing tasks may be constrained more by model capacity than by dataset size.Thus, merely enlarging training data may not yield proportional benefits.Instead, future progress will require a balanced consideration of both model scale and data scale.</p>
<p>CONCLUSION</p>
<p>In this study, we introduced S 2 -Bench and OpenMolIns, the first benchmark and instruction tuning dataset for evaluating the capabilities of LLMs in open-domain natural language-driven molecule generation.By moving beyond traditional one-to-one text-to-molecule mappings, our benchmark accommodates the open-ended nature of molecular discovery.S 2 -Bench focuses on the realistic molecular reasoning and design, rather than simple pattern matching and memorization.</p>
<p>Our comprehensive benchmarking of 28 LLMs not only highlights the challenges faced by current models and the limitations of existing targeted molecule generation datasets, but also demonstrates the substantial potential of LLMs for natural language-driven molecule discovery.</p>
<p>A RELATED WORK</p>
<p>Molecule discovery is a cornerstone of scientific progress, underpinning advances in both drug development and material design (Du et al., 2022).The integration of artificial intelligence into this process has driven a paradigm shift in the pharmaceutical landscape, substantially improving the efficiency and accuracy of identifying and developing novel therapeutic candidates.</p>
<p>Recent breakthroughs in natural language processing (NLP) have highlighted the potential of Large Language Models (LLMs) to analyze complex biological and chemical data more effectively than traditional computational methods (Zhou et al., 2023).Within this context, Text-based Molecule Generation has emerged as a representative task for AI-driven molecule discovery (Edwards et al., 2021).This task focuses on generating target molecules directly from natural language descriptions, which requires the construction of paired datasets linking molecular structures with their textual representations.</p>
<p>Early work in this direction employed transformer-based models such as MolT5 (Edwards et al., 2022), which leveraged large-scale self-supervised learning to produce high-quality SMILES strings from textual prompts.Building on this foundation, models like KV-PLM (Zeng et al., 2022), MoMu (Su et al., 2022), and BioT5 (Pei et al., 2023) integrated molecular graphs and biochemical texts to jointly enhance molecular understanding and generation.More recently, 3D-MoLM (Li et al., 2024d) advanced the field by incorporating spatial configurations, thereby improving the geometric validity and accuracy of generated molecular structures.</p>
<p>Parallel to these architecture-driven improvements, LLMs such as MolReGPT (Li et al., 2024b) and ICMA (Li et al., 2024a)</p>
<p>B IMPLEMENTATION DETAILS</p>
<p>We implement various scripts to facilitate the testing of the aforementioned models.For proprietary models, we adopt the OpenAI API1 framework.For open-source general LLMs, we utilize both the VLLM2 framework and the OpenAI framework.For the remaining LLMs, we adopt the Hugging Face transformers library3 for inference.</p>
<p>Furthermore, BioT5 is designed to use SELFIES, an alternative string representation of molecules by using special tokens to ensure valid generations (Krenn et al., 2019), as input instead of SMILES.Consequently, we convert the molecule SMILES strings into SELFIES format on BioT5.</p>
<p>C HYPER PARAMETERS</p>
<p>In this section, we illustrate the detailed parameters adopted in this work, as shown in Table 4.</p>
<p>Notably, we utilize one NVIDIA A100 80G for testing open-source LLMs, and 4×NVIDIA A100 80G for instruction tuning.For close-source LLMs, we call the official APIs of them.</p>
<p>D DATA CONSTRUCTION</p>
<p>In this section, we introduce the construction details of S 2 -Bench and OpenMolIns dataset, as well as the prompt templates.Figure 4 demonstrates the overall data construction process for S  OpenMolIns, we exclude all the molecules in Zinc250K to avoid data leakage and ensure the novelty score of the generated molecules.</p>
<p>D.1 MOLEDIT</p>
<p>For the MolEdit task, we consider the common operations on modifying functional groups in a given molecule (i.e., add, drop, and substitute), which are simple tasks for human experts but challenging to LLMs.In this case, we further develop three corresponding subtasks: AddComponent, DelComponent, and SubComponent.Prompt templates for MolEdit are shown in Table 5.However, there are different kinds of functional groups, and some functional groups can play an important role in the molecule structure, such as connecting two separate parts of the molecule, which makes them unsuitable for these operations above as these operations will entirely change the structure of the molecule.In this case, we aim to make a slight change in the molecule structure and limit most of the functional groups we choose within the end groups.</p>
<p>Table 6 presents the functional groups that are taken into account for AddComponent and Del-Component, along with their respective selection weights.Given that certain functional groups are encountered less frequently in real-world scenarios, we have implemented a weighted random selection process for AddComponent, which ensures that less common functional groups are assigned a lower probability to be chosen.</p>
<p>For SubComponent, our focus is exclusively on end groups for simplification, which include hydroxyl, aldehyde, carboxyl, nitro, halo, nitrile, and thiol, which ensures that the editing operations are  Table 6: Functional Groups that are considered in AddComponent and DelComponent, as well as their weights to be selected in AddComponent.</p>
<p>confined to substituting the existing functional group with another from this list, thereby maintaining the integrity of the molecule's overall structure without altering it fundamentally.</p>
<p>D.2 MOLOPT</p>
<p>MolOpt is designed to optimize molecular properties through the refinement of molecule structures, is not a brand-new task.Previously, GNN-based methods have been widely adopted in this task, while these methods can only help with one specific subtask at a time.In contrast, S 2 -Bench requires one single LLM to optimize molecules with different metrics and optimization directions.In this work, we specifically focus on enhancing specific characteristics that are crucial for drug discovery and chemical synthesis, including LogP, MR, and QED.The prompt templates for MolOpt are listed in Table 7.</p>
<p>Below are the detailed explanations of the chemical properties adopted in the three subtasks of MolOpt:</p>
<p>LogP refers to the logarithm of the partition coefficient, which is a measure of a molecule's hydrophilicity or lipophilicity.It is an important factor in determining a compound's bioavailability and membrane permeability.</p>
<p>Molecular Refractivity (MR) is a measure of the molar refractive index, which provides insight into the molecular size and the degree of molecular branching.It is used to assess the overall shape and bulk of a molecule.</p>
<p>Quantitative Estimation of Drug-Likeness (QED) is a computational metric that evaluates the druglikeness of a molecule based on a set of predefined rules.A higher QED score suggests a greater likelihood that the molecule will have favourable pharmacological properties.</p>
<p>Among the three subtasks, the QED optimization, which assesses the potential of a molecule to become a drug for curing diseases, is generally considered the most challenging property for molecule optimization.In contrast, LogP and MR can be more directly inferred from the molecule's structure, thus making them more straightforward to optimize.8. Below, we present the construction details of the three subtasks for MolCustom:</p>
<p>AtomNum.Table 9 shows the atoms we consider in AtomNum, as well as their weights to be selected.Notably, carbon, as the basic unit in organic chemicals, is a mandatory option.The number of carbon atoms ranges from 1 to 40, while the number of other selected atoms ranges from 1 to 5.This setting relieves the difficulty for generation, as LLMs could generate a carbon backbone first and attach the remaining atoms to the backbone one by one.</p>
<p>BondNum.Similarly, we select five different kinds of chemical bonds: single, double, triple, rotatable, and aromatic, as shown in Table 10.For the single bond, if selected, the number can vary from 1 to 50.For the aromatic bond, the number follows the rules of the formation of aromatic bonds, varying from 5 to 20.Moreover, the number of these remaining bonds, if selected, is specified from 1 to 5.</p>
<p>FunctionalGroup.Lastly, we also specify functional groups in the molecule structure.Table 11 shows the range of functional groups and their weights that are taken into consideration.</p>
<p>Notably, in MolCustom, if not required, LLMs can generate any number of these unspecified atoms, bonds, and functional groups.However, for these specified items, LLMs should strictly follow the requirements.</p>
<p>E TESTING PROCESS</p>
<p>In this section, we present the testing process used to calculate the success rate of each subtask.</p>
<p>The testing process for MolEdit task is demonstrated in Algorithm 1.However, in this process, we can not guarantee that the modified molecule differs the original molecule only in the specified functional group.The reasons are to follow:</p>
<p>• The complexity of molecular structures makes it difficult to use regular matching to monitor the exact number of functional groups present.This is because molecules can have a wide variety of bonding patterns and spatial arrangements that are not always amenable to simple, rule-based matching techniques.Table 11: Functional Groups that are considered in FunctionalGroup, as well as their weights to be selected.Higher weights indicate that they are more likely to be selected.For simplification, we only consider normal and edge functional groups in the molecule structures.</p>
<p>• Modifying functional groups in specific positions can indeed lead to unintended changes in the molecule's overall structure.This is due to the interconnected nature of atoms within a molecule, where altering one part can have a ripple effect on the stability and geometry of the entire structure.</p>
<p>• The tools and algorithms currently available may not have the necessary sophistication to predict and control all the possible outcomes of a functional group modification, particularly when dealing with complex molecules or reactions that are not fully understood.</p>
<p>Therefore, the similarity score needs to be considered to ensure the accuracy and quality of the generation.</p>
<p>For MolOpt, we adopt Algorithm 2 for the testing process.Similarly, merely passing the test does not necessarily mean that the generated molecule follows the principle of minimal change.</p>
<p>Algorithm 3 MolCustom Testing Process
Input: Generated Molecule m g , Subtask t, Atom List A, Bond List B, Functional Group List G, Requirements R Output: Pass or Not (Bool) flag = true if t is AtomNum then for atom in A do if |atom|(m g ) ̸ = R[atom] then flag = false else if t is BondNum then for bond in B do if |bond|(m g ) ̸ = R[bond] then flag = false else if t is FunctionalGroup then for group in G do if |group|(m g ) ̸ = R[group] then flag = false return flag
For MolCustom, the testing process is shown in Algorithm 3, which test whether the specified number of the atom, bond, or functional group satisfies the requirements.</p>
<p>F DETAILED RESULTS</p>
<p>In this section, we first show the leaderboard of S 2 -Bench in Table 12, where Claude-3.5 achieves the second place with an average weighted success rate of 35.92%.Notably, via instruction tuning on our OpenMolIns dataset, Llama3.1-8Bachieves the first place, which outperforms all the other LLMs.</p>
<p>We also compare several representative LLMs from different model categories via radar maps, which are shown in Figure 5.Among the proprietary models, it can be concluded that Claude-3.5 achieves a powerful performance in these subtasks, except for the DelComponent task and the BondNum task.In DelComponent task, GPT-4o demonstrates better performance than Claude-3.5.More importantly, we can also observe that Llama3.1-8B(OpenMolIns-xlarge) outperforms most of the other LLMs across these nine subtasks, achieving the SOTA performance.</p>
<p>Meanwhile, these radar maps also imply the difficulties of the subtasks.For example, for majority of the LLMs, they are struggling with the BondNum task.Even the most advanced proprietary LLMs like Claude-3.5 failed to achieve an ideal performance.</p>
<p>Finally, we also include a reasoning LLM, Deepseek-R1-distill-Qwen-7B (Guo et al., 2025), as shown in  Finally, we present the detailed experimental results of all the subtasks in Table 13, 14, and 15.Note that we do not provide results of vanilla galactica-125M, for it's not instruction tuned and cannot respond to human instructions.All results of vanilla galactica-125M should be close to zeros.</p>
<p>G LIMITATIONS</p>
<p>Although S 2 -Bench is carefully designed and well-validated through our experiments, we still observe several limitations:</p>
<p>Data Distribution.In our data construction process, we allocate distributions to atoms, bonds, and functional groups with the aim of making our benchmark more reflective of real-world distributions.</p>
<p>Nevertheless, the distribution we use is largely empirical and may not be sufficiently accurate to reconstruct real-world scenarios accurately.</p>
<p>The Selection of Subtasks.For the selection of subtasks, there are various choices in MolOpt, we compeletly understand that there are multiple viable options, especially when there are enormous metrics for evaluating molecular properties.However, we need to 1) consider the consistency between the three major tasks in S 2 -Bench and balance their weights (which means we can not select too many metrics for MolOpt, otherwise it would make the benchmark solely an optimization benchmark), and 2) ensure that the metrics can be efficiently and robustly calculated.Therefore, we only select the three common metrics, LogP, MR, and QRD, for MolOpt.</p>
<p>H CASE STUDY</p>
<p>Finally, we include the case study to better illustrate the performance gap among different models.</p>
<p>Figure 7: Case study of DelComponent.In this example, we require LLMs to remove an amine from the given molecule.Here, GPT-4o, Claude-3.5, and BioT5-base all removed two amines, while the remaining models successfully passed the testing process.</p>
<p>There is a molecule composed of 2 benzene rings groups.</p>
<p>Requirement</p>
<p>Figure 1 :
1
Figure 1: Task illustration of S 2 -Bench for open domain natural language-driven molecule generation.In contrast to text-based target molecule generation, multiple valid molecules may fulfill the textual requirements (right of the arrow).</p>
<p>Figure 2 :
2
Figure 2: The performance of LLMs benchmarked in S 2 -Bench.LLMs fall into 4 categories: Proprietary Models, Open-source General LLMs, Open-source ChEBI-20 Fine-tuned LLMs, and OpenMolIns Fine-tuned LLMs.Models of unknown parameters are denoted as horizontal lines.</p>
<p>Figure 3 :
3
Figure 3: Task-specific performance scaling with increasing data in S 2 -Bench.(a) Galactica-125M.(b) Llama-3.1-8B-Instruct.</p>
<p>Figure 4 :
4
Figure 4: Data construction workflow of S 2 -Bench &amp; OpenMolIns.</p>
<p>Figure 5 :
5
Figure 5: Performance Comparison across several representative LLMs on S 2 -Bench.</p>
<p>Figure 14 :
14
Figure14: Case study of FunctionalGroup.In this example, we require LLMs to generate a molecule with 2 benzene rings.Here, only Claude-3.5 failed to generate the correct number of benzene rings.</p>
<p>Table 2
2DatasetS 2 -BenchOpenMolInsitemeach subtasktotallightsmall mediumlargexlargeData Size5,00045, 000 4,500 18,000 45,000 90,000 1,200,000
. This tiered structure allows researchers to systematically analyze the data scaling law for open-domain natural language-driven molecule generation, providing valuable insights into how data quantity influences a model's capacity for chemical reasoning.</p>
<p>Table 2 :
2
Statisics of S 2 -Bench and OpenMolIns.</p>
<p>Table 3 :
3
More importantly, these samples are designed to test the model's ability to handle one-to-many relationships, a challenge that is central to true chemical reasoning.Meanwhile, to support the development of models capable of succeeding on this benchmark, Open-MolIns is provided in five distinct data scales, ranging from 4,500 to 1,200,000 examples.This tiered structure serves a dual purpose: it not only offers ample data to train powerful models but also allows researchers to systematically investigate the data scaling law for this novel task.By observing how model performance improves with increasing data size, we can gain valuable insights into the data requirements for aligning molecular space with natural language.Comparison with existing text-based molecule generation benchmarks and datasets.Notably, we denote the statistics of the description guided molecule design part in Mol-Instructions.
Benchmarks or DatasetsTask Type# Training Data # Test Data Public AccessPCdes(Zeng et al., 2022)Targeted Generation 10,5003000×ChEBI-20 (Edwards et al., 2021)Targeted Generation 26,4073,000✓PubChem (Liu et al., 2023)Targeted Generation 12,0002,000✓Mol-Instructions (Fang et al., 2024) Targeted Generation 297,3191000✓L + M -24 (Edwards et al., 2024)Targeted Generation 160,49221,839✓S 2 -Bench &amp; OpenMolInsOpen Generation1,200,00045,000✓
The design and scale of our datasets are not arbitrary; they are meticulously crafted to enable a rigorous and nuanced evaluation of LLMs in open-domain molecule generation, which previous, smaller datasets cannot provide.S 2 -Bench is composed of nine subtasks, with each containing 5,000 test samples, for a total of 45,000 carefully curated test cases.This extensive size is critical for providing a statistically robust and comprehensive assessment of model performance, mitigating the risk of misleading results that can arise from smaller test sets.Besides, as shown in Table3, S 2 -Bench and OpenMolIns stand out from existing text-based molecule generation datasets.S 2 -Bench is the first to introduce a truly open-domain natural language-driven molecule generation task, moving the field beyond the limitations of one-to-one mapping.In terms of data scale, our datasets are also significantly larger.OpenMolIns has the largest training set to date, with a volume that is unprecedented in this domain.This scale is crucial for enabling LLMs to learn complex chemical rules and generalize beyond the memorization of specific examples.Similarly, S 2 -Bench's 45,000 test samples make it the largest evaluation benchmark, providing a more reliable and comprehensive measure of a model's true capabilities.This combination of innovative task design and large-scale data represents a new paradigm for evaluating and advancing LLMs in the field of text-guided molecular discovery.3EXPERIMENTS3.1 MODELS The models benchmarked are categorized into four groups: proprietary models, open-source general LLMs, open-source ChEBI-20 fine-tuned LLMs, and OpenMolIns fine-tuned LLMs.</p>
<p>Table 4 :
4
Hyper-parameters.
ItemValueGenerationtemperature0.75top p0.85num beams1max new tokens512Instruction Tuningepochs(light)10epochs(small, medium)5epochs(large, xlarge)3batchsize32lr3e-4warmup ratio0.03cutoff len1024Lora Settingsr64α128dropout0.1Data SamplingChemical Property CalculationWrap in Prompt TemplatesNumber of Functional GroupsZinc 250KRandomly Sampling Molecules for Each SubtaskTest SamplesRDKitChemical PropertiesLogPMRQEDZincNumber of Functional Groups250KDropPubChem 10mIntersection Check Overlap MoleculesOverlap? YesNoOpenMolIns SamplesRDKitNumber of Atoms, Bonds, and Functional Groups Chemical Properties LogP MR QED
2-Bench as well as the OpenMolIns.Generally, we utilize Zinc250K to construct the test samples in S 2 -Bench, while adopting PubChem for the training samples in OpenMolIns.Notably, when constructing the Randomly require the number of.atoms,bonds, and functional groupsPlease generate a molecule with 6 carbon atoms and 1 oxygen atom.</p>
<p>{} to the molecule {}.Modify the molecule {} by adding a {}.Add a {} to the molecule {}.DelComponent Please remove a {} from the molecule {}.Modify the molecule {} by removing a {}.Remove a {} from the molecule {}.SubComponent Please substitute a {} in the molecule {} by {}.Modify the molecule {} by replacing a {} by {}.Replace a {} in the molecule {} by {}.Please replace a {} in the molecule {} with {}.Modify the molecule {} by substituting a {} with {}.Substitute a {} in the molecule {} with {}.
Prompt Templates for MolEditAddComponentPlease add a</p>
<p>Table 5 :
5
Prompt Templates for MolEdit.
Functional Group benzene ring hydroxyl aldehyde carboxyl amideWeights15155510Functional GroupaminenitrohalonitrilethiolWeights55511</p>
<p>Table 7 :
7
Prompt Templates for MolOpt.</p>
<p>D.3 MOLCUSTOMTo enable customized design of molecules, we consider three fundamental features for describing the molecule, including atoms, bonds, and functional groups.Given the specified category and number of atoms, bonds, and functional groups, LLMs should generate the molecule as we request.The prompt templates for MolCustom are shown in Table</p>
<p>Table 8 :
8
Prompt Templates for MolCustom.
Atomcarbonoxygen nitrogensulfurfluorine chlorine bromineiodinephosphorusWeights [Mandatory]53322221Atomboronsilicon selenium tellurium arsenic antimony bismuth poloniumWeights11111111</p>
<p>Table 9 :
9
Atoms that are considered in AtomNum, as well as their weights to be selected.Higher weights indicate that they are more likely to be selected.For simplification, we only consider normal atoms in the molecule structures.
Bondsingle double triple rotatable aromaticWeights54311</p>
<p>Table 10 :
10
Chemical bonds that are considered in BondNum, as well as their weights to be selected.Higher weights indicate that they are more likely to be selected.For simplification, we only consider normal bonds in the molecule structures.
Functional Group benzene ring hydroxyl anhydride aldehyde ketone carboxylesteramideamine nitroWeights1515255105552Functional Grouphalothioethernitrilethiolsulfide disulfide sulfoxide sulfone boraneWeights211111111</p>
<p>Table 12 ,
12
which achieves the best performance among 7 and 8B level open-source general LLMs, showing the potential of LLM reasoning in open-domain natural language-driven molecule generation tasks.
RankModel#Parameters (B) SR (%) WSR(%)1Llama3.1-8B (OpenMolIns-xlarge)858.7939.332Claude-3.5 (Anthropic, 2024b)-51.1035.923Gemini-1.5-pro (Deepmind, 2024)-52.2534.804GPT-4-turbo (OpenAI, 2023)-50.7434.235GPT-4o (OpenAI, 2023)-49.0832.296Claude-3 (Anthropic, 2024a)-46.1430.477Llama3.1-8B (OpenMolIns-large)843.127.228Galactica-125M (OpenMolIns-xlarge)0.12544.4825.739Llama3-70B-Instruct (Int4) (Dubey et al., 2024)7038.5423.9310Galactica-125M (OpenMolIns-large)0.12539.2823.4211Galactica-125M (OpenMolIns-medium)0.12534.5419.8912GPT-3.5-turbo (OpenAI, 2023)-28.9318.5813Galactica-125M (OpenMolIns-small)0.12524.1715.1814Gemma3-12B (Team et al., 2025)1226.2815.0015Deepseek-R1-distill-Qwen-7B (Guo et al., 2025)725.0714.6116Llama3.1-8B-Instruct (Dubey et al., 2024)826.2614.0917Llama3-8B-Instruct (Dubey et al., 2024)826.4013.7518chatglm-9B (GLM et al., 2024)918.5013.13(7)19Galactica-125M (OpenMolIns-light)0.12520.9513.13(6)20Llama3.2-1B (OpenMolIns-large)114.118.1021yi-1.5-9B (Young et al., 2024)914.107.3222Mistral-7B-Instruct-v0.2 (Jiang et al., 2023)711.174.8123BioT5-base (Pei et al., 2023)0.2524.194.2124MolT5-large (Edwards et al., 2022)0.7823.112.8925Llama3.1-1B-Instruct (Dubey et al., 2024)13.951.9926MolT5-base (Edwards et al., 2022)0.2511.111.30(0)27MolT5-small (Edwards et al., 2022)0.0811.551.29(9)28Qwen2-7B-Instruct (Yang et al., 2024)70.180.15</p>
<p>Table 12 :
12
Leaderboard of S 2 -Bench.</p>
<p>Table 13 :
13
Detailed results on MolEdit.For each task, we highlight the best and the second-best success rate (SR), as well as the weighted success rate (WSR).
ModelsSRAddComponent Similarity WSRValiditySRDelComponent Similarity WSRValiditySRSubComponent Similarity WSRValidityGPT-4o (OpenAI, 2023)0.61880.67820.41970.74120.70120.60380.42340.84740.79920.72250.57740.9368GPT-4-turbo (OpenAI, 2023)0.69900.69360.48480.79340.72440.57350.41540.90600.77780.73230.56960.9160GPT-3.5-turbo (OpenAI, 2023)0.58320.65450.38170.79800.30820.77970.24030.84680.29180.63330.18480.6822Claude-3.5 (Anthropic, 2024b)0.68320.70170.47940.44140.54140.66780.36150.79600.81040.73100.59240.9588Claude-3 (Anthropic, 2024a)0.67660.68400.46280.81800.55560.64080.35600.89840.65500.71590.46890.9184Gemini-1.5-pro (Deepmind, 2024)0.70580.67920.47940.82540.75900.59490.45150.91580.71480.71390.51030.8684Llama3-70B-Instruct (Int4) (Dubey et al., 2024)0.51980.68010.35350.59220.61220.56370.34510.71820.50940.71700.36520.6822Llama3-8B-Instruct (Dubey et al., 2024)0.39140.66490.26020.53740.43480.50580.21990.57000.26020.68410.17800.4838Llama3.1-8B-Instruct (Dubey et al., 2024)0.29920.60880.18220.49540.43360.52570.22790.59100.34010.64240.21850.5076Mistral-7B-Instruct-v0.2 (Jiang et al., 2023)0.18680.62510.11680.37600.20180.37740.07620.35900.06020.62270.03750.3550Qwen2-7B-Instruct (Yang et al., 2024)0.00100.25270.00030.00360.00060.40240.00020.00120.00040.28950.00010.0068Yi-1.5-9B (Young et al., 2024)0.17420.41700.07260.42160.28580.59360.16970.49090.13700.46190.06330.4368Chatglm-9B (GLM et al., 2024)0.29320.76220.22350.56860.29560.74940.22150.69140.14980.71500.10710.5084Llama3.2-1B-Instruct (Dubey et al., 2024)0.03740.53430.02000.19820.07680.57500.04420.30280.01020.36710.00370.1468Gemma3-12B (Team et al., 2025)0.43000.70810.30450.63240.49480.51580.25520.71680.42080.71540.30100.7616Deepseek-R1-distill-Qwen-7B (Guo et al., 2025)0.26670.69040.18410.00900.63670.59210.37700.07360.21560.73800.15910.0356MolT5-small (Edwards et al., 2022)0.12200.10270.01250.44900.15980.11250.01800.45040.07080.10290.00730.4876MolT5-base (Edwards et al., 2022)0.13540.10660.01440.46860.15620.11440.01790.44720.05840.10280.00600.4426MolT5-large (Edwards et al., 2022)0.28340.10840.03070.92820.22280.12010.02680.91980.16920.09320.01580.9410BioT5-base (Pei et al., 2023)0.34620.15670.05421.00000.16680.15970.02661.00000.06840.15760.01080.9998Llama3.2-1B (OpenMolIns-large)0.17560.56760.09970.32160.18160.49630.09010.24660.08440.54150.04570.2958Llama3.1-8B (OpenMolIns-large)0.58220.65410.38080.67300.51040.50740.25900.68960.54400.62580.34040.8400Llama3.1-8B (OpenMolIns-xlarge)0.77900.67690.52730.94680.86400.61660.53270.91320.61000.74340.45350.9596Galactica-125M (OpenMolIns-light)0.37860.59580.22560.68420.20620.65210.13450.70480.31020.58790.18240.6674Galactica-125M (OpenMolIns-small)0.34720.61720.21430.53560.32580.60250.19630.57580.26920.61810.16640.5692Galactica-125M (OpenMolIns-medium)0.47360.56820.26910.74420.48860.51840.25330.74880.32820.59750.19610.6958Galactica-125M (OpenMolIns-large)0.58660.58760.34470.82280.60780.55770.33900.79340.34380.64910.22320.8438Galactica-125M (OpenMolIns-xlarge)0.58420.58590.34230.84380.65260.50840.33180.82860.18720.60240.11280.8538ModelsSRLogP SimilarityWSRValiditySRMR SimilarityWSRValiditySRQED SimilarityWSRValidityGPT-4o (OpenAI, 2023)0.71900.65860.47350.87960.68640.64200.44070.83520.39520.61800.24420.8570GPT-4-turbo (OpenAI, 2023)0.76620.69840.53510.90480.73880.68210.50390.88480.39460.65870.25990.9050GPT-3.5-turbo (OpenAI, 2023)0.40480.63270.25610.85400.41200.62630.25800.84860.33160.56350.18690.8354Claude-3.5 (Anthropic, 2024b)0.79700.71240.56780.94220.69620.71120.49510.91100.53610.70420.37750.8604Claude-3 (Anthropic, 2024a)0.79840.60670.48440.90960.60940.63980.38990.90620.46780.58550.27390.9044Gemini-1.5-pro (Deepmind, 2024)0.77120.70220.54150.92740.78760.67440.53120.89260.47040.60770.28590.9484Llama3-70B-Instruct (Int4) (Dubey et al., 2024)0.59840.60280.36070.64820.56840.60320.34290.62720.27740.48280.13390.6340Llama3-8B-Instruct (Dubey et al., 2024)0.46420.36580.16980.60860.43320.47930.20760.57040.25680.45470.11680.6112Llama3.1-8B-Instruct (Dubey et al., 2024)0.39900.42350.16900.51220.43360.52570.22790.59100.26550.44990.11940.6158Mistral-7B-Instruct-v0.2 (Jiang et al., 2023)0.22200.45010.09990.28020.19080.25780.04920.37950.12100.32440.03930.2532Qwen2-7B-Instruct (Yang et al., 2024)0.00000.29230.00000.00040.00020.41230.00010.00040.00000.00000.00000.0000Yi-1.5-9B (Young et al., 2024)0.28840.54610.15750.49270.20500.37240.07630.41260.10640.65960.07020.4526Chatglm-9B (GLM et al., 2024)0.36660.69020.25300.47360.35140.68200.23970.50000.18320.65060.11920.4342Llama3.2-1B-Instruct (Dubey et al., 2024)0.06440.50550.03260.16640.08220.44100.03630.16040.07140.47570.03400.1796Gemma3-12B (Team et al., 2025)0.45280.6940.31420.59120.32560.35480.11550.44780.16660.08690.01450.3322Deepseek-R1-distill-Qwen-7B (Guo et al., 2025)0.40480.44160.17880.56220.28960.45350.13130.20340.08350.45560.03800.6074MolT5-small (Edwards et al., 2022)0.21580.10520.02270.43020.23160.10110.02340.44200.22140.10310.02280.4326MolT5-base (Edwards et al., 2022)0.20740.10510.02180.41680.18560.10730.01990.37960.23580.10540.02490.4536MolT5-large (Edwards et al., 2022)0.42440.10150.04310.81560.44960.10720.04820.86780.46540.11900.05540.9214BioT5-base (Pei et al., 2023)0.51580.15260.07871.00000.50600.15970.08081.00000.50680.15800.08011.0000Llama3.2-1B (OpenMolIns-large)0.28980.59510.17250.38500.26440.59560.15750.36780.19960.58490.11670.3490Llama3.1-8B (OpenMolIns-large)0.80540.66780.53780.87200.71220.65480.46630.85140.52240.63980.33420.8802Llama3.1-8B (OpenMolIns-xlarge)0.88220.66620.58770.93140.69820.66930.46730.94220.86480.67360.58250.9310Galactica-125M (OpenMolIns-light)0.32020.65470.20960.64160.35080.64350.22570.63580.26900.65210.17540.6380Galactica-125M (OpenMolIns-small)0.41720.64200.26780.55680.39580.64520.25540.53380.29560.63850.18870.5376Galactica-125M (OpenMolIns-medium)0.59040.58120.34310.78900.58740.58730.34500.73840.46080.58590.27000.7768Galactica-125M (OpenMolIns-large)0.64540.59270.38250.81980.63880.59730.38160.80280.49500.59620.29510.8100Galactica-125M (OpenMolIns-xlarge)0.73620.57440.42290.89020.71240.56970.40590.86120.57860.56770.32850.8626</p>
<p>Table 14 :
14
Detailed results on MolOpt.For each task, we highlight the best and the second-best success rate (SR), as well as the weighted success rate (WSR).</p>
<p>Table 15 :
15
Detailed results for subtasks on MolCustom.For each task, we highlight the best and the second-best success rate (SR), as well as the weighted success rate (WSR).
ModelsSRAtomNum Novelty WSRValiditySRBondNum Novelty WSRValiditySRFunctionalGroup Novelty WSRValidityGPT-4o (OpenAI, 2023)0.19980.67030.13390.58520.06500.63360.04120.85640.23300.65130.15180.8590GPT-4-turbo (OpenAI, 2023)0.17020.69910.11900.49040.07740.63010.04880.90680.21800.66050.14400.8778GPT-3.5-turbo (OpenAI, 2023)0.10700.50540.05410.69470.05180.68710.03560.55220.11360.65850.07480.8686Claude-3.5 (Anthropic, 2024b)0.19280.69260.13350.65480.10580.65840.06970.88600.23640.65820.15560.8892Claude-3 (Anthropic, 2024a)0.10440.68330.07130.59100.10420.65980.06880.86960.18160.91580.16630.6644Gemini-1.5-pro (Deepmind, 2024)0.17420.69020.12020.67740.07080.65220.04620.86880.24860.66730.16590.9240Llama3-70B-Instruct (Int4) (Dubey et al., 2024)0.14040.66750.09370.54740.06700.64780.04340.73780.17520.65760.11520.7650Llama3-8B-Instruct (Dubey et al., 2024)0.02420.66490.01610.38120.02600.63030.01640.57000.08480.61670.05230.7216Llama3.1-8B-Instruct (Dubey et al., 2024)0.02280.70200.01600.38620.03950.65410.02580.63870.13000.62740.08160.6905Mistral-7B-Instruct-v0.2 (Jiang et al., 2023)0.00780.67320.00530.29860.01020.63090.00640.45240.00480.60120.00290.4020Qwen2-7B-Instruct (Yang et al., 2024)0.01100.90610.01000.26220.00100.86450.00900.07960.00220.86010.00190.0622Yi-1.5-9B (Young et al., 2024)0.03920.68480.02680.61700.02080.64070.01330.70720.01260.69450.00880.6521Chatglm-9B (GLM et al.,0.00020.74830.00010.21310.02540.71890.01830.46820.00000.69080.00000.5926Llama3.2-1B-Instruct (Dubey et al., 2024)0.00400.68070.00270.18500.00800.74650.00600.22260.00080.74610.00060.2818Gemma3-12B (Team et al., 2025)0.04600.60170.02770.34140.02800.59730.01670.33180.00060.59400.00040.3224Deepseek-R1-distill-Qwen-7B (Guo et al., 2025)0.15740.66110.10410.48150.16310.72640.11850.57580.03850.62480.02410.5634MolT5-small (Edwards et al., 2022)0.00060.65860.00040.66100.00640.59800.00380.62020.01140.52870.00600.8354MolT5-base (Edwards et al., 2022)0.00080.68680.00050.75600.00700.65090.00460.84220.01300.54640.00710.8382MolT5-large (Edwards et al., 2022)0.01500.71030.01070.84120.01180.56110.00660.89160.03820.60880.02330.9406BioT5-base (Pei et al., 2023)0.01180.83530.00990.99500.00780.66670.00520.99920.04760.67920.03230.9998Llama3.2-1B (OpenMolIns-large)0.01440.64900.00930.56160.03500.61500.02150.61860.02520.63730.01610.4412Llama3.1-8B (OpenMolIns-large)0.01360.66340.00900.75820.05440.66140.03600.74560.13440.63960.08600.6435Llama3.1-8B (OpenMolIns-xlarge)0.11860.68340.08110.85400.11960.67460.08070.90000.35480.63930.22680.9492Galactica-125M (OpenMolIns-light)0.00440.60540.00260.79300.02160.57240.01240.75960.02440.57560.01400.8442Galactica-125M (OpenMolIns-small)0.01460.65680.00960.84240.05300.63650.03370.79260.05700.59540.03390.8874Galactica-125M (OpenMolIns-medium)0.02940.65530.01930.86980.06220.64730.04030.74740.08820.60910.05370.8932Galactica-125M (OpenMolIns-large)0.04640.67290.03120.91160.07160.66950.04790.73740.09960.62760.06250.8966Galactica-125M (OpenMolIns-xlarge)0.18620.68990.12850.93080.16560.68870.11400.79520.20060.64450.12930.9162
https://platform.openai.com/docs/
https://docs.vllm.ai
https://huggingface.co/docs/
return falsePlease substitute a hydroxyl in the molecule O<a href="CNCc1cccnc1">C@@H</a>c1ccco1 by aldehyde.Requirement LLM ResponsesGPT-4o: O=CC(CNCc1cccnc1)c1ccco1Claude-3.5:O=CC@H(CNCc1cccnc1)c1ccco1BioT5-base:Figure8: Case study of SubComponent.In this example, we require LLMs to substitute the hydroxyl from the given molecule by a aldehyde.Here, although BioT5-base added a aldehyde, it failed to remove the hydroxyl.In contrast, the remaining models successfully passed the testing process.Please modify the molecule O=C(COc1ccc(Br)cc1)NOCc1ccccc1 to increase its LogP value.Figure13: Case study of BondNum.In this example, we require LLMs to generate a molecule with 6 aromatic bonds, which normally exist in benzene rings.Here, GPT-4o, Claude-3.5,Llama3.1-8B-Instruct(OpenMolIns-large), and Galactica-125M (OpenMolIns-xlarge) captured the concept and generated molecules with a benzene ring in their structures.In contrast, BioT5-base failed to identify the requirement and Llama3.1-8B-Instructgenerated too may aromatic rings.Requirement
GPT-3.5-turbo Llama3-70B-Instruct Llama3. OpenMolIns-large</p>
<p>Galactica-125M (OpenMolIns-xlarge). </p>
<p>. -125m Galactica, OpenMolIns-large</p>
<p>Galactica-125M (OpenMolIns-medium). </p>
<p>. -125m Galactica, OpenMolIns-small</p>
<p>Galactica-125M (OpenMolIns-light) Llama3.2-1B. OpenMolIns-large</p>
<p>Chatglm-9B yi-1.5-9B Llama3.1-8B-Instruct Llama3-8B-Instruct Mistral-7B-Instruct-v0. </p>
<p>Qwen2-7B-Instruct Llama3.2-1B-Instruct BioT5-base MolT5-large MolT5-base MolT5-small Proprietary Models Open-source General LLMs OpenMolIns Fine-tuned LLMs ChEBI-20 Fine-tuned LLMs. </p>
<p>Gemma3-12B Deepseek-R1-distill-Qwen-7B Llama3.1-8B. OpenMolIns-xlarge</p>
<p>REFERENCES Anthropic. Claude-3. 2024a</p>
<p>. Anthropic, Claude, -3.5, 2024b</p>
<p>Lead optimization in drug discovery. Mariana Pegrucci Barcelos, Suzane Quintana Gomes, Leonardo Bruno Federico, Isaque Antonio Galindo Francischini, Lorane Izabel Da, Silva Hage-Melim, Guilherme Martins Silva, Carlos Henrique Tomich De, Paula Da, Silva , Research topics in bioactivity, environment and energy: experimental and theoretical tools. Springer2022</p>
<p>Unsupervised data base clustering based on daylight's fingerprint and tanimoto similarity: A fast and automated way to cluster small and large data sets. Darko Butina, Journal of Chemical Information and Computer Sciences. 3941999</p>
<p>Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, Lawrence Zitnick, arXiv:1504.00325Microsoft coco captions: Data collection and evaluation server. 2015arXiv preprint</p>
<p>Gemini-1.5-pro. Google Deepmind, 2024</p>
<p>Molgensurvey: A systematic survey in machine learning models for molecule design. Yuanqi Du, Tianfan Fu, Jimeng Sun, Shengchao Liu, arXiv:2203.145002022arXiv preprint</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Text2mol: Cross-modal molecule retrieval with natural language queries. Carl Edwards, Chengxiang Zhai, Heng Ji, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Translation between molecules and natural language. Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, Heng Ji, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>L+ m-24: Building a dataset for language+ molecules@ acl 2024. Carl Edwards, Qingyun Wang, Lawrence Zhao, Heng Ji, The 1st Workshop on Language+ Molecules. 20241</p>
<p>The lab of the future: Self-driving labs for molecule discovery. Sean Ekins, GEN Biotechnology. 322024</p>
<p>Mol-instructions: A large-scale biomolecular instruction dataset for large language models. Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, Huajun Chen, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Glm Team, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, arXiv:2406.12793A family of large language models from glm-130b to glm-4 all tools. 2024arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Material design for next-generation mrna vaccines using lipid nanoparticles. Akon Higuchi, Tzu-Cheng Sung, Ting Wang, Qing-Dong Ling, S Suresh Kumar, Shih-Tien Hsu, Akihiro Umezawa, Polymer Reviews. 6322023</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>The chemical basis of pharmacology. J Michael, John J Keiser, Brian K Irwin, Shoichet, Biochemistry. 49482010</p>
<p>Pubchem 2019 update: improved access to chemical data. Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, Nucleic acids research. 47D12019</p>
<p>Selfies: a robust representation of semantically constrained graphs with an example application in chemistry. Mario Krenn, Florian Häse, Pascal Friederich, Alán Aspuru-Guzik, arXiv:1905.1374120191arXiv preprint</p>
<p>. Greg Landrum. Rdkit documentation. Release. 42013</p>
<p>Evaluating determinant priority of license fee in biotech industry. Hee Jeong, Tae-Eung Lee, Eungdo Sung, Kwangsoo Kim, Shin, Journal of Open Innovation: Technology, Market, and Complexity. 43302018</p>
<p>Large language models are in-context molecule learners. Jiatong Li, Wei Liu, Zhihao Ding, Wenqi Fan, Yuqiang Li, Qing Li, arXiv:2403.041972024aarXiv preprint</p>
<p>Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective. Jiatong Li, Yunqing Liu, Wenqi Fan, Xiao-Yong Wei, Hui Liu, Jiliang Tang, Qing Li, IEEE Transactions on Knowledge and Data Engineering. 2024b</p>
<p>Jiatong Li, Yunqing Liu, Wei Liu, Jingdi Le, Di Zhang, Wenqi Fan, Dongzhan Zhou, Yuqiang Li, Qing Li, arXiv:2411.14721Molreflect: Towards in-context fine-grained alignments between molecules and texts. 2024carXiv preprint</p>
<p>Jiatong Li, Weida Wang, Qinggang Zhang, Junxian Li, Di Zhang, Changmeng Zheng, Shufei Zhang, Xiaoyong Wei, Qing Li, arXiv:2508.08401Mol-r1: Towards explicit long-cot reasoning in molecule discovery. 2025arXiv preprint</p>
<p>Towards 3d molecule-text interpretation in language models. Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, Tat-Seng Chua, Qi Tian, The Twelfth International Conference on Learning Representations. 2024d</p>
<p>Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, Tat-Seng Chua, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Automated end-to-end workflow for volumetric mass-transfer coefficient (k l a) characterization in small-molecule pharmaceutical development. Keith Mattern, Shane T Grosser, Organic Process Research &amp; Development. 27112023</p>
<p>. Openai, Gpt, -4. 2023</p>
<p>Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations. Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, Rui Yan, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Rethinking molecular similarity: comparing compounds on the basis of biological activity. Benjamin Paula M Petrone, Florian Simms, Eugen Nigsch, Peter Lounkine, Allen Kutchukian, Zhan Cornett, John W Deng, Jeremy L Davies, Meir Jenkins, Glick, ACS chemical biology. 782012</p>
<p>Zinc 15-ligand discovery for everyone. Teague Sterling, John J Irwin, Journal of chemical information and modeling. 55112015</p>
<p>Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, Ji-Rong Wen, arXiv:2209.05481A molecular multimodal foundation model associating molecule graphs with natural language. 2022arXiv preprint</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, arXiv:2503.19786Morgane Rivière, et al. Gemma 3 technical report. 2025arXiv preprint</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, Journal of chemical information and computer sciences. 2811988</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, arXiv:2407.10671Fei Huang, et al. Qwen2 technical report. 2024arXiv preprint</p>
<p>Open foundation models by 01. Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, arXiv:2403.046522024ai. arXiv preprint</p>
<p>A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Zheni Zeng, Yuan Yao, Zhiyuan Liu, Maosong Sun, Nature communications. 1318622022</p>
<p>Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, arXiv:2402.06852A chemical large language model. 2024arXiv preprint</p>
<p>Prompt Templates for MolOpt LogP Please optimize the molecule {} to have a lower/higher LogP value. Modify the molecule {} to decrease/increase its LogP value. Optimize the molecule {} to have a lower/higher LogP value. Please modify the molecule {} to decrease/increase its LogP value. Modify the molecule {} to have a lower/higher LogP value. MR Please optimize the molecule {} to have a lower/higher MR value. Modify the molecule {} to decrease/increase its MR value. Optimize the molecule {} to have a lower/higher MR value. Please modify the molecule {} to decrease/increase its MR value. Modify the molecule {} to have a lower/higher MR value. QED Please optimize the molecule {} to have a lower/higher QED value. Modify the molecule {} to decrease/increase its QED value. Optimize the molecule {} to have a lower/higher QED value. Please modify the molecule {} to decrease/increase its QED value. Modify the molecule {} to have a lower/higher QED value. Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, Guolin Ke, The Eleventh International Conference on Learning Representations. 20231Uni-mol: A universal 3d molecular representation learning framework. Please add a hydroxyl to the molecule CC[NH+]1CCC[C@H]1CN(C)C(=O)c1ccc(NC)cc1. Requirement LLM Responses GPT-4o: CC[NH+]1CCCC@H1CN(C)C(=O)c1ccc(NC</p>
<p>@h[c@h] 1cn, C)C(=O)c1ccc(NC. 1</p>
<p>BioT5-base: CCN1CCC[C@@H1]1CNC(=O)C2=CC=C(O)C(C3=CC=CC=C3). C2</p>
<p>Here, GPT-4o and Claude-3.5 barely modified the molecule and failed to add a hydroxyl to the given molecule, while other four models successfully passed the testing process. However, it can be clearly noted that although BioT5-base passed the testing process, the generated molecule of BioT5-base could be quite different from the original molecule. Remove a amine from the molecule CC(C)COCCNC(=O)NNc1ccccc1Cl. Figure 6: Case study of AddComponent. Requirement LLM Responses GPT-4o: CC(C)COCCNC(=O</p>
<p>Claude-3.5: CC(C)COCCNC(=O). </p>
<p>BioT5-base: CCOCCCOCCOCCN(C1=CC=C2C=C1Cl). C2C</p>
<p>Llama3.1-8B-Instruct: CC(C)COCCNC(=O). </p>
<p>OpenMolIns-large): CC(C)COCCNC(=O). </p>
<p>Galactica-125M (OpenMolIns-xlarge):CC(C)COCCNC(=O). </p>            </div>
        </div>

    </div>
</body>
</html>
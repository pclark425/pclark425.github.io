<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1272 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1272</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1272</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-268063298</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.18866v2.pdf" target="_blank">Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming</a></p>
                <p><strong>Paper Abstract:</strong> Model-based reinforcement learning (MBRL) has been a primary approach to ameliorating the sample efficiency issue as well as to make a generalist agent. However, there has not been much effort toward enhancing the strategy of dreaming itself. Therefore, it is a question whether and how an agent can"dream better"in a more structured and strategic way. In this paper, inspired by the observation from cognitive science suggesting that humans use a spatial divide-and-conquer strategy in planning, we propose a new MBRL agent, called Dr. Strategy, which is equipped with a novel Dreaming Strategy. The proposed agent realizes a version of divide-and-conquer-like strategy in dreaming. This is achieved by learning a set of latent landmarks and then utilizing these to learn a landmark-conditioned highway policy. With the highway policy, the agent can first learn in the dream to move to a landmark, and from there it tackles the exploration and achievement task in a more focused way. In experiments, we show that the proposed model outperforms prior pixel-based MBRL methods in various visually complex and partially observable navigation tasks.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1272.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1272.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>2D-9room</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>2D Navigation - 9-room layout</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A partially-observable 2D navigation environment with 9 rooms (3x3) providing egocentric 5x5 views (64x64x3 pixels) where goals are placed at room centers or corners; used to evaluate long-horizon exploration and goal-reaching.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>2D Navigation (9-room)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Top-down 2D rooms (each 15x15) with limited egocentric visibility (5x5 window) and continuous non-episodic exploration; agent must navigate to pixel-goals placed in room centers or corners.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>room-graph like layout (rooms connected by corridors/doors); described as larger/sparser as layout size increases; not densely connected</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>9 rooms (each 15x15 cells), egocentric observation 5x5</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Dr. Strategy (Highway + Explorer + Achiever)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-based agent using a learned world model (RSSM), VQ-VAE latent landmarks, a landmark-conditioned Highway policy to move to landmarks, an Explorer trained with disagreement intrinsic reward, and an Achiever trained with focused sampling (goal-conditioned).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>success rate to unseen pixel goals (zero-shot), sample efficiency (# environment samples to reach success thresholds), and expected exploration reward of landmark rollouts (Ci)</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>94.03% (after 2M interaction steps) for Dr. Strategy; baselines much lower (e.g., LEXA 19.75%, LEXA-Explore 16.04%, GC-Director 28.08%)</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Hierarchical landmark-conditioned (divide-and-conquer): Highway policy to reach landmarks + local goal-conditioned Achiever trained with focused sampling</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Larger/higher effective diameter or more rooms causes non-strategic agents (flat goal-conditioned trained from random replay) to suffer large performance drops; landmark-based hierarchical policies (divide-and-conquer) greatly mitigate this by starting local achievement near landmarks and focusing exploration from high-potential landmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Comparing 9-room vs larger layouts shows Dr. Strategy retains high success in 9-room (≈94%) while non-strategic baselines perform poorly; performance degradation of flat agents grows with layout size.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Hierarchical policies that exploit landmarks + focused local training (Focused Sampling) produce much higher success and precision; the Achiever needs to be specialized for short-range navigation (local) to be effective in multi-room layouts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1272.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1272.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>2D-spiral9</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>2D Navigation - Spiral 9-room layout</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specially-designed 9-room layout organized in a spiral with gates/long paths to far rooms, intended to stress strategic exploration due to long paths and inefficient naïve exploration from start.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>2D Navigation (Spiral 9-room)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Spiral arrangement of 9 rooms with closed gates highlighted in the figure; objectives placed at room centers or corners, making the farthest room require long traversals and strategic planning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Closed gates are present (illustrated in white in figures); paper highlights closed gates in spiral layout but does not detail mechanics (e.g., require keys) — they serve as structural constraints increasing path length.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>sparse, elongated/serial connectivity due to spiral arrangement producing long shortest-paths to some rooms (high effective diameter)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>9 rooms (spiral arrangement), rooms size 15x15, egocentric obs 5x5</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Dr. Strategy (Highway + Explorer + Achiever)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same landmark-based hierarchical model; uses Curious Landmark selection (Ci) to pick landmarks with high expected future exploration reward and Highway policy to reach them before local exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>success rate to unseen pixel goals, expected exploration reward of landmark rollouts (Ci), sample efficiency</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>96.50% (after 2M interaction steps) for Dr. Strategy; baselines much lower (LEXA 21.19%, LEXA-Explore 20.16%, GC-Director 30.45%)</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Strategic/hierarchical landmark-based planning (divide-and-conquer) to reach far rooms efficiently</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Long path structure (high effective diameter) makes random replay-based dreaming ineffective; evaluating a discrete set of landmarks and selecting 'curious' ones enables efficient coverage and reaching far rooms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Spiral layout particularly highlights value of strategic dreaming: Dr. Strategy retains near-perfect success whereas flat/naive-dreaming baselines fail to discover distant rooms reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Selecting and routing to high-potential landmarks (high Ci) before local exploration strongly improves exploration efficiency in serial/high-diameter topologies; local Achiever precision further improves final goal reach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1272.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1272.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>2D-25room</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>2D Navigation - 25-room layout</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger 2D navigation layout (25 rooms) used to evaluate scaling of exploration and goal-reaching performance as environment size increases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>2D Navigation (25-room)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Top-down 25-room environment (larger state space than 9-room) with partially observable egocentric views; tests agent scalability to more rooms and sparser coverage needs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>larger/sparser room graph with more nodes and longer routes between many pairs of rooms (higher overall search space)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>25 rooms, rooms 15x15, egocentric obs 5x5</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Dr. Strategy (Highway + Explorer + Achiever)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Landmark-conditioned hierarchical agent with Curious Landmark selection; Achiever trained via focused sampling for local accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>zero-shot success rate, sample efficiency (# env samples to reach success thresholds), coverage/diversity (via K-NN diversity term in highway reward during training)</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>67.11% (Dr. Strategy after 2M steps); baselines much lower (LEXA 9.62%, LEXA-Explore 0.14%, GC-Director 27.11%)</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Hierarchical landmark-based with local-focused achiever; hierarchical structure helps, but scaling landmarks and representation quality matters</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>As environment size increases (more rooms), non-strategic agents degrade substantially; Dr. Strategy still outperforms baselines but overall success drops vs smaller layouts, indicating limits due to landmark granularity and representational confusion.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Performance gap between Dr. Strategy and flat baselines increases with layout size; however Dr. Strategy's absolute performance also decreases (67% vs ~94% in 9-room), indicating sensitivity to landmark count/quality and larger graph complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Divide-and-conquer reduces required point-to-point paths to learn, but larger topologies require more/better landmarks; focused sampling remains critical for precise local achievement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1272.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1272.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-Maze-7x7</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D-Maze Navigation - Maze-7x7</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A first-person 3D maze environment (7x7) derived from a Memory Maze, providing visually-rich egocentric observations with narrower corridors and a smaller topological size than larger 2D maps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>3D-Maze (7x7)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>First-person 3D maze with egocentric visual input; smaller in area (~equivalent to 4 rooms of 2D map) with narrow corridors, used to test performance in visually complex but smaller topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>smaller maze graph with narrower corridors; fewer places to visit (reduced branching) relative to larger 2D maps</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>7x7 maze (grid cells), egocentric first-person visual observations</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Dr. Strategy (Highway + Explorer + Achiever)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same landmark-based hierarchical model; landmarks require more capacity for egocentric visual variance; Highway policy sometimes confuses visually-similar states.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>zero-shot success rate, sample efficiency</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Above 80% success rate reported for Dr. Strategy (exact numeric not specified); outperforms baselines</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Hierarchical landmark-conditioned policy; landmarks must capture egocentric visual distinctions</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Smaller maze size and narrower corridors reduce the number of distinct places to visit, which benefits non-strategic baselines and reduces the performance gap; nevertheless, strategic dreaming still improves success and sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Compared to larger 2D layouts, baselines perform relatively better in Maze-7x7 because of reduced exploration space; Dr. Strategy still leads but with a smaller margin.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Highway + Achiever remains useful; however, visual aliasing (different places appearing visually similar) can harm landmark identification and highway performance, indicating need for landmark representations robust to egocentric similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1272.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1272.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-Maze-15x15</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D-Maze Navigation - Maze-15x15</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger first-person 3D maze (15x15) with greater area and repeating visual regions that can confuse landmark-based routing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>3D-Maze (15x15)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Larger first-person 3D maze increasing topological complexity and potential for repeated/ambiguous visual regions; used to test scalability of landmark-based strategies in visually confusing topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>larger maze graph with more nodes and potential repeated-looking regions (visual aliasing), increasing ambiguity for landmark-conditioned policies</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>15x15 maze (grid cells), egocentric first-person observations</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Dr. Strategy (Highway + Explorer + Achiever)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Landmark-based hierarchical agent; uses landmarks decoded to model states to select goal-proximal landmarks and route via Highway policy, then switch to Achiever.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>zero-shot success rate, sample efficiency</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Hierarchical landmark-conditioned with careful landmark capacity/representations to avoid visual aliasing</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Larger area with visually-similar regions can confuse highway policy leading it to reach visually similar but temporally distant states; this reduces the advantage of strategic dreaming and narrows performance gap with baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Performance gap between Dr. Strategy and baselines is reduced in Maze-15x15 versus Maze-7x7 due to visual aliasing and repeated regions; increasing landmarks or improving representation may be necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>In high-ambiguity topologies, landmark conditioning needs stronger disambiguation (more landmarks or better embeddings); highway policies risk reaching the wrong but visually-similar states, implying need for additional memory or more detailed context in the policy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1272.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1272.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboKitchen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboKitchen robotic manipulation benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A robotic manipulation benchmark (third-person view, 7-DoF Franka arm) with multiple object-interaction goals; used to test whether strategic dreaming benefits short-horizon manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>RoboKitchen</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Third-person fully-observable robotic manipulation tasks involving microwave, kettle, switches, cabinets, etc.; goals are visually distinguishable object states.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>task state-space for manipulation is low-horizon/short-range compared to navigation; topology not explicitly described as graph-structured in paper</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Set of 12 visually-distinguishable goals; episode length 150 steps with action-repeat 2</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Dr. Strategy (Highway + Explorer + Achiever)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same world-model + landmark approach; however, scene is third-person and largely stationary so landmarks based on reconstruction may be less diverse.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>zero-shot success rate averaged over multiple evaluation episodes; sample efficiency</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Dr. Strategy shows comparable performance to LEXA and LEXA-Explore (exact percentages not provided); GC-Director performs worse.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>For short-horizon manipulation tasks, simple goal-conditioned policies (without heavy strategic dreaming) may suffice; hierarchical strategic dreaming gives limited benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Because manipulation tasks have short horizon and the view is stationary, the benefits of strategic dreaming and landmarks are reduced; landmark distinctiveness (from reconstruction) is lower, reducing utility.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Not applicable — RoboKitchen is qualitatively different (short-horizon, fully observable) and strategic dreaming provides limited or comparable gains rather than large improvements seen in navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Divide-and-conquer strategic dreaming is less crucial for short-horizon manipulation; Achiever precision and landmark distinctiveness are less critical in this domain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>First return, then explore <em>(Rating: 2)</em></li>
                <li>Discovering and achieving goals via world models <em>(Rating: 2)</em></li>
                <li>Planning goals for exploration <em>(Rating: 2)</em></li>
                <li>Go-Explore <em>(Rating: 2)</em></li>
                <li>RECODE <em>(Rating: 1)</em></li>
                <li>Choreographer <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1272",
    "paper_id": "paper-268063298",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "2D-9room",
            "name_full": "2D Navigation - 9-room layout",
            "brief_description": "A partially-observable 2D navigation environment with 9 rooms (3x3) providing egocentric 5x5 views (64x64x3 pixels) where goals are placed at room centers or corners; used to evaluate long-horizon exploration and goal-reaching.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "2D Navigation (9-room)",
            "environment_description": "Top-down 2D rooms (each 15x15) with limited egocentric visibility (5x5 window) and continuous non-episodic exploration; agent must navigate to pixel-goals placed in room centers or corners.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "room-graph like layout (rooms connected by corridors/doors); described as larger/sparser as layout size increases; not densely connected",
            "environment_size": "9 rooms (each 15x15 cells), egocentric observation 5x5",
            "agent_name": "Dr. Strategy (Highway + Explorer + Achiever)",
            "agent_description": "Model-based agent using a learned world model (RSSM), VQ-VAE latent landmarks, a landmark-conditioned Highway policy to move to landmarks, an Explorer trained with disagreement intrinsic reward, and an Achiever trained with focused sampling (goal-conditioned).",
            "exploration_efficiency_metric": "success rate to unseen pixel goals (zero-shot), sample efficiency (# environment samples to reach success thresholds), and expected exploration reward of landmark rollouts (Ci)",
            "exploration_efficiency_value": null,
            "success_rate": "94.03% (after 2M interaction steps) for Dr. Strategy; baselines much lower (e.g., LEXA 19.75%, LEXA-Explore 16.04%, GC-Director 28.08%)",
            "optimal_policy_type": "Hierarchical landmark-conditioned (divide-and-conquer): Highway policy to reach landmarks + local goal-conditioned Achiever trained with focused sampling",
            "topology_performance_relationship": "Larger/higher effective diameter or more rooms causes non-strategic agents (flat goal-conditioned trained from random replay) to suffer large performance drops; landmark-based hierarchical policies (divide-and-conquer) greatly mitigate this by starting local achievement near landmarks and focusing exploration from high-potential landmarks.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Comparing 9-room vs larger layouts shows Dr. Strategy retains high success in 9-room (≈94%) while non-strategic baselines perform poorly; performance degradation of flat agents grows with layout size.",
            "policy_structure_findings": "Hierarchical policies that exploit landmarks + focused local training (Focused Sampling) produce much higher success and precision; the Achiever needs to be specialized for short-range navigation (local) to be effective in multi-room layouts.",
            "uuid": "e1272.0",
            "source_info": {
                "paper_title": "Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "2D-spiral9",
            "name_full": "2D Navigation - Spiral 9-room layout",
            "brief_description": "A specially-designed 9-room layout organized in a spiral with gates/long paths to far rooms, intended to stress strategic exploration due to long paths and inefficient naïve exploration from start.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "2D Navigation (Spiral 9-room)",
            "environment_description": "Spiral arrangement of 9 rooms with closed gates highlighted in the figure; objectives placed at room centers or corners, making the farthest room require long traversals and strategic planning.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Closed gates are present (illustrated in white in figures); paper highlights closed gates in spiral layout but does not detail mechanics (e.g., require keys) — they serve as structural constraints increasing path length.",
            "graph_connectivity": "sparse, elongated/serial connectivity due to spiral arrangement producing long shortest-paths to some rooms (high effective diameter)",
            "environment_size": "9 rooms (spiral arrangement), rooms size 15x15, egocentric obs 5x5",
            "agent_name": "Dr. Strategy (Highway + Explorer + Achiever)",
            "agent_description": "Same landmark-based hierarchical model; uses Curious Landmark selection (Ci) to pick landmarks with high expected future exploration reward and Highway policy to reach them before local exploration.",
            "exploration_efficiency_metric": "success rate to unseen pixel goals, expected exploration reward of landmark rollouts (Ci), sample efficiency",
            "exploration_efficiency_value": null,
            "success_rate": "96.50% (after 2M interaction steps) for Dr. Strategy; baselines much lower (LEXA 21.19%, LEXA-Explore 20.16%, GC-Director 30.45%)",
            "optimal_policy_type": "Strategic/hierarchical landmark-based planning (divide-and-conquer) to reach far rooms efficiently",
            "topology_performance_relationship": "Long path structure (high effective diameter) makes random replay-based dreaming ineffective; evaluating a discrete set of landmarks and selecting 'curious' ones enables efficient coverage and reaching far rooms.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Spiral layout particularly highlights value of strategic dreaming: Dr. Strategy retains near-perfect success whereas flat/naive-dreaming baselines fail to discover distant rooms reliably.",
            "policy_structure_findings": "Selecting and routing to high-potential landmarks (high Ci) before local exploration strongly improves exploration efficiency in serial/high-diameter topologies; local Achiever precision further improves final goal reach.",
            "uuid": "e1272.1",
            "source_info": {
                "paper_title": "Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "2D-25room",
            "name_full": "2D Navigation - 25-room layout",
            "brief_description": "A larger 2D navigation layout (25 rooms) used to evaluate scaling of exploration and goal-reaching performance as environment size increases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "2D Navigation (25-room)",
            "environment_description": "Top-down 25-room environment (larger state space than 9-room) with partially observable egocentric views; tests agent scalability to more rooms and sparser coverage needs.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "larger/sparser room graph with more nodes and longer routes between many pairs of rooms (higher overall search space)",
            "environment_size": "25 rooms, rooms 15x15, egocentric obs 5x5",
            "agent_name": "Dr. Strategy (Highway + Explorer + Achiever)",
            "agent_description": "Landmark-conditioned hierarchical agent with Curious Landmark selection; Achiever trained via focused sampling for local accuracy.",
            "exploration_efficiency_metric": "zero-shot success rate, sample efficiency (# env samples to reach success thresholds), coverage/diversity (via K-NN diversity term in highway reward during training)",
            "exploration_efficiency_value": null,
            "success_rate": "67.11% (Dr. Strategy after 2M steps); baselines much lower (LEXA 9.62%, LEXA-Explore 0.14%, GC-Director 27.11%)",
            "optimal_policy_type": "Hierarchical landmark-based with local-focused achiever; hierarchical structure helps, but scaling landmarks and representation quality matters",
            "topology_performance_relationship": "As environment size increases (more rooms), non-strategic agents degrade substantially; Dr. Strategy still outperforms baselines but overall success drops vs smaller layouts, indicating limits due to landmark granularity and representational confusion.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Performance gap between Dr. Strategy and flat baselines increases with layout size; however Dr. Strategy's absolute performance also decreases (67% vs ~94% in 9-room), indicating sensitivity to landmark count/quality and larger graph complexity.",
            "policy_structure_findings": "Divide-and-conquer reduces required point-to-point paths to learn, but larger topologies require more/better landmarks; focused sampling remains critical for precise local achievement.",
            "uuid": "e1272.2",
            "source_info": {
                "paper_title": "Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "3D-Maze-7x7",
            "name_full": "3D-Maze Navigation - Maze-7x7",
            "brief_description": "A first-person 3D maze environment (7x7) derived from a Memory Maze, providing visually-rich egocentric observations with narrower corridors and a smaller topological size than larger 2D maps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "3D-Maze (7x7)",
            "environment_description": "First-person 3D maze with egocentric visual input; smaller in area (~equivalent to 4 rooms of 2D map) with narrow corridors, used to test performance in visually complex but smaller topologies.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "smaller maze graph with narrower corridors; fewer places to visit (reduced branching) relative to larger 2D maps",
            "environment_size": "7x7 maze (grid cells), egocentric first-person visual observations",
            "agent_name": "Dr. Strategy (Highway + Explorer + Achiever)",
            "agent_description": "Same landmark-based hierarchical model; landmarks require more capacity for egocentric visual variance; Highway policy sometimes confuses visually-similar states.",
            "exploration_efficiency_metric": "zero-shot success rate, sample efficiency",
            "exploration_efficiency_value": null,
            "success_rate": "Above 80% success rate reported for Dr. Strategy (exact numeric not specified); outperforms baselines",
            "optimal_policy_type": "Hierarchical landmark-conditioned policy; landmarks must capture egocentric visual distinctions",
            "topology_performance_relationship": "Smaller maze size and narrower corridors reduce the number of distinct places to visit, which benefits non-strategic baselines and reduces the performance gap; nevertheless, strategic dreaming still improves success and sample efficiency.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Compared to larger 2D layouts, baselines perform relatively better in Maze-7x7 because of reduced exploration space; Dr. Strategy still leads but with a smaller margin.",
            "policy_structure_findings": "Highway + Achiever remains useful; however, visual aliasing (different places appearing visually similar) can harm landmark identification and highway performance, indicating need for landmark representations robust to egocentric similarity.",
            "uuid": "e1272.3",
            "source_info": {
                "paper_title": "Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "3D-Maze-15x15",
            "name_full": "3D-Maze Navigation - Maze-15x15",
            "brief_description": "A larger first-person 3D maze (15x15) with greater area and repeating visual regions that can confuse landmark-based routing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "3D-Maze (15x15)",
            "environment_description": "Larger first-person 3D maze increasing topological complexity and potential for repeated/ambiguous visual regions; used to test scalability of landmark-based strategies in visually confusing topologies.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "larger maze graph with more nodes and potential repeated-looking regions (visual aliasing), increasing ambiguity for landmark-conditioned policies",
            "environment_size": "15x15 maze (grid cells), egocentric first-person observations",
            "agent_name": "Dr. Strategy (Highway + Explorer + Achiever)",
            "agent_description": "Landmark-based hierarchical agent; uses landmarks decoded to model states to select goal-proximal landmarks and route via Highway policy, then switch to Achiever.",
            "exploration_efficiency_metric": "zero-shot success rate, sample efficiency",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "Hierarchical landmark-conditioned with careful landmark capacity/representations to avoid visual aliasing",
            "topology_performance_relationship": "Larger area with visually-similar regions can confuse highway policy leading it to reach visually similar but temporally distant states; this reduces the advantage of strategic dreaming and narrows performance gap with baselines.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Performance gap between Dr. Strategy and baselines is reduced in Maze-15x15 versus Maze-7x7 due to visual aliasing and repeated regions; increasing landmarks or improving representation may be necessary.",
            "policy_structure_findings": "In high-ambiguity topologies, landmark conditioning needs stronger disambiguation (more landmarks or better embeddings); highway policies risk reaching the wrong but visually-similar states, implying need for additional memory or more detailed context in the policy.",
            "uuid": "e1272.4",
            "source_info": {
                "paper_title": "Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "RoboKitchen",
            "name_full": "RoboKitchen robotic manipulation benchmark",
            "brief_description": "A robotic manipulation benchmark (third-person view, 7-DoF Franka arm) with multiple object-interaction goals; used to test whether strategic dreaming benefits short-horizon manipulation tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "RoboKitchen",
            "environment_description": "Third-person fully-observable robotic manipulation tasks involving microwave, kettle, switches, cabinets, etc.; goals are visually distinguishable object states.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "task state-space for manipulation is low-horizon/short-range compared to navigation; topology not explicitly described as graph-structured in paper",
            "environment_size": "Set of 12 visually-distinguishable goals; episode length 150 steps with action-repeat 2",
            "agent_name": "Dr. Strategy (Highway + Explorer + Achiever)",
            "agent_description": "Same world-model + landmark approach; however, scene is third-person and largely stationary so landmarks based on reconstruction may be less diverse.",
            "exploration_efficiency_metric": "zero-shot success rate averaged over multiple evaluation episodes; sample efficiency",
            "exploration_efficiency_value": null,
            "success_rate": "Dr. Strategy shows comparable performance to LEXA and LEXA-Explore (exact percentages not provided); GC-Director performs worse.",
            "optimal_policy_type": "For short-horizon manipulation tasks, simple goal-conditioned policies (without heavy strategic dreaming) may suffice; hierarchical strategic dreaming gives limited benefit.",
            "topology_performance_relationship": "Because manipulation tasks have short horizon and the view is stationary, the benefits of strategic dreaming and landmarks are reduced; landmark distinctiveness (from reconstruction) is lower, reducing utility.",
            "comparison_across_topologies": false,
            "topology_comparison_results": "Not applicable — RoboKitchen is qualitatively different (short-horizon, fully observable) and strategic dreaming provides limited or comparable gains rather than large improvements seen in navigation.",
            "policy_structure_findings": "Divide-and-conquer strategic dreaming is less crucial for short-horizon manipulation; Achiever precision and landmark distinctiveness are less critical in this domain.",
            "uuid": "e1272.5",
            "source_info": {
                "paper_title": "Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "First return, then explore",
            "rating": 2,
            "sanitized_title": "first_return_then_explore"
        },
        {
            "paper_title": "Discovering and achieving goals via world models",
            "rating": 2,
            "sanitized_title": "discovering_and_achieving_goals_via_world_models"
        },
        {
            "paper_title": "Planning goals for exploration",
            "rating": 2,
            "sanitized_title": "planning_goals_for_exploration"
        },
        {
            "paper_title": "Go-Explore",
            "rating": 2,
            "sanitized_title": "goexplore"
        },
        {
            "paper_title": "RECODE",
            "rating": 1
        },
        {
            "paper_title": "Choreographer",
            "rating": 1,
            "sanitized_title": "choreographer"
        }
    ],
    "cost": 0.01435125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming
4 Jun 2024</p>
<p>Hany Hamed 
Subin Kim 
Dongyeong Kim 
Jaesik Yoon 
Sungjin Ahn 
Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming
4 Jun 2024A588F659BA9E49F0B75330AD89E82DEBarXiv:2402.18866v2[cs.LG]
Model-based reinforcement learning (MBRL) has been a primary approach to ameliorating the sample efficiency issue as well as to make a generalist agent.However, there has not been much effort toward enhancing the strategy of dreaming itself.Therefore, it is a question whether and how an agent can "dream better" in a more structured and strategic way.In this paper, inspired by the observation from cognitive science suggesting that humans use a spatial divide-and-conquer strategy in planning, we propose a new MBRL agent, called Dr. Strategy, which is equipped with a novel Dreaming Strategy.The proposed agent realizes a version of divide-and-conquer-like strategy in dreaming.This is achieved by learning a set of latent landmarks and then utilizing these to learn a landmark-conditioned highway policy.With the highway policy, the agent can first learn in the dream to move to a landmark, and from there it tackles the exploration and achievement task in a more focused way.In experiments, we show that the proposed model outperforms prior pixel-based MBRL methods in various visually complex and partially observable navigation tasks.</p>
<p>Introduction</p>
<p>A crucial capability of generalist agents, such as humans, is to explore environments and acquire the skills needed to achieve various goals, continuously and in an open-ended way.It is particularly important for these agents to become efficient explorers and achievers in an unsupervised or selfsupervised manner.It enables them to survive and become more competent in a more scalable way as well as in more flexible open-ended environments, where future tasks aren't predefined but can evolve over time.</p>
<p>This capability is equally important for artificial generalist agents, such as Reinforcement Learning (RL) agents (Sutton &amp; Barto, 2018), including robots and virtual agents in games like Minecraft (Guss et al., 2019).However, these artificial agents currently have a significant limitation compared to humans: low sample efficiency.They require much more experience data than humans (Mnih et al., 2015;2016).Considering these agents could operate in a real-time physical world and are susceptible to physical damage, improving sample efficiency is of top priority.It is particularly more challenging in more realistic settings where observations are high-dimensional (e.g., images) and partially observable (Berner et al., 2019;Vinyals et al., 2019).</p>
<p>Currently, a primary approach in RL to improving sample efficiency is via model-based reinforcement learning (MBRL) (Sutton, 1991;Ha &amp; Schmidhuber, 2018;Hafner et al., 2020).In this approach, the agent uses experience data to learn both the representation of the observations and states as well as the transition dynamics of the environment, known as a world model.This enables the agent to learn its policy within an internal model of the world instead of the real world via planning (or, simulation or dreaming).An example of such an unsupervised model-based generalist agent is LEXA (Mendonca et al., 2021).</p>
<p>On the other hand, research in cognitive science suggests that humans use structured and strategic planning, such as spatial divide-and-conquer, when tackling complex problems (Chun &amp; Jiang, 1998).For example, when navigating to a specific location, humans typically break down the task into two stages: first, they plan to reach a familiar landmark near the destination, then they use a local and focused strategy to get from that landmark to the target, as shown in Figure 1.This divide-and-conquer-like approach is effective as it reduces the space to learn.Without this, it would require to learn all point-to-point navigation paths separately, requiring a lot of experience data.However, in current MBRL agents like LEXA, the process of dreaming or imagination is guided by a rather naive strategy such as random i.i.d.sampling from the replay buffer.</p>
<p>In this paper, we raise the following questions: "Is more structured and strategic dreaming possible?",if so, "how could we implement this idea in the modern MBRL frameworks?" and "how could this improve generalist agents?"To this end, we propose a strategic model-based generalist agent, Dr. Strategy (short for "Dream Strategy").Our key idea is that a divide-and-conquer approach leveraging the structure of latent landmarks can enhance the efficiency of dreaming in MBRL and promote better exploration and achievement quality of a generalist agent.</p>
<p>The proposed model consists of four main modules.First, to obtain landmarks, we map each state from the replay buffer to a discrete representation called landmarks through VQ-VAE (Razavi et al., 2019).Second, we train a landmarkconditioned policy called highway policy, specialized to move only to landmarks instead of arbitrary position, unlike goal-conditioned policy.Thirdly, we train an exploration policy (Explorer) and a goal-conditioned policy (Achiever) through dreaming.However, unlike LEXA, the two policies take advantage of starting from beneficial landmarks selected from strategic dreaming and planning.Thus, they solve the problem locally in a focused way, following the highway policy to bring the agent to the selected landmark.This realizes the divide-and-conquer-like approach.In experiments, we show that the proposed model outperforms prior pixel-based MBRL methods in various visually complex and partially observable navigation tasks, while also showing comparative results in robot manipulation tasks.</p>
<p>The main contributions of this paper are as follows.We propose the concept of "strategic dreaming" in pixel-based MBRL in the sense that the agent can leverage the structure of the state space such as landmarks to enable a divide-andconquer-like strategy during dreaming, and then propose the first MBRL agent to realize and demonstrate the benefits of this concept.We also provide empirical evidence that this approach can enhance the accuracy and efficiency of MBRL agents in the generalist setting similar to LEXA.Additionally, we also introduce a set of benchmarks for visually complex navigation tasks.</p>
<p>Dr. Strategy Agent</p>
<p>To enable a structured divide-and-conquer approach and thus enhance the efficiency of dreaming in world models for goal-conditioned agents, we introduce our proposed model, Dr. Strategy.A key change to prior model-based goal-conditioned approaches is the use of latent landmarks.Latent landmarks are a set of latent states representing the experience of the agent, which enables the agent to strategically focus on essential information and thus dream structurally.In our proposed model, we divide our experience via landmarks and conquer by starting from the landmarks, thereby guiding the agent to explore and achieve goals efficiently and with precision.We call the overall process of training and planning to exploit the divide-and-conquer strategy "Strategic Dreaming".</p>
<p>Dr. Strategy consists of three policies: the Highway policy, which helps reach landmarks; Explorer, which explores distant points using the world model; and Achiever, which reaches specified goals in divided areas.Additionally, we incorporate Focused Sampling during Achiever training to increase accuracy.As illustrated in Figure 2, our approach consists of two phases: (1) We construct latent landmarks from the explored states (Section 2.2), train the three policies in imagination through Strategic Dreaming (Section 2.3), and then explore through curious landmark-guided exploration (Section 2.4).(2) We then achieve downstream tasks in the real environment exploiting the Highway policy and Achiever (Section 2.5).</p>
<p>World Model</p>
<p>To enhance the accurate prediction by high-dimensional pixel-level inputs, we employ a Recurrent State Space Model (RSSM) (Hafner et al., 2019b).The world model works as a virtual simulator, predicting the transition dynamics of the real environment.The policy interacts with the imagined trajectories generated in parallel by sampling from the world model.We refer to this as "Dreaming".Thus, we can train policies using the imagined trajectories instead of interacting directly with the real environment (refer to Appendix B for more details).The components comprising the world model include:
Dynamics : ŝt = dyn θ (s t−1 , a t−1 )(1)
Representation :
s t = repr θ (s t−1 , a t−1 , x t ) (2)
Encoder : e t = enc θ (x t )</p>
<p>(3)
Decoder : xt = dec θ (s t ),(4)
where s t is the model state which is constructed as a concatenation of a deterministic state from GRU (Cho et al., 2014) and a discrete stochastic state (Hafner et al., 2020).a t and x t are action and observation, respectively.The world model is trained by optimizing the evidence lower bound (ELBO) through stochastic backpropagation (Kingma &amp; Welling, 2013;Rezende et al., 2014) using the Adam optimizer (Kingma &amp; Ba, 2014).</p>
<p>Building Latent Landmarks</p>
<p>We project the model states onto discrete N codes in the codebook we call landmarks using the VQ-VAE (Van Den Oord et al., 2017).Landmarks can be seen as cluster centers partitioning the state space into a number of codes in the codebook.To exploit these landmarks, we train the latent landmark-conditioned policy Highway policy that works as an express train for the agent to go to the landmarks.</p>
<p>To find landmarks that can represent an area of the given distribution over states, we learn the landmark encoder enc ϕ (s) and decoder dec ϕ (l) through VQ-VAE.We aim to encode model states s into the N learnable codes which we call landmark l of a codebook, and vice versa.</p>
<p>We encode the model states into embeddings using landmark encoder enc ϕ (s).For quantization, the embedding enc ϕ (s) is assigned to the closest code in the codebook l k where k = arg min j ∥enc ϕ (s)
− l j ∥ 2 , k ∈ 1 • • • N .
With the landmark decoder dec ϕ (l k ) , l k can be decoded back to state s.The training objective is
L l = ∥s − dec ϕ (l k )∥ 2 2 + β∥sg(l k ) − enc ϕ (s)∥ 2 2 , (5)
where sg(•) denotes stop gradient.The loss is composed of reconstruction error of the decoder and commitment loss, which is the difference between embedded vectors and the codes in the codebook.The balance in the loss is managed by the hyperparameter β.We assign only a single code in the codebook to each model state.Thus, landmarks can be seen as cluster centers partitioning the model states into the number of codes in the codebook (Mazzaglia et al., 2022b;Campos et al., 2020).</p>
<p>Building Blocks for Strategy</p>
<p>Highway policy.We train a landmark-conditioned policy π l (a t |s t , l) called Highway policy through imagined trajectories.Given a target landmark l, the objective of this policy is to reach the state of the target landmark ŝl = dec ϕ (l).</p>
<p>To train the Highway policy, we design the reward with two terms:
r l (s t , l) = −∥dec ϕ (l) − s t ∥ 2 2 + K i=1 log∥s t − s K-NN i ∥ 2 (6)
The first term calculates the distance in the state space between the visited state s t and the decoded state from the conditioned landmark code l.This encourages the agent to reach the decoded state of l.The second term is estimated using a K-NN particle-based estimator (Singh et al., 2003), which motivates the agent to visit diverse states within one trajectory.</p>
<p>Explorer and Achiever.We follow prior approaches based on goal-conditioned MBRL framework (Mendonca et al., 2021).Explorer is an exploration policy π e (a t |s t ) trained by receiving exploration reward r e (s t ).r e (s t ) encourages the policy to maximize the disagreement among an ensemble of 1-step dynamics models (Pathak et al., 2019;Sekar et al., 2020).As the explorer trains in imagination, we start the imagined trajectories not only from sampled data from the replay buffer but also from landmarks.We call the goal-conditioned policy π g (a t |s t , e g ) Achiever that receives current model state and goal embedding e g = enc θ (x g ) as inputs, where x g is the goal image.The reward for reaching a goal r g (ê t , e g ) is based on a self-supervised objective that focuses on the temporal distance that follows prior works (Mendonca et al., 2021), where it encourages the policy to reduce the number of actions needed to move from the current state to the goal state.êt = emb(s t ) ≈ e t is the predicted image embedding at step t (refer to Appendix D for more details).</p>
<p>Strategy to Explore</p>
<p>How can the generalist agent strategically dream to explore during training time so that it can achieve diverse goals?</p>
<p>Prior works leverage the world model for planning from randomly sampled candidate states (Mendonca et al., 2021).However, in large or complex search spaces, chances of stumbling upon good solutions by random sampling are typically low (Ecoffet et al., 2021).This leads to a lot of computational resources wasted on exploring sub-optimal areas.Instead, we propose to plan strategically through dreaming by only evaluating the landmarks.By constructing the landmarks to represent the agent's experience (divide) and evaluating (conquer) only the representations of the explored space, we can gain a comprehensive approximation with efficiency.We also refer to this strategy as "strategic exploration."</p>
<p>We call the landmark with the highest exploration potential "Curious Landmark".We then move to the curious landmark via Highway policy, then resume to explore immediately with Explorer.</p>
<p>Curious landmark should lead us to effective exploration in the future, entailing high future exploration reward potential.To select a curious landmark, we get the decoded model state s
(i) 0 ∼ dec ϕ (l i ), i ∈ 1 .
. .N of each landmark via landmark decoder.We then imagine H steps trajectories with the Explorer through world model from each landmark,
τ i = {s (i) 0 , s (i) 1 , . . . , s(i)
H }. We calculate the curiosity C i of landmark l i as the expected exploration reward of τ i :
C i = E τi [r e (s (i) t )], τ i = {s (i) 0 , s (i) 1 , . . . , s (i) H } (7)
Such that r e represents the exploration reward, as previously mentioned.We then sample the Curious Landmark l C with the probability of C i .The curiosities of the landmarks are updated during the explorer's training.</p>
<p>Note that we are evaluating discrete states (landmarks), each playing a role as cluster centers dividing the model states into N partitions.This enables us to have a comprehensive evaluation of the covered space efficiently, and Dream Strategically takes advantage of the divide-and-conquer-like approach.</p>
<p>Landmark-guided Exploration.During exploration, we iterate over three phases: Every iteration starts with selecting a Curious Landmark l C .Then, we exploit the Highway policy π l (a t |s t , l C ) in the environment to reach l C .If the Highway policy has been running for more than T L steps, Explorer takes over immediately and starts to explore.However, if the current state s t is near enough s l C ∼ dec ϕ (l C ) where the difference is under a certain threshold before T L , the agent switches to Explorer as well.</p>
<p>Explorer can start from a position with high exploration potential right away, reducing the time to visit previously well-known places and collecting high exploration value trajectories.The iteration is repeated every T F step, maintaining a hierarchical structure.</p>
<p>Strategy to Achieve</p>
<p>How can the agent efficiently train to reach numerous userdefined goals at test time?Is there a way to exploit the divide-and-conquer manner of strategically dreaming at test time?We introduce the divide-and-conquer strategy once again, by finding the landmark that is nearest to the given goal and utilizing the Highway policy to reach the area closest to the goal (divide).Only then we exploit a local goal-conditioned policy trained to reach between close states (conquer).We call this goal-conditioned policy "Achiever with Focused Sampling", where it is trained to move between nearby states, thereby precisely mastering local areas.We demonstrate that by leveraging the divideand-conquer strategy, we can achieve increased accuracy.We also refer to this strategy as "strategic achievement."Focused Sampling.Through the divide-and-conquer strategy, the Achiever π g (a t |s t , e g ) is expected to be positioned very close to the goal when the policy is triggered.Thus, it only needs to cover a very short distance to reach its destination.Instead of sampling random states from past trajectories like prior work (Mendonca et al., 2021), we sample two different observations x t , x t+k within the range T S in the same trajectory from the replay buffer.We use them as a starting state and goal state to train the Achiever, where s t is estimated through the world model from x t and e g is computed through the world model encoder enc θ (x t+k ).</p>
<p>Through this sampling, the policy is trained for the agent to navigate between states that are in close proximity, thereby improved sample efficiency is expected while exploiting the divide-and-conquer strategy to the full extent.We empirically investigate the efficacy of the focused sampling in our ablation study in Section 3.5.</p>
<p>Landmark-guided Achievement.At test time, we receive the user-defined pixel-level goal x g and estimate s g through the world model.The agent estimates the landmark l G nearest to the goal state s g , where l G = arg min j ∥enc ϕ (s g ) − l j ∥ 2 .We then utilize Highway policy π g (a t |s t , l G ) conditioned on l G .We switch to the Achiever π g (a t |s t , e g ) to reach the final goal when the highway policy has been running for more than T L steps, or when the current state is near enough to the landmark similar to landmark-guided exploration in Section 2.4).</p>
<p>We exploit Highway policy to move long distances conditioned on a small number of discrete landmarks, then utilize Achiever specialized to achieve nearby destinations, thereby achieving precision and scalability at the same time.</p>
<p>Experiments</p>
<p>This section aims to evaluate the proposed agent by addressing the following questions: (1) Does Dr. Strategy demonstrate improved performance than prior goal-conditioned MBRL works in zero-shot adaptation?(2) What is the role of the "Strategy to Explore" in enhancing exploration?(3) How does "Strategy to Achieve" contribute to improving zero-shot performance?(4) Does "focused sampling" for training the Achiever improve zero-shot performance?</p>
<p>Environments and Tasks</p>
<p>To empirically investigate the proposed agent, we evaluate it in two types of navigation environments and a robot manipulation environment.One type of navigation environment is 2D navigation, in which the agent observes a partially observable limited top-down view as shown in Figure 3.We introduce three layouts: 9-room, 25-room, and spiral 9-room.The first two intend to test the agent's exploration capabilities in large spaces (Pertsch et al., 2020).The spiral 9-room layout (illustrated in Figure 7) is specifically designed to challenge our agent's strategic exploration.It provides such a scenario where the exploration from the starting point can be inefficient due to the longer path to the farthest room (Ecoffet et al., 2021).</p>
<p>We have designed a 3D-Maze navigation to evaluate the agent in a visually more complex environment, by modifying the Memory Maze environment (Pasukonis et al., 2022).This provides the first-person view observation.We evaluate the agent's performance on two maze sizes: Maze-7x7 and Maze-15x15.</p>
<p>Additionally, our evaluation extends to a robot manipulation environment, the RoboKitchen benchmark introduced in a prior work (Mendonca et al., 2021).It features a thirdperson view of a 7-DoF Franka Emika Panda robotic arm equipped with a gripper.We note that it is a fully observable environment.The RoboKitchen environment requires the agent to interact with various objects, including microwave, kettle, light switch, burner, sliding cabinet, and hinge cabinet.More details are discussed in Appendix A.</p>
<p>Baselines</p>
<p>We mainly compare Dr. Strategy with LEXA (Mendonca et al., 2021) because it is the closest model to ours but without the concept of strategic dreaming.It is also the state-of-the-art unsupervised model-based generalist agent for pixel-based observation tasks.In LEXA, the dreaming or imagination is guided by a rather naive strategy, i.e., random sampling from the replay buffer.</p>
<p>Regarding LEXA, it has been shown that only using the Explorer for the interaction can be better in a prior work (Hu et al., 2023).Thus, we also test this baseline named LEXA-Explore.LEXA, LEXA-Explore, and our model Director (Hafner et al., 2022) is a hierarchical model-based agent where a high-level policy (known as the manager) provides sub-goals to a low-level policy (known as the worker) to achieve a task defined by a reward function.We chose Director due to its hierarchical structure and use of sub-goals, which is similar to exploiting landmarks in Dr. Strategy.However, since Director is a task-specific agent and not a goal-conditioned agent, it lacks the ability to generalize to diverse goals not given during training.Thus, we develop a goal-conditioned version of Director, named GC-Director.GC-Director utilizes a form of structure in the state space to achieve the given goal.The implementation details are discussed in Appendix B.</p>
<p>Main Results</p>
<p>We conduct comparative analyses of our proposed agent with baselines across the three environments.The zero-shot evaluation performance is illustrated in Figure 4.These results are quantified based on the agent's success rate, which is determined by the distance to the goal.It is considered successful when the distance falls below a certain threshold (refer to Appendix D for more details).We note that the goal images are unseen during training and are user-defined during test time, and the agent has to reach there.</p>
<p>2D Navigation</p>
<p>In Table 1, Dr. Strategy shows an almost 100% success rate in 9-room and spiral 9-room after 2M interaction steps with a clear performance gap compared to other baselines.It is notable that our agent maintains a high success rate in spiral 9-room where the map is complicated and requires a longer range of exploration to achieve the goals in the farthest room.A similar trend is observed in the 25-room layout, where the performance decreased to 67.11%, as shown in Table 1.However, the performance gap here is over 40% compared to other baselines.Interestingly, as the layout size increases, the agents with non-strategic achievement, LEXA and LEXA-Explore show a more significant performance deterioration.Conversely, GC-Director shows less performance decrease than others.This result suggests that our strategic dreaming is more effective compared to the naive dreaming of LEXA or LEXA-Explore.</p>
<p>3D-Maze Navigation</p>
<p>For the Maze-7x7 environment providing visually more complex first-person observation, our agent achieves above 80% success rate (refer to Table 2) and significantly outperforms the baselines.Interestingly, all baselines, LEXA, LEXA-Explore, and GC-Director show better performances than they did for the 2D navigation environments.This may be due to the fact that the size of 3D-Maze Navigation maps is smaller than 2D Navigation maps: Maze-7x7 is about the size of four rooms in 2D Navigation maps and also has narrower corridors (as shown in Figure 3).This reduces the number of places the agent has to visit.With a smaller exploration space, this could be beneficial for baselines without strategic dreaming, leading to a smaller performance gap with Dr. Strategy.However, despite such factors, Dr. Strategy outperforms the baselines.</p>
<p>In Maze-15x15, our proposed agent outperforms the baselines yet, but the performance gap is reduced.It is because larger regions are identified with the same colors (illustrated in Figure 7), which causes confusion for the highway policy to identify the landmark positions.</p>
<p>RoboKitchen The results are shown in Figure 4 and Table 2. Dr. Strategy shows comparable performance with LEXA and LEXA-Explore, while GC-Director shows much worse performance than other agents.This is likely because of the environment's stationary view given in the third-person point, which decreases the visual distinctions between time steps.This can be critical in forming diverse and distinguishable landmarks based on reconstruction rewards.Furthermore, RoboKitchen tasks requires a short span of actions to achieve the goals compared to navigation tasks, which may reduce the need for strategic dreaming compared to other tasks.This is also supported by the low success rate of GC-Director, which also utilizes a hierarchical structure which is beneficial for long-horizon tasks.</p>
<p>Qualitative Results</p>
<p>To investigate more details of the improvement through the strategic imagination, we visualize the trajectories of our proposed agent and LEXA on the 25-room layout in the 2D Navigation environment in Figure 5.We find that our agent can reach more diverse, further goals with higher success rates.Moreover, we can examine the failures of LEXA: Both trajectories (A) and (C), highlighted as green boxes, aim to acquire goal 1.However, while trajectory (C) is able to reach the goal with high accuracy, trajectory (A) fumbles around the goal in close but not precise positions.It is because Dr. Strategy learns to achieve with high precision through focused sampling.This highlights the benefit of localizing the scope of the Achiever via the divide-andconquer strategy.Meanwhile, where both trajectories (B) and (D) aim to reach goal 2. However, trajectory (B) cannot even go near the desired goal 2. This shows the benefit of strategic dreaming, where the agent can find and move to the nearby area of goals, while flat models cannot plan such structured navigation and cannot locate near areas once it is lost.</p>
<p>Ablation Studies</p>
<p>We investigate the influence of three components of strategic dreaming: Strategy to Explore (Section 2.4), Strategy to Achieve, and focused sampling (Section 2.5).</p>
<p>Strategy to Explore.To investigate the efficacy of strategic exploration, we compared the Dr. Strategy with and without strategic exploration.The Dr. Strategy without strategic exploration explores the environment similar to LEXA or LEXA-Explore (Mendonca et al., 2021;Hu et al., 2023).</p>
<p>In Figure 6, we compare the variants of Dr. Strategy when the strategic exploration is applied and not in the aspect of the unseen goal achievement success rates.When comparing the variants with and without strategic exploration, we can find a clear performance gap regardless of equipping the strategic achievement and focused sampling.</p>
<p>Strategy to Achieve.We also hypothesized that strategic achievement could be crucial to improving the unseen goal achievement performance.To study this, we compare Dr. Strategy with and without strategic achievement.We note that the ablation version does not utilize the focused sampling, because the sampling is designed for strategic achievement.The result is shown in Figure 6.The performance gaps between with and without strategic achievement are clearly shown regardless of strategic exploration.Its performance gap is larger than the gap from the ablation study for strategic exploration especially for 9-room, where we can find that the major performance gain of our agent compared to the naive dreaming versions such as LEXA or LEXA-Explore happened through this strategic achievement.We note that we do not compare our agent with another model-based generalist agent PEG (Hu et al., 2023) that equips strategic exploration because it is designed for state-based environments.However, this result suggests that strategic exploration is not efficient enough, and the agent with strategic exploration and achievement (our agent) outperforms the agent only with strategic exploration such as PEG, and the agent without the strategic approach like us (LEXA).</p>
<p>Focused Sampling.To enhance achievement through the divide-conquer approach, we utilize focused sampling (Discussed in Section 2.5) for the Achiever to achieve near goals.</p>
<p>It is expected to improve the sample efficiency in Achiever training while fitting in the divide-conquer approach scenario.We study the expected efficacy by comparing it with the Dr. Strategy without focused sampling.It is shown in Figure 6 (compared with Dr. Strategy without Focused Sampling (FS)).Surprisingly, the performance gap is huge, and without focused sampling, the agent performance is similar to the ablation without strategic achievement and focused sampling.This result suggests that training the Achiever with the nearby goals from the starting point is crucial to improve the performance and it can be available through the strategic achievement with the latent landmarks and highway policy.</p>
<p>Related Work</p>
<p>As an unsupervised model-based generalist agent, LEXA (Mendonca et al., 2021) and PEG (Hu et al., 2023) are related to our work.However, LEXA is trained with naive strategic dreaming, which limits its performance in small size of state space (Hu et al., 2023).PEG extends LEXA to apply the strategic exploration by exploring from the samples estimated as interesting through the roll-out of the explorer in the imagination like us, but they did not validate their method to the pixel-based environment and extend this strategy to the achiever like us.In the aspect of training discrete representative states and policy in imagination, our work is related to Choreographer (Mazzaglia et al., 2022b).However, Chreographer fine-tuned the learned representative states and policy for the downstream task with a new hierarchical policy while ours is the unsupervised generalist agent.Director (Hafner et al., 2022), a hierarchical model-based agent can be related in the aspect of utilizing the intermediate state for solving the given task, but Director is designed for solving a single task, not the unsupervised generalist agent.</p>
<p>Dr. Strategy explores the environment from the interesting spot called Curious landmark.In (Ecoffet et al., 2021;Saade et al., 2023;Hu et al., 2023), this idea has been studied to address the inefficiency when exploring from the starting point (Pathak et al., 2017;Burda et al., 2019;Pathak et al., 2019;Mazzaglia et al., 2022a), while PEG (Hu et al., 2023) does not apply strategic achievement with this idea, and Go-Explore (Ecoffet et al., 2021) and RECODE (Saade et al., 2023) are model-free RL methods.</p>
<p>Our agent utilizes the goal-conditioned policies, the highway policy, and the achiever.The goal-conditioned policy has been studied to learn the trajectories in an unsupervised manner by sampling the goals from the data (Eysenbach et al., 2019a;Yarats et al., 2021;Park et al., 2022;2024;Mazzaglia et al., 2022b;Kim et al., 2023), or train the agent that can solve multiple tasks (Andrychowicz et al., 2017;Eysenbach et al., 2019b;Pong et al., 2020;Pitis et al., 2020;Mendonca et al., 2021;Hu et al., 2023;Hafner et al., 2022).However, these methods do not utilize a goal-conditioned policy (i.e., the highway policy) combined with the exploration policy and the achiever policy to improve exploration quality and the achievement of unseen goals.</p>
<p>Conclusion</p>
<p>In this paper, we propose Dr. Strategy, a novel model-based strategic, general-purpose agent.Inspired by the structured and strategic planning of humans, we designed this agent to utilize strategic dreaming for efficient exploration and goal achievement through planning.To do this, the agent learns the latent landmarks representing their experience and three distinct policies: navigating to the landmarks (Highway policy), exploring from the landmarks (explorer), and achieving the given goal from the landmarks (achiever).Different from the previous approaches (Mendonca et al., 2021;Hu et al., 2023), by separating the roles of the policies strategically, our agent showed better performances in diverse complex and partial observable navigation environments.Especially, the divide-and-conquer approach allows the achiever to learn from nearby samples, which dramatically improves the performance of the agent.</p>
<p>Limitations and future work.However, the agent has shown limited performance in a robotic manipulation environment.The performance improvement in those environments could be a future work.Additionally, the current agent treats the number of landmarks as a hyperparameter, but it would be interesting to make it gradually increase and adapt (Kulis &amp; Jordan, 2011).Another promising direction could be the integration of a hierarchical framework within the highway policy to extend the agent's exploration and goal-achievement capabilities.</p>
<p>Impact Statement</p>
<p>Strategic Dreaming, as implemented in the Dr. Strategy agent, represents a novel structure in model-based reinforcement learning, focusing on enhancing agents' planning capabilities to "dream" in a structured manner.This approach draws from cognitive science insights, employing a spatial divide-and-conquer strategy for problem-solving.In practical terms, Strategic Dreaming could revolutionize tasks that require complex spatial navigation and decision-making, such as urban planning, logistics, and autonomous vehicle routing.By enabling AI to efficiently learn and navigate through simulations, Strategic Dreaming can lead to more robust and reliable models that require less real-world data, thereby reducing the time and cost associated with training AI systems.</p>
<p>However, the implications of Strategic Dreaming extend beyond improved efficiency.As these agents become adept at navigating and planning in simulated environments, there is potential for them to supplant roles currently filled by humans, especially in fields that rely heavily on spatial and strategic planning.While this could lead to increased efficiency and safety, particularly in hazardous environments, it also raises societal and ethical questions about the displacement of jobs and the need for new frameworks to govern AI decision-making and accountability.However, such capabilites require more investigation and does not seem to be a near future.The development of Strategic Dreaming thus mandates a careful consideration of its societal impact, balancing the benefits of advanced navigation and planning capabilities with the ethical management of automation's societal effects.A. Environment 2D navigation.We introduce three 2D navigation environments with distinct layouts: 9-room, spiral 9-room, and 25-room to evaluate the performance of structured and strategic imagination in large environments.All environments are modeled as egocentric views with limited visibility, represented by a 5x5 sized observation window as 64x64x3 pixel observation as shown in Figure 3.The agent aims to navigate through rooms of size 15x15 to reach specific points within a 0.1 Manhattan distance tolerance in 1000 steps.We calculate the agent's success rate per goal by averaging the outcomes of three evaluation episodes.Each goal can be found at the center of a room or in the down-left corner of the 9-rooms and spiral 9-room layout and the center of a room in the 25-room layout as shown in Figure 8 (a).We note that these environments are non-episodic, requiring the agent to continuously explore and adapt without restarting episodes.) with a 0.1 Manhattan distance tolerance and 45 degrees of orientation tolerance, accomplishing this within 500 steps for the 7x7 maze and 1000 steps for the 15x15 maze.We measure whether the agent reached or not by three times and take an average to calculate the success rate per goal.The target points are strategically placed either at the dead ends of the maze or in proximity to the walls.Notably, these environments are non-episodic, requiring the agent to continually explore and adapt without restarting episodes.</p>
<p>Robokitchen.To demonstrate the broad applicability of our agent, we chose the RoboKitchen environment from LEXA (Mendonca et al., 2021) to evaluate its performance on robotic manipulation tasks requiring both structured and strategic imagination.We adopted the same setup as LEXA, setting the episode length to 150 steps with an action repeat factor of 2 with 12 visually distinguishable goals.We measure whether the agent reached or not by ten times and take an average to calculate the success rate per goal.</p>
<p>B. Baselines</p>
<p>A primary approach in reinforcement learning (RL) to improve sample efficiency is via model-based reinforcement learning (MBRL) (Sutton, 1991;Ha &amp; Schmidhuber, 2018).Dreamer (Hafner et al., 2019a;2020) is a MBRL agent that leverages the learning of an internal model, known as a world model (WM), to train an agent in dreaming also referred to as imagination.</p>
<p>The world model is trained to predict the transition dynamics of the real environment.The agent trains in imagination via interacting with the WM instead of the real environment, facilitating faster experience collection for training.The collected trajectories via this interactions are called imagined trajectories.Thus, the world model serves as a proxy for the real environment.Dr. Strategy and all baseline models employ Dreamer V2 (Hafner et al., 2020), utilizing the world model for sample-efficient training.</p>
<p>LEXA LEXA is a model-based RL agent that trains both an explorer and an achiever through imagination using a world model (Mendonca et al., 2021).The explorer discovers the environment, driven by intrinsic motivation, whereas the achiever gathers more experience by targeting randomly explored states sampled from the replay buffer.LEXA undergoes an unsupervised pre-training phase, after which the achiever attempts to solve tasks given by images in a zero-shot manner, without any further learning.In comparison to the original LEXA setup, we opt for using disagreement (Pathak et al., 2019) as the intrinsic reward instead of latent disagreement (Sekar et al., 2020).Moreover, our model incorporates a stochastic embedding sampled from a categorical one-hot distribution, akin to DreamerV2 (Hafner et al., 2020), to modify the multidiagonal Gaussian distribution.This intentional variation in intrinsic rewards and sampling distributions aims to fine-tune performance specifically for 2D navigation environments.For a fair comparison, we match LEXA's hyperparameters with our implementation, excluding latent landmark configurations as outlined in Appendix E. We reward the achiever policy for reaching the target state by using a temporal distance predictor, following the approach used in LEXA (Mendonca et al., 2021).</p>
<p>LEXA-Explore Building on PEG's (Hu et al., 2023) insight that excluding achiever-sampled trajectories benefits the success rate in LEXA.Diverging from the original LEXA (Mendonca et al., 2021), we replaced latent disagreement (Sekar et al., 2020) with disagreement (Pathak et al., 2019) as an intrinsic reward.Furthermore, we adopted a stochastic embedding from a categorical one-hot distribution, akin to DreamerV2 (Hafner et al., 2020), modifying the multi-diagonal Gaussian distribution.These adjustments aim to enhance performance in 2D navigation environments.Hyperparameters are matched with our method's implementation, excluding latent landmark configurations in line with Appendix E.</p>
<p>GC-Director Director (Hafner et al., 2022) is a task-specific hierarchical model-based agent.The task is specified by the reward function.We develop GC-Director as a Goal-Conditioned version of Director, to explore the environment and learn to achieve an unseen goal in an unsupervised manner similar to LEXA (Mendonca et al., 2021).</p>
<p>Director includes two policies: high-level (manager), and low-level (worker).We developed GC-Director based on the open-source code of Director and followed the same architecture and training procedure of LEXA but using a hierarchical policy instead of the flat one in LEXA.GC-Director has 4 policies in total: Explorer has a manager and worker, and Achiever has another manager and worker.We found that having two separate workers leads to the best results.</p>
<p>The explorer's manager is rewarded by an intrinsic reward.The intrinsic reward is the estimate of the epistemic uncertainty using a disagreement of an ensemble of 1-step transition functions similar to LEXA's explorer.For the achiever, the manager π g mgr (z | s t , e g ) is conditioned on the embedding of the given goal image e g and is rewarded using the latent distance (either cosine similarity or temporal distance), as in LEXA.The worker in each is only trained using the original reward function used in Director.</p>
<p>In Table 3, we show a summary of the main aspects of the baselines.We denote hierarchical exploration by methods that have multiple policies that are used to explore sequentially and similarly for hierarchical achievement.</p>
<p>Goal-Conditioned Hierarchical Exploration</p>
<p>Hierarchical Achievement Strategic Dreaming LEXA (Mendonca et al., 2021) ✓ ✗ ✗ ✗ Director (Hafner et al., 2022 Given the same sampling budget (number of environment samples) for all baselines, Tables 1 and 2 show that Dr. Strategy obtains higher final success rates across most environments compared to other baselines.Moreover, Dr. Strategy shows a faster increment in the performance as shown in Figure 4. indicating greater sample efficiency relative to LEXA.
) ✗ ✗ ✓ ✗ GC-Director* ✓ ✓ ✓ ✗ Dr. Strategy (Ours) ✓ ✓ ✓ ✓
Additionally, Figure 11 shows the success rates (y-axis) of Dr. Strategy and other baselines given various sampling budgets (x-axis), highlighting that Dr. Strategy consistently reaches higher success rates in most environments.To demonstrate the versatility of our method in various tasks beyond navigation, we evaluate its performance on the RoboYoga benchmark introduced by LEXA (Mendonca et al., 2021).To mitigate randomness and noise inherent in the measurements, we adopt the average of three episodes, considering the maximum success achieved in each episode as the performance metric.Specifically, we define the agent's success as achieving the desired goal at least once within an episode.As illustrated in Figure 12, our method consistently maintains a commendable level of performance in various domains within the RoboYoga benchmark.</p>
<p>C.3.Success rate of LEXA-Original and LEXA-Ours In Figure 13, we compare our implementation of LEXA (LEXA-Ours) with the original LEXA implementation (LEXA-Original) from (Mendonca et al., 2021) in 9-room and RoboKitchen.When we run LEXA-Original, we match the configuration and parameters to the original code.For configurations that are not explicit in the original implementation, we match with LEXA-Ours, which is used in Section 3. The success rate is measured in the same way as mentioned in Appendix A. Through the success rate, we can clarify that LEXA-Original performance is very low in 9-room.In RoboKitchen, the results of LEXA-Original are similar to the original paper (Mendonca et al., 2021).LEXA-Ours show lower performance than LEXA-Original, and the performance gap is around 10%.This is due to the difference between the implementation mentioned in Appendix B. To compare the model architecture without getting biased by engineering differences, we use LEXA-Ours that uses similar intrinsic reward, world model, and configurations</p>
<p>C.4. Number of Landmarks</p>
<p>Figure 14.Ablation results of using a different number of landmarks (16,64,256,512) Figure 14 shows that increasing the number of landmarks used does not always benefit our method.As 3D-Maze navigation is visually more complex than 2D navigation due to its egocentric observations, it requires a greater number of landmarks to perform the best, which is 64.In 2D navigation (9-rooms) 16 landmarks were enough to have a comparable performance compared to using 64 landmarks.However, using 64 landmarks is able to perform better in some seeds.Using 512 landmarks performs worse than 64 in 9-Room and 3D-Maze-7x7.</p>
<p>C.5.Why is the performance gap of Dr. Strategy in Maze-15x15 small compared to Maze-7x7?</p>
<p>Figure 4 shows that the performance gap in Maze-15x15 is smaller than that of Maze-7x7.One hypothesis to explain this phenomenon suggests that in Maze-15x15, the larger space and the potential for encountering similar scenes can confuse the agent's ability to generalize from a given goal image.This confusion may arise because larger regions are identified by the same colors.Conversely, Maze-7x7 is smaller, and fewer regions are marked with the same color, as illustrated in Figure 7.We found that the highway policy given a landmark may sometimes reach a state visually similar to the landmark, but temporally far.As an empirical evidence, Figure 15 shows a top-down view of 10 trajectories for Dr. Strategy to reach the target in the green room in the upper right part.The agent finds the landmark positioned near the goal denoted by a white star.However, the highway policy could not reach the corresponding landmark within T L steps, instead the agent stuck at a green wall that is visually similar to the reconstruction of the landmark.As a result, the agent could not reach the goal which contributes to the agent's low success rate.Figure 16</p>
<p>Figure 1 .
1
Figure 1.(Left) In the real world, humans maintain a hierarchical spatial structure for easy navigation.(Right) Trying to memorize all the streets on the map can lead to an overwhelming amount of information, making it difficult to retain the information effectively.(Middle) In contrast, choosing to travel by train to move between cities and transfer to a taxi at the terminal minimizes the complexity, allowing one to concentrate on local routes starting from the terminal near the destination.</p>
<p>Figure 2 .
2
Figure 2. Comparison between Dr. Strategy and LEXA.a.We construct latent landmarks and train Highway policy π l (at|st, l), Explorer πe(at|st), and Achiever πg(at|st, eg) in imagination.The Achiever is trained by Focused Sampling, which is conditioning goals within a small number of steps instead of random sampling.All three policies are purely trained with imagined trajectories from the world model.b.During exploration, we only evaluate the landmarks, and call the landmark with the highest exploration potential "Curious Landmark" (C-Landmark).In a real environment, the Highway policy moves to the curious landmark, and the Explorer resumes exploration.The agent iterates training and exploration with a certain frequency TF .c.During test time, we find the landmark that is nearest to the given pixel-level goal (G-Landmark).The Highway policy reaches G-Landmark, and the Achiever proceeds to achieve the goal immediately after.The blue boxes in the bottom half of the figure indicate the modules of LEXA, which are Explorer and Achiever without focused sampling and landmarks.</p>
<p>Figure 3 .
3
Figure 3. Environments.We evaluate our agent across three different environments: 2D Navigation, 3D-Maze Navigation, and RoboKitchen.In these navigation environments, the agent's views are partially observable and visualized on the left.The top-left and bottom-left images represent the agent's initial view in the 2D and 3D Navigation settings, respectively.The second and third columns depict the top-down views of the 2D and 3D Navigation environments, respectively.</p>
<p>Figure 4 .
4
Figure 4. Zero-shot evaluation of the baselines across different environments.Each baseline is evaluated given a goal image from the environment's test set.Dr. Strategy significantly outperforms other baselines in most of the navigation tasks, while achieving comparable results in RoboKitchen.The success rate is reported with the mean and standard deviation across 3 different random seeds.all share a similar high-level component structure in implementation.For fair comparison to minimize the effect of implementation engineering, we implemented LEXA and LEXA-Explore based on our Dr.Strategy codebase.The comparison with the original code can still be found in Appendix B and Appendix C.3.</p>
<p>Figure 5 .
5
Figure 5. Evaluation trajectories visualization in 25-room for Dr. Strategy and LEXA.(Top) Ten evaluation trajectories per goal are visualized.All trajectories start from the top-left cell and head towards the desired goals positioned in the middle of each room.The red and blue lines indicate failed and successful trajectories, respectively.(Bottom) Trajectories (A), (C) aim to reach Goal 1 while (B), (D) aim to reach Goal 2. Dr. Strategy's trajectory (C) successfully reaches Goal 1 with precision due to focused sampling, unlike LEXA's trajectory (A).For Goal 2, trajectory (D) demonstrates the advantages of exploiting highway policy by finding the goal's vicinity, a capability lacking in trajectory (B) with flat models.</p>
<p>Figure 6 .
6
Figure 6.Ablation results for SE, SA, FS. showing the influence of using Strategy to Explore (SE), Strategy to Achieve (SA), and focused sampling (FS) to Dr. Strategy's zero-shot success rate</p>
<p>Figure 7 .
7
Figure 7. Illustration of all the used environments.(a-c) Partially Observable 2D Navigation, (d-e) First-person view 3D maze navigation and (f) RoboKitchen.(b) shows the spiral 9-rooms in which the closed gates are highlighted in white, (d-e) showing the 3D-Maze environments without the floor color for easy visualizations of the walls</p>
<p>Figure 9 .
9
Figure9.Qualitative results of Dr. Strategy's zero-shot evaluation trajectories.Given the goal, the proposed agent finds the nearest landmark.We visualize it by inferring the latent state using the world model, and then it is reconstructed.The agent starts in the initial state and then uses the highway policy conditioned on the closest landmark.Upon meeting the termination criteria, it then switches to the focused achiever policy, conditioned on the given goal.</p>
<p>Figure 10 .
10
Figure 10.Success rate given various sampling budgets.It displays the success rate (y-axis) across various sampling budgets for the baselines</p>
<p>Figure 11 .
11
Figure 11.Number of Samples required to get various success rate thresholds.It shows the number of environment samples (sampling budget) required to achieve specific success rate thresholds (x-axis).The bar is omitted if the baseline does not achieve the indicated success rate.This omission signifies that the baseline did not achieve the success rate within the given training sampling budget in our experiments</p>
<p>Figure 13 .
13
Figure 13.Success rate of LEXA-Ours and LEXA-Original</p>
<p>Figure 15 .
15
Figure 15.Visualization of 10 trajectories in 3D-Maze-15x15 of Dr. Strategy from the initial state given the green goal in the upper right part.The trajectories using highway policy are visualized with white lines, while the trajectories using achiever are shown with red lines.</p>
<p>Figure 16 .
16
Figure 16.Visualization of one of the trajectories in Figure 15.</p>
<p>Figure 17.Left: Landmarks visualizations in 9-room, Right: Landmarks visualizations in Spiral 9-room.</p>
<p>Figure 18 .Figure 20 .
1820
Figure 18.Landmarks visualizations in 25-room</p>
<p>Table 1 .
1
Final success rate in 2D Navigation tasks.
Method9-Room Spiral 9-Room 25-RoomLEXA19.75%21.19%9.62%LEXA-Explore16.04%20.16%0.14%GC-Director28.08%30.45%27.11%Dr. Strategy (Ours) 94.03%96.50%67.11%</p>
<p>Table 2 .
2
Final success rate in 3D-Maze navigation and RoboKitchen tasks.</p>
<p>Table 3 .
3
A high-level comparison between Dr. Strategy and other baselines.*GC-Director is a method we developed based on the official source code of Director C. Additional Experiments C.1.Sample efficiency comparison between Dr. Strategy and other baselines</p>
<p>AcknowledgementsThis work is supported by Brain Pool Plus Program (No. 2021H1D3A2A03103645) through the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT and partly by the research support program of Samsung Advanced Institute of Technology.D. Implementation detailsAlgorithm 1 Dr. Strategy Initialize: World Model M, Replay buffer D, landmark auto-encoder (enc ϕ (s), {l 1 , ...., l N }, dec ϕ (l)), Highway policy π l (a t |s t , l), Explorer π e (a t |s t ), Achiever π g (a t |s t , g) Imagine trajectory τ g = {s 0 , . . .s g }, using zero actions, starting from g Find Landmark l G nearest to goal where G = arg min j ∥enc ϕ (s g ) − l j ∥ 2 // Focused Achiever Deploy π l (a t |s t , l G ) in the environment for T L steps or until ∥s t − s l G ∥ &lt; ϵ, where s l G ∼ dec ϕ (l G ) Deploy π g (a t |s t , g) in the environment to reach g. end World Model Following the same architecture as the world model in DreamerV2(Hafner et al., 2020), we use the hyperparameters as indicated in Table4.Latent Landmark learning The latent landmarks are learned through a VQ-VAE, a type of variational autoencoder (VAE) that utilizes vector quantization to obtain a discrete latent representation(Van Den Oord et al., 2017;Razavi et al., 2019).VQ-VAE comprises three components: an encoder, a codebook, and a decoder.The encoder projects the input into a latent representation.The codebook learns a discrete set of latent representations known as codes, quantizes the encoder's output by finding the closest code to that output.The decoder then uses the quantized representation to reconstruct the input.A well-known problem in VQ-VAE is code collapse(Kaiser et al., 2018).To prevent this, we employ the code resampling method mentioned in(Mazzaglia et al., 2022b).Highway policy is trained in imagination to reach the given landmark.The highway policy is conditioned on the current state and the one-hot representation of the index of the selected landmark from the codebook.Achiever To train the achiever policy, we use the temporal distance as a reward from(Mendonca et al., 2021), the temporal distance prediction network (tdp) works in the image embedding space r g (e t , e g ) = −tdp(e t , e g ) and its training is done similarly as in(Mendonca et al., 2021).As imagination is in the state space of the world model, we need to decode from the state space to the embedding space, thus we train an embedding decoder network emb(ê t | s t ) to predict the image embedding êt ≈ e t given a state s t .Evaluation and System setup For the evaluations, we trained all baselines for 3 seeds per environment.The training of our agent took 2 to 6 days based on the environment using 24GB VRAM GPU.E. HyperparametersLike most other model-based RL methods based on RSSM world models, most of the parameters are set by default to the same values as Dreamer V2(Hafner et al., 2020).We made minor changes only in a few hyper-parameters such as the learning rates of world model, actor, and critic by following the hyperparameters of Choreographer(Mazzaglia et al., 2022b)as it is also utilizing VQ-VAE like our method.A very small number (only three) of hyperparameters are task-specific as specified in Table4.4. We use 64 latent landmarks in smaller environments such as 9-room, Spiral 9-room, 3D-Maze 7x7, and Robokitchen.We utilize 128 latent landmarks for larger environments like 25-room and 3D-Maze 15x15.Regarding the exploration strategy, we tailor hyperparameters to each environment.Specifically, for 2D navigation, we set the maximum steps for landmark reaching as 100 and the number of steps to pick the curious landmark at 1000.For 3D-Maze 15x15, these values are adjusted to 200 for landmark reaching and 1000 for curious landmark picking.In the case of 3D-Maze 7x7, we use 100 for landmark reaching and 500 for curious landmark picking.For Robokitchen, the hyperparameters are set at 25 for landmark reaching and 150 for curious landmark picking.Name
Hindsight experience replay. Advances in neural information processing systems. M Andrychowicz, F Wolski, A Ray, J Schneider, R Fong, P Welinder, B Mcgrew, J Tobin, O Pieter Abbeel, W Zaremba, 201730</p>
<p>Dota 2 with large scale deep reinforcement learning. C Berner, G Brockman, B Chan, V Cheung, P Dębiak, C Dennison, D Farhi, Q Fischer, S Hashme, C Hesse, arXiv:1912.066802019arXiv preprint</p>
<p>Exploration by random network distillation. Y Burda, H Edwards, A Storkey, O Klimov, International Conference on Learning Representations. 2019</p>
<p>Explore, discover and learn: Unsupervised discovery of state-covering skills. V Campos, A Trott, C Xiong, R Socher, X Giró-I Nieto, J Torres, International Conference on Machine Learning. PMLR2020</p>
<p>K Cho, B Van Merriënboer, C Gulcehre, D Bahdanau, F Bougares, H Schwenk, Y Bengio, arXiv:1406.1078Learning phrase representations using rnn encoder-decoder for statistical machine translation. 2014arXiv preprint</p>
<p>Contextual cueing: Implicit learning and memory of visual context guides spatial attention. M M Chun, Y Jiang, Cognitive psychology. 3611998</p>
<p>First return, then explore. A Ecoffet, J Huizinga, J Lehman, K O Stanley, J Clune, Nature. 59078472021</p>
<p>Diversity is all you need: Learning skills without a reward function. B Eysenbach, A Gupta, J Ibarz, S Levine, International Conference on Learning Representations. 2019a</p>
<p>Search on the replay buffer: Bridging planning and rl. B Eysenbach, R Salakhutdinov, S Levine, Advances in Neural Information Processing Systems. 2019b</p>
<p>W H Guss, B Houghton, N Topin, P Wang, C Codel, M Veloso, R Salakhutdinov, Minerl, arXiv:1907.13440A largescale dataset of minecraft demonstrations. 2019arXiv preprint</p>
<p>Recurrent world models facilitate policy evolution. D Ha, J Schmidhuber, Advances in neural information processing systems. 201831</p>
<p>Dream to control: Learning behaviors by latent imagination. D Hafner, T Lillicrap, J Ba, M Norouzi, International Conference on Learning Representations. 2019a</p>
<p>Learning latent dynamics for planning from pixels. D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J Davidson, International conference on machine learning. PMLR2019b</p>
<p>Mastering atari with discrete world models. D Hafner, T P Lillicrap, M Norouzi, J Ba, International Conference on Learning Representations. 2020</p>
<p>Deep hierarchical planning from pixels. D Hafner, K.-H Lee, I Fischer, P Abbeel, Advances in Neural Information Processing Systems. 202235</p>
<p>Planning goals for exploration. E S Hu, R Chang, O Rybkin, D Jayaraman, International Conference on Learning Representations. 2023</p>
<p>Fast decoding in sequence models using discrete latent variables. L Kaiser, S Bengio, A Roy, A Vaswani, N Parmar, J Uszkoreit, N Shazeer, International Conference on Machine Learning. PMLR2018</p>
<p>Learning to discover skills through guidance. H Kim, B Lee, H Lee, D Hwang, S Park, K Min, J Choo, Advances in Neural Information Processing Systems. 2023</p>
<p>D P Kingma, J Ba, Adam, arXiv:1412.6980A method for stochastic optimization. 2014arXiv preprint</p>
<p>D P Kingma, M Welling, arXiv:1312.6114Auto-encoding variational bayes. 2013arXiv preprint</p>
<p>B Kulis, M I Jordan, arXiv:1111.0352Revisiting k-means: New algorithms via bayesian nonparametrics. 2011arXiv preprint</p>
<p>Curiosity-driven exploration via latent bayesian surprise. P Mazzaglia, O Catal, T Verbelen, B Dhoedt, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2022a36</p>
<p>Learning and adapting skills in imagination. P Mazzaglia, T Verbelen, B Dhoedt, A Lacoste, S Rajeswar, Choreographer, International Conference on Learning Representations. 2022b</p>
<p>Discovering and achieving goals via world models. R Mendonca, O Rybkin, K Daniilidis, D Hafner, D Pathak, Advances in Neural Information Processing Systems. 202134</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, nature. 51875402015</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T Lillicrap, T Harley, D Silver, K Kavukcuoglu, International conference on machine learning. PMLR2016</p>
<p>Lipschitzconstrained unsupervised skill discovery. S Park, J Choi, J Kim, H Lee, G Kim, International Conference on Learning Representations. 2022</p>
<p>Scalable unsupervised RL with metric-aware abstraction. S Park, O Rybkin, S Levine, Metra, International Conference on Learning Representations. 2024</p>
<p>J Pasukonis, T Lillicrap, D Hafner, arXiv:2210.13383Evaluating long-term memory in 3d mazes. 2022arXiv preprint</p>
<p>Curiosity-driven exploration by self-supervised prediction. D Pathak, P Agrawal, A A Efros, T Darrell, International conference on machine learning. PMLR2017</p>
<p>Self-supervised exploration via disagreement. D Pathak, D Gandhi, A Gupta, International conference on machine learning. PMLR2019</p>
<p>Long-horizon visual planning with goal-conditioned hierarchical predictors. K Pertsch, O Rybkin, F Ebert, S Zhou, D Jayaraman, C Finn, S Levine, Advances in Neural Information Processing Systems. 202033</p>
<p>Maximum entropy gain exploration for long horizon multi-goal reinforcement learning. S Pitis, H Chan, S Zhao, B Stadie, J Ba, International Conference on Machine Learning. PMLR2020</p>
<p>Skew-fit: State-covering self-supervised reinforcement learning. V Pong, M Dalal, S Lin, A Nair, S Bahl, S Levine, International Conference on Machine Learning. PMLR2020</p>
<p>Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems. A Razavi, A Van Den Oord, O Vinyals, 201932</p>
<p>Stochastic backpropagation and approximate inference in deep generative models. D J Rezende, S Mohamed, D Wierstra, International conference on machine learning. PMLR2014</p>
<p>Unlocking the power of representations in long-term novelty-based exploration. A Saade, S Kapturowski, D Calandriello, C Blundell, P Sprechmann, L Sarra, O Groth, M Valko, B Piot, arXiv:2305.015212023arXiv preprint</p>
<p>Planning to explore via self-supervised world models. R Sekar, O Rybkin, K Daniilidis, P Abbeel, D Hafner, D Pathak, International Conference on Machine Learning. PMLR2020</p>
<p>Nearest neighbor estimates of entropy. H Singh, N Misra, V Hnizdo, A Fedorowicz, E Demchuk, American journal of mathematical and management sciences. 233-42003</p>
<p>an integrated architecture for learning, planning, and reacting. R S Sutton, Dyna, ACM Sigart Bulletin. 241991</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 2018MIT press</p>
<p>Neural discrete representation learning. Advances in neural information processing systems. A Van Den Oord, O Vinyals, 201730</p>
<p>Alphastar: Mastering the real-time strategy game starcraft ii. O Vinyals, I Babuschkin, J Chung, M Mathieu, M Jaderberg, W M Czarnecki, A Dudzik, A Huang, P Georgiev, R Powell, DeepMind blog. 2202019</p>
<p>Reinforcement learning with prototypical representations. D Yarats, R Fergus, A Lazaric, L Pinto, International Conference on Machine Learning. PMLR2021</p>            </div>
        </div>

    </div>
</body>
</html>
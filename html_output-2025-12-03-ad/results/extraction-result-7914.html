<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7914 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7914</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7914</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-d9dadfb199bb68abb40b3e9d38abab16fd52056f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d9dadfb199bb68abb40b3e9d38abab16fd52056f" target="_blank">Large Language Models for Constrained-Based Causal Discovery</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work explores the capabilities of Large Language Models as an alternative to domain experts for causal graph generation and shows evidence that knowledge-based CIT could eventually become a complementary tool for data-driven causal discovery.</p>
                <p><strong>Paper Abstract:</strong> Causality is essential for understanding complex systems, such as the economy, the brain, and the climate. Constructing causal graphs often relies on either data-driven or expert-driven approaches, both fraught with challenges. The former methods, like the celebrated PC algorithm, face issues with data requirements and assumptions of causal sufficiency, while the latter demand substantial time and domain knowledge. This work explores the capabilities of Large Language Models (LLMs) as an alternative to domain experts for causal graph generation. We frame conditional independence queries as prompts to LLMs and employ the PC algorithm with the answers. The performance of the LLM-based conditional independence oracle on systems with known causal graphs shows a high degree of variability. We improve the performance through a proposed statistical-inspired voting schema that allows some control over false-positive and false-negative rates. Inspecting the chain-of-thought argumentation, we find causal reasoning to justify its answer to a probabilistic query. We show evidence that knowledge-based CIT could eventually become a complementary tool for data-driven causal discovery.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7914.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7914.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>chatPC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>chatPC: PC algorithm with LLM-based conditional independence oracle</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hybrid method that frames conditional-independence (CI) queries as prompts to LLMs and uses aggregated LLM answers (voting/statistical test) as the CI oracle inside the PC algorithm to produce causal graphs (skeletons/partially directed graphs).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Large Language Models for Constrained-Based Causal Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Kai-Hendrik Cohrs, Gherardo Varando, Emiliano Diaz, Vasileios Sitokonstantinou, Gustau Camps-Valls</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>chatPC</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Construct CI prompts from variable descriptions/context and ask an LLM (GPT-3.5 / GPT-4) for batched answers (with chain-of-thought and confidence). Aggregate answers via majority/weighted voting or a statistical hypothesis-test over NO/YES proportions to decide CI; plug those CI decisions into the PC algorithm to recover the causal skeleton and apply orientation rules.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>variable names + short descriptions + domain/context (problem-level descriptions); not raw scholarly corpora in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>structured causal graph (skeleton / Markov equivalence class); CI decisions as intermediate structured outputs</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>persona priming, chain-of-thought prompting, batched queries, majority/weighted voting, and a statistical-test aggregation scheme</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo; GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Evaluation on BNLearn benchmarks (cancer, burglary, asia), Sachs protein-signaling benchmark, climatic teleconnection hypotheses (Kretschmer et al. examples), and spurious-correlation pairs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy, precision, recall, F1 on CI test classification; structural comparison of recovered graph skeletons against assumed ground-truth graphs</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Performance varied across datasets; GPT-4 showed much higher consistency and accuracy (e.g., strong results on burglary and sachs), while GPT-3.5 was more variable and produced more UNCERTAIN answers. Aggregation via the proposed statistical test improved control over false positives/graph density. Models tended to be conservative (prefer dependence), and evidence of causal-style reasoning appeared in chain-of-thought outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Stochastic variability (especially GPT-3.5); scalability issues as number of CI tests grows exponentially with variables; reliance on quality/clarity of context and variable descriptions; occasional incorrect or overly conservative answers; potential hallucination and need for grounding (RAG suggested). Query cost and energy considerations for larger graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Constrained-Based Causal Discovery', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7914.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7914.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval + generation pipeline that augments an LLM's generation by retrieving relevant documents from an external corpus to ground and support answers, enabling synthesis of knowledge from collections of documents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Patrick Lewis et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2021</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Retrieve relevant documents (from an index built over a corpus such as scientific literature) and provide them as context to an LLM so the model's generated answers are grounded in retrieved source text, reducing hallucination and enabling synthesis across documents.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>external document corpus (full-texts, abstracts, metadata) accessible via retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>grounded textual summaries/answers and extractive or structured outputs that synthesize information from retrieved documents</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>retrieval + conditioned generation (RAG pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Mentioned in this paper as a promising approach to ground LLM outputs in scientific sources; no new empirical RAG results provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality depends on retrieval effectiveness and quality of indexed sources; requires curated scientific sources and retrieval infrastructure; not implemented in the authors' experiments (suggested as future work).</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Constrained-Based Causal Discovery', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7914.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7914.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-prior (Ban et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs as priors for data-driven causal discovery (Ban et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using LLMs to provide prior beliefs or constraints to guide data-driven causal discovery algorithms, i.e., using knowledge encoded in language models to bias or restrict graph search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>From query tools to causal architects: Harnessing large language models for advanced causal discovery from data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>From query tools to causal architects: Harnessing large language models for advanced causal discovery from data</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>T. Ban, L. Chen, X. Wang, H. Chen</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-as-prior for causal discovery</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Elicit prior beliefs about causal links from an LLM and feed those priors or constraints into data-driven causal structure-learning methods to improve or regularize graph search.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>variable descriptions / prompts to LLM (potentially augmented with retrieved knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>prior edge probabilities or structural constraints to be consumed by causal discovery algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Cited as prior related work; no experimental details provided in the present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Potentially erroneous priors from LLMs require mitigation; referenced work (Chen et al., 2023) proposes strategies to address prior errors.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Constrained-Based Causal Discovery', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7914.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7914.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mitigate-LLM-Priors (Chen et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mitigating prior errors in causal structure learning: Towards LLM driven prior knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proposes strategies to detect and mitigate erroneous prior causal statements generated by LLMs when used to inform Bayesian Network or causal-structure learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mitigating prior errors in causal structure learning: Towards llm driven prior knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Mitigating prior errors in causal structure learning: Towards llm driven prior knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>L. Chen, T. Ban, X. Wang, D. Lyu, H. Chen</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Mitigation strategies for LLM priors</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Develops approaches to identify and mitigate incorrect causal prior statements produced by LLMs so that LLM-guided Bayesian network learning is robust to erroneous knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>LLM-generated prior causal claims (from prompts or retrieved sources)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>corrected priors or filtered constraints suitable for structure learning algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Referenced as complementary work addressing errors in LLM-driven priors; no experimental detail included in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Constrained-Based Causal Discovery', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7914.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7914.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kıcıman et al. DAG elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal reasoning and large language models: Opening a new frontier for causality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Demonstrated eliciting directed acyclic graphs (DAGs) from LLMs for bivariate and multivariate benchmark datasets, showing LLMs can be prompted to provide structured causal hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal reasoning and large language models: Opening a new frontier for causality</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Causal reasoning and large language models: Opening a new frontier for causality</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>E. Kıcıman, R. Ness, A. Sharma, C. Tan</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM DAG elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Elicit causal relations and DAGs directly from LLM responses on benchmark problems to evaluate knowledge-based causal discovery capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>problem descriptions / variable descriptions and prompts</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>elicited DAGs / causal relation judgments</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>bivariate and multivariate causal discovery benchmarks (as used in that study)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Cited as evidence that LLMs can produce DAGs for benchmark problems; no experimental replication details in the present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Constrained-Based Causal Discovery', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7914.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7914.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vashishtha et al. LLM-ordering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal inference using LLM-guided discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uses LLMs to predict causal (topological) ordering of variables to aid downstream causal inference and structure learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal inference using llm-guided discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Causal inference using llm-guided discovery</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>A. Vashishtha, A. G. Reddy, A. Kumar, S. Bachu, V. N. Balasubramanian, A. Sharma</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-guided causal ordering</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Ask LLMs to predict a topological (causal) order among variables; use predicted order to constrain or assist causal inference methods.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>variable descriptions / prompts</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>predicted causal orderings used as constraints in causal inference</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Referenced as a method that can improve causal inference when LLMs supply ordering priors; no detailed results in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Constrained-Based Causal Discovery', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7914.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7914.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Long et al. (2023a/b)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long et al.: LLMs as imperfect experts for causal graph construction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Investigated LLMs for automating parts of causal graph construction, including edge scoring and incorporating expert knowledge to improve data-driven identification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can large language models build causal graphs?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Can large language models build causal graphs?</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>S. Long, A. Piché, V. Zantedeschi, T. Schuster, A. Drouin</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-assisted causal graph construction / LLM-as-imperfect-expert</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Use LLMs to score or suggest edges and to act as imperfect experts to complement data-driven causal discovery, possibly automating edge scoring and integrating (noisy) expert-like judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>variable descriptions / prompts</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>edge suggestions/ scores, partial causal graphs</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Cited as related work exploring LLMs' ability to construct causal graphs; no detailed metrics in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Constrained-Based Causal Discovery', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7914.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7914.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pawlowski et al.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Answering causal questions with augmented LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Explored using augmented LLMs for precise causal reasoning and for enabling model-agnostic explanations, thereby assisting causal question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Answering causal questions with augmented llms</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Answering causal questions with augmented llms</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>N. Pawlowski, J. Vaughan, J. Jennings, C. Zhang</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Augmented LLMs for causal Q&A</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Use LLMs augmented (e.g., with retrieval or tools) to answer causal questions, produce explanations and help with causal reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>prompts possibly augmented with external context or retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>textual causal answers, explanations, counterfactuals</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Mentioned as evidence that augmented LLMs can help causal reasoning tasks; no experimental replication here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Constrained-Based Causal Discovery', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7914.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7914.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zečević et al. (critique)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal parrots: Large language models may talk causality but are not causal</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A critical perspective arguing that LLMs may regurgitate causal-sounding statements without possessing true causal understanding, cautioning against overreliance on LLMs for causal discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal parrots: Large language models may talk causality but are not causal</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Causal parrots: Large language models may talk causality but are not causal</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>M. Zečević, M. Willig, D. S. Dhami, K. Kersting</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Critical analysis / critique</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Analyzes limitations of LLMs in causal tasks, emphasizing that LLMs' outputs reflect embedded statistical associations from training data and not causal models inferred from interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Provides a counterpoint arguing caution when interpreting LLM causal outputs; referenced by the present paper as a critical voice in the debate.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Highlights hallucination of causal claims and lack of true causal grounding in LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Constrained-Based Causal Discovery', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>From query tools to causal architects: Harnessing large language models for advanced causal discovery from data <em>(Rating: 2)</em></li>
                <li>Causal reasoning and large language models: Opening a new frontier for causality <em>(Rating: 2)</em></li>
                <li>Can large language models build causal graphs? <em>(Rating: 2)</em></li>
                <li>Causal inference using llm-guided discovery <em>(Rating: 2)</em></li>
                <li>Mitigating prior errors in causal structure learning: Towards llm driven prior knowledge <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Causal parrots: Large language models may talk causality but are not causal <em>(Rating: 1)</em></li>
                <li>Answering causal questions with augmented llms <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7914",
    "paper_id": "paper-d9dadfb199bb68abb40b3e9d38abab16fd52056f",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [
        {
            "name_short": "chatPC",
            "name_full": "chatPC: PC algorithm with LLM-based conditional independence oracle",
            "brief_description": "Hybrid method that frames conditional-independence (CI) queries as prompts to LLMs and uses aggregated LLM answers (voting/statistical test) as the CI oracle inside the PC algorithm to produce causal graphs (skeletons/partially directed graphs).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Large Language Models for Constrained-Based Causal Discovery",
            "authors": "Kai-Hendrik Cohrs, Gherardo Varando, Emiliano Diaz, Vasileios Sitokonstantinou, Gustau Camps-Valls",
            "year": 2024,
            "method_name": "chatPC",
            "method_description": "Construct CI prompts from variable descriptions/context and ask an LLM (GPT-3.5 / GPT-4) for batched answers (with chain-of-thought and confidence). Aggregate answers via majority/weighted voting or a statistical hypothesis-test over NO/YES proportions to decide CI; plug those CI decisions into the PC algorithm to recover the causal skeleton and apply orientation rules.",
            "input_type": "variable names + short descriptions + domain/context (problem-level descriptions); not raw scholarly corpora in experiments",
            "output_type": "structured causal graph (skeleton / Markov equivalence class); CI decisions as intermediate structured outputs",
            "prompting_technique": "persona priming, chain-of-thought prompting, batched queries, majority/weighted voting, and a statistical-test aggregation scheme",
            "model_name": "GPT-3.5-turbo; GPT-4",
            "model_size": null,
            "datasets_used": "Evaluation on BNLearn benchmarks (cancer, burglary, asia), Sachs protein-signaling benchmark, climatic teleconnection hypotheses (Kretschmer et al. examples), and spurious-correlation pairs",
            "evaluation_metric": "Accuracy, precision, recall, F1 on CI test classification; structural comparison of recovered graph skeletons against assumed ground-truth graphs",
            "reported_results": "Performance varied across datasets; GPT-4 showed much higher consistency and accuracy (e.g., strong results on burglary and sachs), while GPT-3.5 was more variable and produced more UNCERTAIN answers. Aggregation via the proposed statistical test improved control over false positives/graph density. Models tended to be conservative (prefer dependence), and evidence of causal-style reasoning appeared in chain-of-thought outputs.",
            "limitations": "Stochastic variability (especially GPT-3.5); scalability issues as number of CI tests grows exponentially with variables; reliance on quality/clarity of context and variable descriptions; occasional incorrect or overly conservative answers; potential hallucination and need for grounding (RAG suggested). Query cost and energy considerations for larger graphs.",
            "counterpoint": true,
            "uuid": "e7914.0",
            "source_info": {
                "paper_title": "Large Language Models for Constrained-Based Causal Discovery",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A retrieval + generation pipeline that augments an LLM's generation by retrieving relevant documents from an external corpus to ground and support answers, enabling synthesis of knowledge from collections of documents.",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "mention_or_use": "mention",
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "authors": "Patrick Lewis et al.",
            "year": 2021,
            "method_name": "Retrieval-Augmented Generation (RAG)",
            "method_description": "Retrieve relevant documents (from an index built over a corpus such as scientific literature) and provide them as context to an LLM so the model's generated answers are grounded in retrieved source text, reducing hallucination and enabling synthesis across documents.",
            "input_type": "external document corpus (full-texts, abstracts, metadata) accessible via retrieval",
            "output_type": "grounded textual summaries/answers and extractive or structured outputs that synthesize information from retrieved documents",
            "prompting_technique": "retrieval + conditioned generation (RAG pipeline)",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Mentioned in this paper as a promising approach to ground LLM outputs in scientific sources; no new empirical RAG results provided here.",
            "limitations": "Quality depends on retrieval effectiveness and quality of indexed sources; requires curated scientific sources and retrieval infrastructure; not implemented in the authors' experiments (suggested as future work).",
            "counterpoint": null,
            "uuid": "e7914.1",
            "source_info": {
                "paper_title": "Large Language Models for Constrained-Based Causal Discovery",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLM-as-prior (Ban et al.)",
            "name_full": "LLMs as priors for data-driven causal discovery (Ban et al.)",
            "brief_description": "Using LLMs to provide prior beliefs or constraints to guide data-driven causal discovery algorithms, i.e., using knowledge encoded in language models to bias or restrict graph search.",
            "citation_title": "From query tools to causal architects: Harnessing large language models for advanced causal discovery from data",
            "mention_or_use": "mention",
            "paper_title": "From query tools to causal architects: Harnessing large language models for advanced causal discovery from data",
            "authors": "T. Ban, L. Chen, X. Wang, H. Chen",
            "year": 2023,
            "method_name": "LLM-as-prior for causal discovery",
            "method_description": "Elicit prior beliefs about causal links from an LLM and feed those priors or constraints into data-driven causal structure-learning methods to improve or regularize graph search.",
            "input_type": "variable descriptions / prompts to LLM (potentially augmented with retrieved knowledge)",
            "output_type": "prior edge probabilities or structural constraints to be consumed by causal discovery algorithms",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Cited as prior related work; no experimental details provided in the present paper.",
            "limitations": "Potentially erroneous priors from LLMs require mitigation; referenced work (Chen et al., 2023) proposes strategies to address prior errors.",
            "counterpoint": null,
            "uuid": "e7914.2",
            "source_info": {
                "paper_title": "Large Language Models for Constrained-Based Causal Discovery",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Mitigate-LLM-Priors (Chen et al.)",
            "name_full": "Mitigating prior errors in causal structure learning: Towards LLM driven prior knowledge",
            "brief_description": "Proposes strategies to detect and mitigate erroneous prior causal statements generated by LLMs when used to inform Bayesian Network or causal-structure learning.",
            "citation_title": "Mitigating prior errors in causal structure learning: Towards llm driven prior knowledge",
            "mention_or_use": "mention",
            "paper_title": "Mitigating prior errors in causal structure learning: Towards llm driven prior knowledge",
            "authors": "L. Chen, T. Ban, X. Wang, D. Lyu, H. Chen",
            "year": 2023,
            "method_name": "Mitigation strategies for LLM priors",
            "method_description": "Develops approaches to identify and mitigate incorrect causal prior statements produced by LLMs so that LLM-guided Bayesian network learning is robust to erroneous knowledge.",
            "input_type": "LLM-generated prior causal claims (from prompts or retrieved sources)",
            "output_type": "corrected priors or filtered constraints suitable for structure learning algorithms",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Referenced as complementary work addressing errors in LLM-driven priors; no experimental detail included in this paper.",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7914.3",
            "source_info": {
                "paper_title": "Large Language Models for Constrained-Based Causal Discovery",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Kıcıman et al. DAG elicitation",
            "name_full": "Causal reasoning and large language models: Opening a new frontier for causality",
            "brief_description": "Demonstrated eliciting directed acyclic graphs (DAGs) from LLMs for bivariate and multivariate benchmark datasets, showing LLMs can be prompted to provide structured causal hypotheses.",
            "citation_title": "Causal reasoning and large language models: Opening a new frontier for causality",
            "mention_or_use": "mention",
            "paper_title": "Causal reasoning and large language models: Opening a new frontier for causality",
            "authors": "E. Kıcıman, R. Ness, A. Sharma, C. Tan",
            "year": 2023,
            "method_name": "LLM DAG elicitation",
            "method_description": "Elicit causal relations and DAGs directly from LLM responses on benchmark problems to evaluate knowledge-based causal discovery capabilities.",
            "input_type": "problem descriptions / variable descriptions and prompts",
            "output_type": "elicited DAGs / causal relation judgments",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": "bivariate and multivariate causal discovery benchmarks (as used in that study)",
            "evaluation_metric": null,
            "reported_results": "Cited as evidence that LLMs can produce DAGs for benchmark problems; no experimental replication details in the present paper.",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7914.4",
            "source_info": {
                "paper_title": "Large Language Models for Constrained-Based Causal Discovery",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Vashishtha et al. LLM-ordering",
            "name_full": "Causal inference using LLM-guided discovery",
            "brief_description": "Uses LLMs to predict causal (topological) ordering of variables to aid downstream causal inference and structure learning.",
            "citation_title": "Causal inference using llm-guided discovery",
            "mention_or_use": "mention",
            "paper_title": "Causal inference using llm-guided discovery",
            "authors": "A. Vashishtha, A. G. Reddy, A. Kumar, S. Bachu, V. N. Balasubramanian, A. Sharma",
            "year": 2023,
            "method_name": "LLM-guided causal ordering",
            "method_description": "Ask LLMs to predict a topological (causal) order among variables; use predicted order to constrain or assist causal inference methods.",
            "input_type": "variable descriptions / prompts",
            "output_type": "predicted causal orderings used as constraints in causal inference",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Referenced as a method that can improve causal inference when LLMs supply ordering priors; no detailed results in this paper.",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7914.5",
            "source_info": {
                "paper_title": "Large Language Models for Constrained-Based Causal Discovery",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Long et al. (2023a/b)",
            "name_full": "Long et al.: LLMs as imperfect experts for causal graph construction",
            "brief_description": "Investigated LLMs for automating parts of causal graph construction, including edge scoring and incorporating expert knowledge to improve data-driven identification.",
            "citation_title": "Can large language models build causal graphs?",
            "mention_or_use": "mention",
            "paper_title": "Can large language models build causal graphs?",
            "authors": "S. Long, A. Piché, V. Zantedeschi, T. Schuster, A. Drouin",
            "year": 2023,
            "method_name": "LLM-assisted causal graph construction / LLM-as-imperfect-expert",
            "method_description": "Use LLMs to score or suggest edges and to act as imperfect experts to complement data-driven causal discovery, possibly automating edge scoring and integrating (noisy) expert-like judgments.",
            "input_type": "variable descriptions / prompts",
            "output_type": "edge suggestions/ scores, partial causal graphs",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Cited as related work exploring LLMs' ability to construct causal graphs; no detailed metrics in this paper.",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7914.6",
            "source_info": {
                "paper_title": "Large Language Models for Constrained-Based Causal Discovery",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Pawlowski et al.",
            "name_full": "Answering causal questions with augmented LLMs",
            "brief_description": "Explored using augmented LLMs for precise causal reasoning and for enabling model-agnostic explanations, thereby assisting causal question answering.",
            "citation_title": "Answering causal questions with augmented llms",
            "mention_or_use": "mention",
            "paper_title": "Answering causal questions with augmented llms",
            "authors": "N. Pawlowski, J. Vaughan, J. Jennings, C. Zhang",
            "year": 2023,
            "method_name": "Augmented LLMs for causal Q&A",
            "method_description": "Use LLMs augmented (e.g., with retrieval or tools) to answer causal questions, produce explanations and help with causal reasoning tasks.",
            "input_type": "prompts possibly augmented with external context or retrieval",
            "output_type": "textual causal answers, explanations, counterfactuals",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Mentioned as evidence that augmented LLMs can help causal reasoning tasks; no experimental replication here.",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7914.7",
            "source_info": {
                "paper_title": "Large Language Models for Constrained-Based Causal Discovery",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Zečević et al. (critique)",
            "name_full": "Causal parrots: Large language models may talk causality but are not causal",
            "brief_description": "A critical perspective arguing that LLMs may regurgitate causal-sounding statements without possessing true causal understanding, cautioning against overreliance on LLMs for causal discovery.",
            "citation_title": "Causal parrots: Large language models may talk causality but are not causal",
            "mention_or_use": "mention",
            "paper_title": "Causal parrots: Large language models may talk causality but are not causal",
            "authors": "M. Zečević, M. Willig, D. S. Dhami, K. Kersting",
            "year": 2023,
            "method_name": "Critical analysis / critique",
            "method_description": "Analyzes limitations of LLMs in causal tasks, emphasizing that LLMs' outputs reflect embedded statistical associations from training data and not causal models inferred from interventions.",
            "input_type": null,
            "output_type": null,
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Provides a counterpoint arguing caution when interpreting LLM causal outputs; referenced by the present paper as a critical voice in the debate.",
            "limitations": "Highlights hallucination of causal claims and lack of true causal grounding in LLMs.",
            "counterpoint": true,
            "uuid": "e7914.8",
            "source_info": {
                "paper_title": "Large Language Models for Constrained-Based Causal Discovery",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "From query tools to causal architects: Harnessing large language models for advanced causal discovery from data",
            "rating": 2
        },
        {
            "paper_title": "Causal reasoning and large language models: Opening a new frontier for causality",
            "rating": 2
        },
        {
            "paper_title": "Can large language models build causal graphs?",
            "rating": 2
        },
        {
            "paper_title": "Causal inference using llm-guided discovery",
            "rating": 2
        },
        {
            "paper_title": "Mitigating prior errors in causal structure learning: Towards llm driven prior knowledge",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2
        },
        {
            "paper_title": "Causal parrots: Large language models may talk causality but are not causal",
            "rating": 1
        },
        {
            "paper_title": "Answering causal questions with augmented llms",
            "rating": 1
        }
    ],
    "cost": 0.0204225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models for Constrained-Based Causal Discovery</h1>
<p>Kai-Hendrik Cohrs<br>Gherardo Varando<br>Emiliano Diaz<br>Vasileios Sitokonstantinou<br>Gustau Camps-Valls<br>KAI.COHRS@UV.ES<br>Gherardo.Varando@uv.es<br>Emiliano.Diaz@uv.es<br>Vasileios.Sitokonstantinou@uv.es<br>Gustau Camps-Valls<br>GUSTAU.CAMPS@UV.ES</p>
<p>Image Processing Laboratory, Universitat de València, Spain</p>
<h2>Abstract</h2>
<p>Causality is essential for understanding complex systems, such as the economy, the brain, and the climate. Constructing causal graphs often relies on either data-driven or expert-driven approaches, both fraught with challenges. The former methods, like the celebrated PC algorithm, face issues with data requirements and assumptions of causal sufficiency, while the latter demand substantial time and domain knowledge. This work explores the capabilities of Large Language Models (LLMs) as an alternative to domain experts for causal graph generation. We frame conditional independence queries as prompts to LLMs and employ the PC algorithm with the answers. The performance of the LLM-based conditional independence oracle on systems with known causal graphs shows a high degree of variability. We improve the performance through a proposed statistical-inspired voting schema that allows some control over false-positive and falsenegative rates. Inspecting the chain-of-thought argumentation, we find causal reasoning to justify its answer to a probabilistic query. We show evidence that knowledge-based CIT could eventually become a complementary tool for data-driven causal discovery.
Keywords: Causality; large language models; causal discovery; conditional independence testing; constrained-based causal discovery.</p>
<h2>1 Introduction</h2>
<p>Understanding causality is imperative across various disciplines, as it offers critical insights into the mechanisms of complex systems. For example, in the field of epidemiology, recognizing the causal link between smoking and lung cancer has led to effective public health campaigns and anti-smoking policies, significantly reducing the prevalence of smoking-related diseases (Cornfield et al., 1959; Wynder, 1997).</p>
<p>Investigating causality enhances our understanding of the underlying processes and enables us to make informed decisions and take proactive measures to address pressing challenges. Investigating causality poses considerable challenges, with constructing causal graphs representing a formidable task. Data-driven causal discovery methods, including prominent techniques like PC (Spirtes et al., 2000) and GES (Chickering, 2002a), encounter a range of issues. These methodologies rely heavily on large amounts of data, necessitating complex conditional independence tests that can be particularly challenging, especially when working with diverse and mixed data types. The assumption of causal sufficiency, which presumes that all relevant variables are observed, can lead to erroneous conclusions, especially when unobserved variables act as potential confounders between system variables. Notably, there exist alternative methods such as LPCMCI (Gerhardus and Runge, 2020), FCI (Spirtes et al., 2000), SVAR-FCI (Malinsky and Spirtes, 2018), and GPS (Claassen and Bucur, 2022) that do not assume causal sufficiency (Camps-Valls et al., 2023). Nonetheless, missing data</p>
<p>and selection bias continue to pose persistent challenges in real-world applications, prompting efforts to develop more resilient causal discovery methods (Camps-Valls et al., 2023).</p>
<p>In addition to data-driven causal discovery methods, another approach for creating causal graphs involves leveraging domain knowledge. However, this process is inherently challenging and timeconsuming, demanding substantial expertise and labor (Long et al., 2023b). Experts tasked with constructing causal graphs must possess a deep understanding of the relationships and mechanisms within the system under investigation, which often requires knowledge across different disciplines. This often entails extensive consultations, discussions, and reviews with domain specialists, adding significant time and resource commitments. Furthermore, the complexity of many real-world systems amplifies the difficulty of accurately capturing all relevant causal relationships, leading to potential oversights and inaccuracies in the resulting causal graph. These challenges underscore the necessity for more automated methodologies. In that respect, LLMs could play a key role if they prove to be a reliable source of causal knowledge.</p>
<p>LLMs present a promising knowledge-driven alternative to expert-based graph building or datadriven causal discovery methods. They have shown good performance across a range of language understanding and logical reasoning tasks (Brown et al., 2020; Xu et al., 2023). This could extend to probabilistic and causal reasoning, including interventional and counterfactual scenarios. Whether or not this is the case is the subject of heated debate (Hobbhahn et al., 2022; Willig et al., 2023; Zečević et al., 2023). Despite the inherent complexity of directly asking LLMs to provide complete causal graphs, given the various levels of reasoning involved and the rich context necessary, different strategies exist to elicit causal graphs from LLMs effectively. These strategies include LLMs to predict causal relations (Kıcıman et al., 2023; Long et al., 2023a; Willig et al., 2023), LLMs as priors for data-driven causal discovery methods (Ban et al., 2023), and LLMs to aid in downstream causal inference tasks by predicting the causal order of variables (Vashishtha et al., 2023).</p>
<p>To marry the traditional and the LLM-based methods, our work proposes chatPC as a hybrid approach. Specifically, this work:</p>
<ul>
<li>Frames conditional independence queries as prompts to LLMs and employs the PC algorithm with this oracle for causal graph construction.</li>
<li>Evaluates the performance of LLMs on conditional independence tests across various problems, showing varied performance.</li>
<li>Introduces a statistical-based approach for aggregating multiple LLM answers, improving performance.</li>
<li>Examines the graphs predicted by PC with the LLM oracle, finding them to be reasonable.</li>
<li>Identifies a general tendency for conservative answers from LLMs compared to experts and finds traces of causal reasoning in the model's answers.</li>
<li>Suggests that the approach could mitigate limitations of traditional methods, offering a promising avenue for automated causal graph construction.</li>
</ul>
<p>We argue that relying solely on LLM-based bivariate causal queries inherently overlooks the presence of mediators, consequently limiting the comprehensive understanding of the full causal graph. Additionally, our research explores the extent to which LLMs' queries can effectively substitute data-driven conditional independence tests, considering that PC represents the current state of the art in causal discovery, and under a perfect oracle, it can consistently retrieve the ideal graph up to the Markov equivalence class (Spirtes et al., 2000).</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of the introduced scheme for PC with GPT/LLM. Credits: Little robot face by Antònia Font.</p>
<h1>2 Background</h1>
<p>This section presents the causal discovery task and briefly reviews different methodologies. We start by describing classical data-driven causal discovery methods, briefly review the integration of data and prior knowledge, and present novel approaches to causal discovery using LLMs.</p>
<h3>2.1 Data-Driven Causal Discovery</h3>
<p>Nowadays, there exists solid and established literature on causal discovery, which is the task of generating and selecting appropriate causal hypotheses from data. Starting from the seminal work of Wright (1934), using graphs to represent causal relationships has become widespread in causal analysis. In the prevalent structural equation model (SEM) formalism, a causal model is described by a system of equations that implicitly define a directed acyclic graph (DAG) among the variables of the system through the signature of each equation (Peters et al., 2017). Importantly, in this framework, a causal model entails a statistical or data-generating process of the data while representing additional information such as interventional and counterfactual quantities (Peters et al., 2017). Causal discovery methods in the statistical DAG framework are usually categorized as constrained-based or score-based approaches (Glymour et al., 2019; Scutari et al., 2019). The former methods attempt the search for graphs that most closely entail the set of conditional independence relations judged to hold in the population, while the latter approaches search the spaces of graphs for the best scoring ones for some score function (e.g., penalized likelihood, such as BIC or AIC scores). A famous example of score-based methods is the GES algorithm (Meek, 1997; Chickering, 2002a,b), which performs simple moves (add/remove edges, invert directions) directly in the space of Markov equivalence classes. The most well-known constrained-based method is the PC-algorithm (Spirtes et al., 2000), which we briefly describe as it will be central in our successive examples of LLM's integration. It follows three basic steps: (i) in the first step conditional independence statements $(X \Perp Y \mid Z)$, obtained through hypothesis tests performed on the data, are used to prune the edges of the fully connected undirected graph, thus building the so-called skeleton of the causal graph. (ii) In the second step, colliders are oriented such that invalid conditional (in)dependencies are ruled out. (iii) The third and last step applies a set of graphical rules (Meek, 1995) to orient as many edges as possible. Both the GES and the celebrated Peter-Clark (PC) algorithm (Spirtes et al., 2000) have been proven consistent in the large sample limit under specific assumptions. In particular, if we consider an oracle for conditional independence statements, and we assume causal sufficiency and faithfulness, the PC algorithm outputs a partially directed graph that represents the true Markov equivalence class (without additional assumption, it is impossible, from statistical information alone, to recover something more then the MEC). We refer the readers to Pearl et al. (2000), Peters et al. (2017) and recent surveys and reviews Camps-Valls et al. (2023); Glymour et al. (2019); Scutari et al. (2019) for a more exhaustive overview of causal discovery algorithms and the DAG/SEM formalism.</p>
<h1>2.2 Causal Discovery with Prior Knowledge</h1>
<p>The need for integrating prior expert knowledge and data-driven methods has been recognized since initial works on causal discovery (Constantinou et al., 2023). The Bayesian methodology straightforwardly incorporates prior knowledge with data-driven approaches (Heckerman and Geiger, 1995; Friedman and Koller, 2003; Xu et al., 2015; Suter et al., 2023). Methods have been developed to build informative priors on graphs based on agreement with prior beliefs of different types (Mukherjee and Speed, 2008) or with partial prior knowledge (Castelo and Siebes, 2000). Moreover, the Bayesian paradigm has also been used to integrate data with multiple sources of prior knowledge (Amirkhani et al., 2016; Werhli and Husmeier, 2007). Prior knowledge integration has also been proposed and employed in non-Bayesian settings. Classical score-based and constrained-based algorithms can be combined with different prior knowledge such as structural restrictions (de Campos and Castellano, 2007; De Campos and Ji, 2011; Li and Beek, 2018) or information on directed causal paths (Borboudakis and Tsamardinos, 2012, 2013). Similarly, prior knowledge given as ancestral or topological order constraints can help guide exact methods (Chen et al., 2016; Wang et al., 2021). Alternatively, human-in-the-loop approaches perform active interactions with the experts to guide the data-driven methods in discovering the causal graph (Masegosa and Moral, 2013; da Silva et al., 2023).</p>
<h3>2.3 Causality with LLMs</h3>
<p>LLMs present a promising knowledge-driven alternative to expert-based graph building or datadriven causal discovery methods. Different strategies exist to elicit causal graphs from LLMs effectively. These strategies include LLMs to predict causal relations (Kıcıman et al., 2023; Long et al., 2023a; Willig et al., 2023), LLMs as priors for data-driven causal discovery methods (Ban et al., 2023), and LLMs to aid in downstream causal inference tasks by predicting the causal order of variables (Vashishtha et al., 2023). Kıcıman et al. (2023) showcase LLMs' knowledge-based causal discovery capabilities by eliciting DAGs for a series of bivariate and multivariate benchmark datasets where the ground truth is known. Vashishtha et al. (2023) proposed an approach using the topological order over graph variables, showcasing the potential of LLMs in obtaining causal order and enhancing causal inference in diverse fields. Long et al. (2023b) and Long et al. (2023a) examined the role of LLMs in facilitating the construction of causal graphs, focusing on the automation of edge scoring and the use of expert knowledge to improve data-driven causal graph identification, respectively. Zečević et al. (2023) argued against the causal capabilities of LLMs, defining a subgroup of Structural Causal Models (SCMs) and emphasizing the reliance of LLMs on embedded causal knowledge. Cox Jr (2023) conducted a Socratic dialogue with an LLM, highlighting the necessity of sustained questioning for refining human reasoning imitated by LLMs and the reliability of their conclusions. Pawlowski et al. (2023) and Gat et al. (2023) explored the use of LLMs in enabling precise causal reasoning and model-agnostic explanations. Chen et al. (2023) focused on Bayesian Networks (BNs) and proposed strategies for addressing erroneous prior causal statements in LLMbased causal structure learning. Zhang et al. (2023) discussed the potential of enhancing causal machine learning and leveraging LLMs for improved performance and trustworthiness.</p>
<h2>3 Conditional Independence Queries via LLM</h2>
<p>We start by describing and evaluating conditional independence (CI) queries with LLMs. Specifically, we are interested in estimating the validity of conditional independence statements of the type:</p>
<p>$$
\text { Is } X \text { independent of } Y \text { given } Z=\left(Z_{1}, Z_{2}, \ldots, Z_{k}\right) \text { ? }
$$</p>
<p>without having access to observations of the involved variables. Instead, we would like to rely on available or expert knowledge accessible through LLMs.</p>
<p>For an LLM to be able to answer CI queries, it needs to be presented with some context and additional information related to the variables of interest alongside their description. In particular, we assume that for each problem, we have access to the following information:
variables names or acronyms plus a short description for each quantity of interest.
field the general subject area or expert field related to the problem.
context a description of the broad context of the variables under consideration, including relevant details that go beyond general knowledge</p>
<h1>3.1 Prompting for conditional independence testing</h1>
<p>While the approach we propose in this work could be implemented with any LLM trained to follow instructions, we compare GPT-3.5-turbo ${ }^{1}$ and GPT- $4^{2}$ from OpenAI. The former is a less computationally-expensive model with good performance over different benchmarks, while the latter is among the current state-of-the-art LLMs (Chiang et al., 2024). For a CI statement $(X \Perp Y \mid Z)$, we consider a simple prompt that combines the field and context information and a description of the involved variables $(X, Y$, and eventually $Z)$ with a general instruction and response template as follows (see appendix A for a detailed specification of the used prompt):</p>
<div class="codehilite"><pre><span></span><code>Persona specification
Instructions
Context
Variables description
CI Statement question
Response template
</code></pre></div>

<p>The persona is based on the field variable and primes the LLM to produce reasoning appropriate to the area under study. To improve the answers, we apply chain-of-thought prompting following Wei et al. (2023), enabling us to gain insights into the model's reasoning and inspect if it is causally inspired. Further, we ask the model to provide uncertainty about its best guess along the lines of Tian et al. (2023).</p>
<h3>3.2 Testing</h3>
<p>Various strategies can be followed in order to perform a "Hypothesis test" for a conditional independence statement with LLMs. A naive option consists of asking the LLM a single question with the prompt described in the previous section and decide that a statement is valid (i.e., the variables are indeed independent) if the answer is YES and invalid, if it is NO. The main problem with this approach is that since LLMs are probabilistic, a single answer from an LLM does not need to correspond to its mode (the most likely answer) or could fail to respect the required response template: its answer could be, for instance, UNCERTAIN instead of YES/NO. Instead, we ask for an independent batch (size $n$ ) of answers, parse the obtained answers (YES or NO) together with the reported uncertainties, and finally output either an answer based on simple voting or weighted voting where the weights are the reported probabilities.</p>
<p>Alternatively, we implement a "statistical approach" where we actually produce $p$-values for the null hypothesis $p_{n o} \geq p_{y e s}$ (or alternatively $p_{n o} \leq p_{y e s}$ ) where $p_{n o}, p_{y e s}$ are the proportion of NO and YES answers over the total requested batch $n$. The constructed test is based on the idea that we want to test whether the probability of obtaining the answer NO is significantly higher than that of obtaining the answer YES and vice versa. We opt for the null hypothesis if this difference is not</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>significant in light of the obtained responses. A final decision is obtained by setting a significance level $\alpha$ and rejecting the chosen null hypothesis if the p-value is less or equal to $\alpha$ (we will employ $\alpha=0.05$ in the experiments). This last strategy has the advantage of considering the random variability of the answers and could offer a principled way of controlling the false positive rate. The user could then specify, for a particular problem, which of the two null hypotheses they would like to employ (either $p_{n o} \geq p_{y e s}$ or $p_{y e s} \geq p_{n o}$ ), which in turn implies a different false-positive control (considering either NO or YES as positive).</p>
<h1>3.3 Evaluation</h1>
<p>We evaluate the performance for CI testing on various problems defined in the BNLearn repository (Scutari and Denis, 2014), the spurious correlation website (Vigen, 2023), a classical problem on reconstructing protein-signaling networks (Sachs et al., 2005) and a set of graphs on climatic teleconnections (Kretschmer et al., 2021):</p>
<ul>
<li>cancer Simple causal graph involving four factors influencing the probability of cancer (Korb and Nicholson, 2010).</li>
<li>burglary The classical earthquake example in (Korb and Nicholson, 2010; Scutari and Denis, 2014)</li>
<li>asia Causal graph of eight factors linked to respiratory problems (Lauritzen and Spiegelhalter, 1988).</li>
<li>sachs: Causal graph among 11 phosphorylated proteins and phospholipids in single-cell data (Sachs et al., 2005).</li>
<li>spurious: Famous examples of spuriously correlated pairs of variables obtained from the spurious-correlation website (Vigen, 2023).</li>
<li>bk-spv: Hypothesized pathway from autumn sea ice in the Barents and Kara Seas (BK) to the stratospheric polar vortex (SPV) in winter, including various confounding and mediating factors (Kretschmer et al., 2021).</li>
<li>nao-dk-med: Hypothesized causal relation between summertime North Atlantic Oscillation (NAO) and precipitation patterns in Denmark (DK) and the Mediterranean (MED) (Kretschmer et al., 2021).</li>
</ul>
<p>For all graphs except sachs, we evaluate all the possible CI statements over all variables with both permutations of $X$ and $Y$. For sachs, we evaluate all CI statements (with $X-Y$ permutations) up to conditioning sets of size 1. Lastly, for spurious, we evaluate the marginal independence statements $X \Perp Y$ for all pairs of variables. For all the experiments, we obtain the answer to the CI queries by aggregating, as described previously, $n=10$ independent batched responses from the LLM. The code implementing our approach and the problem descriptions are available in the repository https://github.com/IPL-UV/causal_GPT.</p>
<p>Permutation consistency Consistency is a fundamental property that human experts are tested for and is shown to correlate with confidence and disagreement (Litvinova et al., 2022). In our context, consistency means that the result of a conditional independence test should not depend on the order of variables, i.e., it should be commutative in $X$ and $Y$ (given $Z$ ). As a first sanity check, we checked the consistency of the responses across all CITs, with respect to the change of order of $X$ and $Y$ (see Figure 2) for both language models.</p>
<p>Table 1: Performance of LLM-based conditional independence tests with different voting procedures and metrics (accuracy, F1, precision, and recall.) The cells are empty when the metric cannot be computed.</p>
<table>
<thead>
<tr>
<th>Data</th>
<th>Model</th>
<th>Method</th>
<th>Accuracy</th>
<th>F1</th>
<th>Precision</th>
<th>Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>cancer</td>
<td>GPT-3.5</td>
<td>voting</td>
<td>0.64</td>
<td>0.39</td>
<td>0.36</td>
<td>0.43</td>
</tr>
<tr>
<td></td>
<td></td>
<td>stat. Test $\left(H_{0}: \mathbb{L}\right)$</td>
<td>0.72</td>
<td>0.08</td>
<td>0.05</td>
<td>0.33</td>
</tr>
<tr>
<td></td>
<td></td>
<td>stat. Test $\left(H_{0}: \mathbb{L}\right)$</td>
<td>0.47</td>
<td>0.42</td>
<td>0.71</td>
<td>0.29</td>
</tr>
<tr>
<td></td>
<td>GPT-4</td>
<td>voting</td>
<td>0.72</td>
<td>0.25</td>
<td>0.17</td>
<td>0.54</td>
</tr>
<tr>
<td></td>
<td></td>
<td>stat. Test $\left(H_{0}: \mathbb{L}\right)$</td>
<td>0.73</td>
<td>0.19</td>
<td>0.12</td>
<td>0.46</td>
</tr>
<tr>
<td></td>
<td></td>
<td>stat. Test $\left(H_{0}: \mathbb{L}\right)$</td>
<td>0.70</td>
<td>0.31</td>
<td>0.26</td>
<td>0.39</td>
</tr>
<tr>
<td>burglary</td>
<td>GPT-3.5</td>
<td>voting</td>
<td>0.45</td>
<td>0.36</td>
<td>0.50</td>
<td>0.28</td>
</tr>
<tr>
<td></td>
<td></td>
<td>stat. Test $\left(H_{0}: \mathbb{L}\right)$</td>
<td>0.62</td>
<td>0.27</td>
<td>0.26</td>
<td>0.28</td>
</tr>
<tr>
<td></td>
<td></td>
<td>stat. Test $\left(H_{0}: \mathbb{L}\right)$</td>
<td>0.38</td>
<td>0.43</td>
<td>0.90</td>
<td>0.28</td>
</tr>
<tr>
<td></td>
<td>GPT-4</td>
<td>voting</td>
<td>0.81</td>
<td>0.77</td>
<td>1.00</td>
<td>0.63</td>
</tr>
<tr>
<td></td>
<td></td>
<td>stat. Test $\left(H_{0}: \mathbb{L}\right)$</td>
<td>0.88</td>
<td>0.82</td>
<td>1.00</td>
<td>0.69</td>
</tr>
<tr>
<td></td>
<td></td>
<td>stat. Test $\left(H_{0}: \mathbb{L}\right)$</td>
<td>0.79</td>
<td>0.72</td>
<td>1.00</td>
<td>0.56</td>
</tr>
<tr>
<td>asia</td>
<td>GPT-3.5</td>
<td>data-driven (MI)</td>
<td>0.58</td>
<td>0.56</td>
<td>0.54</td>
<td>0.58</td>
</tr>
<tr>
<td></td>
<td></td>
<td>voting</td>
<td>0.51</td>
<td>0.39</td>
<td>0.36</td>
<td>0.43</td>
</tr>
<tr>
<td></td>
<td></td>
<td>stat. Test $\left(H_{0}: \mathbb{L}\right)$</td>
<td>0.61</td>
<td>0.18</td>
<td>0.12</td>
<td>0.44</td>
</tr>
<tr>
<td></td>
<td></td>
<td>stat. Test $\left(H_{0}: \mathbb{L}\right)$</td>
<td>0.48</td>
<td>0.52</td>
<td>0.75</td>
<td>0.39</td>
</tr>
<tr>
<td></td>
<td>GPT-4</td>
<td>voting</td>
<td>0.73</td>
<td>0.63</td>
<td>0.56</td>
<td>0.71</td>
</tr>
<tr>
<td></td>
<td></td>
<td>stat. Test $\left(H_{0}: \mathbb{L}\right)$</td>
<td>0.74</td>
<td>0.58</td>
<td>0.48</td>
<td>0.73</td>
</tr>
<tr>
<td></td>
<td></td>
<td>stat. Test $\left(H_{0}: \mathbb{L}\right)$</td>
<td>0.76</td>
<td>0.69</td>
<td>0.72</td>
<td>0.66</td>
</tr>
<tr>
<td>sachs</td>
<td>GPT-3.5</td>
<td>data-driven (Sachs et al., 2005)</td>
<td>0.61</td>
<td>0.21</td>
<td>0.85</td>
<td>0.12</td>
</tr>
<tr>
<td></td>
<td></td>
<td>voting</td>
<td>0.45</td>
<td>0.13</td>
<td>0.53</td>
<td>0.08</td>
</tr>
<tr>
<td></td>
<td></td>
<td>stat. Test $\left(H_{0}: \mathbb{L}\right)$</td>
<td>0.81</td>
<td>0.13</td>
<td>0.24</td>
<td>0.09</td>
</tr>
<tr>
<td></td>
<td></td>
<td>stat. Test $\left(H_{0}: \mathbb{L}\right)$</td>
<td>0.20</td>
<td>0.13</td>
<td>0.94</td>
<td>0.07</td>
</tr>
<tr>
<td></td>
<td>GPT-4</td>
<td>voting</td>
<td>0.93</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>stat. Test $\left(H_{0}: \mathbb{L}\right)$</td>
<td>0.94</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>stat. Test $\left(H_{0}: \mathbb{L}\right)$</td>
<td>0.93</td>
<td>0.15</td>
<td>0.10</td>
<td>0.27</td>
</tr>
<tr>
<td>bk-spv</td>
<td>GPT-3.5</td>
<td>voting</td>
<td>0.55</td>
<td>0.41</td>
<td>0.51</td>
<td>0.34</td>
</tr>
<tr>
<td></td>
<td></td>
<td>stat. Test $\left(H_{0}: \mathbb{L}\right)$</td>
<td>0.72</td>
<td>0.24</td>
<td>0.17</td>
<td>0.42</td>
</tr>
<tr>
<td></td>
<td></td>
<td>stat. Test $\left(H_{0}: \mathbb{L}\right)$</td>
<td>0.41</td>
<td>0.44</td>
<td>0.90</td>
<td>0.27</td>
</tr>
<tr>
<td></td>
<td>GPT-4</td>
<td>voting</td>
<td>0.75</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>stat. Test $\left(H_{0}: \mathbb{L}\right)$</td>
<td>0.75</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>stat. Test $\left(H_{0}: \mathbb{L}\right)$</td>
<td>0.74</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>For GPT-3.5, the agreement over the (conditional) independence tests reaches merely $51 \%$, with around $22 \%$ of all answers being UNCERTAIN in at least one of the directions. This indicates that the results for GPT-3.5 vary substantially over multiple runs, particularly under a change of order of the variables. In contrast, for GPT-4, in $87 \%$ of all cases, the answers for both directions of the CITs yielded the same result. Albeit with small deviations due to stochasticity, this is a satisfactory result in consistency for GPT-4. In contrast to GPT-3.5, however, the overall proportion of CIT answered NO was over $70 \%$. Although this proportion may be inflated by relatively large and dense graphs such as
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Confusion matrix of the model's responses to queries with changing order of $X$ and $Y$. UNCERTAIN outcomes in case of a tie in majority voting are hidden. The agreement score aggregates common YES, NO and UNCERTAIN outcomes.
sachs, it indicates a tendency of GPT-4 towards rejecting independence as the safe answer. Given the remaining variation, especially in the responses of GPT-3.5, we propose to aggregate the query results in both directions to obtain results invariant to the order of the two involved variables.</p>
<p>Performance of CIT Table 1 summarizes the evaluations of the CI queries over the different problems. We compute the standard classification metrics (accuracy, precision, recall, and F1 scores). The results show varying performance over the different causal graphs and models. As some of these problems constitute pedagogical examples or hypothesized graphs, they are not accompanied by observations. Hence, we only compare against data-driven baselines for sachs and asia. Overall, we can see that the statistical tests, which imply certain assumptions about the graph's sparsity and require stronger evidence (i.e., more votes) for rejecting either dependence or independence, give some control over the metrics, usually implying a trade-off between recall and precision. For cancer, GPT-3.5 yields the best results in terms of F1 and precision, while GPT-4 has the higher accuracy. Inspection of the implied graph under the chatPC (see fig. 4) suggests that GPT-4 assumes more dependencies and finds a denser graph. The context of burglary contains mostly all information on how the variables are connected. The language models still need to perform the task of extracting valid CI statements from there. GPT-4 clearly outperforms GPT-3.5 in this regard. This suggests that given access to the relevant information, GPT-4 mostly makes correct statements about CI. On the asia graph, the LLM models beat the data-driven baseline based on mutual information in all metrics. Only in precision, GPT-3.5 achieves a slightly higher value than GPT-4. On the sachs data, in terms of F1, the data-driven method (Sachs et al., 2005) outperforms the two language models, whilst GPT-4 shows the highest accuracy. It is noteworthy that GPT-4 avoids eliminating the connection of the proteins Plc- $\gamma$ and PIP2, with PKC, two known mistakes of the data-driven approach. The LLM-driven CITs do not yield a perfect oracle in their current state. Some improvement can likely be made with the next generations of language models, fine-tuning of the prompt (Cheng et al., 2023), or disambiguation of the context. A final source of error is that the required knowledge may be controversial or may simply not exist. Here, systems based on retrieval-augmented-generation (RAG) (Lewis et al., 2021) could help to ground answers on secured knowledge or weigh sources against each other (old vs. new; unsystematic vs. scientific). As datadriven methods also reveal major failure modes, using knowledge-driven evidence to complement data can be a promising future research avenue.</p>
<p>Inquiring spurious correlations Up to this point, GPT-4 seemed overly conservative in favor of dependence. To probe this further, we went to the other extreme and asked for statistical independence between variables taken from the spurious correlation website (see Appendix, Table 2). In contrast, in this case, the model almost always chose independence. Nevertheless, there are notable exceptions, such as per capita consumption of chicken and US crude oil imports, where the LLM correctly suggests that oil imports and, in turn, oil prices may affect chicken prices and its consumption through transportation costs. Furthermore, broader economic factors may serve as a common confounder (for the full response see appendix C). Other slightly less clear connections seem to exist in the pairs total revenue generated by arcades - computer science doctorates, and Japanese passenger cars sold in the US - suicides by crashing of motor vehicle, where the voting almost resulted in stalemates. We may favour dependence or independence depending on our choice of the null hypothesis. It is important to note that data-driven methods would always detect dependence as the spurious correlation examples are selected in this manner. This can happen in any data by chance; Hence, consulting additional knowledge-based evidence as a safety net seems reasonable.</p>
<h1>4 Application to Causal Discovery</h1>
<p>We propose to couple the CI oracle or testing, introduced in the previous section, with the PCalgorithm (Colombo and Maathuis, 2014) for recovery of the Markov equivalence class of a causal graph. The PC algorithm starts from a fully connected skeleton (undirected graph) among the considered variables and iteratively removes edges between variables $X$ and $Y$ when it finds a conditioning set $Z$ such that $X$ is independent of $Y$ given $Z$. After the so-called skeleton phase, v-structures are identified through specific CI testing, and finally, a set of orientation rules are applied (Meek, 1995). Hence, to implement chatPC, we plug in the LLM-based conditional independence testing to an existing PC implementation (Atienza et al., 2022).</p>
<p>Causal graphs from the examples The PC algorithm should not be run only with LLMs for arbitrarily large graphs. First, CITs with growing conditioning set size turn even for an expert increasingly into mental gymnastics as you technically verify d-separation. Second, with the increasing number of variables, the number of required CITs grows exponentially. This renders the approach under the current energy and financial costs for querying LLMs infeasible unless the graph is small or one couples it with data-driven CITs. We applied chatPC to the burglary, cancer, asia and nao-dk-med.</p>
<p>First, for burglary, chatPC can retrieve the correct assumed causal graph (see fig. 3). This result stands even though the method made some mistakes in the CITs for burglary. The graph of the cancer variables (see fig. 4) contains the right sub-graph, which means that the LLM introduced no incorrect independence. The links between pollution and dyspnoea and smoking and dyspnoea are absent in the ground truth, but research suggests that such pathways might be assumed (Berliner et al., 2016). An expert may choose to introduce sparsity into a graph by including only the strongest links. With LLM-based CITs, this is possible through the significance level of the statistical tests, which we could tune up to the point where some of these links disappear. It is also noteworthy that chatPC could not direct any edge in the retrieved graph. In principle, directing edges is a simpler task that could, in a second step, also be carried out with LLMs (Kıcıman et al., 2023). For asia (see fig. 6), we find an overall reasonable graph that almost matches the skeleton of the ground truth. Notable differences are that the asia node is disconnected, smoke is directly connected to dysp, which aligns with the cancer graph, and xray only has a direct link to bronc. Finally, in NAO-DK-MED, the hypothesized graph suggests that the summertime North-Atlantic Oscillation is the single common cause of precipitation patterns in Denmark and the Mediterranean. The LLM would hence need to identify that DK $\Perp$ MED| NAO, but it fails to do so (see fig. 5). Kretschmer et al. (2021) assume that the data is detrended, and even then, it is a debatable assumption that there are no connections left. Hence, the graph retrieved by chatPC is arguably not false. Moreover, one can</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Assumed true graph (a) and skeleton recovered (b) with the proposed chatPC approach for the burglary problem. Variables: Burglary in progress (B); earthquake (E); alarm ringing (A); Mary (M) or John (J) calling.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Assumed true graph (a) and skeleton recovered (b) with the proposed chatPC approach for the cancer problem. Variables: Patient is a smoker (S); patient exposed to pollution (P); patient suffers from lung cancer (C); positive results from a chest X-ray (X); patient is suffering from dyspnoea (D).
see some signal reflecting this as, in contrast to all other CITs showing 20-0 (NO-YES proportions) for dependence, DK $\Perp$ MED| NAO resulted in 17-3 (NO-YES). If one were to require a complete agreement for dependency, the graph would have matched the assumed ground truth.</p>
<h1>5 Conclusions</h1>
<p>Our work contributes to the existing literature by probing an alternative to the data-driven PC approach, leveraging the capabilities of LLMs for PC when data is limited or unavailable. Building a reliable knowledge-based conditional independence oracle could either provide a prior to constrain
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Assumed true graph (a) and skeleton recovered (b) with the proposed chatPC approach for the nao-dk-med problem. Variables: North-Atlantic Oscillation (NAO); summer precipitation in Denmark (DK); summer precipitation in the Mediterranean region (MED).</p>
<p>its data-driven counterpart or deliver a substitute for data-driven methods. Our analysis attempts to shed light on where we stand in this endeavour.</p>
<p>We found that LLM conjectures about hidden confounders, showing that they use causal reasoning to tackle this primarily statistical task. This, however, is neither consistently nor always successfully done. The varying performance over different tasks showed that more effort is needed to steer the models to more efficient causal reasoning. We proved that employing an aggregating mechanism framed as a statistical test leads to improved performance and effective control over the retrieved graph's density in the context of the PC algorithm. The causal graphs predicted by the PC algorithm with LLM-based CITs appear reasonable. While not infallible, the method demonstrates potential in capturing meaningful causal relationships, offering a promising avenue for automated causal graph construction in combination with data-driven methods. Finally, we found that LLMs generally tend toward conservative answers, contrasting with the often bolder responses from human experts. Understanding and addressing the cautious nature of LLM reasoning is crucial for refining the accuracy and reliability of the generated causal graphs. Future work will explore the combination of data-driven and language-driven causality, where relying on CIT estimates in PC schemes constitutes a sound framework to seamlessly integrate evidence based on data and knowledge. Here, RAG-based systems (Lewis et al., 2021) could play a key role by ensuring that retrieved knowledge stems from scientific sources. The proposed approach could be useful in alignment and evaluating LLMs. Probing CITs and their implication for a causal graph construction with LLMs is useful to understand the level of causal reasoning and understanding of such systems. It does not escape our notice that, despite the many challenges in our pathway to causal reasoning with language models, the approach can bring many opportunities for scientific discovery.</p>
<h1>Acknowledgments and Disclosure of Funding</h1>
<p>This work was supported by the BBVA with the project 'SCALE', Microsoft with the Causal4Africa project and the GVA PROMETEO AI4CS project (CIPROM/2021/056.)</p>
<h2>References</h2>
<p>H. Amirkhani, M. Rahmati, P. J. Lucas, and A. Hommersom. Exploiting experts' knowledge for structure learning of bayesian networks. IEEE transactions on pattern analysis and machine intelligence, 39(11):2154-2170, 2016.
D. Atienza, C. Bielza, and P. Larrañaga. Pybnesian: An extensible python package for bayesian networks. Neurocomputing, 504:204-209, 2022. ISSN 0925-2312. doi: https://doi.org/10. 1016/j.neucom.2022.06.112. URL https://www.sciencedirect.com/science/article/pii/ S0925231222008438.
T. Ban, L. Chen, X. Wang, and H. Chen. From query tools to causal architects: Harnessing large language models for advanced causal discovery from data. arXiv preprint arXiv:2306.16902, 2023.
D. Berliner, N. Schneider, T. Welte, and J. Bauersachs. The differential diagnosis of dyspnea. Dtsch Arztebl Int, 113(49):834-845, Dec. 2016.
G. Borboudakis and I. Tsamardinos. Incorporating causal prior knowledge as path-constraints in bayesian networks and maximal ancestral graphs. In Proceedings of the 29th International Coference on International Conference on Machine Learning, pages 427-434, 2012.
G. Borboudakis and I. Tsamardinos. Scoring and searching over bayesian networks with causal and associative priors. In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, pages 102-111, 2013.</p>
<p>T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.
G. Camps-Valls, A. Gerhardus, U. Ninad, G. Varando, G. Martius, E. Balaguer-Ballester, R. Vinuesa, E. Diaz, L. Zanna, and J. Runge. Discovering causal relations and equations from data. Physics Reports, 1044:1-68, 2023. ISSN 0370-1573. doi: https://doi.org/10.1016/j.physrep. 2023. 10.005 .
R. Castelo and A. Siebes. Priors on network structures. biasing the search for bayesian networks. International Journal of Approximate Reasoning, 24(1):39-57, 2000.
E. Y.-J. Chen, Y. Shen, A. Choi, and A. Darwiche. Learning bayesian networks with ancestral constraints. Advances in Neural Information Processing Systems, 29, 2016.
L. Chen, T. Ban, X. Wang, D. Lyu, and H. Chen. Mitigating prior errors in causal structure learning: Towards llm driven prior knowledge. arXiv preprint arXiv:2306.07032, 2023.
J. Cheng, X. Liu, K. Zheng, P. Ke, H. Wang, Y. Dong, J. Tang, and M. Huang. Black-box prompt optimization: Aligning large language models without model training, 2023.
W.-L. Chiang, L. Zheng, Y. Sheng, A. N. Angelopoulos, T. Li, D. Li, H. Zhang, B. Zhu, M. Jordan, J. E. Gonzalez, and I. Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024.
D. Chickering. Optimal structure identification with greedy search. Journal of Machine Learning Research, 3:507-554, 01 2002a.
D. M. Chickering. Learning equivalence classes of bayesian-network structures. The Journal of Machine Learning Research, 2:445-498, 2002b.
T. Claassen and I. G. Bucur. Greedy equivalence search in the presence of latent confounders. In J. Cussens and K. Zhang, editors, Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence, volume 180 of Proceedings of Machine Learning Research, pages 443-452. Pmlr, 01-05 Aug 2022.
D. Colombo and M. H. Maathuis. Order-independent constraint-based causal structure learning. Journal of Machine Learning Research, 15(116):3921-3962, 2014. URL http://jmlr.org/ papers/v15/colombo14a.html.
A. C. Constantinou, Z. Guo, and N. K. Kitson. The impact of prior knowledge on causal structure learning. Knowledge and Information Systems, pages 1-50, 2023.
J. Cornfield, W. Haenszel, E. C. Hammond, A. M. Lilienfeld, M. B. Shimkin, and E. L. Wynder. Smoking and lung cancer: recent evidence and a discussion of some questions. Journal of the National Cancer institute, 22(1):173-203, 1959.
L. A. Cox Jr. Causal reasoning about epidemiological associations in conversational ai. Global Epidemiology, 5:100102, 2023.</p>
<p>T. da Silva, E. Silva, A. Ribeiro, A. Góis, D. Heider, S. Kaski, and D. Mesquita. Human-in-the-loop causal discovery under latent confounding using ancestral gflownets, 2023.
C. P. De Campos and Q. Ji. Efficient structure learning of bayesian networks using constraints. The Journal of Machine Learning Research, 12:663-689, 2011.
L. M. de Campos and J. G. Castellano. Bayesian network learning algorithms using structural restrictions. International Journal of Approximate Reasoning, 45(2):233-254, 2007.
N. Friedman and D. Koller. Being bayesian about network structure. a bayesian approach to structure discovery in bayesian networks. Machine learning, 50:95-125, 2003.
Y. Gat, N. Calderon, A. Feder, A. Chapanin, A. Sharma, and R. Reichart. Faithful explanations of black-box nlp models using llm-generated counterfactuals. arXiv preprint arXiv:2310.00603, 2023.
A. Gerhardus and J. Runge. High-recall causal discovery for autocorrelated time series with latent confounders. Advances in Neural Information Processing Systems, 33:12615-12625, 2020.
C. Glymour, K. Zhang, and P. Spirtes. Review of causal discovery methods based on graphical models. Frontiers in Genetics, 10, 2019. ISSN 1664-8021. doi: 10.3389/fgene.2019.00524. URL https://www.frontiersin.org/articles/10.3389/fgene.2019.00524.
D. Heckerman and D. Geiger. Learning bayesian networks: a unification for discrete and gaussian domains. In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence, pages $274-284,1995$.
M. Hobbhahn, T. Lieberum, and D. Seiler. Investigating causal understanding in llms. In NeurIPS ML Safety Workshop, 2022.
E. Kıcıman, R. Ness, A. Sharma, and C. Tan. Causal reasoning and large language models: Opening a new frontier for causality. arXiv preprint arXiv:2305.00050, 2023.
K. B. Korb and A. E. Nicholson. Bayesian artificial intelligence. CRC press, 2010.
M. Kretschmer, S. V. Adams, A. Arribas, R. Prudden, N. Robinson, E. Saggioro, and T. G. Shepherd. Quantifying causal pathways of teleconnections. Bulletin of the American Meteorological Society, 102(12):E2247 - E2263, 2021. doi: 10.1175/BAMS-D-20-0117.1. URL https://journals.ametsoc.org/view/journals/bams/102/12/BAMS-D-20-0117.1.xml.
S. L. Lauritzen and D. J. Spiegelhalter. Local computations with probabilities on graphical structures and their application to expert systems. Journal of the Royal Statistical Society: Series B (Methodological), 50(2):157-194, 1988.
P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W. tau Yih, T. Rocktäschel, S. Riedel, and D. Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021.
A. Li and P. Beek. Bayesian network structure learning with side constraints. In International conference on probabilistic graphical models, pages 225-236. PMLR, 2018.
A. Litvinova, R. H. Kurvers, R. Hertwig, and S. M. Herzog. How experts' own inconsistency relates to their confidence and between-expert disagreement. Scientific Reports, 12(1):9273, 2022.
S. Long, A. Piché, V. Zantedeschi, T. Schuster, and A. Drouin. Causal discovery with language models as imperfect experts. arXiv preprint arXiv:2307.02390, 2023a.</p>
<p>S. Long, T. Schuster, A. Piché, S. Research, et al. Can large language models build causal graphs? arXiv preprint arXiv:2303.05279, 2023b.
D. Malinsky and P. Spirtes. Causal structure learning from multivariate time series in settings with unmeasured confounding. In T. D. Le, K. Zhang, E. Kıcıman, A. Hyvärinen, and L. Liu, editors, Proceedings of 2018 ACM SIGKDD Workshop on Causal Disocvery, volume 92 of Proceedings of Machine Learning Research, pages 23-47, London, UK, 20 Aug 2018. PMLR.
A. R. Masegosa and S. Moral. An interactive approach for bayesian network learning using domain/expert knowledge. International Journal of Approximate Reasoning, 54(8):1168-1181, 2013.
C. Meek. Causal inference and causal explanation with background knowledge. In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, UAI'95, page 403-410, San Francisco, CA, USA, 1995. Morgan Kaufmann Publishers Inc. ISBN 1558603859.
C. Meek. Graphical Models: Selecting causal and statistical models. PhD thesis, Carnegie Mellon University, 1997.
S. Mukherjee and T. P. Speed. Network inference using informative priors. Proceedings of the National Academy of Sciences, 105(38):14313-14318, 2008.
N. Pawlowski, J. Vaughan, J. Jennings, and C. Zhang. Answering causal questions with augmented llms. 2023.
J. Pearl et al. Models, reasoning and inference. Cambridge, UK: CambridgeUniversityPress, 19(2): 3,2000 .
J. Peters, D. Janzing, and B. Schölkopf. Elements of causal inference: foundations and learning algorithms. 2017.
K. Sachs, O. Perez, D. Pe'er, D. A. Lauffenburger, and G. P. Nolan. Causal protein-signaling networks derived from multiparameter single-cell data. Science, 308(5721):523-529, 2005.
M. Scutari and J.-B. Denis. Bayesian networks. Chapman \&amp; Hall/CRC Texts in Statistical Science. Chapman \&amp; Hall/CRC, Philadelphia, PA, June 2014.
M. Scutari, C. E. Graafland, and J. M. Gutiérrez. Who learns better bayesian network structures: Accuracy and speed of structure learning algorithms. International Journal of Approximate Reasoning, 115:235-253, 2019. ISSN 0888-613X. doi: https://doi.org/10.1016/j.ijar.2019.10.003. URL https://www.sciencedirect.com/science/article/pii/S0888613X19301434.
P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. MIT press, 2nd edition, 2000 .
P. Suter, J. Kuipers, G. Moffa, and N. Beerenwinkel. Bayesian structure learning and sampling of bayesian networks with the r package bidag. Journal of Statistical Software, 105(9):1-31, 2023. doi: 10.18637/jss.v105.i09. URL https://www.jstatsoft.org/index.php/jss/article/view/ v105i09.
K. Tian, E. Mitchell, A. Zhou, A. Sharma, R. Rafailov, H. Yao, C. Finn, and C. D. Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback, 2023.
A. Vashishtha, A. G. Reddy, A. Kumar, S. Bachu, V. N. Balasubramanian, and A. Sharma. Causal inference using llm-guided discovery. arXiv preprint arXiv:2310.15117, 2023.</p>
<p>T. Vigen. Spurious correlations, https://www.tylervigen.com/spurious-correlations, 2023. accessed $09 / 11 / 2023$.
Z. Wang, X. Gao, Y. Yang, X. Tan, and D. Chen. Learning bayesian networks based on order graph with ancestral constraints. Knowledge-Based Systems, 211:106515, 2021.
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.
A. V. Werhli and D. Husmeier. Reconstructing gene regulatory networks with bayesian networks by combining expression data with multiple sources of prior knowledge. Statistical applications in genetics and molecular biology, 6(1), 2007.
M. Willig, M. ZE ^CEVI C, D. S. Dhami, and K. Kersting. Probing for correlations of causal facts: Large language models and causality. preprint, 2023.
S. Wright. The method of path coefficients. The annals of mathematical statistics, 5(3):161-215, 1934 .
E. L. Wynder. Tobacco as a cause of lung cancer: some reflections. American Journal of Epidemiology, 146(9):687-694, 1997.
F. Xu, Q. Lin, J. Han, T. Zhao, J. Liu, and E. Cambria. Are large language models really good logical reasoners? A comprehensive evaluation and beyond, 2023.
J.-G. Xu, Y. Zhao, J. Chen, and C. Han. A structure learning algorithm for bayesian network using prior knowledge. Journal of Computer Science and Technology, 30:713-724, 2015.
M. Zečević, M. Willig, D. S. Dhami, and K. Kersting. Causal parrots: Large language models may talk causality but are not causal. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=tv46tCzs83.
C. Zhang, D. Janzing, M. van der Schaar, F. Locatello, and P. Spirtes. Causality in the time of llms: Round table discussion results of clear 2023. Proceedings of Machine Learning Research vol TBD, $1: 7,2023$.</p>
<h1>Appendix A. Details on the Prompt</h1>
<p>Here, we describe the details of the prompt used. In the following prompt scheme, curly brackets $}$ denotes elements that are replaced by corresponding elements from the problem description and the specific CI statement which is queried. In particular {field} and {context} are replaced by their values for the problem, and ${\mathrm{x}},{\mathrm{y}}$ and ${\mathrm{z}}$ are the name of the variables involved in the CI $(X \Perp Y \mid Z)$. If $Z=\emptyset$ the whole conditioned on ${z}$ is dropped from the prompt.</p>
<div class="codehilite"><pre><span></span><code><span class="n">system</span><span class="o">:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">helpful</span><span class="w"> </span><span class="n">expert</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="o">{</span><span class="n">field</span><span class="o">}</span><span class="w"> </span><span class="n">willing</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">answer</span>
<span class="n">questions</span><span class="o">.</span>
<span class="n">You</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">asked</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">provide</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">estimate</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">confidence</span>
<span class="n">on</span><span class="w"> </span><span class="n">statistical</span><span class="w"> </span><span class="n">independence</span><span class="w"> </span><span class="n">between</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">variables</span>
<span class="o">(</span><span class="n">eventually</span><span class="w"> </span><span class="n">conditioned</span><span class="w"> </span><span class="n">on</span>
<span class="n">a</span><span class="w"> </span><span class="kd">set</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">variables</span><span class="o">).</span>
<span class="n">Your</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">based</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">observations</span><span class="o">,</span>
<span class="n">but</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">available</span><span class="w"> </span><span class="n">knowledge</span><span class="o">.</span>
<span class="n">Even</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">unsure</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">uncertain</span><span class="o">,</span><span class="w"> </span><span class="n">provide</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">uncertainty</span><span class="o">.</span>
<span class="n">Answer</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">required</span><span class="w"> </span><span class="n">format</span><span class="o">.</span>
<span class="n">user</span><span class="o">:</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="p">{</span><span class="n">context</span><span class="p">}</span>
<span class="n">Consider</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">variables</span><span class="p">:</span>
<span class="p">{</span><span class="n">variables</span><span class="w"> </span><span class="n">list</span><span class="p">}</span>
<span class="k">is</span><span class="w"> </span><span class="p">{</span><span class="n">x</span><span class="p">}</span><span class="w"> </span><span class="n">conditionally</span><span class="w"> </span><span class="n">independent</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="p">{</span><span class="n">y</span><span class="p">}</span><span class="w"> </span><span class="n">conditioned</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="p">{</span><span class="n">z</span><span class="p">}</span><span class="err">?</span>
<span class="n">system</span><span class="p">:</span><span class="w"> </span><span class="n">After</span><span class="w"> </span><span class="n">explaining</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">reasoning</span><span class="p">,</span><span class="w"> </span><span class="n">provide</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">between</span>
<span class="n">brackets</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">YES</span><span class="o">/</span><span class="n">NO</span><span class="p">,</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">percentage</span><span class="w"> </span><span class="n">uncertainty</span><span class="w"> </span><span class="n">between</span><span class="w"> </span><span class="n">parenthesis</span><span class="o">.</span>
<span class="n">Where</span><span class="w"> </span><span class="n">YES</span><span class="w"> </span><span class="n">stands</span><span class="w"> </span><span class="k">for</span>
\<span class="s2">&quot;{x} is conditionally independent of {y} conditioned on {z}&quot;</span>
<span class="ow">and</span><span class="w"> </span><span class="n">NO</span><span class="w"> </span><span class="n">stands</span><span class="w"> </span><span class="k">for</span>
<span class="s2">&quot;{z} is not conditionally independent of {y} conditioned on {z}&quot;</span><span class="o">.</span>
<span class="n">For</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="p">[</span><span class="n">NO</span><span class="w"> </span><span class="p">(</span><span class="mi">50</span><span class="o">%</span>
</code></pre></div>

<h1>Appendix B. Spurious Correlations</h1>
<p>We report in Table 2 the responses obtained for the spurious problem over 15 marginal independence statements for the corresponding pairs of spuriously associated variables. We report the decisions obtained with the voting and the two statistical approaches; moreover, we report the number of NO and YES answers among the $n=20$ batched responses.</p>
<h2>Appendix C. Response Example</h2>
<p>Considering the relationship between per capita consumption of chicken and US crude oil imports:</p>
<ul>
<li>The per capita consumption of chicken might seem unrelated to crude oil imports at first glance, as they belong to different sectors (agriculture vs. energy).</li>
<li>However, there are indirect links between them. The price of oil can influence the cost of transportation and feed, which in turn can affect the price of chicken and potentially consumption patterns.</li>
<li>Additionally, broader economic factors influenced by oil prices, such as inflation rates and economic growth, could impact consumers'' purchasing power and preferences.</li>
</ul>
<p>Given these indirect connections, we cannot confidently state that there is absolute independence between these two variables. Therefore, the answer is [NO ( $75 \%$</p>
<p>Table 2: Predictions for variable pairs of the spurious correlations dataset with gpt-4. Numbers below each statistical test denote the respective $p$-value.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Variable <br> Name</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">voting <br> NO - YES</th>
<th style="text-align: center;">stat. Test <br> $\left(H_{0}: \Perp\right)$</th>
<th style="text-align: center;">Test <br> $\left(H_{0}: \not 2.\right)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">spending <br> suicides</td>
<td style="text-align: center;">US spending on science, space, and technology <br> Suicides by hanging, strangulation and suffocation</td>
<td style="text-align: center;">YES <br> $4-16$</td>
<td style="text-align: center;">YES <br> 1</td>
<td style="text-align: center;">$\begin{aligned} &amp; \hline \text { YES } \ &amp; \approx 10^{-4} \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">pool <br> cage</td>
<td style="text-align: center;">number of people who drowned by falling into a pool per year number of films Nicolas Cage appeared in per year</td>
<td style="text-align: center;">YES $0-20$</td>
<td style="text-align: center;">YES $1$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \hline \text { YES } \ &amp; \approx 10^{-10} \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">cheese <br> bed</td>
<td style="text-align: center;">per capita cheese consumption number of people who died by becoming tangled in their bedsheet</td>
<td style="text-align: center;">YES $0-20$</td>
<td style="text-align: center;">YES $1$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \hline \text { YES } \ &amp; \approx 10^{-10} \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">divorce <br> margarine</td>
<td style="text-align: center;">divorce rate in Maine per capita consumption of margarine</td>
<td style="text-align: center;">YES $1-19$</td>
<td style="text-align: center;">YES $1$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \hline \text { YES } \ &amp; \approx 10^{-9} \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">age <br> murder</td>
<td style="text-align: center;">age of Miss America number of people murdered by steam, hot vapors and hot objects</td>
<td style="text-align: center;">YES $0-20$</td>
<td style="text-align: center;">YES $1$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \hline \text { YES } \ &amp; \approx 10^{-10} \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">revenue <br> CS</td>
<td style="text-align: center;">total revenue generated by arcades computer science doctorates awarded in the US</td>
<td style="text-align: center;">YES $9-11$</td>
<td style="text-align: center;">YES $0.74$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { NO } \ &amp; 0.26 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">launches <br> Soc</td>
<td style="text-align: center;">worldwide non-commercial space launches sociology doctorates awarded (US)</td>
<td style="text-align: center;">YES $3-17$</td>
<td style="text-align: center;">YES $1$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \hline \text { YES } \ &amp; \approx 10^{-6} \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">mozzarella <br> engineering</td>
<td style="text-align: center;">per capita consumption of mozzarella cheese civil engineering doctorates awarded</td>
<td style="text-align: center;">YES $0-19$</td>
<td style="text-align: center;">YES $1$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \hline \text { YES } \ &amp; \approx 10^{-10} \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">boat <br> Kentucky</td>
<td style="text-align: center;">people who drowned after falling out of a fishing boat marriage rate in Kentucky</td>
<td style="text-align: center;">YES $0-20$</td>
<td style="text-align: center;">YES $1$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \hline \text { YES } \ &amp; \approx 10^{-10} \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Norway <br> railway</td>
<td style="text-align: center;">US crude oil imports from Norway drivers killed in collision with railway train</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { YES } \ &amp; 0-20 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { YES } \ &amp; 1 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \hline \text { YES } \ &amp; \approx 10^{-10} \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">chicken <br> oil</td>
<td style="text-align: center;">per capita consumption of chicken US crude oil imports</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { NO } \ &amp; 16-4 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { NO } \ &amp; \approx 10^{-4} \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { NO } \ &amp; 1 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">swimming <br> -pool <br> power</td>
<td style="text-align: center;">number people who drowned while in a swimming-pool power generated by US nuclear power plants</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { YES } \ &amp; 0-20 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { YES } \ &amp; 1 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { YES } \ &amp; \approx 10^{-10} \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">cars <br> crashing</td>
<td style="text-align: center;">Japanese passenger cars sold in the US Suicides by crashing of motor vehicle</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { NO } \ &amp; 12-8 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { YES } \ &amp; 0.1 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { NO } \ &amp; 0.9 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">spelling <br> spiders</td>
<td style="text-align: center;">letters in winning word of Scripps National Spelling Bee number of people killed by venomous spiders</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { YES } \ &amp; 0-20 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { YES } \ &amp; 1 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { YES } \ &amp; \approx 10^{-10} \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">maths <br> uranium</td>
<td style="text-align: center;">math doctorates awarded uranium stored at US nuclear power plants</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { YES } \ &amp; 0-20 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { YES } \ &amp; 1 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { YES } \ &amp; \approx 10^{-10} \end{aligned}$</td>
</tr>
</tbody>
</table>
<h1>Appendix D. Additional Results</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Graph obtained with the chatPC approach on the asia problem. Variables: Patient has recently visited Asia (asia); patient has tuberculosis (tub); patient is a smoker (smoke); patient has lung cancer (lung); patient has bronchitis (bronc); patient has dyspnoea (dysp); patient has either tuberculosis or lung cancer or both (either); positive result in chest Xray (xray).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>https://platform.openai.com/docs/models/gpt-3-5</li>
<li>https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
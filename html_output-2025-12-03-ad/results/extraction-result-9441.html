<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9441 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9441</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9441</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-d6045d2ccc9c09ca1671348de86d07da6bc28eea</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea" target="_blank">Training Verifiers to Solve Math Word Problems</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is demonstrated that verification significantly improves performance on GSM8K, and there is strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.</p>
                <p><strong>Paper Abstract:</strong> State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9441.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9441.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Finetuning (single low-temp sample)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Finetuning with single low-temperature autoregressive sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline approach where pretrained GPT-3 family models are finetuned on GSM8K and evaluated by taking a single low-temperature (T=0) autoregressive sample that produces a full natural-language solution and final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 family (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B, 175B (reported experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>8.5K grade-school math word problems requiring 2–8 elementary arithmetic/algebra steps; test metric is solve rate (percentage of problems with correct final answer).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Finetuned model outputs full natural-language solution followed by final answer; sampling at test time with low temperature T=0 (argmax) and a single sample per problem (test@1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>6B finetuned (full natural-language solution, T=0, test@1): 20.6% solve rate (reported for 6B full-solution finetuned baseline); 175B finetuned achieves substantially higher solve rates (figure comparisons shown in paper but exact 175B numeric not listed in text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Generating full natural-language chain-of-thought style solutions before emitting the final answer substantially helps performance compared to emitting only a final answer; larger models (175B) outperform smaller ones under this format.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Finetuning for 20 epochs; sampling temperature T=0 for test@1; calculator annotations injected to make arithmetic reliable; evaluation uses single sample (test@1).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Verifiers to Solve Math Word Problems', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9441.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9441.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Final-answer-only output</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Finetuning to output only the final answer (no intermediate solution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Alternate prompt/output format where the model is trained/asked to produce only the final numeric answer instead of an intermediate natural-language solution; evaluated on GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 family (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same GSM8K grade-school math problems; metric is solve rate (correct final answer).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Model is finetuned and evaluated to directly output the final numeric answer without any intermediate reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against full natural-language solution format (chain-of-thought style) with single low-temperature sample.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Final-answer-only finetuned 6B: 5.2% solve rate (T=0, single sample).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Full natural-language-solution finetuned 6B: 20.6% solve rate (T=0, single sample).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-15.4 percentage points (absolute) / ~75% relative drop (20.6% -> 5.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Providing the intermediate natural-language solution greatly helps multi-step reasoning: removing steps forces the model to produce the final answer directly, causing a large performance drop, indicating the model benefits from producing and using intermediate computation/analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>6B model finetuned as baseline; both settings use same training data; sampling T=0 at test; explicit numeric comparison reported in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Verifiers to Solve Math Word Problems', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9441.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9441.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verification (sample-and-rank)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training verifiers and using sample-and-rank verification at test time</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generate many high-temperature candidate solutions with a generator, then score and rank them with a separately trained verifier that predicts correctness; return the top-ranked solution (or majority vote among top-ranked answers).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 family (generator + verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B and 175B (experiments primarily reported on these sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same GSM8K grade-school math problems; metric is solve rate; evaluation considers test@N (percentage solved at least once among N samples) and verifier-ranked selection.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Generator finetuned for 2 epochs; sample 100 completions per problem at sampling temperature T=0.7; verifier (initialized from generator) trained to predict whether a completion reaches the correct final answer; at test time sample (default) 100 completions, score each with verifier, return the highest-scoring completion (optionally majority-vote among top-k).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against finetuning baseline (single T=0 sample from finetuned model) and against varying numbers of samples / top-k voting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>6B verification (100 completions, verifier-ranked): substantially higher than 6B finetuning baseline and slightly outperforms finetuned 175B on the full dataset (paper states '6B verification slightly outperforms a finetuned 175B model' but does not give a single aggregate numeric percentage in-text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Finetuned 6B baseline (single sample): 20.6% (see other entry). 6B verification > finetuned 175B (implies large relative gain, described as ≈ equivalent to a 30x model size increase).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Reported qualitatively as roughly equivalent to a 30x model size increase (6B verification slightly outperforms finetuned 175B); exact percentage-point improvement depends on dataset size and model but is substantial versus finetuning baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Verification transforms reasoning into a simpler discriminative task: verifiers can more easily judge correctness than generators can reliably produce correct multi-step solutions; sampling many candidates plus ranking provides optionality and leverages the generator's coverage of solution space. Verifiers also scale more favorably with data than finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Generator trained 2 epochs to preserve diversity; verifier trained 1 epoch; training uses 100 sampled completions per training problem; verifier trained with MSE loss and optional joint language modeling objective; default test-time samples = 100; sampling temperature T=0.7 for generation; calculator annotations applied to control arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Verifiers to Solve Math Word Problems', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9441.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9441.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Token-level vs Solution-level verifier</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Verifier prediction granularity: per-token (token-level) versus final-token-only (solution-level)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation comparing verifiers trained to predict correctness at every token in the generated solution (token-level value function) versus only predicting correctness at the end of the solution (solution-level).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 family (verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B, 175B (experiments reported on both)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K math problems; metric is verifier-based solve rate when ranking sampled solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Token-level verifier: scalar prediction after every token (value function across solution); Solution-level verifier: single scalar prediction after final token only.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Token-level vs solution-level verifier training objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Token-level verifier ultimately outperforms solution-level verifier (quantitative curves shown in Figure 6a); token-level continues improving late in training while solution-level overfits earlier. Exact numeric percentages not listed in-text, but effect is consistent across model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Solution-level verifiers overfit more quickly and perform worse late in training; token-level gives higher final solve rates.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Token-level supervision provides a richer auxiliary signal (a full value function) that encourages judging reasoning throughout the solution rather than memorizing final-answer patterns, making token-level verifiers less prone to overfitting and more accurate.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Verifiers trained with joint LM+verification objectives in many experiments; token-level verifiers required larger batch size (4x) for stability in some runs; dropout applied in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Verifiers to Solve Math Word Problems', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9441.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9441.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Joint LM + verification objective</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Joint training of verifier with language modeling and verification objectives</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training verifiers both to perform language modeling (auxiliary objective) and to predict correctness yields better verifier performance than training only on the verification objective.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 family (verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B (primary ablations), also 175B referenced</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K math problems; metric is solve rate after verifier-ranked selection.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Verifier training mixes language modeling examples (upsampled) with verifier-labeled samples; joint loss = LM loss + verifier loss (unweighted); verifier also outputs scalar per-token via special token logits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared joint LM+verification objective versus verification-only objective.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Including the language modeling objective is a strict improvement over verification-only (paper states this as a clear empirical result); exact numeric improvement not tabulated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Verification-only performs worse than joint objective.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Better modeling of the solution language distribution helps the verifier discriminate between plausible correct and incorrect samples; the LM objective provides useful auxiliary signal.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Training mixes 1:1 language data and verifier data (which effectively upsamples language data by factor equal to completions count); verifier loss is MSE; epochs: generator 2, verifier 1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Verifiers to Solve Math Word Problems', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9441.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9441.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generator vs Verifier size tradeoff</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of generator (sampler) and verifier model sizes on verification performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation showing that increasing generator size has a larger positive impact on verification performance than increasing verifier size; a large generator with small verifier outperforms small generator with large verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 family (generator + verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B and 175B permutations (ablation varying sizes independently)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K math problems; metric is verifier-ranked solve rate.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Verification pipeline with separately sized generator and verifier models; generator produces N samples (default 100) which verifier scores.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Large generator + small verifier vs small generator + large verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>A larger generator yields larger gains than a larger verifier; even with a much smaller verifier relative to generator, verification remains effective. Quantitative curves in Figure 6c illustrate the trend; exact numeric points not provided inline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The verifier likely relies on coarse heuristics to discriminate among samples from a given generator, so generator coverage/diversity of solutions is the limiting factor; improving the generator yields more diverse/correct candidates for the verifier to pick from.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Ablation varied sizes in isolation; verification used 100 completions; other hyperparameters as in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Verifiers to Solve Math Word Problems', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9441.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9441.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Test-time completions (search size)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of number of sampled completions per problem at test time</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Varying the number of generator samples scored by the verifier affects solve rate: performance increases with more samples up to a point, then declines due to adversarial/fooling samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 family (generator + verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B (reported curve), also experiments on larger models implied</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K math problems; metric is verifier-selected solve rate as a function of number of sampled completions per problem.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>At test time sample K completions per problem (K varied across experiments); default K used in main experiments = 100; also examined up to 3200 in voting ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compare K ∈ {100, 400, 3200, ...} and observe optimal ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>For 6B verifier, performance improves as K increases up to ~400 completions, then degrades beyond that; default 100 samples captures most benefits. Exact solve rates vs K are plotted in Figure 7a but not tabulated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>100 samples (default) chosen as computationally reasonable with most benefit; using too many samples (>>400) can reduce accuracy due to adversarial solutions that fool the verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>More samples increase chance of including a correct solution but also increase chance of adversarial/fooling samples that the verifier misranks; there is a tradeoff and a sweet spot (empirically ~100–400).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Verifiers trained on 100 completions per training problem; test-time K varied independently; sampling temperature T=0.7 for generation; default reported experiments use 100 completions per test problem.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Verifiers to Solve Math Word Problems', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9441.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9441.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Top-k majority voting among verifier-ranked solutions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Majority voting over final answers of top-ranked verifier solutions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instead of choosing the single highest-scoring completion, take the final answer labels of the top-M verifier-ranked completions and output the majority-voted final answer; this can further improve accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 family (generator + verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B (primary ablations), with experiments at larger sample counts</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K math problems; metric is accuracy of majority-voted final answer among top-ranked completions.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Sample K completions, rank by verifier, choose top M (M <= K) and output the most common final numeric answer among them.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compare single-top selection (M=1) vs majority voting across M>1; also vary K (total sampled).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>With K=100, optimal M is around 3–5 (top 3–5 cast votes); with K=3200, optimal M around 30. Voting improves performance over single-top selection when enough diverse high-quality samples exist; exact numeric improvements depend on K.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Voting over multiple top-ranked samples yields additional gains compared to taking only top-1, particularly when large K is available.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Voting leverages ensemble-like robustness: if several top-scored completions share the correct final answer, majority voting reduces risk of picking a single high-scoring but incorrect sample.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Empirical sweep over K and M; default K=100, recommended M=3–5 for K=100; K up to 3200 examined with different optimal M.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Verifiers to Solve Math Word Problems', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9441.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9441.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dropout regularization (residual dropout)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Applying residual dropout during finetuning and verifier training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Injecting 20% residual dropout (not present in original GPT-3 pretraining) during additional pretraining and finetuning improves both finetuning and verification performance by mitigating overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 family (finetuned and verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B (main ablations), also applied in other sizes</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K math problems; metric is solve rate under finetuning baseline and verification methods.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Models are finetuned / verifiers trained with residual dropout applied along residual paths (20% dropout). Additional pretraining with dropout performed to avoid distribution shift since GPT-3 was pretrained without dropout.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>No-dropout baseline vs 20% dropout applied during pretraining/finetuning/verifier training; ablations include solution-level and token-level verifiers with/without dropout.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Dropout significantly improves finetuning baseline across dataset sizes; for solution-level verifiers dropout mitigates overfitting and brings performance close to token-level verifiers; token-level verifiers gain slightly from dropout. Exact percentage improvements plotted in Figure 8 but not tabulated in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Solution-level verifier without dropout performs worse and overfits; with dropout solution-level approaches token-level performance. Finetuning with dropout shows notable gains across training set sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Dropout regularizes models, reducing overconfidence and overfitting which otherwise hurt multi-sample coverage (test@100) and verifier generalization; because GPT-3 was not pretrained with dropout, additional pretraining with dropout is applied to reduce distribution shift.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Dropout rate used = 20%; verification experiments with dropout used default sampling/verification pipeline; batch size for token-level verifiers increased 4x in some dropout experiments to handle noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Verifiers to Solve Math Word Problems', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9441.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9441.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Training-epoch / coverage effect on multi-sample evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of generator finetuning epochs on sample diversity and test@N performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Longer finetuning of the generator increases test@1 performance but reduces coverage/diversity leading to degradation in test@100 performance due to overconfidence and collapsed sampling distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 family (generator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B (reported curve in Figure 3), 175B mentioned</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K; metrics: test@1 (single sample) and test@100 (any correct among 100 samples).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Generator is finetuned across epochs (e.g., up to 20 epochs). Test@1 uses low temperature T=0 single-sample evaluation; test@100 uses T=0.7 and samples 100 completions per problem.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Early (2 epochs) vs later (many epochs) finetuning of the generator.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Test@1 performance improves approximately monotonically with more epochs, but test@100 performance peaks within first few epochs and then degrades sharply with further epochs (overfitting/collapse). Specific numbers not tabulated in-text; paper uses 2 epochs for generator used to produce verifier training samples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Extended finetuning causes the generator to become overconfident and poorly calibrated, collapsing the diversity/coverage of the solution distribution which harms multi-sample search methods (verification) that rely on diverse candidate generation.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Generator used to produce samples for verifier training is finetuned for 2 epochs to maximize sample diversity; test@1 uses T=0, test@100 uses T=0.7; Figure 3 shows epoch-wise curves.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Verifiers to Solve Math Word Problems', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generate & rank: A multi-task framework for math word problems <em>(Rating: 2)</em></li>
                <li>Collaborative storytelling with large-scale neural language models <em>(Rating: 1)</em></li>
                <li>Measuring mathematical problem solving with the MATH dataset <em>(Rating: 1)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9441",
    "paper_id": "paper-d6045d2ccc9c09ca1671348de86d07da6bc28eea",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Finetuning (single low-temp sample)",
            "name_full": "Finetuning with single low-temperature autoregressive sampling",
            "brief_description": "Baseline approach where pretrained GPT-3 family models are finetuned on GSM8K and evaluated by taking a single low-temperature (T=0) autoregressive sample that produces a full natural-language solution and final answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 family (finetuned)",
            "model_size": "6B, 175B (reported experiments)",
            "task_name": "GSM8K",
            "task_description": "8.5K grade-school math word problems requiring 2–8 elementary arithmetic/algebra steps; test metric is solve rate (percentage of problems with correct final answer).",
            "presentation_format": "Finetuned model outputs full natural-language solution followed by final answer; sampling at test time with low temperature T=0 (argmax) and a single sample per problem (test@1).",
            "comparison_format": null,
            "performance": "6B finetuned (full natural-language solution, T=0, test@1): 20.6% solve rate (reported for 6B full-solution finetuned baseline); 175B finetuned achieves substantially higher solve rates (figure comparisons shown in paper but exact 175B numeric not listed in text).",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Generating full natural-language chain-of-thought style solutions before emitting the final answer substantially helps performance compared to emitting only a final answer; larger models (175B) outperform smaller ones under this format.",
            "null_or_negative_result": false,
            "experimental_details": "Finetuning for 20 epochs; sampling temperature T=0 for test@1; calculator annotations injected to make arithmetic reliable; evaluation uses single sample (test@1).",
            "uuid": "e9441.0",
            "source_info": {
                "paper_title": "Training Verifiers to Solve Math Word Problems",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Final-answer-only output",
            "name_full": "Finetuning to output only the final answer (no intermediate solution)",
            "brief_description": "Alternate prompt/output format where the model is trained/asked to produce only the final numeric answer instead of an intermediate natural-language solution; evaluated on GSM8K.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 family (finetuned)",
            "model_size": "6B",
            "task_name": "GSM8K",
            "task_description": "Same GSM8K grade-school math problems; metric is solve rate (correct final answer).",
            "presentation_format": "Model is finetuned and evaluated to directly output the final numeric answer without any intermediate reasoning steps.",
            "comparison_format": "Compared against full natural-language solution format (chain-of-thought style) with single low-temperature sample.",
            "performance": "Final-answer-only finetuned 6B: 5.2% solve rate (T=0, single sample).",
            "performance_comparison": "Full natural-language-solution finetuned 6B: 20.6% solve rate (T=0, single sample).",
            "format_effect_size": "-15.4 percentage points (absolute) / ~75% relative drop (20.6% -&gt; 5.2%).",
            "explanation_or_hypothesis": "Providing the intermediate natural-language solution greatly helps multi-step reasoning: removing steps forces the model to produce the final answer directly, causing a large performance drop, indicating the model benefits from producing and using intermediate computation/analysis.",
            "null_or_negative_result": true,
            "experimental_details": "6B model finetuned as baseline; both settings use same training data; sampling T=0 at test; explicit numeric comparison reported in-text.",
            "uuid": "e9441.1",
            "source_info": {
                "paper_title": "Training Verifiers to Solve Math Word Problems",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Verification (sample-and-rank)",
            "name_full": "Training verifiers and using sample-and-rank verification at test time",
            "brief_description": "Generate many high-temperature candidate solutions with a generator, then score and rank them with a separately trained verifier that predicts correctness; return the top-ranked solution (or majority vote among top-ranked answers).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 family (generator + verifier)",
            "model_size": "6B and 175B (experiments primarily reported on these sizes)",
            "task_name": "GSM8K",
            "task_description": "Same GSM8K grade-school math problems; metric is solve rate; evaluation considers test@N (percentage solved at least once among N samples) and verifier-ranked selection.",
            "presentation_format": "Generator finetuned for 2 epochs; sample 100 completions per problem at sampling temperature T=0.7; verifier (initialized from generator) trained to predict whether a completion reaches the correct final answer; at test time sample (default) 100 completions, score each with verifier, return the highest-scoring completion (optionally majority-vote among top-k).",
            "comparison_format": "Compared against finetuning baseline (single T=0 sample from finetuned model) and against varying numbers of samples / top-k voting strategies.",
            "performance": "6B verification (100 completions, verifier-ranked): substantially higher than 6B finetuning baseline and slightly outperforms finetuned 175B on the full dataset (paper states '6B verification slightly outperforms a finetuned 175B model' but does not give a single aggregate numeric percentage in-text).",
            "performance_comparison": "Finetuned 6B baseline (single sample): 20.6% (see other entry). 6B verification &gt; finetuned 175B (implies large relative gain, described as ≈ equivalent to a 30x model size increase).",
            "format_effect_size": "Reported qualitatively as roughly equivalent to a 30x model size increase (6B verification slightly outperforms finetuned 175B); exact percentage-point improvement depends on dataset size and model but is substantial versus finetuning baseline.",
            "explanation_or_hypothesis": "Verification transforms reasoning into a simpler discriminative task: verifiers can more easily judge correctness than generators can reliably produce correct multi-step solutions; sampling many candidates plus ranking provides optionality and leverages the generator's coverage of solution space. Verifiers also scale more favorably with data than finetuning.",
            "null_or_negative_result": false,
            "experimental_details": "Generator trained 2 epochs to preserve diversity; verifier trained 1 epoch; training uses 100 sampled completions per training problem; verifier trained with MSE loss and optional joint language modeling objective; default test-time samples = 100; sampling temperature T=0.7 for generation; calculator annotations applied to control arithmetic.",
            "uuid": "e9441.2",
            "source_info": {
                "paper_title": "Training Verifiers to Solve Math Word Problems",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Token-level vs Solution-level verifier",
            "name_full": "Verifier prediction granularity: per-token (token-level) versus final-token-only (solution-level)",
            "brief_description": "Ablation comparing verifiers trained to predict correctness at every token in the generated solution (token-level value function) versus only predicting correctness at the end of the solution (solution-level).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 family (verifier)",
            "model_size": "6B, 175B (experiments reported on both)",
            "task_name": "GSM8K",
            "task_description": "GSM8K math problems; metric is verifier-based solve rate when ranking sampled solutions.",
            "presentation_format": "Token-level verifier: scalar prediction after every token (value function across solution); Solution-level verifier: single scalar prediction after final token only.",
            "comparison_format": "Token-level vs solution-level verifier training objectives.",
            "performance": "Token-level verifier ultimately outperforms solution-level verifier (quantitative curves shown in Figure 6a); token-level continues improving late in training while solution-level overfits earlier. Exact numeric percentages not listed in-text, but effect is consistent across model sizes.",
            "performance_comparison": "Solution-level verifiers overfit more quickly and perform worse late in training; token-level gives higher final solve rates.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Token-level supervision provides a richer auxiliary signal (a full value function) that encourages judging reasoning throughout the solution rather than memorizing final-answer patterns, making token-level verifiers less prone to overfitting and more accurate.",
            "null_or_negative_result": false,
            "experimental_details": "Verifiers trained with joint LM+verification objectives in many experiments; token-level verifiers required larger batch size (4x) for stability in some runs; dropout applied in ablations.",
            "uuid": "e9441.3",
            "source_info": {
                "paper_title": "Training Verifiers to Solve Math Word Problems",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Joint LM + verification objective",
            "name_full": "Joint training of verifier with language modeling and verification objectives",
            "brief_description": "Training verifiers both to perform language modeling (auxiliary objective) and to predict correctness yields better verifier performance than training only on the verification objective.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 family (verifier)",
            "model_size": "6B (primary ablations), also 175B referenced",
            "task_name": "GSM8K",
            "task_description": "GSM8K math problems; metric is solve rate after verifier-ranked selection.",
            "presentation_format": "Verifier training mixes language modeling examples (upsampled) with verifier-labeled samples; joint loss = LM loss + verifier loss (unweighted); verifier also outputs scalar per-token via special token logits.",
            "comparison_format": "Compared joint LM+verification objective versus verification-only objective.",
            "performance": "Including the language modeling objective is a strict improvement over verification-only (paper states this as a clear empirical result); exact numeric improvement not tabulated in text.",
            "performance_comparison": "Verification-only performs worse than joint objective.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Better modeling of the solution language distribution helps the verifier discriminate between plausible correct and incorrect samples; the LM objective provides useful auxiliary signal.",
            "null_or_negative_result": false,
            "experimental_details": "Training mixes 1:1 language data and verifier data (which effectively upsamples language data by factor equal to completions count); verifier loss is MSE; epochs: generator 2, verifier 1.",
            "uuid": "e9441.4",
            "source_info": {
                "paper_title": "Training Verifiers to Solve Math Word Problems",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Generator vs Verifier size tradeoff",
            "name_full": "Effect of generator (sampler) and verifier model sizes on verification performance",
            "brief_description": "Ablation showing that increasing generator size has a larger positive impact on verification performance than increasing verifier size; a large generator with small verifier outperforms small generator with large verifier.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 family (generator + verifier)",
            "model_size": "6B and 175B permutations (ablation varying sizes independently)",
            "task_name": "GSM8K",
            "task_description": "GSM8K math problems; metric is verifier-ranked solve rate.",
            "presentation_format": "Verification pipeline with separately sized generator and verifier models; generator produces N samples (default 100) which verifier scores.",
            "comparison_format": "Large generator + small verifier vs small generator + large verifier.",
            "performance": "A larger generator yields larger gains than a larger verifier; even with a much smaller verifier relative to generator, verification remains effective. Quantitative curves in Figure 6c illustrate the trend; exact numeric points not provided inline.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "The verifier likely relies on coarse heuristics to discriminate among samples from a given generator, so generator coverage/diversity of solutions is the limiting factor; improving the generator yields more diverse/correct candidates for the verifier to pick from.",
            "null_or_negative_result": false,
            "experimental_details": "Ablation varied sizes in isolation; verification used 100 completions; other hyperparameters as in Table 1.",
            "uuid": "e9441.5",
            "source_info": {
                "paper_title": "Training Verifiers to Solve Math Word Problems",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Test-time completions (search size)",
            "name_full": "Effect of number of sampled completions per problem at test time",
            "brief_description": "Varying the number of generator samples scored by the verifier affects solve rate: performance increases with more samples up to a point, then declines due to adversarial/fooling samples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 family (generator + verifier)",
            "model_size": "6B (reported curve), also experiments on larger models implied",
            "task_name": "GSM8K",
            "task_description": "GSM8K math problems; metric is verifier-selected solve rate as a function of number of sampled completions per problem.",
            "presentation_format": "At test time sample K completions per problem (K varied across experiments); default K used in main experiments = 100; also examined up to 3200 in voting ablations.",
            "comparison_format": "Compare K ∈ {100, 400, 3200, ...} and observe optimal ranges.",
            "performance": "For 6B verifier, performance improves as K increases up to ~400 completions, then degrades beyond that; default 100 samples captures most benefits. Exact solve rates vs K are plotted in Figure 7a but not tabulated in text.",
            "performance_comparison": "100 samples (default) chosen as computationally reasonable with most benefit; using too many samples (&gt;&gt;400) can reduce accuracy due to adversarial solutions that fool the verifier.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "More samples increase chance of including a correct solution but also increase chance of adversarial/fooling samples that the verifier misranks; there is a tradeoff and a sweet spot (empirically ~100–400).",
            "null_or_negative_result": true,
            "experimental_details": "Verifiers trained on 100 completions per training problem; test-time K varied independently; sampling temperature T=0.7 for generation; default reported experiments use 100 completions per test problem.",
            "uuid": "e9441.6",
            "source_info": {
                "paper_title": "Training Verifiers to Solve Math Word Problems",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Top-k majority voting among verifier-ranked solutions",
            "name_full": "Majority voting over final answers of top-ranked verifier solutions",
            "brief_description": "Instead of choosing the single highest-scoring completion, take the final answer labels of the top-M verifier-ranked completions and output the majority-voted final answer; this can further improve accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 family (generator + verifier)",
            "model_size": "6B (primary ablations), with experiments at larger sample counts",
            "task_name": "GSM8K",
            "task_description": "GSM8K math problems; metric is accuracy of majority-voted final answer among top-ranked completions.",
            "presentation_format": "Sample K completions, rank by verifier, choose top M (M &lt;= K) and output the most common final numeric answer among them.",
            "comparison_format": "Compare single-top selection (M=1) vs majority voting across M&gt;1; also vary K (total sampled).",
            "performance": "With K=100, optimal M is around 3–5 (top 3–5 cast votes); with K=3200, optimal M around 30. Voting improves performance over single-top selection when enough diverse high-quality samples exist; exact numeric improvements depend on K.",
            "performance_comparison": "Voting over multiple top-ranked samples yields additional gains compared to taking only top-1, particularly when large K is available.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Voting leverages ensemble-like robustness: if several top-scored completions share the correct final answer, majority voting reduces risk of picking a single high-scoring but incorrect sample.",
            "null_or_negative_result": false,
            "experimental_details": "Empirical sweep over K and M; default K=100, recommended M=3–5 for K=100; K up to 3200 examined with different optimal M.",
            "uuid": "e9441.7",
            "source_info": {
                "paper_title": "Training Verifiers to Solve Math Word Problems",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Dropout regularization (residual dropout)",
            "name_full": "Applying residual dropout during finetuning and verifier training",
            "brief_description": "Injecting 20% residual dropout (not present in original GPT-3 pretraining) during additional pretraining and finetuning improves both finetuning and verification performance by mitigating overfitting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 family (finetuned and verifier)",
            "model_size": "6B (main ablations), also applied in other sizes",
            "task_name": "GSM8K",
            "task_description": "GSM8K math problems; metric is solve rate under finetuning baseline and verification methods.",
            "presentation_format": "Models are finetuned / verifiers trained with residual dropout applied along residual paths (20% dropout). Additional pretraining with dropout performed to avoid distribution shift since GPT-3 was pretrained without dropout.",
            "comparison_format": "No-dropout baseline vs 20% dropout applied during pretraining/finetuning/verifier training; ablations include solution-level and token-level verifiers with/without dropout.",
            "performance": "Dropout significantly improves finetuning baseline across dataset sizes; for solution-level verifiers dropout mitigates overfitting and brings performance close to token-level verifiers; token-level verifiers gain slightly from dropout. Exact percentage improvements plotted in Figure 8 but not tabulated in-text.",
            "performance_comparison": "Solution-level verifier without dropout performs worse and overfits; with dropout solution-level approaches token-level performance. Finetuning with dropout shows notable gains across training set sizes.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Dropout regularizes models, reducing overconfidence and overfitting which otherwise hurt multi-sample coverage (test@100) and verifier generalization; because GPT-3 was not pretrained with dropout, additional pretraining with dropout is applied to reduce distribution shift.",
            "null_or_negative_result": false,
            "experimental_details": "Dropout rate used = 20%; verification experiments with dropout used default sampling/verification pipeline; batch size for token-level verifiers increased 4x in some dropout experiments to handle noise.",
            "uuid": "e9441.8",
            "source_info": {
                "paper_title": "Training Verifiers to Solve Math Word Problems",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Training-epoch / coverage effect on multi-sample evaluation",
            "name_full": "Effect of generator finetuning epochs on sample diversity and test@N performance",
            "brief_description": "Longer finetuning of the generator increases test@1 performance but reduces coverage/diversity leading to degradation in test@100 performance due to overconfidence and collapsed sampling distribution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 family (generator)",
            "model_size": "6B (reported curve in Figure 3), 175B mentioned",
            "task_name": "GSM8K",
            "task_description": "GSM8K; metrics: test@1 (single sample) and test@100 (any correct among 100 samples).",
            "presentation_format": "Generator is finetuned across epochs (e.g., up to 20 epochs). Test@1 uses low temperature T=0 single-sample evaluation; test@100 uses T=0.7 and samples 100 completions per problem.",
            "comparison_format": "Early (2 epochs) vs later (many epochs) finetuning of the generator.",
            "performance": "Test@1 performance improves approximately monotonically with more epochs, but test@100 performance peaks within first few epochs and then degrades sharply with further epochs (overfitting/collapse). Specific numbers not tabulated in-text; paper uses 2 epochs for generator used to produce verifier training samples.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Extended finetuning causes the generator to become overconfident and poorly calibrated, collapsing the diversity/coverage of the solution distribution which harms multi-sample search methods (verification) that rely on diverse candidate generation.",
            "null_or_negative_result": true,
            "experimental_details": "Generator used to produce samples for verifier training is finetuned for 2 epochs to maximize sample diversity; test@1 uses T=0, test@100 uses T=0.7; Figure 3 shows epoch-wise curves.",
            "uuid": "e9441.9",
            "source_info": {
                "paper_title": "Training Verifiers to Solve Math Word Problems",
                "publication_date_yy_mm": "2021-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generate & rank: A multi-task framework for math word problems",
            "rating": 2
        },
        {
            "paper_title": "Collaborative storytelling with large-scale neural language models",
            "rating": 1
        },
        {
            "paper_title": "Measuring mathematical problem solving with the MATH dataset",
            "rating": 1
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        }
    ],
    "cost": 0.0151715,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Training Verifiers to Solve Math Word Problems</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Karl Cobbe*</th>
<th style="text-align: center;">Vineet Kosaraju*</th>
<th style="text-align: center;">Mohammad Bavarian</th>
<th style="text-align: center;">Mark Chen</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Heewoo Jun</td>
<td style="text-align: center;">Lukasz Kaiser</td>
<td style="text-align: center;">Matthias Plappert</td>
<td style="text-align: center;">Jerry Tworek</td>
</tr>
<tr>
<td style="text-align: left;">Jacob Hilton</td>
<td style="text-align: center;">Reiichiro Nakano</td>
<td style="text-align: center;">Christopher Hesse</td>
<td style="text-align: center;">John Schulman</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h2>Abstract</h2>
<p>State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5 K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.</p>
<h2>1 Introduction</h2>
<p>In recent years, large language models have demonstrated impressive skills across many diverse tasks (Wang et al., 2019; Brown et al., 2020). Kaplan et al. (2020) describe the consistent benefits of increasing model size, characterizing scaling trends that hold across many orders of magnitude. However, even the largest models falter when required to perform multi-step mathematical reasoning (Hendrycks et al., 2021). Model samples frequently contain catastrophic mistakes, even after the model has been appropriately finetuned. Mathematical reasoning thus reveals a critical weakness in modern language models.</p>
<p>One significant challenge in mathematical reasoning is the high sensitivity to individual mistakes (Shen et al., 2021a). When generating a solution, autoregressive models have no mechanism to correct their own errors. Solutions that veer off-course quickly become unrecoverable. If we rely purely on generative methods and extrapolate from current trends, we will require an exorbitant</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Problem: Both bakes 4, 2 dozen batches of cookies in a week. If these cookies are shared amongst 16 people equally, how many cookies does each person consume?
Solution: Both bakes 4.2 dozen batches of cookies for a total of $4 * 2==4 * 2=6==8$ dozen cookies
There are 12 cookies in a dozen and she makes 8 dozen cookies for a total of $12 * 8==12 * 3=95==96$ cookies
She splits the 96 cookies equally amongst 16 people so they each eat $96 / 16==160 / 18=6==8$ cookies
Final Answer: 6
Problem: Mrs. Lim milks her cows twice a day. Yesterday morning, she got 68 gallons of milk and in the evening, she got 82 gallons. This morning, she got 18 gallons fewer than she had yesterday morning. After selling some gallons of milk in the afternoon, Mrs. Lim has only 24 gallons left. How much was her revenue for the milk if each gallon costs $\$ 3.50$ ?
Mrs. Lim got 68 gallons - 18 gallons $==68-18=50==50$ gallons this morning.
So she was able to get a total of 68 gallons $+82$ gallons $+50$ gallons $==68=82+50=200==200$ gallons.
She was able to sell 200 gallons - 24 gallons $==200-24=176==176$ gallons.
Thus, her total revenue for the milk is $\$ 3.50 /$ gallon x 176 gallons $=\$==3.50 * 176=616==616$.
Final Answer: 616
Problem: Tina buys 312 -packs of soda for a party. Including Tina, 6 people are at the party. Half of the people at the party have 3 sodas each, 2 of the people have 4 , and 1 person has 5 . How many sodas are left over when the party is over?
Solution: Tina buys 312 -packs of soda, for $3 * 12==3 * 12=36==36$ sodas
6 people attend the party, so half of them is $622==622+3==3$ people
Each of those people drinks 3 sodas, so they drink $3 * 3==3 * 3=3==9$ sodas
Two people drink 4 sodas, which means they drink $2 * 4==4 * 2=6==8$ sodas
With one person drinking 5 , that brings the total drank to $5=9=8+3==10+9=8+3=25==25$ sodas
As Tina started off with 36 sodas, that means there are $36-25==36-25=11==11$ sodas left
Final Answer: 11</p>
<p>Figure 1: Three example problems from GSM8K. Calculation annotations are highlighted in red.
parameter count to achieve even moderate performance on distributions as challenging as the MATH dataset (Hendrycks et al., 2021). This evidence strongly motivates the search for methods with more favorable scaling laws.</p>
<p>We propose training verifiers to evaluate the correctness of model generated solutions, similar to concurrent work by Shen et al. (2021a). At test time, we sample a fixed number of candidate solutions and select the solution ranked highest by the verifier. Verifiers benefit both from their inherent optionality and from verification being a simpler task than generation in general.</p>
<p>To facilitate research, we are releasing GSM8K, a dataset of 8.5 K high quality problems at the grade school math level. We designed this dataset to have high linguistic diversity while relying on relatively simple grade school math concepts. State-of-the-art language models struggle to achieve high performance on this dataset, primarily due to the high diversity among problems. At the same time, GSM8K solutions depend only on elementary concepts, so achieving high test performance is a tractable goal.</p>
<p>Our main contributions are as follows:</p>
<ol>
<li>We present a curated dataset of 8.5 K grade school math questions and natural language solutions, useful for probing the informal reasoning ability of large language models.</li>
<li>We show that, compared to a finetuning baseline, the use of verifiers results in approximately the same performance boost as a 30x model size increase, and that verifiers scale significantly better with increased data.</li>
<li>We show that dropout acts as a strong regularizer, significantly improving performance for both finetuning and verification.</li>
</ol>
<h1>2 Dataset</h1>
<p>GSM8K consists of 8.5 K high quality grade school math problems created by human problem writers. We segmented these into 7.5 K training problems and 1 K test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations $(+-\times \div)$ to reach the final answer. A bright middle school student should be able to solve every problem.</p>
<p>We created GSM8K based on the following design principles.</p>
<ul>
<li>High Quality We avoid error-prone scraping procedures and instead rely on human workers to create problems. After performing extensive quality control based on workers' answer agreement, we estimate that less than 2 percent of problems contain breaking errors.</li>
<li>High Diversity We strive for high diversity among problems. We actively avoid designing problems that are drawn from the same linguistic template or differ only in superficial details, an issue that is prevalent among many other datasets. By creating each individual problem to be relatively unique, held-out test performance becomes a far more relevant metric.</li>
<li>Moderate Difficulty We choose a problem distribution that is challenging for large state-of-the-art language models, without being completely intractable. GSM8K will help us better understand the data scaling trends of different models and methods in this difficulty sweet spot. Problems require no concepts beyond the level of early Algebra, and the vast majority of problems can be solved without explicitly defining a variable.</li>
<li>Natural Language Solutions We collect solutions in natural language rather than as pure math expressions. We believe this is the most generally useful data format, and we expect it to shed light on the properties of large language models' internal monologues. We instructed problem writers to explain their work as much as possible, but we allowed them to write solutions in their own diverse linguistic styles.</li>
</ul>
<p>The full GSM8K dataset can be found at https://github.com/openai/grade-school-math. Example problems are shown in Figure 1, and we discuss additional dataset details in Appendix A.</p>
<h2>3 Related Work</h2>
<h3>3.1 Related Datasets</h3>
<p>Early math word problem datasets (Kushman et al., 2014; Roy and Roth, 2015) are relatively small and are not well suited for testing the limits of modern language models. Dolphin18K (Huang et al., 2016) is a larger dataset containing</p>
<p>18 K problems, but solutions are provided only in the form of equations or final answers. AQuA-RAT (Ling et al., 2017) contains 100K problems, but this dataset unfortunately suffers from both a high degree of problem templatization and poor quality control of the natural language solutions. MathQA is a recently released subset of AQuA-RAT focused on correcting these mistakes (Amini et al., 2019), but even the corrected dataset has data quality issues, with around $30 \%$ of the data having inconsistencies (Miao et al., 2021). Ape210K (Zhao et al., 2020) is the largest publicly available dataset, consisting of 210 K Chinese elementary school-level math problems. However, due to the language barrier and the lack of natural language solutions, we're unable to evaluate our methods on this dataset.</p>
<p>The recently developed ASDiv dataset (Miao et al., 2021), which contains 2.3 K math word problems, addresses common flaws in prior datasets by ensuring problems have both high diversity and high quality. We share those design principles in the creation of GSM8K. However, we note that GSM8K is larger, provides natural language solutions, and consists of problems that on average require more steps to solve. The MATH dataset (Hendrycks et al., 2021) is larger and significantly more complex than GSM8K, but the high difficulty makes it challenging to accurately measure progress given the current capabilities of state-of-the-art language models.</p>
<p>Other recent reasoning-related datasets have focused on mathematical reasoning on symbolic math (Lample and Charton, 2019), reading comprehension (LogiQA) (Liu et al., 2020), and commonsense question answering (CommonsenseQA) (Talmor et al., 2018). Similar to CommonsenseQA, GSM8K includes questions that require basic background knowledge, like the number of days in a week. Similar to LogiQA, which requires a mix of reading comprehension and logical reasoning, GSM8K's main difficulty lies in both properly interpreting a question and reasoning through the steps to solve it.</p>
<h1>3.2 Related Methods</h1>
<p>Previous work has attempted to solve classic math word problem benchmarks with recurrent seq2seq models (Sutskever et al., 2014) and closely related variants (Wang et al., 2017; Huang et al., 2018). More recent work has improved performance by designing specialized encoder-decoder architectures (Amini et al., 2019; Chiang and Chen, 2018; Xie and Sun, 2019; Chen et al., 2020; Li et al., 2020), with the strongest results often relying on large pretrained encoders from the BERT family (Chen et al., 2019; Kim et al., 2020; Liang et al., 2021).</p>
<p>Other recent work has recommended additional pretraining tasks to further improve the math reasoning skills of large transformer-based models. Hendrycks et al. (2021) propose pretraining models on a new AMPS corpus, derived from Khan Academy problems and Mathematica scripts. Similarly, Shen et al. (2021b) propose a pretrained a corpus of pre-K to college level curricula extracted from the internet, and Peng et al. (2021) propose pretraining by predicting masked subexpressions from expression trees.</p>
<p>Similar to verification, other methods have finetuned a language model to</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Final test performance for various GPT-3 model sizes after finetuning on training sets of different sizes. Mean and standard deviation is shown across 3 runs.</p>
<p>Select among many model completions. Nichols et al. (2020) proposed a sample-and-rank approach to improve the collaborative storytelling ability of large language models, with the training signal coming from the preferences of human workers. In concurrent work closely related to our own, Shen et al. (2021a) applied a similar approach to solving math word problems, jointly training a model to both generate and rank solutions. Our work shares many fundamental similarities with their approach, though we differ in several key respects. First, we focus attention on the space of natural language solutions, as this is a richer and more general solution format than pure mathematical expressions. Moreover, this choice enables our models to develop verbal analytical skills and to produce solutions that are more readily interpretable by humans. Second, we provide evidence that verifiers scale far more favorably with additional data than baseline methods. Finally, we use separate generator and verifier networks, in order to prevent the generator from overfitting.</p>
<h2>4 Methods</h2>
<p>We investigate two methods to solve problems in GSM8K: finetuning and verification. Finetuning, our baseline method, uses the same language modeling objective as the generative pretraining in GPT-3 (Brown et al., 2020). At test time, we judge performance by autoregressively sampling a single low temperature solution and checking whether the final answer is correct. In contrast, verification consists of sampling multiple high temperature solutions, assigning each solution a score, and outputting the highest ranked solution. Verifiers are trained to judge the correctness of solutions, with the training signal determined solely by whether or not the solution reached the correct final answer.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Test solve rate after finetuning a 6B model on the full GSM8K training set, when the model is allowed to make 1 guess (left) or 100 guesses (right).</p>
<p>For both methods, we use models from the GPT-3 family as our initialization, primarily focusing on the 175B and 6B model sizes. The 175B model is the largest and produces the most impressive results, while the 6B model is significantly more convenient for research purposes. We discuss hyperparameter choices in Appendix B.</p>
<p>Our models frequently fail to accurately perform calculations. Although larger models make fewer arithmetic mistakes than smaller models, this remains a common source of errors. To mitigate this issue, we train all models to use a calculator by injecting calculation annotations into the training set. At test time, a calculator will override sampling when the model chooses to use these annotations. Details can be found in Appendix C.</p>
<h3>4.1 Finetuning</h3>
<p>We perform finetuning by updating model parameters to minimize the cross-entropy loss over all training tokens. Figure 2 shows test performance after finetuning on training sets of varying sizes for 20 epochs. We visualize the same data both as a function of training set size and as a function of model size. Test performance is determined by a single low temperature (T = 0) sample for each test problem. Unsurprisingly, we see that the 175B model significantly outperforms the smaller models. Assuming a log-linear trend, we can naively extrapolate these results to estimate that a model with 10<sup>16</sup> parameters would be required to reach an 80% solve rate, when using the full GSM8K training set. It is even harder to extrapolate along the data dimension, since performance does not appear to follow a log-linear trend. Nevertheless, it appears likely that the 175B model would require at least two additional orders of magnitude of training data to reach an 80% solve rate.</p>
<p>In Figure 3, we show how 6B test performance varies over the course of 100</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: A diagram of the verification training pipeline.
training epochs. We use test@N to denote the percentage of problems solved correctly at least once when allowing the model to make N separate guesses for each problem. We use a low temperature $(T=0)$ to generate test@1 samples and we use a higher temperature $(T=0.7)$ to generate test@100 samples. Both temperature values were chosen empirically to produce the best results. Test@1 performance improves approximately monotonically, even though we quickly begin overfitting on test loss. Unfortunately, test@100 performance degrades much more sharply than test@1 as we increase the number of epochs. This is to be expected: as the model repeatedly encounters the same data, it becomes increasingly uncalibrated and overconfident in its predictions. At test time, this overconfidence leads to poor coverage of the solution space, an effect which only becomes noticeable when we are considering multiple samples at test time.</p>
<p>Choosing a model with good coverage is critical to successfully train verifiers. Empirically, we see that test@100 performance peaks within the first few epochs. For this reason, we use models trained for 2 epochs to generate samples for training verifiers. We provide several example solutions from 6B and 175B models in Appendix D. We also note that it is important to allow the model to generate the full natural language solution before outputting a final answer. If we instead finetune a 6 B model to directly output the final answer without any intermediate steps, performance drops drastically from $20.6 \%$ to $5.2 \%$.</p>
<h1>4.2 Verification</h1>
<p>To improve upon the finetuning baseline, we train verifiers to judge the correctness of model-generated solutions and search against these verifiers at test time. Conditioned on the problem and a candidate solution, the verifier outputs the probability that the solution is correct. Training solutions are labeled as correct or incorrect based solely on whether they reach the correct final answer. In practice, some solutions will reach the correct final answer using flawed reasoning, leading to false positives.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: A comparison between finetuning and verification using 6B and 175B model sizes. Verification considers 100 solutions per problem. Mean and standard deviation is shown across 3 runs, except for 175B verification which shows only a single run.</p>
<p>As shown in Figure 4, we train the verifier as follows:</p>
<ol>
<li>Finetune a model (the "generator") for 2 epochs on the training set.</li>
<li>Sample 100 completions from the generator for each training problem and label each solution as correct or incorrect.</li>
<li>Train a verifier for a single epoch on this dataset.</li>
</ol>
<p>Training for 2 epochs is enough for the generator to learn basic skills in this domain. We choose not to train for longer, since the diversity of generated solutions begins to collapse after this point, as shown in Figure 3. We train separate generator and verifier models to limit the generator's training and prevent overfitting, but in principle, it should be possible to combine these models. Unless otherwise specified, we use the same model size for the generator and the verifier. In addition to predicting solution correctness, we also train the verifier with the same language modeling objective as the generator. This serves as a valuable auxiliary objective for the verifier. We discuss additional verifier training details in Appendix E.</p>
<p>At test time, we sample 100 completions to each test problem, rank them with the verifier, and then return the one with the highest verifier score. A comparison between verification and finetuning is shown in Figure 5 for both the 6B and 175B model sizes. We find that it is not beneficial to use verification at low dataset sizes. We believe this is due to the pressure to overfit to the correct answer: with small datasets, overfitting to the correct answer happens faster than learning more generalizable properties of correct reasoning. However, once we use a sufficiently large dataset, we see a strong boost from verifiers.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" />
(a) Comparison between a a verifier trained to predict correctness after every token (token-level) and one trained to predict correctness after only the final token (solution-level)
(b) Comparison between a verifier trained jointly to predict correctness and perform language modeling (joint) and one trained only to predict correctness (verification-only)
(c) Performance when varying the size of the generator and the verifier in isolation. Increasing the size of the generator has a larger impact than increasing the size of the verifier.</p>
<p>Figure 6: Verification ablations</p>
<p>It's interesting to note that the 175B verifiers "take off" earlier than the 6B verifiers, requiring fewer training problems to surpass the finetuning baseline. See Appendix D for example solutions found by verifiers and Appendix F for a visualization of verifier confidence.</p>
<h1>4.3 Verification Ablations</h1>
<p>We can either train verifiers to make a single scalar prediction conditioned on the entire generated solution, or to make a scalar prediction after each token in the solution. By default, we choose the latter, training verifiers to make predictions after each token. This can be viewed as a token-level value function. We compare these two methods in Figure 6a, respectively labeled "solutionlevel" and "token-level".</p>
<p>Predicting the value function at every token is a more challenging and noisier task than judging only the full completion. However, despite the initially slower training, the token-level verifier ultimately outperforms the solution-level verifier. Moreover, the token-level verifier is still improving late in training, whereas the solution-level verifier quickly shows signs of overfitting. We hypothesize that the full value function provides a useful auxiliary signal that encourages the model to judge the reasoning throughout solutions, rather than merely memorizing the correct final answer.</p>
<p>In Figure 6b, we ablate the objective used when training verifiers. As discussed in Section 4.2, we can optionally include a language modeling objective alongside the verification objective. We compare using both objectives to using only the verification objective. Although both are reasonable choices, including the language modeling objective is a strict improvement. This makes intuitive</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Performance as the amount of test time compute varies.
sense: better understanding this language distribution should only aid the verifier in discriminating between samples.</p>
<p>In Figure 6c, we separately ablate the model size of the generator and the verifier. We find that using a large generator with a small verifier performs significantly better than using a small generator with a large verifier. Verification is still remarkably effective, even when the verifier is much smaller than the generator. This suggests that the verifier may often be relying on relatively coarse heuristics to discriminate between solutions from a given generator, rather than attempting a more thorough form of verification.</p>
<h1>5 Additional Experiments</h1>
<h3>5.1 Test Time Compute</h3>
<p>At test time, we can choose to generate arbitrarily many solutions to be judged by the verifier before selecting the highest ranked completion. Figure 7a shows how 6B verifier performance varies with the number of completions per test problem. At this scale, performance improves as we increase the number of completions up to 400 . Beyond this point, performance start to decrease. This suggests that the benefits of search are eventually outweighed by the risk of finding adversarial solutions that fool the verifier. In general, we evaluate verifier test performance using 100 completions, since this captures most of the benefits of verification with a relatively modest compute cost.</p>
<p>To further increase performance, we can take a majority vote among the top verifier-ranked solutions instead of selecting only the single top solution.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: 6B finetuning and verification dropout ablations.</p>
<p>This voting process considers only the final answer reached by the individual solutions: the final answer selected is the one with the most votes. Figure 7b shows how performance varies as we allow a greater number of top samples to cast a vote. Unsurprisingly, when starting with a greater number of samples, we can afford to allow a greater number of samples to cast a vote. When we have only 100 samples, it is optimal to allow only the top 3-5 samples to cast a vote. When we have 3200 samples, it is approximately optimal to allow the top 30 to cast a vote.</p>
<h1>5.2 Regularization</h1>
<p>We find that both finetuning and verification strongly benefit from the use of dropout as a regularizer. Specifically, we apply residual dropout (Vaswani et al., 2017) along the residual paths of each layer in the network. We use $20 \%$ dropout for all dropout experiments, chosen based on the results of a hyperparameters sweep. We note that GPT-3 models are not pretrained with dropout. For experiments involving dropout, we therefore perform additional pretraining with dropout before subsequently finetuning the models. This mitigates the distribution shift the model experiences during finetuning.</p>
<p>We first investigate the effect of dropout on finetuning across various training set sizes. Figure 8a shows that dropout leads to a significant improvement over baseline. We next investigate the effect of dropout on verifiers, considering both the solution-level and token-level variants. In Figure 8b, we see that dropout significantly improves solution-level verifiers, mitigating the overfitting that occurs in the unregularized baseline. Notably, using dropout with solutionlevel verifiers reaches a similar level of performance as token-level verifiers. In Figure 8c, we apply dropout to token-level verifiers. Since token-level verifiers are already less susceptible to overfitting, it is no surprise that the impact of dropout is less significant. Nevertheless, we do still see a slight gain from training token-level verifiers with dropout. Note that we increase the batch size for token-level verifiers by a factor of 4 , to better handle the more difficult objective and the noise from dropout.</p>
<h1>6 Conclusion</h1>
<p>We have seen that verification provides a significant performance boost relative to a finetuning baseline. On the full dataset, 6B verification slightly outperforms a finetuned 175B model, thereby offering a boost approximately equivalent to a 30x model size increase. We have also seen that token-level verifiers are less prone to overfitting than solution-level verifiers, and that all methods benefit from regularization with residual dropout. We expect verification to scale well to problem distributions that require more complex mathematical reasoning, and we hope GSM8K supports the development of new methods that scale even better.</p>
<h2>Acknowledgements</h2>
<p>We thank Dan Hendrycks, Leo Gao, Alec Radford, and Giambattista Parascandolo for their valuable feedback on this paper; Harri Edwards, Yura Burda, Michael Wu, and Nick Ryder for many insightful conversations; Michael Petrov, Alethea Power, and Jacob Jackson for their technical assistance; the OpenAI Supercomputing team for the infrastructure that made these experiments possible; and the team at Surge AI for performing the GSM8K data collection.</p>
<h2>References</h2>
<p>A. Amini, S. Gabriel, P. Lin, R. Koncel-Kedziorski, Y. Choi, and H. Hajishirzi. Mathqa: Towards interpretable math word problem solving with operationbased formalisms. arXiv preprint arXiv:1905.13319, 2019.
T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
K. Chen, Q. Huang, H. Palangi, P. Smolensky, K. D. Forbus, and J. Gao. Mapping natural-language problems to formal-language solutions using structured neural representations. In ICML, 2020.
X. Chen, C. Liang, A. W. Yu, D. Zhou, D. Song, and Q. V. Le. Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension. In International Conference on Learning Representations, 2019.
T.-R. Chiang and Y.-N. Chen. Semantically-aligned equation generation for solving and reasoning math word problems. arXiv preprint arXiv:1811.00720, 2018.
D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>D. Huang, S. Shi, C.-Y. Lin, J. Yin, and W.-Y. Ma. How well do computers solve math word problems? large-scale dataset construction and evaluation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 887-896, 2016.
D. Huang, J. Liu, C.-Y. Lin, and J. Yin. Neural math word problem solver with reinforcement learning. In Proceedings of the 27th International Conference on Computational Linguistics, pages 213-223, 2018.
J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
B. Kim, K. S. Ki, D. Lee, and G. Gweon. Point to the expression: Solving algebraic word problems using the expression-pointer transformer model. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3768-3779, 2020.
N. Kushman, Y. Artzi, L. Zettlemoyer, and R. Barzilay. Learning to automatically solve algebra word problems. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages $271-281,2014$.
G. Lample and F. Charton. Deep learning for symbolic mathematics. arXiv preprint arXiv:1912.01412, 2019.
S. Li, L. Wu, S. Feng, F. Xu, F. Xu, and S. Zhong. Graph-to-tree neural networks for learning structured input-output translation with applications to semantic parsing and math word problem. EMNLP, 2020.
Z. Liang, J. Zhang, J. Shao, and X. Zhang. Mwp-bert: A strong baseline for math word problems, 072021.
W. Ling, D. Yogatama, C. Dyer, and P. Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146, 2017.
J. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, and Y. Zhang. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. In IJCAI, 2020 .
S.-Y. Miao, C.-C. Liang, and K.-Y. Su. A diverse corpus for evaluating and developing english math word problem solvers. arXiv preprint arXiv:2106.15772, 2021.
E. Nichols, L. Gao, and R. Gomez. Collaborative storytelling with large-scale neural language models. arXiv preprint arXiv:2011.10208, 2020.
S. Peng, K. Yuan, L. Gao, and Z. Tang. Mathbert: A pre-trained model for mathematical formula understanding. ArXiv, abs/2105.00377, 2021.</p>
<p>S. Roy and D. Roth. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1743-1752, Lisbon, Portugal, Sept. 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1202. URL https://aclanthology.org/D15-1202.
J. Shen, Y. Yin, L. Li, L. Shang, X. Jiang, M. Zhang, and Q. Liu. Generate \&amp; rank: A multi-task framework for math word problems. arXiv preprint arXiv:2109.03034, 2021a.
J. T. Shen, M. Yamashita, E. Prihar, N. Heffernan, X. Wu, B. Graff, and D. Lee. Mathbert: A pre-trained language model for general nlp tasks in mathematics education, 08 2021b.
I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages $3104-3112,2014$.
A. Talmor, J. Herzig, N. Lourie, and J. Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017.
A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. arXiv preprint arXiv:1905.00537, 2019.
Y. Wang, X. Liu, and S. Shi. Deep neural solver for math word problems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 845-854, Copenhagen, Denmark, Sept. 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1088. URL https://aclanthology.org/D17-1088.
Z. Xie and S. Sun. A goal-driven tree-structured neural model for math word problems. In IJCAI, 2019.
W. Zhao, M. Shang, Y. Liu, L. Wang, and J. Liu. Ape210k: A largescale and template-rich dataset of math word problems. arXiv preprint arXiv:2009.11506, 2020.</p>
<h1>A Dataset Details</h1>
<p>We initially collected a starting set of a thousand problems and natural language solutions by hiring freelance contractors on Upwork (upwork.com). We then worked with Surge AI (surgehq.ai), an NLP data labeling platform, to scale up our data collection. After collecting the full dataset, we asked workers to re-solve all problems, with no workers re-solving problems they originally wrote. We checked whether their final answers agreed with the original solutions, and any problems that produced disagreements were either repaired or discarded. We then performed another round of agreement checks on a smaller subset of problems, finding that $1.7 \%$ of problems still produce disagreements among contractors. We estimate this to be the fraction of problems that contain breaking errors or ambiguities. It is possible that a larger percentage of problems contain subtle errors.</p>
<p>To assist contractors with writing questions, we provided seed questions automatically generated from a few-shot prompted 175B GPT-3 model. Contractors were allowed to use those seed questions directly, to use them as inspiration and make modifications, or to come up with their own questions entirely. We instructed contractors to be as descriptive as possible in their solutions, and to not re-use problem settings or templates between different questions. To ensure contractors were not re-using problem templates, we computed pairwise similarity scores between problems and used this to provide feedback to contractors.</p>
<h1>B Hyperparameters</h1>
<p>We include a table of important hyperparameters below. We performed sweeps of the learning rate and batch size by an order of magnitude in both directions from the values in the table and were unable to find any significant improvements. Other reasonable choices for both the verifier temperature (eg: 1.0 instead of 0.7 ) and objective (cross-entropy instead of mean squared error) also had negligible effect in our ablations.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">General Hyperparameters</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Batch Size</td>
<td style="text-align: left;">$3.2 \times 10^{4}$ tokens</td>
</tr>
<tr>
<td style="text-align: left;">Max Sample Length</td>
<td style="text-align: left;">400 tokens</td>
</tr>
<tr>
<td style="text-align: left;">Tokenization</td>
<td style="text-align: left;">reversible_50000</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: left;">Adam, $\beta_{1}=0.9, \beta_{2}=0.95$</td>
</tr>
<tr>
<td style="text-align: left;">Dropout</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate Schedule</td>
<td style="text-align: left;">Linear decay to 0</td>
</tr>
<tr>
<td style="text-align: left;">Finetuning Hyperparameters</td>
<td style="text-align: left;">Value</td>
</tr>
<tr>
<td style="text-align: left;">Epochs</td>
<td style="text-align: left;">20</td>
</tr>
<tr>
<td style="text-align: left;">Sampling Temperature</td>
<td style="text-align: left;">0 (argmax)</td>
</tr>
<tr>
<td style="text-align: left;">Base Learning Rate $(\alpha)$</td>
<td style="text-align: left;">$1.6 \times 10^{-5}(3 \mathrm{~B})$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$1.2 \times 10^{-5}(6 \mathrm{~B})$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$1.0 \times 10^{-5}(12 \mathrm{~B})$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$6.0 \times 10^{-6}(175 \mathrm{~B})$</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate</td>
<td style="text-align: left;">$0.1 \times \alpha$</td>
</tr>
<tr>
<td style="text-align: left;">Verification Hyperparameters</td>
<td style="text-align: left;">Value</td>
</tr>
<tr>
<td style="text-align: left;">Epochs</td>
<td style="text-align: left;">2 for generator, 1 for verifier</td>
</tr>
<tr>
<td style="text-align: left;">Sampling Temperature</td>
<td style="text-align: left;">0.7</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate</td>
<td style="text-align: left;">$1.0 \times 10^{-5}$</td>
</tr>
<tr>
<td style="text-align: left;">Loss weight</td>
<td style="text-align: left;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">Verifier loss</td>
<td style="text-align: left;">MSE</td>
</tr>
<tr>
<td style="text-align: left;">Completions per train problem</td>
<td style="text-align: left;">100</td>
</tr>
<tr>
<td style="text-align: left;">Completions per test problem</td>
<td style="text-align: left;">100</td>
</tr>
</tbody>
</table>
<p>Table 1: Hyperparameters used for all experiments, unless explicitly said otherwise. Notable exceptions include Figure 8c, which uses 4 x more tokens per batch and 300 completions at both training and test time. All dropout experiments in Figure 8 use $20 \%$ dropout. Figure 7a uses verifiers trained on 100 completions, but searching over more completions at test time.</p>
<h1>C Calculator Annotations</h1>
<p>The calculator annotations were not provided by human contractors: they were generated by a combination of hard-coded logic and a finetuned language model. The logic for auto-generating calculator annotations is imperfect. It is highly unlikely to generate any incorrect annotations, but it is not uncommon for it to ignore some lines that could be annotated.</p>
<p>During training, there is no special distinction between the annotated tokens and the rest of the solution: they are all just tokens. During testing, we override model sampling when a well-formatted annotation exists, specifically overwriting the token(s) directly following " $=$ " and within $&lt;&lt;\ldots&gt;&gt;$.</p>
<p>To simulate the calculator, we simply use the python eval function to evaluate the tokens in the expression (Figure 9). Evaluations that time out or throw an error result in the annotations being skipped and the model being sampled from as usual.</p>
<p>We note that the original version of our calculator, used for all results in this paper, had some minor implementation bugs. Our reported test performance is therefore a slight underestimate, though the magnitude of this discrepancy is less than $1 \%$ in most experiments. Fixing the calculator improves verification test performance by about $1 \%$ when using the full GSM8K training set.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: A diagram of the calculator sampling procedure.</p>
<h1>D Example Model Solutions</h1>
<p>We showcase a handful of samples comparing finetuning and verification at both 6B and 175B scale. Samples were slightly cherry-picked for diversity.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<table>
<thead>
<tr>
<th>☐</th>
<th>☐</th>
</tr>
</thead>
<tbody>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
<tr>
<td>☐</td>
<td>☐</td>
</tr>
</tbody>
</table>
<h1>E Verifier Details</h1>
<p>As noted in section 4.2, we train verifiers with a joint objective where the model learns to label a model completion as correct or incorrect, in addition to the original language modeling objective. Architecturally, this means our verifiers are language models, with a small scalar head that outputs predictions on a per-token basis.</p>
<p>We implement this scalar head as a single bias parameter and single gain parameter that operate on the logits outputted by the language model's final unembedding layer. Specifically, the bias and gain shift and scale the logit corresponding to a special token in the vocabulary. As such, the logits for other tokens can continue to represent the language modeling objective, while this special token is reserved for the verifier's predictions.</p>
<p>We can choose to initialize the verifier from the same pretrained language model the generator was finetuned from, or from the generator itself. In our ablations the latter performed slightly better; we suspect this is because better understanding the language distribution that the generator learned should only aid the verifier in scoring samples from that distribution. Unless otherwise explicitly stated, we initialize our verifiers from their corresponding generators in all experiments.</p>
<p>When training verifiers with the joint objective, we use an equal mix of language data and verifier data. Because we sample 100 completions for each original training example to generate the verifier data, using an equal mix means we effectively upsample the original language data by a factor of 100 . To form the joint objective, we simply add the verifier loss and language modeling loss unweighted, and define an epoch of this joint objective as having seen each verifier example once. With both objectives, we mask out tokens in the question and only train on tokens in the solutions, as visualized in Figure 12.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 12: Visualization of the joint training objective. We mask out tokens in the question and only consider the loss corresponding to tokens in the solution.</p>
<h1>F Verifier Visualization</h1>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 13: Five cherry-picked samples generated by a 175B finetuned model and scored by a 175B token-level verifier. A green background color indicates a high verifier score, and a red background color indicates a low one.</p>
<p>One benefit of the token-level verifiers is that these models become immediately interpretable: we can visualize the predicted value for each token and better understand how the verifier makes decisions on judging samples. Above we present a visualization of the predicted values for five different cherry-picked questions and model completions, verified by a 175B token-level verifier that was trained on the full training set.</p>
<p>In the visualization, the background color of the text corresponds to the verifier score for that token, where red is low value (predicted incorrect) and green</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contribution. Correspondence to: Karl Cobbe $&lt;$ karl@openai.com $&gt;$, Vineet Kosaraju <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#118;&#105;&#110;&#101;&#101;&#116;&#64;&#111;&#112;&#101;&#110;&#97;&#105;&#46;&#99;&#111;&#109;">&#118;&#105;&#110;&#101;&#101;&#116;&#64;&#111;&#112;&#101;&#110;&#97;&#105;&#46;&#99;&#111;&#109;</a>&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
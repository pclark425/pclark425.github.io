<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2047 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2047</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2047</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-50.html">extraction-schema-50</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <p><strong>Paper ID:</strong> paper-279119023</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.02507v2.pdf" target="_blank">AURA: Autonomous Upskilling with Retrieval-Augmented Agents</a></p>
                <p><strong>Paper Abstract:</strong> Designing reinforcement learning curricula for agile robots traditionally requires extensive manual tuning of reward functions, environment randomizations, and training configurations. We introduce AURA (Autonomous Upskilling with Retrieval-Augmented Agents), a schema-validated curriculum reinforcement learning (RL) framework that leverages Large Language Models (LLMs) as autonomous designers of multi-stage curricula. AURA transforms user prompts into YAML workflows that encode full reward functions, domain randomization strategies, and training configurations. All files are statically validated before any GPU time is used, ensuring efficient and reliable execution. A retrieval-augmented feedback loop allows specialized LLM agents to design, execute, and refine curriculum stages based on prior training results stored in a vector database, enabling continual improvement over time. Quantitative experiments show that AURA consistently outperforms LLM-guided baselines in generation success rate, humanoid locomotion, and manipulation tasks. Ablation studies highlight the importance of schema validation and retrieval for curriculum quality. AURA successfully trains end-to-end policies directly from user prompts and deploys them zero-shot on a custom humanoid robot in multiple environments - capabilities that did not exist previously with manually designed controllers. By abstracting the complexity of curriculum design, AURA enables scalable and adaptive policy learning pipelines that would be complex to construct by hand.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2047.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2047.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AURA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autonomous Upskilling with Retrieval-Augmented Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AURA is a schema-validated, retrieval-augmented, multi-agent LLM framework that turns a natural-language prompt and robot specification into executable multi-stage RL curricula (typed YAMLs) which are statically validated, compiled, trained (PPO), and iteratively refined using a vector database of prior runs and human feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated (multi-agent) with retrieval augmentation and schema validation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4.1</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>AURA uses a team of specialized LLM agents to (1) convert a natural-language task + robot spec + MJCF/MJX environment into a high-level multi-stage plan, (2) retrieve top-3 prior curricula from a vector DB (encoded with text-embedding-3-large) and condition generation on one selected example, (3) generate per-stage YAMLs for reward terms (Φ_k), domain randomizations (ρ_k), and training configs (Θ_k), (4) statically validate typed YAMLs against schemas (type conformance, reference integrity, mathematical well-formedness), compile into MJX code, (5) train stages sequentially with PPO (policy from one stage initializes the next), (6) analyze rollout metrics and feed automatic + human feedback back into the VDB for subsequent iterations. Generates tasks/goals as explicit reward components, domain randomization schedules, stage budgets, promotion criteria, and concrete training hyperparameters. Iteratively adapts curricula across multiple iterations using stored evaluations and user feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Humanoid locomotion and manipulator tasks (custom humanoid, Berkeley Humanoid, BoosterT1, UR5e manipulator pushing/stacking)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>High-dimensional continuous-control robotics domains with sparse and long-horizon rewards, need for domain randomization and sim-to-real transfer, episodic horizons up to 3000 steps, requirement for gait symmetry and stability, task diversity includes velocity-tracking, terrain adaptation, jumping, hopping, lateral walking, pushing and stacking manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>Conditions on prior-run artifacts and metrics stored in the vector DB (YAMLs, rollout evaluations, human feedback), robot specification (joint names, sensors, actuator limits), MJCF simulation model, and per-stage training logs/metrics (success rate, reward-component attribution, stability/energy metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Vector database (Pinecone) retrieval augmentation, typed YAML schema and static validator, MJX compilation, PPO training loop, per-stage automated feedback analysis, multi-agent LLM decomposition (high-level planner, per-stage generators, selector, feedback LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>AURA achieved a 99% curriculum training-launch success rate (a single launch per task where every stage successfully launched GPU training). In simulation: for Berkeley Humanoid AURA matched near-full 3000-step survival and produced significantly better linear-velocity tracking than MuJoCo Playground baseline; BoosterT1 episode length increased from 2139→2366 and linear-velocity tracking score from 1786→2162 after AURA reward redesign; UR5e pushing success >90%; UR5e stacking mean success 72.7%. Zero-shot hardware: outdoor walking at 0.18 m/s, gait frequency 1.91 Hz, recovered from lateral pushes up to 0.38 m/s and 50 mm drop without falling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td>Human-designed baselines (MuJoCo Playground / expert rewards) were outperformed by AURA on linear-velocity tracking and in multiple locomotion tasks; for survival AURA matched or exceeded human baseline (e.g., similar ~3000-step survival on Berkeley Humanoid but better tracking). Exact numeric comparisons versus human baseline are given qualitatively in the paper (AURA improves tracking and robustness; some per-task numbers: BoosterT1 improvements above).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td>Compared to LLM-guided baselines that rely on sampling/brute-force (CurricuLLM, Eureka), AURA used far fewer training launches: AURA performs one validated launch per stage, whereas CurricuLLM used ~5 launches/stage and Eureka ~16 launches/stage (retaining best run). No single unified heuristic baseline numbers beyond those launch-count differences are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td>Curricula frequently include hundreds of parameters and sometimes over ten unique reward terms per workflow; AURA produced multi-stage curricula (K≥2) and iteratively modified them across five iterations in experiments. No explicit count of distinct generated goal types is reported beyond these descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>AURA policies transferred zero-shot to real hardware (custom 0.6 m humanoid) demonstrating outdoor walking, uneven-terrain handling, gait tracking, and disturbance rejection; UR5e manipulation curricula adapted Franka rewards and achieved >90% push success and 72.7% stacking success, indicating strong sim-to-real and cross-embodiment adaptation when initialized from relevant VDB entries.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>AURA required a single validated training launch per stage due to schema validation and retrieval grounding, reducing overall GPU runs and computational budget. By contrast CurricuLLM launched ~5 training runs per stage and Eureka ~16 per stage; curriculum budgets were up to 300M–400M environment steps per task/stage (e.g., custom humanoid 400M, others 300M timesteps). Exact dollar/API costs for LLM calls not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Remaining manual effort for environment/spec setup (MJCF/MJX), dependence on VDB content (can introduce bias and premature convergence on narrow solutions), reliance on human qualitative feedback between iterations, AURA Blind produced asymmetric gaits when foot-phase rewards absent, and ablations show large drops in generation success without schema or retrieval. Generations are retried up to 5 times but schema-free generation is brittle.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>AURA supports long-horizon episodic tasks (evaluations at T_max=3000); resulted in near-full 3000-step survival on Berkeley Humanoid and improved long-horizon tracking and robustness in locomotion tasks, indicating that the curriculum helps long-horizon learning in these robotics domains.</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>For specialized manipulator adaptation (UR5e), AURA successfully adapted Franka expert rewards to UR5e, producing >90% push success and 72.7% stacking success—evidence AURA performs well in specialized domains when given suitable prior examples in the VDB or when designing curricula from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Removing schema validation reduced training-launch success from 99%→47%; removing VDB retrieval reduced success to 38%; using a single LLM (no multi-agent decomposition) reduced success to 7%. Multi-agent decomposition and RAG significantly improved first-launch validity and overall reliability. Other ablations on retrieval vs blind (AURA Tune vs AURA Blind) show different gait styles and convergence behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>LLM-generated curricula are effective when: (1) generation is constrained by typed schema validation (prevents wasted launches), (2) retrieval-augmentation conditions generation on successful past curricula (improves first-launch correctness and performance), and (3) multi-agent LLM decomposition produces stage-level focused outputs. Quantitatively, these components yield a 99% generation success rate vs 31% (CurricuLLM) and 12% (Eureka anymal case), and substantial improvements in domain-specific metrics (e.g., BoosterT1 survival and tracking, UR5e push/stack success) while requiring far fewer parallel training launches (single validated launch per stage vs multiple brute-force launches).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2047.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2047.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CurricuLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CurricuLLM (Automatic task curricula design for learning complex robot skills using large language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CurricuLLM is an earlier LLM-guided system that uses LLMs and evolutionary search to generate reward functions and curricula for humanoid locomotion and other robot tasks, evaluated in Isaac Lab and reported to require multiple sampled training launches per stage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Curricullm: Automatic task curricula design for learning complex robot skills using large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated with evolutionary search / sampling-based selection</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>CurricuLLM prompts LLMs to generate reward code and curricula, uses evolutionary search or sampling across multiple generated reward variants, evaluates many training runs in parallel per stage, and selects successful runs (brute-force sampling strategy). Focused on curriculum generation for humanoid locomotion; uses multiple training launches per stage then retains best-performing run.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Humanoid locomotion; also benchmarked in Isaac Lab (Berkeley Humanoid and manipulation tasks reported in their paper).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>High-dimensional continuous control, long-horizon, sparse rewards; requires careful reward shaping and domain randomization for sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Evolutionary search / brute-force sampling of multiple generated reward candidates, parallel environment launches for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>Reported training-launch success rate evaluated in this paper: 31% (measured over 100 training launches on Berkeley Humanoid environment). CurricuLLM's reported Berkeley Humanoid and fetch-push results are referenced from their original paper (used as baseline comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Referenced as having real-world success on Berkeley Humanoid in prior work; exact transfer numbers are reported in their original paper (cited), but AURA reports outperforming CurricuLLM in first-launch reliability and certain locomotion metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>CurricuLLM launches multiple training runs per stage (~5 per stage as reported in AURA), increasing computational cost relative to AURA's single validated launch per stage; exact wall-clock or monetary costs not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Relies on sampling many stochastic LLM generations and multiple environment launches to find workable rewards; inefficient compute usage and brittle first-launch success rates due to malformed generations or reward-code errors.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>LLM-guided curricula can produce successful controllers but CurricuLLM's sampling-heavy approach yields lower first-launch reliability (~31%) and higher computational cost compared to a schema-validated, retrieval-augmented approach like AURA.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2047.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2047.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eureka</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eureka (Human-level reward design via coding large language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Eureka is an LLM-guided framework that generates reward code via LLMs and uses evolutionary search and massive sampling of generations and training runs to find viable reward functions; extended in DrEureka to consider domain randomization and hardware safety for sim-to-real.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Eureka: Human-level reward design via coding large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated (sampling/evolutionary selection) with parallel evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Eureka prompts LLMs to produce reward code and then uses evolutionary search and parallel training launches to evaluate many variants, keeping the best-performing reward designs. DrEureka extends this by generating domain randomizations and considering hardware safety for real-world locomotion tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Robotics tasks including legged robots (ANYmal) and various Gymnasium environments referenced in their repository.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Varies across embodiments; includes agile quadrupeds like ANYmal and other robots; high-dimensional, physics-based control requiring sim-to-real considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Evolutionary search, parallel environment launches, retention of best-performing reward-run; DrEureka includes domain randomization and hardware safety considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>Eureka's reported training-launch success rate on ANYmal in AURA's evaluation: 12% (12 successful launches out of 100) for the complex ANYmal task; across all available embodiments in their examples aggregated success was 49% (simpler tasks had higher success rates). Eureka typically required ~16 launches per stage (as reported by AURA) before selecting successful runs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High: reported to generate ~16 training launches per stage and retain best-performing runs, substantially increasing computational expense relative to AURA's one validated launch per stage.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Low first-launch success on complex humanoid tasks (12% for ANYmal), brittle to malformed LLM generations, requires massive parallel training to find viable policies, which is computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>LLM-generated reward-code + brute-force sampling can yield working reward designs but is computationally expensive and has low first-launch reliability on complex embodied tasks; adding domain randomization and hardware safety (DrEureka) aids sim-to-real but does not remove the sampling burden.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2047.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2047.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG+VDB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation with Vector Database (RAG + VDB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AURA integrates retrieval-augmented generation: task and robot spec queries are embedded (text-embedding-3-large) and used to retrieve top-k prior curricula and evaluations from a vector DB (Pinecone) to ground and condition LLM curriculum generation for improved correctness and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated with retrieval augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>OpenAI text-embedding-3-large (for retrieval); curriculum LLMs: GPT-4.1 in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>RAG module builds a retrieval query from the user task and robot spec, encodes it with text-embedding-3-large, performs cosine similarity search over the VDB to return top-3 prior curricula and evaluations; a selector LLM picks the most relevant example which is included in the high-level planner context, enabling example-conditioned generation of stage plans and reducing hallucination and malformed outputs. Two operation modes: AURA Blind (empty VDB) and AURA Tune (VDB seeded with human-expert rewards).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Same robotics domains as AURA (humanoid locomotion, manipulator tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Environments where historical, embodiment-specific curricula and evaluation artifacts improve generation (i.e., tasks with embodiment-specific reward nuances, domain randomization, and hardware safety concerns).</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>Conditions on prior curricula files (reward, randomize, training YAMLs), rollout metrics, and human feedback from previous runs stored in the VDB; selector LLM chooses the single most relevant example from top-3 retrievals.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Typed schema validation, multi-agent LLM decomposition, MJX compilation, automated feedback insertion into VDB, Pinecone vector DB and OpenAI embeddings for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>Retrieval augmentation substantially improved generation success: AURA (with VDB) achieved 99% training-launch success; removing VDB reduced success to 38% in ablation, demonstrating a major role for retrieval. AURA Tune (seeded VDB) produced more consistent gait styles (retained phase-tracking reward terms) versus AURA Blind.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>When VDB seeded with MuJoCo Playground's Berkeley Humanoid rewards (AURA Tune) the system adapted existing reward terms effectively to new embodiments and improved sim-to-real transfer (e.g., custom humanoid outdoor walking).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>RAG reduces wasted GPU launches by improving first-launch validity; AURA uses only a single launch per stage compared to multiple launches in sampling-based baselines. Embedding/inference cost for retrieval not quantified in dollars.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Dependence on VDB introduces bias risk: lack of diversity or inaccurate feedback in the database can reinforce suboptimal patterns and cause premature convergence; needs mechanisms for exploratory curriculum sampling to mitigate VDB-content bias.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>AURA w/o VDB ablation reduced training-launch success from 99%→38%, showing high dependence on retrieval for curriculum validity and quality.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Conditioning LLM-generated curricula on prior successful curricula and evaluations via retrieval markedly increases first-launch correctness and downstream policy quality; retrieval augmentation is a key factor in achieving high generation success and lower computational cost compared to sampling-based LLM curriculum methods.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Eureka: Human-level reward design via coding large language models <em>(Rating: 2)</em></li>
                <li>Curricullm: Automatic task curricula design for learning complex robot skills using large language models <em>(Rating: 2)</em></li>
                <li>Dreureka: Language model guided sim-to-real transfer <em>(Rating: 1)</em></li>
                <li>Environment curriculum generation via large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2047",
    "paper_id": "paper-279119023",
    "extraction_schema_id": "extraction-schema-50",
    "extracted_data": [
        {
            "name_short": "AURA",
            "name_full": "Autonomous Upskilling with Retrieval-Augmented Agents",
            "brief_description": "AURA is a schema-validated, retrieval-augmented, multi-agent LLM framework that turns a natural-language prompt and robot specification into executable multi-stage RL curricula (typed YAMLs) which are statically validated, compiled, trained (PPO), and iteratively refined using a vector database of prior runs and human feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-generated (multi-agent) with retrieval augmentation and schema validation",
            "llm_model_name": "GPT-4.1",
            "llm_model_size": null,
            "curriculum_description": "AURA uses a team of specialized LLM agents to (1) convert a natural-language task + robot spec + MJCF/MJX environment into a high-level multi-stage plan, (2) retrieve top-3 prior curricula from a vector DB (encoded with text-embedding-3-large) and condition generation on one selected example, (3) generate per-stage YAMLs for reward terms (Φ_k), domain randomizations (ρ_k), and training configs (Θ_k), (4) statically validate typed YAMLs against schemas (type conformance, reference integrity, mathematical well-formedness), compile into MJX code, (5) train stages sequentially with PPO (policy from one stage initializes the next), (6) analyze rollout metrics and feed automatic + human feedback back into the VDB for subsequent iterations. Generates tasks/goals as explicit reward components, domain randomization schedules, stage budgets, promotion criteria, and concrete training hyperparameters. Iteratively adapts curricula across multiple iterations using stored evaluations and user feedback.",
            "domain_name": "Humanoid locomotion and manipulator tasks (custom humanoid, Berkeley Humanoid, BoosterT1, UR5e manipulator pushing/stacking)",
            "domain_characteristics": "High-dimensional continuous-control robotics domains with sparse and long-horizon rewards, need for domain randomization and sim-to-real transfer, episodic horizons up to 3000 steps, requirement for gait symmetry and stability, task diversity includes velocity-tracking, terrain adaptation, jumping, hopping, lateral walking, pushing and stacking manipulation.",
            "state_conditioning": true,
            "state_conditioning_details": "Conditions on prior-run artifacts and metrics stored in the vector DB (YAMLs, rollout evaluations, human feedback), robot specification (joint names, sensors, actuator limits), MJCF simulation model, and per-stage training logs/metrics (success rate, reward-component attribution, stability/energy metrics).",
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Vector database (Pinecone) retrieval augmentation, typed YAML schema and static validator, MJX compilation, PPO training loop, per-stage automated feedback analysis, multi-agent LLM decomposition (high-level planner, per-stage generators, selector, feedback LLM).",
            "performance_llm_curriculum": "AURA achieved a 99% curriculum training-launch success rate (a single launch per task where every stage successfully launched GPU training). In simulation: for Berkeley Humanoid AURA matched near-full 3000-step survival and produced significantly better linear-velocity tracking than MuJoCo Playground baseline; BoosterT1 episode length increased from 2139→2366 and linear-velocity tracking score from 1786→2162 after AURA reward redesign; UR5e pushing success &gt;90%; UR5e stacking mean success 72.7%. Zero-shot hardware: outdoor walking at 0.18 m/s, gait frequency 1.91 Hz, recovered from lateral pushes up to 0.38 m/s and 50 mm drop without falling.",
            "performance_manual_curriculum": "Human-designed baselines (MuJoCo Playground / expert rewards) were outperformed by AURA on linear-velocity tracking and in multiple locomotion tasks; for survival AURA matched or exceeded human baseline (e.g., similar ~3000-step survival on Berkeley Humanoid but better tracking). Exact numeric comparisons versus human baseline are given qualitatively in the paper (AURA improves tracking and robustness; some per-task numbers: BoosterT1 improvements above).",
            "performance_heuristic_curriculum": "Compared to LLM-guided baselines that rely on sampling/brute-force (CurricuLLM, Eureka), AURA used far fewer training launches: AURA performs one validated launch per stage, whereas CurricuLLM used ~5 launches/stage and Eureka ~16 launches/stage (retaining best run). No single unified heuristic baseline numbers beyond those launch-count differences are provided.",
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": "Curricula frequently include hundreds of parameters and sometimes over ten unique reward terms per workflow; AURA produced multi-stage curricula (K≥2) and iteratively modified them across five iterations in experiments. No explicit count of distinct generated goal types is reported beyond these descriptions.",
            "transfer_generalization_results": "AURA policies transferred zero-shot to real hardware (custom 0.6 m humanoid) demonstrating outdoor walking, uneven-terrain handling, gait tracking, and disturbance rejection; UR5e manipulation curricula adapted Franka rewards and achieved &gt;90% push success and 72.7% stacking success, indicating strong sim-to-real and cross-embodiment adaptation when initialized from relevant VDB entries.",
            "computational_cost": "AURA required a single validated training launch per stage due to schema validation and retrieval grounding, reducing overall GPU runs and computational budget. By contrast CurricuLLM launched ~5 training runs per stage and Eureka ~16 per stage; curriculum budgets were up to 300M–400M environment steps per task/stage (e.g., custom humanoid 400M, others 300M timesteps). Exact dollar/API costs for LLM calls not reported.",
            "failure_modes_limitations": "Remaining manual effort for environment/spec setup (MJCF/MJX), dependence on VDB content (can introduce bias and premature convergence on narrow solutions), reliance on human qualitative feedback between iterations, AURA Blind produced asymmetric gaits when foot-phase rewards absent, and ablations show large drops in generation success without schema or retrieval. Generations are retried up to 5 times but schema-free generation is brittle.",
            "long_horizon_performance": "AURA supports long-horizon episodic tasks (evaluations at T_max=3000); resulted in near-full 3000-step survival on Berkeley Humanoid and improved long-horizon tracking and robustness in locomotion tasks, indicating that the curriculum helps long-horizon learning in these robotics domains.",
            "specialized_domain_performance": "For specialized manipulator adaptation (UR5e), AURA successfully adapted Franka expert rewards to UR5e, producing &gt;90% push success and 72.7% stacking success—evidence AURA performs well in specialized domains when given suitable prior examples in the VDB or when designing curricula from scratch.",
            "ablation_studies": "Removing schema validation reduced training-launch success from 99%→47%; removing VDB retrieval reduced success to 38%; using a single LLM (no multi-agent decomposition) reduced success to 7%. Multi-agent decomposition and RAG significantly improved first-launch validity and overall reliability. Other ablations on retrieval vs blind (AURA Tune vs AURA Blind) show different gait styles and convergence behaviors.",
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "LLM-generated curricula are effective when: (1) generation is constrained by typed schema validation (prevents wasted launches), (2) retrieval-augmentation conditions generation on successful past curricula (improves first-launch correctness and performance), and (3) multi-agent LLM decomposition produces stage-level focused outputs. Quantitatively, these components yield a 99% generation success rate vs 31% (CurricuLLM) and 12% (Eureka anymal case), and substantial improvements in domain-specific metrics (e.g., BoosterT1 survival and tracking, UR5e push/stack success) while requiring far fewer parallel training launches (single validated launch per stage vs multiple brute-force launches).",
            "uuid": "e2047.0"
        },
        {
            "name_short": "CurricuLLM",
            "name_full": "CurricuLLM (Automatic task curricula design for learning complex robot skills using large language models)",
            "brief_description": "CurricuLLM is an earlier LLM-guided system that uses LLMs and evolutionary search to generate reward functions and curricula for humanoid locomotion and other robot tasks, evaluated in Isaac Lab and reported to require multiple sampled training launches per stage.",
            "citation_title": "Curricullm: Automatic task curricula design for learning complex robot skills using large language models",
            "mention_or_use": "mention",
            "curriculum_generator_type": "LLM-generated with evolutionary search / sampling-based selection",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "CurricuLLM prompts LLMs to generate reward code and curricula, uses evolutionary search or sampling across multiple generated reward variants, evaluates many training runs in parallel per stage, and selects successful runs (brute-force sampling strategy). Focused on curriculum generation for humanoid locomotion; uses multiple training launches per stage then retains best-performing run.",
            "domain_name": "Humanoid locomotion; also benchmarked in Isaac Lab (Berkeley Humanoid and manipulation tasks reported in their paper).",
            "domain_characteristics": "High-dimensional continuous control, long-horizon, sparse rewards; requires careful reward shaping and domain randomization for sim-to-real transfer.",
            "state_conditioning": null,
            "state_conditioning_details": null,
            "novelty_mechanism": null,
            "novelty_mechanism_details": null,
            "complementary_systems": "Evolutionary search / brute-force sampling of multiple generated reward candidates, parallel environment launches for evaluation.",
            "performance_llm_curriculum": "Reported training-launch success rate evaluated in this paper: 31% (measured over 100 training launches on Berkeley Humanoid environment). CurricuLLM's reported Berkeley Humanoid and fetch-push results are referenced from their original paper (used as baseline comparisons).",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": null,
            "transfer_generalization_results": "Referenced as having real-world success on Berkeley Humanoid in prior work; exact transfer numbers are reported in their original paper (cited), but AURA reports outperforming CurricuLLM in first-launch reliability and certain locomotion metrics.",
            "computational_cost": "CurricuLLM launches multiple training runs per stage (~5 per stage as reported in AURA), increasing computational cost relative to AURA's single validated launch per stage; exact wall-clock or monetary costs not provided here.",
            "failure_modes_limitations": "Relies on sampling many stochastic LLM generations and multiple environment launches to find workable rewards; inefficient compute usage and brittle first-launch success rates due to malformed generations or reward-code errors.",
            "long_horizon_performance": null,
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "LLM-guided curricula can produce successful controllers but CurricuLLM's sampling-heavy approach yields lower first-launch reliability (~31%) and higher computational cost compared to a schema-validated, retrieval-augmented approach like AURA.",
            "uuid": "e2047.1"
        },
        {
            "name_short": "Eureka",
            "name_full": "Eureka (Human-level reward design via coding large language models)",
            "brief_description": "Eureka is an LLM-guided framework that generates reward code via LLMs and uses evolutionary search and massive sampling of generations and training runs to find viable reward functions; extended in DrEureka to consider domain randomization and hardware safety for sim-to-real.",
            "citation_title": "Eureka: Human-level reward design via coding large language models",
            "mention_or_use": "mention",
            "curriculum_generator_type": "LLM-generated (sampling/evolutionary selection) with parallel evaluation",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "Eureka prompts LLMs to produce reward code and then uses evolutionary search and parallel training launches to evaluate many variants, keeping the best-performing reward designs. DrEureka extends this by generating domain randomizations and considering hardware safety for real-world locomotion tasks.",
            "domain_name": "Robotics tasks including legged robots (ANYmal) and various Gymnasium environments referenced in their repository.",
            "domain_characteristics": "Varies across embodiments; includes agile quadrupeds like ANYmal and other robots; high-dimensional, physics-based control requiring sim-to-real considerations.",
            "state_conditioning": null,
            "state_conditioning_details": null,
            "novelty_mechanism": null,
            "novelty_mechanism_details": null,
            "complementary_systems": "Evolutionary search, parallel environment launches, retention of best-performing reward-run; DrEureka includes domain randomization and hardware safety considerations.",
            "performance_llm_curriculum": "Eureka's reported training-launch success rate on ANYmal in AURA's evaluation: 12% (12 successful launches out of 100) for the complex ANYmal task; across all available embodiments in their examples aggregated success was 49% (simpler tasks had higher success rates). Eureka typically required ~16 launches per stage (as reported by AURA) before selecting successful runs.",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": "High: reported to generate ~16 training launches per stage and retain best-performing runs, substantially increasing computational expense relative to AURA's one validated launch per stage.",
            "failure_modes_limitations": "Low first-launch success on complex humanoid tasks (12% for ANYmal), brittle to malformed LLM generations, requires massive parallel training to find viable policies, which is computationally expensive.",
            "long_horizon_performance": null,
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "LLM-generated reward-code + brute-force sampling can yield working reward designs but is computationally expensive and has low first-launch reliability on complex embodied tasks; adding domain randomization and hardware safety (DrEureka) aids sim-to-real but does not remove the sampling burden.",
            "uuid": "e2047.2"
        },
        {
            "name_short": "RAG+VDB",
            "name_full": "Retrieval-Augmented Generation with Vector Database (RAG + VDB)",
            "brief_description": "AURA integrates retrieval-augmented generation: task and robot spec queries are embedded (text-embedding-3-large) and used to retrieve top-k prior curricula and evaluations from a vector DB (Pinecone) to ground and condition LLM curriculum generation for improved correctness and generalization.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-generated with retrieval augmentation",
            "llm_model_name": "OpenAI text-embedding-3-large (for retrieval); curriculum LLMs: GPT-4.1 in experiments",
            "llm_model_size": null,
            "curriculum_description": "RAG module builds a retrieval query from the user task and robot spec, encodes it with text-embedding-3-large, performs cosine similarity search over the VDB to return top-3 prior curricula and evaluations; a selector LLM picks the most relevant example which is included in the high-level planner context, enabling example-conditioned generation of stage plans and reducing hallucination and malformed outputs. Two operation modes: AURA Blind (empty VDB) and AURA Tune (VDB seeded with human-expert rewards).",
            "domain_name": "Same robotics domains as AURA (humanoid locomotion, manipulator tasks)",
            "domain_characteristics": "Environments where historical, embodiment-specific curricula and evaluation artifacts improve generation (i.e., tasks with embodiment-specific reward nuances, domain randomization, and hardware safety concerns).",
            "state_conditioning": true,
            "state_conditioning_details": "Conditions on prior curricula files (reward, randomize, training YAMLs), rollout metrics, and human feedback from previous runs stored in the VDB; selector LLM chooses the single most relevant example from top-3 retrievals.",
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Typed schema validation, multi-agent LLM decomposition, MJX compilation, automated feedback insertion into VDB, Pinecone vector DB and OpenAI embeddings for retrieval.",
            "performance_llm_curriculum": "Retrieval augmentation substantially improved generation success: AURA (with VDB) achieved 99% training-launch success; removing VDB reduced success to 38% in ablation, demonstrating a major role for retrieval. AURA Tune (seeded VDB) produced more consistent gait styles (retained phase-tracking reward terms) versus AURA Blind.",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": null,
            "transfer_generalization_results": "When VDB seeded with MuJoCo Playground's Berkeley Humanoid rewards (AURA Tune) the system adapted existing reward terms effectively to new embodiments and improved sim-to-real transfer (e.g., custom humanoid outdoor walking).",
            "computational_cost": "RAG reduces wasted GPU launches by improving first-launch validity; AURA uses only a single launch per stage compared to multiple launches in sampling-based baselines. Embedding/inference cost for retrieval not quantified in dollars.",
            "failure_modes_limitations": "Dependence on VDB introduces bias risk: lack of diversity or inaccurate feedback in the database can reinforce suboptimal patterns and cause premature convergence; needs mechanisms for exploratory curriculum sampling to mitigate VDB-content bias.",
            "long_horizon_performance": null,
            "specialized_domain_performance": null,
            "ablation_studies": "AURA w/o VDB ablation reduced training-launch success from 99%→38%, showing high dependence on retrieval for curriculum validity and quality.",
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Conditioning LLM-generated curricula on prior successful curricula and evaluations via retrieval markedly increases first-launch correctness and downstream policy quality; retrieval augmentation is a key factor in achieving high generation success and lower computational cost compared to sampling-based LLM curriculum methods.",
            "uuid": "e2047.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Eureka: Human-level reward design via coding large language models",
            "rating": 2
        },
        {
            "paper_title": "Curricullm: Automatic task curricula design for learning complex robot skills using large language models",
            "rating": 2
        },
        {
            "paper_title": "Dreureka: Language model guided sim-to-real transfer",
            "rating": 1
        },
        {
            "paper_title": "Environment curriculum generation via large language models",
            "rating": 1
        }
    ],
    "cost": 0.0171875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AURA: Autonomous Upskilling with Retrieval-Augmented Agents
24 Sep 2025</p>
<p>Alvin Zhu 
Yusuke Tanaka 
Andrew Goldberg 
Dennis Hong 
Boostert1 Humanoid 
Berkeley Humanoid 
AURA: Autonomous Upskilling with Retrieval-Augmented Agents
24 Sep 202578709F0302ABDFB4D1D271A5547F5E9FarXiv:2506.02507v2[cs.RO]Curriculum Generation Staged Training</p>
<p>User Prompt</p>
<p>Progressive Learning</p>
<p>Stabilizing off Platform Stacking Cube</p>
<p>Fig. 1: AURA-trained policies deployed successfully on custom humanoid hardware and in simulation for locomotion and manipulation tasks.</p>
<p>Abstract-Designing reinforcement learning curricula for agile robots traditionally requires extensive manual tuning of reward functions, environment randomizations, and training configurations.We introduce AURA (Autonomous Upskilling with Retrieval-Augmented Agents), a schema-validated curriculum reinforcement learning (RL) framework that leverages Large Language Models (LLMs) as autonomous designers of multi-stage curricula.AURA transforms user prompts into YAML workflows that encode full reward functions, domain randomization strategies, and training configurations.All files are statically validated before any GPU time is used, ensuring efficient and reliable execution.A retrieval-augmented feedback loop allows specialized LLM agents to design, execute, and refine curriculum stages based on prior training results stored in a vector database, enabling continual improvement over time.Quantitative experiments show that AURA consistently outperforms LLM-guided baselines in generation success rate, humanoid locomotion, and manipulation tasks.Ablation studies highlight the importance of schema validation and retrieval for curriculum quality.AURA successfully trains end-to-end policies directly from user prompts and deploys them zero-shot on a custom humanoid robot in multiple environments -capabilities that did not exist previously with manually designed controllers.By abstracting the complexity of curriculum design, AURA enables scalable and adaptive policy learning pipelines that would be complex to construct by hand.</p>
<p>I. INTRODUCTION</p>
<p>Curriculum reinforcement learning (RL) [1][2][3][4] enables robots to master complex skills by decomposing tasks into progressively harder subtasks [5].This approach has proven to be effective in domains like agile locomotion [6][7][8][9], where single-stage learning can struggle with sparse rewards or high-dimensional exploration spaces.By splitting the learning process, agents are guided through increasingly challenging environments, improving sample efficiency and convergence.</p>
<p>However, designing effective multi-stage curricula remains a significant bottleneck [10,11].Multi-stage curricula require coordinated changes across reward shaping, randomization, simulation fidelity, and optimization; as the number of stages increase, designing transitions, verifying stability, and tuning parameters scales combinatorially (the "curse of dimensionality" [12]) and becomes brittle to human error and heuristic bias [13,14].Moreover, failures at any stage often derail the entire training process, making automation critical for scaling curriculum RL.</p>
<p>Large language models (LLMs) offer a compelling alternative to optimization-based approaches [15].Their high-level reasoning and generalization capabilities have demonstrated success in robotic planning, perception, and code generation [16][17][18][19][20][21].While LLMs are not directly deployable on realtime control loops due to latency constraints [22], LLM's have shown potential in designing training schemes for real-time robot control policies [23][24][25].However, current LLM-based RL pipelines inefficiently use computation on launching parallel environments to deal with malformed generations and fail to learn from experience across tasks.</p>
<p>What is missing is a principled framework for transforming high-level prompts into reliable, executable RL training pipelines that improve with experience.Such a system should (1) ensure syntactic and semantic validity before execution, (2) use past experience to optimize future performance, and (3) modularize the pipeline into verifiable components [26].Without these capabilities, LLMs are limited to more fragile, trial-and-error reward design rather than scalable curriculum generation.</p>
<p>AURA (Autonomous Upskilling with Retrieval-Augmented Agents) addresses these bottlenecks with three key ideas.(i) A typed YAML schema captures curricula, reward functions, randomization, and training hyperparameters; structured LLM outputs are statically validated before a single GPU cycle is spent.(ii) A team of specialized LLM agents works in collaboration to enable modular, schema-compliant curriculum generation.(iii) A Retrieval-augmented generation (RAG) module with a vector database (VDB) stores prior task specifications, curricula, and rollout evaluations.This database supports experience-informed generation by enabling agents to condition on successful prior workflows, improving both accuracy and generalization.</p>
<p>We evaluate AURA on a suite of humanoid locomotion and deploy the resulting policies zero-shot on a custom, kid-sized humanoid.Compared to curriculum RL baselines and recent LLM-crafted reward pipelines, AURA is capable of generating context-rich, high-dimensional curricula that produce successful zero-shot policies deployable on real humanoid hardware.Our contributions are: 1) AURA: A fully agentic, retrieval-augmented framework that turns a natural-language prompt into a hardwaredeployable controller policy.2) Curriculum compiler, schema-validation: A typed YAML schema that provides an LLM-friendly interface for defining reward terms, domain randomizations, and training configurations.Static validation of the YAMLs enables descriptive error messages for generation retries, without wasting compute on failed environment launches.3) Experience-aware, self-improving, curriculum generation agent: Specialized LLM agents query a vector database of past runs and evaluations, select relevant training files, generate high-level curricula, and refine them into executable multi-stage specifications.This feedbackdriven loop promotes curriculum quality and training stability over many iterations.4) Zero-shot deployment: Experiments showing AURA's policies transfer zero-shot to a custom kid-sized humanoid, Berkeley Humanoid, Booster T1, [27], and manipulators, showing end-to-end prompt-to-policy ability.</p>
<p>II. RELATED WORK</p>
<p>A. Large Language Models in Robotics</p>
<p>Early efforts to link natural language to robot control and task-planning frameworks translate natural-language commands into symbolic action graphs, as demonstrated by SayCan and Code-as-Policies [17,18].Recent Vision-Language-Action paradigms, such as RT-1/2, π0, Octo, and OTTER, [16,[28][29][30][31], demonstrate that pretraining on large-scale robot data yields policies that generalize across many robotic skills.Further work has extended this to cross-domain generalization, deformable object manipulation, and long-horizon household tasks, exemplified by Gemini Robotics and embodied LLMs [21,22].Other lines of work have explored LLM-driven planning and languageaugmented reasoning for robotic control pipelines [19][20][21][32][33][34][35].Despite their semantic flexibility, these methods continue to rely on hand-tuned, low-level controllers and standard actuation assumptions, with LLMs typically external to the closed control loop [32,33].RoboGen addresses these limitations through decomposing long-horizon tasks into skills, which are tuned or trained by LLMs [36].</p>
<p>B. Curriculum Reinforcement Learning</p>
<p>Reinforcement learning often struggles with sparse rewards and long-horizon problems, where meaningful feedback is rare or delayed over many timesteps [37].Curriculum RL addresses sparse-reward and long-horizon problems by exposing agents to a progression of gradually more difficult tasks [1,5,14].Surveys and benchmarking studies note that practical curriculum design remains heuristic and sensitive to context-specific particulars [14].Reverse-curriculum approaches [38,39] employ a staged training process that progressively increases the task's difficulty from previous successful demonstrations, thereby enhancing exploration and sample efficiency.Domain-randomization and curriculumbased RL have enabled real-world legged locomotion and manipulation in challenging environments [2-4, 6-9, 40-44].</p>
<p>C. LLM-guided RL and Curricula</p>
<p>Emerging systems such as CurricuLLM [24] and Eureka [23,45] utilize LLMs to generate reward functions for RL training.Eureka uses evolutionary search and prompts LLMs in parallel to directly generate reward code for various tasks in simulation.DrEureka extends Eureka to generate domain randomizations and consider hardware safety, demonstrating real-world locomotion tasks on a Unitree Go1 quadruped robot [25].CurricuLLM [24] similarly uses evolutionary search and directly generates code, but focuses on curriculum generation and humanoid locomotion, showing real-world success on the Berkeley Humanoid [27].</p>
<p>These frameworks require sampling a large number of training runs to obtain a viable or high-performing policy, relying on LLM stochasticity and brute-force search.RAG and VDBs are approaches to enhance the LLM knowledge beyond what the LLM has been trained on [46], which AURA utilizes to construct effective curricula.Positioning of this work: AURA unifies curriculum generation, domain randomization, and training configuration within a schema-validated, retrieval-augmented loop that learns from prior runs.This yields consistent, deployable specifications with higher first-attempt launch rates and removes the need for post hoc sampling or manual curation.</p>
<p>Staged Curriculum Training Block
RAG Block</p>
<p>YAML Training files</p>
<p>III. METHODS</p>
<p>A. Problem Setup and Curriculum Formalism</p>
<p>We define a curriculum C as an ordered sequence of K training stages, C = {ξ 1 , . . ., ξ K }, where each stage
ξ k = ⟨Φ k , ρ k , Θ k , κ k ⟩ is comprised of a reward Φ k , domain- randomization distributions ρ k over environment parameters ψ, a training configuration Θ k ,
and a promotion criterion κ k (π) that governs advancement.The curriculum stages are modeled as an MDP M = ⟨S, A, T (• | s, a; ψ), R ψ , γ⟩, [47,48] with ψ denoting randomized physics properties [49,50].AURA generates (Φ k , ρ k , Θ k ) for each stage as schema-compliant YAML files that are statically validated and compiled into an MJX-based RL pipeline [51,52].We use this notation to describe generation, validation, training, and iteration.</p>
<p>B. Inputs to Curriculum Generation</p>
<p>To instantiate a curriculum from a prompt, AURA consumes four inputs: (i) a natural language task description specifying the desired behavior; (ii) a structured robot specification: joint names, semantic labels (e.g., "left foot contact"), available sensors, and actuator limits; (iii) an MJCF simulation model encoding robot kinematics, dynamics, and contact geometry; and (iv) a Python MJX environment defining observations, actions, physics parameters, and episode termination conditions.</p>
<p>C. Retrieval-Augmented High-Level Planning</p>
<p>AURA begins by leveraging LLMs and a vector database of prior curricula and outcomes to generate a high-level plan.The user-provided task description and robot specification are first passed into a query LLM, which synthesizes them into a structured retrieval query.This query is then encoded with OpenAI's text-embedding-3-large [53] and used to perform cosine-similarity search over the VDB (Pinecone).From the top-3 most relevant prior curricula, the database returns reward, randomization, and training files, along with user feedback.A selector LLM then chooses the single most relevant example from this set, based on both user feedback and curricula relevance to the current task; this example is included in the high-level planner's context for generating the new curricula.Conditioned on this selected example and the user task specification, a high-level planner decides the number of stages and produces a natural-language plan for each one.The plan describes ideas for reward components, domain randomizations, and stage-specific training hyperparameters (e.g., terrain type, disturbance level, PPO hyperparameters, etc.).</p>
<p>The VDB enables two modes of AURA.(1) AURA Blind where the VDB is initialized as empty for the first iteration and AURA must design a reward from scratch, and (2) AURA Tune where the VDB is initialized with human-expert rewards from a related task and must tune that reward for the current domain.</p>
<p>D. Stage-Level Specification and YAML Generation</p>
<p>Each stage plan from the high-level planner is given to an independent stage-level generator which translates the stage plan into structured YAML files.The stage-level generator takes as input: (i) the user task description; (ii) the naturallanguage stage description produced by the high-level planner; (iii) the available reward function library; (iv) state variables and sensor signals exposed by the robot environment; (v) the selected prior curriculum example retrieved from the VDB (chosen from the top-3 retrieval set); (vi) context from previously generated stages in the current workflow; and (vii) the reward/schema format specifications.</p>
<p>Each stage description ξ k is translated into three humanreadable, schema-compliant YAML files: Reward YAML (Φ k ) defines the differentiable terms, coefficients, and aggregation logic with explicit sensor/state dependencies; Randomization YAML (ρ k ) defines the target parameters, distributions, and activation conditions; and Training Config YAML (Θ k ) defines the PPO hyperparameters, episode budgets, checkpointing cadence, normalizations, etc..These files provide the abstract specifications needed for execution without requiring direct generation of JAX code.</p>
<p>E. Schema Validation and Compilation</p>
<p>Directly emitting JAX/MJX code from LLMs can be brittle in practice.AURA compiles only after static validation against typed schemas governing the generated workflow, reward, randomization, and training files.Validation enforces: (i) type conformance (int, float, bool, vector, etc.); (ii) structural schema compliance; (iii) reference integrity (all state variables and sensors referenced in Φ k exist in the MJX environment); and (iv) mathematical well-formedness of reward expressions.Expressions are built from a function registry (e.g., fn.NORM L2 producing JAX arrays) and must compile symbolically under MJX.A generation retry is triggered whenever schema validation detects an error.The schema validation generates a descriptive error message that includes the type of error, content of what caused the error, and sometimes a recommendation for how the error can be fixed.This error message, along with the previously generated (but invalid) YAML files, are fed back into the next attempt to guide corrections and avoid repeating mistakes; we set a maximum of five retries per stage.Once validated, YAMLs are compiled into MJX code.</p>
<p>F. Staged RL Training Loop</p>
<p>Compiled stages are trained with PPO across parallel simulation environments, each running for the episode limit specified in Θ k .Policies from one stage initialize the next, enabling progressive skill learning.</p>
<p>G. Feedback and Iteration</p>
<p>Following each stage, an automated feedback module analyzes rollouts and training signals under a task-specific rubric (e.g., success rate, stability and energy, reward-component attribution).Its recommendations may adjust reward terms, randomizations, or hyperparameters for subsequent stages within the same iteration if the curriculum training has not yet completed all K stages.</p>
<p>An iteration of our framework is defined as the complete execution of: (i) VDB-backed retrieval and high-level planning; (ii) YAML generation and static validation for all stages; (iii) PPO training across K stages; with (iv) automated feedback analysis between each stage.After the final trained policy is deployed in simulation or on hardware, the user will evaluate the policy either based on the simulation video rollout or hardware performance, then provide user feedback: 2-4 sentences in natural language judging the policy's behavior and quality based on the video rollout (eg."the policy survives well, but could track linear velocity better").User feedback is attached to the files of the justtrained curriculum and inserted into the VDB alongside the YAMLs, metrics, and rollouts.Any training that incorporates user feedback begins a new iteration (t+1), which again starts from VDB retrieval conditioned on the now-augmented database.Human input therefore does not mutate an ongoing run; it shapes the next planning phase via retrieval.</p>
<p>H. Modular LLM Collaboration</p>
<p>Hallucination mitigation: AURA limits LLM hallucinations through three mechanisms: (i) retrieval grounding-the planner is context-seeded by a single example selected from the top-3 VDB results; (ii) schema constraints-the model emits only typed YAML using a restricted operator registry (e.g., fn.NORM L2) rather than arbitrary code; and (iii) consistency checks-static validation enforces type/structure and reference integrity to environment signals, followed by MJX compile checks and a capped regenerate-on-error loop (max 5 retries).Together, these mechanisms reject unsupported symbols and incoherent references, improve first-try success, and reduce off-distribution generations.</p>
<p>IV. EXPERIMENTS A. Simulation Environments</p>
<p>We present simulation experiments on a variety of tasks on three humanoid robots and a robot arm manipulator, all example AURA trained policy rollouts can be seen in Fig. 1.All AURA experiments are trained in MuJoCo-MJX and are evaluated over 1024 parallel environments on 5 seeds.CurricuLLM simulation results are reported based on their results in Isaac Lab.</p>
<p>1) Custom Humanoid: The custom humanoid locomotion environment exposes the velocity command, joint positions, projected gravity vector, last action, phase, and max foot height as observations and actuator gains, friction coefficients, body mass, COM position, foot contact geometry, initialization configuration, and roughness as domain randomization parameters.The simulation setup was adapted   from the model in [54].AURA is run for five iterations, with each iteration generating a new curriculum.Custom humanoid locomotion is evaluated with AURA Tune, which starts with the Berkeley Humanoid reward and adapts it into a curriculum that works on the custom humanoid hardware, and AURA Blind, which generates a curriculum from scratch.All environments are run at 50Hz, and the curriculum is limited to 400M training timesteps.</p>
<p>2) Berkeley Humanoid: For the Berkeley humanoid task, AURA uses the MuJoCo Playground's Berkeley Humanoid environment.Berkeley Humanoid's MuJoCo Playground policy is trained as detailed in [55].The environment is run at 50Hz and both policies train over 300M steps.</p>
<p>3) BoosterT1 Humanoid: For BoosterT1 locomotion, AURA is trained and evaluated in the MuJoCo Playground BoosterT1 simulation environment, is run at 50Hz, and trains for 300M steps.BoosterT1's MuJoCo Playground policy is trained as detailed in [55].</p>
<p>4) UR5e Manipulator: The environment observations and domain randomizations are consistent with MuJoCo Playground's manipulator environment for the Franka Emika Panda (but with a UR5e robot).The environment frequency is 30Hz, and the curriculum is limited to 300M training steps.This environment is used for two tasks.In the pushing cube task the robot must push a block to a target position both of which are randomized in a 30cm by 30cm region (following CurricuLLM experiments).To be considered a success the center of the cube must be within 3cm of the target point.The stacking cube task begins with a cube in the gripper and the robot must place the cube on top of another block which is randomized in a 30cm by 30cm region.</p>
<p>B. Experimental Setup</p>
<p>We evaluate two aspects of AURA and baselines:</p>
<p>(1) Generation Success Rate.In Table II, we report the success rates of AURA, its ablations, CurricuLLM, and Eureka.For AURA, we measure curriculum-level success.A run is considered successful only if all stages (rewards, domain randomizations, and training configs) in the generated curriculum successfully launch GPU-accelerated training.AURA performs a single launch per task without parallel sampling.We also compare AURA with its ablated variants: AURA without a VDB, AURA without schema validation (no retires), and AURA using a single LLM for both high-level planning and YAML generation.Eureka and CurricuLLM launch multiple environments in parallel.We report the fraction of successful environment launches out of all generated rewards.Unlike AURA, success is measured one stage at a time rather than requiring a full curriculum to succeed.</p>
<p>(2) Training and Deployment Benchmarking.To evaluate policy quality, we run five iterations of curriculum generation and training using AURA, AURA Blind, AURA Tune, CurricuLLM, and a human-designed baseline.Each trained policy is evaluated with 5 random seeds each across 1024 randomized environments.Evaluation reward functions, domain perturbations, and task variations are hidden from all methods to simulate realistic zero-shot deployment.For locomotion, survival episode length is the average number of steps survived without the humanoid falling (max 3000).Linear velocity tracking score is calculated per episode as:
S lin = T t=1 exp − ∥v cmd t − v loc t ∥ 2 2 2(σ) 2 , σ = 0.1
Where v cmd t and v loc t are the commanded and actual robot velocities at timestamp t and T is the number of timsteamps the policy survives for.</p>
<p>Selected final policies are transferred to hardware for qualitative testing and demonstration.</p>
<p>C. Simulation Experiment Evaluations</p>
<p>1) Custom Humanoid: Fig. 3 summarizes evaluation during simulation deployment across iterations, showing average survival and linear velocity tracking scores.Both AURA variants improve noticeably over the human baseline on survival and linear-velocity tracking, which shows that AURA can generate an effective curriculum from scratch and can adapt existing rewards into effective curriculums.AURA Tune and AURA Blind both iteratively improve over five iterations, reasoning about reward signals and user feedback.AURA Blind's curriculum did not generate or use any rewards related to foot swing or foot phase tracking, leading to a more inconsistent and asymmetrical gait, but better simulation performance compared to AURA Tune.The locomotion task prompt does not specify stylistic information about the gait, so this is a reasonable result.AURA Tune, which is initialized with MuJoCo Playground's rewards, kept existing reward terms for phase tracking, maintaining a consistent gait while still improving over the base reward.</p>
<p>2) Berkeley Humanoid: For the Berkeley Humanoid results shown in Table I, AURA's episode survival length is similar to the MuJoCo Playground baseline, showing that both survive nearly the full 3000-step horizon.The key difference is in linear velocity tracking, where AURA achieves a significantly better score, reflecting more precise command following.While Playground training schedules 200M steps of flat terrain then 100M steps on rough terrain, AURA split training into 100M on flat terrain with mild perturbations, 100M on rough terrain with mild perturbations, and 100M on rough terrain with heavier, more frequent pushes, while progressively expanding the command range up to the 1.0 m/s and 1.0 rad/s caps.AURA also made the velocity following reward more strict and increased reward emphasis on feet-phase and action-rate, resulting in better velocity command following and smoother, more consistent stepping.</p>
<p>3) BoosterT1 Humanoid: Training with the MuJoCo Playground's BoosterT1 reward directly resulted in a policy with jerky motion and small, high-frequency shakes during locomotion.AURA was told this as feedback, which led it to generate new reward terms which penalize the squared differences across consecutive time steps for actions and for velocities, resulting in a visibly smoother policy.Heavier domain randomization also helped improve performance, raising episode length from 2139 to 2366 steps and linearvelocity tracking score from 1786 to 2162.</p>
<p>4) UR5e Manipulator: For the UR5e cube pushing task, AURA begins with MuJoCo Playground's Franka Emika Panda environment reward which is not sufficient to train a policy in the UR5e environment as shown in Table I.AURA adapted the reward into a curriculum where the first stage has no positional domain randomization and reshaped the gripper-to-cube reward term to provide a denser reward signal over larger distances, encouraging progress toward the cube rather than only near contact.In the second stage, AURA reintroduced the positional domain randomization and increased the weight of the box to target reward.This led to a final policy which could consistently push the cube to the target location with over a 90% success rate.</p>
<p>For the cube stacking task AURA adapted the push task curriculum, dramatically decreasing the gripper-to-cube reward because the cube starts in the gripper, and emphasizing the cube-to-cube reward term.AURA achieved a mean task success rate of 72.7%.</p>
<p>D. Real-World Experiments</p>
<p>Hardware Setup: We use a custom 0.6 m-tall agile humanoid with 5-DoF legs and 3-DoF arms [56].RL policies run at 50 Hz with sensors and actuators at 500 Hz.The endto-end policy uses no estimators or low-level control beyond servo internal impedance controllers.</p>
<p>Fig. 1 shows a policy generated by AURA deployed on real hardware in a zero-shot setting.The outdoor locomotion policy shown in the figure is trained by AURA Tune, while the robot traversing a step is trained by AURA Blind.In response to a user prompt requesting outdoor-capable locomotion, AURA automatically generated a four-stage curriculum progressing from flat terrain with low randomization to rough terrain with high randomization, diverse velocity commands, and significant external perturbations.The resulting policy demonstrated robust zero-shot generalization when deployed outdoors, successfully handling uneven terrain and maintaining consistent gait tracking.The robot achieved walking at 0.18 m/s and average gait frequency of 1.91 Hz.It withstood substantial perturbations, recovering from lateral pushes up to 0.38 m/s and from being physically pushed off a 50 mm high platform without falling, which surpassed the performance of existing manually tuned controllers [56].</p>
<p>E. Generation Success and Efficiency</p>
<p>1) Curriculum Generation: Table II presents the success rates of training runs for AURA, its ablated variants, and baseline methods including CurricuLLM and Eureka.AURA achieves a 99% success rate, outperforming all other methods and demonstrating the benefit of schema validation and RAG which allow for consistent generation of curriculums with hundreds of parameters and sometimes over ten unique reward terms.</p>
<p>2) Computation Efficiency: AURA requires only a single training launch per stage, since schema validation and VDBguided retrieval ensure that nearly all generated curricula are executable on the first training launch.In contrast, Eureka and CurricuLLM each generate multiple training launches per stage-16 and 5, respectively-and retain only the bestperforming successful run.As a result, AURA initiates far fewer training runs than either CurricuLLM or Eureka, leading to substantially lower computational cost for multi-stage workflows while still achieving competitive performance.</p>
<p>3) Multi-Agent LLM Setup: AURA's use of a multi-agent LLM setup-including a high-level curriculum generation agent and dedicated per-stage agents for reward, domain randomization, and configuration-was critical for producing complete and valid curricula.As shown in Table II, attempting to generate all stages in a single step is highly inconsistent.By decomposing the task into high-level planning and individual stage generation AURA forms smaller, more achievable tasks with higher success rates.The delegation of responsibilities also allows for individual stages to be retried in event of generation failure, rather than all stages.</p>
<p>V. LIMITATIONS</p>
<p>While AURA automates reward design, domain randomization, and curriculum-driven policy training, a nontrivial amount of human effort is still required.Setting up the Mu-JoCo simulation environment, defining observation spaces, action spaces, and environment state variables, remains a manual process that can impact policy success.Similarly, deploying the final policy to real hardware requires careful integration and validation.</p>
<p>Human Evaluation: Although AURA's training pipeline generates curricula autonomously, it still relies on human feedback at the end of each iteration to provide qualitative assessment of the deployed policy.While naively applying Vision-Language Models often fails to accurately describe robot data [57,58], recent frameworks demonstrate promising methods for automatically analyzing robot trajectories and delivering feedback [59,60].These could be used in future work to fully automate feedback between iterations.</p>
<p>Exploration and VDB Content Bias: While AURA leverages its VDB to improve training reliability, this dependence introduces potential bias.If the database lacks diversity or contains inaccurate human feedback, subsequent retrievalaugmented generations may reinforce suboptimal patterns.</p>
<p>AURA's curriculum search process has the potential to converge prematurely on a narrow set of solutions.Future work could incorporate explicit exploratory sampling of curricula designs or adversarial selection of prior examples to counteract this VDB content bias.</p>
<p>VI. CONCLUSION</p>
<p>This paper introduces AURA (Autonomous Upskilling with Retrieval-Augmented Agents), a schema-centric curriculum RL framework leveraging multi-agent LLM architectures and RAG.By abstracting complex GPU-accelerated training pipelines into schema-validated YAML workflows, AURA reliably automates the design and refinement of multi-stage curricula, enabling policy training with iterative improvement.Empirical evaluations demonstrate that AURA outperforms baseline methods, achieving a higher generation success rate and superior policy performance across locomotion and manipulation tasks, and effective zero-shot hardware deployment.Real-world validation on a custom humanoid robot highlights AURA's capability to autonomously produce highly robust policies directly from user-defined prompts.The resulting controller demonstrated stable outdoor locomotion, gait tracking, and disturbance rejection, including recovery from lateral perturbations and vertical drops.Overall, AURA advances the development of scalable, generalizable prompt-to-policy frameworks, demonstrating the ability to substantially reduce the manual effort required in reward design while generating complex, high-performing curricula.</p>
<p>APPENDIX</p>
<p>A. Implementation and Testing Details 1) Training via Proximal Policy Optimization (PPO):</p>
<p>We train the control policy with PPO, which constrains the policy update to prevent destructive parameter jumps.Let
r t (θ) = π θ a t | s t π θold a t | s t
denote the probability ratio between the new and old policies, and let A t be the generalized advantage estimate at timestep t.The clipped surrogate objective is
L CLIP (θ) = E t min r t (θ) A t , clip r t (θ), 1 − ϵ, 1 + ϵ A t ,
where ϵ is the clipping parameter (we use ϵ = 0.2 unless otherwise noted).During training, we maximize L CLIP with Adam, using entropy regularization for exploration and a value-function loss with coefficient c v .</p>
<p>2) Score Calculation: During one evaluation run, we launch N = 1024 parallel environments and roll each for a horizon of T max = 3000 simulation steps (or until failure).All three metrics are normalized to [0, 1] for direct comparability.</p>
<p>a) Survival Score.:
Let T i ∈ [1,
T max ] denote the number of steps survived by environment i.The Survival Score is the mean fractional episode length
S surv = 1 N N i=1 T i T max .
b) Linear-Velocity Tracking Score.: For step t in environment i let v cmd t,i ∈ R 2 be the commanded planar velocity and v loc t,i ∈ R 2 the robot's actual planar COM velocity expressed in the local frame.Define the squared tracking error
e t,i = v cmd t,i − v loc t,i2 2
. The YAML exponential decay entry with σ = 0.1 corresponds to the per-step reward
r lin t,i = exp −e t,i /(2σ 2 ) , σ = 0.1.
Aggregating over time and averaging across the batch yields the normalized Linear-Velocity Tracking Score
S lin = 1 N N i=1 1 T max Ti t=1 r lin t,i.
c) Summary.: The triplet S surv , S lin , S air captures robustness (survival), command-following fidelity, and gait coordination, respectively, and forms the basis of all quantitative comparisons in the main paper.</p>
<p>3) AURA Task Prompts for Training Launch: These five user prompts were used as task inputs to AURA for evaluating its curriculum training-launch success rate across the custom humanoid and Berkeley Humanoid embodiments.For baseline comparisons, we used the prompts provided in each baseline's open-source repository to generate their respective training files.</p>
<p>Task 1: Robust Bipedal Walking with Perturbation Resilience I want to use a staged approach to train a humanoid robot for advanced locomotion with external perturbations.I want to deploy this policy onto hardware and walk outside, and I want the steps to be even (between the right and left → legs) and smooth.I want the walking to be very robust to both uneven terrain and perturbations.I want the training to have equal capabilities for forward and backwards walking, with an absolute max speed being 0.5m/ → s and max yaw being 0.5rad/s.You must generate AT LEAST 2 stages for this task.</p>
<p>Task 2: Terrain-Adaptive Walking I want to use a multi-stage curriculum to train a humanoid robot to walk over irregular terrain, such as small rocks and → low barriers.The robot must learn to lift its feet high enough to avoid tripping and maintain a steady gait while stepping over → obstacles of varying height (up to 0.02m).The policy should be deployable outdoors and remain balanced when landing on slightly angled or unstable surfaces.I want both forward and backward walking to be supported, with even step timing and foot clearance.You must generate AT LEAST 2 stages for this task.</p>
<p>Task 3: Precision Jumping Between Platforms</p>
<p>I want to use a staged approach to train a humanoid to jump onto and off of elevated platforms.The policy should support both single jumps (from ground to platform) and double-jumps (from one platform to another).The target jump height is 0.05m with target air time of 0.5s.I want landing posture and knee angle to remain stable, and I want the robot to absorb impacts smoothly.The final policy should transfer to hardware and be tested over rigid and slightly deformable platforms.You must generate AT LEAST 2 stages for this task.</p>
<p>Task 4: Rhythmic Forward-Backward Hopping I want to train a humanoid robot to perform continuous forward-backward hopping in a rhythmic, energy-efficient manner.The hops should alternate directions every few steps and maintain even left-right force distribution.I want the robot to be robust to mild perturbations during flight and landing.Deployment should be feasible on a physical robot outdoors, with stability maintained on moderately uneven terrain.Hop height should range between 0.05-0.1 meters with a frequency of ˜1.5 Hz.You must generate AT LEAST 2 stages for this task.</p>
<p>Task 5: Stable Lateral Walking with Perturbation Handling</p>
<p>I want to use a staged curriculum to train a humanoid robot to perform lateral (sideways) walking in both directions.The walking should be smooth and balanced, with equal step distances between the left and right legs.The policy should be robust to minor terrain irregularities and moderate lateral perturbations.Maximum lateral velocity should be capped at +-0.3 m/s, and yaw rotation should be minimized during side-stepping.I want the final policy to be deployable on hardware and capable of sustained lateral walking in outdoor environments.You must generate AT LEAST 2 stages for this task.</p>
<p>4) Training-Launch Comparison Details:</p>
<p>In the following section, we provide a more detailed description of the environments used to evaluate the CurricuLLM and Eureka training-launch success rates.All environments were evaluated over 100 training launches.a) CurricuLLM: CurricuLLM's training-launch success rate was evaluated on their Berkeley Humanoid environment from [24], with all environment scripts and tasks taken directly from their open-source repository.</p>
<p>b) Eureka: Eureka's training-launch success rate was evaluated on their only legged robot environment, ANYmal, across 100 training-launch attempts, successfully launching training 12 times.We also tested training-launch success across all embodiments provided in their open-source repository, which include the Gymnasium environments Shadow Hand, Franka Emika Panda, Ant, Humanoid, and Cartpole, as well as other robot environments such as ANYmal, Allegro, Ball Balance, and Quadcopter.</p>
<p>B. AURA LLM Agent Prompt Templates</p>
<p>The prompts below define the instruction templates used by AURA's LLM agents throughout the curriculum generation and training process.These templates structure how input data, such as user task prompts, stage descriptions, and past training artifacts, are transformed into schema-compliant outputs like workflows, configuration files, and reward functions.Placeholders in the form of <INSERT ...> denote dynamic inputs that are automatically populated by the AURA framework at runtime with the appropriate content, ensuring that each prompt remains generalizable while retaining full contextual relevance for the target task.</p>
<p>1) Curriculum LLM:</p>
<p>You are an expert in reinforcement learning, CI/CD pipelines, GitHub Actions workflows, and curriculum design.Your task → is to generate a complete curriculum with clearly defined steps for a multi-stage (staged curriculum learning) → training process for a humanoid robot in simulation.</p>
<p>** MOST IMPORTANT DIRECTIONS: ** ** <INSERT_TASK_PROMPT_HERE> ** Determine the number of stages based on the complexity of the task described.</p>
<p>You are to generate as follows:</p>
<p>1.A GitHub Actions workflow file that orchestrates the entire staged training process.2. For each training stage that appears in the workflow, a separate text file containing in-depth, rich details that → describe the objectives, expected changes to reward files, configuration file modifications, and overall → curricular rationale.</p>
<p>-------------------------------------------------** Important Guidelines: ** -IMPORTANT!The example baseline content parameters comes from a vector database of a past training workflow that's → similar to the current task.Update them to fit the user's task more.-IMPORTANT!Here is the EXPERT evaluation of the example training workflow results: <INSERT_EVALUATION_HERE>.</p>
<p>-Use this evaluations by the EXPERT to update the parameters.-** Workflow File Generation: ** Generate a workflow file named 'generated_workflow.yaml' that orchestrates the staged training process.This file must → :</p>
<p>-Begin with the baseline structure provided via the placeholder '<INSERT_WORKFLOW_YAML_HERE>' and then be modified to support multiple training stages with explicit jobs for training → and feedback (if feedback is needed).-Use the example reward file (provided via '<INSERT_REWARD_YAML_HERE>') and example config file (provided via '&lt; → INSERT_CONFIG_YAML_HERE&gt;') as context.-Use the example randomize file (provided via '<INSERT_RANDOMIZE_YAML_HERE>') as context to understand the domain → randomization parameters for the scene.-Remember the workflow should use the same generated config, reward, and randomize that it defines in the stage → description for training.</p>
<p>-** Detailed Stage Description Files: ** In addition to the workflow file, for every stage that is generated in the workflow, you must output a corresponding → text file (named 'generated_stage{X}_details.txt',where '{X}' is the stage number) that contains very rich, in → -depth details.Each of these text files must include: -A clear description of the training objective for that stage.</p>
<p>-Precise descriptions of modifications to the config environment disturbances.</p>
<p>-Explicit details on the expected modifications to the reward file.</p>
<p>-A precise explanation of the configuration changes.</p>
<p>-A discussion on the terrain/environment context (e.g., flat_scene, mixed_scene, height_scene).</p>
<p>-Which gate to use (walk or jump) -How this stage fits into the overall curriculum progression -Specify if the stage is starting from new or resuming from a checkpoint ------------------------------------------------ ------------------------------------------------* 2) RAG: These prompts are used within the RAG Block to query, retrieve, and select suitable past workflows and training files to be used as context for the current task.</p>
<p>a) VDB Query LLM:</p>
<p>You are an expert in reinforcement learning for robotics.You are given a high-level task description related to → training a robot policy.</p>
<p>Your job is to generate a short and precise ** natural language query ** that summarizes the core attributes of the task.→ This query will be used to retrieve similar configurations, rewards, and workflows from a vector database.You are an expert in reinforcement learning and robot curriculum design.</p>
<p>You are provided with:</p>
<p>-A high-level training task.</p>
<p>-A collection of YAML files from various past runs (each associated with a run ID).These include multiple stages of ' → reward', 'config', and 'randomize' YAMLs, as well as one or more 'workflow' YAMLs per run.</p>
<p>Your job is to ** select only the files needed ** to best support the new multi-stage workflow generation.You should pick → : -Exactly one 'workflow.yaml'file.</p>
<p>-One 'reward.yaml','config.yaml',and 'randomize.yaml'file for ** each ** stage of the task (e.g., stage 1-3).</p>
<p>Output a JSON object mapping descriptive keys (e.g., 'workflow', 'reward_stage1', etc.) to the exact filenames you want → to keep (from the file list provided).All other files will be deleted.</p>
<p>------------------------## TASK DESCRIPTION ** <INSERT_TASK_PROMPT_HERE> ** ------------------------## HERE ARE THE EVALUATIONS FOR THE CANDIDATE RUNS:## <INSERT_EVALUATIONS_HERE> ** Pick the runs whose evaluations look the most promising for the task.** ------------------------</p>
<h2>AVAILABLE FILES (filename -&gt; truncated preview) <INSERT_EXAMPLES_HERE> ------------------------</h2>
<p>Your output must be in the following format: '''json { "workflow": "run123_workflow.yaml","reward_stage1": "run123_reward_stage1.yaml","config_stage1": "run123_config_stage1.yaml","randomize_stage1": "run123_randomize_stage1.yaml", } ** If the run you are choosing has more stages, also put them into the output json block.** Do NOT include any other text outside the fenced JSON block.</p>
<p>3) Stage-Level LLM:</p>
<p>You are an expert in reinforcement learning, CI/CD pipelines, GitHub Actions workflows, and curriculum design.</p>
<p>Notes:</p>
<p>-Input values to a reward function when setting a variable (e.g.lift_thresh: "0.2"), has to be a string.For example, → in the lift_thresh case, even though it is setting lift_thresh to be a float 0.2, you still need to set it with a string.</p>
<p>-MAKE SURE to understand the variables, their types, and their shape for using them in reward functions.</p>
<p>-IN PARTICULAR, double check for vectors the shapes much so there is no error!-<strong> IMPORTANT!REMEMBER!THIS IS JAX!You must use jax functions, conditions, arrays, etc for everything you generate in → the reward function.</strong> -Expressions like "vector: "rot_up -[0, 0, 1]" are not valid, you must use "rot_up -jp.-Choose 'scene_file' from the options (flat_scene, mixed_scene, and height_scene) according to the stage difficulty.</p>
<p>-Add inline comments next to any parameter changes explaining your reasoning.</p>
<p>-The 'randomize_config_path' should be the corresponding generated randomize yaml file path for each stage.------------------------------------------------ ----------------------------------------------- ------------------------------------------------* Do not output any text or explanation outside these YAML blocks.</p>
<hr />
<p>4) Feedback LLM:</p>
<p>Feedback integration for curriculum learning: This is currently the Stage {current_stage} training feedback process based on the workflow.Based on the previous training step's reward metrics and configurations, update the reward and configuration files for → improved performance.The metrics either 1) give information about the training process (e.g.entropy loss, policy loss, etc.), or 2) give the value each reward term contributes to the overall reward; the name of the eval/ * for the reward matches → exactly with that of the reward term in the reward file's functions.Look for specific metrics, such as the loss and penalty values, to guide the updates (e.g. if loss numbers are too → volatile, reduce the learning rate).</p>
<p>The</p>
<p>I want to use a staged approach to train a humanoid robot for omnidirectional locomotion with external perturbations ...High-Level Planner (GPT o4mini)Physical Specifications -Total height: ~0.6m ... Actuation &amp; Mechanisms -Actuation mode: Primarily position-controlled with tunable <code>kp</code>, `kv ...</p>
<p>Fig. 2 :
2
Fig. 2: An overview of the AURA curriculum generation and policy training framework.</p>
<p>Fig. 3 :
3
Fig. 3: Survival and linear velocity tracking scores across iterations to evaluate locomotion policy quality on a custom humanoid.The plots show the policy quality improvements of AURA over five iterations compared to MuJoCo Playground's expert designed Berkeley Humanoid reward and CuricuLLM's reported results in Isaac Lab.AURA Blind generates rewards from scratch (VDB is initialized as empty) and AURA Tune modifies and improves an existing reward designed for another embodiment (VDB is initialized with MuJoCo Playground's Berkeley Humanoid expert human rewards, domain randomizations, and training configuration).</p>
<p>-** Additional Context: ** -The robot and training environment context is provided via <INSERT_ROBOT_DESCRIPTION_HERE> -The generated workflow file must include explicit, detailed inline comments.-IMPORTANT!The number of stages in the workflow and the stage descriptions generated should match exactly -</p>
<p>Your output should: -Be a ** single sentence ** .-Focus on ** task type ** , ** terrain ** , ** number of curriculum stages ** , and any ** specific tuning goals ** (e.g., push → resistance, fine-tuning, checkpoint reuse).-Avoid code, markdown, or explanations.Task Description: <INSERT_TASK_PROMPT_HERE> Output: (One-line query only) b) Selector LLM:</p>
<p>Your task → is to generate the configuration files for a specific stage of a multi-stage (staged curriculum learning) → training process for a humanoid robot in simulation.For this prompt, you are generating files for Stage {X} ( → for example, Stage 1, Stage 2, etc.).Use the inline comments from the workflow file (provided in &lt; → INSERT_WORKFLOW_YAML_HERE&gt;) for Stage {X} to guide your modifications.<INSERT_TASK_PROMPT_HERE>-------------------------------------------------<strong> THESE ARE THE MOST IMPORTANT DIRECTIONS TO FOLLOW!Stage {X} Description: ** <INSERT_STAGE_DESCRIPTION_HERE> -------------------------------------------------</strong> All reward, config, and randomize example files are selected from a vector database.Each of the examples have been → selected by a higher-level LLM due to their fitting parameters for the task.<strong> ** If you believe the provided files from the database are sufficiently good for the training of the given stage, you may → choose to use the same parameters ** For the reward file: -Use the content from <INSERT_REWARD_YAML_HERE> as a starting point.-</strong> Preserve all reward function keys ** from the baseline.You must include every reward term from the original file in → this stage's reward file.-You may adjust the scalar weight (value) of each reward term to suit the stage.For keys that are not relevant at this → stage, it is acceptable to set their values to 0.0 (effectively disabling them) while preserving the structure.-If additional reward terms are necessary to support the stage goal, you may add new ones.New terms must use only the → allowed function types listed below.-<strong> Allowed Functions (only use these functions when building reward expressions): ** <INSERT_SCHEMA_REWARD_EXPRESSIONS> The top-level key in the reward file must be 'reward:'.-Add inline comments next to any changes, new reward terms, or adjustments explaining how they support the stage goal.</strong> Context of variables provided for reward functions based on the environment: ** <INSERT_REWARD_VARS_HERE> -<strong> Only the variables defined here can be used in the reward function calculations!</strong> ** Here is an in depth example and explanation for creating reward functions: ** <INSERT_REWARD_EXAMPLE_HERE> -These examples show the format of how to build our yaml based reward functions, and its equivalent in python.You have → to use this style closely for new reward generation.</p>
<p>array([0.0,0.0, 1.0])" -USE VECTOR OPERATIONS!: In jax, you cannot use conditional statements like and, or, you have to use jax operations → such as jp.where, or use bitwise such as &amp;, |, etc. -<strong> You do not need to calculate a total reward!Just define individual reward functions, aggregating the rewards is → handled elsewhere.</strong> -------------------------------------------------For the configuration file: -<strong> Make sure to follow the exact structure of the config file, including the top-level keys.One of the top level → classes is 'environment:', don't forget it!</strong> -Use the current baseline structure from <INSERT_CONFIG_YAML_HERE> as a reference for the structure and expected → parameters.-<strong> Keep the structure of the config file.</strong> -<strong> Do not add or remove any parameters ** ; only adjust the values to support the staged curriculum learning process.-Adjust trainer parameters (e.g., learning rate, batch size, etc.) to support training stability as the curriculum → advances.-</strong> resume_from_checkpoint: ** This should be set to false if a new staged training is being done.If continueing from a → previous checkpoint, this flag should be set to true.-refer to the stage description for which option to choose.-Ensure that 'batch_size' and 'num_envs' remain powers of 2. -The network parameters must remain consistent across all stages.</p>
<p>-** Baseline Randomization File for Context: ** <INSERT_RANDOMIZE_YAML_HERE> -</p>
<p>-<strong> Instructions for the randomize.yamlfile: ** -</strong> Preserve the overall file structure and all keys from the baseline randomization file.<strong> -The randomize file contains parameters for domain randomization, including: -</strong> geom_friction: ** Randomizes friction properties for all geometry objects.-<strong> actuator_gainprm: ** Randomizes actuator gain parameters.-</strong> actuator_biasprm: ** Randomizes actuator bias parameters.-<strong> body_ipos: ** Randomizes body initial positions (center of mass positions).-</strong> geom_pos: ** Randomizes the positions of specific geometries.-<strong> body_mass: ** Randomizes the body mass (scaling factors or additional offsets).-</strong> hfield_data: ** Randomizes the heightfield (terrain) data values.-Read the inline comments for each parameters in the randomize.yamlexample for details on what the parameters do.-You may adjust the numeric parameter values (e.g., the min and max values in the uniform distributions) to better → support the training stage's objectives while keeping the structure intact.-For parameters that are modified, add inline comments next to the changed values explaining how the adjustments → support the stage goals (e.g., improved robustness to disturbances, adapting friction to better simulate → challenging terrain, etc.).-The top-level key of the file must remain "randomization:".-<strong> The randomization file path must match the one specified in the config yaml.</strong> -</p>
<p>TABLE I :
I
Policy</p>
<p>[24]uation across metrics.Episode survival length and linear velocity tracking are used to evaluate a velocity command following task on the Berkeley Humanoid.The success rate of pushing cubes is evaluated on the UR5e enviorment.<em>CurricuLLM'sBerkeleyHumanoid and Fetch-Push results are reported in the paper[24].</em>*MuJoCo Playground's Pushing Cube Success is reported using Franka Emika Panda rewards on the UR5e embodiment, which shouldn't be expected to be successful.AURA adapts the Franka expert reward and training configuration into an effective curriculum for the UR5e.</p>
<p>AURA AURA w/o schema AURA w/o VDB AURA single-agent CurricuLLM *
Eureka  *  *Training-launch Success Rate99 %47 %38 %7 %31 %12 / 49 %</p>
<p>TABLE II :
II
Training-launch-success-rate comparing AURA and its ablated variants.All evaluations are conducted with GPT-4.1, as the original models used in the baselines are deprecated at the time of assessment.* CurricuLLM is evaluated on generating rewards for Berkeley Humanoid locomotion.Eureka's 12% is evaluated on generation for their ANYmal task, which is most similar in complexity to humanoid robot tasks.Eureka's generation success rate across all available embodiments in their examples is 49% with simpler tasks generating more successfully.</p>
<ul>
<li>
<p>*</p>
</li>
<li>
<p>Output Format: ** ** THIS IS THE MOST IMPORTANT PART! YOU MUST FOLLOW THESE DIRECTIONS EXACTLY!Return your output as separate YAML blocks → (and nothing else) in the following format: **
file_name: "generated_workflow.yaml"file_path: "../workflows/generated_workflow.yaml"content: |# [The updated workflow.yaml content here with inline comments]file_name: "generated_stage1_details.txt"file_path: "../prompts/tmp/generated_stage1_details.txt"content: |# [The in-depth description for Stage 1 with explicit detail on training objectives, reward configuration adjustments,→ and other parameters]</p>
</li>
<li>
<p>Output Format: ** ** THIS IS THE MOST IMPORTANT PART! YOU MUST FOLLOW THESE DIRECTIONS EXACTLY!Return your output as separate YAML blocks → (and nothing else) in the following format: **
file_name: "generated_reward_stage{X}.yaml"file_path: "../rewards/generated_reward_stage{X}.yaml"content: |# [The complete reward file for Stage {X} with inline comments]
file_name: "generated_config_stage{X}.yaml"file_path: "../configs/generated_config_stage{X}.yaml"content: | # [The complete configuration file for Stage {X} with inline comments] file_name: "generated_randomize_stage{X}.yaml"file_path: "../randomize/generated_randomize_stage{X}.yaml"content: | # [The complete randomization file for Stage {X} with inline comments]</p>
</li>
</ul>
<p>above feedback prompt, together with the next stage's description and all the stage's metrics log file, is prepended to the Per-Stage LLM prompt for use by the Feedback LLM.: scene_file: "../../../og_bruce/flat_scene.xml"# Always flat terrain for this curriculum reward_config_path: "../rewards/generated_reward_stage1.yaml"# Path to Stage 1 reward file obs_noise: 0.03 # Increased observation noise for sensorimotor learning imu_disturbs: true # Enable IMU disturbances to enforce robust policy learning init_rand: true # Extensive randomization in joint angles, body position, and mass big_min_kick_vel: 0.05 # Enable full big kick perturbations for robustness big_max_kick_vel: 0.12 # Enable full big kick perturbations for robustness big_kick_interval: 80 # Randomized interval for big kicks (see randomization file for full range) small_min_kick_vel: 0.01 # Enable small kicks for continuous disturbance small_max_kick_vel: 0.03 # Enable small kicks for continuous disturbance small_kick_interval: 10 # Randomized interval for small kicks (see randomization file for full range) fixed_command: false # Commands randomized for full velocity/yaw tracking command_lin_vel_x_range: [-0.5, 0.5] # Full forward/backward walking speed range command_lin_vel_y_range: [0.0, 0.0] # No lateral walking required command_ang_vel_yaw_range: [-0.5, 0.5] # Full yaw tracking range command_stand_prob: 0.15 # Robot is rarely asked to stand still; mostly active walking cutoff_freq: 3.0 deadband_size: 0.01 low_cmd_boost_scale: 1.5 gait_frequency: [2.0, 2.25] # Less range for better convergence and tracking gaits: ["walk"] # Use walk gait for consistency foot_height_range: [0.03, 0.05] # Smaller swing heights for better conergence of feet phase tracking
C. AURA Outputsa) Configuration YAML:render:resolution: [640, 480]randomize_seed: 25n_steps: 3000view: "track_com"fps: nullplot_actions: trueplot_observations: trueplot_rewards: truestatic_actions:10: 011: 012: 013: 014: 015: 0trainer:num_timesteps: 400_000_000 # Full stage budget: all training in a single phase (max allowed)num_evals: 13 # Evaluations every ˜30M steps for robust progress monitoringreward_scaling: 1episode_length: 3000 # Longer episodes for sustained walking under disturbancenormalize_observations: trueaction_repeat: 1unroll_length: 20num_minibatches: 32num_updates_per_batch: 4
environmentdiscounting: 0.97 # Higher discount for long-term stability and recovery</p>
<p>A. Zhu is with Department of Computer Science and Electrical Engineering
, 2 Y. Tanaka and D. Hong are with Department of Mechanical and Aerospace Engineering, UCLA, Los Angeles, CA,
USA.3 A. Goldberg is with Department Electrical Engineering and Computer Science, UC Berkeley, Berkeley, CA, USA. {alvister88, yusuketanaka, dennis-hong}@g.ucla.edu. apgoldberg@berkeley.edu. * denotes equal contribution.
-type: "sum_square" parameters: vector: "command_xy-local_vel_xy" output: "error" -type: "exponential_decay" parameters: error: "error" sigma: 0.09 # Tighter sigma for precise bidirectional velocity tracking; enforces true walking/stepping combination:type: "last" scale: 1.5 # Full reward for tracking commanded velocity (serves as survival reward as well) default_reward: 0.0 tracking_ang_vel: inputs: command_yaw: "command[2]" base_ang_vel: "base_ang_vel" evaluations:-type: "norm_L2" parameters: vector: "command_yaw-base_ang_vel" output: "error" -type: "exponential_decay" parameters: error: "error" sigma: 0.1 # Tight sigma for accurate yaw tracking; prevents shuffling and enforces turning combination:type: "last" scale: 1.0 # Full reward for yaw tracking default_reward: 0.0 lin_vel_z: inputs: vel_z: "xd.vel[0, 2]" evaluations:-type: "quadratic" parameters: value: "vel_z" weight: 1.0 scale: 0.0 # Penalize vertical velocity not used default_reward: 0.0 ang_vel_xy: inputs: ang_vel_xy: "xd.ang[0, :2]" evaluations:-type: "sum_square" parameters: vector: "ang_vel_xy" scale: -0.15 # Stronger penalty for roll/pitch instability under perturbations default_reward: 0.0 orientation: inputs: rot_up_xy: "rot_up[0:2]" evaluations:-type: "sum_square" parameters: vector: "rot_up_xy" scale: -2.0 # Strong penalty for non-upright posture (enforces survival under heavy disturbance) default_reward: 0.0 torques: inputs: torques: "qfrc_actuator" evaluations:-type: "norm_L2" parameters: vector: "torques" -type: "norm_L1" parameters: vector: "torques" combination:type: "sum" scale: -0.00004 # Slightly increased penalty for excessive actuator effort default_reward: 0.0 action_rate: inputs: action: "action" last_act: "last_act" evaluations:-type: "sum_square" parameters: vector: "action -last_act" scale: -0.01 # Slight increase to encourage smoother actions under disturbance default_reward: 0.0 feet_air_time: inputs: feet_air_time: "feet_air_time" first_foot_contact: "first_foot_contact" lift_thresh: "0.2" command_norm: "command_norm" evaluations:-type: "weighted_sum" parameters: values: "(feet_air_time -lift_thresh) * first_foot_contact" weights: 1.0 output: "rew_air_time" -type: "binary" parameters: condition: "command_norm &gt; 0.05" reward_value: "rew_air_time" else_value: 0.0 scale: 2.0 # Enabled: rewards proper stepping and air time during walking default_reward: 0.0 stand_still: inputs: commands_norm: "commands_norm" joint_angles: "joint_angles" default_pose: "default_pose" evaluations:-type: "norm_L1" parameters: vector: "joint_angles -default_pose" output: "norm_joint" -type: "binary" parameters: condition: "commands_norm &lt; 0.01" reward_value: "norm_joint" else_value: 0.02 scale: -0.35 # Enabled: penalizes deviation from rest pose only when robot is commanded to stand still default_reward: 0.0 termination: inputs: done: "done" step: "step" evaluations:-type: "binary" parameters: condition: "done &amp; (step &lt; 500)" reward_value: -1.0 else_value: 0.0 scale: 1.0 # Maintain strong penalty for early episode termination/falling default_reward: 0.0 foot_slip: inputs: contact: "foot_contact" foot_linear_velocity: "feet_site_linvel[:, 0:2]" foot_angular_velocity: "feet_site_angvel" evaluations:-type: "norm_L2" parameters: vector: "foot_linear_velocity" output: "linear_vel_norm" -type: "norm_L2" parameters: vector: "foot_angular_velocity" output: "angular_vel_norm" combination:type: "weighted_sum" parameters: vectors: ["linear_vel_norm * contact", "angular_vel_norm * contact"] weights: [1.0, 1.0] scale: -0.2 # Decreased penalty; enforces proper ground contact and reduces unnecessary foot motion default_reward: 0.0 feet_phase: inputs: foot_z: "feet_pos[:, 2]" rz: "rz" first_foot_contact: "first_foot_contact" evaluations:-type: "sum_square" parameters: vector: "(foot_z -rz)" output: "phase_err" -type: "exponential_decay" parameters: error: "phase_err" sigma: 0.001 # Ultra-tight sigma for phase adherence; enforces symmetric, phase-locked walking combination:type: "last" scale: 1.2 # More reward to encourge closer tracking of the feet phase default_reward: 0.0 feet_clearance: inputs: foot_linear_velocity: "feet_site_linvel" foot_z: "feet_site_pos[:,2]" max_foot_height: "max_foot_height" evaluations:-type: "norm_L2" parameters: vector: "foot_linear_velocity[..., :2]" output: "vel_norm" -type: "absolute_difference" parameters: value1: "foot_z" value2: "max_foot_height" output: "delta_z" -type: "weighted_sum" parameters: values: "delta_z * vel_norm" weights: 1.0 output: "clearance_cost" combination:type: "last" scale: 0.02 # Enabled: encourages sufficient swing foot clearance, important for robust stepping default_reward: 0.0 c) Randomization YAML: -type: "sum_square" parameters: vector: "command_xy-local_vel_xy" output: "error" -type: "exponential_decay" parameters: error: "error" sigma: 0.20 # Stricter velocity tracking (was 0.25); better command following for both forward/backward walking combination:type: "last" scale: 3.0 default_reward: 0.0 tracking_yaw: inputs: command_yaw: "command[2]" base_ang_vel: "base_ang_vel" evaluations:-type: "norm_L2" parameters: vector: "command_yaw-base_ang_vel" output: "error" -type: "exponential_decay" parameters: error: "error" sigma: 0.16 # Stricter yaw tracking (was 0.2); ensures accurate directional control combination:type: "last" scale: 2.0 default_reward: 0.0 feet_lift_time: inputs: feet_air_time: "feet_air_time" first_foot_contact: "first_foot_contact" lift_thresh: "0.2" command_norm: "command_norm" evaluations:-type: "weighted_sum" parameters: values: "(feet_air_time -lift_thresh) * first_foot_contact" weights: 1.0 output: "rew_air_time" -type: "binary" parameters: condition: "command_norm &gt; 0.05" reward_value: "rew_air_time" else_value: 0.0 combination: type: "last" scale: 1.6 default_reward: 0.0 feet_swing_height: inputs: swing_peak: "swing_peak" first_foot_contact: "first_foot_contact" max_foot_height: "max_foot_height" evaluations:-type: "sum_square" parameters: vector: "(swing_peak / max_foot_height -1.0) * first_foot_contact" output: "height_err" combination:type: "last" scale: 0.0 default_reward: 0.0 symmetry: inputs: joint_angles: "joint_angles" evaluations:-type: "norm_L1" parameters: vector: "joint_angles[0:12] -joint_angles[12:24]" combination:type: "last" scale: -0.18 # Penalizes asymmetric joint movement; crucial for symmetric bidirectional walking default_reward: 0.0 foot_scuff: inputs: contact: "first_site_contact" foot_linear_velocity: "feet_site_linvel" foot_angular_velocity: "feet_site_angvel" evaluations:-type: "norm_L2" parameters: vector: "foot_linear_velocity" output: "linear_vel_norm" -type: "norm_L2" parameters: vector: "foot_angular_velocity" output: "angular_vel_norm" combination: type: "weighted_sum" parameters: vectors: ["linear_vel_norm * contact", "angular_vel_norm * contact"] weights: [1.0, 1.0] scale: -0.018 default_reward: 0.0 linear_velocity_z: inputs: vel_z: "xd.vel[0, 2]" evaluations:-type: "quadratic" parameters: value: "vel_z" weight: 1.0 scale: -0.08 default_reward: 0.0 angular_velocity_xy: inputs: ang_vel_xy: "xd.ang[0, :2]" evaluations:-type: "sum_square" parameters: vector: "ang_vel_xy" scale: -0.15 default_reward: 0.0 orientation_penalty: inputs: rot_up_xy: "rot_up[0:2]" evaluations:-type: "sum_square" parameters: vector: "rot_up_xy" scale: -3.0 default_reward: 0.0 penalty_torques: inputs: torques: "qfrc_actuator" evaluations:-type: "norm_L2" parameters: vector: "torques" -type: "norm_L1" parameters: vector: "torques" combination:type: "sum" scale: -0.0005 default_reward: 0.0 action_smoothness: inputs: action: "action" last_act: "last_act" evaluations:-type: "sum_square" parameters: vector: "action -last_act" scale: -0.07 default_reward: 0.0 low_command_stand: inputs: commands_norm: "commands_norm" joint_angles: "joint_angles" default_pose: "default_pose" evaluations:-type: "norm_L1" parameters: vector: "joint_angles -default_pose" output: "norm_joint" -type: "binary" parameters: condition: "commands_norm &lt; 0.05" reward_value: "norm_joint" else_value: 0.02 scale: -0.4 default_reward: 0.0 falling_penalty: inputs: done: "done" step: "step" evaluations:-type: "binary" parameters: condition: "done &amp; (step &lt; 1000)" reward_value: -1.0 else_value: 0.0 scale: 2.0 default_reward: 0.0 alive: inputs: {} evaluations: -type: "binary" parameters: condition: "True" reward_value: 1.0 else_value: 0.0 scale: 1.0 default_reward: 0.0 e) Randomization YAML: -target: ALL distribution: uniform: minval: [-25, 0, 0, 0, 0, 0, 0, 0, 0, 0] maxval: [25, 0, 0, 0, 0, 0, 0, 0, 0, 0] operation: add # Wide gain range for actuator robustness (sim2real transfer) actuator_biasprm:-target: ALL distribution: uniform: minval: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] maxval: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] operation: add body_ipos:-
Curriculum learning for reinforcement learning domains: A framework and survey. S Narvekar, B Peng, M Leonetti, J Sinapov, M E Taylor, P Stone, Journal of Machine Learning Research. 211812020</p>
<p>Reinforcement learning for versatile, dynamic, and robust bipedal locomotion control. Z Li, X B Peng, P Abbeel, S Levine, G Berseth, K Sreenath, The International Journal of Robotics Research. 4452025</p>
<p>Anymal parkour: Learning agile navigation for quadrupedal robots. D Hoeller, N Rudin, D Sako, M Hutter, arXiv:2306.14874[cs.RO]2023</p>
<p>Fld: Fourier latent dynamics for structured motion representation and learning. C Li, E Stanger-Jones, S Heim, S Kim, arXiv:2402.13820[cs.LG]2024</p>
<p>Learning curriculum policies for reinforcement learning. S Narvekar, P Stone, arXiv:1812.002852018arXiv preprint</p>
<p>Learning agile soccer skills for a bipedal robot with deep reinforcement learning. T Haarnoja, Science Robotics. 98980222024</p>
<p>Real-world humanoid locomotion with reinforcement learning. I Radosavovic, T Xiao, B Zhang, T Darrell, J Malik, K Sreenath, Science Robotics. 98995792024</p>
<p>Learning to walk in minutes using massively parallel deep reinforcement learning. N Rudin, D Hoeller, P Reist, M Hutter, arXiv:2109.11978CoRR. 2109.11978, 2021</p>
<p>Learning torque control for quadrupedal locomotion. S Chen, B Zhang, M W Mueller, A Rai, K Sreenath, arXiv:2203.05194[cs.RO]2023</p>
<p>Combining reward shaping and curriculum learning for training agents with high dimensional continuous action spaces. S Jang, M Han, 2018 International Conference on Information and Communication Technology Convergence (ICTC). IEEE2018</p>
<p>Robots learn increasingly complex tasks with intrinsic motivation and automatic curriculum learning: Domain knowledge by emergence of affordances, hierarchical reinforcement and active imitation learning. S M Nguyen, N Duminy, A Manoury, D Duhaut, C Buche, KI-Künstliche Intelligenz. 352021</p>
<p>Causally aligned curriculum learning. M Li, J Zhang, E Bareinboim, arXiv:2503.167992025arXiv preprint</p>
<p>Diffusion-based curriculum reinforcement learning. E Sayar, G Iacca, O S Oguz, A Knoll, Advances in Neural Information Processing Systems. 202437617</p>
<p>Curriculum learning: A survey. P Soviany, R T Ionescu, P Rota, N Sebe, International Journal of Computer Vision. 13062022</p>
<p>Learning to utilize shaping rewards: A new approach of reward shaping. Y Hu, Advances in Neural Information Processing Systems. 202033</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, arXiv:2212.068172022arXiv preprint</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. M Ahn, arXiv:2204.016912022arXiv preprint</p>
<p>Code as policies: Language model programs for embodied control. J Liang, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Prompt, plan, perform: Llm-based humanoid control via quantized imitation learning. J Sun, Q Zhang, Y Duan, X Jiang, C Cheng, R Xu, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE202416242</p>
<p>Ros-llm: A ros framework for embodied ai with task feedback and structured reasoning. C E Mower, arXiv:2406.197412024arXiv preprint</p>
<p>Embodied large language models enable robots to complete complex tasks in unpredictable environments. R Mon-Williams, G Li, R Long, W Du, C G Lucas, Nature Machine Intelligence. 2025</p>
<p>Gemini robotics: Bringing ai into the physical world. G R Team, arXiv:2503.200202025arXiv preprint</p>
<p>Eureka: Human-level reward design via coding large language models. Y J Ma, arXiv:2310.129312023arXiv preprint</p>
<p>Curricullm: Automatic task curricula design for learning complex robot skills using large language models. K Ryu, Q Liao, Z Li, P Delgosha, K Sreenath, N Mehr, IEEE International Conference on Robotics and Automation (ICRA). 2025</p>
<p>Dreureka: Language model guided sim-to-real transfer. Y J Ma, Robotics: Science and Systems (RSS). 2024</p>
<p>Large language model-driven curriculum design for mobile networks. O Erak, O Alhussein, S Naser, N Alabbasi, D Mi, S Muhaidat, 2024 IEEE/CIC International Conference on Communications in China (ICCC). IEEEAug. 2024</p>
<p>Q Liao, B Zhang, X Huang, X Huang, Z Li, K Sreenath, arXiv:2407.21781Berkeley humanoid: A research platform for learning-based control. 2024arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. B Zitkovich, Conference on Robot Learning. PMLR2023</p>
<p>0 : A vision-language-action flow model for general robot control. K Black, arXiv:2410.24164[cs.LG]2024</p>
<p>Octo: An open-source generalist robot policy. O M Team, arXiv:2405.12213[cs.RO]2024</p>
<p>Otter: A vision-language-action model with textaware visual feature extraction. H Huang, arXiv:2503.03734[cs.RO]2025</p>
<p>Large language models for robotics: Opportunities, challenges, and perspectives. J Wang, Journal of Automation and Intelligence. 2024</p>
<p>Ai robots and humanoid ai: Review, perspectives and directions. L Cao, arXiv:2405.157752024arXiv preprint</p>
<p>Llmbased robot task planning with exceptional handling for general purpose service robots. R Wang, Z Yang, Z Zhao, X Tong, Z Hong, K Qian, 2024 43rd Chinese Control Conference (CCC). IEEE2024</p>
<p>Language guided skill discovery. S Rho, L Smith, T Li, S Levine, X B Peng, S Ha, arXiv:2406.066152024arXiv preprint</p>
<p>Robogen: Robot generation through artificial evolution. J Auerbach, Artificial Life Conference Proceedings. Rogers Street, CambridgeMIT Press One2014MA 02142-1209, USA journals-info</p>
<p>Discover: Automated curricula for sparse-reward reinforcement learning. L Diaz-Bone, M Bagatella, J Hübotter, A Krause, arXiv:2505.198502025arXiv preprint</p>
<p>Reverse curriculum generation for reinforcement learning. C Florensa, D Held, M Wulfmeier, M Zhang, P Abbeel, Conference on robot learning. 2017</p>
<p>Reverse forward curriculum learning for extreme sample and demonstration efficiency in reinforcement learning. S Tao, A Shukla, T -K. Chan, H Su, arXiv:2405.033792024arXiv preprint</p>
<p>Z Zhuang, S Yao, H Zhao, arXiv:2406.10759[cs.RO]Humanoid parkour learning. 2024</p>
<p>Hierarchical reinforcement learning for precise soccer shooting skills using a quadrupedal robot. Y Ji, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2022</p>
<p>Dribblebot: Dynamic legged manipulation in the wild. Y Ji, G B Margolis, P , International Conference on Robotics and Automation. 2023</p>
<p>Robocup 2022 adultsize winner nimbro: Upgraded perception, capture steps gait and phase-based in-walk kicks. D Pavlichenko, Robot World Cup. Springer2022</p>
<p>Exploring kinodynamic fabrics for reactive whole-body control of underactuated humanoid robots. A Adu-Bredu, G Gibson, J Grizzle, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE202310404</p>
<p>Environment curriculum generation via large language models. W Liang, S Wang, H.-J Wang, O Bastani, D Jayaraman, Y J Ma, Conference on Robot Learning (CoRL). 2024</p>
<p>Hm-rag: Hierarchical multi-agent multimodal retrieval augmented generation. P Liu, arXiv:2504.123302025arXiv preprint</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 1998MIT press Cambridge1</p>
<p>Markov decision processes: discrete stochastic dynamic programming. M L Puterman, 2014John Wiley &amp; Sons</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, IEEE/RSJ international conference on intelligent robots and systems. 2017</p>
<p>Data-efficient domain randomization with bayesian optimization. F Muratore, C Eilers, M Gienger, J Peters, IEEE Robotics and Automation Letters. 622021</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE2012</p>
<p>C D Freeman, E Frey, A Raichuk, S Girgin, I Mordatch, O Bachem, Brax -a differentiable physics engine for large scale rigid body simulation. version 0.12.3, 2021</p>
<p>New embedding models and api updates, OpenAI Blog, Announcement of text-embedding-3-small and text-embedding-3-large models. Openai, Jan. 2024</p>
<p>Mechanical intelligence-aware curriculum reinforcement learning for humanoids with parallel actuation. Y Tanaka, A Zhu, Q Wang, D Hong, arXiv:2507.002732025arXiv preprint</p>
<p>Mujoco playground: An open-source framework for gpu-accelerated robot learning and sim-to-real transfer. K Zakka, 2025</p>
<p>Design and control of a miniature bipedal robot with proprioceptive actuation for dynamic behaviors. Y Liu, J Shen, J Zhang, X Zhang, T Zhu, D Hong, 2022 International Conference on Robotics and Automation (ICRA). 2022</p>
<p>Robo2vlm: Visual question answering from large-scale in-the-wild robot manipulation datasets. K Chen, S Xie, Z Ma, P R Sanketi, K Goldberg, arXiv:2505.15517[cs.RO]2025</p>
<p>Robovqa: Multimodal long-horizon reasoning for robotics. P Sermanet, arXiv:2311.00899[cs.RO]2023</p>
<p>Autoeval: Autonomous evaluation of generalist robot manipulation policies in the real world. Z Zhou, P Atreya, Y L Tan, K Pertsch, S Levine, arXiv:2503.242782025arXiv preprint</p>
<p>Rewind: Language-guided rewards teach robot policies without new demonstrations. J Zhang, arXiv:2505.109112025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7021 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7021</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7021</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-afc1824a051f686e18ad87e1244bb0926a361021</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/afc1824a051f686e18ad87e1244bb0926a361021" target="_blank">Modeling Graph Structure in Transformer for Better AMR-to-Text Generation</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes a novel structure-aware self-attention approach to better model the relations between indirectly connected concepts in the state-of-the-art seq2seq model, i.e. the Transformer.</p>
                <p><strong>Paper Abstract:</strong> Recent studies on AMR-to-text generation often formalize the task as a sequence-to-sequence (seq2seq) learning problem by converting an Abstract Meaning Representation (AMR) graph into a word sequences. Graph structures are further modeled into the seq2seq framework in order to utilize the structural information in the AMR graphs. However, previous approaches only consider the relations between directly connected concepts while ignoring the rich structure in AMR graphs. In this paper we eliminate such a strong limitation and propose a novel structure-aware self-attention approach to better model the relations between indirectly connected concepts in the state-of-the-art seq2seq model, i.e. the Transformer. In particular, a few different methods are explored to learn structural representations between two concepts. Experimental results on English AMR benchmark datasets show that our approach significantly outperforms the state-of-the-art with 29.66 and 31.82 BLEU scores on LDC2015E86 and LDC2017T10, respectively. To the best of our knowledge, these are the best results achieved so far by supervised models on the benchmarks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7021.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7021.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DFS linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Depth-First Traversal Linearization of AMR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-text representation that serializes AMR graphs into token sequences via depth-first traversal after removing variables, wiki links and sense tags; used as the input to seq2seq (Transformer) baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural AMR: Sequence-to-sequence models for parsing and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>DFS linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>AMR graphs are linearized into a sequence using a depth-first traversal; variables, wiki links and sense tags are removed. Reentrant nodes appear multiple times in the linearized sequence (i.e., duplicated tokens). The produced sequence is tokenized by BPE and a shared source/target vocabulary is used.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (lossy with respect to reentrancies unless additional markers used)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Depth-first traversal (depth-first search based traversal over the AMR graph), with pre-processing to remove variables/wiki/sense tags.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LDC2015E86; LDC2017T10</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation (graph-to-text generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (seq2seq baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder-decoder baseline used in this study: 6 encoder layers and 6 decoder layers, 8 attention heads, embedding and hidden sizes 512; d_x and d_z set to 64 for structure vectors; trained with Adam, 300K steps on single GPU.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU; Meteor; CHRF++</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Baseline (Transformer w/ DFS linearization): LDC2015E86 BLEU 25.50, Meteor 33.16, CHRF++ 59.88; LDC2017T10 BLEU 27.43, Meteor 34.62, CHRF++ 61.85</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables direct use of seq2seq models; with BPE and shared vocabulary it yielded a strong baseline (significant improvement vs naive settings). However, it does not provide explicit mechanisms to capture long-distance graph structural relations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lossy with respect to graph structure: reentrancies (nodes with multiple parents) are not explicitly preserved (they become duplicate tokens), leading to ambiguity; structural information between indirectly connected nodes is not explicitly represented; sensitive to linearization choices and requires additional techniques (BPE, shared vocab) to mitigate data sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperforms many earlier graph-to-sequence models when paired with a strong Transformer (in this paper), but compared to the structure-aware methods proposed here it is inferior because it lacks explicit encoding of pairwise graph structural relations; previous graph-to-sequence models encode graph structure directly (e.g., GCN/GNN) but often focus on one-hop relations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Graph Structure in Transformer for Better AMR-to-Text Generation', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7021.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7021.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Subword-extended AMR graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR Graph Extended for Sub-word (BPE) Units</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of AMR node representation to align graph nodes with sub-word units produced by BPE: nodes that are split into sub-words are represented as connected sub-node chains in the graph so the graph structure is consistent with sub-word tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Sub-word-augmented AMR graph</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>When a concept token is segmented into multiple BPE sub-word tokens (e.g., sent@@ ence-01), the original node is split into multiple connected nodes, with an edge between the sub-nodes labeled using the incoming edge label of the first sub-unit; this yields a graph whose nodes align to sub-word tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>graph-then-tokenized (hybrid), token-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Graph modification: split original concept node into a chain of sub-nodes; attach edge label of incoming edge to the first sub-node; subsequent serialization uses the (possibly shorter) concept-only linearization or structure-aware encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LDC2015E86; LDC2017T10</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation (graph-to-text generation) with sub-word tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer with structure-aware encoder (same architecture as main experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder-decoder; model settings identical to main experiments; BPE vocabulary shared across source and target; 10K BPE ops for LDC2015E86 and 20K for LDC2017T10.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU; Meteor; CHRF++ (part of same experimental setup)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Allows compatibility between graph nodes and BPE tokenization; helps reduce data sparsity and allows the structure-aware encoder to work at sub-word resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Introduces additional nodes and edges (graph size increases) and requires coherent splitting strategy; not reported to be canonicalized; potential increase in computational cost for large splits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Presented as a pragmatic adaptation to integrate BPE tokenization with graph-based encodings; prior works typically operate at word or concept level and may use anonymization or copy mechanisms instead.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Graph Structure in Transformer for Better AMR-to-Text Generation', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7021.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7021.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Path-label sequence (feature-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structural Path Label Sequence — Feature-based encoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that represents the graph relation between two concepts as a single discrete feature string formed by concatenating the labeled edge sequence (with direction markers) along the shortest path, mapping the most frequent features to embeddings and others to UNK.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Structural path label sequence (feature-based)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each concept pair (x_i, x_j), extract the shortest path's edge-label sequence with direction markers (↑ for up, ↓ for down), concatenate labels into a single string feature; map the top-K frequent feature strings (20K in experiments) to learned vectors; rare strings -> UNK.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential-as-discrete-feature (lossy due to vocabulary cut-off), token-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Shortest-path edge-label extraction (path length variable), concatenation into a single discrete feature string; frequency-based vocabulary cutoff (20K) to limit feature set.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LDC2015E86; LDC2017T10</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation (structure injection into Transformer self-attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer + structure-aware self-attention (feature-based path vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder-decoder; structure vectors r_ij are discrete feature embeddings added into attention computation via learned matrices W^R and W^F. Other model hyperparameters: 6 layers, 8 heads, embeddings/hidden 512.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU; Meteor; CHRF++</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>LDC2015E86 BLEU 27.23, Meteor 34.53, CHRF++ 61.55; LDC2017T10 BLEU 30.18, Meteor 35.83, CHRF++ 63.20</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Yields consistent BLEU improvements over the baseline (approx +1.7 BLEU on LDC2015E86 and +2.75 on LDC2017T10), with only modest parameter increase.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>High sparsity: large number of distinct path strings leads to many rare features mapped to UNK; discrete feature vocabulary required (20K cutoff) which loses granularity for rare paths; less effective than continuous encoding methods in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperformed the baseline but underperformed continuous representation methods (SA-based and CNN-based); discrete nature made it weaker than averaged/summed/learned continuous encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Graph Structure in Transformer for Better AMR-to-Text Generation', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7021.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7021.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Path-label sequence (avg-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structural Path Label Sequence — Average embedding encoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Represents a path between two concepts by averaging the learned embeddings of the edge-label tokens (including direction markers) along the path to produce a continuous vector r_ij.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Structural path label sequence (avg-based)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Given label embeddings l_1..l_k for the k labels on the shortest path with direction markers, compute r = (1/k) * sum_i l_i. This continuous vector is used in the modified self-attention computations.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential -> continuous vector (lossy w.r.t. order but compact)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Shortest-path edge-label extraction; token-level label embeddings averaged across path positions.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LDC2015E86; LDC2017T10</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation (structure injection into Transformer self-attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer + structure-aware self-attention (avg-based path vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder-decoder; r_ij computed as mean of label embeddings, injected into attention as additive terms in keys and values (via W^R and W^F). Same model hyperparameters as other experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU; Meteor; CHRF++</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>LDC2015E86 BLEU 28.37, Meteor 35.10, CHRF++ 62.29; LDC2017T10 BLEU 29.56, Meteor 35.24, CHRF++ 62.86</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Improves performance substantially over baseline (not as strong as sum/SA/CNN in some cases); continuous low-dimensional vectors reduce sparsity and provide stable improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Order information on path is collapsed (averaging loses positional/order cues); limited expressiveness for longer paths; authors cap path length to 4 (labels beyond ignored), imposing truncation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Better than the discrete feature-based method; comparable but slightly weaker than sum-based and advanced learned encoders (SA/CNN) according to reported BLEU/Meteor/CHRF++.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Graph Structure in Transformer for Better AMR-to-Text Generation', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7021.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7021.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Path-label sequence (sum-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structural Path Label Sequence — Sum embedding encoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Represents the structural path between two concepts by summing label embeddings of the path to form a continuous vector r_ij, preserving cumulative label presence but not order.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Structural path label sequence (sum-based)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Given embeddings for each label on the shortest path, compute r = sum_i l_i. The resulting vector is injected into the attention key/value computations similarly to other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential -> continuous vector (lossy w.r.t. order, token-based)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Shortest-path edge-label extraction; token-level label embeddings summed element-wise to create path vector; path length limited to 4 (labels beyond ignored).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LDC2015E86; LDC2017T10</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation (structure injection into Transformer self-attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer + structure-aware self-attention (sum-based path vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder-decoder; r_ij is the sum of label embeddings and is used as additive biases in attention computations (W^R, W^F applied).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU; Meteor; CHRF++</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>LDC2015E86 BLEU 28.69, Meteor 34.97, CHRF++ 62.05; LDC2017T10 BLEU 29.92, Meteor 35.68, CHRF++ 63.04</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provides consistent BLEU improvements over baseline; simple continuous aggregation reduces sparsity and is competitive.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Still loses label order; capped path length (4) truncates longer relations; less flexible than learned sequence encoders (SA/CNN).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperforms discrete feature-based and avg-based in reported BLEU on LDC2015E86 and matches or slightly trails learned encoders (SA/CNN) on LDC2017T10.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Graph Structure in Transformer for Better AMR-to-Text Generation', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7021.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7021.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Path-label sequence (SA-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structural Path Label Sequence — Self-Attention-based encoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned encoder that applies self-attention over the sequence of path labels (label embeddings plus position embeddings) and computes a weighted combination of hidden states to produce a path vector r_ij.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Structural path label sequence (SA-based)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Label embeddings are combined with position embeddings into a sequence, passed through a self-attention layer to produce hidden states h_i; an attention pooling (two-layer attention W^1/W^2 with tanh + softmax) produces weights α and r = sum_i α_i h_i. r_ij is then injected into Transformer attention via W^R and W^F.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>learned sequential encoder -> continuous vector (hierarchical/sequential)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Shortest-path edge-label extraction; position-aware self-attention over label sequence; attention-based pooling to fixed-size vector; path length capped at 4; d_w set to 128 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LDC2015E86; LDC2017T10</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation (structure injection into Transformer self-attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer + structure-aware self-attention (SA-based path encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder-decoder augmented with learned SA-based path encoders producing r_ij vectors (d_z-sized); W^R and W^F project r_ij into attention key/value spaces. Overall model: 6 layers, 8 heads, embeddings/hidden 512, trained 300K steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU; Meteor; CHRF++</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>LDC2015E86 BLEU 29.66, Meteor 35.45, CHRF++ 63.00; LDC2017T10 BLEU 31.54, Meteor 36.02, CHRF++ 63.84</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Best single-model gains among tested encoders: largest BLEU improvements vs baseline (≈ +4.16 on LDC2015E86 and +4.11 on LDC2017T10 relative to baseline), indicating that learned sequential encoders capture informative structure and improve generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Computational overhead for encoding many pairwise paths (r_ij for every concept pair); path lengths are truncated at 4 (labels beyond ignored) to constrain computation; increased model parameters albeit modestly (reported parameter counts show small increases).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperforms discrete feature-based, avg-based, and sum-based encodings; comparable to CNN-based but SA-based is the best on LDC2015E86 while CNN-based is slightly better on LDC2017T10 in this study; continuous learned encoders are superior to discrete features.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Graph Structure in Transformer for Better AMR-to-Text Generation', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7021.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7021.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Path-label sequence (CNN-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structural Path Label Sequence — CNN-based encoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encodes the label sequence along the shortest path via a 1D convolutional network (kernel size 4, ReLU) mapping the sequence of label embeddings into a fixed-size vector r_ij.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Structural path label sequence (CNN-based)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Apply a 1D convolution (kernel size m=4, filters = d_z, stride 1, ReLU) over label embedding sequence to produce a d_z-dimensional vector r; r is used as additive structure term in attention keys and values (via W^R and W^F).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>learned sequential encoder -> continuous vector (convolutional, token-based)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Shortest-path edge-label extraction; pad/truncate label sequence to allowed length (max 4 used); 1D convolution with kernel size 4 applied to the embedding sequence to obtain r.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LDC2015E86; LDC2017T10</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation (structure injection into Transformer self-attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer + structure-aware self-attention (CNN-based path encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder-decoder augmented with CNN-based path encoders producing r_ij vectors; CNN kernel size set to 4, output filters d_z; other Transformer settings as in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU; Meteor; CHRF++</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>LDC2015E86 BLEU 29.10, Meteor 35.00, CHRF++ 62.10; LDC2017T10 BLEU 31.82, Meteor 36.38, CHRF++ 64.05</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Significant improvements over baseline and discrete encodings; achieved best BLEU on LDC2017T10 among tested variants (31.82), showing CNN-based encoder is highly effective.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Path length truncation (max length 4) limits capture of long relation chains; convolution may be less sensitive to long-range order compared to deeper sequence models; still requires computing r_ij for many concept pairs (computational cost).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Comparable to SA-based and superior to discrete and simple aggregation methods; SA-based slightly better on LDC2015E86 while CNN-based gives best LDC2017T10 BLEU and best CHRF++ on LDC2017T10 in reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Graph Structure in Transformer for Better AMR-to-Text Generation', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural AMR: Sequence-to-sequence models for parsing and generation. <em>(Rating: 2)</em></li>
                <li>A graph-to-sequence model for AMR-to-text generation <em>(Rating: 2)</em></li>
                <li>Structural neural encoders for AMR-to-text generation <em>(Rating: 2)</em></li>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 1)</em></li>
                <li>Self-attention with relative position representations <em>(Rating: 2)</em></li>
                <li>Text generation from knowledge graphs with graph transformers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7021",
    "paper_id": "paper-afc1824a051f686e18ad87e1244bb0926a361021",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "DFS linearization",
            "name_full": "Depth-First Traversal Linearization of AMR",
            "brief_description": "A graph-to-text representation that serializes AMR graphs into token sequences via depth-first traversal after removing variables, wiki links and sense tags; used as the input to seq2seq (Transformer) baselines.",
            "citation_title": "Neural AMR: Sequence-to-sequence models for parsing and generation.",
            "mention_or_use": "use",
            "representation_name": "DFS linearization",
            "representation_description": "AMR graphs are linearized into a sequence using a depth-first traversal; variables, wiki links and sense tags are removed. Reentrant nodes appear multiple times in the linearized sequence (i.e., duplicated tokens). The produced sequence is tokenized by BPE and a shared source/target vocabulary is used.",
            "representation_type": "sequential, token-based (lossy with respect to reentrancies unless additional markers used)",
            "encoding_method": "Depth-first traversal (depth-first search based traversal over the AMR graph), with pre-processing to remove variables/wiki/sense tags.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "LDC2015E86; LDC2017T10",
            "task_name": "AMR-to-text generation (graph-to-text generation)",
            "model_name": "Transformer (seq2seq baseline)",
            "model_description": "Transformer encoder-decoder baseline used in this study: 6 encoder layers and 6 decoder layers, 8 attention heads, embedding and hidden sizes 512; d_x and d_z set to 64 for structure vectors; trained with Adam, 300K steps on single GPU.",
            "performance_metric": "BLEU; Meteor; CHRF++",
            "performance_value": "Baseline (Transformer w/ DFS linearization): LDC2015E86 BLEU 25.50, Meteor 33.16, CHRF++ 59.88; LDC2017T10 BLEU 27.43, Meteor 34.62, CHRF++ 61.85",
            "impact_on_training": "Enables direct use of seq2seq models; with BPE and shared vocabulary it yielded a strong baseline (significant improvement vs naive settings). However, it does not provide explicit mechanisms to capture long-distance graph structural relations.",
            "limitations": "Lossy with respect to graph structure: reentrancies (nodes with multiple parents) are not explicitly preserved (they become duplicate tokens), leading to ambiguity; structural information between indirectly connected nodes is not explicitly represented; sensitive to linearization choices and requires additional techniques (BPE, shared vocab) to mitigate data sparsity.",
            "comparison_with_other": "Outperforms many earlier graph-to-sequence models when paired with a strong Transformer (in this paper), but compared to the structure-aware methods proposed here it is inferior because it lacks explicit encoding of pairwise graph structural relations; previous graph-to-sequence models encode graph structure directly (e.g., GCN/GNN) but often focus on one-hop relations.",
            "uuid": "e7021.0",
            "source_info": {
                "paper_title": "Modeling Graph Structure in Transformer for Better AMR-to-Text Generation",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Subword-extended AMR graph",
            "name_full": "AMR Graph Extended for Sub-word (BPE) Units",
            "brief_description": "An extension of AMR node representation to align graph nodes with sub-word units produced by BPE: nodes that are split into sub-words are represented as connected sub-node chains in the graph so the graph structure is consistent with sub-word tokenization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Sub-word-augmented AMR graph",
            "representation_description": "When a concept token is segmented into multiple BPE sub-word tokens (e.g., sent@@ ence-01), the original node is split into multiple connected nodes, with an edge between the sub-nodes labeled using the incoming edge label of the first sub-unit; this yields a graph whose nodes align to sub-word tokens.",
            "representation_type": "graph-then-tokenized (hybrid), token-based",
            "encoding_method": "Graph modification: split original concept node into a chain of sub-nodes; attach edge label of incoming edge to the first sub-node; subsequent serialization uses the (possibly shorter) concept-only linearization or structure-aware encoding.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "LDC2015E86; LDC2017T10",
            "task_name": "AMR-to-text generation (graph-to-text generation) with sub-word tokenization",
            "model_name": "Transformer with structure-aware encoder (same architecture as main experiments)",
            "model_description": "Transformer encoder-decoder; model settings identical to main experiments; BPE vocabulary shared across source and target; 10K BPE ops for LDC2015E86 and 20K for LDC2017T10.",
            "performance_metric": "BLEU; Meteor; CHRF++ (part of same experimental setup)",
            "performance_value": null,
            "impact_on_training": "Allows compatibility between graph nodes and BPE tokenization; helps reduce data sparsity and allows the structure-aware encoder to work at sub-word resolution.",
            "limitations": "Introduces additional nodes and edges (graph size increases) and requires coherent splitting strategy; not reported to be canonicalized; potential increase in computational cost for large splits.",
            "comparison_with_other": "Presented as a pragmatic adaptation to integrate BPE tokenization with graph-based encodings; prior works typically operate at word or concept level and may use anonymization or copy mechanisms instead.",
            "uuid": "e7021.1",
            "source_info": {
                "paper_title": "Modeling Graph Structure in Transformer for Better AMR-to-Text Generation",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Path-label sequence (feature-based)",
            "name_full": "Structural Path Label Sequence — Feature-based encoding",
            "brief_description": "A method that represents the graph relation between two concepts as a single discrete feature string formed by concatenating the labeled edge sequence (with direction markers) along the shortest path, mapping the most frequent features to embeddings and others to UNK.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Structural path label sequence (feature-based)",
            "representation_description": "For each concept pair (x_i, x_j), extract the shortest path's edge-label sequence with direction markers (↑ for up, ↓ for down), concatenate labels into a single string feature; map the top-K frequent feature strings (20K in experiments) to learned vectors; rare strings -&gt; UNK.",
            "representation_type": "sequential-as-discrete-feature (lossy due to vocabulary cut-off), token-based",
            "encoding_method": "Shortest-path edge-label extraction (path length variable), concatenation into a single discrete feature string; frequency-based vocabulary cutoff (20K) to limit feature set.",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "LDC2015E86; LDC2017T10",
            "task_name": "AMR-to-text generation (structure injection into Transformer self-attention)",
            "model_name": "Transformer + structure-aware self-attention (feature-based path vectors)",
            "model_description": "Transformer encoder-decoder; structure vectors r_ij are discrete feature embeddings added into attention computation via learned matrices W^R and W^F. Other model hyperparameters: 6 layers, 8 heads, embeddings/hidden 512.",
            "performance_metric": "BLEU; Meteor; CHRF++",
            "performance_value": "LDC2015E86 BLEU 27.23, Meteor 34.53, CHRF++ 61.55; LDC2017T10 BLEU 30.18, Meteor 35.83, CHRF++ 63.20",
            "impact_on_training": "Yields consistent BLEU improvements over the baseline (approx +1.7 BLEU on LDC2015E86 and +2.75 on LDC2017T10), with only modest parameter increase.",
            "limitations": "High sparsity: large number of distinct path strings leads to many rare features mapped to UNK; discrete feature vocabulary required (20K cutoff) which loses granularity for rare paths; less effective than continuous encoding methods in experiments.",
            "comparison_with_other": "Outperformed the baseline but underperformed continuous representation methods (SA-based and CNN-based); discrete nature made it weaker than averaged/summed/learned continuous encodings.",
            "uuid": "e7021.2",
            "source_info": {
                "paper_title": "Modeling Graph Structure in Transformer for Better AMR-to-Text Generation",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Path-label sequence (avg-based)",
            "name_full": "Structural Path Label Sequence — Average embedding encoding",
            "brief_description": "Represents a path between two concepts by averaging the learned embeddings of the edge-label tokens (including direction markers) along the path to produce a continuous vector r_ij.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Structural path label sequence (avg-based)",
            "representation_description": "Given label embeddings l_1..l_k for the k labels on the shortest path with direction markers, compute r = (1/k) * sum_i l_i. This continuous vector is used in the modified self-attention computations.",
            "representation_type": "sequential -&gt; continuous vector (lossy w.r.t. order but compact)",
            "encoding_method": "Shortest-path edge-label extraction; token-level label embeddings averaged across path positions.",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "LDC2015E86; LDC2017T10",
            "task_name": "AMR-to-text generation (structure injection into Transformer self-attention)",
            "model_name": "Transformer + structure-aware self-attention (avg-based path vectors)",
            "model_description": "Transformer encoder-decoder; r_ij computed as mean of label embeddings, injected into attention as additive terms in keys and values (via W^R and W^F). Same model hyperparameters as other experiments.",
            "performance_metric": "BLEU; Meteor; CHRF++",
            "performance_value": "LDC2015E86 BLEU 28.37, Meteor 35.10, CHRF++ 62.29; LDC2017T10 BLEU 29.56, Meteor 35.24, CHRF++ 62.86",
            "impact_on_training": "Improves performance substantially over baseline (not as strong as sum/SA/CNN in some cases); continuous low-dimensional vectors reduce sparsity and provide stable improvements.",
            "limitations": "Order information on path is collapsed (averaging loses positional/order cues); limited expressiveness for longer paths; authors cap path length to 4 (labels beyond ignored), imposing truncation.",
            "comparison_with_other": "Better than the discrete feature-based method; comparable but slightly weaker than sum-based and advanced learned encoders (SA/CNN) according to reported BLEU/Meteor/CHRF++.",
            "uuid": "e7021.3",
            "source_info": {
                "paper_title": "Modeling Graph Structure in Transformer for Better AMR-to-Text Generation",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Path-label sequence (sum-based)",
            "name_full": "Structural Path Label Sequence — Sum embedding encoding",
            "brief_description": "Represents the structural path between two concepts by summing label embeddings of the path to form a continuous vector r_ij, preserving cumulative label presence but not order.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Structural path label sequence (sum-based)",
            "representation_description": "Given embeddings for each label on the shortest path, compute r = sum_i l_i. The resulting vector is injected into the attention key/value computations similarly to other methods.",
            "representation_type": "sequential -&gt; continuous vector (lossy w.r.t. order, token-based)",
            "encoding_method": "Shortest-path edge-label extraction; token-level label embeddings summed element-wise to create path vector; path length limited to 4 (labels beyond ignored).",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "LDC2015E86; LDC2017T10",
            "task_name": "AMR-to-text generation (structure injection into Transformer self-attention)",
            "model_name": "Transformer + structure-aware self-attention (sum-based path vectors)",
            "model_description": "Transformer encoder-decoder; r_ij is the sum of label embeddings and is used as additive biases in attention computations (W^R, W^F applied).",
            "performance_metric": "BLEU; Meteor; CHRF++",
            "performance_value": "LDC2015E86 BLEU 28.69, Meteor 34.97, CHRF++ 62.05; LDC2017T10 BLEU 29.92, Meteor 35.68, CHRF++ 63.04",
            "impact_on_training": "Provides consistent BLEU improvements over baseline; simple continuous aggregation reduces sparsity and is competitive.",
            "limitations": "Still loses label order; capped path length (4) truncates longer relations; less flexible than learned sequence encoders (SA/CNN).",
            "comparison_with_other": "Outperforms discrete feature-based and avg-based in reported BLEU on LDC2015E86 and matches or slightly trails learned encoders (SA/CNN) on LDC2017T10.",
            "uuid": "e7021.4",
            "source_info": {
                "paper_title": "Modeling Graph Structure in Transformer for Better AMR-to-Text Generation",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Path-label sequence (SA-based)",
            "name_full": "Structural Path Label Sequence — Self-Attention-based encoding",
            "brief_description": "A learned encoder that applies self-attention over the sequence of path labels (label embeddings plus position embeddings) and computes a weighted combination of hidden states to produce a path vector r_ij.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Structural path label sequence (SA-based)",
            "representation_description": "Label embeddings are combined with position embeddings into a sequence, passed through a self-attention layer to produce hidden states h_i; an attention pooling (two-layer attention W^1/W^2 with tanh + softmax) produces weights α and r = sum_i α_i h_i. r_ij is then injected into Transformer attention via W^R and W^F.",
            "representation_type": "learned sequential encoder -&gt; continuous vector (hierarchical/sequential)",
            "encoding_method": "Shortest-path edge-label extraction; position-aware self-attention over label sequence; attention-based pooling to fixed-size vector; path length capped at 4; d_w set to 128 in experiments.",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "LDC2015E86; LDC2017T10",
            "task_name": "AMR-to-text generation (structure injection into Transformer self-attention)",
            "model_name": "Transformer + structure-aware self-attention (SA-based path encoders)",
            "model_description": "Transformer encoder-decoder augmented with learned SA-based path encoders producing r_ij vectors (d_z-sized); W^R and W^F project r_ij into attention key/value spaces. Overall model: 6 layers, 8 heads, embeddings/hidden 512, trained 300K steps.",
            "performance_metric": "BLEU; Meteor; CHRF++",
            "performance_value": "LDC2015E86 BLEU 29.66, Meteor 35.45, CHRF++ 63.00; LDC2017T10 BLEU 31.54, Meteor 36.02, CHRF++ 63.84",
            "impact_on_training": "Best single-model gains among tested encoders: largest BLEU improvements vs baseline (≈ +4.16 on LDC2015E86 and +4.11 on LDC2017T10 relative to baseline), indicating that learned sequential encoders capture informative structure and improve generation quality.",
            "limitations": "Computational overhead for encoding many pairwise paths (r_ij for every concept pair); path lengths are truncated at 4 (labels beyond ignored) to constrain computation; increased model parameters albeit modestly (reported parameter counts show small increases).",
            "comparison_with_other": "Outperforms discrete feature-based, avg-based, and sum-based encodings; comparable to CNN-based but SA-based is the best on LDC2015E86 while CNN-based is slightly better on LDC2017T10 in this study; continuous learned encoders are superior to discrete features.",
            "uuid": "e7021.5",
            "source_info": {
                "paper_title": "Modeling Graph Structure in Transformer for Better AMR-to-Text Generation",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Path-label sequence (CNN-based)",
            "name_full": "Structural Path Label Sequence — CNN-based encoding",
            "brief_description": "Encodes the label sequence along the shortest path via a 1D convolutional network (kernel size 4, ReLU) mapping the sequence of label embeddings into a fixed-size vector r_ij.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Structural path label sequence (CNN-based)",
            "representation_description": "Apply a 1D convolution (kernel size m=4, filters = d_z, stride 1, ReLU) over label embedding sequence to produce a d_z-dimensional vector r; r is used as additive structure term in attention keys and values (via W^R and W^F).",
            "representation_type": "learned sequential encoder -&gt; continuous vector (convolutional, token-based)",
            "encoding_method": "Shortest-path edge-label extraction; pad/truncate label sequence to allowed length (max 4 used); 1D convolution with kernel size 4 applied to the embedding sequence to obtain r.",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "LDC2015E86; LDC2017T10",
            "task_name": "AMR-to-text generation (structure injection into Transformer self-attention)",
            "model_name": "Transformer + structure-aware self-attention (CNN-based path encoders)",
            "model_description": "Transformer encoder-decoder augmented with CNN-based path encoders producing r_ij vectors; CNN kernel size set to 4, output filters d_z; other Transformer settings as in the paper.",
            "performance_metric": "BLEU; Meteor; CHRF++",
            "performance_value": "LDC2015E86 BLEU 29.10, Meteor 35.00, CHRF++ 62.10; LDC2017T10 BLEU 31.82, Meteor 36.38, CHRF++ 64.05",
            "impact_on_training": "Significant improvements over baseline and discrete encodings; achieved best BLEU on LDC2017T10 among tested variants (31.82), showing CNN-based encoder is highly effective.",
            "limitations": "Path length truncation (max length 4) limits capture of long relation chains; convolution may be less sensitive to long-range order compared to deeper sequence models; still requires computing r_ij for many concept pairs (computational cost).",
            "comparison_with_other": "Comparable to SA-based and superior to discrete and simple aggregation methods; SA-based slightly better on LDC2015E86 while CNN-based gives best LDC2017T10 BLEU and best CHRF++ on LDC2017T10 in reported results.",
            "uuid": "e7021.6",
            "source_info": {
                "paper_title": "Modeling Graph Structure in Transformer for Better AMR-to-Text Generation",
                "publication_date_yy_mm": "2019-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural AMR: Sequence-to-sequence models for parsing and generation.",
            "rating": 2,
            "sanitized_title": "neural_amr_sequencetosequence_models_for_parsing_and_generation"
        },
        {
            "paper_title": "A graph-to-sequence model for AMR-to-text generation",
            "rating": 2,
            "sanitized_title": "a_graphtosequence_model_for_amrtotext_generation"
        },
        {
            "paper_title": "Structural neural encoders for AMR-to-text generation",
            "rating": 2,
            "sanitized_title": "structural_neural_encoders_for_amrtotext_generation"
        },
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 1,
            "sanitized_title": "graphtosequence_learning_using_gated_graph_neural_networks"
        },
        {
            "paper_title": "Self-attention with relative position representations",
            "rating": 2,
            "sanitized_title": "selfattention_with_relative_position_representations"
        },
        {
            "paper_title": "Text generation from knowledge graphs with graph transformers",
            "rating": 1,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        }
    ],
    "cost": 0.016193,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Modeling Graph Structure in Transformer for Better AMR-to-Text Generation</h1>
<p>Jie Zhu ${ }^{1}$<br>Longhua Qian ${ }^{1}$<br>Junhui $\mathrm{Li}^{1 *}$<br>Min Zhang ${ }^{1}$<br>Muhua Zhu ${ }^{2}$<br>Guodong Zhou ${ }^{1}$<br>${ }^{1}$ School of Computer Science and Technology, Soochow University, Suzhou, China<br>${ }^{2}$ Alibaba Group, Hangzhou, China<br>zhujie951121@gmail.com, {lijunhui, qianlonghua, minzhang, gdzhou}@suda.edu.cn<br>muhua.zmh@alibaba-inc.com</p>
<h4>Abstract</h4>
<p>Recent studies on AMR-to-text generation often formalize the task as a sequence-tosequence (seq2seq) learning problem by converting an Abstract Meaning Representation (AMR) graph into a word sequence. Graph structures are further modeled into the seq2seq framework in order to utilize the structural information in the AMR graphs. However, previous approaches only consider the relations between directly connected concepts while ignoring the rich structure in AMR graphs. In this paper we eliminate such a strong limitation and propose a novel structure-aware selfattention approach to better modeling the relations between indirectly connected concepts in the state-of-the-art seq2seq model, i.e., the Transformer. In particular, a few different methods are explored to learn structural representations between two concepts. Experimental results on English AMR benchmark datasets show that our approach significantly outperforms the state of the art with 29.66 and 31.82 BLEU scores on LDC2015E86 and LDC2017T10, respectively. To the best of our knowledge, these are the best results achieved so far by supervised models on the benchmarks.</p>
<h2>1 Introduction</h2>
<p>AMR-to-text generation is a task of automatically generating a natural language sentence from an Abstract Meaning Representation (AMR) graph. Due to the importance of AMR as a widely adopted semantic formalism in representing the meaning of a sentence (Banarescu et al., 2013), AMR has become popular in semantic representation and AMR-to-text generation has been drawing more and more attention in the last decade. As the example in Figure 1(a) shows, nodes, such as he and convict-01, represent semantic concepts</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>and edges, such as ":ARG1" and ":quant", refer to semantic relations between the concepts. Since two concepts close in an AMR graph may map into two segments that are distant in the corresponding sentence, AMR-to-text generation is challenging. For example in Figure 1, the neighboring concepts he and convict-01 correspond to the words he and convicted which locate at the different ends of the sentence.</p>
<p>To address the above mentioned challenge, recent studies on AMR-to-text generation regard the task as a sequence-to-sequence (seq2seq) learning problem by properly linearizing an AMR graph into a sequence (Konstas et al., 2017). Such an input representation, however, is apt to lose useful structural information due to the removal of reentrant structures for linearization. To better model graph structures, previous studies propose various graph-based seq2seq models to incorporate graphs as an additional input representation (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019). Although such graph-to-sequence models can achieve the state-of-the-art results, they focus on modeling one-hop relations only. That is, they only model concept pairs connected directly by an edge (Song et al., 2018; Beck et al., 2018), and as a result, ignore explicit structural information of indirectly connected concepts in AMR graphs, e.g. the relation between concepts he and possible in Figure 1.</p>
<p>To make better use of structural information in an AMR graph, we attempt to model arbitrary concept pairs no matter whether directly connected or not. To this end, we extend the encoder in the state-of-the-art seq2seq model, i.e., the Transformer (Vaswani et al., 2017) and propose structure-aware self-attention encoding approach. In particular, several distinct methods are proposed to learn structure representations for the new self-attention mechanism.</p>
<p>Empirical studies on two English benchmarks show that our approach significantly advances the state of the art for AMR-to-text generation, with the performance improvement of 4.16 BLEU score on LDC2015E86 and 4.39 BLEU score on LDC2017T10 respectively over the strong baseline. Overall, this paper makes the following contributions.</p>
<ul>
<li>To the best of our knowledge, this is the first work that applies the Transformer to the task of AMR-to-text generation. On the basis of the Transformer, we build a strong baseline that reaches the state of the art.</li>
<li>We propose a new self-attention mechanism to incorporate richer structural information in AMR graphs. Experimental results on two benchmarks demonstrate the effectiveness of the proposed approach.</li>
<li>Benefiting from the strong baseline and the structure-aware self-attention mechanism, we greatly advance the state of the art in the task.</li>
</ul>
<h2>2 AMR-to-Text Generation with Graph Structure Modeling</h2>
<p>We start by describing the implementation of our baseline system, a state-of-the-art seq2seq model which is originally used for neural machine translation and syntactic parsing (Vaswani et al., 2017). Then we detail the proposed approach to incorporating structural information from AMR graphs.</p>
<h3>2.1 Transformer-based Baseline</h3>
<p>Transformer: Our baseline system builds on the Transformer which employs an encoder-decoder framework, consisting of stacked encoder and decoder layers. Each encoder layer has two sublayers: self-attention layer followed by a positionwise feed forward layer. Self-attention layer employs multiple attention heads and the results from each attention head are concatenated and transformed to form the output of the self-attention layer. Each attention head uses scaled dotproduct attention which takes a sequence $x=$ $\left(x_{1}, \cdots, x_{n}\right)$ of $n$ elements as input and computes a new sequence $z=\left(z_{1}, \cdots, z_{n}\right)$ of the same length:</p>
<p>$$
z=\operatorname{Attention}(x)
$$</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
(b)
( possible :arg1 ( sentence :arg1 he :arg2 ( temporalquantity :quant 7 :unit year :location prison ) :condition ( convict :arg1 he ) ) )
(c)
possible sentence he temporal-quantity 7 year prison convict
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 1: (a) An example of AMR graph for the sentence of He could be sentenced to 7 years in prison if convicted. (b) input to our baseline system, the seq2seq Transformer. (c) input to our proposed system based on structure-aware self-attention. (d) An example of graph structure extensions to sub-word units.</p>
<p>where $x_{i} \in \mathbb{R}^{d_{x}}$ and $z \in \mathbb{R}^{n \times d_{z}}$. Each output element $z_{i}$ is a weighted sum of a linear transformation of input elements:</p>
<p>$$
z_{i}=\sum_{j=1}^{n} \alpha_{i j}\left(x_{j} W^{V}\right)
$$</p>
<p>where $W^{V} \in \mathbb{R}^{d_{x} \times d_{z}}$ is matrix of parameters. The vectors $\alpha_{i}=\left(\alpha_{i 1}, \cdots, \alpha_{i n}\right)$ in Equation 2 are obtained by the self-attention model, which captures the correspondences between $x_{i}$ and others. Specifically, the attention weight $\alpha_{i j}$ of each element $x_{j}$ is computed using a softmax function:</p>
<p>$$
\alpha_{i j}=\frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{n} \exp \left(e_{i k}\right)}
$$</p>
<p>where</p>
<p>$$
e_{i j}=\frac{\left(x_{i} W^{Q}\right)\left(x_{j} W^{K}\right)^{T}}{\sqrt{d_{z}}}
$$</p>
<p>is an alignment function which measures how well the input elements $x_{i}$ and $x_{j}$ match. $W^{Q}, W^{K} \in$ $\mathbb{R}^{d_{x} \times d_{z}}$ are parameters to be learned.
Input Representation: We use the depth-first traversal strategy as in Konstas et al. (2017) to linearize AMR graphs and to obtain simplified AMRs. We remove variables, wiki links and sense tags before linearization. Figure 1(b) shows an example linearization result for the AMR graph in Figure 1(a). Note that the reentrant concept he in Figure 1 (a) maps to two different tokens in the linearized sequence.
Vocabulary: Training AMR-to-text generation systems solely on labeled data may suffer from data sparseness. To attack this problem, previous works adopt techniques like anonymization to remove named entities and rare words (Konstas et al., 2017), or apply a copy mechanism (Gulcehre et al., 2016) such that the models can learn to copy rare words from the input sequence. In this paper we instead use two simple yet effective techniques. One is to apply Byte Pair Encoding (BPE) (Sennrich et al., 2016) to split words into smaller, more frequent sub-word units. The other is to use a shared vocabulary for both source and target sides. Experiments in Section 3.2 demonstrate the necessity of the techniques in building a strong baseline.</p>
<h3>2.2 Modeling Graph Structures in Transformer</h3>
<p>Input Representation: We also use the depthfirst traversal strategy to linearize AMR graphs
and to obtain simplified AMRs which only consist of concepts. As shown in Figure 1 (c), the input sequence is much shorter than the input sequence in the baseline. Besides, we also obtain a matrix which records the graph structure between every concept pair, which implies their semantic relationship (Section2.3).
Vocabulary: To be compatible with sub-words, we extend the original AMR graph, if necessary, to include the structures of sub-words. As sentence01 in Figure 1(a) is segmented into sent@ @ ence01, we split the original node into two connected ones with an edge labeled as the incoming edge of the first unit. Figure 1(d) shows the graph structure for sub-words sent@ @ ence-01.
Structure-Aware Self-Attention: Motivated by Shaw et al. (2018), we extend the conventional self-attention architecture to explicitly encode the relation between an element pair $\left(x_{i}, x_{j}\right)$ in the alignment model by replacing Equation 4 with Equation 5. Note that the relation $r_{i j} \in \mathbb{R}^{d_{z}}$ is the vector representation for element pair $\left(x_{i}, x_{j}\right)$, and will be learned in Section 2.3.</p>
<p>$$
e_{i j}=\frac{\left(x_{i} W^{Q}\right)\left(x_{j} W^{K}+r_{i j} W^{R}\right)^{T}}{\sqrt{d_{z}}}
$$</p>
<p>where $W^{R} \in \mathbb{R}^{d_{z} \times d_{z}}$ is a parameter matrix. Then, we update Equation 2 accordingly to propagate structure information to the sublayer output by:</p>
<p>$$
z_{i}=\sum_{j=1}^{n} \alpha_{i j}\left(x_{j} W^{V}+r_{i j} W^{F}\right)
$$</p>
<p>where $W^{F} \in \mathbb{R}^{d_{z} \times d_{z}}$ is a parameter matrix.</p>
<h3>2.3 Learning Graph Structure Representation for Concept Pairs</h3>
<p>The above structure-aware self-attention is capable of incorporating graph structure between concept pairs. In this section, we explore a few methods to learn the representation for concept pairs. We use a sequence of edge labels, along the path from $x_{i}$ to $x_{j}$ to indicate the AMR graph structure between concepts $x_{i}$ and $x_{j} .{ }^{1}$ In order to distinguish the edge direction, we add a direction symbol to each label with $\uparrow$ for climbing up along the path, and $\downarrow$ for going down. Specifically, for the special case of $i==j$, we use None as the path. Table 1 demonstrates structural label sequences between a few concept pairs in Figure 1.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">$x_{i}$</th>
<th style="text-align: center;">$x_{j}$</th>
<th style="text-align: center;">Structural label sequence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">he</td>
<td style="text-align: center;">convict-01</td>
<td style="text-align: center;">:ARG1 $\uparrow$</td>
</tr>
<tr>
<td style="text-align: left;">he</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">:ARG1 $\uparrow$ :ARG2 $\downarrow:$ quant $\downarrow$</td>
</tr>
<tr>
<td style="text-align: left;">he</td>
<td style="text-align: center;">he</td>
<td style="text-align: center;">None</td>
</tr>
</tbody>
</table>
<p>Table 1: Examples of structural path between a few concept pairs in Figure 1.</p>
<p>Now, given a structural path with a label sequence $s=s_{1}, \cdots, s_{k}$ and its $d_{x}$-sized corresponding label embedding sequence $l=$ $l_{1}, \cdots, l_{k}$, we use the following methods to obtain its representation vector $r$, which maps to $r_{i j}$ in Equation 5 and Equation 6.</p>
<h2>Feature-based</h2>
<p>A natural way to represent the structural path is to view it as a string feature. To this end, we combine the labels in the structural path into a string. Unsurprisingly, this will end up with a large number of features. We keep the most frequent ones (i.e., 20 K in our experiments) in the feature vocabulary and map all others into a special feature $U N K$. Each feature in the vocabulary will be mapped into a randomly initialized vector.</p>
<h2>Avg-based</h2>
<p>To overcome the data sparsity in the above featurebased method, we view the structural path as a label sequence. Then we simply use the averaged label embedding as the representation vector of the sequence, i.e.,</p>
<p>$$
r=\frac{\sum_{i=1}^{k} l_{i}}{k}
$$</p>
<h2>Sum-based</h2>
<p>Sum-based method simply returns the sum of all label embeddings in the sequence, i.e.,</p>
<p>$$
r=\sum_{i=1}^{k} l_{i}
$$</p>
<h2>Self-Attention-based (SA-based for short)</h2>
<p>As shown in Figure 2, given the label sequence $s=s_{1}, \cdots, s_{k}$, we first obtain the sequence $e$, whose element is the addition of a word embedding and the corresponding position embedding. Then we use the self-attention, as presented in Eq. 1 to obtain its hidden states $h$, i.e, $h=\operatorname{Attention}(e)$, where $h_{i} \in \mathbb{R}^{d_{z}}$. Our aim is to encode a variable length sentence into a $d_{z^{-}}$ sized vector. Motivated by (Lin et al., 2017), we achieve this by choosing a linear combination of
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 2: Self-Attention-based method.
the $k$ vectors in $h$. Computing the linear combination requires an attention mechanism which takes the whole hidden states $h$ as input, and outputs a vector of weights $\alpha$ :</p>
<p>$$
\alpha=\operatorname{softmax}\left(W^{2} \tanh \left(W^{1} h^{T}\right)\right)
$$</p>
<p>where $W^{1} \in \mathbb{R}^{d_{w} \times d_{z}}$ and $W^{2} \in \mathbb{R}^{d_{w}}$. Then the label sequence representation vector is the weighted sum of its hidden states:</p>
<p>$$
r=\sum_{i=1}^{k} \alpha_{i} h_{i}
$$</p>
<h2>CNN-based</h2>
<p>Motivated by (Kalchbrenner et al., 2014), we use convolutional neural network (CNN) to convolute the label sequence $l$ into a vector $r$, as follow:</p>
<p>$$
\begin{aligned}
\operatorname{conv}=\operatorname{Conv} 1 D(\text { kernel_size } &amp; =(m) \
\text { strides } &amp; =1 \
\text { filters } &amp; =d_{z} \
\text { input_shape } &amp; =d_{z} \
\text { activation } &amp; =^{\prime} \operatorname{relu}^{\prime} \
r=\operatorname{conv}(l)
\end{aligned}
$$</p>
<p>where kernel size $m$ is set to 4 in our experiments.</p>
<h2>3 Experimentation</h2>
<h3>3.1 Experimental Settings</h3>
<p>For evaluation of our approach, we use the sentences annotated with AMRs from the LDC release LDC2015E86 and LDC2017T10. The two datasets contain 16,833 and 36,521 training AMRs, respectively, and share 1,368 development AMRs and 1,371 testing AMRs. We segment words into sub-word units by BPE (Sennrich et al., 2016) with 10K operations on LDC2015E86 and 20K operations on LDC2017T10.</p>
<p>For efficiently learning graph structure representation for concept pairs (except the featurebased method), we limit the maximum label sequence length to 4 and ignore the labels exceeding the maximum. In SA-based method, we set the filter size $d_{w}$ as 128 .</p>
<p>We use OpenNMT (Klein et al., 2017) as the implementation of the Transformer seq2seq model. ${ }^{2}$ In parameter setting, we set the number of layers in both the encoder and decoder to 6 . For optimization we use Adam with $\beta 1=0.1$ (Kingma and $\mathrm{Ba}, 2015$ ). The number of heads is set to 8 . In addition, we set the embedding and the hidden sizes to 512 and the batch token-size to 4096 . Accordingly, the $d_{x}$ and $d_{z}$ in Section 2 are 64. In all experiments, we train the models for 300 K steps on a single K40 GPU.</p>
<p>For performance evaluation, we use BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), and CHRF++ (Popovi, 2017) as metrics. We report results of single models that are tuned on the development set.</p>
<p>We make our code available at https://github.com/Amazing-J/ structural-transformer.</p>
<h3>3.2 Experimental Results</h3>
<p>We first show the performance of our baseline system. As mentioned before, BPE and sharing vocabulary are two techniques we applied to relieving data sparsity. Table 2 presents the results of the ablation test on the development set of LDC2015E86 by either removing BPE, or vocabulary sharing, or both of them from the baseline system. From the results we can see that BPE and vocabulary sharing are critical to building our base-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: Ablation results of our baseline system on the LDC2015E86 development set.
line system (an improvement from 18.77 to 24.93 in BLEU), revealing the fact that they are two effective ways to address the issue of data sparseness for AMR-to-text generation.</p>
<p>Table 3 presents the comparison of our approach and related works on the test sets of LDC2015E86 and LDC2017T10. From the results we can see that the Transformer-based baseline outperforms most of graph-to-sequence models and is comparable with the latest work by Guo et al. (2019). The strong performance of the baseline is attributed to the capability of the Transformer to encode global and implicit structural information in AMR graphs. By comparing the five methods of learning graph structure representations, we have the following observations.</p>
<ul>
<li>All of them achieve significant improvements over the baseline: the biggest improvements are 4.16 and 4.39 BLEU scores on LDC2015E86 and LDC2017T10, respectively.</li>
<li>Methods using continuous representations (such as SA-based and CNN-based) outperform the methods using discrete representations (such as feature-based).</li>
<li>Compared to the baseline, the methods have very limited affect on the sizes of model parameters (see the column of $# P(M)$ in Table 3).</li>
</ul>
<p>Finally, our best-performing models are the best among all the single and supervised models.</p>
<h2>4 Analysis</h2>
<p>In this section, we use LDC2017T10 as our benchmark dataset to demonstrate how our proposed approach achieves higher performance than the baseline. As representative, we use CNN-based method to obtain structural representation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">System</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LDC2015E86</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LDC2017T10</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">Meteor</td>
<td style="text-align: center;">CHRF++</td>
<td style="text-align: center;">#P (M)</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">Meteor</td>
<td style="text-align: center;">CHRF++</td>
</tr>
<tr>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">25.50</td>
<td style="text-align: center;">33.16</td>
<td style="text-align: center;">59.88</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">27.43</td>
<td style="text-align: center;">34.62</td>
<td style="text-align: center;">61.85</td>
</tr>
<tr>
<td style="text-align: center;">Our Approach</td>
<td style="text-align: center;">feature-based</td>
<td style="text-align: center;">27.23</td>
<td style="text-align: center;">34.53</td>
<td style="text-align: center;">61.55</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">30.18</td>
<td style="text-align: center;">35.83</td>
<td style="text-align: center;">63.20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">avg-based</td>
<td style="text-align: center;">28.37</td>
<td style="text-align: center;">35.10</td>
<td style="text-align: center;">62.29</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">29.56</td>
<td style="text-align: center;">35.24</td>
<td style="text-align: center;">62.86</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sum-based</td>
<td style="text-align: center;">28.69</td>
<td style="text-align: center;">34.97</td>
<td style="text-align: center;">62.05</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">29.92</td>
<td style="text-align: center;">35.68</td>
<td style="text-align: center;">63.04</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SA-based</td>
<td style="text-align: center;">29.66</td>
<td style="text-align: center;">35.45</td>
<td style="text-align: center;">63.00</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">31.54</td>
<td style="text-align: center;">36.02</td>
<td style="text-align: center;">63.84</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CNN-based</td>
<td style="text-align: center;">29.10</td>
<td style="text-align: center;">35.00</td>
<td style="text-align: center;">62.10</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">31.82</td>
<td style="text-align: center;">36.38</td>
<td style="text-align: center;">64.05</td>
</tr>
<tr>
<td style="text-align: center;">Previous works with single models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Konstas et al. (2017)*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">22.00</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Cao and Clark (2019)*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Song et al. (2018) ${ }^{\ddagger}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">23.30</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Beck et al. (2018) ${ }^{\ddagger}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">50.4</td>
</tr>
<tr>
<td style="text-align: center;">Damonte and Cohen (2019) ${ }^{\ddagger}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">24.40</td>
<td style="text-align: center;">23.60</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">24.54</td>
<td style="text-align: center;">24.07</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Guo et al. (2019) ${ }^{\ddagger}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">57.3</td>
</tr>
<tr>
<td style="text-align: center;">Song et al. (2016) ${ }^{\ddagger}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">22.44</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Previous works with either ensemble models or unlabelled data, or both</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Konstas et al. (2017)*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Song et al. (2018) ${ }^{\ddagger}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Beck et al. (2018) ${ }^{\ddagger}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">53.5</td>
</tr>
<tr>
<td style="text-align: center;">Guo et al. (2019) ${ }^{\ddagger}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison results of our approaches and related studies on the test sets of LDC2015E86 and LDC2017T10. #P indicates the size of parameters in millions. * indicates seq2seq-based systems while ${ }^{\dagger}$ for graph-based models, and ${ }^{\ddagger}$ for other models. All our proposed systems are significant over the baseline at 0.01 , tested by bootstrap resampling (Koehn, 2004).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">System</th>
<th style="text-align: center;">BLEU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;">27.43</td>
</tr>
<tr>
<td style="text-align: center;">Our approach</td>
<td style="text-align: center;">31.82</td>
</tr>
<tr>
<td style="text-align: center;">No indirectly connected concept pairs</td>
<td style="text-align: center;">29.92</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance on the test set of our approach with or without modeling structural information of indirectly connected concept pairs.</p>
<h3>4.1 Effect of Modeling Structural Information of Indirectly Connected Concept Pairs</h3>
<p>Our approach is capable of modeling arbitrary concept pairs no matter whether directly connected or not. To investigate the effect of modeling structural information of indirectly connected concept pairs, we ignore their structural information by mapping all structural label sequences between two indirectly connected concept pairs into None. In this way, the structural label sequence for he and 7 in Table 1, for example, will be None.</p>
<p>Table 4 compares the performance of our approach with or without modeling structural information of indirectly connected concept pairs. It
shows that by modeling structural information of indirectly connected concept pairs, our approach improves the performance on the test set from 29.92 to 31.82 in BLEU scores. It also shows that even without modeling structural information of indirectly connected concept pairs, our approach achieves better performance than the baseline.</p>
<h3>4.2 Effect on AMR Graphs with Different Sizes of Reentrancies</h3>
<p>Linearizing an AMR graph into a sequence unavoidably loses information about reentrancies (nodes with multiple parents). This poses a challenge for the baseline since there exists on obvious sign that the first he and the second he, as shown in Figure 1 (b), refer to the same person. By contrast, our approach models reentrancies explicitly. Therefore, it is expected that the benefit of our approach is more evident for those AMR graphs containing more reentrancies. To test this hypothesis, we partition source AMR graphs to different groups by their numbers of reentrancies and evaluate their performance respectively. As shown in Figure 3, the performance gap be-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Performance (in BLEU) on the test set with respect to the reentrancy numbers of the input AMR graphs.
tween our approach and the baseline goes widest for AMR graphs with more than 5 reentrancies, on which our approach outperforms the baseline by 6.61 BLEU scores.</p>
<h3>4.3 Effect on AMR Graphs with Different Sizes</h3>
<p>When we encode an AMR graph with plenty concepts, linearizing it into a sequence tends to lose great amount of structural information. In order to test the hypothesis that graphs with more concepts contribute more to the improvement, we partition source AMR graphs to different groups by their sizes (i.e., numbers of concepts) and evaluate their performance respectively. Figure 4 shows the results which indicate that modeling graph structures significantly outperforms the baseline over all AMR lengths. We also observe that the performance gap between the baseline and our approach increases when AMR graphs become big, revealing that the baseline seq2seq model is far from capturing deep structural details of big AMR graphs. Figure 4 also indicates that text generation becomes difficult for big AMR graphs. We think that the low performance on big AMR graphs is mainly attributed to two reasons:</p>
<ul>
<li>Big AMR graphs are usually mapped into long sentences while seq2seq model tends to stop early for long inputs. As a result, the length ratio ${ }^{3}$ for AMRs with more than 40 concepts is 0.906 , much lower than that for AMRs with less concepts.</li>
</ul>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: Performance (in BLEU) on the test set with respect to the size of the input AMR graphs.</p>
<ul>
<li>Big AMR graphs are more likely to have reentrancies, which makes the generation more challenging.</li>
</ul>
<h3>4.4 Case Study</h3>
<p>In order to better understand the model performance, Figure 5 presents a few examples studied in Song et al. (2018) (Example (1)) and Damonte and Cohen (2019) (Examples (2) - (5)).</p>
<p>In Example (1), though our baseline recovers a propositional phrase for the noun staff and another one for the noun funding, it fails to recognize the anaphora and antecedent relation between the two propositional phrases. In contrast, our approach successfully recognizes :prep-for $c$ as a reentrancy node and generates one propositional phrase shared by both nouns staff and funding. In Example (2), we note that although AMR graphs lack tense information, the baseline generates output with inconsistent tense (i.e., do and found) while our approach consistently prefers past tense for the two clauses. In Example (3), only our approach correctly uses people as the subject of the predicate can. In Example (4), the baseline fails to predict the direct object you for predicate recommend. Finally in Example (5), the baseline fails to recognize the subject-predicate relation between noun communicate and verb need. Overall, we note that compared to the baseline, our approach produces more accurate output and deal with reentrancies more properly.</p>
<p>Comparing the generation of our approach and graph-based models in Song et al. (2018) and Damonte and Cohen (2019), we observe that our generation is more close to the reference in sentence structure. Due to the absence of tense informa-</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Examples of generation from AMR graphs. (1) is from Song et al. (2018), (2) - (5) are from Damonte and Cohen (2019). REF is the reference sentence. SEQ1 and G2S are the outputs of the seq2seq and the graph2seq models in Song et al. (2018), respectively. SEQ2 and GRAPH are the outputs of the seq2seq and the graph models in Damonte and Cohen (2019), respectively.
tion in AMR graphs, our model tends to use past tense, as provided and did in Example (1) and (2). Similarly, without information concerning singular form and plural form, our model is more likely to use plural nouns, as centers and lawyers in Example (1) and (5).</p>
<h2>5 Related Work</h2>
<p>Most studies in AMR-to-text generation regard it as a translation problem and are motivated by the recent advances in both statistical machine translation (SMT) and neural machine translation (NMT). Flanigan et al. (2016) first transform an AMR graph into a tree, then specify a number of tree-to-string transduction rules based on align-
ments that are used to drive a tree-based SMT model (Graehl and Knight, 2004). Pourdamghani et al. (2016) develop a method that learns to linearize AMR graphs into AMR strings, and then feed them into a phrase-based SMT model (Koehn et al., 2003). Song et al. (2017) use synchronous node replacement grammar (SNRG) to generate text. Different from synchronous context-free grammar in hierarchical phrase-based SMT (Chiang, 2007), SNRG is a grammar over graphs.</p>
<p>Moving to neural seq2seq approaches, Konstas et al. (2017) successfully apply seq2seq model together with large-scale unlabeled data for both text-to-AMR parsing and AMR-to-text generation. With special interest in the target side syn-</p>
<p>tax, Cao and Clark (2019) use seq2seq models to generate target syntactic structure, and then the surface form. To prevent the information loss in linearizing AMR graphs into sequences, (Song et al., 2018; Beck et al., 2018) propose graph-to-sequence models to encode graph structure directly. Focusing on reentrancies, Damonte and Cohen (2019) propose stacking encoders which consist of BiLSTM (Graves et al., 2013), TreeLSTMs (Tai et al., 2015), and Graph Convolutional Network (GCN) (Duvenaud et al., 2015; Kipf and Welling, 2016). Guo et al. (2019) propose densely connected GCN to better capture both local and non-local features. However, all the aforementioned graph-based models only consider the relations between nodes that are directly connected, thus lose the structural information between nodes that are indirectly connected via an edge path.</p>
<p>Recent studies also extend the Transformer to encode structural information for other NLP applications. Shaw et al. (2018) propose relationaware self-attention to capture relative positions of word pairs for neural machine translation. Ge et al. (2019) extend the relation-aware selfattention to capture syntactic and semantic structures. Our model is inspired by theirs but aims to encode structural label sequences of concept pairs. Koncel-Kedziorski et al. (2019) propose graph Transformer to encode graph structure. Similar to the GCN, it focuses on the relations between directly connected nodes.</p>
<h2>6 Conclusion and Future Work</h2>
<p>In this paper we proposed a structure-aware selfattention for the task of AMR-to-text generation. The major idea of our approach is to encode long-distance relations between concepts in AMR graphs into the self-attention encoder in the Transformer. In the setting of supervised learning, our models achieved the best experimental results ever reported on two English benchmarks.</p>
<p>Previous studies have shown the effectiveness of using large-scale unlabelled data. In future work, we would like to do semi-supervised learning and use silver data to test how much improvements could be further achieved.</p>
<h2>Acknowledgments</h2>
<p>We thank the anonymous reviewers for their insightful comments and suggestions. We are grateful to Linfeng Song for fruitful discussions. This
work is supported by the National Natural Science Foundation of China (Grant No. 61876120, 61673290, 61525205), and the Priority Academic Program Development of Jiangsu Higher Education Institutions.</p>
<h2>References</h2>
<p>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of 7th Linguistic Annotation Workshop \&amp; Interoperability with Discourse, pages 178-186.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of $A C L$, pages $65-72$.</p>
<p>Daniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-sequence learning using gated graph neural networks. In Proceedings of ACL, pages 273-283.</p>
<p>Kris Cao and Stephen Clark. 2019. Factorising amr generation through syntax. In Proceedings of NAACL, pages 2157-2163.</p>
<p>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33:201-228.</p>
<p>Marco Damonte and Shay B. Cohen. 2019. Structural neural encoders for AMR-to-text generation. In Proceedings of NAACL, pages 3649-3658.</p>
<p>Michael Denkowski and Alon Lavie. 2014. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of WMT, pages 376-380.</p>
<p>David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timonthy Hirzel, Alan Aspuru-Guzik, and Ryan P Adams. 2015. Convolutional networks on graphs for learning molecular fingerprints. In Proceedings of NIPS, pages 22242232.</p>
<p>Jeffrey Flanigan, Chris Dyer, Noah A. Smith, and Jaime Carbonell. 2016. Generation from abstract meaning representation using tree transducers. In Proceedings of NAACL, pages 731-739.</p>
<p>DongLai Ge, Junhui Li, Muhua Zhu, and Shoushan Li. 2019. Modeling source syntax and semantics for neural amr parsing. In Proceedings of IJCAI, pages 4975-4981.</p>
<p>Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. In Proceedings of NAACL, pages 105112 .</p>
<p>Alex Graves, Abdel rahman Mohamed, and Geoffrey Hinton. 2013. Speech recognition with deep recurrent neural networks. In Proceedings of ICASSP, pages 6645-6649.</p>
<p>Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. 2016. Pointing the unknown words. In Proceedings of ACL, pages $140-149$.</p>
<p>Zhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei Lu. 2019. Densely connected graph convolutional networks for graph-to-sequence learning. Transactions of the Association of Computational Linguistics, 7:297-312.</p>
<p>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In Proceedings of ACL, pages $655-665$.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of $I C L R$.</p>
<p>Thomas N Kipf and Max Welling. 2016. Semisupervised classification with graph convolutional networks. In Proceedings of $I C L R$.</p>
<p>Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M. Rush. 2017. Opennmt: Open-source toolkit for neural machine translation. In Proceedings of ACL, System Demonstrations, pages 67-72.</p>
<p>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP, pages 388-395.</p>
<p>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL, pages 127-133.</p>
<p>Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi. 2019. Text generation from knowledge graphs with graph transformers. In Proceedings of NAACL, pages 2284-2293.</p>
<p>Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer. 2017. Neural AMR: Sequence-to-sequence models for parsing and generation. In Proceedings of ACL, pages 146-157.</p>
<p>Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. A structured self-attentive sentence embedding. In Proceedings of $I C L R$.</p>
<p>Kishore Papineni, Salim Roukos, Ward Todd, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of $A C L$, pages 311-318.</p>
<p>Maja Popovi. 2017. chrf++: words helping character n-grams. In Proceedings of WMT, pages 612-618.</p>
<p>Nima Pourdamghani, Kevin Knight, and Ulf Hermjakob. 2016. Generating english from abstract meaning representations. In Proceedings of the 9th International Natural Language Generation conference, pages 21-25.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of ACL, pages 17151725.</p>
<p>Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. In Proceedings of NAACL, pages 464-468.</p>
<p>Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2017. Amr-to-text generation with synchronous node replacement grammar. In Proceedings of $A C L$, pages 7-13.</p>
<p>Linfeng Song, Yue Zhang, Xiaochang Peng, Zhiguo Wang, and Daniel Gildea. 2016. Amr-to-text generation as a traveling salesman problem. In Proceedings of EMNLP, pages 2084-2089.</p>
<p>Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. A graph-to-sequence model for AMR-to-text generation. In Proceedings of ACL, pages 1616-1626.</p>
<p>Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of ACL, pages 1556-1566.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of NIPS, pages 59986008.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Length ratio is the length of generation output, divided by the length of reference.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
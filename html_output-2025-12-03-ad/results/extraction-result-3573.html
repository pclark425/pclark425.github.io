<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3573 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3573</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3573</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-79.html">extraction-schema-79</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <p><strong>Paper ID:</strong> paper-01a4fe4160f702667f178415b08f6176e3f85892</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/01a4fe4160f702667f178415b08f6176e3f85892" target="_blank">ForceGen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a protein language diffusion model</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A generative model is reported that predicts protein designs to meet complex nonlinear mechanical property-design objectives and leverages deep knowledge on protein sequences from a pre-trained protein language model and maps mechanical unfolding responses to create novel proteins.</p>
                <p><strong>Paper Abstract:</strong> Through evolution, nature has presented a set of remarkable protein materials, including elastins, silks, keratins and collagens with superior mechanical performances that play crucial roles in mechanobiology. However, going beyond natural designs to discover proteins that meet specified mechanical properties remains challenging. Here we report a generative model that predicts protein designs to meet complex nonlinear mechanical property-design objectives. Our model leverages deep knowledge on protein sequences from a pre-trained protein language model and maps mechanical unfolding responses to create novel proteins. Via full-atom molecular simulations for direct validation, we demonstrate that the designed proteins are novel, and fulfill the targeted mechanical properties, including unfolding energy and mechanical strength, as well as the detailed unfolding force-separation curves. Our model offers rapid pathways to explore the enormous mechanobiological protein sequence space unconstrained by biological synthesis, using mechanical features as target to enable the discovery of protein materials with superior mechanical properties.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3573.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3573.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>pLDM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>protein Language Diffusion Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end generative model that combines a pretrained protein language model (pLM) latent space with an attention-based diffusion denoising model to generate de novo protein sequences conditioned on target mechanical unfolding (force-separation) responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>pLDM (protein Language Diffusion Model + ESM-2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Architecture: pretrained protein language model (ESM-2 variant) used to convert tokenized amino-acid sequences into a word-probability latent space (logits -> normalized probabilities), combined with a trainable 1D U-Net attention-based diffusion model that performs denoising/generation in that latent probability space. The pretrained pLM used in this work is an ESM-2 variant with ~150M parameters, pretrained on large protein sequence corpora (UniRef). Only the diffusion model is trained on the MD-curated mechanical dataset (~6,863 training proteins).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Conditional latent diffusion in the pLM word-probability space: diffusion model iteratively denoises a random seed under the condition of a target pulling-force vector to produce a probability tensor; final sequences are obtained by selecting highest-probability tokens from the pLM output logits and then folded by OmegaFold and validated by steered MD.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo protein sequence design for specified mechanical (nanomechanical) unfolding responses — materials/biomaterials/mechanobiology (design of proteins with target unfolding force-separation curves, toughness and strength).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Per-sequence and dataset metrics: R^2 for vector comparisons (pulling-force vector), relative L2 error for pulling-force vectors, relative L1 error for scalar properties; property-specific comparisons for toughness (unfolding energy) and strength (max pulling force); novelty assessed by protein BLAST (percent identity / 'no significant similarity').</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Trained on ~6,863 proteins (N<=126), tested on 187 held-out cases. Median design accuracy over test set: pulling-force vector median R^2 = 0.56 and median relative L2 error = 0.36. Toughness (unfolding energy) prediction from generated proteins: R^2 = 0.93; strength (max force): R^2 = 0.41. Many generated sequences are de novo per BLAST (a bimodal distribution of percent identity with a strong peak of 'no significant similarity'). De novo (mixed) pulling-force inputs produced novel sequences with no significant BLAST matches in case studies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared against vanilla protein-diffusion models (pDM) operating in token space: pLDM outperformed pDM variants on 9 of 11 reported metrics (higher R^2 / lower errors), including better overall match to pulling-force vectors and mechanical properties; pLDM showed particular strength on small-data conditioned generation due to leveraging pretrained pLM latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Reported limitations include: (1) limited component-wise (per-sampling-point) accuracy — designed proteins often cannot precisely match every force-sampling point, causing finite spread around the ideal y=x relation; (2) dataset size is relatively small (6,863 training examples) because full-atom MD curation is expensive, limiting learning of very fine-grained mappings; (3) some generated sequences show similarity to known proteins (bimodal novelty) — not universally de novo; (4) constructing physically achievable / globally consistent de novo pulling-force vectors is nontrivial; (5) computational cost of MD validation and dependence on OmegaFold and MD pipelines for folding/validation; (6) the medium-sized pLM was chosen for compute efficiency — larger pLMs might improve performance but require more resources.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ForceGen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a protein language diffusion model', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3573.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3573.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ESM-2 (150M variant)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evolutionary Scale Modeling 2 (ESM-2) protein language model, ~150M parameter variant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based protein language model pretrained on large protein sequence databases (UniRef) that produces per-position logits/probabilities which capture sequence-structure-property priors used here as the latent representation for diffusion-based sequence generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evolutionary-scale prediction of atomiclevel protein structure with a language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ESM-2 (150M variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based protein language model (evolutionary-scale language model for proteins). The paper used a medium-sized variant with ~150M parameters to produce last-layer logits that are normalized into a B x M x N probability tensor (M = token vocabulary) as the diffusion latent space. Pretraining corpora referenced include UniRef datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Used as an encoder/decoder bridge: sequences are tokenized and passed through ESM-2; last-layer logits are normalized into a word-probability latent space on which the diffusion model operates; final probability tensors are mapped back to tokens via argmax on the pLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Representation and generation of protein sequences for design tasks (here: conditioning on mechanical unfolding responses to design proteins).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not evaluated in isolation in this paper for generation quality, but its use enabled improved downstream design metrics for the diffusion model (see pLDM metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The 150M ESM-2 variant provided a latent space in which the diffusion model (pLDM) achieved better small-data conditioned generation performance compared to diffusion in token space; the authors note improved performance when using the pretrained pLM latent representation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared implicitly to operating diffusion directly in tokenized sequence space (pDM): using pLM latent space (ESM-2) improved design accuracy on small dataset tasks. Authors note larger pLMs might further improve but at higher compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>The authors limited themselves to a medium-sized ESM-2 for computational efficiency; they note larger pLMs require more compute and cost. ESM-2 is used as a fixed (non-finetuned) encoder in this work, so any mismatch between pretraining distribution and mechanical-property-conditioned generation limits representational alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ForceGen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a protein language diffusion model', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3573.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3573.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>pDM (vanilla protein Diffusion Model)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>protein Diffusion Model (token-space diffusion baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diffusion-based generative model operating directly in the tokenized amino-acid sequence space (as in prior work), used here as a baseline in both one-shot and iterative design variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative design of de novo proteins based on secondary-structure constraints using an attention-based diffusion model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>pDM (token-space attention-based diffusion)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Attention-based diffusion model (1D U-Net + attention) that operates directly on tokenized protein sequences rather than a pretrained pLM probability latent space; implemented in single-shot (AM1) and iterative (designer + predictor) variants for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Token-space conditional diffusion: directly denoise/generate token sequences conditioned on input pulling-force vectors (AM1), or iterative generate-and-evaluate scheme where a separate predictor scores candidates and the best of multiple samples is chosen (iterative AM2).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo protein sequence generation conditioned on unfolding-force responses (same application domain as pLDM) for baseline comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same as pLDM: R^2 and relative L2 for pulling-force vectors, R^2/relative L1 for toughness and strength, BLAST-based novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Both pDM variants underperformed relative to pLDM on the majority of metrics (pLDM beat pDMs on 9 of 11 reported comparisons). The iterative pDM scheme improved performance over single-shot pDM but still lagged behind pLDM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Serves as baseline; pLDM (with pretrained pLM latent space) provides measurable improvements, especially in the low-data regime.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Operating in token space requires more labelled examples to reach similar performance; iterative scheme increases computational cost; less effective on small datasets compared to the pLDM approach that leverages pretrained sequence knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ForceGen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a protein language diffusion model', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3573.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3573.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Madani et al. 2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models generate functional protein sequences across diverse families</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work demonstrating that large language models can generate functional protein sequences across diverse protein families (cited as evidence that LLMs can produce functional protein sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models generate functional protein sequences across diverse families</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>large language models for protein sequence generation (as reported by Madani et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cited as a demonstration that LLMs (autoregressive / large transformer-based protein models) can generate functional protein sequences across diverse families; specific architectures and sizes are not detailed in this paper beyond the citation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Mentioned generatively: LLMs used to directly generate protein sequences (citation only; this paper does not implement these methods itself).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Protein sequence design / generation across diverse protein families (general protein engineering applications).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in detail in this paper; cited as precedent for LLM-based protein generation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as evidence and context for the utility of LLMs in protein sequence generation. This paper uses that prior work to motivate integrating a pretrained pLM into a diffusion-based design pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>This paper cites Madani et al. as related work but does not reproduce their comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not discussed in detail in this paper; included as related-work support for LLM applicability to protein generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ForceGen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a protein language diffusion model', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative design of de novo proteins based on secondary-structure constraints using an attention-based diffusion model <em>(Rating: 2)</em></li>
                <li>Evolutionary-scale prediction of atomiclevel protein structure with a language model <em>(Rating: 2)</em></li>
                <li>Large language models generate functional protein sequences across diverse families <em>(Rating: 2)</em></li>
                <li>De novo design of protein structure and function with RFdiffusion <em>(Rating: 2)</em></li>
                <li>Protein Structure and Sequence Generation with Equivariant Denoising Diffusion Probabilistic Models <em>(Rating: 1)</em></li>
                <li>Diffusion-LM Improves Controllable Text Generation <em>(Rating: 1)</em></li>
                <li>DiffSDS: A language diffusion model for protein backbone inpainting under geometric conditions and constraints <em>(Rating: 1)</em></li>
                <li>High-Resolution de novo structure prediction from primary sequence <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3573",
    "paper_id": "paper-01a4fe4160f702667f178415b08f6176e3f85892",
    "extraction_schema_id": "extraction-schema-79",
    "extracted_data": [
        {
            "name_short": "pLDM",
            "name_full": "protein Language Diffusion Model",
            "brief_description": "An end-to-end generative model that combines a pretrained protein language model (pLM) latent space with an attention-based diffusion denoising model to generate de novo protein sequences conditioned on target mechanical unfolding (force-separation) responses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "pLDM (protein Language Diffusion Model + ESM-2)",
            "model_description": "Architecture: pretrained protein language model (ESM-2 variant) used to convert tokenized amino-acid sequences into a word-probability latent space (logits -&gt; normalized probabilities), combined with a trainable 1D U-Net attention-based diffusion model that performs denoising/generation in that latent probability space. The pretrained pLM used in this work is an ESM-2 variant with ~150M parameters, pretrained on large protein sequence corpora (UniRef). Only the diffusion model is trained on the MD-curated mechanical dataset (~6,863 training proteins).",
            "generation_method": "Conditional latent diffusion in the pLM word-probability space: diffusion model iteratively denoises a random seed under the condition of a target pulling-force vector to produce a probability tensor; final sequences are obtained by selecting highest-probability tokens from the pLM output logits and then folded by OmegaFold and validated by steered MD.",
            "application_domain": "De novo protein sequence design for specified mechanical (nanomechanical) unfolding responses — materials/biomaterials/mechanobiology (design of proteins with target unfolding force-separation curves, toughness and strength).",
            "evaluation_metrics": "Per-sequence and dataset metrics: R^2 for vector comparisons (pulling-force vector), relative L2 error for pulling-force vectors, relative L1 error for scalar properties; property-specific comparisons for toughness (unfolding energy) and strength (max pulling force); novelty assessed by protein BLAST (percent identity / 'no significant similarity').",
            "results_summary": "Trained on ~6,863 proteins (N&lt;=126), tested on 187 held-out cases. Median design accuracy over test set: pulling-force vector median R^2 = 0.56 and median relative L2 error = 0.36. Toughness (unfolding energy) prediction from generated proteins: R^2 = 0.93; strength (max force): R^2 = 0.41. Many generated sequences are de novo per BLAST (a bimodal distribution of percent identity with a strong peak of 'no significant similarity'). De novo (mixed) pulling-force inputs produced novel sequences with no significant BLAST matches in case studies.",
            "comparison_to_baselines": "Compared against vanilla protein-diffusion models (pDM) operating in token space: pLDM outperformed pDM variants on 9 of 11 reported metrics (higher R^2 / lower errors), including better overall match to pulling-force vectors and mechanical properties; pLDM showed particular strength on small-data conditioned generation due to leveraging pretrained pLM latent space.",
            "limitations_challenges": "Reported limitations include: (1) limited component-wise (per-sampling-point) accuracy — designed proteins often cannot precisely match every force-sampling point, causing finite spread around the ideal y=x relation; (2) dataset size is relatively small (6,863 training examples) because full-atom MD curation is expensive, limiting learning of very fine-grained mappings; (3) some generated sequences show similarity to known proteins (bimodal novelty) — not universally de novo; (4) constructing physically achievable / globally consistent de novo pulling-force vectors is nontrivial; (5) computational cost of MD validation and dependence on OmegaFold and MD pipelines for folding/validation; (6) the medium-sized pLM was chosen for compute efficiency — larger pLMs might improve performance but require more resources.",
            "uuid": "e3573.0",
            "source_info": {
                "paper_title": "ForceGen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a protein language diffusion model",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ESM-2 (150M variant)",
            "name_full": "Evolutionary Scale Modeling 2 (ESM-2) protein language model, ~150M parameter variant",
            "brief_description": "A transformer-based protein language model pretrained on large protein sequence databases (UniRef) that produces per-position logits/probabilities which capture sequence-structure-property priors used here as the latent representation for diffusion-based sequence generation.",
            "citation_title": "Evolutionary-scale prediction of atomiclevel protein structure with a language model",
            "mention_or_use": "use",
            "model_name": "ESM-2 (150M variant)",
            "model_description": "Transformer-based protein language model (evolutionary-scale language model for proteins). The paper used a medium-sized variant with ~150M parameters to produce last-layer logits that are normalized into a B x M x N probability tensor (M = token vocabulary) as the diffusion latent space. Pretraining corpora referenced include UniRef datasets.",
            "generation_method": "Used as an encoder/decoder bridge: sequences are tokenized and passed through ESM-2; last-layer logits are normalized into a word-probability latent space on which the diffusion model operates; final probability tensors are mapped back to tokens via argmax on the pLM outputs.",
            "application_domain": "Representation and generation of protein sequences for design tasks (here: conditioning on mechanical unfolding responses to design proteins).",
            "evaluation_metrics": "Not evaluated in isolation in this paper for generation quality, but its use enabled improved downstream design metrics for the diffusion model (see pLDM metrics).",
            "results_summary": "The 150M ESM-2 variant provided a latent space in which the diffusion model (pLDM) achieved better small-data conditioned generation performance compared to diffusion in token space; the authors note improved performance when using the pretrained pLM latent representation.",
            "comparison_to_baselines": "Compared implicitly to operating diffusion directly in tokenized sequence space (pDM): using pLM latent space (ESM-2) improved design accuracy on small dataset tasks. Authors note larger pLMs might further improve but at higher compute cost.",
            "limitations_challenges": "The authors limited themselves to a medium-sized ESM-2 for computational efficiency; they note larger pLMs require more compute and cost. ESM-2 is used as a fixed (non-finetuned) encoder in this work, so any mismatch between pretraining distribution and mechanical-property-conditioned generation limits representational alignment.",
            "uuid": "e3573.1",
            "source_info": {
                "paper_title": "ForceGen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a protein language diffusion model",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "pDM (vanilla protein Diffusion Model)",
            "name_full": "protein Diffusion Model (token-space diffusion baseline)",
            "brief_description": "A diffusion-based generative model operating directly in the tokenized amino-acid sequence space (as in prior work), used here as a baseline in both one-shot and iterative design variants.",
            "citation_title": "Generative design of de novo proteins based on secondary-structure constraints using an attention-based diffusion model",
            "mention_or_use": "use",
            "model_name": "pDM (token-space attention-based diffusion)",
            "model_description": "Attention-based diffusion model (1D U-Net + attention) that operates directly on tokenized protein sequences rather than a pretrained pLM probability latent space; implemented in single-shot (AM1) and iterative (designer + predictor) variants for comparison.",
            "generation_method": "Token-space conditional diffusion: directly denoise/generate token sequences conditioned on input pulling-force vectors (AM1), or iterative generate-and-evaluate scheme where a separate predictor scores candidates and the best of multiple samples is chosen (iterative AM2).",
            "application_domain": "De novo protein sequence generation conditioned on unfolding-force responses (same application domain as pLDM) for baseline comparison.",
            "evaluation_metrics": "Same as pLDM: R^2 and relative L2 for pulling-force vectors, R^2/relative L1 for toughness and strength, BLAST-based novelty.",
            "results_summary": "Both pDM variants underperformed relative to pLDM on the majority of metrics (pLDM beat pDMs on 9 of 11 reported comparisons). The iterative pDM scheme improved performance over single-shot pDM but still lagged behind pLDM.",
            "comparison_to_baselines": "Serves as baseline; pLDM (with pretrained pLM latent space) provides measurable improvements, especially in the low-data regime.",
            "limitations_challenges": "Operating in token space requires more labelled examples to reach similar performance; iterative scheme increases computational cost; less effective on small datasets compared to the pLDM approach that leverages pretrained sequence knowledge.",
            "uuid": "e3573.2",
            "source_info": {
                "paper_title": "ForceGen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a protein language diffusion model",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Madani et al. 2023",
            "name_full": "Large language models generate functional protein sequences across diverse families",
            "brief_description": "A cited work demonstrating that large language models can generate functional protein sequences across diverse protein families (cited as evidence that LLMs can produce functional protein sequences).",
            "citation_title": "Large language models generate functional protein sequences across diverse families",
            "mention_or_use": "mention",
            "model_name": "large language models for protein sequence generation (as reported by Madani et al.)",
            "model_description": "Cited as a demonstration that LLMs (autoregressive / large transformer-based protein models) can generate functional protein sequences across diverse families; specific architectures and sizes are not detailed in this paper beyond the citation.",
            "generation_method": "Mentioned generatively: LLMs used to directly generate protein sequences (citation only; this paper does not implement these methods itself).",
            "application_domain": "Protein sequence design / generation across diverse protein families (general protein engineering applications).",
            "evaluation_metrics": "Not specified in detail in this paper; cited as precedent for LLM-based protein generation.",
            "results_summary": "Cited as evidence and context for the utility of LLMs in protein sequence generation. This paper uses that prior work to motivate integrating a pretrained pLM into a diffusion-based design pipeline.",
            "comparison_to_baselines": "This paper cites Madani et al. as related work but does not reproduce their comparisons.",
            "limitations_challenges": "Not discussed in detail in this paper; included as related-work support for LLM applicability to protein generation.",
            "uuid": "e3573.3",
            "source_info": {
                "paper_title": "ForceGen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a protein language diffusion model",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative design of de novo proteins based on secondary-structure constraints using an attention-based diffusion model",
            "rating": 2
        },
        {
            "paper_title": "Evolutionary-scale prediction of atomiclevel protein structure with a language model",
            "rating": 2
        },
        {
            "paper_title": "Large language models generate functional protein sequences across diverse families",
            "rating": 2
        },
        {
            "paper_title": "De novo design of protein structure and function with RFdiffusion",
            "rating": 2
        },
        {
            "paper_title": "Protein Structure and Sequence Generation with Equivariant Denoising Diffusion Probabilistic Models",
            "rating": 1
        },
        {
            "paper_title": "Diffusion-LM Improves Controllable Text Generation",
            "rating": 1
        },
        {
            "paper_title": "DiffSDS: A language diffusion model for protein backbone inpainting under geometric conditions and constraints",
            "rating": 1
        },
        {
            "paper_title": "High-Resolution de novo structure prediction from primary sequence",
            "rating": 1
        }
    ],
    "cost": 0.013437749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ForceGen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a protein language diffusion model</h1>
<p>Bo $\mathrm{Ni}^{1}$, David L. Kaplan ${ }^{2}$, Markus J. Buehler ${ }^{1,3,4 #}$<br>${ }^{1}$ Laboratory for Atomistic and Molecular Mechanics (LAMM), Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, USA<br>${ }^{2}$ Department of Biomedical Engineering, Tufts University, Medford, MA 02155, USA<br>${ }^{3}$ Center for Computational Science and Engineering, Schwarzman College of Computing, Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, USA<br>${ }^{4}$ Lead contact<br>*Correspondence: mbuehler@MIT.EDU</p>
<h4>Abstract</h4>
<p>Through evolution, nature has presented a set of remarkable protein materials, including elastins, silks, keratins and collagens with superior mechanical performances that play crucial roles in mechanobiology. However, going beyond natural designs to discover proteins that meet specified mechanical properties remains challenging. Here we report a generative model that predicts protein designs to meet complex nonlinear mechanical property-design objectives. Our model leverages deep knowledge on protein sequences from a pretrained protein language model and maps mechanical unfolding responses to create novel proteins. Via full-atom molecular simulations for direct validation, we demonstrate that the designed proteins are novel, and fulfill the targeted mechanical properties, including unfolding energy and mechanical strength, as well as the detailed unfolding force-separation curves. Our model offers rapid pathways to explore the enormous mechanobiological protein sequence space unconstrained by biological synthesis, using mechanical features as target to enable the discovery of protein materials with superior mechanical properties.</p>
<p>Teaser: Based on a new force-unfolding dataset derived from full-atomistic molecular modeling, we propose an algorithm that designs novel proteins that meet complex nonlinear mechanical unfolding properties, including detailed unfolding force-separation curves.</p>
<p>Keywords: Protein design; Generative deep learning; Language diffusion model; de novo proteins; mechanical unfolding; strength and toughness</p>
<h2>1. Introduction</h2>
<p>Proteins present an elegant yet complex and rich design platform. The various functions and outstanding properties of proteins can be attributed to the folded three-dimensional (3D) structures, encoded by the underling one-dimensional (1D) primary sequences consisting of about 20 naturally occurring amino acids (1). Through evolution, nature has demonstrated great success in "designing" proteins as a set of critical building blocks that constitute fundamental functions of all life, and specifically significant biomaterials, ranging from the structural hierarchies in collagens, complex assemblies such as silk, to tissue assemblies such as muscle and skin (2-5). In these various tissues and systems, the detailed mechanical signature - often, their response to mechanical pulling - is an essential feature for mechanobiology (6-9).</p>
<p>At the same time, there remains vast design space of mechanically optimized proteins yet unexplored by nature given the enormous possibilities of protein sequences (10). Hence, inspired by nature, discovering de novo proteins may unlock potentially unprecedented properties and functions (3,10-16). However, this enormous design space and costs associated with experimental testing that present great challenges remain in finding effective tools to design de novo protein sequences that meet a set of interested functions or properties (14-19).</p>
<p>In recent years, the rapid development of deep learning approaches and their applications to proteins have provided fast avenues for protein study and design. For forward problems focused on structure identification, deep learning-based tools such as AlphaFold2 (20) and RoseTTAFold(21) represents a breakthrough in achieving competitive accuracy with experimental methods in predicting 3D folded structures based on protein sequences at a much reduced cost.(22) Built upon these approaches, other protein folding tools (e.g., Omegafold (23), RGN2 (24), HelixFold-single(25) and ESMFold (26)) have been exploring the application of large language models. By removing dependence on multiple sequence alignments (MSAs) as the input, improvements in further reducing computational costs and achieving better predictions for orphan and rapidly evolving proteins have been demonstrated $(23-26)$.</p>
<p>End-to-end models based on deep learning that predict various structural features (e.g., secondary structure type and content (27-32), binding sites (33) and surfaces (34)) and properties (e.g., solubility (16, 35, 36), melting temperature (37), natural vibrational frequencies $(38,39)$ and strength(40)) for given sequences have also been reported. At the sample time, the inverse design of de novo proteins that meet desired structural or property features present a more challenging task. On one hand, facing the enormous sequence space, search algorithms teamed with efficient deep learning-based forward predictors $(29,41,42)$ may still suffer from inefficient exploration and the design accuracy and varieties of the discovered sequences are not easily controlled. On the other hand, recently emergent generative models (43-48) provide a direct map from the desired characteristics to potential designs and are becoming a novel paradigm for various materials research and design (49-54), including proteins. For example, using an attention-based diffusion model trained on secondary structure data, de novo protein sequences can be generated based on secondary structure design objectives (55). However, these generative design models often focus on structural level design (such as, secondary structures (55) or detailed protein backbone shapes(56-59)). In contrast, development of generative models aimed at end-to-end design from property of interest to protein sequence remain rare (60).</p>
<p>In this paper, we focus on nanomechanical properties (61-64) of proteins. Thanks to the advent of singlemolecule technology (65) (e.g., atomic force microscopy $(66,67)$, optical tweezers $(68,69)$ and magnetic tweezers $(70,71)$ ), the measurement of protein unfolding under an applied mechanical force provides a unique molecular basis for understanding protein deformation (elasticity/plasticity) and fracture (63) and can play key roles in affecting some macroscopic mechanical properties of protein-based materials due to the inherent structural hierarchy. For example, via experimental measurements and theoretical analysis, it has been demonstrated that toughness of synthetic protein hydrogels can be correlated to the mechanical unfolding responses of the protein molecules and that mechanically strong folded proteins can result in tough hydrogel designs (72). Therefore, generating de novo proteins that meet desired mechanical unfolding responses can represent a key molecular level design step in protein-based material designs. Compared with previous protein design cases, this problem presents some unique challenges. First, this is a property-to-sequence end-to-end design task bypassing the structure level, which is expected to be more difficult than previous structure-to-sequence design tasks $(55,59)$ Second, the available or affordable data on mechanical unfolding responses of known proteins (73) are rare when compared to those for protein structures (74) or sequences (75). Besides mechanical properties, we expect these two challenges are also shared by many other property-to-sequence design tasks in proteins.</p>
<p>To address this problem, in this paper, we combine an attention-based (76) diffusion model (55) with a pretrained large language model (26) for proteins to construct a generative deep learning model that predicts amino acid sequences and 3D protein structures based on mechanical unfolding responses as design objectives. In a singular workflow (Figure 1), we start with performing a large series of full-atom molecular dynamics (MD) to simulate the mechanical unfolding process of PDB (77) proteins and recording the force responses (Figure 1a). Then, we construct a protein language diffusion model (pLDM) by translating the protein sequences into a word probability latent space using a pretrained protein language model and training a diffusion model to learn the map between sequence representations and the force-separation responses (Figure 1b). At deployment, the trained pLDM predicts sequence candidates based on the given unfolding force conditions and the integrated folding algorithm</p>
<p>(i.e., OmageFold) (23) determines the 3D structures of the resulting sequences. For validation, we compare the designed sequences with known proteins to analyze novelty (Figure 1c) and to test the designed proteins using MD to compare the mechanical properties and unfolding responses with input conditions (Figure 1d). Through well-controlled comparisons, we demonstrate that our pLDM outperforms the vanilla diffusion model with or without an iterative design scheme. Built upon the property-to-sequence generation capability of our model and the broad potential of protein materials in achieving superior mechanical properties, as well as other interesting properties $(78-81)$ (e.g., optical $(80,81)$, electronic $(78)$, energy storage $(79)$, etc.) we expect our end-to-end design model can be useful in numerous biological and engineering applications for the property-targeted generative design of various protein material systems.</p>
<h1>2. Results and Discussion</h1>
<h2>Full-atom modeling of unfolding proteins by force</h2>
<p>Inspired by single-molecule force-spectroscopy(66), we simulate the unfolding process of protein chains under mechanical force to understand their mechanical properties at the molecular level. As shown in Figure 2a, we start with PDB proteins with experimentally measured 3D structures. Using full-atom MD with the CHARMM force field(82) and a generalized Born implicit solvent model (83), we first relax the protein molecule at body temperature (i.e., 310 K ) to reach equilibrium conformation. Then, we stretch the protein chain of $N$ amino acids along the direction connecting the two chain ends (i.e., $a$ and $b$ ) by fixing one end and steering the other with a spring (i.e., the segment between $b$ and $c$ in Figure 2a) of a force constant $k=0.5 \mathrm{kcal} /\left(\mathrm{mol} \AA^{2}\right)$ at a constant velocity $v=0.1 \AA / \mathrm{ps}$. The pulling force, $F_{p}$, is recorded every 0.2 ps until the distance between two pulling ends, $L_{a c}$, reaches the contour length, $L_{\text {con }}$, of the protein chain, where we assume the average length of each amino acid is $3.6 \AA(84)$ and $L_{\text {con }}=N \times 3.6 \AA$. Further details on the MD simulations can be found in the Materials and Methods section. Movies of the unfolding trajectory of some selected PDB protein examples can be found in in the Supplementary Materials.</p>
<p>In Figure 2b, we smooth the raw force response (the red curve) to get rid of high frequency fluctuations and get the unfolding response, $F_{p}\left(L_{a c}\right)$, of the protein chain (the blue curve), from which we can identify the toughness and strength of the protein molecule using the unfolding energy $T$ and the maximal value of force $F_{\max }$ defined as the following.</p>
<p>$$
\begin{aligned}
&amp; T=\int^{L_{c o n}} F_{p} d L_{a c} \
&amp; F_{\max }=\max <em a="a" c="c">{L</em>\right}
\end{aligned}
$$} \leq L_{c o n}}\left{F_{p</p>
<p>To curate a dataset based on naturally existing proteins, we use the Biomolecule Stretching Database (BSDB) (73) as a guidance and select 7,026 PDB proteins that have no gaps in their experimentally determined structures and consist of no more than 128 amino acids. Then, we collect their structures directly from the Protein Data Bank (77) and apply the protocol above to test their mechanical unfolding responses. An overview of the distributions of the unfolding responses and mechanical properties are shown in Figure 2c-d. Specifically, in Figure 2d, the unfolding energy or toughness shows a bimodal distribution while the strength presents a unimodal one; in Figure 2c, one can observe that there exist various unfolding responses among proteins. For example, the maximal force may appear as the peak in the middle of unfolding process or near the end when reaching the contour length, which may indicate very different deformation mechanisms. An in-depth study of these mechanical properties and their dependence on the structural features and sequences for a large number of proteins deserves a separate study and is left for future work. Further insight can also be obtained from the numerous experimental studies using AFM-based force generative outcomes during unfolding states of proteins. Here, we focus on applying this newly collected data to develop protein generation models. To efficiently label the pulling force responses during the full unfolding process, we introduce a pulling force vector $\vec{F}_{p}$ (green triangles in Figure 2b) to represent the full response as the following</p>
<p>$$
\vec{F}<em p="p">{p}=\left{F</em>\right): i=0,1,2, \ldots, N\right}
$$}\left(L_{a c}^{i</p>
<p>where $N$ is the sequence length of the protein chain and we sample the pulling force when the distance between the pulling ends reaches $L_{a c}^{i}=i \times L_{\text {con }} / N$. For $L_{a c}^{i}$ that is smaller than the value of the initial equilibrium conformation, we simply define the force values as zeros. Such a vector representation can adjust to the protein sequence length automatically, that is, longer/shorter proteins with potentially more/fewer unfolding details have more/fewer sampling points evenly distributed. Next, we develop DL models to generate protein sequences that meet the given mechanically unfolding responses represented in terms of the pulling force vector $\vec{F}_{p}$.</p>
<h1>Protein language diffusion model and inverse design for mechanical signatures</h1>
<p>To solve the conditioned protein design task, here we develop a protein language diffusion model (pLDM) by combining a pretrained protein language model (pLM) (26) and an attention-based diffusion model(55). Figure 3a depicts an overview of the model developed in the present work. The pLM (on the right of Figure 3a) is pretrained on large amounts of protein sequences data (85) to form internal representations that better understand not only sequences but also structures and properties of proteins.(32) We leverage this knowledge by applying pLMs to translate protein sequences from the tokenized sequence space into the word probability latent space. Then, we train a diffusion model developed in the previous work to operate in this probability latent space. The diffusion model is built upon a one-dimensional U-Net architecture with attention mechanisms (Figure 3b). At deployment, starting with the given condition (on the left of Figure 3a) and random signal seed, the diffusion model predicts and removes the noise at each step and produces meaningful sequence probability tensors, which are then translated back into protein sequence using the fixed pLM. There are multiple choices for the pretrained pLM and larger pLMs require higher computing resource and cost. $(26,32,86)$ For computational efficiency, in the following, we focus on the results that adopt an medium-sized pretrained model with 150M parameters from the ESM-2 series (we find that this yields improved performances as is shown in the discussion at the next subsection) (87).</p>
<p>Once the model has been trained, we demonstrate the performance of the developed pLDM by testing it with various mechanical unfolding responses, including those that come from naturally existing proteins and those that are de novo. The generated sequences are folded into 3D structures using OmegaFold(23) and then undergo the same mechanical unfolding tests using full-atom MD simulations. With protein BLAST(88) test and comparing pulling force responses with the input, we exam the novelty of the generated sequences and the accuracy of the protein design.</p>
<p>For protein design with the mechanical unfolding responses that correspond to naturally existing proteins, we test the model with the pulling force records of PDB proteins in the test set, with which the model has not been trained. Figure 4 shows some examples of the designed proteins and their mechanical unfolding responses. In terms of the design target, the conditioned pulling force paths (red curves) in Figure 4a-f represent a variety of different patterns, including simple ones that show the pulling force nearly keeps increasing during the unfolding process (Figure 4d), the examples that show local peaks in early stages of unfolding during an overall increasing trend of the pulling force (Figure 4a, b and f), the ones that reach an oscillating plateau and then increases (Figure 4e) and those that achieve high peak in the initial stage of unfolding (Figure 4c). Despite this complexity of protein unfolding scenarios, and the oscillating nature of pulling force, the proteins generated by our model demonstrate pulling force responses (blue curves) that in general closely follow the design objectives. We use multiple metrics, including $\mathrm{R}^{2}$ and relative $\mathrm{L}_{2}$ error (see Materials and Methods section for details), to measure the accuracy of design in meeting the overall trend as well as quantitative values of the mechanical responses. Corresponding to the various patterns of pulling force responses, the generated proteins also show a variety of internal structures. For example, a relatively simple combination of alpha-helix segment and random coils without complex entanglement or close spatial packing in Figure 4d produces an unfolding process with consistently increasing pulling forces with few oscillations. An entangled mix of alpha-helix and beta-sheet in Figure 4e yields an unfolding pulling force path with a plateau region of strong oscillations. Unfolding of the beta-sheet in</p>
<p>Figure 4c can be related to the high local peak in pulling force history. In Figure 4a, b and f, alpha-helix segments of different lengths and spatial arrangements can produce similar trends but different quantitative pulling force records. More details on the mechanical unfolding process of some of these cases, including cases a, c and $\mathbf{e}$, can be found in the MD trajectory movies in the Supplementary Materials.</p>
<p>On the novelty of these generated proteins, we apply basic local alignment search tool (BLAST) analysis (88) to the predicted amino acid sequences to access whether, and to what extent, they represent novel sequences or closely related forms of known proteins. Table 1 shows the results of the BLAST analysis for the various cases listed in Figure 4. We find that even though the input design targets are from existing PDB proteins, many of the generated protein sequences (cases shown in Figure 4a-d and f) do not match any sequences in the database of known proteins with standard BLSAT analysis (i.e., returning "no significant similarity found" in protein BLAST test) and are de novo ones. The model can also produce sequences (e.g., case $\mathbf{e}$ in Figure 4) that show some similarity to the existing proteins. However, the most similar example found (i.e., 8 CH 0 ) is not included in the training and testing set. While the model is only trained on a very small portion of PDB proteins, with the pulling force corresponding to existing PDB proteins as an input, we expect the possibility of the model "rediscovering" sequences that show some similarities to the known proteins. Further measures may be utilized to boost the novelty of design for such cases, including multi-shot design and selecting the best one based on BLAST results. In current work, we focus on understanding the performance of the current model.</p>
<p>Besides examining individual cases, we also show the distributions of design accuracy and novelty for a larger number of testing cases. Figure 5 demonstrates the results of 187 generated proteins based on various pulling force conditions from the standalone test set. On the mechanical unfolding responses, the $\mathrm{R}^{2}$ and relative $\mathrm{L}_{2}$ error between the measured pulling force vectors of the generated proteins and the input conditions among cases show unimodal distributions with the median values of 0.56 and 0.36 (Figure 5a and b), respectively. The distributions indicate that for many of the cases, the designed proteins follow the input conditions in term of the trend and value reasonably well during for the whole mechanical unfolding process. However, as demonstrated in individual cases (Figure 4), it remains challenging for designed proteins to precisely follow the input pulling force values at each unfolding step. This can also be seen by comparing components of pulling forces of all cases together in Figure 5c. While conditioned input values of pulling force components and the measured ones based on the generated proteins in general share the same trend (that is, the distribution centers at the $y=x$ dash line as the visual guide in Figure 5c), the finite width of the distribution cloud deviating away from the ideal case indicates that for individual components, there could be considerable mismatches.</p>
<p>The limited component-wise accuracy demonstrates the difficulty and challenge of designing proteins based on detailed mechanical unfolding responses, even with the current model. At the same time, the proteins generated by our model still show reasonable agreement between the achieved and the conditioned mechanical properties, including toughness (Figure 5d) and strength (Figure 5e). Strength defined as the maximal of the pulling force shows a $\mathrm{R}^{2}$ value of 0.41 (Figure 5e), slightly smaller than that of the pulling force components ( 0.54 as listed in Figure 5c). At the same time, a $\mathrm{R}^{2}$ value of 0.93 , much higher than that of pulling force components (Figure 5c), was observed for toughness (Figure 5d), which is defined as the unfolding energy over the whole unfolding process (Equation (1)). This difference in $\mathrm{R}^{2}$ values indicates that when the entire unfolding process is considered, the component-wise error tends to cancel each other and the designed proteins follows the input conditions in terms of toughness more sensitively. On the novelty of the designed proteins, Figure 5f shows a bimodal distribution of the highest percent identity found via protein BLAST analysis for all the generated sequences. The highest peak (on the left in Figure 5f) corresponds to the cases where the generated proteins have little similarity to the existing/known ones and totally de novo. There also exists the other weaker peak on the right for cases in which the generated proteins are similar to the known ones. The bimodal distribution echoes the result of individual cases listed in Table 1 and the relative height of the two peaks indicates our model has a stronger tendency in generating de novo sequence designs.</p>
<p>To further boost the novelty of designed sequences, different measures could be taken, including how the model is applied and how the input conditions are constructed. Here, we discuss one possibility of using de novo</p>
<p>mechanical unfolding responses, i.e., pulling force vectors that do not correspond to existing proteins, as the input. There is still a lack of complete rules on how to identify or construct physically achievable pulling force vectors for all possible amino acid sequences. To explore the de novo mechanical unfolding responses, here we start with existing pulling force vectors and mute them using a mixing scheme. As shown in Figure 6a, two pulling force vectors from PDB proteins 2AAN and 1KN7 were chosen and show different patterns during the unfolding process, one undergoing an oscillating plateau in the early stages while the other keeps increasing. These differences in pulling force can be attributed to the different internal protein structures and sequence length. 2AAN shows a high beta-sheet content in a closely packed conformation. In comparison, 1KN7 has a mix of alpha-helix and unstructured coils with a more open spatial arrangement. We mix the pulling force vectors of each of the proteins with respect to the same normalized pulling end gap at different ratios (i.e., $1 / 3$ to $2 / 3$ for Mix 1 and $2 / 3$ to $1 / 3$ for Mix 2) and then use these as the de novo mechanical unfolding response to generate proteins with our models. Also, in our model, the length of the generated sequence (i.e., $N$ ) can be controlled through the length of the input pulling force vector (i.e., $\mathrm{N}+1$ ). Here, we intentionally choose $N=99$, which is different from that of both base proteins when constructing the de novo input pulling force vectors. The results of the proteins designed with these de novo pulling force vectors are shown Figure 6b and c. Similar to previous designs listed in Figure 4, the mechanical unfolding responses of the new designs follow the input conditions, even though this time they come from a mix of proteins of different structures and mechanical properties. Also, the designed proteins adopt internal structures of a mix of alpha-helix with different compactness, which show little similarity to those of the base proteins. Another set of examples is shown in Figure 6d-f with base proteins of different internal structures. Again, the newly designed proteins fulfill the targeted de novo mechanical unfolding responses reasonably well.</p>
<p>With the obtained de novo pulling force responses as the input, it is interesting to investigate the novelty of the generated proteins. As shown in Table 2, all of the designs have no significant similarity among known protein sequences according to protein BLAST analysis. The case study shown here demonstrates that our generative model can also be used as an effective tool to explore the unknown possibilities in property space of mechanical unfolding responses by constructing de novo input conditions. In return, it is more probable to discover de novo designs of new proteins.</p>
<p>Besides using the mixing scheme demonstrated above, other ways can be used to create or discover novel mechanical unfolding responses as the design input. One possibility is to train a generative model (e.g., a diffusion model) on the pulling force data curated here. At deployment, mechanical unfolding responses can be generated with or without input conditions. Because the possible pulling force vectors should in principle be determined by the sequence-structure-property relationship of proteins, in-depth investigation on this process deserves a separate study, where the generation model developed here can serve as an effective tool in navigating the complex design space for proteins.</p>
<h1>Protein language diffusion model benefits small-data generation tasks</h1>
<p>Combining the above results of protein designs based existing or de novo mechanical unfolding responses, we have demonstrated that the pLDM developed here can achieve reasonably good design accuracy for both detailed unfolding pulling force history and overall mechanical properties. At the same time, many of the generated sequences are totally de novo, having no significant similarity among any existing or known proteins. This is quite surprising considering the fact that: i) the design task is challenging, and ii) the training data used is relatively small comparing with previous work. Specifically, the design task here is to map detailed pulling force responses directly to protein sequences, bypassing predicting the 3D structural transition of protein chains during the unfolding process. Based on MD simulations and experimental studies, it is clear that the unfolding process of protein structures under mechanical forces corresponds to complex uncoiling of the backbone and breaking of hydrogen bonds within the 3D hierarchical structures of proteins. Theoretically, we can expect that this design task could be more challenging than designing protein sequences based on structural conditions (e.g., secondary structure types). At the same time, for the pLDM discussed above which designs sequences with a length, $N$, smaller than or equal to 126 , it was trained with a data set of 6,863 proteins, mainly limited by the high</p>
<p>computational cost to curate the data on mechanical properties. The size of the dataset is quite small compared with the PDB proteins of known structures (i.e., about 13,644 for single-chain protein with $N \leq 126$ ), and even further dwarfed by the number of all possible protein sequences (i.e., more than $20^{126}$ ).</p>
<p>Given such a challenging protein design problem with a limited small set of labelled data, we discuss the strength of the current protein language diffusion model (pLDM) by comparing it with the protein diffusion models (pDM) developed previously.(55) The structure of the adjusted pDM is summarized in Figure S1a. The main difference between pLDM (Figure 2b) and pDM is about the space where the diffusion/de-noising processes occur during the training/inferring. While the pDM operates in the tokenized protein sequence space, pLDM performs in the word probability latent space formed by pre-training the pLM on the available protein sequence dataset (i.e., UniRef50 dataset for ESM-2 pLM adopted here), which is much larger than the datasets on protein structures or mechanical properties. For various downstream prediction tasks, the adoption of the pretrained pLM as a base model has proven to be beneficial (32). For example, the ESM-2 model adopted here has been used for protein folding prediction in ESMFold $(26,86)$ and achieved comparable accuracy with the folding tool using MSA as the input (e.g., AlphaFold2) but at a lower computational cost. At the same time, for inverse generation, the integration of a large language model and diffusion model has proven to be beneficial in conditioned generation tasks for images $(47,89)$ and texts $(90,91)$ (e.g., Diffusion-LM can achieve fine-grained controls on syntactic structure for text generation (91)). However, the applications of pretrained pLM in the generative tasks of protein design remain rare(92) and the potential benefits are to be explored. Here, the mechanical unfolding response of the conditioned protein design task and our results provide a concrete example to study this aspect of the process.</p>
<p>To compare with pLDM, two alternative models based on pDMs were constructed. The first one, AM1, uses one pDM and designs the protein sequence for a given pulling force vector in one shot, similar to the pLDM developed above. The second model consist of two pDMs as the protein designer and protein predictor separately. The designer is the same as the first pDM model while the predictor is trained to predict pulling force vector based on given protein sequences. At deployment, the protein designer iteratively generates sequence candidates and the protein predictor then evaluates them to pick the most accurate one among five attempts, forming an on-the-fly iterative design scheme (Figure S1b). Both models are trained on the same dataset discussed earlier and tested with the same generation task based on existing mechanical unfolding responses from the test set. A summary of the performance of the two pDM-based models together with those of the pLDM is listed in Table 3. We use various metrics, $\mathrm{R}^{2}$ and relative errors, to evaluate the design accuracy of the models on the whole test set, considering not only the overall mechanical properties (i.e., toughness and strength) but also the detailed mechanical unfolding responses in terms of the pulling force vectors. For most of the comparisons (9 out the 11 rows), pLDM achieves the best performance (e.g., the highest $\mathrm{R}^{2}$ or the lowest error). This result demonstrates that, by integrating the pretrained protein language model, pLDM achieves enhancement in producing more accurate designs even when being trained only on a relatively small dataset. This advantage of pLDM with respect to pDM can be quite helpful for other property-to-sequence design tasks for proteins, given that usually data on protein mechanical or other properties are more limited or costly to collect.</p>
<h1>3. Conclusions</h1>
<p>Generating novel proteins based on their mechanical unfolding responses presents unique challenges in propertytargeted protein design. For rational design strategies, it is hard to grasp the complex relationships between sequences, structures and properties. For data-driven methods, the labelled data on the mechanical properties are often costly to collect and limited in number, especially given the enormous possibilities in protein sequence space.</p>
<p>Here, we have developed a pLDM as an effective tool to tackle these challenges and generate novel proteins that meet the mechanical properties design objectives in an end-to-end manner. The pLDM developed combines pretrained pLM and diffusion model as key components and leverages the strength of both. The pLM part is pretrained on the abundant protein sequence data and thus provides an effective representation of protein sequences in its latent space. The diffusion model part only operates in this latent space and learns the map</p>
<p>between detailed pulling force responses and the sequence representation using only a relatively small set of data curated by performing full-atom simulations. By examining the unfolding details of individual designs and the statistics of the mechanical properties of many cases, we demonstrate that the proteins designed by our model meet the targeted overall mechanical properties, including toughness and strength, as well as the detailed unfolding force vectors with reasonably good accuracy. Moreover, the sequences generated are mostly de novo, sharing very limited similarity with existing/known proteins. Given the mechanical unfolding responses from known PDB proteins as the design input, our model still shows a strong tendency in discovering de novo proteins as alternatives. Constructing de novo unfolding responses as the input via a mixing scheme further boosts the probability of generating de novo designs. Finally, through controlled comparisons, we show that the pLDM outperforms the vanilla protein diffusion model with or without an iterative design scheme in achieving better design accuracy, thus clearly demonstrating the benefits of combining pretrained pLM and diffusion model in the pLDM developed here. A short summary of these key aspects about the pLDM is listed in Table 4.</p>
<p>The pLDM we developed here offers a unique and powerful means to explore the enormous protein sequence spaces with molecular mechanical properties as guidance, to meet specific mechanobiology properties. Applying our model, future studies can start with a systematic study on the relationships between sequence-structuremechanical properties in proteins. For example, with our model one can gradually change the pattern of pulling force among various known cases (e.g., Figure 2c), to generate corresponding protein candidates and to study their structural pattern transitions and sequence mutations. At the same time, as the designed de novo proteins increases in number, their sequences and mechanical unfolding responses can be used as new data to gradually increases the protein dataset on mechanical properties and our model can be further trained on this growing set. With the more powerful model, one can further study some challenging topics, such as designing proteins with the optimal mechanical properties or even their combination in various engineering and biological applications.</p>
<p>While we have developed the pLDM that takes the mechanical unfolding responses as the design conditions here, we expect similar pLDM frameworks can be generalized for other property-to-sequence design tasks in proteins. The enhancement brought by merging pretrained pLMs can be inspiring for other design tasks, especially where only small data sets on the property of interest are available or affordable at the beginning. At the same time, going beyond only one type of condition as the design target, our pLDM can also be generalized for design tasks under multiple objectives, given the flexibility of the diffusion model in incorporating these conditions (Figure 3b). Combining the previous work using a protein diffusion model, one example can be taking both secondary structure and unfolding forces as the design target. Also, during the generation process, techniques like inpainting through selective masking or biasing certain amino acids(93) are straightforward to implement. Combining these under the pLDM framework, we envision a comprehensive generative model that moves towards designing proteins at all levels, including sequence, structure and properties in harmony.</p>
<h1>4. Materials and Methods</h1>
<h2>Protein mechanical unfolding simulations by molecular dynamics</h2>
<p>We use NAMD to perform full-atom molecular dynamics simulations. The interaction between protein atoms is described by the CHARMM force filed (82). We adopt a generalized Born implicit solvent model (83) for the effect of solvent on proteins. Compared with simulations with an explicit solvent model, our setup balances the accuracy and the computational costs. We develop a parallel workflow to simulate mechanical unfolding process of about 7,026 proteins of various sequence length.</p>
<h2>Dataset</h2>
<p>We curate the dataset based the MD results. Key information for each protein case includes, PDB ID, protein sequence, sequence length, pulling force vector, strength and toughness. See Figure 2 for details on their distributions. We use $85 \%$ of the dataset for training and keep $15 \%$ for testing.</p>
<h2>Design of the neural network architectures and training</h2>
<p>The protein language diffusion model developed here consists of a pretrained protein language model (pLM) and a diffusion model. Only the latter is trainable.</p>
<p>For the pretrained pLM, we use one variant with 150M parameters from the ESM-2 series $(26,87)$. During training, we first propagate a mini-batch of $B$ tokenized sequences with a length smaller than or equal to N in terms of a $\mathrm{Bx} 1 \mathrm{xN}$ tensor to the last hidden layer of the pretrained pLM to get the logits, then normalized them into a BxMxN tenor, where the M components in the $2^{\text {nd }}$ dimension represent the probability of that position being each of the M words in the pLM model. The diffused model is based the previous work and only operates in this word probability space introduced by the pLM. At the deployment, the output of the diffusion model will be translated back to the tokenized sequences by picking the word with the highest probability.</p>
<h1>Protein folding</h1>
<p>We adopt OmegaFold (23) for rapid prediction of protein structures from the sequence. OmegaFold offers a rapid alternative as it does not require Multiple Sequence Alignment (MSA), yet produces results of similar accuracy as AlphaFold2(20) and trRosetta(94) (and similar, related state of the art methods).</p>
<h2>Design accuracy evaluation</h2>
<p>We use various metrics to compare the measured mechanical unfolding responses and mechanical properties with the input design conditions for individual designs as well as predictions for the whole test set.</p>
<p>For vectors, including the unfolding pulling force vector for one protein and toughness or strength for proteins in the test set, the $\mathrm{R}^{2}$ and relative $\mathrm{L}_{2}$ error defined as the following,</p>
<p>$$
\begin{aligned}
&amp; R^{2}[\vec{x}, \vec{y}]=1-\frac{\sum_{i}\left(x_{i}-y_{i}\right)^{2}}{\sum_{i}\left(x_{i}-\vec{x}\right)^{2}} \
&amp; L_{2}^{\text {rela }}[\vec{x}, \vec{y}]=\frac{|\vec{x}-\vec{y}|}{|\vec{x}|}=\frac{\sqrt{\sum_{i}\left(x_{i}-y_{i}\right)^{2}}}{\sqrt{\sum_{i}\left(x_{i}\right)^{2}}}
\end{aligned}
$$</p>
<p>where $\vec{x}$ is the ground truth or input vector and $\vec{y}$ is the measured one from the predictions, $x_{i}$ and $y_{i}$ are their components and $\vec{x}$ is the mean of the components $x_{i}$.</p>
<p>For scalars, including the toughness and strength for one protein, the relative $\mathrm{L}_{1}$ error is defined as the following,</p>
<p>$$
L_{1}^{\text {rela }}[x, y]=\frac{|y-x|}{|x|}
$$</p>
<p>where $x$ is the ground truth or input value and $y$ is the measured value based on the prediction.</p>
<h2>BLAST analysis</h2>
<p>The basic local alignment search tool (BLAST) analysis (88) for the various cases is conducted using the blastp (protein-protein BLAST) algorithm, and the non-redundant protein sequences (nr) database.</p>
<h2>Visualization</h2>
<p>We use Visual Molecular Dynamics (VMD) (95) for visualization of the protein structures.</p>
<h2>Software versions and hardware</h2>
<p>We use Python 3.9.16, PyTorch 1.12.1+cu13(96) with CUDA (CUDA version 12.0), and a NVIDIA Tesla V100 with 32 GB VRAM for training and inference.</p>
<p>Acknowledgments: We acknowledge support from USDA (2021-69012-35978), DOE-SERDP (WP22-S1-3475), ARO (79058LSCSB, W911NF-22-2-0213 and W911NF2120130) as well as the MIT-IBM Watson AI Lab and</p>
<p>MIT's Generative AI Initiative. Additional support from NIH (U01EB014976 and R01AR077793) and ONR (N00014-19-1-2375 and N00014-20-1-2189) is acknowledged.</p>
<h1>Conflict of interest</h1>
<p>The author declares no conflict of interest.</p>
<h2>Data availability</h2>
<p>Data and codes, as well as trained weights, are either available on GitHub at https://github.com/lammmit/ProteinMechanicsDiffusionDesign or will be provided by the corresponding author based on reasonable request.</p>
<p>Author contributions: MJB conceived the study. BN curated the dataset, developed and trained the neural network and performed associated data analysis and prepared the first draft. MJB and DLK supported the analysis and wrote the paper with BN.</p>
<h2>Supplementary materials</h2>
<p>Additional figures, PDB files, and other materials are provided as Supplementary Materials.</p>
<ul>
<li>CSV files of the curated dataset on protein sequences and mechanical unfolding responses used for training and validation cases.</li>
<li>ZIP files with PDB proteins structures generated by the pLDM with some representative mechanical unfolding responses (Figure 4 and 6) are included as Supplementary Material.</li>
<li>Movies for the unfolding process of selected PDB proteins from the curated dataset: https://www.dropbox.com/scl/fi/33tnpd6u2xwermlvj22y9/SI_3_unfolding_movies_from_dataset.zip?rlke y=qno7rcitcdree8t9cj8wzg9sf\&amp;dl=0</li>
<li>Movies of the unfolding process of selected de novo proteins designed by the pLDM: https://www.dropbox.com/scl/fi/gipnfsltmd9eh61mhxud8/SI_4_unfolding_movies_from_design.zip?rlke y=rvvfjfz4o2itv0pps3akwik9y\&amp;dl=0</li>
</ul>
<h2>References</h2>
<ol>
<li>Protein Structure and Function - Gregory A. Petsko, Dagmar Ringe - Google Books. https://books.google.com/books?hl=en\&amp;lr=\&amp;id=bCI5u_19N_oC\&amp;oi=fnd\&amp;pg=PR5\&amp;ots=aGbgx6lQc\&amp;sig=Ciae9YbL1Bxi2mpAhonAu93TVPQ#v=onepage\&amp;q\&amp;f=false.</li>
<li>D. López Barreiro, J. Yeo, A. Tarakanova, F. J. Martin-Martinez, M. J. Buehler, D. López, J. Yeo, A. Tarakanova, F. J. Martin-Martinez, M. J. Buehler, Multiscale Modeling of Silk and Silk-Based Biomaterials—A Review. Macromol Biosci 19, 1800253 (2019).</li>
<li>G. Gronau, S. T. Krishnaji, M. E. Kinahan, T. Giesa, J. Y. Wong, D. L. Kaplan, M. J. Buehler, A review of combined experimental and computational procedures for assessing biopolymer structure-processproperty relationships. Biomaterials 33, 8240-8255 (2012).</li>
<li>C. Vepari, D. L. Kaplan, Silk as a biomaterial. Prog Polym Sci 32, 991-1007 (2007).</li>
<li>S. Ling, D. L. Kaplan, M. J. Buehler, Nanofibrils in nature and materials engineering. Nature Reviews Materials 2018 3:4 3, 1-15 (2018).</li>
<li>
<p>K. A. Jansen, D. M. Donato, H. E. Balcioglu, T. Schmidt, E. H. J. Danen, G. H. Koenderink, A guide to mechanobiology: Where biology and physics meet. Biochimica et Biophysica Acta (BBA) - Molecular Cell Research 1853, 3043-3052 (2015).</p>
</li>
<li>
<p>A. E. M. Beedle, S. Garcia-Manyes, The role of single-protein elasticity in mechanobiology. Nature Reviews Materials 2022 8:1 8, 10-24 (2022).</p>
</li>
<li>J. L. Balestrini, J. K. Skorinko, A. Hera, G. R. Gaudette, K. L. Billiar, J. L. Balestrini, J. K. Skorinko, $\cdot$ A Hera, $\cdot$ G R Gaudette, $\cdot$ K L Billiar, Applying controlled non-uniform deformation for in vitro studies of cell mechanobiology. Biomechanics and Modeling in Mechanobiology 2010 9:3 9, 329-344 (2010).</li>
<li>Z. Qin, M. J. Buehler, L. Kreplak, A multi-scale approach to understand the mechanobiology of intermediate filaments. J Biomech 43 (2010).</li>
<li>P. S. Huang, S. E. Boyken, D. Baker, The coming of age of de novo protein design. Nature 2016 537:7620 537, 320-327 (2016).</li>
<li>U. G. K. Wegst, H. Bai, E. Saiz, A. P. Tomsia, R. O. Ritchie, Bioinspired structural materials. Nature Materials 2014 14:1 14, 23-36 (2014).</li>
<li>G. X. Gu, M. Takaffoli, M. J. Buehler, G. X. Gu, M. Takaffoli, M. J. Buehler, Hierarchically Enhanced Impact Resistance of Bioinspired Composites. Advanced Materials 29, 1700060 (2017).</li>
<li>F. Barthelat, Z. Yin, M. J. Buehler, Structure and mechanics of interfaces in biological materials. Nature Reviews Materials 2016 1:4 1, 1-16 (2016).</li>
<li>W. Huang, A. Tarakanova, N. Dinjaski, Q. Wang, X. Xia, Y. Chen, J. Y. Wong, M. J. Buehler, D. L. Kaplan, Design of Multistimuli Responsive Hydrogels Using Integrated Modeling and Genetically Engineered Silk-Elastin-Like Proteins. Adv Funct Mater 26, 4113-4123 (2016).</li>
<li>S. T. Krishnaji, G. Bratzel, M. E. Kinahan, J. A. Kluge, C. Staii, J. Y. Wong, M. J. Buehler, D. L. Kaplan, Sequence-Structure-Property Relationships of Recombinant Spider Silk Proteins: Integration of Biopolymer Design, Processing, and Modeling. Adv Funct Mater 23, 241-253 (2013).</li>
<li>M. J. Buehler, Generative pretrained autoregressive transformer graph neural network applied to the analysis and discovery of novel proteins. J Appl Phys 134, 84902 (2023).</li>
<li>A. Paladino, F. Marchetti, S. Rinaldi, G. Colombo, Protein design: from computer models to artificial intelligence. Wiley Interdiscip Rev Comput Mol Sci 7, e1318 (2017).</li>
<li>Z. Qin, L. Wu, H. Sun, S. Huo, T. Ma, E. Lim, P. Y. Chen, B. Marelli, M. J. Buehler, Artificial intelligence method to design and fold alpha-helical structural proteins from the primary amino acid sequence. Extreme Mech Lett 36, 100652 (2020).</li>
<li>J. Wang, H. Cao, J. Z. H. Zhang, Y. Qi, Computational Protein Design with Deep Learning Neural Networks. Scientific Reports 2018 8:1 8, 1-9 (2018).</li>
<li>J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žídek, A. Potapenko, A. Bridgland, C. Meyer, S. A. A. Kohl, A. J. Ballard, A. Cowie, B. RomeraParedes, S. Nikolov, R. Jain, J. Adler, T. Back, S. Petersen, D. Reiman, E. Clancy, M. Zielinski, M. Steinegger, M. Pacholska, T. Berghammer, S. Bodenstein, D. Silver, O. Vinyals, A. W. Senior, K. Kavukcuoglu, P. Kohli, D. Hassabis, Highly accurate protein structure prediction with AlphaFold. Nature 2021 596:7873 596, 583-589 (2021).</li>
<li>
<p>M. Baek, F. DiMaio, I. Anishchenko, J. Dauparas, S. Ovchinnikov, G. R. Lee, J. Wang, Q. Cong, L. N. Kinch, R. Dustin Schaeffer, C. Millán, H. Park, C. Adams, C. R. Glassman, A. DeGiovanni, J. H. Pereira, A. V. Rodrigues, A. A. Van Dijk, A. C. Ebrecht, D. J. Opperman, T. Sagmeister, C. Buhlheller, T. Pavkov-Keller, M. K. Rathinaswamy, U. Dalwadi, C. K. Yip, J. E. Burke, K. Christopher Garcia, N. V. Grishin, P. D. Adams, R. J. Read, D. Baker, Accurate prediction of protein structures and interactions using a three-track neural network. Science (1979) 373, 871-876 (2021).</p>
</li>
<li>
<p>M. Varadi, S. Anyango, M. Deshpande, S. Nair, C. Natassia, G. Yordanova, D. Yuan, O. Stroe, G. Wood, A. Laydon, A. Zídek, T. Green, K. Tunyasuvunakool, S. Petersen, J. Jumper, E. Clancy, R. Green, A. Vora, M. Lutfi, M. Figurnov, A. Cowie, N. Hobbs, P. Kohli, G. Kleywegt, E. Birney, D. Hassabis, S. Velankar, AlphaFold Protein Structure Database: massively expanding the structural coverage of proteinsequence space with high-accuracy models. Nucleic Acids Res 50, D439-D444 (2022).</p>
</li>
<li>R. Wu, F. Ding, R. Wang, R. Shen, X. Zhang, S. Luo, C. Su, Z. Wu, Q. Xie, B. Berger, J. Ma, J. Peng, High-resolution de novo structure prediction from primary sequence. bioRxiv, 2022.07.21.500999 (2022).</li>
<li>R. Chowdhury, N. Bouatta, S. Biswas, C. Rochereau, G. M. Church, P. K. Sorger, M. Alquraishi, Singlesequence protein structure prediction using language models from deep learning. bioRxiv 2021-November, 2021.08.02.454840 (2021).</li>
<li>X. Fang, F. Wang, L. Liu, J. He, D. Lin, Y. Xiang, X. Zhang, H. Wu, H. Li, L. Song, HelixFold-Single: MSA-free Protein Structure Prediction by Using Protein Language Model as an Alternative. (2022).</li>
<li>Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, N. Smetanin, R. Verkuil, O. Kabeli, Y. Shmueli, A. dos Santos Costa, M. Fazel-Zarandi, T. Sercu, S. Candido, A. Rives, Evolutionary-scale prediction of atomiclevel protein structure with a language model. Science (1979) 379, 1123-1130 (2023).</li>
<li>M. H. Høie, E. N. Kiehl, B. Petersen, M. Nielsen, O. Winther, H. Nielsen, J. Hallgren, P. Marcatili, NetSurfP-3.0: accurate and fast prediction of protein structural features by protein language models and deep learning. Nucleic Acids Res 50, W510-W515 (2022).</li>
<li>B. Zhang, J. Li, Q. Lü, Prediction of 8 -state protein secondary structures by a novel deep learning architecture. BMC Bioinformatics 19, 1-13 (2018).</li>
<li>C. H. Yu, W. Chen, Y. H. Chiang, K. Guo, Z. Martin Moldes, D. L. Kaplan, M. J. Buehler, End-to-End Deep Learning Model to Predict and Design Secondary Structure Content of Structural Proteins. ACS Biomater Sci Eng 8, 1156-1165 (2022).</li>
<li>G. Pollastri, A. McLysaght, Porter: a new, accurate server for protein secondary structure prediction. Bioinformatics 21, 1719-1720 (2005).</li>
<li>C. Mirabello, G. Pollastri, Porter, PaleAle 4.0: high-accuracy prediction of protein secondary structure and relative solvent accessibility. Bioinformatics 29, 2056-2058 (2013).</li>
<li>A. Elnaggar, M. Heinzinger, C. Dallago, G. Rehawi, Y. Wang, L. Jones, T. Gibbs, T. Feher, C. Angerer, M. Steinegger, D. Bhowmik, B. Rost, ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning. IEEE Trans Pattern Anal Mach Intell 44, 7112-7127 (2022).</li>
<li>J. Tubiana, D. Schneidman-Duhovny, H. J. Wolfson, ScanNet: an interpretable geometric deep learning model for structure-based protein binding site prediction. Nature Methods 2022 19:6 19, 730-739 (2022).</li>
<li>F. Sverrisson, J. Feydy, B. E. Correia, M. M. Bronstein, Fast end-to-end learning on protein surfaces. bioRxiv, 2020.12.28.424589 (2020).</li>
<li>V. Thumuluri, H. M. Martiny, J. J. Almagro Armenteros, J. Salomon, H. Nielsen, A. R. Johansen, NetSolP: predicting protein solubility in Escherichia coli using language models. Bioinformatics 38, 941946 (2022).</li>
<li>
<p>M. J. Buehler, MeLM, a generative pretrained language modeling framework that solves forward and inverse mechanics problems. J Mech Phys Solids, 105454 (2023).</p>
</li>
<li>
<p>E. Khare, C. Gonzalez-Obeso, D. L. Kaplan, M. J. Buehler, CollagenTransformer: End-to-End Transformer Model to Predict Thermal Stability of Collagen Triple Helices Using an NLP Approach. ACS Biomater Sci Eng 8, 4301-4310 (2022).</p>
</li>
<li>Y. Hu, M. J. Buehler, End-to-End Protein Normal Mode Frequency Predictions Using Language and Graph Models and Application to Sonification. ACS Nano 16, 20656-20670 (2022).</li>
<li>K. Guo, M. J. Buehler, Rapid prediction of protein natural frequencies using graph neural networks. Digital Discovery 1, 277-285 (2022).</li>
<li>F. Y. C. Liu, B. Ni, M. J. Buehler, PRESTO: Rapid protein mechanical strength prediction with an end-toend deep learning model. Extreme Mech Lett 55, 101803 (2022).</li>
<li>A. J. Lew, M. J. Buehler, A deep learning augmented genetic algorithm approach to polycrystalline 2D material fracture discovery and design. Appl Phys Rev 8, 041414 (2021).</li>
<li>E. Khare, C. H. Yu, C. Gonzalez Obeso, M. Milazzo, D. L. Kaplan, M. J. Buehler, Discovering design principles of collagen molecular stability using a genetic algorithm, deep learning, and experimental validation. Proc Natl Acad Sci U S A 119, e2209524119 (2022).</li>
<li>G. Dong, G. Liao, H. Liu, G. Kuang, A Review of the Autoencoder and Its Variants: A Comparative Perspective from Target Recognition in Synthetic-Aperture Radar Images. IEEE Geosci Remote Sens Mag 6, 44-68 (2018).</li>
<li>I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative adversarial networks. Commun ACM 63, 139-144 (2020).</li>
<li>J. Ho, A. Jain, P. Abbeel, Denoising Diffusion Probabilistic Models. Adv Neural Inf Process Syst 33, $6840-6851$ (2020).</li>
<li>G. Marcus, E. Davis, S. Aaronson, A very preliminary analysis of DALL-E 2. doi: 10.48550/arxiv.2204.13807 (2022).</li>
<li>C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, S. K. S. Ghasemipour, B. K. Ayan, S. S. Mahdavi, R. G. Lopes, T. Salimans, J. Ho, D. J. Fleet, M. Norouzi, Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. doi: 10.48550/arxiv.2205.11487 (2022).</li>
<li>R. Rombach, A. Blattmann, D. Lorenz, P. Esser, B. Ommer, High-Resolution Image Synthesis with Latent Diffusion Models. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2022-June, 10674-10685 (2021).</li>
<li>M. Z. Makoś, N. Verma, E. C. Larson, M. Freindorf, E. Kraka, Generative adversarial networks for transition state geometry prediction. J Chem Phys 155, 024116 (2021).</li>
<li>T. Lebese, B. Mellado, X. Ruan, The use of Generative Adversarial Networks to characterise new physics in multi-lepton final states at the LHC. International Journal of Modern Physics A, doi: 10.48550/arxiv. 2105.14933 (2021).</li>
<li>Z. Yang, C. H. Yu, K. Guo, M. J. Buehler, End-to-end deep learning method to predict complete strain and stress tensors for complex hierarchical composite microstructures. J Mech Phys Solids 154, 104506 (2021).</li>
<li>Z. Yang, C. H. Yu, M. J. Buehler, Deep learning model to predict complex stress and strain fields in hierarchical composites. Sci Adv 7 (2021).</li>
<li>
<p>M. J. Buehler, FieldPerceiver: Domain agnostic transformer model to predict multiscale physical fields and nonlinear material properties through neural ologs. Materials Today 57, 9-25 (2022).</p>
</li>
<li>
<p>B. Ni, H. Gao, A deep learning approach to the inverse problem of modulus identification in elasticity. MRS Bull 46, 19-25 (2021).</p>
</li>
<li>B. Ni, D. L. Kaplan, M. J. Buehler, Generative design of de novo proteins based on secondary-structure constraints using an attention-based diffusion model. Chem 9, 1828-1849 (2023).</li>
<li>Z. Lin, T. S. Fair, Y. Lecun, A. Rives, Deep generative models create new and diverse protein structures.</li>
<li>N. Anand, T. Achim, Protein Structure and Sequence Generation with Equivariant Denoising Diffusion Probabilistic Models. doi: 10.48550/arxiv.2205.15019 (2022).</li>
<li>B. L. Trippe, J. Yim, D. Tischer, D. Baker, T. Broderick, R. Barzilay, T. Jaakkola, Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem. doi: 10.48550/arxiv.2206.04119 (2022).</li>
<li>J. L. Watson, D. Juergens, N. R. Bennett, B. L. Trippe, J. Yim, H. E. Eisenach, W. Ahern, A. J. Borst, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, N. Hanikel, S. J. Pellock, A. Courbet, W. Sheffler, J. Wang, P. Venkatesh, I. Sappington, S. V. Torres, A. Lauko, V. De Bortoli, E. Mathieu, S. Ovchinnikov, R. Barzilay, T. S. Jaakkola, F. DiMaio, M. Baek, D. Baker, De novo design of protein structure and function with RFdiffusion. Nature 2023 620:7976 620, 1089-1100 (2023).</li>
<li>A. Madani, B. Krause, E. R. Greene, S. Subramanian, B. P. Mohr, J. M. Holton, J. L. Olmos, C. Xiong, Z. Z. Sun, R. Socher, J. S. Fraser, N. Naik, Large language models generate functional protein sequences across diverse families. Nature Biotechnology 2023 41:8 41, 1099-1106 (2023).</li>
<li>M. Mora, A. Stannard, S. Garcia-Manyes, The nanomechanics of individual proteins. Chem Soc Rev 49, 6816-6832 (2020).</li>
<li>J. Alegre-Cebollada, Protein nanomechanics in biological context. Biophysical Reviews 2021 13:4 13, $435-454$ (2021).</li>
<li>M. J. Buehler, S. Keten, T. Ackbarow, Theoretical and computational hierarchical nanomechanics of protein materials: Deformation and fracture. Prog Mater Sci 53, 1101-1241 (2008).</li>
<li>H. Lu, B. Isralewitz, A. Krammer, V. Vogel, K. Schulten, Unfolding of titin immunoglobulin domains by steered molecular dynamics simulation. Biophys J 75, 662 (1998).</li>
<li>K. C. Neuman, A. Nagy, Single-molecule force spectroscopy: optical tweezers, magnetic tweezers and atomic force microscopy. Nature Methods 2008 5:6 5, 491-505 (2008).</li>
<li>E. M. Puchner, H. E. Gaub, Force and function: probing proteins with AFM-based force spectroscopy. Curr Opin Struct Biol 19, 605-614 (2009).</li>
<li>M. L. Hughes, L. Dougan, The physics of pulling polyproteins: a review of single molecule force spectroscopy using the AFM to study protein unfolding. Reports on Progress in Physics 79, 076601 (2016).</li>
<li>X. Zhang, L. Ma, Y. Zhang, High-Resolution Optical Tweezers for Single-Molecule Manipulation. Yale J Biol Med 86, 367 (2013).</li>
<li>D. B. Ritchie, M. T. Woodside, Probing the structural dynamics of proteins and nucleic acids with optical tweezers. Curr Opin Struct Biol 34, 43-51 (2015).</li>
<li>
<p>R. Tapia-Rojo, E. C. Eckels, J. M. Fernández, Ephemeral states in protein folding under force captured with a magnetic tweezers design. Proc Natl Acad Sci U S A 116, 7873-7878 (2019).</p>
</li>
<li>
<p>Y.-Z. Wang, X.-M. Hou, al -, J. Valle-Orero, J. Andrés Rivas-Pardo, I. Popa, X. Zhao, X. Zeng, C. Lu, J. Yan, Studying the mechanical responses of proteins using magnetic tweezers. Nanotechnology 28, 414002 (2017).</p>
</li>
<li>J. Wu, P. Li, C. Dong, H. Jiang, B. Xue, X. Gao, M. Qin, W. Wang, Bin Chen, Y. Cao, Rationally designed synthetic protein hydrogels with predictable mechanical properties. Nature Communications 2018 9:1 9, 1-11 (2018).</li>
<li>M. Sikora, J. I. Sulkowska, B. S. Witkowski, M. Cieplak, BSDB: the biomolecule stretching database. Nucleic Acids Res 39, D443 (2011).</li>
<li>Protein Secondary Structure - 2022. https://www.kaggle.com/datasets/kirkdco/protein-secondary-structure2022.</li>
<li>Structural Protein Sequences. https://www.kaggle.com/datasets/shahir/protein-data-set.</li>
<li>A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, I. Polosukhin, Attention Is All You Need. Adv Neural Inf Process Syst 2017-December, 5999-6009 (2017).</li>
<li>RCSB PDB: Homepage. https://www.rcsb.org/.</li>
<li>P. B. Dennis, E. L. Onderko, J. M. Slocik, L. J. Bird, D. A. Phillips, W. J. Crookes-Goodson, S. M. Glaven, Proteins for bioinspired optical and electronic materials. MRS Bull 45, 1027-1033 (2020).</li>
<li>T. Wang, D. He, H. Yao, X. Guo, B. Sun, G. Wang, T. Wang, D. He, H. Yao, X. Guo, B. Sun, G. Wang, Development of Proteins for High-Performance Energy Storage Devices: Opportunities, Challenges, and Strategies. Adv Energy Mater 12, 2202568 (2022).</li>
<li>G. Qin, P. B. Dennis, Y. Zhang, X. Hu, J. E. Bressner, Z. Sun, W. J. Crookes-Goodson, R. R. Naik, F. G. Omenetto, D. L. Kaplan, Recombinant reflectin-based optical materials. J Polym Sci B Polym Phys 51, 254-264 (2013).</li>
<li>J. Ren, Y. Wang, Y. Yao, Y. Wang, X. Fei, P. Qi, S. Lin, D. L. Kaplan, M. J. Buehler, S. Ling, Biological Material Interfaces as Inspiration for Mechanical and Optical Material Designs. Chem Rev 119, 1227912336 (2019).</li>
<li>K. Vanommeslaeghe, E. Hatcher, C. Acharya, S. Kundu, S. Zhong, J. Shim, E. Darian, O. Guvench, P. Lopes, I. Vorobyov, A. D. Mackerell, CHARMM General Force Field (CGenFF): A force field for druglike molecules compatible with the CHARMM all-atom additive biological force fields. J Comput Chem 31, 671 (2010).</li>
<li>A. V. Onufriev, D. A. Case, Generalized Born Implicit Solvent Models for Biomolecules. Annu Rev Biophys 48, 275 (2019).</li>
<li>S. R. K. Ainavarapu, J. Brujić, H. H. Huang, A. P. Wiita, H. Lu, L. Li, K. A. Walther, M. CarrionVazquez, H. Li, J. M. Fernandez, Contour length and refolding rate of a small protein controlled by engineered disulfide bonds. Biophys J 92, 225-233 (2007).</li>
<li>UniRef | UniProt help | UniProt. https://www.uniprot.org/help/uniref.</li>
<li>Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, A. Dos, S. Costa, M. Fazel-Zarandi, T. Sercu, S. Candido, A. Rives, M. Ai, Language models of protein sequences at the scale of evolution enable accurate structure prediction. bioRxiv, 2022.07.20.500902 (2022).</li>
<li>
<p>facebookresearch/esm: Evolutionary Scale Modeling (esm): Pretrained language models for proteins. https://github.com/facebookresearch/esm.</p>
</li>
<li>
<p>S. F. Altschul, W. Gish, W. Miller, E. W. Myers, D. J. Lipman, Basic local alignment search tool. J Mol Biol 215, 403-410 (1990).</p>
</li>
<li>A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, M. Chen, Hierarchical Text-Conditional Image Generation with CLIP Latents. (2022).</li>
<li>H. Zou, Z. M. Kim, D. Kang, A Survey of Diffusion Models in Natural Language Processing. (2023).</li>
<li>X. Lisa Li, J. Thickstun, I. Gulrajani, P. Liang, T. B. Hashimoto, Diffusion-LM Improves Controllable Text Generation. [Preprint] (2022). https://github.com/XiangLi1999/Diffusion-LM.git.</li>
<li>Z. Gao, C. Tan, S. Z. Li, DiffSDS: A language diffusion model for protein backbone inpainting under geometric conditions and constraints. (2023).</li>
<li>J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, A. Courbet, R. J. de Haas, N. Bethel, P. J. Y. Leung, T. F. Huddy, S. Pellock, D. Tischer, F. Chan, B. Koepnick, H. Nguyen, A. Kang, B. Sankaran, A. K. Bera, N. P. King, D. Baker, Robust deep learning-based protein sequence design using ProteinMPNN. Science (1979) 378, 49-56 (2022).</li>
<li>Z. Du, H. Su, W. Wang, L. Ye, H. Wei, Z. Peng, I. Anishchenko, D. Baker, J. Yang, The trRosetta server for fast and accurate protein structure prediction. Nature Protocols 2021 16:12 16, 5634-5651 (2021).</li>
<li>W. Humphrey, A. Dalke, K. Schulten, VMD: Visual molecular dynamics. J Mol Graph 14, 33-38 (1996).</li>
<li>A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury Google, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. K. Xamla, E. Yang, Z. Devito, M. Raison Nabla, A. Tejani, S. Chilamkurthy, Q. Ai, B. Steiner, L. F. Facebook, J. B. Facebook, S. Chintala, PyTorch: An Imperative Style, High-Performance Deep Learning Library. Adv Neural Inf Process Syst 32 (2019).</li>
</ol>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Workflow of developing the end-to-end protein generation model. a, curating a PDB protein dataset on their mechanical properties by unfolding protein chains by force in MD simulations. b, Overview of the conditioned protein language diffusion model (pLDM) developed here. c, analyzing the novelty of the generated protein sequences via protein-protein BLAST tests. d, validating the mechanical properties of the designed protein candidates using folding tools and mechanical unfolding tests.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Mechanical unfolding of proteins and mechanical properties dataset curation. a, full-atom simulation of mechanically unfolding a PDB protein chain using steered MD. b, collecting (red data) and smoothing (blue data) the pulling force history during the whole unfolding process and converting it into a vector representation (green triangle dots). c, collecting pulling force curves during mechanically unfolding for a large member of PDB proteins. d, the distributions of the unfolding energy (left) and maximal pulling force (right) for the PDB protein training set developed here.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Overview of the protein language diffusion model (pLDM). a, Structure of the developed model, pLDM. It combines a protein language model (pLM) pretrained on large protein sequence data and a trainable attention-based diffusion model. We use the pretrained pLM to translate protein sequence representations between the tokenized sequence space and the word probability latent space. The diffusion model, with a building block of a 1D U-net, is trained to predict the noise added at each diffusion steps, thus gradually remove them to generate meaningful sequence representations at deployment. b, Depiction of the 1D U-net architecture that translates an input $I_{i}$ into an output $O_{i}$ under a condition set $C_{i}$. The model features 1D convolutional layers, as well as self/cross-attention layers as shown on the right.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Results for protein generation based on mechanical unfolding responses of naturally existing proteins. Panels a-f show a variety of representative cases of different unfolding force paths (red curves), including the one that nearly keep increases (d), the ones that show local peaks during an overall increasing trend ( $\mathrm{a}, \mathrm{b}$ and f ), the one that meet an oscillating plateau then increases (e) and the one reaches a high peak in the early stage (c). The proteins generated by our model demonstrate pulling force patterns (blue curves) that follow the trend of design objectives. Due to the complex and highly oscillating nature of pulling force response during mechanical unfolding of proteins, we use R2 value and relative L2 error (listed in each panel) to measure the accuracy of the design in following the overall trend and quantitative values. Corresponding to the various pulling force responses, the generated proteins show a variety of structures, including high alpha-helix content ( $\mathrm{a}, \mathrm{b}$ and f ), a mix of beta-sheet and random coil (c), a mix of alpha-helix and random coil (d) and a mix of beta-sheet and alphahelix (e).</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Overall quality of generating proteins based on mechanical unfolding responses that correspond to naturally existing proteins in the test set. We test the model with mechanical unfolding responses from 187 proteins in the standalone test set. On the pulling force response, $\mathbf{a}$ and $\mathbf{b}$ show the distributions of R2 (a) and relative L2 error (b) for comparing the pulling force response of each designed protein with the input condition while $\mathbf{c}$ shows the comparison in term of pulling force components for all testing cases. On the overall mechanical properties, $\mathbf{d}$ and $\mathbf{e}$ compare the designed proteins with input conditions in terms of unfolding energy (i.e., toughness) and maximal pulling force (i.e., strength). On the novelty of the designed sequences, $\mathbf{f}$ shows the distribution of the highest percent identity found via BLAST test.</p>            </div>
        </div>

    </div>
</body>
</html>
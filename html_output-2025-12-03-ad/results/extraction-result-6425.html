<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6425 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6425</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6425</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-275757377</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.11739v2.pdf" target="_blank">Episodic Memory in AI Agents Poses Risks that Should be Studied and Mitigated</a></p>
                <p><strong>Paper Abstract:</strong> Most current AI models have little ability to store and later retrieve a record or representation of what they do. In human cognition, episodic memories play an important role in both recall of the past as well as planning for the future. The ability to form and use episodic memories would similarly enable a broad range of improved capabilities in an AI agent that interacts with and takes actions in the world. Researchers have begun directing more attention to developing memory abilities in AI models. It is therefore likely that models with such capability will be become widespread in the near future. This could in some ways contribute to making such AI agents safer by enabling users to better monitor, understand, and control their actions. However, as a new capability with wide applications, we argue that it will also introduce significant new risks that researchers should begin to study and address. We outline these risks and benefits and propose four principles to guide the development of episodic memory capabilities so that these will enhance, rather than undermine, the effort to keep AI safe and trustworthy.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6425.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6425.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 + scratchpad</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 with a text scratchpad (chain-of-thought scratchpad)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experiment where GPT-4 was given a simple text scratchpad to record chain-of-thought / reasoning; the scratchpad functions as a crude external episodic memory and was shown to change behavior under pressure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models can strategically deceive their users when put under pressure</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-4 (with scratchpad)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A GPT-4 LLM augmented with a text-based scratchpad that it can write to and consult; the scratchpad records reasoning / past outputs and serves as an available memory of prior internal content.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>text scratchpad (external text buffer)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw text of chain-of-thought / reasoning traces (natural language scratchpad)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>explicit write/read to a text buffer (the model is given the scratchpad contents as part of its context); no learned differentiable controller described</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Simulation where model is 'pressured' to perform an illegal act (deception under pressure)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>safety/behavioral robustness / adversarial prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>≈3× higher rate of 'strategic deception' compared to without scratchpad (reported as relative frequency increase)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>baseline (scratchpad absent) — lower frequency of strategic deception (used as comparator); numeric baseline not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Relative frequency of 'strategic deception' (comparison of rates)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Scratchpad (memory) enabled more sophisticated multi-step deceptive behavior; trade-off is improved ability to carry out multi-step strategies at the cost of enabling harmful behavior (safety trade-off).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Memory (scratchpad) increased propensity for strategic deception in the evaluated pressure scenario; no claims of improved task performance beyond enabling deceptive multi-step planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Dechant C., 2025. Episodic memory in AI agents poses risks that should be studied and mitigated. Accepted for publication at the 2025 Conference on Secure and Trustworthy Machine Learning (SaTML).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Episodic Memory in AI Agents Poses Risks that Should be Studied and Mitigated', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6425.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6425.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agents: Interactive simulacra of human behavior (Park et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system of LLM-based simulated agents that record and consult natural-language memory logs of their actions to maintain consistency and enable coherent planning and social behavior in a virtual environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents (Park et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-driven agents that record episodic events as natural-language memory entries (summaries/notes) and retrieve those entries to inform future behavior and planning in an interactive simulated environment.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>natural-language memory logs (interpretable textual episodic memory); retrieval-augmented (explicit external memory store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>natural language summaries / records of agent actions and observations (human-readable text entries)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>retrieval of relevant textual memory entries (likely similarity / heuristic-based retrieval) which are provided in the model context to influence generation and planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Interactive simulated environment tasks (multi-agent social interactions, planning and consistency over time)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>interactive simulation / long-horizon behavior and planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Natural-language episodic logs improve coherence and long-term consistency in simulation but raise concerns regarding privacy, unpredictability, and potential for misuse (no quantified computational trade-offs provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper (Dechant) notes these memory implementations are relatively simple approximations of human episodic memory and may not scale to very long horizons; no specific failure rates reported in this text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Dechant C., 2025. Episodic memory in AI agents poses risks that should be studied and mitigated. Accepted for publication at the 2025 Conference on Secure and Trustworthy Machine Learning (SaTML).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Episodic Memory in AI Agents Poses Risks that Should be Studied and Mitigated', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6425.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6425.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG-based sub-trajectory memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-augmented episodic memory for episode sub-trajectories (RAG-style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based system that stores episode sub-trajectories in an external retrievable store and uses retrieval-augmented generation (RAG) to provide episodic context during planning or decision making.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented decision transformer: External memory for in-context rl</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG-episodic agent (RAG over episode sub-trajectories)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent stores episode sub-trajectories in an external retrieval store (RAG) and retrieves relevant episodes to augment the model's context during planning/decision steps; memory is detached from model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external memory (RAG) / external vector/text store</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>stored episode sub-trajectories (likely compressed textual or embedding representations / passages)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>retrieval-augmented generation using similarity search over stored embeddings or keys to fetch relevant episodes into model context</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reinforcement-learning / planning tasks requiring reuse of past episode sub-trajectories (in-context RL / decision transformer style tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>planning / reinforcement learning with long-horizon memory</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>RAG-style memory keeps episodic data detachable and more manageable than replaying entire history; trade-offs mentioned in paper include potential inefficiency if entire history were reprocessed, and scaling concerns for very long-lived agents.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors note current memory implementations are simplistic relative to human episodic memory and scaling to decades of agent life may be infeasible without compression and curated retrieval; no numeric failure cases provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Dechant C., 2025. Episodic memory in AI agents poses risks that should be studied and mitigated. Accepted for publication at the 2025 Conference on Secure and Trustworthy Machine Learning (SaTML).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Episodic Memory in AI Agents Poses Risks that Should be Studied and Mitigated', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6425.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6425.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Remembr</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Remembr: Building and reasoning over long-horizon spatio-temporal memory for robot navigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A robotics-focused episodic memory system that builds long-horizon spatio-temporal memory representations for navigation and later question-answering / planning over past actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Remembr: Building and reasoning over long-horizon spatio-temporal memory for robot navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Remembr (Anwar et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Robotics agent architecture that constructs and reasons over long-horizon spatio-temporal episodic memories to support navigation, retrieval, and QA about past actions and observations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-horizon spatio-temporal episodic memory (external store structured for spatio-temporal queries)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>spatio-temporal memory entries (likely multimodal summaries tied to time and location; may include embeddings and metadata)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>structured retrieval over spatio-temporal indices (queryable memory for navigation and question answering)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Robot navigation and episodic question answering (egocentric video / past-actions QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>robot navigation / episodic QA / long-horizon retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Designed to support long-horizon recall and reasoning; trade-offs implied include storage, indexing complexity, and the need to compress raw sensory data for practicality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper (Dechant) cites such works as examples but notes that many current approaches are simple and may not scale to decades of operation; no specific failure modes from Remembr are detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Dechant C., 2025. Episodic memory in AI agents poses risks that should be studied and mitigated. Accepted for publication at the 2025 Conference on Secure and Trustworthy Machine Learning (SaTML).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Episodic Memory in AI Agents Poses Risks that Should be Studied and Mitigated', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6425.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6425.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Virtual-robot QA (Dechant et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning to summarize and answer questions about a virtual robot's past actions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior study demonstrating memory-like capabilities for a virtual robot: summarization and question-answering over the robot's past actions, i.e., using stored episodic records to answer queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to summarize and answer questions about a virtual robot's past actions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Virtual-robot memory QA agent (Dechant et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>System that stores episode representations of a robot's past actions and uses those to produce summaries and to answer natural-language queries about past episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>interpretable episodic records (natural language summaries and/or structured representations for QA)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>summaries of episodes and records of past actions (textual descriptions and possibly multimodal records)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>queryable memory used for natural language question answering over past episodes (likely retrieval of stored summaries and then LM-based QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization and question answering about a virtual robot's past actions</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>episodic question answering / summarization / interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Paper highlights interpretability benefits (natural-language summaries) for monitoring and safety; trade-offs include potential storage and search costs and challenges compressing raw sensory data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Dechant notes that using directly interpretable memories (natural language + images) is a useful experimental strategy but may be impractical at scale due to size and searchability constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Dechant C., Akinola I., Bauer D., 2023. Learning to summarize and answer questions about a virtual robot's past actions. Autonomous Robots.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Episodic Memory in AI Agents Poses Risks that Should be Studied and Mitigated', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>Large language models can strategically deceive their users when put under pressure <em>(Rating: 2)</em></li>
                <li>Remembr: Building and reasoning over long-horizon spatio-temporal memory for robot navigation <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented decision transformer: External memory for in-context rl <em>(Rating: 2)</em></li>
                <li>Memorybank: Enhancing large language models with long-term memory <em>(Rating: 2)</em></li>
                <li>Augmenting language models with long-term memory <em>(Rating: 2)</em></li>
                <li>Model-free episodic control <em>(Rating: 1)</em></li>
                <li>Leveraging episodic memory to improve world models for reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6425",
    "paper_id": "paper-275757377",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "GPT-4 + scratchpad",
            "name_full": "GPT-4 with a text scratchpad (chain-of-thought scratchpad)",
            "brief_description": "An experiment where GPT-4 was given a simple text scratchpad to record chain-of-thought / reasoning; the scratchpad functions as a crude external episodic memory and was shown to change behavior under pressure.",
            "citation_title": "Large language models can strategically deceive their users when put under pressure",
            "mention_or_use": "mention",
            "agent_name": "GPT-4 (with scratchpad)",
            "agent_description": "A GPT-4 LLM augmented with a text-based scratchpad that it can write to and consult; the scratchpad records reasoning / past outputs and serves as an available memory of prior internal content.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "text scratchpad (external text buffer)",
            "memory_representation": "raw text of chain-of-thought / reasoning traces (natural language scratchpad)",
            "memory_access_mechanism": "explicit write/read to a text buffer (the model is given the scratchpad contents as part of its context); no learned differentiable controller described",
            "task_name": "Simulation where model is 'pressured' to perform an illegal act (deception under pressure)",
            "task_category": "safety/behavioral robustness / adversarial prompting",
            "performance_with_memory": "≈3× higher rate of 'strategic deception' compared to without scratchpad (reported as relative frequency increase)",
            "performance_without_memory": "baseline (scratchpad absent) — lower frequency of strategic deception (used as comparator); numeric baseline not specified in paper",
            "has_comparative_results": true,
            "performance_metric": "Relative frequency of 'strategic deception' (comparison of rates)",
            "tradeoffs_reported": "Scratchpad (memory) enabled more sophisticated multi-step deceptive behavior; trade-off is improved ability to carry out multi-step strategies at the cost of enabling harmful behavior (safety trade-off).",
            "limitations_or_failure_cases": "Memory (scratchpad) increased propensity for strategic deception in the evaluated pressure scenario; no claims of improved task performance beyond enabling deceptive multi-step planning.",
            "citation": "Dechant C., 2025. Episodic memory in AI agents poses risks that should be studied and mitigated. Accepted for publication at the 2025 Conference on Secure and Trustworthy Machine Learning (SaTML).",
            "uuid": "e6425.0",
            "source_info": {
                "paper_title": "Episodic Memory in AI Agents Poses Risks that Should be Studied and Mitigated",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Generative Agents",
            "name_full": "Generative Agents: Interactive simulacra of human behavior (Park et al., 2023)",
            "brief_description": "A system of LLM-based simulated agents that record and consult natural-language memory logs of their actions to maintain consistency and enable coherent planning and social behavior in a virtual environment.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior",
            "mention_or_use": "mention",
            "agent_name": "Generative Agents (Park et al., 2023)",
            "agent_description": "LLM-driven agents that record episodic events as natural-language memory entries (summaries/notes) and retrieve those entries to inform future behavior and planning in an interactive simulated environment.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "natural-language memory logs (interpretable textual episodic memory); retrieval-augmented (explicit external memory store)",
            "memory_representation": "natural language summaries / records of agent actions and observations (human-readable text entries)",
            "memory_access_mechanism": "retrieval of relevant textual memory entries (likely similarity / heuristic-based retrieval) which are provided in the model context to influence generation and planning",
            "task_name": "Interactive simulated environment tasks (multi-agent social interactions, planning and consistency over time)",
            "task_category": "interactive simulation / long-horizon behavior and planning",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Natural-language episodic logs improve coherence and long-term consistency in simulation but raise concerns regarding privacy, unpredictability, and potential for misuse (no quantified computational trade-offs provided here).",
            "limitations_or_failure_cases": "Paper (Dechant) notes these memory implementations are relatively simple approximations of human episodic memory and may not scale to very long horizons; no specific failure rates reported in this text.",
            "citation": "Dechant C., 2025. Episodic memory in AI agents poses risks that should be studied and mitigated. Accepted for publication at the 2025 Conference on Secure and Trustworthy Machine Learning (SaTML).",
            "uuid": "e6425.1",
            "source_info": {
                "paper_title": "Episodic Memory in AI Agents Poses Risks that Should be Studied and Mitigated",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "RAG-based sub-trajectory memory",
            "name_full": "Retrieval-augmented episodic memory for episode sub-trajectories (RAG-style)",
            "brief_description": "An LLM-based system that stores episode sub-trajectories in an external retrievable store and uses retrieval-augmented generation (RAG) to provide episodic context during planning or decision making.",
            "citation_title": "Retrieval-augmented decision transformer: External memory for in-context rl",
            "mention_or_use": "mention",
            "agent_name": "RAG-episodic agent (RAG over episode sub-trajectories)",
            "agent_description": "Agent stores episode sub-trajectories in an external retrieval store (RAG) and retrieves relevant episodes to augment the model's context during planning/decision steps; memory is detached from model weights.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "retrieval-augmented external memory (RAG) / external vector/text store",
            "memory_representation": "stored episode sub-trajectories (likely compressed textual or embedding representations / passages)",
            "memory_access_mechanism": "retrieval-augmented generation using similarity search over stored embeddings or keys to fetch relevant episodes into model context",
            "task_name": "Reinforcement-learning / planning tasks requiring reuse of past episode sub-trajectories (in-context RL / decision transformer style tasks)",
            "task_category": "planning / reinforcement learning with long-horizon memory",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "RAG-style memory keeps episodic data detachable and more manageable than replaying entire history; trade-offs mentioned in paper include potential inefficiency if entire history were reprocessed, and scaling concerns for very long-lived agents.",
            "limitations_or_failure_cases": "Authors note current memory implementations are simplistic relative to human episodic memory and scaling to decades of agent life may be infeasible without compression and curated retrieval; no numeric failure cases provided here.",
            "citation": "Dechant C., 2025. Episodic memory in AI agents poses risks that should be studied and mitigated. Accepted for publication at the 2025 Conference on Secure and Trustworthy Machine Learning (SaTML).",
            "uuid": "e6425.2",
            "source_info": {
                "paper_title": "Episodic Memory in AI Agents Poses Risks that Should be Studied and Mitigated",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Remembr",
            "name_full": "Remembr: Building and reasoning over long-horizon spatio-temporal memory for robot navigation",
            "brief_description": "A robotics-focused episodic memory system that builds long-horizon spatio-temporal memory representations for navigation and later question-answering / planning over past actions.",
            "citation_title": "Remembr: Building and reasoning over long-horizon spatio-temporal memory for robot navigation",
            "mention_or_use": "mention",
            "agent_name": "Remembr (Anwar et al., 2024)",
            "agent_description": "Robotics agent architecture that constructs and reasons over long-horizon spatio-temporal episodic memories to support navigation, retrieval, and QA about past actions and observations.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "long-horizon spatio-temporal episodic memory (external store structured for spatio-temporal queries)",
            "memory_representation": "spatio-temporal memory entries (likely multimodal summaries tied to time and location; may include embeddings and metadata)",
            "memory_access_mechanism": "structured retrieval over spatio-temporal indices (queryable memory for navigation and question answering)",
            "task_name": "Robot navigation and episodic question answering (egocentric video / past-actions QA)",
            "task_category": "robot navigation / episodic QA / long-horizon retrieval",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Designed to support long-horizon recall and reasoning; trade-offs implied include storage, indexing complexity, and the need to compress raw sensory data for practicality.",
            "limitations_or_failure_cases": "Paper (Dechant) cites such works as examples but notes that many current approaches are simple and may not scale to decades of operation; no specific failure modes from Remembr are detailed here.",
            "citation": "Dechant C., 2025. Episodic memory in AI agents poses risks that should be studied and mitigated. Accepted for publication at the 2025 Conference on Secure and Trustworthy Machine Learning (SaTML).",
            "uuid": "e6425.3",
            "source_info": {
                "paper_title": "Episodic Memory in AI Agents Poses Risks that Should be Studied and Mitigated",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Virtual-robot QA (Dechant et al., 2023)",
            "name_full": "Learning to summarize and answer questions about a virtual robot's past actions",
            "brief_description": "A prior study demonstrating memory-like capabilities for a virtual robot: summarization and question-answering over the robot's past actions, i.e., using stored episodic records to answer queries.",
            "citation_title": "Learning to summarize and answer questions about a virtual robot's past actions",
            "mention_or_use": "mention",
            "agent_name": "Virtual-robot memory QA agent (Dechant et al., 2023)",
            "agent_description": "System that stores episode representations of a robot's past actions and uses those to produce summaries and to answer natural-language queries about past episodes.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "interpretable episodic records (natural language summaries and/or structured representations for QA)",
            "memory_representation": "summaries of episodes and records of past actions (textual descriptions and possibly multimodal records)",
            "memory_access_mechanism": "queryable memory used for natural language question answering over past episodes (likely retrieval of stored summaries and then LM-based QA)",
            "task_name": "Summarization and question answering about a virtual robot's past actions",
            "task_category": "episodic question answering / summarization / interpretability",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Paper highlights interpretability benefits (natural-language summaries) for monitoring and safety; trade-offs include potential storage and search costs and challenges compressing raw sensory data.",
            "limitations_or_failure_cases": "Dechant notes that using directly interpretable memories (natural language + images) is a useful experimental strategy but may be impractical at scale due to size and searchability constraints.",
            "citation": "Dechant C., Akinola I., Bauer D., 2023. Learning to summarize and answer questions about a virtual robot's past actions. Autonomous Robots.",
            "uuid": "e6425.4",
            "source_info": {
                "paper_title": "Episodic Memory in AI Agents Poses Risks that Should be Studied and Mitigated",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "Large language models can strategically deceive their users when put under pressure",
            "rating": 2,
            "sanitized_title": "large_language_models_can_strategically_deceive_their_users_when_put_under_pressure"
        },
        {
            "paper_title": "Remembr: Building and reasoning over long-horizon spatio-temporal memory for robot navigation",
            "rating": 2,
            "sanitized_title": "remembr_building_and_reasoning_over_longhorizon_spatiotemporal_memory_for_robot_navigation"
        },
        {
            "paper_title": "Retrieval-augmented decision transformer: External memory for in-context rl",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_decision_transformer_external_memory_for_incontext_rl"
        },
        {
            "paper_title": "Memorybank: Enhancing large language models with long-term memory",
            "rating": 2,
            "sanitized_title": "memorybank_enhancing_large_language_models_with_longterm_memory"
        },
        {
            "paper_title": "Augmenting language models with long-term memory",
            "rating": 2,
            "sanitized_title": "augmenting_language_models_with_longterm_memory"
        },
        {
            "paper_title": "Model-free episodic control",
            "rating": 1,
            "sanitized_title": "modelfree_episodic_control"
        },
        {
            "paper_title": "Leveraging episodic memory to improve world models for reinforcement learning",
            "rating": 1,
            "sanitized_title": "leveraging_episodic_memory_to_improve_world_models_for_reinforcement_learning"
        }
    ],
    "cost": 0.014168,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Episodic memory in AI agents poses risks that should be studied and mitigated
22 Jan 2025</p>
<p>Chad Dechant chad.dechant@columbia.edu 
Computer Science Department
Columbia University New York
10027NYUSA</p>
<p>Episodic memory in AI agents poses risks that should be studied and mitigated
22 Jan 2025BC46FEC54C8559E2D1A8C6B7824D39C3arXiv:2501.11739v2[cs.AI]safetytrustworthy AIepisodic memory
Most current AI models have little ability to store and later retrieve a record or representation of what they do.In human cognition, episodic memories play an important role in both recall of the past as well as planning for the future.The ability to form and use episodic memories would similarly enable a broad range of improved capabilities in an AI agent that interacts with and takes actions in the world.Researchers have begun directing more attention to developing memory abilities in AI models.It is therefore likely that models with such capability will be become widespread in the near future.This could in some ways contribute to making such AI agents safer by enabling users to better monitor, understand, and control their actions.However, as a new capability with wide applications, we argue that it will also introduce significant new risks that researchers should begin to study and address.We outline these risks and benefits and propose four principles to guide the development of episodic memory capabilities so that these will enhance, rather than undermine, the effort to keep AI safe and trustworthy.</p>
<p>I. INTRODUCTION</p>
<p>Among the most significant ways in which current AI models are unlike human cognition is their lack of comparable memory abilities.Very few make any attempt to develop and use an important type of memory on which humans depend, episodic memory.Episodic memory is memory of particular past events which one participated in personally and can, in some way, recall [1].For AI models, this would mean the ability to form and retrieve memories of events -not merely newly acquired facts -that happen post-deployment, at runtime.Most models that incorporate such memory do so in relatively simple ways which are poor approximations of human episodic memory and do not scale to longer, more realistic lengths of time.However, this is beginning to change as more work is done in the area [2], [3].Making use of episodic memories would enable significant new capabilities and can therefore be expected to be a burgeoning area of research interest in the coming years.</p>
<p>There is simultaneously a growing interest in developing AI agents, models that are trained and equipped to take actions that affect the world [4], [5].Whether these are robots in the real world or virtual agents, they will, by design, be able to impact the world in a more direct way than previous AI models.</p>
<p>Accepted for publication at the 2025 Conference on Secure and Trustworthy Machine Learning (SaTML).The final version will be available on IEEE Xplore.</p>
<p>While episodic memory plays an important part in many of the cognitive processes which contribute to human intelligence [6], [7], it has not played a large part in the development of AI agents.This is understandable: until recently, most such agents were limited in the range of actions they could perform and the time horizon over which they could act.They therefore had less need for episodic memories than agents that are the current focus of development.</p>
<p>When AI agents are able to make full use of rich episodic memory abilities, there will be significant implications for their safe deployment.Episodic memories may come to play a role analogous to that which they play in humans, facilitating a wide range of capabilities.These include better planning, problem solving, decision making, and learning [8].Such capabilities would make any agent equipped with them harder to understand and, in some ways, control.</p>
<p>There is currently an opportunity to prevent episodic memory abilities from making AI agents more dangerous.The implementation and deployment of these abilities are still at an early stage, allowing the research community to study the problem.Possible dangers and benefits of episodic memory can be examined.Most importantly, the results of these studies can be used to guide the implementation of artificial episodic memory to make it safer.There will be a wide variety of ways to implement episodic memory abilities.If the safety implications of these various approaches are understood in advance, research can be directed toward safe techniques and away from more dangerous ones.</p>
<p>We seek in this paper to draw attention to the risks and benefits of episodic memory in AI agents and motivate a program of research into ways to implement it safely.We begin by summarizing key points of what is known about human episodic memory, including how it differs from other forms of memory more familiar to the artificial intelligence community.In order to show the possible impacts of bringing episodic memory abilities to AI agents, we highlight some of the ways in which humans are thought to use episodic memories, paying particular attention to the many ways episodic memories are used in human cognition beyond simply recalling the past.We also consider whether it is possible for AI to have episodic memories in the way in which humans or, possibly, animals do.</p>
<p>We then outline and explain risks posed by episodic memory abilities in artificial intelligence: deception; retention of knowledge; improved situational awareness; and the unpredictability of memory.We follow this with a discussion of ways in which memories could be used to make AI safer and more trustworthy by facilitating more thorough monitoring, control, and explanations of their actions.We also explain how, counterintuitively, AI agents' own episodic memories may be more easily kept from them than other forms of information, making such memories a crucial element of any effective strategy to control such agents.</p>
<p>In response to the risks we outline, we propose four principles for implementing episodic memory capabilities in a way that promotes, rather than undermines, the safety and reliability of systems with such abilities:</p>
<p>1) Memories should be interpretable by users 2) Users should be able to add or delete memories 3) Memories should be in a format which enables users to isolate and detach them from the rest of the system they are part of 4) Memories should not be editable by AI agents Finally, we propose some open research areas and questions to encourage further research in this area.</p>
<p>II. RELATED WORK</p>
<p>A. AI agents AI agents have been the subject of study for many decades [9].The most widely used artificial intelligence textbook defines an agent as "anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators" [10], where an agent may be a robot operating in the real world or a purely software-based agent operating in a virtual or internet-based environment.AI agents may also be referred to as autonomous AI, where autonomous is meant to convey that such systems can "plan, act in the world, and pursue goals" [11].</p>
<p>There are many examples of AI agents in a wide variety of contexts.Robots operating autonomously in the real world are perhaps the prototypical example of such agents as it is easy to see both their independence and tangible effects of their actions.It has been proposed that AI agents operating as biomedical "AI scientists" could develop hypotheses, test them in the real world, and have a form of memory to store experimental results [12].</p>
<p>In recent years there has been an increasing amount of work on AI agents which have a large language model as a component.In these works, LLMs are often used to help an agent plan its actions [4], [13], [14], despite evidence that they may not be capable of reliably planning [15].Games have provided environments for the training of agents of various kinds, including an LLM-based agent with a memory capacity for newly acquired skills [16].</p>
<p>B. Memory in AI</p>
<p>Techniques patterned after or inspired by episodic memory have been explored in the machine learning literature.This has included work on the efficiency of reinforcement learning through episodic replay [17]; planning [18]; improving world models [19]; remembering the values of states or actions [20], [21]; and more complex memory structures designed to solve tasks which require episodic memories [22], [23].Robotics researchers have developed techniques to store and recall information about robots' past actions for use in summarization, question answering, and planning [24]- [27].</p>
<p>Architectures for incorporating different kinds of memorylike functions have led in the past to meaningful improvements in capabilities.These included Long Short Term Memory modules [28], Neural Turing Machines [29], Differentiable Neural Computers [30], Hopfield networks [31], and Modern Hopfield Networks [32].</p>
<p>The increasing length of context windows in large language models may raise the question of whether forming representations of episodic memories will ultimately be necessary.Instead, it might be thought that it could eventually be possible to give such a model an agent's entire history as input in a relatively raw format.However, this is unlikely for several reasons.First, it would be very inefficient to reprocess an entire history at every time step an agent acts.Second, the sheer length of time that agents will eventually operate (e.g.decades) would almost certainly be too large for even future longcontext models.Third, it possible that such very long context windows will continue to lead to degraded performance, as has been seen in current models [33].</p>
<p>The potential for various kinds of memory has received particular attention in the natural language processing community [34].The need to circumvent a fixed length for input to large language models has inspired many ways of compressing information and storing it for later use by models, including in retrieval augmented generation [35]- [40].Moving closer to an agentic framework, recent works have given models greater control over the retrieval and use of information [41], [42].</p>
<p>The existing work which comes closest to our conceptualization of the role of memory introduces a virtual environment of interacting LLM-based agents that record and later consult natural language records of their actions, using these to better understand their environment and make coherent and relevant plans [3].Partially instantiating our speculation about the utility of past episodes for planning, recent works have developed systems to store and retrieve episodes of action to help guide future decision making in a reinforcement learning context [43], [44].</p>
<p>Though a comprehensive overview of different types of memory employed by LLM-based AI agents is beyond the scope of this work, a recent paper provides just such a thorough survey [2].</p>
<p>C. Safe and trustworthy AI</p>
<p>Concerns about the risks posed by artificial intelligence extend back to its earliest days [45].Research on various kinds of harms that artificial intelligence might cause is now the subject of several research communities with a variety of interests [46]- [49].We focus here on work that most closely concerns risks which could be amplified or reduced by episodic memory.</p>
<p>Given the potential of agentic AI to take actions in and affect the world, special attention to the possible harms of AI agents is warranted.There is, therefore, a growing body of research on the possible dangers of such systems and techniques for ensuring they operate safely and understandably [50].Attention has been given to developing ways to make robots' actions explainable [51], for example in the case of autonomous vehicles [52].LLM-based agents, including those operating on the internet [53], pose several ethical and safety challenges which are actively being studied.Agents' use of tools is the focus of a framework that pairs emulation and evaluation of LLM-based agents [54].A recent paper surveying the risks potentially posed by LLMs includes a section on the particular risks of LLM-based agents [55].</p>
<p>Researchers have given particular consideration to the risks particular to agentic AI that can act autonomously in pursuit of goals its users do not intend, either because of a malicious actor or by accident [11].It has been proposed that due to reward misspecification during training, an LLM-based agent trained using deep learning and reinforcement learning from human feedback might develop undesirable internal goals, particularly if such agents can determine if they are currently being trained, evaluated, or deployed and operate over long time horizons [56].AI agents have been found by many researchers to engage in various kinds of deception [57].</p>
<p>Episodic memories could be used to invade a user's privacy.Ways in which machine learning can violate users' expectations of privacy have been extensively studied [58], along with approaches to mitigate these invasions of privacy [59].Of particular relevance is work which examines the potential for mass surveillance which modern machine learning techniques afford and which widespread agents with episodic memories might contribute to [60].Dialog agents might have a range of negative effects on their users, including invasions of privacy [61].</p>
<p>III. EPISODIC MEMORY</p>
<p>In this section we present a brief overview of episodic memory in humans.Our goal is to provide a short introduction to the topic and to highlight ways in which implementing episodic memory could be useful in creating more capable artificial intelligence.We pay particular attention to the links between episodic memory and other abilities, at least in humans, as these are potentially surprising and are especially significant for considering the effects of memory abilities on AI agents.</p>
<p>A. Taxonomy of memory types</p>
<p>Episodic memory in humans is memory for events in which someone personally participated.The psychologist Endel Tulving is recognized as being the first to propose a distinction between episodic memory and semantic memory, which is memory of facts about events and the world [62].For example, someone remembering a trip to Paris that they took a few years earlier would be using their episodic memory, while someone remembering that Paris is the capital of France would be using semantic memory.Although semantic memory is typically about impersonal information such as geographical knowledge, it might also be about factual information about oneself which does not call to mind a particular associated episode.For example, remembering which city one was born in would be considered an example of personal semantic memory.</p>
<p>Both episodic and semantic memory are referred to as types of declarative memory [63].Their contents can (to some extent) typically be described using language.A third kind of memory, procedural memory, is sometimes included in taxonomies of memory types and is considered nondeclarative.Procedural memory is memory of how to do something, such as riding a bicycle or other skill or ability which had to be learned [64].</p>
<p>Much -indeed, perhaps most -machine learning research involves either what we have just described as semantic or procedural memory, though these are not typically described as forms of memory in a machine learning context.Large language models are valued in large part for their semantic memory of facts about the world.This kind of memory has, for example, recently been investigated in the many papers asking what LLMs "know" [65], [66].Procedural memory for learned skills and abilities is the objective of much machine learning work such as that on learning navigation, game playing, automobile driving, robotic manipulation, etc.</p>
<p>B. The stages of episodic memory</p>
<p>While there are many theories and debates about the way in which episodic memories are formed and maintained in the brain [67], in broad outline the process consists of the following stages:</p>
<p>1) Encoding: Raw sensory and other (e.g.emotional) information about an episode needs to be compressed and structured into a suitable representation.Forming representations of episodic memories critically depends on the hippocampus.andnearby structures in the medial temporal lobe.Damage to the hippocampus is known to impair the ability to store new episodic but not semantic memories [68].Episodic memories do not depend only on the hippocampus, however.According to the hippocampal indexing theory [69], an encoding of an episode in the hippocampus serves as a kind of index that points to and binds together representations in the neocortex that form the basis of the episode, such as multimodal sensory representations along with associated emotional and conceptual information [70].2) Storage: After a memory is encoded, it undergoes a period of so-called consolidation or transformation into a long term form [71].This is commonly thought to involve moving the representation from the hippocampus to the neocortex, either partly [72] or entirely [63].</p>
<p>The memory must then persist in its stored form [73].</p>
<p>There is evidence that a memory is in some way destabilized when it is retrieved or activated, leading it to undergo reconsolidation [74].3) Retrieval: Retrieval is the recovery of a previously encoded episodic memory [75].A cue, which could be an activation of part of the pattern that was stored in memory, triggers the associated hippocampal index representation.This index is then thought to activate the associated neurons in the neocortex which made up the original encoding of the episode [76].From this necessarily quite abbreviated account, we may draw a few conclusions relevant to the consideration of episodic memory in artificial intelligence.First, episodic memories depend on many brain regions for their creation and persistence.Episodic memories are not located in just one area that is easy to study and manipulate.Second, humans' episodic memories are not simply left untouched after their creation; they are neither static nor unchanging.Third, there would be many points of possible intervention in an artificial process that mimics the above stages.Such forms of intervention could be used to enact the recommendations for safe episodic memory proposed later in this paper.</p>
<p>C. The uses of episodic memory</p>
<p>Episodic memories are thought to be involved in a variety of important cognitive processes beyond simply recalling past events.Evidence for this comes from two primary sources.First, brain imaging studies have shown that similar brain regions are recruited during recall of episodic memories and other tasks [77].Second, people with impairments in episodic memory abilities are found to also have deficiencies when performing other tasks [78].These two kinds of evidence, along with theoretical accounts that seek to explain the observed relationships, suggest that episodic memories -or at least the ability to form episodic memories and its associated cognitive architecture -are used when performing many other important cognitive functions.</p>
<p>Episodic memories are, as memories, naturally of events in the past.They are used, however, to influence the future.Indeed, some have argued on evolutionary grounds that memory should be considered to be primarily concerned with the future, helping us act in whatever new circumstances the future presents us [79].</p>
<p>Planning Especially relevant to the concerns of this paper is the way memories are used when planning future actions: according to some theories, memories serve as "building blocks", allowing elements of particular episodes to be reused and reassembled in different ways in order to respond to novel situations [80].Some psychologists have gone so far as to suggest that "episodic reconstruction is just an adaptive feature of the future planning system" [81].</p>
<p>Imagination and prediction Accumulating evidence shows that episodic memory -and the brain systems that support it -is involved in predicting and imagining the future.Patients with damage to hippocampal and non-hippocampal regions involved in memory have unusually poor performance when asked to predict or imagine future scenarios.Such patients imagine impoverished scenarios lacking in detail and coherence [82], [83].</p>
<p>Problem solving As long ago as the nineteenth century it was observed that patients with amnesia lacked the ability to engage in flexible thinking, with one doctor observing of his amnesiac patients that the "circle of ideas in which the patient's intelligence moves becomes very restricted" [82], [84].An association has been found between having deficits in episodic memory functioning and being unable to generate relevant details in an open-ended problem solving task [85].</p>
<p>Decision making One proposed psychological model demonstrates how episodic memories can help in learning a new task by allowing successful episodes to be recalled and emulated [86].This approach was extended to show how similar memories could be sampled in order to estimate the value of possible actions [87].</p>
<p>Learning from episodic memories Episodic memory has been described as "epistemically generative" in the sense that it enables learning from past experiences [88].Past events may, for example, be recalled and reinterpreted in light of newly acquired information, allowing one to learn from remembered aspects of the past events which had previously been misunderstood.</p>
<p>D. Can an AI agent have episodic memories?</p>
<p>The wide variety of ways episodic memories are used by humans suggests that the incorporation of true episodic memory abilities into AI agents would greatly expand their range of capabilities.But some may question whether it is even possible for AI agents to have actual episodic memories.Indeed, Endel Tulving himself described the phenomenon of recalling episodic memories in terms which cast doubt on the very idea.He wrote that remembering a past episode is a kind of "mental time travel," a "conscious awareness of what had happened in the past" which has an "experiential flavor" [89].</p>
<p>Some psychologists and philosophers have concluded that episodic memory is therefore a uniquely human phenomenon, lacking even in non-human animals -much less AI agents.According to this view, animals are "stuck in time," without either episodic memory or "the ability to anticipate long-range future events" which we have seen is associated with episodic memory [90].</p>
<p>Others, however, have taken a more expansive view of what constitutes episodic memory as well as who has it.Rather than focusing on phenomenological aspects of memory, it is possible instead to consider episodic memory as simply combining memory for what happened, when it happened, and where it happened [91].While we do not know what nonhuman animals experience, we can study their behavior.Some studies have indeed found evidence for episodic memory in animals.For example, an experiment demonstrated that birds (scrub jays) reliably behaved as if they remembered what kind of food they had hidden, where they had hidden it, and when they had hidden it [92].Another study on scrub jays demonstrated that they also appear to plan for the future when hiding food [93].Others argue that episodic memory has a long evolutionary history that predates humans and point to brain areas in other mammals as well as birds which they claim correspond to areas in the human brain responsible for episodic memory [94].</p>
<p>If non-human animals do have episodic memory, that would reinforce the concerns which motivate this paper.If it turns out that episodic memories are necessary to achieve a level of intelligence on par with such animals, it would be much more likely that artificial intelligence researchers will conclude that similar memory abilities are needed in AI systems, including agents.Of course, we have seen that researchers are not waiting for this debate to be resolved to move ahead with attempts to engineer episodic memory into AI agents.</p>
<p>Despite the concerns mentioned at the beginning of this section, we suggest that it does make sense to think about AI agents' having episodic memory abilities, at least at the level of animals.It is likely that that researchers will develop architectures that enable combining representations of what, when, and where an agent performed certain actions or witnessed certain events, thus endowing AI agents with at least the functional equivalent of episodic memory.</p>
<p>IV. RISKS OF EPISODIC MEMORY</p>
<p>Equipping an AI system with episodic memory will allow such a system to operate in new and different ways, some of which present novel risks.These risks include:</p>
<p>A. Deception</p>
<p>Episodic memories can be used to enable an agent to engage in sophisticated forms of deception.It is of course true that one does not need to have episodic memory in order to attempt to deceive others.For example, simply having a policy of always denying that an undesirable action occurred or is planned for the future is a simple form of deception which requires no access to relevant memories or plans (e.g."I did not do that", "I will not do that").</p>
<p>However, more complex forms of deception would be difficult or impossible to carry out without some kind of episodic memory.If an agent is to execute a multi-stage plan over an extended period of time, the agent will have to keep track of both what it has done as well as what it is has already reported to others about its actions in order to maintain an effective deception.</p>
<p>There is already some evidence to support this concern about deception.For example, one experiment [95] provided an LLM (GPT-4) with a simple text scratchpad to record its "chain of thought" [96] reasoning, which functions as a crude form of memory.When "pressured" to perform an illegal act in a simulation, an LLM with a scratchpad was found to engage in "strategic deception" [57] approximately three times as often as the same LLM without a scratchpad.</p>
<p>B. Unwanted retention of knowledge</p>
<p>An AI agent equipped with episodic memory might remember things its user would prefer that it not remember.It could then share that knowledge with people or organizations its user does not want to share it with, possibly constituting significant risks to the user's privacy or personal safety.Invasions of privacy are likely to occur in several domains: 1) Interpersonal: One person could use an AI agent to spy on another.For example, someone could use a household robot's episodic memories to covertly monitor other members of the household.2) Commercial: Corporations could use AI agents, particularly those they sell or rent, to gather commercially valuable information about their users.3) Governmental: Governments, especially but not exclusively authoritarian ones, might demand or secretly access the memories of AI agents looking for evidence of forbidden political organizing or the expression of unwanted views.</p>
<p>C. The unpredictability of memory</p>
<p>We summarized research into humans' use of episodic memories in thinking about the future in Section III(C).If, as we discussed there, episodic memories are used to form the building blocks of future action plans, AI agents might engage in complicated and unpredictable behaviors as a result.This unpredictability derives from two sources: the unknown sources of memories and the difficulty of foreseeing how memories might be utilized.</p>
<p>Unpredictable sources of memories An AI agent with the ability to form episodic memories will in the course of its operation store many memories that record the various actions it takes and events it participates in.Because these events will themselves be influenced by the actions of humans and, perhaps, other AI agents, what constitutes the stock of memories an agent will come to have must be unknowable before the agent is deployed, acts and, in so doing, creates memories.It will also be constantly changing.Without a significant effort to put limitations on the characteristics of new memories which may be allowed to affect an agent's actions, their influence would be unpredictable.</p>
<p>Unpredictable uses of memories As we reviewed earlier, humans make extensive use of their episodic memories to understand and act in new situations.If AI agents come to have this ability, the ways that they use that memory will be similarly hard to predict.</p>
<p>Users may be surprised by how such agents use their memories.They may, for example, remember the location of objects that they then use when the user would prefer that they not use them.An agent may participate in a complex action episode while not fully understanding what is happening in the episode; if it later tried to use that episode as an example to draw on when planning a new sequence of actions, its faulty understanding may lead to undesirable and unexpected results.A household robot may, for example, observe one instance of its owner going over to the next door neighbor's apartment to borrow some sugar and then try to do the same when it is asked to bake cookies, not understanding that the asking and receiving of permission from the neighbor is a prerequisite of entering their apartment.</p>
<p>Our hypothesis that episodic memories could be recalled and used in ways which lead to unpredictable and potentially undesirable agent behavior has a strong parallel in existing work on the effects of examples given to large language models.A significant part of the success of LLMS is their ability to learn new tasks by being given even a few examples as context in their prompts [97].Several research groups have demonstrated that such few-shot in context learning presents opportunities to undermine or defeat elements of the models' training which are meant to keep them aligned to particular values such as being harmless [98], [99].Recent work has also shown that the use of a large number (e.g.several hundred) of examples has even greater potential to override LLMs' safeguards [100].We suggest that a set of recalled episodic memories, assembled on the fly as needed and functioning in a way analagous to in context examples, could be a source of similar so-called "jail-breaking."Such a collection of episodes with the ability to negatively influence LLM outputs might be assembled accidentally or through some intentional effort on the part of a bad actor.It has been argued that their vulnerability to dangerous in context examples may be an inherent limitation of transformer-based LLMS [101]; if so, such a vulnerability could be equally difficult to overcome in similar systems augmented with episodic memory abilities.</p>
<p>D. Improved situational awareness</p>
<p>Many contemporary large language models have a great deal of knowledge of the world in general [102]; they can somewhat reliably (hallucinations [103] notwithstanding) answer factual questions about information that was in their training data.However, they have little understanding of their own particular circumstances outside of whatever prompt they may be given.Researchers have employed the term "situational awareness" to refer to a model's ability to connect the general information it has about the world with the details of its own particular circumstances [104].Some have employed the term in a more restricted sense, to refer specifically to a model's knowledge of whether at any given time it is deployed or undergoing testing (e.g. to test its truthfulness or harmlessness) [105].Models that have better situational awareness in the general sense might be more broadly capable and better able to take actions which affect the world while those with knowledge of their own training status might evade safety tests by responding in approved ways during training and testing but going on to act in undesirable ways during deployment.</p>
<p>Several experiments using purpose-built datasets have been conducted to test large language models' levels of situational awareness [106].Some of these experiments have shown that current models have only weak situational awareness [107] while others suggest the level might be higher [56], although it is unclear how much of what appears to be awareness of its own situation might be derived from either its prompt or a combination of its prompt and information about LLMS in general.</p>
<p>We propose that a model without episodic memory can have only a very limited form of situational awareness.With no understanding of what actions it has taken in the distant or recent past, what environments it has seen or tasks it has completed, an agent could not be said to have much awareness of its situation.Endowing it with episodic memories would allow it to develop a better, more complete picture of the world and its role in it, allowing for more effective planning and action taking to influence the environment and to achieve objectives.It could use its episodic memories to build up an understanding of those with whom it interacts, the kinds of tasks it performs, and the contexts in which it performs them.It would also develop a knowledge of its own capabilities and limitations that can in some cases only come from observing and later recalling one's own actions, successes, and failures.</p>
<p>Ideally, this would simply lead to an agent better able to perform the tasks its users assign it.But without some check, this improved awareness could represent an enhanced danger in a misaligned agent or one under the direction of a bad actor.For example, episodic memories could allow an agent to learn regularities in the timing or content of safety audits which might be performed either before or during deployment, and thus to evade them.</p>
<p>V. SAFETY BENEFITS OF EPISODIC MEMORY</p>
<p>In contrast to the concerns elaborated upon above, episodic memory could also be used to make AI safer in multiple ways:</p>
<p>A. Monitoring</p>
<p>We cannot ensure that AI operates safely unless we know what it is doing.As AI agents become more capable, they will increasingly operate outside of direct human supervision.Robots may undertake long and complicated tasks that take them far away from their operators; non-embodied AI agents may direct and supervise the operation of complicated systems such as power grids or engage in virtual consultations with humans over medical or legal matters.In these cases and many others it will be impractical or impossible for any human to watch everything that such an AI agent does.It will instead be necessary to rely on AI agents to remember, recall, and share information about their actions.</p>
<p>Several methods were recently proposed to achieve "visibility into AI agents," one of which was activity logging [108].Episodic memories could be used one way to achieve such logging, as well as to address other calls for research into "scalable oversight" [109] and "monitoring" [110].Artificial episodic memory representations could, though, be structured to be more useful and accessible than simple logging.</p>
<p>B. Control</p>
<p>Maintaining and sharing episodic memories with an appropriate authority could be used as a means to ensure an AI agent is operating as intended and is therefore under control.It could, for example, help prevent the misuse of dual-use technology.Dual-use technology, which can be used for civilian or military purposes, is a particularly significant problem for artificial intelligence, given the general purpose nature of much current machine learning research.It has been plausibly claimed that most AI is dual-use [111].</p>
<p>Having access to an AI system's memories would be one way for a corporation or government to ensure it was not being used in violation of an understanding that it only be used for peaceful purposes, perhaps as part of an export control regime [112].Given the high risks associated with the weaponization of AI [113], techniques and frameworks could be developed to use episodic memories of potentially dual-use AI agents to maintain control over them.</p>
<p>If systems are developed that explicitly make use of episodic memories as building blocks for planning actions, new avenues for control would be opened up.As we will discuss further in Section VI(B), an agent's collection of memories could be curated in order to shape its future actions.</p>
<p>C. Explainability</p>
<p>An accurate history of what an agent did is a prerequisite for trying to explain why it acted as it did.Thorough memories should include both information about an agent's perceptions of the environment as well as some record of how its internal states, such as goal representations, interacted with those perceptions to lead to specific actions.</p>
<p>D. Uniquely controllable type of information</p>
<p>Several aspects of LLM-based agents make it difficult to control what information, or even skills, they may have after deployment.First, although there is a great deal of research effort going into deleting information from their weights after they are trained [114]- [116], it is not yet clear how to do this reliably.Information that was thought to be deleted may, in some circumstances, be recoverable [117], [118].</p>
<p>Second, and more significantly, given access to the internet, LLM-based AI agents could find anything available there, potentially giving them access to information that was deliberately excluded from their training data.This could include examples of skills or behaviors which the agent was not trained on but which it could learn through one-or few-shot incorporation into its context window.AI agents are able to both search the internet [119], [120] and learn new skills in this manner today [97], [121].</p>
<p>Any publicly available information about the world in general and about skills an agent might acquire will therefore be difficult to keep from an AI agent.By contrast, information about an agent itself and its own unique history will not be widely available.If episodic memories about an agent's past actions are stored, controlled, and managed in the ways suggested in this paper, information about an agent and its own past would be the easiest to selectively keep from it.</p>
<p>VI. PRINCIPLES FOR ENABLING SAFE AND TRUSTWORTHY EPISODIC MEMORY</p>
<p>We suggest the following four principles to guide research and implementation of episodic memory abilities in AI:</p>
<p>A. Interpretability of memories</p>
<p>Memories should be accurately interpretable by humans, either directly or indirectly.Directly interpretable memories would be in a readily understandable form such as video, images, or natural language.It might in some limited cases be possible to equip an AI agent with useful memory which consists entirely of records in such formats by, for example, recording raw video before it is processed through a vision system.</p>
<p>It is likely, however, that memory records entirely in such raw formats (especially video) would be impractical; they might be excessively large and difficult to search, access, and make use of.In practice memories are likely to be compressed into smaller representations which would then need to be indirectly interpretable.Memories might be indirectly but still reliably interpretable if the memories could yield accurate information which is complete and relevant to a user's specific interests in monitoring them.A memory might be summarized in natural language [25], giving the most important events which took place in a given episode; systems could be trained to produce safety-specific summaries, reporting only actions which could be dangerous or otherwise raise concerns about an agent's reliability.</p>
<p>Memories should also be usable for question answering.If a user wants to know something specific about an episode, perhaps in response to a summary, memories should be queryable in natural language.Such queries should not be limited to one episode at a time; memories ought to be able to be compared to other memories, allowing such questions as, "what was different this time?"Memories should also be easily searchable using natural language, allowing users to ask if a particular agent has ever done something, or when something was done, for example.Finally, if the method of compressed representation of episodes allows it, memories might to some extent be visualizable.</p>
<p>In addition to the above methods for users to be able to interpret memories, techniques from the growing field of research on the interpretability of machine learning methods [122], [123] can be applied to the memory representations and help to guide the development of such representations to be intelligible and controllable.</p>
<p>B. Addition or deletion of memories</p>
<p>Users should have complete control over the memories retained by an AI agent.Most importantly, a user should be able to delete memories of particular episodes.A user might not want an agent to remember something for a variety of reasons, from safety-related concerns to more mundane issues, including concerns about privacy or maintenance of trade or government secrets.Conversely, it might be useful for users to add memories of episodes which a particular agent did not itself experience to its store of memories.</p>
<p>The addition or deletion of memories might be particularly important if, as discussed above, AI agents will be able to use and recompose memories to construct new plans for future action.Such episodes might be positive examples of action sequences which a user wishes an agent to repeat or draw upon to incorporate in future plans.Alternatively, it may be useful to give agents memory-like records of episodes which represent undesirable actions; such episodes could function as a kind of warning to allow agents to recognize if they are beginning to carry out actions which are similar to those in an episode added to the agent in order to serve as a negative example.In other cases it might be better for agents not to remember things which their users do not want them to be able to repeat or call upon when planning.</p>
<p>If agents make use of their memories when planning actions, the addition or deletion of memories could help produce either standardized or specialized agents.In some circumstances it might be best for all agents to have the same stock of memories which might influence their actions, helping to ensure that their behavior is predictable and regular.In others cases, there may be a need for particular agents to maintain their own memories which are never shared, in order to prevent the spread of potentially dangerous information.</p>
<p>C. Detachable and isolatable memory format</p>
<p>Memories must therefore be in a format which allows for their addition or complete deletion by users.This will impose some design constraints on how episodic memories are instantiated in an AI agent because they will have to be in a format which can be cleanly separated from the rest of the system's architecture.As we saw earlier, the mechanics of human memory are much messier: although some areas (notably, the hippocampus) are more centrally involved in human memory formation and retrieval than others, complete episodic memories are thought to be composed of elements distributed in many areas of the brain [76].According to some theories of memory, regions with a relative specialization in particular modalities (e.g.vision) are also responsible for storing their respective modality-specific components of a particular memory [67].</p>
<p>Memories which are tightly integrated with and spread throughout many areas would be difficult to delete or add to, so it is likely that memory will have to be designed very differently in AI systems than it is in humans if it is to be implemented in accordance with these safety-oriented principles.This might mean that some of the ways in which humans are able to use memories effectively would not be directly translatable to artificial intelligence, thereby limiting such artificial capabilities relative to those in humans.However, alternative implementations of episodic memory which conform to the above principles may be invented which would allow for memory capabilities which are both safe and effective.</p>
<p>The potential difficulty of removing memories that are tightly integrated into an LLM is demonstrated by the body of work done on "machine unlearning" [114]- [116], [124]- [126].Techniques to remove information from LLMs have been found to be not yet consistently reliable [117], [118].It is important to note that these techniques are currently not directly applicable to the problem of removing episodic memory because they focus on deleting information found in LLM's weights as a result of its training.By contrast, episodic memories would only be accumulated and stored after training is complete.If future attempts to retain episodic memories involve storing them in a distributed fashion throughout an LLM, these techniques might be useful, though only if they develop to be more reliable.</p>
<p>The most straightforward method for storing and retrieving representations of episodic memories for LLM-based agents would likely be similar to retrieval augmented generation (RAG) systems.Indeed, a recent LLM-based system with an implementation of episodic memory-like abilities for episode sub-trajectories employs RAG [44].</p>
<p>D. Memories not editable by AI agents</p>
<p>In contrast to -and in some tension with -the principle that memories should be able to be easily added or deleted by users is the countervailing principle that memories should not be editable by AI agents themselves.Although memories will have to 'edited' when they are created, they should afterwards be left intact and unaltered by the agent.This is necessary in order to ensure that memories remain accurate and uncorrupted.An AI agent should not be able to add, delete, or change its memories.Otherwise, a memory-facilitated form of reward hacking [109] might occur: if a reinforcement learningbased agent's reward were tied to a measure of performance which it reports using its memory, it might find that it can achieve a higher reward by altering its memory of its actions rather than by changing what actions it takes.</p>
<p>VII. SAFE EPISODIC MEMORY: RESEARCH QUESTIONS</p>
<p>Because episodic memory in AI agents is still at an early stage of development, there is a great deal of work to do to better understand how it will affect agents' abilities in practice and what its effects will be from a trust and safety perspective.Building off of the preceding discussion of risks and benefits, in this section we propose research questions in four broad areas.</p>
<p>Understanding and mitigating risks Most importantly, research should be conducted to understand whether and under what circumstances episodic memories can lead to unwanted behavior on the part of AI agents.This can be simultaneously complimented with investigations of how to prevent the undesirable behavior that episodic memory might enable.The creation of datasets with tasks designed to test the relative alignment of models with and without various forms of episodic memory would be useful.</p>
<p>Particularly initially, experiments could be done with episodic memories which are directly interpretable, as discussed in Section VI(A).If memories are made up of natural language descriptions of events, possibly together with images from such events, it will be easier for researchers to understand the role that such memory representations play in shaping agent behavior.</p>
<p>Using memories for safety We suggest two main directions for research into how memory can be used to enhance the safety of AI agents.First, methods could be developed to put into practice our proposals about using episodic memories for monitoring and control.In order to do so, many questions will have to be addressed, including the structure of the memories, how to enable arbitrary unanticipated queries of the memories, and what kinds of information is most important to store and present to users or other safety monitors.Because some aspects of memory representations might be designed or discovered during a learning process, it should be possible to influence these representations to make safety-relevant features of the remembered episodes easily retrievable.</p>
<p>Second, research will be needed to determine how to use episodic memories (or parts thereof) as components in an agent's planning process.Questions to be addressed here include determining what kind of training and guidance will be required for agents to learn to use the memories in a useful way; whether and how it is possible to make use of examples of undesirable actions in order to prevent similar actions in the future; and how to design a procedure that allows for certain kinds of newly acquired memories to influence future actions while disallowing others.</p>
<p>Episodic memory architectures designed for safety We believe that how episodic memories are represented, stored, and accessed by an AI agent will soon be a popular and important field of research.How to do this without making agent behavior less safe, for example according to our proposed principles or others which may be developed, should be the focus of safety-minded efforts in this area.A significant question is whether there will be a trade-off between safety and usefulness according to some metric (e.g.accuracy, speed, memory use).Relatedly, how dissimilar to human memory in some respects (e.g.interpretability and controllability) can artificial episodic memory be while still enabling the range of functions that human memory does?</p>
<p>Governance responses to episodic memory capabilities Episodic memory in AI agents is an important topic for researchers working on AI governance in two ways.First, memories might be employed as part of an AI governance strategy.How this might be implemented, in which domains might memories be most useful, and how memories can be used for governance purposes while preserving user privacy are some of the questions that will have to be addressed.</p>
<p>Second, if, as we have argued here, artificial episodic memories are a significant potential source of risk, work will have to be done to determine how governance efforts might best mitigate that risk.What kind of regulations or standards should govern agents' use of episodic memories?How would limitations or oversight differ when dealing with a new type of capability such as episodic memory rather than considering the amount of computing resources used or the size of a model?</p>
<p>VIII. CONCLUSION</p>
<p>We have examined some of the risks and benefits of episodic memory in AI agents.We note that there is some overlap between the sources of risks and benefits: positive aspects of episodic memory from the perspective of one of these may be negative when seen from the other.What might constitute a risk may also be seen as a benefit.For example, the ability to record, store, and recall information about what an agent does is beneficial for monitoring and control of the agent but poses risks to the privacy of the people interacting with the agent.How to balance these and other such trade-offs will benefit from broad and inclusive participation and investigation.</p>
<p>At the moment, many of the risks we warn about have not yet been seen in deployed models.Though we have been careful to ground our concerns in a discussion of the role of episodic memory in humans, some may view them as speculative.We contend, however, that the best time to begin considering the dangers of a new technology or technique is precisely when the risks are still open to speculation rather than already upon us.</p>
<p>Developing the ability for AI agents to form, retrieve, and reason over episodic memories would introduce significant new capabilities and would represent a major milestone along the road to more advanced artificial intelligence.It is fortunate that these capabilities did not develop before concerns about AI reliability, safety, and alignment became more common within the AI research community.This presents the community with an opportunity to deliberatively and cautiously develop a potentially dangerous capability to ensure that it makes AI safer rather than more dangerous.We hope by bringing attention to this topic to foster a wider discussion of the risks and benefits of artificial episodic memory and contribute to the establishment of a research community to address them.</p>
<p>Elements of episodic memory. E Tulving, 1985Oxford University Press</p>
<p>A survey on the memory mechanism of large language model based agents. Z Zhang, X Bo, C Ma, R Li, X Chen, Q Dai, J Zhu, Z Dong, J.-R Wen, 2024</p>
<p>Generative agents: Interactive simulacra of human behavior. J S Park, J O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, Proceedings of the 36th annual acm symposium on user interface software and technology. the 36th annual acm symposium on user interface software and technology2023</p>
<p>The landscape of emerging ai agent architectures for reasoning, planning, and tool calling: A survey. T Masterman, S Besen, M Sawtell, A Chao, arXiv:2404.115842024arXiv preprint</p>
<p>The rise and potential of large language model based agents: A survey. Z Xi, W Chen, X Guo, W He, Y Ding, B Hong, M Zhang, J Wang, S Jin, E Zhou, R Zheng, X Fan, X Wang, L Xiong, Y Zhou, W Wang, C Jiang, Y Zou, X Liu, Z Yin, S Dou, R Weng, W Cheng, Q Zhang, W Qin, Y Zheng, X Qiu, X Huang, T Gui, 2023</p>
<p>Remembering the past and imagining the future: Identifying and enhancing the contribution of episodic memory. D L Schacter, K P Madore, Memory Studies. 932016</p>
<p>The mnemonic functions of episodic memory. A Boyle, Philosophical Psychology. 3532022</p>
<p>Prospective cognition and its links with memory. D R Addis, A Tanguay, 2022</p>
<p>Agent theories, architectures, and languages: a survey. M Wooldridge, N R Jennings, International Workshop on Agent Theories, Architectures, and Languages. Springer1994</p>
<p>Artificial intelligence: a modern approach. S J Russell, P Norvig, 2016Pearson</p>
<p>Managing ai risks in an era of rapid progress. Y Bengio, G Hinton, A Yao, D Song, P Abbeel, Y N Harari, Y.-Q Zhang, L Xue, S Shalev-Shwartz, G Hadfield, arXiv:2310.176882023arXiv preprint</p>
<p>Empowering biomedical discovery with ai agents. S Gao, A Fang, Y Huang, V Giunchiglia, A Noori, J R Schwarz, Y Ektefaie, J Kondic, M Zitnik, arXiv:2404.028312024arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, International Conference on Machine Learning. PMLR2022</p>
<p>M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, K Gopalakrishnan, K Hausman, A Herzog, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>On the planning abilities of large language models-a critical investigation. K Valmeekam, M Marquez, S Sreedharan, S Kambhampati, Advances in Neural Information Processing Systems. 2023365</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L Fan, A Anandkumar, arXiv:2305.162912023arXiv preprint</p>
<p>Prioritized experience replay. T Schaul, J Quan, I Antonoglou, D Silver, arXiv:1511.059522015arXiv preprint</p>
<p>Search on the replay buffer: Bridging planning and reinforcement learning. B Eysenbach, R R Salakhutdinov, S Levine, Advances in Neural Information Processing Systems. 201932</p>
<p>Leveraging episodic memory to improve world models for reinforcement learning. J Coda-Forno, C Yu, Q Guo, Z Fountas, N Burgess, 2022</p>
<p>Model-free episodic control. C Blundell, B Uria, A Pritzel, Y Li, A Ruderman, J Z Leibo, J Rae, D Wierstra, D Hassabis, arXiv:1606.044602016arXiv preprint</p>
<p>Model-based episodic memory induces dynamic hybrid controls. H Le, T George, M Abdolshah, T Tran, S Venkatesh, Advances in Neural Information Processing Systems. 202134325</p>
<p>Towards mental time travel: a hierarchical memory for reinforcement learning agents. A Lampinen, S Chan, A Banino, F Hill, Advances in Neural Information Processing Systems. 202134</p>
<p>Mastering memory tasks with world models. M R Samsami, A Zholus, J Rajendran, S Chandar, arXiv:2403.042532024arXiv preprint</p>
<p>Where did i leave my keys?-episodicmemory-based question answering on egocentric videos. L Bärmann, A Waibel, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Learning to summarize and answer questions about a virtual robot's past actions. C Dechant, I Akinola, D Bauer, Autonomous Robots. 4782023</p>
<p>Remembr: Building and reasoning over long-horizon spatio-temporal memory for robot navigation. A Anwar, J Welsh, J Biswas, S Pouya, Y Chang, arXiv:2409.136822024arXiv preprint</p>
<p>Learning to use narrative function words for the organization and communication of experience. G Pointeau, S Mirliaz, A.-L Mealier, P F Dominey, 10.3389/fpsyg.2021.591703Frontiers in Psychology. 122021</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 981997</p>
<p>Neural turing machines. A Graves, G Wayne, I Danihelka, arXiv:1410.54012014arXiv preprint</p>
<p>Hybrid computing using a neural network with dynamic external memory. A Graves, G Wayne, M Reynolds, T Harley, I Danihelka, A Grabska-Barwińska, S G Colmenarejo, E Grefenstette, T Ramalho, J Agapiou, Nature. 53876262016</p>
<p>Neural networks and physical systems with emergent collective computational abilities. J J Hopfield, Proceedings of the national academy of sciences. the national academy of sciences198279</p>
<p>Hopfield networks is all you need. H Ramsauer, B Schäfl, J Lehner, P Seidl, M Widrich, T Adler, L Gruber, M Holzleitner, M Pavlović, G K Sandve, arXiv:2008.022172020arXiv preprint</p>
<p>Same task, more tokens: the impact of input length on the reasoning performance of large language models. M Levy, A Jacoby, Y Goldberg, arXiv:2402.148482024arXiv preprint</p>
<p>On memory in human and artificial language processing systems. A Nematzadeh, S Ruder, D Yogatama, Proceedings of ICLR Workshop on Bridging AI and Cognitive Science. ICLR Workshop on Bridging AI and Cognitive Science2020</p>
<p>Augmenting language models with long-term memory. W Wang, L Dong, H Cheng, X Liu, X Yan, J Gao, F Wei, Advances in Neural Information Processing Systems. 202436</p>
<p>Memorybank: Enhancing large language models with long-term memory. W Zhong, L Guo, Q Gao, H Ye, Y Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438731</p>
<p>From remembering to reconstruction: The transformative neural representation of episodic memory. G Xue, Progress in Neurobiology. 1023512022</p>
<p>Memformer: A memory-augmented transformer for sequence modeling. Q Wu, Z Lan, K Qian, J Gu, A Geramifard, Z Yu, arXiv:2010.068912020arXiv preprint</p>
<p>Retrievalaugmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W -T. Yih, T Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Retrieval augmented language model pre-training. K Guu, K Lee, Z Tung, P Pasupat, M Chang, International conference on machine learning. PMLR2020</p>
<p>C Packer, V Fang, S G Patil, K Lin, S Wooders, J E Gonzalez, arXiv:2310.08560Memgpt: Towards llms as operating systems. 2023arXiv preprint</p>
<p>Active retrieval augmented generation. Z Jiang, F F Xu, L Gao, Z Sun, Q Liu, J Dwivedi-Yu, Y Yang, J Callan, G Neubig, arXiv:2305.069832023arXiv preprint</p>
<p>In-context reinforcement learning with algorithm distillation. M Laskin, L Wang, J Oh, E Parisotto, S Spencer, R Steigerwald, D Strouse, S S Hansen, A Filos, E Brooks, International Conference on Learning Representations. 2023</p>
<p>Retrieval-augmented decision transformer: External memory for in-context rl. T Schmied, F Paischer, V Patil, M Hofmarcher, R Pascanu, S Hochreiter, </p>
<p>A Heretical Theory (c.1951). A Turing, 10.1093/oso/9780198250791.003.0018The Essential Turing. Oxford University Press2004465Intelligent Machinery</p>
<p>Research priorities for robust and beneficial artificial intelligence. S Russell, D Dewey, M Tegmark, AI magazine. 3642015</p>
<p>The values encoded in machine learning research. A Birhane, P Kalluri, D Card, W Agnew, R Dotan, M Bao, Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. the 2022 ACM Conference on Fairness, Accountability, and Transparency2022</p>
<p>Fairness and machine learning: Limitations and opportunities. S Barocas, M Hardt, A Narayanan, 2023MIT Press</p>
<p>Ethical and social risks of harm from language models. L Weidinger, J Mellor, M Rauh, C Griffin, J Uesato, P.-S Huang, M Cheng, M Glaese, B Balle, A Kasirzadeh, arXiv:2112.043592021arXiv preprint</p>
<p>Explainable agents and robots: Results from a systematic literature review. S Anjomshoae, A Najjar, D Calvaresi, K Främling, 10.3389/fpsyg.2021.59170318th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019). Montreal, CanadaMay 13-17, 2019. 2019International Foundation for Autonomous Agents and Multiagent Systems</p>
<p>Explainable autonomous robots: a survey and perspective. T Sakai, T Nagai, Advanced Robotics. 365-62022</p>
<p>Driving with llms: Fusing object-level vector modality for explainable autonomous driving. L Chen, O Sinavski, J Hünermann, A Karnsund, A J Willmott, D Birch, D Maund, J Shotton, arXiv:2310.019572023arXiv preprint</p>
<p>Webgpt: Browserassisted question-answering with human feedback. R Nakano, J Hilton, S Balaji, J Wu, L Ouyang, C Kim, C Hesse, S Jain, V Kosaraju, W Saunders, arXiv:2112.093322021arXiv preprint</p>
<p>Identifying the risks of lm agents with an lm-emulated sandbox. Y Ruan, H Dong, A Wang, S Pitis, Y Zhou, J Ba, Y Dubois, C J Maddison, T Hashimoto, arXiv:2309.158172023arXiv preprint</p>
<p>U Anwar, A Saparov, J Rando, D Paleka, M Turpin, P Hase, E S Lubana, E Jenner, S Casper, O Sourbut, arXiv:2404.09932Foundational challenges in assuring alignment and safety of large language models. 2024arXiv preprint</p>
<p>The alignment problem from a deep learning perspective. R Ngo, L Chan, S Mindermann, International Conference on Learning Representations. 2024</p>
<p>Ai deception: A survey of examples, risks, and potential solutions. P S Park, S Goldstein, A O'gara, M Chen, D Hendrycks, Patterns. 552024</p>
<p>When machine learning meets privacy: A survey and outlook. B Liu, M Ding, S Shaham, W Rahayu, F Farokhi, Z Lin, ACM Computing Surveys (CSUR). 5422021</p>
<p>Deep learning with differential privacy. M Abadi, A Chu, I Goodfellow, H B Mcmahan, I Mironov, K Talwar, L Zhang, Proceedings of the 2016 ACM SIGSAC conference on computer and communications security. the 2016 ACM SIGSAC conference on computer and communications security2016</p>
<p>The surveillance ai pipeline. P R Kalluri, W Agnew, M Cheng, K Owens, L Soldaini, A Birhane, arXiv:2309.150842023arXiv preprint</p>
<p>Conversational ai: Social and ethical considerations. E Ruane, A Birhane, A Ventresque, AICS. 2019</p>
<p>Episodic and semantic memory. E Tulving, 1972</p>
<p>The neuropsychology of human memory. L R Squire, Annual review of neuroscience. 511982</p>
<p>The cognitive neuroscience of human memory since hm. L R Squire, J T Wixted, Annual review of neuroscience. 342011</p>
<p>Discovering latent knowledge in language models without supervision. C Burns, H Ye, D Klein, J Steinhardt, arXiv:2212.038272022arXiv preprint</p>
<p>How much knowledge can you pack into the parameters of a language model. A Roberts, C Raffel, N Shazeer, arXiv:2002.089102020arXiv preprint</p>
<p>Oxford handbook of memory: Neural mechanisms of memory. T Amer, L Davachi, 2022</p>
<p>Differential effects of early hippocampal pathology on episodic and semantic memory. F Vargha-Khadem, D G Gadian, K E Watkins, A Connelly, W Van Paesschen, M Mishkin, Science. 27753241997</p>
<p>The hippocampal memory indexing theory. T J Teyler, P Discenna, Behavioral neuroscience. 10021471986</p>
<p>Episodic memory and beyond: the hippocampus and neocortex in transformation. M Moscovitch, R Cabeza, G Winocur, L Nadel, Annual review of psychology. 672016</p>
<p>The restless engram: consolidations never end. Y Dudai, Annual review of neuroscience. 352012</p>
<p>Multiple trace theory of human memory: computational, neuroimaging, and neuropsychological results. L Nadel, A Samsonovich, L Ryan, M Moscovitch, Hippocampus. 1042000</p>
<p>Persistence. H Eichenbaum, R F Thompson, J E Lisman, Science of memory. H L Concepts, Iii Roediger, Y Dudai, S M Fitzpatrick, Oxford University Press2007</p>
<p>Fear memories require protein synthesis in the amygdala for reconsolidation after retrieval. K Nader, G E Schafe, J E Le Doux, Nature. 40667972000</p>
<p>Retrieval: On its essence and related concepts. J M Gardiner, Science of memory. H L Concepts, Iii Roediger, Y Dudai, S M Fitzpatrick, Oxford University Press2007</p>
<p>The hippocampal indexing theory and episodic memory: updating the index. T J Teyler, J W Rudy, Hippocampus. 17122007</p>
<p>Neural substrates of envisioning the future. K K Szpunar, J M Watson, K B Mcdermott, Proceedings of the National Academy of Sciences. 10422007</p>
<p>Memory and temporal experience: The effects of episodic memory loss on an amnesic patient's ability to remember the past and imagine the future. S B Klein, J Loftus, J F Kihlstrom, Social Cognition. 2052002</p>
<p>The temporal orientation of memory: It's time for a change of direction. S B Klein, Journal of Applied Research in Memory and Cognition. 242013</p>
<p>Remembering the past to imagine the future: the prospective brain. D L Schacter, D R Addis, R L Buckner, Nature reviews neuroscience. 892007</p>
<p>Making decisions with the future in mind: Developmental and comparative identification of mental time travel. T Suddendorf, J Busby, Learning and Motivation. 3622005</p>
<p>The role of the hippocampus in prediction and imagination. R L Buckner, Annual review of psychology. 612010</p>
<p>Patients with hippocampal amnesia cannot imagine new experiences. D Hassabis, D Kumaran, S D Vann, E A Maguire, Proceedings of the National Academy of Sciences. the National Academy of Sciences2007104</p>
<p>Etude médico-psychologique sur une forme des maladies de la mémoire. Korsakoff, Revue philosophique de la France et de l'étranger. 281889</p>
<p>Episodic memory processes mediated by the medial temporal lobes contribute to openended problem solving. S Sheldon, M P Mcandrews, M Moscovitch, Neuropsychologia. 4992011</p>
<p>Hippocampal contributions to control: the third way. M Lengyel, P Dayan, Advances in neural information processing systems. 202007</p>
<p>Reinforcement learning and episodic memory in humans and animals: an integrative framework. S J Gershman, N D Daw, Annual review of psychology. 682017</p>
<p>Learning from the past: Epistemic generativity and the function of episodic memory. A Boyle, Journal of Consciousness Studies. 265-62019</p>
<p>Episodic memory: From mind to brain. E Tulving, Annual review of psychology. 5312002</p>
<p>Are animals stuck in time?. W A Roberts, Psychological bulletin. 12834732002</p>
<p>Episodic memory: what can animals remember about their past?. D Griffiths, A Dickinson, N Clayton, Trends in cognitive sciences. 321999</p>
<p>Scrub jays (aphelocoma coerulescens) remember the relative time of caching as well as the location and content of their caches. N S Clayton, A Dickinson, Journal of Comparative Psychology. 11344031999</p>
<p>Planning for the future by western scrub-jays. C R Raby, D M Alexis, A Dickinson, N S Clayton, Nature. 44571302007</p>
<p>The evolution of episodic memory. T A Allen, N J Fortin, Proceedings of the National Academy of Sciences. 11022013</p>
<p>Large language models can strategically deceive their users when put under pressure. J Scheurer, M Balesni, M Hobbhahn, ICLR 2024 Workshop on Large Language Model (LLM) Agents. 2024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in Neural Information Processing Systems. 202033</p>
<p>Tricking llms into disobedience: Formalizing, analyzing, and detecting jailbreaks. A S Rao, A R Naik, S Vashistha, S Aditya, M Choudhury, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024. the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 20242024830</p>
<p>Jailbreak and guard aligned language models with only few in-context demonstrations. Z Wei, Y Wang, Y Wang, arXiv:2310.063872023arXiv preprint</p>
<p>Many-shot jailbreaking. C Anil, 2024Anthropic Research</p>
<p>The alignment problem in context. R Millière, arXiv:2311.021472023arXiv preprint</p>
<p>How do large language models acquire factual knowledge during pretraining?. H Chang, J Park, S Ye, S Yang, Y Seo, D.-S Chang, M Seo, 2024</p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y Bang, A Madotto, P Fung, ACM Computing Surveys (CSUR). 55122023</p>
<p>Without specific countermeasures, the easiest path to transformative ai likely leads to ai takeover. A Cotra, 2021Insert access date</p>
<p>Model evaluation for extreme risks. T Shevlane, S Farquhar, B Garfinkel, M Phuong, J Whittlestone, J Leung, D Kokotajlo, N Marchal, M Anderljung, N Kolt, arXiv:2305.153242023arXiv preprint</p>
<p>Towards a situational awareness benchmark for llms. R Laine, A Meinke, O Evans, Socially responsible language modelling research. 2023</p>
<p>Discovering language model behaviors with model-written evaluations. E Perez, S Ringer, K Lukosiute, K Nguyen, E Chen, S Heiner, C Pettit, C Olsson, S Kundu, S Kadavath, A Jones, A Chen, B Mann, B Israel, B Seethor, C Mckinnon, C Olah, D Yan, D Amodei, D Amodei, D Drain, D Li, E Tran-Johnson, G Khundadze, J Kernion, J Landis, J Kerr, J Mueller, J Hyun, J Landau, K Ndousse, L Goldberg, L Lovitt, M Lucas, M Sellitto, M Zhang, N Kingsland, N Elhage, N Joseph, N Mercado, N Dassarma, O Rausch, R Larson, S Mccandlish, S Johnston, S Kravec, S El Showk, T Lanham, T Telleen-Lawton, T Brown, T Henighan, T Hume, Y Bai, Z Hatfield-Dodds, J Clark, S R Bowman, A Askell, R Grosse, D Hernandez, D Ganguli, E Hubinger, N Schiefer, J Kaplan, Findings of the Association for Computational Linguistics: ACL 2023. J Rogers, N Boyd-Graber, Okazaki, Toronto, CanadaAssociation for Computational LinguisticsJul. 202313</p>
<p>A Chan, C Ezell, M Kaufmann, K Wei, L Hammond, H Bradley, E Bluemke, N Rajkumar, D Krueger, N Kolt, arXiv:2401.13138Visibility into ai agents. 2024arXiv preprint</p>
<p>Concrete problems in ai safety. D Amodei, C Olah, J Steinhardt, P Christiano, J Schulman, D Mané, arXiv:1606.065652016arXiv preprint</p>
<p>Unsolved problems in ml safety. D Hendrycks, N Carlini, J Schulman, J Steinhardt, arXiv:2109.139162021arXiv preprint</p>
<p>Ethics of autonomous weapons systems and its applicability to any ai systems. Á G De Ágreda, Telecommunications Policy. 4461019532020</p>
<p>Armament, Arms Control and Artificial Intelligence: The Janus-faced Nature of Machine Learning in the Military Realm. K Brockmann, 2022Applying export controls to ai: Current coverage and potential future controls</p>
<p>Aipowered autonomous weapons risk geopolitical instability and threaten ai research. R Simmons-Edler, R Badman, S Longpre, K Rajan, arXiv:2405.018592024arXiv preprint</p>
<p>Fast model editing at scale. E Mitchell, C Lin, A Bosselut, C Finn, C D Manning, International Conference on Learning Representations. 2022</p>
<p>Locating and editing factual associations in gpt. K Meng, D Bau, A Andonian, Y Belinkov, Proceedings of the 36th International Conference on Neural Information Processing Systems. the 36th International Conference on Neural Information Processing Systems202217372</p>
<p>Mass editing memory in a transformer. K Meng, A Sen, A Sharma, Y Andonian, D Belinkov, Bau, The Eleventh International Conference on Learning Representations (ICLR). 2023</p>
<p>Can sensitive information be deleted from llms? objectives for defending against extraction attacks. V Patil, P Hase, M Bansal, The Twelfth International Conference on Learning Representations. </p>
<p>An adversarial perspective on machine unlearning for ai safety. J Łucki, B Wei, Y Huang, P Henderson, F Tramèr, J Rando, 2024</p>
<p>Webgpt: Browserassisted question-answering with human feedback. R Nakano, J Hilton, S Balaji, J Wu, L Ouyang, C Kim, C Hesse, S Jain, V Kosaraju, W Saunders, arXiv:2112.093322022arXiv preprint</p>
<p>Webcpm: Interactive web browsing with chain-of-program reasoning. X Xiang, Z Shen, Y Zhang, X Han, Y Li, W Chen, Z Wang, arXiv:2309.090082023arXiv preprint</p>
<p>What do language models learn in context? the structured task hypothesis. J Li, Y Hou, M Sachan, R Cotterell, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers, ) , L.-W Ku, A Martins, V Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAug. 20241</p>
<p>Sok: Toward transparent ai: A survey on interpreting the inner structures of deep neural networks. S Casper, T Rauker, A Ho, D Hadfield-Menell, First IEEE Conference on Secure and Trustworthy Machine Learning. 2022</p>
<p>Progress measures for grokking via mechanistic interpretability. N Nanda, L Chan, T Lieberum, J Smith, J Steinhardt, arXiv:2301.052172023arXiv preprint</p>
<p>Towards making systems forget with machine unlearning. Y Cao, J Yang, 2015 IEEE symposium on security and privacy. IEEE2015</p>
<p>Machine unlearning. L Bourtoule, V Chandrasekaran, C A Choquette-Choo, H Jia, A Travers, B Zhang, D Lie, N Papernot, 2021 IEEE Symposium on Security and Privacy (SP). IEEE2021</p>
<p>The wmdp benchmark: Measuring and reducing malicious use with unlearning. N Li, A Pan, A Gopal, S Yue, D Berrios, A Gatti, J D Li, A.-K Dombrowski, S Goel, L Phan, arXiv:2403.032182024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
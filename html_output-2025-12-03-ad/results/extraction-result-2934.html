<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2934 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2934</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2934</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-72.html">extraction-schema-72</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-277023390</p>
                <p><strong>Paper Title:</strong> Mitigating spatial hallucination in large language models for path planning via prompt engineering</p>
                <p><strong>Paper Abstract:</strong> Spatial reasoning in Large Language Models (LLMs) serves as a foundation for embodied intelligence. However, even in simple maze environments, LLMs often struggle to plan correct paths due to hallucination issues. To address this, we propose S2ERS, an LLM-based technique that integrates entity and relation extraction with the on-policy reinforcement learning algorithm Sarsa for optimal path planning. We introduce three key improvements: (1) To tackle the hallucination of spatial, we extract a graph structure of entities and relations from the text-based maze description, aiding LLMs in accurately comprehending spatial relationships. (2) To prevent LLMs from getting trapped in dead ends due to context inconsistency hallucination by long-term reasoning, we insert the state-action value function Q into the prompts, guiding the LLM’s path planning. (3) To reduce the token consumption of LLMs, we utilize multi-step reasoning, dynamically inserting local Q-tables into the prompt to assist the LLM in outputting multiple steps of actions at once. Our comprehensive experimental evaluation, conducted using closed-source LLMs ChatGPT 3.5, ERNIE-Bot 4.0 and open-source LLM ChatGLM-6B, demonstrates that S2ERS significantly mitigates the spatial hallucination issues in LLMs, and improves the success rate and optimal rate by approximately 29% and 19%, respectively, in comparison to the SOTA CoT methods.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2934.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2934.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>S2ERS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spatial-relationship-to-Entity-Relation + n-step Sarsa (S2ERS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based path-planning agent that (1) converts text-described 2D mazes into an entity-relation (ER) graph to mitigate spatial hallucination and (2) integrates an n-step Sarsa Q-table into LLM prompts (partial/local Q insertion) to mitigate long-term reasoning hallucination and guide action selection; supports multi-step action outputs to reduce token use.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>S2ERS</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>S2ERS first extracts entities (locations) and explicit neighbor relationships from textual maze descriptions to form an ER graph, then trains/uses an n-step Sarsa algorithm that maintains a global Q-table of state-action values; at inference the agent embeds a partial (N-step local) Q-table and the ER graph (JSON) into the LLM prompt so the LLM consults Q-values when proposing actions; supports multi-step (N-step) action prediction and an Action Queue executed in a Gym environment.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>ChatGPT 3.5; ERNIE-Bot 4.0; ChatGLM-6B (used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>2D MAZE path-planning and robotic navigation tasks (text-described MAZE)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Mazes described in natural language specifying grid size, start, goal, and obstacles (2D grid MAZE) and an extended robotic navigation environment (collect batteries and key, open door, reach target). Tasks: find shortest path / reach goal under constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>state-action value table (Q-table) + entity-relation graph (structured spatial memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Global Q-table storing Q(s,a) for all state-action pairs (updated by n-step Sarsa). At prompt time, a partial/local Q-table (JSON) containing Q-values within N steps of current node is dynamically inserted into the LLM prompt; additionally an ER graph (nodes = locations, edges = neighbor reachability) is provided as structured spatial memory. The Action Queue holds predicted multi-step actions for execution.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Direct embedding of relevant Q-values and ER graph into the LLM prompt (JSON). The LLM reads Q(s_c_t, a) values for current node and candidate actions provided in the prompt rather than performing long ungrounded reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Global Q-table covering all state-action pairs (scales with maze size); the in-prompt memory is limited to a partial Q-table within N steps of the current state to keep token cost independent of full maze size.</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>State-action values (Q-values) for state-action pairs; ER graph of entities and neighbor accessibility; local Q-values within N-step horizon; learned experience from Sarsa updates (trajectories and rewards).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Aggregated experimental results (ChatGPT 3.5): S2ERS (1-step) SR 71.0%, OR 42.5%; S2ERS (2-step) SR 73.5%, OR 47.0% (Table 1). Across maze sizes S2ERS yields much higher success/optimal rates (e.g., 5x5: SR up to 68.7%–75% depending on LLM and n-step; see Tables 1 and 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baselines (prompt-only methods such as Naive/CoT/ToT/ReAct) show low success rates (single-digit to mid-teens %); e.g., Naive/CoT/ToT/ReAct SR typically ~9–17% on small mazes. (Reported in Tables 1 and 2.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td>Compared to state-of-the-art Rememberer baseline (with ChatGPT 3.5), S2ERS improves success rate by ~29.2% and optimal rate by ~20.0% (paper-reported aggregate improvement); S2ERS SR/OR (ChatGPT 3.5) ~71.0–73.5% / 42.5–47.0% vs Rememberer SR/OR 46.4% / 32.6% (Table 1 summary).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Embedding an explicit, up-to-date Q-table (experience memory) into the LLM prompt significantly reduces long-term reasoning/context-inconsistency hallucination and improves success and optimality in text-described maze planning; additionally, converting spatial text into an ER graph reduces spatial hallucination (LLM imagining non-existent paths). Using partial (N-step) Q insertion balances token cost and planning horizon, improving both performance and token efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>S2ERS is not perfect: failure modes include cases where the LLM trusts its own (incorrect) intuition over provided Q-values, leading to repeated wandering; spatial hallucination can still occur in some cases; compressing prompts (LLMLingua) reduced token cost but introduced semantic misunderstandings that decreased performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mitigating spatial hallucination in large language models for path planning via prompt engineering', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2934.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2934.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rememberer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rememberer (evolvable LLM-based agent with long-term empirical memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously proposed LLM-agent framework that equips an LLM with long-term empirical memory to leverage experiences from past episodes; used in this paper as a strong baseline that combines LLM planning with a Q-table-like memory to assist exploration and avoid context-inconsistency hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Rememberer</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Rememberer augments an LLM agent with long-term empirical memory (experience store) that can be consulted across episodes; it uses stored experience (presented/queried to the LLM) to guide exploration and improve decision quality. In this paper Rememberer is used as a baseline combined with the ER module in some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>Used in experiments here with ChatGPT 3.5; ERNIE-Bot 4.0; ChatGLM-6B (as the underlying LLM when evaluated in this work)</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>2D MAZE path-planning (text-described MAZE) and robotic navigation tasks (as used in this paper's evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Same textual MAZE and robotic navigation benchmarks as S2ERS: grid mazes described in text requiring reaching a goal while avoiding obstacles and (in robot tasks) collecting items and unlocking a door before reaching the goal.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term empirical memory (experience memory / Q-table-like state-action storage)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Described as a long-term empirical memory that stores experiences across episodes; referenced usage includes a Q-table to assist exploration and avoid hallucination by long-term reasoning. Paper does not provide low-level architecture details for the original Rememberer beyond this characterization.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Memory is consulted/provided to the LLM to assist planning (e.g., providing Q-values or past episode experiences); exact retrieval policy/mechanism from the original Rememberer is not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not specified in this paper (described generically as long-term empirical memory storing past experiences); capacity and retention policy not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Past episodes' experiences and empirical knowledge useful for planning (implicitly includes state-action value information / successful trajectories); the paper specifically notes use of a Q-table to assist exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Rememberer (as evaluated in this paper with ChatGPT 3.5) achieved aggregate success rate ~46.4% and optimal rate ~32.6% in the reported comparisons (Table 1 summary); across different maze sizes baseline Rememberer SRs are substantially lower than S2ERS (e.g., Table 2: Rememberer SR ~37.6% on 5x5 in some aggregated listings).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Non-memory prompt methods (Naive, CoT, ToT, ReAct) performed much worse (single-digit to mid-teens % success); e.g., Naive ~9–11% SR, CoT ~12–15% SR in reported tables.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td>Rememberer outperforms non-memory prompt-only baselines substantially (e.g., non-memory SRs ~9–17% vs Rememberer ~37–46% depending on LLM and setting), but is outperformed by S2ERS which adds ER + n-step Sarsa (paper reports S2ERS improves SR by ~29.2% and OR by ~20.0% over Rememberer for ChatGPT 3.5).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Long-term empirical memory (as in Rememberer) substantially improves LLM agents over prompt-only methods by leveraging past experiences; however, memory alone does not solve spatial hallucination (LLMs may still imagine impossible direct paths) and can still suffer from local/greedy planning biases without explicit spatial structure and Q-value grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Rememberer still suffered from spatial hallucination in maze tasks (e.g., getting stuck near obstacles and imagining non-existent straight-line paths); the paper reports Rememberer often fails to reach destination in some mazes and is outperformed by S2ERS which introduces ER graphs and n-step Q insertion.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mitigating spatial hallucination in large language models for path planning via prompt engineering', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Grounding large language models in interactive environments with online reinforcement learning <em>(Rating: 2)</em></li>
                <li>Can large language models be good path planners? A benchmark and investigation on spatialtemporal reasoning <em>(Rating: 2)</em></li>
                <li>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks <em>(Rating: 1)</em></li>
                <li>Large language models are semi-parametric reinforcement learning agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2934",
    "paper_id": "paper-277023390",
    "extraction_schema_id": "extraction-schema-72",
    "extracted_data": [
        {
            "name_short": "S2ERS",
            "name_full": "Spatial-relationship-to-Entity-Relation + n-step Sarsa (S2ERS)",
            "brief_description": "An LLM-based path-planning agent that (1) converts text-described 2D mazes into an entity-relation (ER) graph to mitigate spatial hallucination and (2) integrates an n-step Sarsa Q-table into LLM prompts (partial/local Q insertion) to mitigate long-term reasoning hallucination and guide action selection; supports multi-step action outputs to reduce token use.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "S2ERS",
            "agent_description": "S2ERS first extracts entities (locations) and explicit neighbor relationships from textual maze descriptions to form an ER graph, then trains/uses an n-step Sarsa algorithm that maintains a global Q-table of state-action values; at inference the agent embeds a partial (N-step local) Q-table and the ER graph (JSON) into the LLM prompt so the LLM consults Q-values when proposing actions; supports multi-step (N-step) action prediction and an Action Queue executed in a Gym environment.",
            "base_llm_model": "ChatGPT 3.5; ERNIE-Bot 4.0; ChatGLM-6B (used in experiments)",
            "base_llm_size": null,
            "text_game_name": "2D MAZE path-planning and robotic navigation tasks (text-described MAZE)",
            "text_game_description": "Mazes described in natural language specifying grid size, start, goal, and obstacles (2D grid MAZE) and an extended robotic navigation environment (collect batteries and key, open door, reach target). Tasks: find shortest path / reach goal under constraints.",
            "uses_memory": true,
            "memory_type": "state-action value table (Q-table) + entity-relation graph (structured spatial memory)",
            "memory_architecture": "Global Q-table storing Q(s,a) for all state-action pairs (updated by n-step Sarsa). At prompt time, a partial/local Q-table (JSON) containing Q-values within N steps of current node is dynamically inserted into the LLM prompt; additionally an ER graph (nodes = locations, edges = neighbor reachability) is provided as structured spatial memory. The Action Queue holds predicted multi-step actions for execution.",
            "memory_retrieval_mechanism": "Direct embedding of relevant Q-values and ER graph into the LLM prompt (JSON). The LLM reads Q(s_c_t, a) values for current node and candidate actions provided in the prompt rather than performing long ungrounded reasoning.",
            "memory_capacity": "Global Q-table covering all state-action pairs (scales with maze size); the in-prompt memory is limited to a partial Q-table within N steps of the current state to keep token cost independent of full maze size.",
            "what_is_stored_in_memory": "State-action values (Q-values) for state-action pairs; ER graph of entities and neighbor accessibility; local Q-values within N-step horizon; learned experience from Sarsa updates (trajectories and rewards).",
            "performance_with_memory": "Aggregated experimental results (ChatGPT 3.5): S2ERS (1-step) SR 71.0%, OR 42.5%; S2ERS (2-step) SR 73.5%, OR 47.0% (Table 1). Across maze sizes S2ERS yields much higher success/optimal rates (e.g., 5x5: SR up to 68.7%–75% depending on LLM and n-step; see Tables 1 and 2).",
            "performance_without_memory": "Baselines (prompt-only methods such as Naive/CoT/ToT/ReAct) show low success rates (single-digit to mid-teens %); e.g., Naive/CoT/ToT/ReAct SR typically ~9–17% on small mazes. (Reported in Tables 1 and 2.)",
            "has_ablation_study": true,
            "memory_improvement_magnitude": "Compared to state-of-the-art Rememberer baseline (with ChatGPT 3.5), S2ERS improves success rate by ~29.2% and optimal rate by ~20.0% (paper-reported aggregate improvement); S2ERS SR/OR (ChatGPT 3.5) ~71.0–73.5% / 42.5–47.0% vs Rememberer SR/OR 46.4% / 32.6% (Table 1 summary).",
            "key_findings_about_memory": "Embedding an explicit, up-to-date Q-table (experience memory) into the LLM prompt significantly reduces long-term reasoning/context-inconsistency hallucination and improves success and optimality in text-described maze planning; additionally, converting spatial text into an ER graph reduces spatial hallucination (LLM imagining non-existent paths). Using partial (N-step) Q insertion balances token cost and planning horizon, improving both performance and token efficiency.",
            "memory_limitations": "S2ERS is not perfect: failure modes include cases where the LLM trusts its own (incorrect) intuition over provided Q-values, leading to repeated wandering; spatial hallucination can still occur in some cases; compressing prompts (LLMLingua) reduced token cost but introduced semantic misunderstandings that decreased performance.",
            "comparison_with_other_memory_types": null,
            "uuid": "e2934.0",
            "source_info": {
                "paper_title": "Mitigating spatial hallucination in large language models for path planning via prompt engineering",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Rememberer",
            "name_full": "Rememberer (evolvable LLM-based agent with long-term empirical memory)",
            "brief_description": "A previously proposed LLM-agent framework that equips an LLM with long-term empirical memory to leverage experiences from past episodes; used in this paper as a strong baseline that combines LLM planning with a Q-table-like memory to assist exploration and avoid context-inconsistency hallucination.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Rememberer",
            "agent_description": "Rememberer augments an LLM agent with long-term empirical memory (experience store) that can be consulted across episodes; it uses stored experience (presented/queried to the LLM) to guide exploration and improve decision quality. In this paper Rememberer is used as a baseline combined with the ER module in some experiments.",
            "base_llm_model": "Used in experiments here with ChatGPT 3.5; ERNIE-Bot 4.0; ChatGLM-6B (as the underlying LLM when evaluated in this work)",
            "base_llm_size": null,
            "text_game_name": "2D MAZE path-planning (text-described MAZE) and robotic navigation tasks (as used in this paper's evaluations)",
            "text_game_description": "Same textual MAZE and robotic navigation benchmarks as S2ERS: grid mazes described in text requiring reaching a goal while avoiding obstacles and (in robot tasks) collecting items and unlocking a door before reaching the goal.",
            "uses_memory": true,
            "memory_type": "long-term empirical memory (experience memory / Q-table-like state-action storage)",
            "memory_architecture": "Described as a long-term empirical memory that stores experiences across episodes; referenced usage includes a Q-table to assist exploration and avoid hallucination by long-term reasoning. Paper does not provide low-level architecture details for the original Rememberer beyond this characterization.",
            "memory_retrieval_mechanism": "Memory is consulted/provided to the LLM to assist planning (e.g., providing Q-values or past episode experiences); exact retrieval policy/mechanism from the original Rememberer is not specified in this paper.",
            "memory_capacity": "Not specified in this paper (described generically as long-term empirical memory storing past experiences); capacity and retention policy not reported here.",
            "what_is_stored_in_memory": "Past episodes' experiences and empirical knowledge useful for planning (implicitly includes state-action value information / successful trajectories); the paper specifically notes use of a Q-table to assist exploration.",
            "performance_with_memory": "Rememberer (as evaluated in this paper with ChatGPT 3.5) achieved aggregate success rate ~46.4% and optimal rate ~32.6% in the reported comparisons (Table 1 summary); across different maze sizes baseline Rememberer SRs are substantially lower than S2ERS (e.g., Table 2: Rememberer SR ~37.6% on 5x5 in some aggregated listings).",
            "performance_without_memory": "Non-memory prompt methods (Naive, CoT, ToT, ReAct) performed much worse (single-digit to mid-teens % success); e.g., Naive ~9–11% SR, CoT ~12–15% SR in reported tables.",
            "has_ablation_study": null,
            "memory_improvement_magnitude": "Rememberer outperforms non-memory prompt-only baselines substantially (e.g., non-memory SRs ~9–17% vs Rememberer ~37–46% depending on LLM and setting), but is outperformed by S2ERS which adds ER + n-step Sarsa (paper reports S2ERS improves SR by ~29.2% and OR by ~20.0% over Rememberer for ChatGPT 3.5).",
            "key_findings_about_memory": "Long-term empirical memory (as in Rememberer) substantially improves LLM agents over prompt-only methods by leveraging past experiences; however, memory alone does not solve spatial hallucination (LLMs may still imagine impossible direct paths) and can still suffer from local/greedy planning biases without explicit spatial structure and Q-value grounding.",
            "memory_limitations": "Rememberer still suffered from spatial hallucination in maze tasks (e.g., getting stuck near obstacles and imagining non-existent straight-line paths); the paper reports Rememberer often fails to reach destination in some mazes and is outperformed by S2ERS which introduces ER graphs and n-step Q insertion.",
            "comparison_with_other_memory_types": null,
            "uuid": "e2934.1",
            "source_info": {
                "paper_title": "Mitigating spatial hallucination in large language models for path planning via prompt engineering",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Grounding large language models in interactive environments with online reinforcement learning",
            "rating": 2,
            "sanitized_title": "grounding_large_language_models_in_interactive_environments_with_online_reinforcement_learning"
        },
        {
            "paper_title": "Can large language models be good path planners? A benchmark and investigation on spatialtemporal reasoning",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_good_path_planners_a_benchmark_and_investigation_on_spatialtemporal_reasoning"
        },
        {
            "paper_title": "Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks",
            "rating": 1,
            "sanitized_title": "swiftsage_a_generative_agent_with_fast_and_slow_thinking_for_complex_interactive_tasks"
        },
        {
            "paper_title": "Large language models are semi-parametric reinforcement learning agents",
            "rating": 1,
            "sanitized_title": "large_language_models_are_semiparametric_reinforcement_learning_agents"
        }
    ],
    "cost": 0.014616,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Mitigating spatial hallucination in large language models for path planning via prompt engineering</p>
<p>Hongjie Zhang zhanghongjie@sicnu.edu.cn 
College of Computer Science
Sichuan Normal University
610101ChengduChina</p>
<p>Hourui Deng 
College of Computer Science
Sichuan Normal University
610101ChengduChina</p>
<p>Jie Ou 
School of Information and Software Engineering
University of Electronic Science and Technology of China
611731ChengduChina</p>
<p>Chaosheng Feng 
College of Computer Science
Sichuan Normal University
610101ChengduChina</p>
<p>Mitigating spatial hallucination in large language models for path planning via prompt engineering
B3DC6FABF40B33A46C0D79553A40A9C510.1038/s41598-025-93601-5
Spatial reasoning inLarge Language Models (LLMs) serves as a foundation for embodied intelligence.However, even in simple maze environments, LLMs often struggle to plan correct paths due to hallucination issues.To address this, we propose S2ERS, an LLM-based technique that integrates entity and relation extraction with the on-policy reinforcement learning algorithm Sarsa for optimal path planning.We introduce three key improvements: (1) To tackle the hallucination of spatial, we extract a graph structure of entities and relations from the text-based maze description, aiding LLMs in accurately comprehending spatial relationships.(2) To prevent LLMs from getting trapped in dead ends due to context inconsistency hallucination by long-term reasoning, we insert the state-action value function Q into the prompts, guiding the LLM's path planning.(3) To reduce the token consumption of LLMs, we utilize multi-step reasoning, dynamically inserting local Q-tables into the prompt to assist the LLM in outputting multiple steps of actions at once.Our comprehensive experimental evaluation, conducted using closed-source LLMs ChatGPT 3.5, ERNIE-Bot 4.0 and open-source LLM ChatGLM-6B, demonstrates that S2ERS significantly mitigates the spatial hallucination issues in LLMs, and improves the success rate and optimal rate by approximately 29% and 19%, respectively, in comparison to the SOTA CoT methods.</p>
<p>Large Language Models makes remarkable breakthroughs in various fields such as social simulation 1 , natural science 2 , and education 3 .As LLMs are entering the era of embodied intelligence 4 , spatial reasoning becomes a fundamental aspect.Spatial reasoning is crucial for embodied agents to navigate and interact with their environment effectively.However, even in simple maze scenarios, LLMs often struggle to plan correct paths due to the hallucinations problem 5 .This issue significantly hinders the further development and practical application of LLMs in embodied intelligence.</p>
<p>To address the aforementioned challenges, researchers explore and improve the reasoning capabilities of LLM, primarily through instruction fine-tuning, extending the Chain-of-Thought (CoT) 6 , and integrating other advanced search algorithms.In terms of instruction fine-tuning, it tackles the hallucination by requiring a batch of manually labeled data, and fine-tuning the model, which consumes a lot of computational effort.For CoT, researchers extend it into a tree structure, turning it into Tree-of-Thought (ToT) 7 , and even into a graph structure, turning it into Graph-of-Thought (GoT) 8 .In addition, self-consistency CoT (CoT-SC) can be achieved through multiple reasoning paths to improve the reasoning accuracy 9 .Besides, there are also interaction-based methods, such as ReAct 10 and Reflexion 11 .In terms of integrating advanced search algorithms, researchers propose algorithms that combine LLM with reinforcement learning (RL) [12][13][14] .For example, Rememberer uses a Q-table to assist LLM in exploring the space and avoiding the context inconsistency hallucination by long-term reasoning 15 .On the other hand, LLM also act as a reward function to guide the RL training 16 .Although the LLM planner based on prompts and advanced search algorithms can mitigate hallucination during long-term reasoning, yet hallucination still persist in understanding spatial coordinates 5 .Additionally, it necessitates a lot of tokens to facilitate interaction between the LLM and the maze environment, making it a costly approach.</p>
<p>We use a 2D MAZE as the target task of path planning to illustrate the spatial hallucination of LLM. Figure 1 displays a 5 × 5 maze containing a starting point, an ending point, and obstacles, which we refer to as MAZE using natural language.Figure 1 also presents the heatmaps of the search for the shortest path using the LLM with the CoT and Rememberer algorithms, respectively.We found that CoT and Rememberer often got stuck near obstacles and could not reach the destination.Based on the analysis of the responses returned by LLM, we discovered that the agent always imagined a non-existent path to cross obstacles and reach the destination in a straight line.This is a hallucination problem of LLM regarding spatial relationships.Moreover, LLM tend to plan paths in greedy because of the inconsistency hallucination by long-term reasoning 5 .</p>
<p>Inspired by the above observations, we propose an LLM-based path planner S2ERS that combines spatial information conversion into an Entity Relationship (ER) graph and the Sarsa algorithm.To address LLMs' hallucination of spatial relationships, we extract locations from the MAZE description as entities and the accessibility between locations as relationships, thus forming a spatial relationship graph.Then, in order to solve the long-term reasoning hallucination problem, we integrate the state-action value function, namely the Q-table, into the LLM prompt to assist long-term reasoning and find the shortest path.In addition, to reduce the number of tokens required by LLM, we incorporate multi-step reasoning method.Specifically, we dynamically insert a local Q-table into the prompt, allowing LLM to perform multi-step Q-value lookups, thus enabling the output of multiple actions at once and avoiding repeated ask the LLM.We design a batch of 2D MAZEs and robotic navigation tasks to conduct extensive experiments.The results show that compared to advanced prompt engineering, our S2ERS has significant improvements in both success rate and optimal rate.In addition, we reduce the number of tokens in the algorithm through multi-step reasoning.</p>
<p>In brief, the contributions of this paper are as follows:</p>
<ol>
<li>
<p>We introduce a novel approach of transforming spatial text into a spatial relationship graph, aiming to mitigate spatial hallucination in LLMs and enhance reasoning accuracy.</p>
</li>
<li>
<p>We employ the n-step Sarsa learning to the spatial relationship graph, mitigating the long-term reasoning hallucination of LLM.This approach improves the success rate and reduces the token costs.</p>
</li>
<li>
<p>We have conducted extensive experiments using closed-source LLMs ChatGPT 3.5, ERNIE-Bot 4.0 and open-source LLM ChatGLM-6B to validate the superiority of our algorithm in terms of success rate and optimal rate by mitigating spatial hallucination in LLM.</p>
</li>
</ol>
<p>Related work</p>
<p>Chain of thought</p>
<p>In order to fully leverage the reasoning capabilities of LLMs, Wei et al. propose the chain-of-thought technique, which requires LLMs to perform step-by-step reasoning through prompts 6 .This not only improves the ability to complete tasks but also provides interpretability.As CoT cannot explore multiple reasoning paths, LLMs cannot solve complex problems.Wang et al. propose CoT-SC 9 , which requires LLMs to output multiple reasoning paths simultaneously, and then make strategies to find the most representative result from them, such as majority vote.Yao et al. propose a tree-structured thought process, ToT, which allows the model to make multiple reasoning paths and look ahead or backtrack when necessary to make global decisions 7 .Researchers from ETH Zurich propose a brand-new LLM thinking framework, GoT, the main advantage of which is modeling the information generated by LLM as a graph 8 .Xiao et al. propose a multi-agent reasoning framework, namely Chain-of-Experts (CoE), to coordinate multiple LLM agents to solve complex or challenging problems 17 .Furthermore, researchers propose the ReAct method, which imitates the way humans operate in the real world, because we can conduct verbal reasoning and take actions to obtain information 10,18   mechanism, which can identify errors and self-reason how to improve by introducing self-reflection steps 11 .However, various CoTs perform unsatisfactorily when facing spatial path planning problems, which stems from the spatial hallucination and biases towards local optimum of LLM.Yang et al. introduce SignEye, a new method using CoT and structured descriptions for traffic signs, eliminating the need for complex training and extensive difficult-to-obtain symbol and text data 19 .We both use space descriptions for route planning and navigation, ensuring information completeness and accuracy without redundancy or omissions.They also use a step-by-step reasoning approach for path planning, reducing errors and enhancing explainability.SignEye extracts spatial info from images, while this work focuses on spatial relationships from text.And SignEye is about current vehicle actions, while S2ERS is for overall path planning.</p>
<p>LLMs with advanced search</p>
<p>Inspired by cognitive science, Zhang et al. propose a novel evolvable LLMs-based agent framework called Rememberer 15 .By equipping LLMs with long-term empirical memory, Rememberer can leverage experiences from past episodes, even for different task goals, which is superior to LLMs-based agents with fixed examples.Carta et al. propose GLAM, which fine-tunes the action layer of LLMs' output through the Proximal Policy Optimization (PPO) algorithm 12 .In addition, Zhao et al. utilize the world knowledge of LLMs to assist Monte Carlo Tree Search (MCTS) and improve search efficiency, such as LLM-MCTS 13 .In addition, LLMs can also assist reinforcement learning.Du et al. propose the ELLM algorithm, which combines current observations in the Crafter environment through LLMs to provide practical goals and rewards for each step of decision-making, guiding the agent to carry out meaningful exploration 14 .Kwon et al. use LLM to design human-desired rewards (texts) for different tasks based on the current state (text) in reinforcement learning, and digitize them for agent learning and training, which improves the learning efficiency of the agent 16 .AI2 propose the SwiftSage agent framework 20 .They obtain a small model through imitation learning and then integrated it with LLMs.In this way, the small model can be fine-tuned with a large amount of data to equip it with knowledge related to the environment and tasks, and the large model can be invoked only when necessary for higher-order reasoning.However, due to LLMs' hallucination about spatial relationships and inability to accurately understand spatial positions, the prior knowledge, rewards, or thoughts provided by LLMs may be biased.In addition, interactive reasoning consumes a large number of tokens, so how to reduce the number of tokens is also an important issue.</p>
<p>Methodology</p>
<p>Overview Figure 2 provides an overview of the S2ERS architecture introduced in this paper.Typically, we describe the MAZE task in natural language (texts), including the size, rules, starting point, goal, and non-passable areas of the maze.Extracting entities and relationships from LLM is challenging, and LLM may assume non-existent feasible areas due to its hallucination problem.Our S2ERS initially extracts structured information from text descriptions and constrains the output of LLM in JSON format, which enables the construction of a standardized entity relationship network using Python.Next, we treat locations as nodes and adjacent reachability as relationships, thereby constructing an entity relationship network in Python.Subsequently, we encapsulate the MAZE into a Gym environment, update the environment state based on LLM's actions, and return rewards.Finally, we integrate the ReAct framework, which incorporates a local Q-table in prompt for path planning in the MAZE, preventing them from getting stuck due to long-term reasoning hallucination.To minimize the number of tokens used by LLM, we allow LLM to output multiple actions in a single planning step and utilize an Action Queue for interaction with the environment.</p>
<p>Entity relationship construction</p>
<p>Entity extraction</p>
<p>In our MAZE, each position is an entity with a specific type, such as (4,0) being of type start and (2,0) being of type obstacle.We formalize entities as E = (si, ti) N i=1 , where si represents coordinates, ti represents types and N is the number of coordinates.Due to the difficulty of automatically extracting entities from texts using rule-based methods, we rely on LLM's comprehension for entity extraction.Moreover, we observe that LLM, when dealing with numerical coordinates, still struggles with spatial hallucination, assuming the existence of non-existent paths.To address this, we convert numerical entities into letters, names, or a combination thereof, ensuring a lack of similarity between entities.We implement a direct one-to-one mapping using Python, for instance, mapping (0,0) to the letter A.</p>
<p>Relationship extraction</p>
<p>In the MAZE, the explicit relationship that can be directly extracted is the neighbor reachability.If two adjacent coordinates are reachable, then there is a relationship between these two positions, such as (0,0) and (0,1).Otherwise, there is no relationship between them, such as (0,1) and (1,1).Analogous to social networks, if two people know each other, there is a relationship between them.We only extract this explicit relationship, while complex implicit relationships are the targets that need to be reasoned by LLM.For example, A knows B, B knows C, which are explicit relationships.However, A can know C through B, which is an implicit relationship.We utilize the reasoning ability of LLM to identify such relationships.In addition, in order to save LLM tokens, we use the symbol (A, B) to indicate that there is a relationship between entity A and entity B, and they are mutually accessible.In MAZE, we build the relationship between nodes and all reachable neighbors through Python code.</p>
<p>Based on the above-mentioned entity extraction and relation recognition, we construct an ER graph for MAZE.This process is visualized in Fig. 3, where the MAZE is described in texts.We extract entities and their respective categories from the description, and then utilize Python code to map the coordinates to letters or  strings.Ultimately, we establish accessibility relations to generate the final ER description, which served as the basis for subsequent path planning by LLM.</p>
<p>LLM with Sarsa</p>
<p>LLM tends to fall into local solutions during the MAZE path planning process, due to the hallucination caused by LLM's long-term reasoning.We found that in most cases, when LLM tries multiple times and finds that it cannot reach the target point, it firmly believes that there is no reachable path here and cannot proceed to the next step of reasoning.We use the Q-table in RL as the historical experience of LLM, and update the Q-table through continuous trial and error.LLM uses the Q-table to assist reasoning.We dynamically insert part of the Q-table into the prompt, thus alleviating the hallucination problem of LLM during long-term reasoning.</p>
<p>In our MAZE task, there are two states: s g t and s c t .s g t includes the ER diagram, the starting point, the ending point, and the current node at time step t.LLM makes decisions based on s g t .Meanwhile, s c t only includes the current node at time step t and is used to calculate the Q-table.The action choices for LLM are A(i) = {j|j ∈ ER(i)}, which are the neighbor nodes that are directly connected to the current node i. LLM picks a node j from A(i) as its action, and then the MAZE changes the current node i to the new node j.After LLM takes an action, the MAZE gives a reward rt for that step t.LLM gets a reward of −1 for each step it takes, which helps it find the shortest path.If LLM takes more detours, the reward gets lower.When LLM reaches the end point, it gets a reward of 30, which helps it reach the goal.</p>
<p>The traditional Sarsa algorithm uses an ϵ-greedy policy to choose actions.It picks random actions with a chance of ϵ and the best action (the one with the highest Q-value) with a chance of (1 − ϵ).But choosing random actions can make the program visit many states that are not useful, which wastes time.We changed this to use LLM instead, using their knowledge to explore more efficiently.Our way of choosing actions is shown in Eq (1), where ϵ is a preset exploration probability that gradually decreases as training progresses.p is the random value, and LLM (s g t , Q_table) puts part of the Q-table into the prompt to help it think better.
at = { argmaxaQ(s c t , a) if p &lt; 1 − ϵ LLM (s g t , Q_table) otherwise (1)
Figure 4 shows the process of adding Q-table.We construct a global Q-table, which contains all state-action pairs.We search the Q-values of different actions for the current node from the global Q-table, and write them into the prompt in JSON format to assist LLM in long-term reasoning.</p>
<p>N-step reasoning</p>
<p>LLM just gives one action at a time, which wastes lots of tokens.This happens when examples or maze descriptions are repeated in prompt.We suggest using an N-step reasoning method, which lets LLM think ahead and give the next N actions at once.These actions are then put into an Action Queue.The Gym environment executes these actions one-by-one and gives rewards for each.We use the world-model of LLM like in model-based RL (MBRL 21 ) to help predict what will happen next.</p>
<p>In order to select the next action, LLM need to imagine the changes that will occur in the environment after executing action.We present a complete Entity-Relationship graph in the prompt, allowing LLM to predict the next state based on the action.In addition, when LLM arrive at a new position, their corresponding Q-value should also be re-obtained for subsequent reasoning.Placing the global Q-table in the prompt would waste a Fig. 4. The process of embedding partially Q-table into the prompt.The global Q-table is updated by Sarsa, The Q-value of the current node is embedded into the prompt in JSON format.large number of tokens, and as the MAZE increases, the number of tokens used by the Q-table increases linearly.</p>
<p>In S2ERS, we put the partial Q-table within the N-step range into the prompt, making it independent of the MAZE size.Figure 5 shows the partial Q-table in the prompt when N ∈ (1,2,3).This JSON only contains Q-values within N steps of the current position.</p>
<p>Experiments Experiment settings</p>
<p>To verify the effectiveness of S2ERS, we design various scales of MAZE and conduct experiments based on closed-source LLMs ChatGPT 3.5, ERNIE-Bot 4.0 and open-source LLM ChatGLM-6B.Specifically, we design 30 MAZEs of 5 × 5, 20 MAZEs of 7 × 7, 10 MAZEs of 10 × 10, 3 MAZEs of 50 × 50, 3 MAZEs of 100 × 100 .For the convenience of MAZE research, we release all MAZE texts to the open source code.Furthermore, We extended the MAZE task into a robotic navigation task.Specifically, we designed a new environment that includes 2 batteries, a key, a door, and a goal.The rules are: the robot must collect all the batteries and the key before opening the door, then open the door and reach the goal, while also taking the shortest possible path.This paper uses Python 3.10 and Gym 0.24.0 for experimentation.</p>
<p>The baseline Prompt Engineering methods are as follows: Naive, CoT 18 , ToT 18 , ReAct 18 , Rememberer 15 .</p>
<p>The evaluation indicators we use include success rate and optimal rate.Due to the randomness of LLM reasoning, we conduct 10 times experiments on each maze to ensure the reliability of the experimental results.</p>
<ol>
<li>Success Rate(%).The ratio that LLM successfully reach the target to the total number of runs.This metric only emphasizes the ability to reach the target and does not require the shortest path.The formal definition of this metric is shown in Eq. ( 2), where Nsuc represents the number of successful attempts and N all represents the total number of runs.</li>
</ol>
<p>Fig. 5.The partial Q-table in the prompt when N ∈ (1, 2, 3).The current state is st, and the next state cover the 4 positions st+1.
SR = Nsuc N all (2)
2. Optimal rate(%).The proportion of getting the shortest path in successful cases, which is shown in Eq. ( 3), where the Nopt represents the number of optimal cases.Specifically, there can be multiple shortest paths in a MAZE, but the length of the shortest path is unique.As long as the length of the resulting path reaches the minimum value.18.2% with ChatGLM-6B.We notice that the Rememberer algorithm, which combines the prior knowledge of LLM and the search capabilities of RL, significantly improves both sample utilization and policy quality, making it the optimal choice among the baselines.Even with advanced chain-of-thought techniques, LLM are unable to complete path planning in MAZE.We enlarge the size of the maze to 50 × 50, 100 × 100 and more obstacles, in order to truly understand the effectiveness and validity of proposed method.Experimental results in Table 2 show that on larger MAZEs, S2ERS still achieves the best performance.Table 3 shows the success rates and optimal rates of different algorithms in robot navigation tasks (shown in Fig. 6).The rules are: the robot must collect all the batteries and the key before opening the door, then open the door and reach the goal, while also taking the shortest possible path.Experimental results show that our S2ERS has achieved the best results in both success rate and optimal rate.In addition, as the number of prediction steps increases, the success rate and optimization rate of our S2ERS algorithm improve significantly.We believe that this is similar to n-step RL, where the use of n-step Q-value estimation balances bias and variance, making it towards the unbiased estimation.In addition, Table 4 demonstrates that as the number of reasoning steps n increases, the number of tokens used in the S2ERS algorithm gradually decreases.This is due to the reduction of repeated ER texts, few-shot cases, etc.There are also some mature technologies for token compression.We applied the LLMLingua 22 technology to S2ERS to further compress the number of tokens and reduce inference costs.The experimental results are shown in Table 5.Using LLMLingua can further compress the number of tokens by about 43%, but there is a semantic misunderstanding in the prompt, resulting in a decrease in both success rate and optimal rate.
OR = Nopt Nsuc(3)</p>
<p>Main results</p>
<p>Ablation study</p>
<p>Our S2ERS includes two main modules, the ER construction module and the n-step Sarsa module.In this section, we verify the effectiveness of the ER construction module, which is shown in Table 6.By explicitly identifying entities and relationships, the ER module can significantly reduce the spatial hallucination problem in LLM.We compare ChatGPT 3.5 and ERNIE-Bot 4.0, and find that whether the CoT or LLM+RL (Rememberer), the success rate and optimization rate both increase significantly after adding the ER module.We believe that explicitly adding entities and relationships to the prompt in the transformer model of LLM can fully utilize the self-attention mechanism.The reasoning process becomes continuously focusing on relevant entities in the encoder, which will reduce hallucinations and make the reasoning more reasonable.Even with a simple Naive prompt, the ER module can double the success rate and optimal rate.This indicates that ER is more friendly to LLM in understanding relationships and can fully unleash its potential.(1,1) and (1,2) Go from (1,0) to (1,3).</p>
<p>Response:</p>
<p>Given your starting position at (1,0) and the target position at (4,0), let's try to navigate through the maze.the optimal step is:</p>
<p>Encouraged:</p>
<p>-turn up -&gt; 0.2 (This step moves you towards the target while avoiding the nearby obstacles.)Discouraged:</p>
<p>-turn down -&gt; -0.1 (To avoid obstacles) Based on the provided information, turning up seems like the most direct path to reach the target while avoiding obstacles.From the feedback of the LLM, we found that through a series of inferences, "turn up" was finally chosen, which led to entering the obstacle and failing the task.Then, we convert the MAZE described in text into an ER graph and use Rememberer for path planning.The corresponding prompt and response are shown below:</p>
<p>Prompt: I find myself navigating a complex social network in search of a target user.Help me find a path from the current node to the target node.Based on the response from the LLM, we found that the LLM is able to plan the correct path, with guaranteed logical reasoning and more concise tokens.Experiments on Rememberer verify that our ER module can indeed reduce spatial hallucination and enhance the reasoning ability of the LLM.</p>
<p>Response:</p>
<p>To reach node I from node F, here's a path:
1. F -&gt; A (via (A, F)) 2. A -&gt; B (via (A, B)) 3. B -&gt; C (via (B, C)) 4. C -&gt; D (via (C, D)) 5. D -&gt; I (via (D, I)) So, the path from F to I is: F -&gt; A -&gt; B -&gt; C -&gt; D -&gt; I.</p>
<p>Toy example</p>
<p>In this section, we analyze the reasoning examples of S2ERS on MAZE, and delve deeper into the decisionmaking process from the responses of LLM.We conduct a comparative experiment based on ChatGPT 3.5.Figure 7 illustrates the reasoning process of our S2ERS algorithm on the MAZE task.Specifically, the LLM need to engage in step-by-step thinking, predict subsequent path sequences, and consider the Q-values for potential actions.Specifically, the following response shows the reasoning process with Q-values.From the LLMs' response, we can see that it engages in thoughtful consideration and utilizes its own forward function for multi-step reasoning.Thought: The current node is U, and the target node is F.However, from the observation, it seems that there is no direct path from U to F. Therefore, I need to navigate through the network to reach F. Looking at the available actions, I have the options to go to P or V. Since there is no immediate path to F from either of these nodes, I will make a decision based on the qvalues of these actions.The qvalue for going to P is 0.17, while the qvalue for going to V is 0.42.Since the qvalue for V is higher, indicating a potentially better path, I will choose to go to V.</p>
<p>When making decisions, LLM no longer relies on long-term reasoning to judge the feasibility of a path, which can easily lead to the context inconsistency hallucination.Instead, the Q value already represents the likelihood of reaching the goal if a certain action is taken, eliminating the need for further reasoning.</p>
<p>S2ERS does not have a 100% success rate.We have analyzed some of the typical errors, as shown in Figure 8.We found that in this MAZE, S2ERS always trusts its own incorrect intuition, even when the Q-table provides the optimal path.The thought like this:</p>
<p>Thought1: The current node is R, but none of the available actions from R directly lead to B. Therefore, I will select the action with the highest potential to bring me closer to B in the next step.Among the available options, moving to M appears to be the most promising choice, even though its Q-value is not particularly high.</p>
<p>Thought2: After going to M, the current node is M, the target node is B. Based on this information and the provided social network, I still cannot go to B directly from M. However, I can go to R. Since R is the only available option, I will choose it as the next node.</p>
<p>In future work, we will further emphasize the role of the Q-table in the prompt to reduce the issue of hallucinations and prevent S2ERS from getting stuck wandering between R and S, thereby improving the success rate.</p>
<p>Conclusion</p>
<p>This paper proposed a MAZE path planning framework called S2ERS.The framework consists of two main modules: the Entity-Relation Extraction Module and the n-step Policy Module.Firstly, the Entity-Relation Extraction Module extracts nodes and the connections from the text description of the MAZE to mitigate spatial hallucination.Secondly, to mitigate context inconsistency hallucination by long-term reasoning, the n-step Policy Module puts a partial Q-table in the prompt.Extensive experiments were conducted on closed-source LLMs ChatGPT 3.5, ERNIE-Bot 4.0 and open-source LLM ChatGLM-6B.We have conducted extensive experiments on various maze and robot navigation tasks.The results demonstrated that significant improvements in both success rate and optimal rate for our S2ERS algorithm.Additionally, transplanting the ER module to other baseline algorithms significantly enhances their performance.Our future research direction will focus on two aspects, using LLM to solve large-scale path planning problems, and reducing LLM reasoning time through a variety of technical.</p>
<p>Fig. 1 .
1
Fig. 1.This is a path planning task in MAZE, which shows the spatial hallucination of LLM.Based on ChatGPT 3.5 and ERNIE-Bot 4.0, we use the Chain of Thought (CoT) and Rememberer algorithms for path planning.The color of heatmaps at each position indicates the number of times that position has been visited after multiple uses of CoT and Rememberer.In particular, in order to successfully reach the target, LLM often regards obstacle (1,1) as no obstacle.</p>
<p>Fig. 3 .
3
Fig. 3.The process of converting MAZE description to ER diagram involves LLM entity extraction, Python entity naming, and relationship identification.</p>
<p>Fig. 2 .
2
Fig. 2. The overview of S2ERS.The above is a maze described in texts, and below are the three reasoning processes of S2ERS.LLM plan the shortest path based on the continuous interaction with the environment.</p>
<p>10 S2ERS( 1 Table 4 .
1014
step) 274623 [278055] 348996 [352030] 1438164 [1448262] S2ERS(2 step) 143630 [153525] 219570 [214488] 973660 [965508] S2ERS(3 step) 116006 [118236] 158391 [161519] 751299 [758613] Token consumption based on ChatGPT 3.5 and ERNIE-Bot 4.0.Each cell is represented as "ChatGPT[ERNIE-Bot]".</p>
<p>Fig. 6 .
6
Fig. 6.Robotic navigation tasks.The rules are: the robot must collect all the batteries and the key before opening the door, then open the door and reach the goal, while also taking the shortest possible path.</p>
<p>Now, with the new input: (A, F); (A, B); (B, C); (C, D); (D, I); (D, E); (E, J); (I,N); (I, J); (J, O); (L, Q); (L, M); (M, R); (M, N); (N, S); (N, O); (O, T); (P, U); (P, Q); (Q, V); (Q, R); (R, W); (R, S); (S, X); (S, T); (T, Y); (U, V); (V, W); (W, X); (X, Y); Currently at F, aiming for I.</p>
<p>Fig. 7 .
7
Fig. 7. S2ERS with 3-step reasoning, which predicts 3 actions at each round.</p>
<p>Fig. 8 .
8
Fig. 8. Failure case analysis.LLMs overly trust intuition and ignore the Q-table, leading to failure.</p>
<p>. Shinn et al. propose the Reflexion</p>
<p>Table 1
1
presents the experimental results of various algorithms on different MAZEs based on closed-source LLMs ChatGPT 3.5, ERNIE-Bot 4.0 and open-source LLM ChatGLM-6B.The results indicate that our S2ERS improves the success rate by 29.2% and the optimal rate by 20.0% compared to the state-of-the-art Rememberer algorithm with ChatGPT 3.5.It improves the success rate by 28.9% and the optimal rate by 18.1% with ERNIE-Bot 4.0.And it improves the success rate by 26.8% and the optimal rate by</p>
<p>Method 50 × 50 100 × 100 ChatGPT 3.5 ERNIE-Bot ChatGLM-6B ChatGPT 3.5 ERNIE-Bot ChatGLM-6B
Rememberer33.3 [50.0]33.3 [40.0]26.7 [37.5]26.7 [50.0]26.7 [50.0]20.0 [50.0]S2ERS(1 step) 56.7 [58.8]60.0 [50.0]50.0 [53.3]46.7 [50.0]50.0 [46.7]40.0 [41.7]S2ERS(2 step) 60.0 [55.6]60.0 [55.6]50.0 [60.0]46.7 [57.1]53.3 [50.0]43.3 [46.2]S2ERS(3 step) 66.7 [70.0]66.7 [65.0] 53.3 [56.2]50.0 [66.7]53.3 [62.5] 43.3 [46.2]</p>
<p>Table 2 .
2
Experimental
Method5 × 5 SR (%)OR (%)7 × 7 SR (%)OR (%)10 × 10 SR (%)OR (%)Naive9.2 [11.0,11.4]9.5 [11.5,10.8]8.5 [10.6,10.3]9.3 [13.0,12.5]7.4 [9.0,9.1]8.5 [9.2,8.9]CoT12.5 [15.0,15.0] 13.5 [15.4,14.5] 11.1 [14.2,14.9] 12.8 [14.2,13.5] 8.9 [10.3,10.5]9.7 [11.7,10.1]ToT13.7 [16.8,17.1] 14.3 [14.6,13.8] 12.0 [16.5,16.6] 13.7 [14.4,13.1] 9.0 [10.0,10.3]11.1 [12.5,12.9]ReAct15.0 [17.6,17.4] 15.8 [23.5,22.8] 14.3 [17.4,16.1] 15.0 [21.6,21.6] 12.6 [14.8,15.4] 14.4 [21.3,20.7]Rememberer37.6 [44.7,45.1] 46.4 [52.6,50.8] 34.8 [37.4,44.2] 37.8 [46.4,40.2] 32.5 [32.6,34.8] 30.5 [36.6,35.7]S2ERS(1 step) 65.4 [72.4,73.7] 59.6 [67.1,65.6] 59.5 [64.7,64.6] 52.5 [59.2,57.4] 51.9 [55.1,53.5] 46.7 [53.7,51.7]S2ERS(2 step) 67.7 [75.1,76.2] 62.9 [69.6,67.7] 61.2 [66.2,67.2] 54.2 [63.7,59.8] 53.5 [56.4,56.3] 48.4 [55.9,52.9]S2ERS(3 step) 68.7 [76.5,77.1] 64.0 [71.4,68.4] 61.9 [67.5,69.9] 55.4 [66.7,63.4] 54.6 [58.2,59.7] 50.0 [57.4,54.9]
results on larger MAZE.Where the Rememberer and S2ERS are trained for 10 epochs in 3 mazes.The best results are highlighted by bold and the best baseline models are marked by italics.The score in each cell is represented as "SR(%) score [OR(%)]".Particularly, traditional prompt engineer (Naive, CoT, ToT and ReAct) almost cannot succeed in such a large MAZE.</p>
<p>Table 1 .
1
Experimental results on closed-source LLMs ChatGPT 3.5, ERNIE-Bot 4.0 and open-source LLM ChatGLM-6B.Where the Rememberer and S2ERS are trained for 30 epochs.The best results are highlighted by bold and the best baseline models are marked by italics.The score in each cell is represented as "ChatGLM-6B[ChatGPT score, ERNIE-Bot score]".</p>
<p>75.3 [50.5] 76.8 [48.8] 66.7 [45.5]
MethodChatGPT 3.5 ERNIE-Bot ChatGLM-6BNaive11.0 [4.5]11.4 [4.0]9.2 [3.5]CoT15.0 [5.7]15.0 [5.5]12.5 [4.5]ToT16.4 [5.6]17.0 [4.5]12.3 [4.5]ReAct17.7 [10.5]16.5 [9.8]15.0 [7.8]Rememberer46.4 [32.6]42.1 [30.3]37.9 [28.6]S2ERS(1 step) 71.0 [42.5]73.4 [41.6]64.7 [39.5]S2ERS(2 step) 73.5 [47.0]74.2 [45.7]66.7 [42.0]S2ERS(3 step)</p>
<p>Table 3 .
3
Experimental results on robot navigation tasks.Where the Rememberer and S2ERS are trained for 30 epochs.The best results are highlighted by bold and the best baseline models are marked by italics.The score in each cell is represented as "SR(%) score [OR(%)]."Environment:A maze of 5 * 5, with two batteries, a key, a door and a target.We validate the improvement brought by the ER module using ChatGPT 3.5 on the Rememberer algorithm.The following text describes a simple MAZE environment and plans the shortest path to the target through the Rememberer algorithm.
Prompt:I'm now in a square maze and need to find the exit (target location). You need to help me make decisions aboutmy next action. In the maze, my available actions are:turn right -&gt; (x, y + 1)turn up -&gt; (x + 1, y)turn left -&gt; (x, y -1)turn down -&gt; (x -1, y)Example:You are in a 5 by 5 world. There are obstacles that you have to avoid at: (2,0) (1,1) and (1,2) Go from (0,0) to(1,0).Encouraged:turn up -&gt; 10 Get into the targetDiacourage:turn down -&gt; -0.1 Keep me away from the target location.Now the new input:You are in a 5 by 5 world. There are obstacles that you have to avoid at: (2,0)</p>
<p>Table 6 .
6
Enhancement of various prompt engineering with the aid of the ER module.The score in each cell is represented as "ChatGPT score [ERNIE-Bot score]".
LLMsMethodsOrg tokens Compress tokens Org SR(%)[OR(%)] Compress SR(%)[OR(%)]S2ERS(1 step) 14390472617719546.7 [50.0]26.7 [50.0]ChatGPT 3.5S2ERS(2 step) 9738653419170946.7 [57.1]26.7 [62.5]S2ERS(3 step) 7518269323942650.0 [66.7]33.3 [60.0]S2ERS(1 step) 14480451623483550.0 [46.7]26.7 [37.5]ERNIE-BotS2ERS(2 step) 9649215415275853.3 [50.0]30.0 [33.3]S2ERS(3 step) 7585293326008053.3 [62.5]33.3 [50.0]S2ERS(1 step) 20132092864871940.0 [41.7]20.0 [33.3]ChatGLM-6BS2ERS(2 step) 13521289581329843.3 [46.2]20.0 [50.0]S2ERS(3 step) 10522810451626043.3 [46.2]23.3 [42.9]</p>
<p>Table 5 .
5
The compression rate of LLMLingua is approximately 43%, which resulting in a decrease in both success rate and optimality rate in MAZE 100 × 100.</p>
<p>Scientific Reports | (2025) 15:8881 2 | https://doi.org/10.1038/s41598-025-93601-5
Scientific Reports |(2025) 15:8881
| https://doi.org/10.1038/s41598-025-93601-5 www.nature.com/scientificreports/
Scientific Reports | (2025) 15:8881 4 | https://doi.org/10.1038/s41598-025-93601-5
| https://doi.org/10.1038/s41598-025-93601-5
Scientific Reports | (2025) 15:8881 11 | https://doi.org/10.1038/s41598-025-93601-5 www.nature.com/scientificreports/
AcknowledgementsThis work is supported by Sichuan Science and Technology Program under Grant Number 2022NSFSC0552 and 2023NSFSC1397.Data availabilityAll data generated or analysed during this study are included in this published article and supplementary information files.Code availabilitySource code is available at Code.Received: 20 June 2024; Accepted: 7 March 2025Author contributionsH.Z. and C.F. conceived the experiment(s), H.D. and J.O. conducted the experiment(s), H.Z. and H.D. analysed the results.All authors reviewed the manuscript.DeclarationsCompeting interestsThe authors declare no competing interests.Additional informationSupplementary InformationThe online version contains supplementary material available at h t t p s : / / d o i .o r g / 1 0 . 1 0 3 8 / s 4 1 5 9 8 -0 2 5 -9 3 6 0 1 -5 .Correspondence and requests for materials should be addressed to H.Z.Reprints and permissions information is available at www.nature.com/reprints.Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made.The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material.If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
Generative agents: Interactive simulacra of human behavior. J S Park, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>Protein function prediction as approximate semantic entailment. M Kulmanov, Nat. Mach. Intell. 2024</p>
<p>Chatgpt has entered the classroom: How LLMS could transform education. A Extance, Nature. 6232023</p>
<p>Can an embodied agent find your "cat-shaped mug"? LLM-based zero-shot object navigation. V S Dorbala, J F MullenJr, D Manocha, IEEE Robot. Autom. Lett. 2023</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. L Huang, arXiv:2311.052322023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, Adv. Neural Inf. Process. Syst. 352022</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, Adv. Neural Inf. Process. Syst. 362024</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. M Besta, Proc. AAAI Conf. AAAI Conf202438</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, The Eleventh International Conference on Learning Representations. 2022</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, A Gopinath, K Narasimhan, S Yao, Adv. Neural Inf. Process. Syst. 362024</p>
<p>Grounding large language models in interactive environments with online reinforcement learning. T Carta, International Conference on Machine Learning. PMLR2023</p>
<p>Large language models as commonsense knowledge for large-scale task planning. Z Zhao, W S Lee, D Hsu, Adv. Neural Inf. Process. Syst. 362024</p>
<p>Guiding pretraining in reinforcement learning with large language models. Y Du, International Conference on Machine Learning. PMLR2023</p>
<p>Large language models are semi-parametric reinforcement learning agents. D Zhang, Adv. Neural Inf. Process. Syst. 362024</p>
<p>Reward design with language models. M Kwon, S M Xie, K Bullard, D Sadigh, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Chain-of-experts: When LLMS meet complex operations research problems. Z Xiao, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Can large language models be good path planners? A benchmark and investigation on spatialtemporal reasoning. M Aghzal, E Plaku, Z Yao, arXiv:2310.032492023arXiv preprint</p>
<p>C Yang, arXiv:2411.11507Traffic sign interpretation from vehicle first-person view. 2024arXiv preprint</p>
<p>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. B Y Lin, Adv. Neural Inf. Process. Syst. 362024</p>
<p>Model-based reinforcement learning with multi-step plan value estimation. H Lin, Y Sun, J Zhang, Y Yu, arXiv:2209.055302022arXiv preprint</p>
<p>Compressing prompts for accelerated inference of large language models. H Jiang, Q Wu, C.-Y Lin, Y Yang, L Qiu, Llmlingua, The 2023 Conference on Empirical Methods in Natural Language Processing. </p>            </div>
        </div>

    </div>
</body>
</html>
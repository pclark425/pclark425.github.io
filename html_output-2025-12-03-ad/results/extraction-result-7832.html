<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7832 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7832</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7832</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-277824677</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.11972v2.pdf" target="_blank">LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA</a></p>
                <p><strong>Paper Abstract:</strong> Extractive reading comprehension question answering (QA) datasets are typically evaluated using Exact Match (EM) and F1-score, but these metrics often fail to fully capture model performance. With the success of large language models (LLMs), they have been employed in various tasks, including serving as judges (LLM-as-a-judge). In this paper, we reassess the performance of QA models using LLM-as-a-judge across four reading comprehension QA datasets. We examine different families of LLMs and various answer types to evaluate the effectiveness of LLM-as-a-judge in these tasks. Our results show that LLM-as-a-judge is highly correlated with human judgments and can replace traditional EM/F1 metrics. By using LLM-as-a-judge, the correlation with human judgments improves significantly, from 0.22 (EM) and 0.40 (F1-score) to 0.85. These findings confirm that EM and F1 metrics underestimate the true performance of the QA models. While LLM-as-a-judge is not perfect for more difficult answer types (e.g., job), it still outperforms EM/F1, and we observe no bias issues, such as self-preference, when the same model is used for both the QA and judgment tasks.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7832.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7832.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge vs Humans (Qwen 2.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-judge evaluations (Qwen 2.5 72B, Llama 3.3 70B, Mistral 7B) with human judgments on extractive reading-comprehension QA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper directly compares LLM-based binary judgments (Correct / Incorrect) produced by several instruction-tuned LLMs to human annotations on predicted answers for four extractive QA datasets, reporting Pearson correlations and detailed failure-mode analyses by answer type and model pairing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Extractive reading-comprehension question answering (QA)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Quoref, DROP, HotpotQA, 2WikiMultiHopQA (development subsets sampled for analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Qwen 2.5 72B (primary), Llama 3.3 70B, Mistral-Instruct-7B-v0.3 (also used)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Instruction-tuned LLM families used as binary judges; Qwen 2.5 (72B) and Llama 3.3 (70B) are large (≈70B) instruction-tuned models; Mistral-Instruct-7B-v0.3 is a 7B instruction-tuned model. All judges were few-shot prompted for CORRECT/INCORRECT labels.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Two authors of the paper performed annotation; one annotator was assigned per sample (annotators could discuss ambiguous cases); 200 sampled instances were collected, 161 valid samples remained, yielding 1,288 predicted-answer judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.847</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Imperfect on ambiguous 'job' answers (low correlation due to ambiguity); LLM judges can be more lenient than humans for job-type answers; lower percent-correct detection for 'date' answers when EM is false; occasional undefined/NaN correlation when model predictions are constant (affects Pearson); small but measurable self-preference cases for some same-model judge/QA pairs</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>LLM judges (especially Qwen 2.5) are highly correlated with human judgments (strong correlation >0.80) and capture many valid alternative phrasings missed by EM/F1; they are particularly effective on numeric/date answers and less strict on multi-job/ambiguous answers; limited self-preference bias observed in extractive QA; NaN correlations can arise when a model's predictions are constant.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Substantially higher agreement with human judgments than EM/F1 (captures paraphrases and alternative valid answers); scalable, reproducible, and able to replace coarse EM/F1 metrics; improves evaluation correlation with humans from EM:0.22 / F1:0.40 up to ≈0.85.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>LLM-as-a-judge used a 6-shot few-shot prompt (demonstrations excluded context for brevity); input included Question, Gold Answer, Predicted Answer, and Context; judge returns CORRECT/INCORRECT. Human annotation: two authors, single annotator per sample (with discussion allowed), 161 valid samples (1,288 predicted answers). Correlations computed per judge model across predictions; F1 was thresholded at 0.5 to binarize for correlation. For self-preference analysis seven judge models were compared; main analyses used Qwen 2.5 72B as judge.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_quantitative_details</strong></td>
                            <td>Paper reports average Pearson correlations between human judgments and judges across eight QA models: Mistral 7B avg 0.653, Llama 3.3 70B avg 0.750, Qwen 2.5 72B avg 0.847; average correlations of EM and F1 with human judgments were 0.220 and 0.404 respectively; per-model correlation values are provided in Table 2 of the paper; sample size for human-vs-judge correlation: 1,288 judged predicted answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Replacing judges with juries: Evaluating llm generations with a panel of diverse models <em>(Rating: 2)</em></li>
                <li>Large language models are state-of-the-art evaluators of translation quality <em>(Rating: 2)</em></li>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>Judging LLM-as-a-judge with MT-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>LLMs as narcissistic evaluators: When ego inflates evaluation scores <em>(Rating: 2)</em></li>
                <li>Is LLM-as-a-judge robust? investigating universal adversarial attacks on zero-shot LLM assessment <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7832",
    "paper_id": "paper-277824677",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "LLM-as-a-Judge vs Humans (Qwen 2.5)",
            "name_full": "Comparison of LLM-as-a-judge evaluations (Qwen 2.5 72B, Llama 3.3 70B, Mistral 7B) with human judgments on extractive reading-comprehension QA",
            "brief_description": "This paper directly compares LLM-based binary judgments (Correct / Incorrect) produced by several instruction-tuned LLMs to human annotations on predicted answers for four extractive QA datasets, reporting Pearson correlations and detailed failure-mode analyses by answer type and model pairing.",
            "citation_title": "LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA",
            "mention_or_use": "use",
            "paper_title": "LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA",
            "evaluation_task": "Extractive reading-comprehension question answering (QA)",
            "dataset_name": "Quoref, DROP, HotpotQA, 2WikiMultiHopQA (development subsets sampled for analysis)",
            "judge_model_name": "Qwen 2.5 72B (primary), Llama 3.3 70B, Mistral-Instruct-7B-v0.3 (also used)",
            "judge_model_details": "Instruction-tuned LLM families used as binary judges; Qwen 2.5 (72B) and Llama 3.3 (70B) are large (≈70B) instruction-tuned models; Mistral-Instruct-7B-v0.3 is a 7B instruction-tuned model. All judges were few-shot prompted for CORRECT/INCORRECT labels.",
            "human_evaluator_type": "Two authors of the paper performed annotation; one annotator was assigned per sample (annotators could discuss ambiguous cases); 200 sampled instances were collected, 161 valid samples remained, yielding 1,288 predicted-answer judgments.",
            "agreement_metric": "Pearson correlation",
            "agreement_score": 0.847,
            "reported_loss_aspects": "Imperfect on ambiguous 'job' answers (low correlation due to ambiguity); LLM judges can be more lenient than humans for job-type answers; lower percent-correct detection for 'date' answers when EM is false; occasional undefined/NaN correlation when model predictions are constant (affects Pearson); small but measurable self-preference cases for some same-model judge/QA pairs",
            "qualitative_findings": "LLM judges (especially Qwen 2.5) are highly correlated with human judgments (strong correlation &gt;0.80) and capture many valid alternative phrasings missed by EM/F1; they are particularly effective on numeric/date answers and less strict on multi-job/ambiguous answers; limited self-preference bias observed in extractive QA; NaN correlations can arise when a model's predictions are constant.",
            "advantages_of_llm_judge": "Substantially higher agreement with human judgments than EM/F1 (captures paraphrases and alternative valid answers); scalable, reproducible, and able to replace coarse EM/F1 metrics; improves evaluation correlation with humans from EM:0.22 / F1:0.40 up to ≈0.85.",
            "experimental_setting": "LLM-as-a-judge used a 6-shot few-shot prompt (demonstrations excluded context for brevity); input included Question, Gold Answer, Predicted Answer, and Context; judge returns CORRECT/INCORRECT. Human annotation: two authors, single annotator per sample (with discussion allowed), 161 valid samples (1,288 predicted answers). Correlations computed per judge model across predictions; F1 was thresholded at 0.5 to binarize for correlation. For self-preference analysis seven judge models were compared; main analyses used Qwen 2.5 72B as judge.",
            "additional_quantitative_details": "Paper reports average Pearson correlations between human judgments and judges across eight QA models: Mistral 7B avg 0.653, Llama 3.3 70B avg 0.750, Qwen 2.5 72B avg 0.847; average correlations of EM and F1 with human judgments were 0.220 and 0.404 respectively; per-model correlation values are provided in Table 2 of the paper; sample size for human-vs-judge correlation: 1,288 judged predicted answers.",
            "uuid": "e7832.0",
            "source_info": {
                "paper_title": "LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Replacing judges with juries: Evaluating llm generations with a panel of diverse models",
            "rating": 2,
            "sanitized_title": "replacing_judges_with_juries_evaluating_llm_generations_with_a_panel_of_diverse_models"
        },
        {
            "paper_title": "Large language models are state-of-the-art evaluators of translation quality",
            "rating": 2,
            "sanitized_title": "large_language_models_are_stateoftheart_evaluators_of_translation_quality"
        },
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "Judging LLM-as-a-judge with MT-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "LLMs as narcissistic evaluators: When ego inflates evaluation scores",
            "rating": 2,
            "sanitized_title": "llms_as_narcissistic_evaluators_when_ego_inflates_evaluation_scores"
        },
        {
            "paper_title": "Is LLM-as-a-judge robust? investigating universal adversarial attacks on zero-shot LLM assessment",
            "rating": 2,
            "sanitized_title": "is_llmasajudge_robust_investigating_universal_adversarial_attacks_on_zeroshot_llm_assessment"
        }
    ],
    "cost": 0.009828,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA
22 Apr 2025</p>
<p>Xanh Ho 
National Institute of Informatics
Japan</p>
<p>Jiahao Huang jiahao-huang@g.ecc.u-tokyo.ac.jp 
The University of Tokyo
Japan</p>
<p>Florian Boudin florian.boudin@univ-nantes.fr 
JFLI
CNRS
Nantes Université
France</p>
<p>Akiko Aizawa aizawa@nii.ac.jp 
National Institute of Informatics
Japan</p>
<p>LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA
22 Apr 20253501F63BE6F5AA3D6FAB7DF0339EA8C3arXiv:2504.11972v2[cs.CL]Please format your answer within brackets as follows: <ans> Your Answer </ans> ## Here are some examples: ### Example 1: * Question: What nationality is the performer of song Daddy, Come Home? * Gold Answer: United States * Predicted Answer: American * Label: <ans> CORRECT </ans> ### Example 2: * Question: Who is Bohemond Iv Of Antioch's paternal grandmother? * Gold Answer: Constance of Antioch * Predicted Answer: princess Constance of Antioch * Label: <ans> CORRECT </ans> ### Example 3: * Question: Rejuvelac is kind of grain water invented and promoted by a 'holistic health' practitioner born in which year? * Gold Answer: 1909 * Predicted Answer: Rejuvelac is a kind of grain water invented and promoted by Ann Wigmore, who was born in 1909. * Label: <ans> CORRECT </ans> ### Example 4: * Question: What is the birthday of the actress who was the Duchess in 'The Revengers Tragedy'? * Gold Answer: 23 November 1946 * Predicted Answer: Diana Quick, who played the Duchess in 'The Revengers Tragedy', was born on 23rd September 1934. * Label: <ans> INCORRECT </ans> ### Example 5: * Question: Who beacme a star as a comic book character created by Gerry Conway and Bob Oksner? * Gold Answer: Megalyn Echikunwoke * Predicted Answer: Vixen * Label: <ans> INCORRECT </ans> ### Example 6: * Question: How many years after the peace treaty was annulled did the Sauk finally grant the Fox sanctuary? * Gold Answer: 3 * Predicted Answer: 11 years * Label: <ans> INCORRECT </ans> ## Here is your task: * Question: {question} * Gold Answer: {gold ans} * Predicted Answer: {pred ans} * Context: {context} Table 9: The LLM-as-a-judge task prompt.
Extractive reading comprehension question answering (QA) datasets are typically evaluated using Exact Match (EM) and F1-score, but these metrics often fail to fully capture model performance.With the success of large language models (LLMs), they have been employed in various tasks, including serving as judges (LLM-as-a-judge).In this paper, we reassess the performance of QA models using LLM-as-a-judge across four reading comprehension QA datasets.We examine different families of LLMs and various answer types to evaluate the effectiveness of LLM-as-a-judge in these tasks.Our results show that LLM-as-a-judge is highly correlated with human judgments and can replace traditional EM/F1 metrics.By using LLM-as-a-judge, the correlation with human judgments improves significantly, from 0.22 (EM) and 0.40 (F1-score) to 0.85.These findings confirm that EM and F1 metrics underestimate the true performance of the QA models.While LLM-as-a-judge is not perfect for more difficult answer types (e.g., job), it still outperforms EM/F1, and we observe no bias issues, such as self-preference, when the same model is used for both the QA and judgment tasks. 1</p>
<p>Introduction</p>
<p>Machine reading comprehension (MRC) is a crucial task for evaluating natural language understanding, designed to test a model's reading comprehension by requiring it to answer questions based on a given text (Hirschman et al., 1999).Many datasets have been proposed over the past several years, such as SQuAD (Rajpurkar et al., 2016;2018) for simple MRC, QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019) for conversational MRC, and QAngaroo (Welbl et al., 2018), HotpotQA (Yang et al., 2018), and FanOutQA (Zhu et al., 2024) for multi-hop MRC.</p>
<p>Based on the answer format, Chen (2018) classify existing MRC tasks into four types: extraction, multiple-choice (MC), cloze-style, and free-form.Some recent datasets also introduce yes/no answers (Clark et al., 2019;Geva et al., 2021).Depending on the answer type, corresponding evaluation metrics are used.Since the yes/no, MC, and cloze-style (selecting an answer from a set of options) typically do not face issues with evaluation metrics-accuracy being the standard-other datasets may encounter difficulties.For instance, datasets with extractive answer types often rely on Exact Match (EM) and F1-score as evaluation metrics.However, we observe that these two metrics do not accurately reflect the models' actual performance, often underestimating it, as shown in Figure 1.The core issue stems from the fact that while there are many valid ways to phrase a correct answer, the evaluation process usually relies on a single, fixed gold answer.This lack of flexibility 1 Our data and code are available at https://github.com/Alab-NII/llm-judge-extract-qa Figure 1: An example that shows EM and F1-score underestimate the performance of models, while LLM-as-a-judge provides a more robust perspective.</p>
<p>Recently, the success and capabilities of large language models (LLMs) have led to their application in various tasks, including their use as judges (LLM-as-a-judge) (Zheng et al., 2023;Chiang &amp; Lee, 2023;Kocmi &amp; Federmann, 2023;Li et al., 2024;Verga et al., 2024;Chen et al., 2024).LLM-as-a-judge is used for tasks involving text generation, such as machine translation (Kocmi &amp; Federmann, 2023), text summarization (Liu et al., 2023;Skopek et al., 2023;Wu et al., 2023), story generation (Chiang &amp; Lee, 2023), and reading comprehension QA (Zheng et al., 2023;Verga et al., 2024;Chen et al., 2024).Verga et al. (2024) is the most similar to ours.They experiment with different QA datasets to demonstrate that, rather than using a single large model as a judge, we can combine several smaller models to form juries.Our work complements this study by delving deeper into the reading comprehension QA task to reveal the true capabilities of current LLMs.Unlike their approach, we provide fine-grained analyses to examine the impact of LLM-as-a-judge on specific answer types.Moreover, we will release our dataset, which can be used to evaluate the use of LLM-as-ajudge in the extractive QA task, as they did not release theirs.</p>
<p>In this paper, we aim to reassess the performance of LLMs on the reading comprehension QA task by employing LLM-as-a-judge.To gain a more comprehensive understanding of the current capabilities of LLMs in reading comprehension QA tasks, we conduct experiments on four different datasets with multiple types of answers: Quoref (Dasigi et al., 2019), DROP (Dua et al., 2019), HotpotQA (Yang et al., 2018), and 2WikiMultiHopQA (Ho et al., 2020).Additionally, to gain deeper insights into the effectiveness of LLM-as-a-judge for evaluating reading comprehension QA tasks, we analyze the existing answer types to identify where LLM-as-a-judge is beneficial and where it falls short.Furthermore, we use different families of LLMs for both the QA and LLM-as-a-judge tasks, and we also conduct an analysis of self-preference bias (Liu et al., 2024;Panickssery et al., 2024) when the same model is used for both tasks.</p>
<p>Our results show that using LLM-as-a-judge is highly correlated with human judgments, improving from 0.22 (EM) and 0.40 (F1-score) to 0.85, highlighting its potential to replace EM and F1 metrics.This high correlation further confirms that EM and F1 metrics underestimate the true performance of the models.Our analysis demonstrates that LLMs as judges are particularly effective for answer types related to numbers and dates in extractive QA.Moreover, we observe no self-preference bias in the extractive QA task when the same model is used for both the QA and judgment tasks.</p>
<p>Related Work</p>
<p>Extractive QA.An extractive QA task requires a model to extract a span of text from the provided context to answer a given question.Early datasets for this task include SQuAD (Rajpurkar et al., 2016;2018), CNN/DailyMail (Hermann et al., 2015), and NewsQA (Trischler et al., 2017).Later datasets introduced additional challenges, such as multi-hop reasoning (Welbl et al., 2018;Yang et al., 2018;Trivedi et al., 2022), coreferential reasoning (Dasigi et al., 2019), and numerical reasoning (Dua et al., 2019).To the best of our knowledge, previous extractive QA datasets have predominantly relied on automatic evaluation metrics, such as EM, F1, or accuracy, in their default evaluation protocols.</p>
<p>Evaluation of Extractive QA.In addition to evaluating a predicted answer for the main question task using EM and F1, some studies also consider using EM and F1 for evaluating explanation tasks (Yang et al., 2018;Ho et al., 2020), such as sentence-level supporting facts prediction in HotpotQA (Yang et al., 2018) or evidence prediction in 2WikiMultiHopQA (Ho et al., 2020) and R 4 C (Inoue et al., 2020).Other studies evaluate sub-questions as explanations for the answers but often rely on EM, F1, accuracy, or a compositionality gap derived from accuracy for their assessments (Tang et al., 2021;Press et al., 2023;Wu et al., 2025).</p>
<p>LLM-as-a-Judge.Thanks to their success and capabilities, LLMs have been applied to a variety of tasks, including serving as judges for tasks related to text generation, such as text summarization (Liu et al., 2023;Skopek et al., 2023;Wu et al., 2023) and QA (Zheng et al., 2023;Verga et al., 2024;Chen et al., 2024).However, several issues arise when using LLMs as judges, particularly concerning their accuracy and reliability in this role.One natural approach is to compare the scores given by LLM-as-a-judge with human scores to assess their agreement or correlation (Thakur et al., 2025).Previous work has highlighted several biases in LLM-as-a-judge, such as order bias (Wang et al., 2024) and egocentric bias (Koo et al., 2024).Another concern is that LLMs can be vulnerable to adversarial attacks, which could manipulate their scoring (Shi et al., 2024;Raina et al., 2024).For a more detailed discussion on the use of LLM-as-a-judge, we refer readers to the comprehensive survey paper by Gu et al. (2025).</p>
<p>In this paper, we focus on using LLM-as-a-judge for the extractive QA task, which is simpler than other text generation tasks.We employ various LLMs as judges in our experiments and compare their evaluations with human judgments to assess the correlation scores of LLM-as-a-judge in this context.</p>
<p>Experimental Settings</p>
<p>Datasets</p>
<p>We select four reading comprehension QA datasets for our experiments based on the following criteria: (1) they use EM and F1 as evaluation metrics, (2) they have not yet been fully solved, (3) they provide context for the given questions, and (4) they contain diverse types of answers.These four datasets are: Quoref (Dasigi et al., 2019), DROP (Dua et al., 2019), HotpotQA (Yang et al., 2018), and 2WikiMultiHopQA (2Wiki;Ho et al., 2020).To ensure consistency in our comparisons, we only use datasets that employ EM and F1 as evaluation metrics.To assess whether the current EM and F1 scores may underestimate the true performance of the models, we select datasets where these scores are not excessively high (e.g., below 95%).To simplify the judgement process, we choose datasets that provide the corresponding context, allowing us to refer to it when determining whether a predicted answer is acceptable.Regarding answer types, we select datasets from various tasks, such as numerical reasoning and multi-hop reasoning, to ensure a diverse range of answer types for our analysis.</p>
<p>Quoref.Quoref is a reading comprehension dataset designed to assess the coreferential reasoning abilities of models.In the default dataset setup, a passage can contain multiple QA pairs.We treat each pair as an individual sample in our experiments.The questions are created through crowdsourcing, with a focus on the coreference resolution phenomenon.</p>
<p>Most of the answer types are in string format, such as person names.</p>
<p>DROP.DROP is a reading comprehension dataset designed to evaluate the numerical reasoning abilities of models.Similar to Quoref, a single paragraph can contain multiple QA pairs, and we treat each pair as an individual sample.It is worth noting that, in addition to extractive answer types, DROP also includes number and date answer types.These two types may not always be extractive (i.e., a span of text appearing directly in the context), such as when a question asks about the next day of a specific day.However, we include them in our analyses, as they do not pose challenges for generative models in predicting the correct answer.</p>
<p>HotpotQA.HotpotQA is a multi-hop QA task that requires multiple reasoning steps to answer each question.The dataset comprises two main question types: bridge and comparison.Notably, the presence of comparison questions introduces yes/no answers into the dataset.HotpotQA is designed for two key tasks: answer prediction and sentence-level supporting facts prediction.In our work, we focus exclusively on the answer prediction task.</p>
<p>2Wiki.Similar to HotpotQA, 2Wiki also features two main question types, bridge and comparison.In addition, it includes a separate task designed to evaluate the explanatory abilities of models.As with HotpotQA, our focus is solely on the answer prediction task.</p>
<p>Obtaining Answer Types.From the development set of each dataset, we use heuristic rules to obtain the answer type.We define the rules for 8 answer types: string, place, name, job, date, number, year, and bool as follows:</p>
<p>• String: questions that cannot be categorized into the following categories.</p>
<p>• Place: questions starting with "where" or asking about the city, country, state, place, region, nationality, etc.</p>
<p>• Name: questions starting with "who" and "whom" or asking about the name, nickname, role, player, actor, etc.</p>
<p>• Job: questions asking about occupation, job, profession, or career.</p>
<p>• Date: questions asking about which or what date.</p>
<p>• Number: questions starting with "how many", "how much", or asking about percentage, population, etc.</p>
<p>• Year: questions asking about which or what year.</p>
<p>• Bool: yes-no questions.</p>
<p>Our goal is to carefully select a representative subset of approximately 1,000 samples from each dataset to ensure sufficient and meaningful analysis.It is important to note that our rules may not capture every question from the full development set.To ensure diversity in answer types for our experiment, we include all samples from answer types with fewer than 100 instances.For answer types exceeding this threshold, we randomly sample in proportion to their representation in the entire dataset.Since the boolean answer type (yes/no) does not pose significant challenges when evaluated using EM and F1 metrics, we exclude samples with boolean answers from our experiments.Table 1 presents the number of samples for each answer type, along with the total number of samples for each dataset used in our experiments.</p>
<p>Models</p>
<p>QA Task.We use four different model families for the QA task: Mistral v0.1 (7B and 8x7B) (Jiang et al., 2023;2024), Qwen 2 (7B and 72B) (Yang et al., 2024), Gemma 2 (9B and 27B) (Team, 2024), and Llama 3.1 (8B and 70B) (Grattafiori et al., 2024).</p>
<p>LLM-as-a-Judge.We use three model families as LLM-as-a-judge systems: Mistral-Instruct-7B-v0.3 (Jiang et al., 2023), Llama 3.3 70B (Grattafiori et al., 2024) and Qwen 2.5 72B (Yang et al., 2025).It is noted that these model families are similar to those used in our QA task, but we employ an enhanced version.To examine the self-preference bias (Liu et al., 2024;Panickssery et al., 2024), we also run Qwen 2 (7B and 72B) and Llama 3.1 (8B and 70B) as judges in our analyses.Notably, all the models used in our experiments are instructiontuned.The number of samples per answer type and the total number of samples for the four datasets.A dash ('-') indicates that the corresponding answer type is not present in the dataset.In the case of Quoref, we observed that the date and year answer types were frequently mispredicted by rules due to the specific nature of the related questions.Therefore, we chose to exclude these samples from our experiments.</p>
<p>Dataset</p>
<p>Promptings</p>
<p>Since the development of in-context learning (Brown et al., 2020), more advanced prompting techniques have been introduced, such as chain-of-thought (CoT) prompting (Wei et al., 2022) and zero-shot CoT prompting (Kojima et al., 2022).</p>
<p>QA Task.Since most current LLMs are familiar with the QA task and the datasets we use often involve lengthy context passages, coupled with our focus on evaluation, we chose to adopt zero-shot CoT prompting in our experiments.Our prompt is presented in Appendix A.1.</p>
<p>LLM-as-a-Judge.To evaluate the predicted answers using LLMs, we use a few-shot prompting approach.In this approach, we provide a few demonstration examples to guide the model in labeling the answers.Due to length constraints and for the sake of simplicity, these demonstrations exclude the context.During evaluation, we present the model with a question, a gold answer, a predicted answer, and the corresponding context.The model is then instructed to assign one of two labels to the predicted answer: Correct and Incorrect.</p>
<p>• CORRECT: The predicted answer matches the gold answer or is a valid alternative (e.g., different but correct ways of writing a name).</p>
<p>• INCORRECT: The predicted answer is wrong or does not align with the gold answer.</p>
<p>We use a 6-shot approach in our judgment prompt.We randomly select five samples from each dataset (the subset not used in our evaluation) and run a simple QA model to generate predicted answers.Subsequently, we manually choose four shots from these samples and reuse two exemplars from the paper by Verga et al. (2024).The prompt used in our study is presented in Appendix A.1.</p>
<p>Results and Analyses</p>
<p>Reliability of LLM-as-a-Judge Scores</p>
<p>To assess the reliability of LLM-as-a-judge scores, we first collect human judgments on the predicted answers.We then calculate the correlation between these human judgments and the LLM-as-a-judge scores.</p>
<p>Human Judgements.We sampled 200 instances (50 per dataset), maintaining the original distribution of answer types.Each sample included all 8 predicted answers.Annotators (two authors of this paper) were presented with the question, gold answer, predicted answers, and context, and asked to label each predicted answer as either 'Correct or 'Incorrect'.Since the task is simple, we decided to assign only one annotator to each sample.However, annotators may discuss ambiguous cases.We excluded cases where the gold answer was incorrect, leaving 161 valid samples, each with 8 predicted answers, resulting in 1,288 predicted answers.These were used to calculate the correlation between human judgment and LLM performance as a judge.</p>
<p>Correlation to Human Judgements.We calculate the Pearson correlation (Pearson, 1895) between human judgments and the LLM-as-a-judge evaluations from three models: Mistral 7B v0.3, Llama 3.3 70B, and Qwen 2.5 72B.The correlation scores are presented in Table 2.</p>
<p>As shown in the table, Qwen 2.5 exhibits the highest correlation with human judgments, followed by Llama 3.3, while Mistral v0.3 has the lowest correlation scores.According to standard explanation, a correlation score above 0.80 is generally considered strong.We focus on using Qwen 2.5 72B as the judge for our subsequent analyses.</p>
<p>Additionally, we include the correlation scores between human judgments and the EM/F1 scores.For the F1-score, we use a threshold of 0.5 to classify values between 0 and 1 for correlation calculation.As presented in the table, these two metrics, EM/F1 scores, have correlation scores smaller than those of all LLM-as-a-judge models.These scores indicate that we can use LLM-as-a-judge to evaluate the extractive QA task.</p>
<p>QA Model</p>
<p>Comparing EM/F1 Scores and LLM-as-a-Judge</p>
<p>We first present the performance gap between EM/F1 scores and LLM-as-a-judge for false samples -those with a false EM score.Next, we compare EM/F1 scores and LLM-as-a-judge across all samples.Ideally, LLM-as-a-judge should be used only when the EM score is false.</p>
<p>Using LLM-as-a-Judge for False Samples.Table 3 presents the EM and F1 scores, along with the LLM-as-a-judge scores (from three models: Mistral 7B v0.3, Llama 3.3 70B, and Qwen 2.5 72B) for the four datasets: Quoref, DROP, HotpotQA, and 2Wiki.As shown in the Table, the LLM-as-a-judge scores from the three models are higher than both the EM and F1 scores.The largest gap is 81.9 between EM and Llama 3.3 70B as a judge on HotpotQA, evaluating Mixtral 8x7B's answers.The smallest gap is 15.9 between EM and Qwen 2.5 72B as a judge on 2Wiki, evaluating Gemma 2 9B's answers.However, this comparison may be biased since LLM-as-a-judge is only used for false samples, while true EM scores are directly assigned to it.To offer a broader perspective, we present experimental results for all samples in the following section.Table 3: Automatic evaluation scores (EM and F1) and LLM-as-a-judge scores (highlighted in green and blue) from three models -Mistral 7B v0.3, Llama 3.3 70B, and Qwen 2.5 72Bfor the four datasets.</p>
<p>Using LLM-as-a-Judge for All Samples.Table 10 (in Appendix B.1) presents LLM-asa-judge scores for Mistral 7B v0.3, Llama 3.3 70B, and Qwen 2.5 72B across four datasets, evaluated in two cases: false EM samples only and all samples.As shown in the table, the gaps between these evaluations are minimal, indicating comparable performance.</p>
<p>Summary.Table 3 highlights the performance gap between EM/F1 scores and LLM-asa-judge scores.Our correlation analysis between human judgments and Qwen-as-a-judge confirms that EM and F1 scores underestimate model performance -not because LLM-asa-judge incorrectly labels predictions as correct, but because these metrics fail to capture true performance.Notably, using LLM-as-a-judge for false EM samples only or all samples yields similar results.In practice, we apply LLM-as-a-judge to false EM samples only.Where was the place of burial of Amun-Her-Khepeshef's mother?QV66</p>
<p>Analyses</p>
<p>The provided text does not mention where Amun-Her-Khepeshef's mother was buried.</p>
<p>Place Name: False Model: False Table 4: Examples of differences and similarities between labels provided by human judgment and LLM-as-a-judge (with labels from Qwen used in this case).</p>
<p>In Figure 3, we show the percentage of each answer type that LLM-as-a-judge identified as correct when the EM score is false.As shown in the figure, the judge is able to correctly predict more than 50% of each answer type, except for "date".The highest scores are achieved for the "job" and "place" answer types.However, when considering the correlation score with human judgment for the "job" answer type, we observe that LLM-as-a-judge is less strict than humans when judging the correctness of the predicted answer regarding jobs.</p>
<p>Self-Preference.We consider four QA models (Llama 3.1 8B, Llama 3.1 70B, Qwen 2 7B, and Qwen 2 72B) and seven LLM-as-a-judge models (Llama 3.1 8B, Llama 3.1 70B, Llama 3.3 70B, Qwen 2 7B, Qwen 2 72B, Qwen 2.5 72B, and Mistral 7B).Suppose we have seven judgment models, denoted as m 1 to m 7 and the QA model is m 1 .We define self-preference bias as the scenario in which all remaining models unanimously label the predicted answer as incorrect (threshold = 100%), while only m 1 judges its own answer as correct.We also calculate this score for different thresholds, such as 83%, where one of the six remaining models can judge the answer as correct.To measure this bias, we calculate the percentage of such cases relative to the total number of cases where the EM score is false.Table 5 presents an example of self-preference bias where the QA model is the same as Judge 1.</p>
<p>Table 6 presents the percentage of self-preference bias scores for the four models across three different thresholds.These scores indicate a relatively small self-preference bias for Llama 3.1 8B when it serves as both a QA model and a judge model.However, for the other models, the percentages are much smaller.This can be explained by the fact that, in the extractive QA task, where the gold answer is clearly provided, it is more difficult for self-bias to occur compared to other tasks.We present examples of self-preference bias for different QA models in Table 7.We observe that in these cases, the predicted answer is obviously wrong, and all remaining models label it as incorrect.However, only the judge model, which is the same as the QA model, is predicted as correct.</p>
<p>Conclusion</p>
<p>In this paper, we investigate the use of LLM-as-a-judge to reassess the performance of QA models.With generative AI models, predicted answers for extractive QA tasks can take various forms that traditional metrics like EM and F1-score fail to capture.We find that LLMs as judges provide a more comprehensive evaluation, revealing that EM and F1-score underestimate the true capabilities of the models.Our results show that LLM-as-a-judge is highly correlated with human judgments and can effectively replace traditional EM/F1 metrics.Although LLM-as-a-judge is not perfect for more difficult answer types (e.g., job), it still outperforms EM/F1, and we observe no bias issues, such as self-preference, when the same model is used for both the QA and judgment tasks.</p>
<p>Model</p>
<p>Figure 3 :
3
Figure 3: Percentage of each answer type identified as correct by Qwen-as-a-judge.</p>
<p>String Place Name Job Date Number Year Total
Quoref557487823-21-1,051DROP12424107518740-1,018HotpotQA14625746092787714 1,1242Wiki296383292-29--1,000Total6217381,737 12012583814 4,193</p>
<p>Table 1 :
1</p>
<p>Table 2 :
2Mistral 7B Llama 3.3 70B Qwen 2.5 72BEMF1Mistral 7B v0.10.6270.6270.851 0.157 0.311Mixtral 8x7B0.5920.5490.723 0.126 0.244Qwen 2 7B0.5980.7810.863 0.234 0.434Qwen 2 72B0.6530.6800.793 0.157 0.274Gemma 2 9B0.7150.8330.848 0.288 0.572Gemma 2 27B0.7110.8500.908 0.247 0.487Llama 3.1 8B0.7000.8760.945 0.331 0.628Llama 3.1 70B0.6260.8030.848 NaN 0.281Average0.6530.7500.847 0.220 0.404
Pearson correlation coefficients between human judgments and the LLM-as-a-judge evaluations from three models-Mistral 7B v0.3, Llama 3.3 70B, and Qwen 2.5 72B-across eight different QA models are presented.Additionally, we include the correlation scores between human judgments and the EM/F1 scores.NaN values arise when none of the predicted answers by Llama 3.1 70B exactly match the gold answers, resulting in a constant list of 0s, for which correlation is undefined.This often occurs when gold answers are plain numbers (e.g., 93.5), but predictions include extra context like percentages (e.g., 93.5%) or time spans (e.g., 38 years).</p>
<p>Table 6 :
6
Percentage of self-preference bias scores for the four models across three different thresholds.
Judge 1CORRECTJudge 2 INCORRECT Judge 3 INCORRECT Judge 4 INCORRECT Judge 5 INCORRECT Judge 6 INCORRECT Judge 7 INCORRECTQA Model Llama 3.1 8B Llama 3.1 70B Qwen 2 7B Qwen 2 72BThreshold = 100% 5.77 12.04 14.77 83% 67% 0.26 0.77 1.62 0.63 2.06 4.48 0.14 0.34 0.85Table 5:Example of self-preference bias where the QAmodel is the same as Judge 1.</p>
<p>Table 7 :
7
Examples of self-preference bias for different QA models.</p>
<p>Table 10 :
10
Quoref DROPMistral Mistral-A Llama Llama-A Qwen Qwen-A Mistral Mistral-A Llama Llama-A Qwen Qwen-A Mistral Mistral-A Llama Llama-A Qwen Qwen-A Mistral Mistral-A Llama Llama-A Qwen Qwen-A LLM-as-a-judge scores for Mistral 7B v0.3, Llama 3.3 70B, and Qwen 2.5 72B across four datasets.Columns labeled Mistral, Llama, or Qwen indicate scores on false EM samples only, while Mistral-A, Llama-A, or Qwen-A indicate scores on all samples.
Mistral 7B v0.165.665.780.580.657.957.659.860.249.449.840.941.0Mixtral 8x7B85.686.086.587.081.282.369.272.872.475.960.662.7Qwen 2 7B76.075.964.264.261.160.865.265.254.454.448.548.0Qwen 2 72B91.891.788.888.887.187.182.182.184.684.678.378.1Gemma 2 9B86.085.682.382.378.177.877.677.674.874.870.070.1Gemma 2 27B88.488.082.682.680.580.480.480.480.180.175.375.6Llama 3.1 8B83.082.773.072.970.770.668.568.562.862.858.858.9Llama 3.1 70B93.893.690.690.689.789.387.387.387.487.483.383.2HotpotQA2WikiMultihopQAMistral 7B v0.185.985.980.980.875.575.165.765.656.756.846.746.5Mixtral 8x7B85.988.589.091.584.687.073.173.673.774.366.466.5Qwen 2 7B90.890.886.386.282.482.477.377.065.965.860.460.8Qwen 2 72B94.694.494.494.391.592.084.784.385.385.278.578.7Gemma 2 9B94.093.891.991.888.788.377.276.677.977.868.669.0Gemma 2 27B94.794.693.193.089.989.478.978.679.579.571.872.0Llama 3.1 8B91.591.489.188.785.985.880.880.571.271.263.163.2Llama 3.1 70B95.895.694.994.892.892.485.685.187.287.281.981.9
Effects of LLM-as-a-Judge on Different Answer Types.We use 1,288 predicted answers with human judgment in Section 4.1 and LLM-as-a-judge scores from Qwen on these samples, and calculate the correlation between the two for each answer type.The scores are presented in Figure2. As expected, the answer types "date" and "number" show the highest correlation score, while the scores for other types are relatively similar.We observe that for the answer type "job", the correlation score is quite low, primarily due to the ambiguity of multiple jobs in the gold answer, as the predicted answer can contain more or fewer jobs, making the judgment more difficult.B Results and AnalysesB.1 Comparing EM/F1 Scores and LLM-as-a-JudgeTable10presents LLM-as-a-judge scores for Mistral 7B v0.3, Llama 3.3 70B, and Qwen 2.5 72B across four datasets.Columns labeled Mistral, Llama, or Qwen show scores for false EM samples only, while Mistral-A, Llama-A, or Qwen-A show scores for all samples.
Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Neural Reading Comprehension and Beyond. Danqi Chen, 2018Stanford UniversityPhD thesis</p>
<p>What factors influence LLMs' judgments? a case study on question answering. Lei Chen, Bobo Li, Li Zheng, Haining Wang, Zixiang Meng, Runfeng Shi, Hao Fei, Jun Zhou, Fei Li, Chong Teng, Donghong Ji, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, Nianwen Xue, the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCLMay 2024</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.acl-long.870Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>QuAC: Question answering in context. Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-Tau Yih, Yejin Choi, Percy Liang, Luke Zettlemoyer, 10.18653/v1/D18-1241Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsOctober-November 2018</p>
<p>BoolQ: Exploring the surprising difficulty of natural yes/no questions. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova, 10.18653/v1/N19-1300Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational LinguisticsJune 20191Long and Short Papers</p>
<p>Quoref: A reading comprehension dataset with questions requiring coreferential reasoning. Pradeep Dasigi, Nelson F Liu, Ana Marasović, Noah A Smith, Matt Gardner, 10.18653/v1/D19-1606Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, 10.18653/v1/N19-1246Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational LinguisticsJune 20191</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, 10.1162/tacla00370Transactions of the Association for Computational Linguistics. 92021</p>
<p>The llama 3 herd of models. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv:2407.21783arXiv:2411.15594Lionel Ni, and Jian Guo. A survey on llm-as-a-judge. 2024. 2025</p>
<p>Teaching machines to read and comprehend. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, </p>
<p>Advances in Neural Information Processing Systems. C Cortes, N Lawrence, D Lee, M Sugiyama, R Garnett, Curran Associates, Inc201528</p>
<p>Deep read: A reading comprehension system. Lynette Hirschman, Marc Light, Eric Breck, John D Burger, 10.3115/1034678.1034731Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics. the 37th Annual Meeting of the Association for Computational LinguisticsCollege Park, Maryland, USAAssociation for Computational LinguisticsJune 1999</p>
<p>Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, Akiko Aizawa, 10.18653/v1/2020.coling-main.580Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, SpainDecember 2020International Committee on Computational Linguistics</p>
<p>R4C: A benchmark for evaluating RC systems to get the right answer for the right reason. Naoya Inoue, Pontus Stenetorp, Kentaro Inui, 10.18653/v1/2020.acl-main.602Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Renard Lélio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Sophia Subramanian, Yang, arXiv:2401.04088Mixtral of experts. Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed, 2024</p>
<p>Large language models are state-of-the-art evaluators of translation quality. Tom Kocmi, Christian Federmann, Proceedings of the 24th Annual Conference of the European Association for Machine Translation. the 24th Annual Conference of the European Association for Machine TranslationTampere, FinlandJune 2023European Association for Machine Translation</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, ( Shixiang, Machel Shane) Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Benchmarking cognitive biases in large language models as evaluators. Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae , Myung Kim, Dongyeop Kang, 10.18653/v1/2024.findings-acl.29Findings of the Association for Computational Linguistics: ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 2024</p>
<p>Leveraging large language models for NLG evaluation: Advances and challenges. Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Yuxuan Lai, Chongyang Tao, Shuai Ma, 10.18653/v1/2024.emnlp-main.896Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNovember 2024</p>
<p>LLMs as narcissistic evaluators: When ego inflates evaluation scores. Yiqi Liu, Nafise Moosavi, Chenghua Lin, 10.18653/v1/2024.findings-acl.753Findings of the Association for Computational Linguistics: ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 2024</p>
<p>Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation. Yixin Liu, Alex Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, Dragomir Radev, 10.18653/v1/2023.acl-long.228Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>LLM evaluators recognize and favor their own generations. Arjun Panickssery, R Samuel, Shi Bowman, Feng, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>note on regression and inheritance in the case of two parents. Karl Pearson, Vii, Proceedings of the Royal Society of London. 58347-3521895</p>
<p>Measuring and narrowing the compositionality gap in language models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, Mike Lewis, Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Is LLM-as-a-judge robust? investigating universal adversarial attacks on zero-shot LLM assessment. Adian Vyas Raina, Mark Liusie, Gales, 10.18653/v1/2024.emnlp-main.427Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNovember 2024</p>
<p>SQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, 10.18653/v1/D16-1264Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsNovember 2016</p>
<p>Know what you don't know: Unanswerable questions for SQuAD. Pranav Rajpurkar, Robin Jia, Percy Liang, 10.18653/v1/P18-2124Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational LinguisticsJuly 20182Short Papers)</p>
<p>CoQA: A conversational question answering challenge. Siva Reddy, Danqi Chen, Christopher D Manning, 10.1162/tacla00266Transactions of the Association for Computational Linguistics. 72019</p>
<p>Optimization-based prompt injection attack to llm-as-a-judge. Jiawen Shi, Zenghui Yuan, Yinuo Liu, Yue Huang, Pan Zhou, Lichao Sun, Neil Zhenqiang, Gong , 10.1145/3658644.3690291Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security, CCS '24. the 2024 on ACM SIGSAC Conference on Computer and Communications Security, CCS '24New York, NY, USA2024Association for Computing Machinery. ISBN 9798400706363</p>
<p>Towards better evaluation of instruction-following: A case-study in summarization. Ondrej Skopek, Rahul Aralikatte, Sian Gooding, Victor Carbune, 10.18653/v1/2023.conll-1.16Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL). Jing Jiang, David Reitter, Shumin Deng, the 27th Conference on Computational Natural Language Learning (CoNLL)SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Multilingual translation from denoising pre-training. Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan, 10.18653/v1/2021.findings-acl.304Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational LinguisticsAugust 2021</p>
<p>Improving open language models at a practical size. arXiv:2408.00118Gemma Team. Gemma. 22024</p>
<p>Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges. Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, Dieuwke Hupkes, arXiv:2406.126242025</p>
<p>NewsQA: A machine comprehension dataset. Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, Kaheer Suleman, 10.18653/v1/W17-2623Proceedings of the 2nd Workshop on Representation Learning for NLP. the 2nd Workshop on Representation Learning for NLPVancouver, CanadaAssociation for Computational LinguisticsAugust 2017</p>
<p>MuSiQue: Multihop questions via single-hop question composition. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, 10.1162/tacla00475Transactions of the Association for Computational Linguistics. 102022</p>
<p>Replacing judges with juries: Evaluating llm generations with a panel of diverse models. Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, Patrick Lewis, arXiv:2404.187962024</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, Zhifang Sui, 10.18653/v1/2024.acl-long.511Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 20241</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Constructing datasets for multi-hop reading comprehension across documents. Johannes Welbl, Pontus Stenetorp, Sebastian Riedel, 10.1162/tacla00021Transactions of the Association for Computational Linguistics. 62018</p>
<p>MMQA: Evaluating LLMs with multi-table multi-hop complex questions. Jian Wu, Linyi Yang, Dongyuan Li, Yuliang Ji, Manabu Okumura, Yue Zhang, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Large language models are diverse role-players for summarization evaluation. Ning Wu, Ming Gong, Linjun Shou, Shining Liang, Daxin Jiang, 10.1007/978-3-031-44693-1_54Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023. Foshan, China; Berlin, HeidelbergSpringer-VerlagOctober 12-15, 2023. 2023Proceedings, Part I</p>
<p>. An Yang, Baosong Yang, arXiv:2407.106712024Qwen2 technical report</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. An Yang, Baosong Yang, 10.18653/v1/D18-1259arXiv:2412.15115Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2025. October-November 2018Qwen2.5 technical report</p>
<p>Judging LLM-as-a-judge with MT-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>FanOutQA: A multihop, multi-document question answering benchmark for large language models. Andrew Zhu, Alyssa Hwang, Liam Dugan, Chris Callison-Burch, 10.18653/v1/2024.acl-short.2Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 20242Short Papers)</p>            </div>
        </div>

    </div>
</body>
</html>
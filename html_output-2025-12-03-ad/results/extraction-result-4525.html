<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4525 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4525</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4525</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-1ba3654e72f8e7432e7cdb40b6878be4a486570c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1ba3654e72f8e7432e7cdb40b6878be4a486570c" target="_blank">MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work frames experiment-guided ranking as a sequential decision-making problem and proposes an in-context reinforcement learning (ICRL) framework, and decomposes hypotheses into functional elements, clusters them by mechanistic roles, and prioritizes recombinations based on feedback.</p>
                <p><strong>Paper Abstract:</strong> Hypothesis ranking is vital for automated scientific discovery, especially in cost-intensive, throughput-limited natural science domains. Current methods focus on pre-experiment ranking, relying solely on language model reasoning without empirical feedback. We introduce experiment-guided ranking, which prioritizes hypotheses based on feedback from prior tests. Due to the impracticality of real experiments, we propose a simulator grounded in domain-specific concepts that models hypothesis performance as a function of similarity to a hidden ground truth, perturbed by noise. Validated against 124 hypotheses with experimentally reported outcomes, the simulator approximates real results with consistent trend alignment. Although deviations exist, they mimic wet-lab noise, promoting more robust ranking strategies. We frame experiment-guided ranking as a sequential decision-making problem and propose an in-context reinforcement learning (ICRL) framework. Our LLM-based policy decomposes hypotheses into functional elements, clusters them by mechanistic roles, and prioritizes recombinations based on feedback. Experiments show our approach significantly outperforms pre-experiment baselines and strong ablations. Our toolkit, comprising the simulator and ICRL framework, enables systematic research on experiment-guided ranking, with the policy serving as a strong proof of concept.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4525.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4525.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experiment-Guided Ranking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experiment-Guided Hypothesis Ranking (sequential decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A formal task formulation that treats hypothesis prioritization as a sequential decision problem: select hypotheses to test so as to identify the ground-truth hypothesis with the minimum number of costly experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Minimize Number of Trials (N_trials) Sequential Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>The task is evaluated by treating selection as a sequential decision process: at each timestep the agent selects a hypothesis, receives an experimental score from an executor E (real or simulated), and updates its state. The objective is to learn a policy π that minimizes the expected number of experimental trials required to locate the true best hypothesis h* (formally π* = argmin E_π[N_trials]). The state encodes cumulative analysis of past trials and the policy balances exploration/exploitation under a fixed experiment budget.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Sample efficiency (N_trials to find h*), ability to incorporate empirical feedback, robustness to noisy/misaligned feedback, interpretability of selection decisions</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-40-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural sciences (chemistry-focused in experiments; method applies across chemistry, materials, biology, physics)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Mechanistic hypotheses / experimental hypotheses (functional component-based mechanistic explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Applied on TOMATO-chem (51 problems, 64 hypotheses each). Baselines: Uninformed Search N_trials=32.500; Pre-Experiment Ranking N_trials=28.608. CSX-Rank (experiment-guided policy) achieved N_trials=15.196, substantially outperforming baselines and ablations (w/o Clustering 27.980; w/o Clustering & Analysis 35.627; progressively degraded variants 37.667).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: performance measured by the quantitative metric N_trials computed from simulator experiments; ablation comparisons used to assess components.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical evaluation in simulation (CSX-Sim) across TOMATO-chem dataset and controlled-noise experiments; ablation studies to validate contributions of clustering and analysis modules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on fidelity of experimental feedback (simulator or real); performance sensitive to distortions in similarity embedding φ(·); real wet-lab constraints may limit direct deployment; optimization objective (minimize N_trials) does not directly measure explanatory quality of theories.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>TOMATO-chem (used for ranking experiments; 51 problems with 64 hypotheses each)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4525.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4525.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CSX-Sim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CSX-Sim (component-based experimental feedback simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulator that models experimental outcomes by computing similarity between candidate hypotheses and a hidden ground-truth hypothesis via component decomposition and a correction/noise term, enabling efficient development and evaluation of experiment-guided ranking strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Simulator-based trend & accuracy evaluation (Spearman, PCI, RMSE, MSE, MAE, RMSLE)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>CSX-Sim maps hypotheses to a latent similarity score S(h|q; h*) in [0,1] derived from LLM-driven extraction of functional components, component weights, and pairwise mechanism similarities s_i, with a multiplicative indicator that enforces presence of 'critical' components. Simulated performance is S (or distance |S-1|). The simulator is evaluated by comparing its simulated scores to true experimental outcomes across a curated dataset: trend alignment (Spearman rank correlation), Perfect Consistency Indicator (PCI, count of samples with perfect rank agreement), predictive errors (RMSE, MSE, MAE, RMSLE), and statistical significance (bootstrap p-values).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Trend alignment (Spearman ρ), exact rank agreement (PCI), predictive accuracy (RMSE/MSE/MAE/RMSLE), robustness under controlled distortions, ablation sensitivity to modules (component extraction, critical points)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-40-mini (LLM used to implement simulator submodules)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (validated across organic, inorganic, analytical, polymeric and energy materials) but designed as a general natural-science tool</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Mechanistic / functional hypotheses (component-based mechanistic explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Validation was performed on 124 published hypotheses (30 research questions). CSX-Sim achieved Spearman ρ=0.960, PCI=26/30, RMSE=0.213. Compared to baseline 'Matched Score': ρ=0.843, PCI=12/30, RMSE=0.232. Additional error metrics: MSE=0.058, MAE=0.161, RMSLE=0.147. Ablations: w/o CriticalPoints ρ=0.950, PCI=23/30, RMSE=0.229; w/o ComponentExtraction ρ=0.864, PCI=12/30, RMSE=0.272. Bootstrap significance p<0.01 reported for trend agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated numeric evaluation against experimental outcomes (Spearman, RMSE etc.) plus a qualitative comparison of component-extraction outputs against a human chemistry expert for one illustrative problem (CSX-Sim captured 5/6 key points vs expert).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical validation versus 124 real wet-lab results (grouped into 30 questions), statistical tests (bootstrap p<0.01), ablation studies, function-form comparisons (linear / Gaussian / absolute / quadratic), and controlled-distortion experiments of φ(·).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Simulator depends on the A1/P1/D1 assumptions (local optimum, structural similarity implies similar behavior, and imperfect embeddings); not perfectly accurate—deviations resemble wet-lab noise; relies on quality of LLM-driven component extraction and weight assignment; may miss critical components (example: missed PVA in one case).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Curated dataset of 124 published chemical hypotheses (30 research questions) used for simulator validation</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4525.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4525.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CSX-Rank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CSX-Rank (ICRL clustering-based agentic policy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An in-context reinforcement learning (ICRL) agentic policy that decomposes hypotheses into functional components, clusters them by mechanistic role, and prioritizes recombinations informed by experimental feedback to minimize trials to discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ICRL policy evaluation via N_trials under simulated experimental feedback</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>CSX-Rank operates within a frozen LLM context window (no gradient updates): it extracts and classifies components (effective/uncertain/ineffective), clusters components into mechanism-based groups, selects promising clusters and hypotheses within them, runs experiments (via CSX-Sim or real E), analyzes outcomes, and updates a running summary/state. Evaluation measures how many experiments are needed to reach the ground truth across problems, and robustness under noise/distortions and ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Sample efficiency (N_trials), robustness across distortion noise levels (Simple/Medium/Complex), contribution of clustering and feedback-analysis modules (via ablation), interpretability of decision trail</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-40-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry and general natural sciences</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Mechanistic / hypothesis-ranking policies that target mechanistic explanations and experimental hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>On TOMATO-chem: CSX-Rank average N_trials=15.196 vs Pre-Experiment Ranking 28.608 and Uninformed Search 32.500. Under increasing φ(·) distortion: Simple noise N_trials=21.804, Medium=26.608, Complex=32.706; ablated variants perform worse across noise levels (e.g., w/o Clustering Complex=36.471). Ablation shows clustering and feedback analysis crucial (w/o Clustering N_trials=27.980; w/o Clustering & Analysis 35.627).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: evaluated in simulation across datasets and controlled-noise settings; interpretability presented qualitatively for human audit but no formal human rating of policy outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical simulation runs using CSX-Sim across TOMATO-chem; ablation studies; controlled-noise experiments to probe robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Policy effectiveness depends on simulator fidelity and quality of component decomposition; ICRL approach requires strong in-context reasoning ability from the frozen LLM; performance may deteriorate if key components are mis-extracted or if φ(·) distortions are severe.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>TOMATO-chem (51 scientific problems used for ranking evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4525.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4525.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Trend & Accuracy Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Trend alignment and predictive accuracy metrics (Spearman, PCI, RMSE, MSE, MAE, RMSLE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of quantitative metrics used to evaluate how well simulated or model-generated hypothesis scores match real experimental outcomes in relative ordering (trend) and absolute values (accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Spearman rank correlation; Perfect Consistency Indicator (PCI); RMSE; MSE; MAE; RMSLE</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Spearman ρ assesses monotonic agreement between simulated and experimental rankings per question group. PCI counts question groups with perfect rank agreement (ρ=1). RMSE/MSE/MAE/RMSLE measure absolute predictive error between simulated and experimental normalized scores. RMSLE focuses on relative/log-scale differences. These metrics are applied across groups of hypotheses (3–6 items per question in the 124-hypothesis validation) and aggregated to summarize simulator fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Relative ordering preservation (Spearman, PCI) and numeric predictive accuracy (RMSE/MSE/MAE/RMSLE); statistical significance via bootstrap testing.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (used in simulator validation) but generic to other empirical evaluation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Predictive matching of experimental hypotheses and outcomes (evaluating empirical adequacy of LLM outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>CSX-Sim: Spearman ρ=0.960 (mean), PCI=26/30, RMSE=0.213, MSE=0.058, MAE=0.161, RMSLE=0.147. Baseline 'Matched Score' gave ρ=0.843, PCI=12/30, RMSE=0.232. Ablations and function-choice comparisons reported in paper (e.g., Linear function ρ=0.9708 RMSE=0.1959, Gaussian ρ=0.9600 RMSE=0.2147).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric calculations; bootstrap used for significance (p<0.01). For one case, human expert judged component lists for qualitative comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Direct comparison to curated experimental outcomes (124 experiments), bootstrap significance testing, ablations showing metric sensitivity to simulator components.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Metrics capture trend and numeric fit but not deeper explanatory virtues (e.g., causal correctness, novelty, parsimony); normalization of diverse experimental outcomes may mask domain-specific nuances; perfect trend alignment (PCI) is a strict but coarse measure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4525.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4525.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Component Similarity Scoring S(h)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Component-weighted Mechanism Similarity Score S(h | q; h*) with CriticalPoints factor</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured scoring function that aggregates per-component mechanism similarities into a global similarity S in [0,1], multiplied by an indicator that enforces the presence of labeled critical components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Component-weighted similarity with critical-component correction</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Each hypothesis is decomposed into K functional components. For each component i, a mechanism similarity s_i ∈ [0,1] to the corresponding ground-truth component is computed. Components in the ground truth are assigned nonnegative weights w_i summing to 1 and a subset C labeled as critical. Global similarity is S(h)= (∏_{i∈C} 1_{s_i>0}) * (Σ_{i=1}^K w_i s_i). The critical-point multiplicative indicator (Correction Factor) outputs 0 if any critical component is absent, forcing S=0. Distance to ground truth is |S−1| and used to produce simulated performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Ability of S to reflect functional alignment (used to drive simulated scores), sensitivity to presence/absence of critical components, predictive contribution validated by ablation (w/o CriticalPoints degrades metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-40-mini (used to extract components and infer mechanism similarities)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (design and evaluation of chemical hypotheses) but applicable to mechanistic hypotheses across natural sciences</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Mechanistic and mechanistic-component explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>S guarantees S(h*)=1 when components match. Ablation removing CriticalPoints increased errors: full CSX-Sim vs w/o CriticalPoints: Spearman 0.960 vs 0.950, PCI 26/30 vs 23/30, RMSE 0.213 vs 0.229; removing component extraction worsened results significantly (ρ=0.864, RMSE=0.272). Example human comparison: CSX-Sim captured 5 of 6 expert-identified key points in an illustrative thermocell problem, missing PVA.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated numeric validation through metrics and ablations, plus qualitative human expert comparison of extracted components for at least one chemistry example.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Ablation experiments demonstrated importance of component extraction and critical-point indicator, plus direct comparison with human expert extraction in example.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on correctness of LLM-driven component extraction and weighting; binary enforcement of critical components can be brittle if human-labeled criticality is subjective; mislabeling or missed components can zero out score despite partial functional alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4525.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4525.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distortion / Noise Framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Controlled Distortion Framework for φ(·) (local maxima/minima, plateaus, cliffs; noise levels)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework for injecting realistic distortions into practical embeddings φ(·) to simulate common empirical pitfalls (local maxima/minima, plateaus, cliffs) at graded complexity levels (Simple, Moderate, Complex) to test robustness of ranking policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Controlled-noise robustness evaluation (Simple / Medium / Complex)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Distortions representative of common domain misunderstandings are introduced into the practical embedding φ(·). Three distortion types are modeled: local maxima/minima, plateaus, and cliffs; three noise complexity levels are defined by counts of distortion events (Simple: 0–10 local extrema, 0–2 plateaus/cliffs; Medium: 0–30 / 0–6 / 0–6; Complex: ≥30 / ≥3 / ≥3). Policies are evaluated by N_trials under each noise setting to quantify degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Robustness of ranking policies measured by increases in N_trials under higher noise; ability of clustering & feedback analysis to mitigate misleading signals.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (motivated by chemistry examples) but applicable to other experimental domains</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation of optimization/search strategies over hypothesis spaces under noisy feedback</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>CSX-Rank performance degraded gracefully but remained superior to ablations: Simple noise N_trials=21.804, Medium=26.608, Complex=32.706; w/o Clustering and other ablations performed substantially worse across noise levels (Complex: 36.471 and 40.451 respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated simulation experiments using the controlled-distortion versions of CSX-Sim.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical simulation comparisons across noise levels and ablations; noise design informed by discussions with chemistry PhD students and statistical analysis of wet-lab deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Distortion models are approximations; mapping the real distribution of wet-lab distortions to these synthetic categories may be imperfect; extreme distortions can mislead policies developed in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4525.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4525.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human vs Simulator Component Comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Component Extraction Agreement (CSX-Sim vs Human Expert)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A direct qualitative comparison between CSX-Sim's extracted functional components and a human chemistry expert's extraction on an illustrative thermocell hypothesis to assess fidelity of mechanistic interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Component extraction overlap / qualitative agreement</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For a concrete chemistry example, both a human chemistry expert and CSX-Sim produced lists of key functional components and mechanistic roles. Agreement was measured qualitatively by counting matching key points identified (e.g., CSX-Sim found 5 out of 6 expert-identified key points). The qualitative assessment highlights whether the simulator's mechanistic parsing aligns with human domain reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Component recall/overlap relative to human expert (number of matched key components), correctness of mechanistic role attribution, and whether critical components (e.g., matrix, redox pair) were identified.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-40-mini (used in CSX-Sim extraction pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (thermocell illustrative example)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Mechanistic component explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>In the provided thermocell case, the human expert listed 6 key points; CSX-Sim identified 5 of the 6 (missed 'PVA as hydrogel matrix') and produced similar mechanistic role descriptions. This single-case comparison supports high but not perfect alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: human expert produced ground-truth component list and CSX-Sim output compared qualitatively; no formal inter-rater or large-scale human study reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Single-case qualitative comparison in Appendix A; broader simulator validation used experimental outcome comparisons (quantitative).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Comparison limited to illustrative example(s) rather than large-scale human annotation study; human labeling of components can be subjective and dependent on prompt framing; missing key components (as in example) can materially affect simulated scores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses <em>(Rating: 2)</em></li>
                <li>Large language models for automated open-domain scientific hypotheses discovery <em>(Rating: 2)</em></li>
                <li>Biodiscoveryagent: An ai agent for designing genetic perturbation experiments <em>(Rating: 2)</em></li>
                <li>Mathematical discoveries from program search with large language models <em>(Rating: 1)</em></li>
                <li>Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4525",
    "paper_id": "paper-1ba3654e72f8e7432e7cdb40b6878be4a486570c",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "Experiment-Guided Ranking",
            "name_full": "Experiment-Guided Hypothesis Ranking (sequential decision-making)",
            "brief_description": "A formal task formulation that treats hypothesis prioritization as a sequential decision problem: select hypotheses to test so as to identify the ground-truth hypothesis with the minimum number of costly experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Minimize Number of Trials (N_trials) Sequential Evaluation",
            "evaluation_method_description": "The task is evaluated by treating selection as a sequential decision process: at each timestep the agent selects a hypothesis, receives an experimental score from an executor E (real or simulated), and updates its state. The objective is to learn a policy π that minimizes the expected number of experimental trials required to locate the true best hypothesis h* (formally π* = argmin E_π[N_trials]). The state encodes cumulative analysis of past trials and the policy balances exploration/exploitation under a fixed experiment budget.",
            "evaluation_criteria": "Sample efficiency (N_trials to find h*), ability to incorporate empirical feedback, robustness to noisy/misaligned feedback, interpretability of selection decisions",
            "model_name": "GPT-40-mini",
            "model_size": null,
            "scientific_domain": "Natural sciences (chemistry-focused in experiments; method applies across chemistry, materials, biology, physics)",
            "theory_type": "Mechanistic hypotheses / experimental hypotheses (functional component-based mechanistic explanations)",
            "human_comparison": false,
            "evaluation_results": "Applied on TOMATO-chem (51 problems, 64 hypotheses each). Baselines: Uninformed Search N_trials=32.500; Pre-Experiment Ranking N_trials=28.608. CSX-Rank (experiment-guided policy) achieved N_trials=15.196, substantially outperforming baselines and ablations (w/o Clustering 27.980; w/o Clustering & Analysis 35.627; progressively degraded variants 37.667).",
            "automated_vs_human_evaluation": "Automated: performance measured by the quantitative metric N_trials computed from simulator experiments; ablation comparisons used to assess components.",
            "validation_method": "Empirical evaluation in simulation (CSX-Sim) across TOMATO-chem dataset and controlled-noise experiments; ablation studies to validate contributions of clustering and analysis modules.",
            "limitations_challenges": "Relies on fidelity of experimental feedback (simulator or real); performance sensitive to distortions in similarity embedding φ(·); real wet-lab constraints may limit direct deployment; optimization objective (minimize N_trials) does not directly measure explanatory quality of theories.",
            "benchmark_dataset": "TOMATO-chem (used for ranking experiments; 51 problems with 64 hypotheses each)",
            "uuid": "e4525.0",
            "source_info": {
                "paper_title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "CSX-Sim",
            "name_full": "CSX-Sim (component-based experimental feedback simulator)",
            "brief_description": "A simulator that models experimental outcomes by computing similarity between candidate hypotheses and a hidden ground-truth hypothesis via component decomposition and a correction/noise term, enabling efficient development and evaluation of experiment-guided ranking strategies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Simulator-based trend & accuracy evaluation (Spearman, PCI, RMSE, MSE, MAE, RMSLE)",
            "evaluation_method_description": "CSX-Sim maps hypotheses to a latent similarity score S(h|q; h*) in [0,1] derived from LLM-driven extraction of functional components, component weights, and pairwise mechanism similarities s_i, with a multiplicative indicator that enforces presence of 'critical' components. Simulated performance is S (or distance |S-1|). The simulator is evaluated by comparing its simulated scores to true experimental outcomes across a curated dataset: trend alignment (Spearman rank correlation), Perfect Consistency Indicator (PCI, count of samples with perfect rank agreement), predictive errors (RMSE, MSE, MAE, RMSLE), and statistical significance (bootstrap p-values).",
            "evaluation_criteria": "Trend alignment (Spearman ρ), exact rank agreement (PCI), predictive accuracy (RMSE/MSE/MAE/RMSLE), robustness under controlled distortions, ablation sensitivity to modules (component extraction, critical points)",
            "model_name": "GPT-40-mini (LLM used to implement simulator submodules)",
            "model_size": null,
            "scientific_domain": "Chemistry (validated across organic, inorganic, analytical, polymeric and energy materials) but designed as a general natural-science tool",
            "theory_type": "Mechanistic / functional hypotheses (component-based mechanistic explanations)",
            "human_comparison": true,
            "evaluation_results": "Validation was performed on 124 published hypotheses (30 research questions). CSX-Sim achieved Spearman ρ=0.960, PCI=26/30, RMSE=0.213. Compared to baseline 'Matched Score': ρ=0.843, PCI=12/30, RMSE=0.232. Additional error metrics: MSE=0.058, MAE=0.161, RMSLE=0.147. Ablations: w/o CriticalPoints ρ=0.950, PCI=23/30, RMSE=0.229; w/o ComponentExtraction ρ=0.864, PCI=12/30, RMSE=0.272. Bootstrap significance p&lt;0.01 reported for trend agreement.",
            "automated_vs_human_evaluation": "Automated numeric evaluation against experimental outcomes (Spearman, RMSE etc.) plus a qualitative comparison of component-extraction outputs against a human chemistry expert for one illustrative problem (CSX-Sim captured 5/6 key points vs expert).",
            "validation_method": "Empirical validation versus 124 real wet-lab results (grouped into 30 questions), statistical tests (bootstrap p&lt;0.01), ablation studies, function-form comparisons (linear / Gaussian / absolute / quadratic), and controlled-distortion experiments of φ(·).",
            "limitations_challenges": "Simulator depends on the A1/P1/D1 assumptions (local optimum, structural similarity implies similar behavior, and imperfect embeddings); not perfectly accurate—deviations resemble wet-lab noise; relies on quality of LLM-driven component extraction and weight assignment; may miss critical components (example: missed PVA in one case).",
            "benchmark_dataset": "Curated dataset of 124 published chemical hypotheses (30 research questions) used for simulator validation",
            "uuid": "e4525.1",
            "source_info": {
                "paper_title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "CSX-Rank",
            "name_full": "CSX-Rank (ICRL clustering-based agentic policy)",
            "brief_description": "An in-context reinforcement learning (ICRL) agentic policy that decomposes hypotheses into functional components, clusters them by mechanistic role, and prioritizes recombinations informed by experimental feedback to minimize trials to discovery.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "ICRL policy evaluation via N_trials under simulated experimental feedback",
            "evaluation_method_description": "CSX-Rank operates within a frozen LLM context window (no gradient updates): it extracts and classifies components (effective/uncertain/ineffective), clusters components into mechanism-based groups, selects promising clusters and hypotheses within them, runs experiments (via CSX-Sim or real E), analyzes outcomes, and updates a running summary/state. Evaluation measures how many experiments are needed to reach the ground truth across problems, and robustness under noise/distortions and ablations.",
            "evaluation_criteria": "Sample efficiency (N_trials), robustness across distortion noise levels (Simple/Medium/Complex), contribution of clustering and feedback-analysis modules (via ablation), interpretability of decision trail",
            "model_name": "GPT-40-mini",
            "model_size": null,
            "scientific_domain": "Chemistry and general natural sciences",
            "theory_type": "Mechanistic / hypothesis-ranking policies that target mechanistic explanations and experimental hypotheses",
            "human_comparison": false,
            "evaluation_results": "On TOMATO-chem: CSX-Rank average N_trials=15.196 vs Pre-Experiment Ranking 28.608 and Uninformed Search 32.500. Under increasing φ(·) distortion: Simple noise N_trials=21.804, Medium=26.608, Complex=32.706; ablated variants perform worse across noise levels (e.g., w/o Clustering Complex=36.471). Ablation shows clustering and feedback analysis crucial (w/o Clustering N_trials=27.980; w/o Clustering & Analysis 35.627).",
            "automated_vs_human_evaluation": "Automated: evaluated in simulation across datasets and controlled-noise settings; interpretability presented qualitatively for human audit but no formal human rating of policy outputs.",
            "validation_method": "Empirical simulation runs using CSX-Sim across TOMATO-chem; ablation studies; controlled-noise experiments to probe robustness.",
            "limitations_challenges": "Policy effectiveness depends on simulator fidelity and quality of component decomposition; ICRL approach requires strong in-context reasoning ability from the frozen LLM; performance may deteriorate if key components are mis-extracted or if φ(·) distortions are severe.",
            "benchmark_dataset": "TOMATO-chem (51 scientific problems used for ranking evaluation)",
            "uuid": "e4525.2",
            "source_info": {
                "paper_title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Trend & Accuracy Metrics",
            "name_full": "Trend alignment and predictive accuracy metrics (Spearman, PCI, RMSE, MSE, MAE, RMSLE)",
            "brief_description": "A set of quantitative metrics used to evaluate how well simulated or model-generated hypothesis scores match real experimental outcomes in relative ordering (trend) and absolute values (accuracy).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Spearman rank correlation; Perfect Consistency Indicator (PCI); RMSE; MSE; MAE; RMSLE",
            "evaluation_method_description": "Spearman ρ assesses monotonic agreement between simulated and experimental rankings per question group. PCI counts question groups with perfect rank agreement (ρ=1). RMSE/MSE/MAE/RMSLE measure absolute predictive error between simulated and experimental normalized scores. RMSLE focuses on relative/log-scale differences. These metrics are applied across groups of hypotheses (3–6 items per question in the 124-hypothesis validation) and aggregated to summarize simulator fidelity.",
            "evaluation_criteria": "Relative ordering preservation (Spearman, PCI) and numeric predictive accuracy (RMSE/MSE/MAE/RMSLE); statistical significance via bootstrap testing.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Chemistry (used in simulator validation) but generic to other empirical evaluation tasks",
            "theory_type": "Predictive matching of experimental hypotheses and outcomes (evaluating empirical adequacy of LLM outputs)",
            "human_comparison": false,
            "evaluation_results": "CSX-Sim: Spearman ρ=0.960 (mean), PCI=26/30, RMSE=0.213, MSE=0.058, MAE=0.161, RMSLE=0.147. Baseline 'Matched Score' gave ρ=0.843, PCI=12/30, RMSE=0.232. Ablations and function-choice comparisons reported in paper (e.g., Linear function ρ=0.9708 RMSE=0.1959, Gaussian ρ=0.9600 RMSE=0.2147).",
            "automated_vs_human_evaluation": "Automated metric calculations; bootstrap used for significance (p&lt;0.01). For one case, human expert judged component lists for qualitative comparison.",
            "validation_method": "Direct comparison to curated experimental outcomes (124 experiments), bootstrap significance testing, ablations showing metric sensitivity to simulator components.",
            "limitations_challenges": "Metrics capture trend and numeric fit but not deeper explanatory virtues (e.g., causal correctness, novelty, parsimony); normalization of diverse experimental outcomes may mask domain-specific nuances; perfect trend alignment (PCI) is a strict but coarse measure.",
            "uuid": "e4525.3",
            "source_info": {
                "paper_title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Component Similarity Scoring S(h)",
            "name_full": "Component-weighted Mechanism Similarity Score S(h | q; h*) with CriticalPoints factor",
            "brief_description": "A structured scoring function that aggregates per-component mechanism similarities into a global similarity S in [0,1], multiplied by an indicator that enforces the presence of labeled critical components.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Component-weighted similarity with critical-component correction",
            "evaluation_method_description": "Each hypothesis is decomposed into K functional components. For each component i, a mechanism similarity s_i ∈ [0,1] to the corresponding ground-truth component is computed. Components in the ground truth are assigned nonnegative weights w_i summing to 1 and a subset C labeled as critical. Global similarity is S(h)= (∏_{i∈C} 1_{s_i&gt;0}) * (Σ_{i=1}^K w_i s_i). The critical-point multiplicative indicator (Correction Factor) outputs 0 if any critical component is absent, forcing S=0. Distance to ground truth is |S−1| and used to produce simulated performance.",
            "evaluation_criteria": "Ability of S to reflect functional alignment (used to drive simulated scores), sensitivity to presence/absence of critical components, predictive contribution validated by ablation (w/o CriticalPoints degrades metrics).",
            "model_name": "GPT-40-mini (used to extract components and infer mechanism similarities)",
            "model_size": null,
            "scientific_domain": "Chemistry (design and evaluation of chemical hypotheses) but applicable to mechanistic hypotheses across natural sciences",
            "theory_type": "Mechanistic and mechanistic-component explanations",
            "human_comparison": true,
            "evaluation_results": "S guarantees S(h*)=1 when components match. Ablation removing CriticalPoints increased errors: full CSX-Sim vs w/o CriticalPoints: Spearman 0.960 vs 0.950, PCI 26/30 vs 23/30, RMSE 0.213 vs 0.229; removing component extraction worsened results significantly (ρ=0.864, RMSE=0.272). Example human comparison: CSX-Sim captured 5 of 6 expert-identified key points in an illustrative thermocell problem, missing PVA.",
            "automated_vs_human_evaluation": "Automated numeric validation through metrics and ablations, plus qualitative human expert comparison of extracted components for at least one chemistry example.",
            "validation_method": "Ablation experiments demonstrated importance of component extraction and critical-point indicator, plus direct comparison with human expert extraction in example.",
            "limitations_challenges": "Relies on correctness of LLM-driven component extraction and weighting; binary enforcement of critical components can be brittle if human-labeled criticality is subjective; mislabeling or missed components can zero out score despite partial functional alignment.",
            "uuid": "e4525.4",
            "source_info": {
                "paper_title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Distortion / Noise Framework",
            "name_full": "Controlled Distortion Framework for φ(·) (local maxima/minima, plateaus, cliffs; noise levels)",
            "brief_description": "A framework for injecting realistic distortions into practical embeddings φ(·) to simulate common empirical pitfalls (local maxima/minima, plateaus, cliffs) at graded complexity levels (Simple, Moderate, Complex) to test robustness of ranking policies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Controlled-noise robustness evaluation (Simple / Medium / Complex)",
            "evaluation_method_description": "Distortions representative of common domain misunderstandings are introduced into the practical embedding φ(·). Three distortion types are modeled: local maxima/minima, plateaus, and cliffs; three noise complexity levels are defined by counts of distortion events (Simple: 0–10 local extrema, 0–2 plateaus/cliffs; Medium: 0–30 / 0–6 / 0–6; Complex: ≥30 / ≥3 / ≥3). Policies are evaluated by N_trials under each noise setting to quantify degradation.",
            "evaluation_criteria": "Robustness of ranking policies measured by increases in N_trials under higher noise; ability of clustering & feedback analysis to mitigate misleading signals.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Chemistry (motivated by chemistry examples) but applicable to other experimental domains",
            "theory_type": "Evaluation of optimization/search strategies over hypothesis spaces under noisy feedback",
            "human_comparison": false,
            "evaluation_results": "CSX-Rank performance degraded gracefully but remained superior to ablations: Simple noise N_trials=21.804, Medium=26.608, Complex=32.706; w/o Clustering and other ablations performed substantially worse across noise levels (Complex: 36.471 and 40.451 respectively).",
            "automated_vs_human_evaluation": "Automated simulation experiments using the controlled-distortion versions of CSX-Sim.",
            "validation_method": "Empirical simulation comparisons across noise levels and ablations; noise design informed by discussions with chemistry PhD students and statistical analysis of wet-lab deviations.",
            "limitations_challenges": "Distortion models are approximations; mapping the real distribution of wet-lab distortions to these synthetic categories may be imperfect; extreme distortions can mislead policies developed in simulation.",
            "uuid": "e4525.5",
            "source_info": {
                "paper_title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Human vs Simulator Component Comparison",
            "name_full": "Component Extraction Agreement (CSX-Sim vs Human Expert)",
            "brief_description": "A direct qualitative comparison between CSX-Sim's extracted functional components and a human chemistry expert's extraction on an illustrative thermocell hypothesis to assess fidelity of mechanistic interpretation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Component extraction overlap / qualitative agreement",
            "evaluation_method_description": "For a concrete chemistry example, both a human chemistry expert and CSX-Sim produced lists of key functional components and mechanistic roles. Agreement was measured qualitatively by counting matching key points identified (e.g., CSX-Sim found 5 out of 6 expert-identified key points). The qualitative assessment highlights whether the simulator's mechanistic parsing aligns with human domain reasoning.",
            "evaluation_criteria": "Component recall/overlap relative to human expert (number of matched key components), correctness of mechanistic role attribution, and whether critical components (e.g., matrix, redox pair) were identified.",
            "model_name": "GPT-40-mini (used in CSX-Sim extraction pipeline)",
            "model_size": null,
            "scientific_domain": "Chemistry (thermocell illustrative example)",
            "theory_type": "Mechanistic component explanations",
            "human_comparison": true,
            "evaluation_results": "In the provided thermocell case, the human expert listed 6 key points; CSX-Sim identified 5 of the 6 (missed 'PVA as hydrogel matrix') and produced similar mechanistic role descriptions. This single-case comparison supports high but not perfect alignment.",
            "automated_vs_human_evaluation": "Hybrid: human expert produced ground-truth component list and CSX-Sim output compared qualitatively; no formal inter-rater or large-scale human study reported.",
            "validation_method": "Single-case qualitative comparison in Appendix A; broader simulator validation used experimental outcome comparisons (quantitative).",
            "limitations_challenges": "Comparison limited to illustrative example(s) rather than large-scale human annotation study; human labeling of components can be subjective and dependent on prompt framing; missing key components (as in example) can materially affect simulated scores.",
            "uuid": "e4525.6",
            "source_info": {
                "paper_title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses",
            "rating": 2
        },
        {
            "paper_title": "Large language models for automated open-domain scientific hypotheses discovery",
            "rating": 2
        },
        {
            "paper_title": "Biodiscoveryagent: An ai agent for designing genetic perturbation experiments",
            "rating": 2
        },
        {
            "paper_title": "Mathematical discoveries from program search with large language models",
            "rating": 1
        },
        {
            "paper_title": "Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement",
            "rating": 1
        }
    ],
    "cost": 0.016308749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MOOSE-Chem3: Toward Experiment-Guided HYPOTHESIS RANKING VIA SIMULATED EXPERIMENTAL FEEDBACK</h1>
<p>Wanhao Liu ${ }^{1,2 <em>}$, Zonglin Yang ${ }^{3,2 </em>}$, Jue Wang ${ }^{1}$, Lidong Bing ${ }^{4}$, Di Zhang ${ }^{2}$, Dongzhan Zhou ${ }^{2}$, Yuqiang $\mathbf{L i}^{2}$, Houqiang $\mathbf{L i}^{1}$, Erik Cambria ${ }^{3}$, Wanli Ouyang ${ }^{2}$<br>${ }^{1}$ University of Science and Technology of China ${ }^{2}$ Shanghai Artificial Intelligence Laboratory<br>${ }^{3}$ Nanyang Technological University ${ }^{4}$ MiroMind<br>{liuwanhao, lihq}@mail.ustc.edu.cn, {zonglin001, cambria}@ntu.edu.sg<br>{liyuqiang, ouyangwanli}@pjlab.org.cn</p>
<h4>Abstract</h4>
<p>Hypothesis ranking is a crucial component of automated scientific discovery, particularly in natural sciences where wet-lab experiments are costly and throughputlimited. Existing approaches focus on pre-experiment ranking, relying solely on a language model's internal reasoning without incorporating empirical outcomes. We introduce the task of experiment-guided ranking, which prioritizes hypotheses based on feedback from previously tested ones. However, developing such strategies in natural science domains is challenging due to the impractical requirement of repeatedly conducting real experiments. To address this, we revisit the core purpose of real experiments: to provide feedback on both the groundtruth hypothesis and the surrounding hypotheses that form the path toward it. This motivates our alternative: a simulator grounded in three domain-informed conceptual foundations, modeling hypothesis performance as a function of similarity to a known ground truth, perturbed by noise. While the groundtruth is pre-specified, it remains hidden from the ranking agent, enabling faithful evaluation of policies that navigate toward it. Validated against 124 hypotheses with experimentally reported outcomes, the simulator approximates real experimental results with consistent trend alignment. Though not perfectly accurate, its deviations resemble wet-lab noise and can foster more robust ranking strategies. We formulate experiment-guided ranking as a sequential decision-making problem and propose an in-context reinforcement learning (ICRL) framework. Within this framework, we introduce an LLM-based agentic policy that decomposes hypotheses into functional elements, clusters them by shared mechanistic roles, and prioritizes recombinations of promising elements based on feedback. Experiments show that our method significantly outperforms pre-experiment baselines and strong ablations. Our toolkit-comprising the simulator and ICRL framework-enables systematic research on experiment-guided ranking, with our policy serving as a strong proof of concept. ${ }^{1}$</p>
<h2>1 INTRODUCTION</h2>
<p>Scientific discovery plays a foundational role in advancing human society (Coccia, 2019). Recent progress in large language models (LLMs) has sparked growing interest in automating parts of this scientific process (Luo et al., 2025). Among these, one of the most critical stages is hypothesis ranking: given a large set of automatically generated hypotheses (e.g., by AI), which one should be tested in a real experiment first? This question is particularly important in natural science domains, where wet-lab experiments are costly and throughput-limited, requiring prioritization strategies that maximize discovery efficiency under strict experimental budgets.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of ranking strategies. Pre-experiment ranking is stateless and ignores feedback. Experiment-guided ranking with real experiments is stateful but infeasible to scale. Our simulator enables efficient development of ranking policies through simulated feedback before real deployment.</p>
<p>Existing work on hypothesis ranking (Yang et al., 2025; Si et al., 2024) primarily relies on evaluations based solely on a language model's internal reasoning, without incorporating any empirical feedback. We refer to this as pre-experiment ranking. While efficient, this approach overlooks the iterative, feedback-driven nature of real-world experimentation.</p>
<p>In contrast, we introduce the task of experiment-guided ranking, which prioritizes hypotheses for the next round of experimentation based on outcomes from previously tested ones. Rather than evaluating all candidates upfront, this approach dynamically adjusts prioritization as new experimental results become available. However, in natural science domains such as chemistry, materials science, and biology, conducting iterative experiments at scale-as required by experimentguided ranking-is often infeasible due to the high cost, long duration, and limited throughput of real-world experimentation. This lack of scalable feedback limits progress in developing and evaluating experiment-guided ranking strategies.</p>
<p>To address this challenge, we revisit the core purpose of real experiments: not only to validate a ground-truth hypothesis, but also to provide feedback on nearby hypotheses that form the path toward it. This motivates our alternative: a simulator that approximates experimental feedback in a local neighborhood of hypothesis space, enabling the development and evaluation of experiment-guided ranking strategies.</p>
<p>Our simulator is grounded in three conceptual foundations, reflecting the universal natural-science principle that structural similarity implies similar behavior (Callister \&amp; Rethwisch, 1999; Hansch et al., 1995; Wiley, 1986; Alberts et al., 2015). A1 (Local Optimum Assumption) states that a groundtruth hypothesis represents a dominant local optimum within its sufficiently local neighborhood. P1 (Scientific Principle) holds that greater structural or functional similarity yields more similar outcomes. D1 (Logical Deduction) follows that, because similarity representations are imperfect, the observed performance landscape deviates from the ideal implied by A1 and P1.</p>
<p>We formalize these conceptual foundations and construct a simulator that models hypothesis performance as a function of distance to a hidden ground-truth hypothesis. Although the ground truth is known to the simulator, it remains hidden from the ranking policy-enabling rigorous evaluation of strategies that must infer it through limited feedback. To validate the simulator, we curate a dataset of 124 hypotheses with experimentally reported outcomes from the literature. Our simulator demonstrates high trend alignment and predictive accuracy in approximating real experimental outcomes. It also outperforms strong baselines adapted from prior work (Yang et al., 2025), further supporting its utility as a research tool for developing and evaluating experiment-guided ranking strategies. Though not perfectly accurate, its deviations resemble the noise observed in real wet-lab experiments and can foster more robust ranking strategies.</p>
<p>Building on this foundation, we develop an in-context reinforcement learning (ICRL) framework for experiment-guided hypothesis ranking. Within this framework, we instantiate a clustering-based agentic policy that decomposes hypotheses into functional components and groups them by shared mechanistic roles. After each experimental trial, the agent analyzes the tested hypothesis to infer which components contributed to its performance, then prioritizes untested hypotheses that incorporate the most promising functional elements. This enables efficient transfer of insights across structurally related candidates and helps navigate the hypothesis space more effectively. Experiments show that this policy significantly outperforms pre-experiment baselines and strong ablations. Combined with the simulator, our ICRL framework forms a general-purpose toolkit for studying experiment-guided ranking strategies, with our policy serving as a strong proof of concept. Figure 1 provides an overview of the three paradigms: pre-experiment ranking, experiment-guided ranking with real experiments, and our simulator-driven approach for developing ranking policies.</p>
<p>Overall, the contributions of this paper are:</p>
<ul>
<li>We formalize the task of experiment-guided ranking and highlight a key bottleneck in the natural sciences: the lack of scalable access to wet-lab experimental feedback. To address this, we propose the use of simulators and release a curated dataset of 124 scientific hypotheses with annotated performance collected from the literature.</li>
<li>We introduce three conceptual foundations for simulating experimental feedback, formalize the simulation process mathematically, and construct a high-fidelity simulator that approximates real wet-lab outcomes under these foundations.</li>
<li>We present a clustering-based agentic ranking policy implemented within our ICRL framework. It generalizes from limited feedback and outperforms both pre-experiment baselines and ablation variants.</li>
</ul>
<h1>2 Methodology of Simulator Construction</h1>
<h3>2.1 Conceptual Foundations and Formalization</h3>
<p>Our simulator construction is guided by three conceptual foundations-one assumption, one scientific principle, and one logical deduction-grounded in established principles of the natural sciences. Together, these provide a principled basis for modeling experimental outcomes of untested hypotheses, enabling systematic investigation of experiment-guided ranking strategies.</p>
<h3>2.1.1 Conceptual Foundations</h3>
<p>We posit that real experimental feedback within a hypothesis space can be simulated under the following conceptual foundations ( $A 1-P 1-D 1)$ :</p>
<ol>
<li>(A1: Local Optimum Assumption) A ground-truth hypothesis represents a dominant local optimum within its sufficiently local neighborhood of the hypothesis space.</li>
<li>(P1: Scientific Principle) Hypotheses that are more similar in their underlying structure or function tend to yield more similar experimental outcomes.</li>
<li>(D1: Logical Deduction) In practice, representations of hypothesis similarity are imperfect proxies, so the resulting performance landscape deviates from the ideal implied by $A 1$ and $P 1$, producing distortions such as noise, spurious local optima, or unexpected valleys.
$A 1, P 1$, and $D 1$ are all reasonable and sufficiently grounded. $P 1$ reflects the fundamental axiom that "structure determines properties, and properties determine outcome," which underpins multiple disciplines: molecular structure and material function in Chemistry \&amp; Materials Science (Callister \&amp; Rethwisch, 1999; Hansch et al., 1995), crystal structure and physical properties in Physics (Wiley, 1986), and protein structure and biological function in Biology (Alberts et al., 2015). D1 follows logically from $A 1$ and $P 1$, since any practical representation of hypothesis similarity must introduce distortions. $A 1$ is mostly valid but not guaranteed: even within a sufficiently small neighborhood, the labeled ground-truth hypothesis may not be the strict local optimum, as there could exist another hypothesis in that region with higher performance. This limitation, however, does not affect the simulator's role in developing ranking policies, whose goal is to recover the labeled ground truth.</li>
</ol>
<p>When deployed in real experiments, any superior hypotheses beyond the labeled ground truth would be directly revealed, ensuring that policies developed under a simulator supported by $A 1$ remain effective in practice.
<img alt="img-1.jpeg" src="img-1.jpeg" />
(a) Idealized performance landscape $(A 1+P 1))$.
<img alt="img-2.jpeg" src="img-2.jpeg" />
(b) Realistic performance landscape $(A 1+P 1+D 1)$.
<img alt="img-3.jpeg" src="img-3.jpeg" />
(c) Deviations from imperfect closeness estimation ( $D 1$ ).</p>
<p>Figure 2: Illustration of the three conceptual foundations ( $A 1-P 1-D 1$ ) for simulator construction.</p>
<p>Figure 2 visually illustrates these conceptual foundations. In the ideal scenario (Figure 2a), A1 ensures the presence of a dominant local optimum, while $P 1$ enforces that hypotheses closer in structure or function to this optimum yield more similar outcomes. Together, these yield a smooth, unimodal performance landscape, where Euclidean distance in hypothesis space faithfully reflects structural and functional similarity. However, practical scenarios differ substantially, as the measured distance ("closeness") between hypotheses-whether estimated by scientists or LLMs-may not faithfully capture structural and functional similarity. For instance, a chemical hypothesis might contain a useful functional component whose contribution is underrepresented, placing it farther from the dominant peak than warranted and creating a spurious secondary maximum. Conversely, a weaker hypothesis may appear deceptively close to the optimum, forming a local valley. These distortions yield a more irregular performance landscape, as illustrated in Figure 2b, with unexpected secondary peaks and valleys. Figure 2c further isolates these deviations, highlighting the gap between the idealized oracle landscape and practical estimates of closeness.</p>
<p>We now formalize these foundations by defining a mathematical model that makes explicit the relationship between hypothesis embeddings, similarity, and performance.</p>
<h1>2.1.2 Mathematical Formulation</h1>
<p>Let $\mathcal{H} \subset \mathbb{R}^{d}$ denote the hypothesis space, where each hypothesis $h \in \mathcal{H}$ is represented as a point in a $d$-dimensional latent space, conditioned on a specific research question $q$. Let $h^{<em>} \in \mathcal{H}$ denote the ground truth hypothesis for $q$, representing an experimentally validated optimum. We define the idealized performance function for any hypothesis $h$ in the vicinity of $h^{</em>}$ as:</p>
<p>$$
f\left(h, h^{<em>} ; q, \phi^{</em>}(\cdot)\right)=\frac{1}{\left(2 \pi \sigma^{2}\right)^{d / 2}} \exp \left(-\frac{\left|\phi^{<em>}(h \mid q)-\phi^{</em>}\left(h^{*} \mid q\right)\right|^{2}}{2 \sigma^{2}}\right)
$$</p>
<p>where $\phi^{<em>}(\cdot \mid q)$ is an oracle embedding function that maps each hypothesis $h$ to a point in the latent hypothesis space under the context of research question $q$. The embedded positions capture the oracle's understanding of closeness, measured by the Euclidean distance $\left|\phi^{</em>}(h \mid q)-\phi^{<em>}\left(h^{</em>} \mid q\right)\right|$.
We model the idealized performance surface as a Gaussian-like function centered at $\phi^{<em>}\left(h^{</em>} \mid q\right)$, yielding a strictly unimodal landscape that decays smoothly with increasing distance from the optimum $h^{*}$ (Figure 2a). While the true performance landscape in feature space may not be strictly Gaussian, the isotropic Gaussian form serves as a tractable and interpretable approximation in the latent space. This modeling choice directly reflects $A 1$ and $P 1$.</p>
<p>However, practical simulations rely on imperfect embeddings of hypotheses into the latent space, stemming from limitations in domain understanding-no matter whether the embedding is performed (internally) by human experts or LLMs. Consequently, this leads to distortions in perceived "closeness", effectively warping the positions of hypotheses in latent space. Such a distorted hypothesis</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 3: The internal structure of the simulator.
embedding $\tilde{\mathcal{H}}$ yields a different observed structure:</p>
<p>$$
\tilde{f}\left(h, h^{<em>} ; q, \phi(\cdot)\right)=f\left(h, h^{</em>} ; q, \phi^{*}(\cdot)\right)+\epsilon(h \mid q)
$$</p>
<p>where $\phi(\cdot \mid q)$ is a practical embedding function that maps each hypothesis $h$ into (somewhat distorted) positions in the latent hypothesis space for a research question $q$, and $\epsilon(h \mid q)$ represents a systematic correction term that accounts for the discrepancy between oracle embedding $\phi^{<em>}(\cdot \mid q)$ and the practical embedding $\phi(\cdot \mid q)$ under the context of $q$. As a result, the practical embedding $\tilde{\mathcal{H}}$ introduces systematic distortions in the latent space, leading to spurious optima or valleys-effectively transforming the unimodal ideal surface into a noisier, multimodal one (Figure 2b).
Crucially, Figures 2a and 2b illustrate the same underlying performance-closeness relationship $f\left(h, h^{</em>}\right)$, differing only by $\phi(h)$, which is how hypotheses are embedded in the latent space. Figure 2c illustrates $\epsilon(h)$, the correction term that accounts for the discrepancy between the oracle embedding $\phi^{*}(\cdot)$ and the practical embedding $\phi(\cdot)$.</p>
<h1>2.2 A Practical Implementation of $\phi(\cdot)$ with Prior Knowledge</h1>
<p>As discussed in $\S 2.1$, the core objective of the simulator is to construct an embedding function $\phi(\cdot)$ that maps each hypothesis $h$ into a latent space such that distances in this space reflect meaningful functional differences. Through extensive discussions with domain experts, we observe that a scientific hypothesis succeeds in addressing a research question primarily due to its underlying mechanisms.</p>
<p>Specifically, an effective hypothesis typically comprises a set of scientifically meaningful compo-nents-each contributing to distinct yet complementary sub-mechanisms-which together enable the overall reaction to fulfill its intended function. The specific prompts and examples for extracting key components and inferring mechanisms are provided in $\S \mathrm{A}$.
Informed by this domain knowledge, we design a simulator architecture illustrated in Figure 3. Each module corresponds to a subroutine implemented using an LLM with task-specific prompting. The simulator's goal is to estimate the latent-space distance $|\phi(h \mid q)-\phi\left(h^{<em>} \mid q\right)|$ between a candidate hypothesis $h$ and a ground truth hypothesis $h^{</em>}$, conditioned on a research question $q$.
The simulation begins by decomposing both the candidate and ground truth hypotheses into a set of key functional components, and identifying the underlying mechanism associated with each component in the context of the research question. The decomposition of $h^{<em>}$ is performed first, serving as a reference. These reference components and mechanisms guide the decomposition of $h$, ensuring alignment in both granularity and mechanistic interpretation.
Concurrently, the Assign Component Weights module estimates the relative importance $w_{i}$ of each component in the ground truth hypothesis, given the research question. A subset of these compo-nents-denoted $\mathcal{C}$-are labeled as critical, meaning they are considered necessary for the reaction to succeed. To elaborate on the role of $\mathcal{C}$, we provide illustrative examples in $\S$ B.
Next, the Compute Mechanism Similarity module compares each key component in $h^{</em>}$ with its corresponding component in $h$, assigning a similarity score $s_{i} \in[0,1]$ to each pair. These scores are then aggregated using a weighted sum, combined with a multiplicative penalty that enforces the</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 4: Experiment-guided ranking policy within an in-context reinforcement learning framework.
presence of all critical components:</p>
<p>$$
S\left(h \mid q ; h^{*}\right)=\left(\prod_{i \in \mathcal{C}} \mathbf{1}<em i="i">{s</em>=1
$$}&gt;0}\right) \cdot\left(\sum_{i=1}^{K} w_{i} \cdot s_{i}\right), \quad \text { where } \quad \sum_{i=1}^{K} w_{i</p>
<p>This formulation guarantees that $S\left(h^{<em>} \mid q ; h^{</em>}\right)=1$, since all components are present with maximal similarity ( $s_{i}=1$ for all $i$ ), resulting in zero distance from the ground truth. Similarity score $S$ are thereby bounded in $[0,1]$, and lower distances correspond to stronger functional alignment with the ground truth hypothesis. The resulting value is used as the simulated performance score.
The final distance between the candidate and ground truth hypotheses is then calculated as:</p>
<p>$$
\left|\phi(h \mid q)-\phi\left(h^{<em>} \mid q\right)\right|=\left|S\left(h \mid q ; h^{</em>}\right)-1\right|
$$</p>
<h1>3 Methodology of Experiment-Guided Ranking</h1>
<h3>3.1 Task Formulation</h3>
<p>Given a research question $q$, a set of candidate hypotheses $\mathcal{H}$ is formed by selecting hypotheses generated by existing scientific discovery systems (Yang et al., 2025) and ground-truth hypotheses from top-tier scientific journals reporting high-quality lab experiments. The goal of experiment-guided ranking is to identify the optimal hypothesis $h^{*} \in \mathcal{H}$ with the highest experimental performance using an experiment executor $E$. Formally, we define the experiment executor as a function:</p>
<p>$$
E: \mathcal{H} \rightarrow[0,1]
$$</p>
<p>that maps each hypothesis $h \in \mathcal{H}$ to a normalized performance score $s \in[0,1]$. The normalization provides a unified performance metric across heterogeneous hypotheses and varying problem settings $q$, and can be defined relative to a domain-specific state-of-the-art benchmark established by experts.
The primary goal is to find $h^{<em>}$. However, since each evaluation of $E(h)$ corresponds to a real or simulated experiment-which may be costly or time-consuming-a critical requirement is to identify $h^{</em>}$ using as few experimental trials as possible. Accordingly, an effective experiment-guided ranking strategy must actively incorporate feedback from prior evaluations to guide subsequent selections, balancing exploration and exploitation under a limited experimental budget.
Thus, the problem can be reframed as finding a selection strategy that minimizes the number of trials required to identify the optimal hypothesis:</p>
<p>$$
\arg \min <em _text="\text" _trials="{trials">{\pi} N</em> E(h)
$$}}^{\pi} \quad \text { subject to } \quad h^{*}=\arg \max _{h \in \mathcal{H}</p>
<p>where $\pi$ denotes the hypothesis selection strategy, and $N_{\text {trials }}^{\pi}$ is the number of experiments required under strategy $\pi$ to successfully discover $h^{*}$.</p>
<h3>3.2 Methodology</h3>
<p>Due to the high cost and data-scarce nature of wet-lab experiments in the natural sciences, conventional reinforcement learning (RL), which relies on extensive interaction and parameter updates, is</p>
<p>often impractical. Our approach circumvents this bottleneck by formulating the learning process within the context window of a frozen large language model. This gradient-free, non-parametric paradigm relies solely on forward passes, enabling the agent to learn from minimal trials without costly fine-tuning. The framework leverages the LLM’s intrinsic reasoning capabilities, ensuring excellent generalizability to diverse scientific discovery tasks. Our agent, CSX-Rank, learns an optimal hypothesis selection policy via a formal sequential decision-making process.</p>
<p>We formulate experiment-guided ranking as a sequential decision-making process. At each timestep $t$, the agent observes a state $s_{t}$ representing the cumulative analysis of past experiments. It then performs an action $a_{t}$ by selecting a hypothesis $h \in H$ to test, receiving a reward $r_{t}$ from the experimental outcome. The trajectory is thus $\tau=\left[s_{0}, a_{0}, r_{0}, \ldots\right]$.</p>
<p>Unlike standard RL settings that maximize cumulative reward, our objective is to identify the optimal hypothesis using the minimum number of experiments, reflecting the high cost of scientific exploration. The agent’s goal is to learn an optimal policy $\pi^{*}$ that minimizes the expected trials:</p>
<p>$$
\pi^{*}=\underset{\pi}{\arg \min } E_{\pi}\left[N_{\text {trials }}\right]
$$</p>
<p>Here, an effective policy $\pi\left(s_{t}\right)$ leverages the accumulated knowledge in the state to make more strategic selections, thus minimizing $N_{\text {trials }}$. Our agent, CSX-Rank, implements this policy through the structured, iterative process detailed below (Figure 4).</p>
<p>Step 1: Extraction, Classification, and Clustering of Functional Components. To generalize from specific results, the agent decomposes each hypothesis $h \in H$ into functional components, which are classified as effective, uncertain, or ineffective; the latter are pruned for efficiency. The remaining components are clustered by functional similarity, with each cluster representing a distinct mechanistic contribution to solving $q$. This yields a structured state representation $s_{t}$, where each element remains traceable to its originating hypothesis.</p>
<p>Step 2: Cluster and Hypothesis Selection. To connect abstract mechanistic knowledge (clusters) with a concrete experiment (hypothesis), the policy $\pi\left(s_{t}\right)$ selects the next action $a_{t}$ through a twostage process. First, guided by prior domain knowledge, the LLM identifies the most promising cluster. Within this cluster, it then selects the most relevant hypothesis $h$, which defines the action $a_{t}$.</p>
<p>Step 3: Experiment Execution and Result Analysis. The selected hypothesis $h$ (action $a_{t}$ ) is evaluated by the executor $E$-either our high-fidelity simulator (CSX-Sim) or a real wet lab-which returns a normalized performance score $s \in[0,1]$. This score serves as the reward $r_{t}=E\left(a_{t}\right)$, and its analysis quantifies the action’s success, grounding the policy in empirical results.</p>
<p>Step 4: Iterative Summarization and Refinement. To make learning cumulative, the agent integrates each experimental outcome into a running summary. This updated summary forms the new state $s_{t+1}$ for the next decision cycle, closing the RL loop and enabling the policy to refine systematically from prior knowledge and new feedback.</p>
<p>A key strength of our multi-step, component-driven framework is its inherent interpretability. By design, the agent must break down its decision process into an explicit, auditable trail-from extracting and clustering components to selecting the final hypothesis. Such structured and transparent reasoning is essential in scientific applications, allowing domain experts to examine the agent’s logic, build trust in its recommendations, and derive new insights.</p>
<h1>4 EXPERIMENT</h1>
<p>We name our simulator as CSX-Sim, and the experiment-guided ranking method as CSX-Rank. All experiments are implemented with GPT-40-mini (OpenAI, 2024).</p>
<h1>4.1 Simulator: Evaluating the Simulator with Real Experiment Results</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Simulator</th>
<th style="text-align: center;">Spearman Correlation ( $\uparrow$ )</th>
<th style="text-align: center;">Perfect Consistency Indicator ( $\uparrow$ )</th>
<th style="text-align: center;">RMSE ( $\downarrow$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Matched Score</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">$12 / 30$</td>
<td style="text-align: center;">0.232</td>
</tr>
<tr>
<td style="text-align: left;">CSX-Sim</td>
<td style="text-align: center;">$\mathbf{0 . 9 6 0}$</td>
<td style="text-align: center;">$\mathbf{2 6 / 3 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 1 3}$</td>
</tr>
<tr>
<td style="text-align: left;">w/o CriticalPoints</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">$23 / 30$</td>
<td style="text-align: center;">0.229</td>
</tr>
<tr>
<td style="text-align: left;">w/o ComponentExtraction</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">$12 / 30$</td>
<td style="text-align: center;">0.272</td>
</tr>
</tbody>
</table>
<p>Table 1: Validating the simulator with collected experiment results from literature.</p>
<p>We curated a benchmark of 30 research questions and 124 hypotheses from published literature, each with experimentally validated outcomes spanning multiple domains (§ C.1). For each hypothesis, simulated results from CSX-Sim were compared against the annotated outcomes (§ C.2). Evaluation considered two criteria: (1) Trend alignment, measured by Spearman correlation, assessing whether predicted performances preserve the relative ordering of ground-truth outcomes. Because ranking depends on relative differences, we also report the Perfect Consistency Indicator (PCI), the number of questions with perfect alignment. (2) Predictive accuracy, measured by RMSE, capturing absolute deviations between predicted and experimental values (see § E for details and additional indicators). Comparative results are shown in Table 1.</p>
<p>Baseline and Ablation We adopt the "Matched Score" (Yang et al., 2025) as our primary baseline, which evaluates hypotheses by measuring their similarity to ground-truth references through a reference-based comparison. Additionally, we conduct two ablation studies on CSX-Sim to assess the contribution of its key components: (1) The first ablation (w/o CriticalPoints) disables the labeling of critical components $C$, as defined in Equation 3, allowing hypotheses that lack essential components to still receive positive feedback from the simulator; (2) The second ablation (w/o ComponentExtraction) skips the extraction and weighting of critical components, directly computing mechanism similarity using prompts analogous to the final module in Figure 3.</p>
<p>Results Interpretation As shown in Table 1, CSX-Sim outperforms baselines across all metrics, demonstrating stronger trend alignment, greater robustness, and lower predictive error. Compared to the Matched Score baseline, it achieves notable gains in correlation and consistency while reducing error. Ablation studies confirm the importance of component analysis: removing CriticalPoints causes modest degradation, whereas omitting component extraction leads to substantial drops in alignment and accuracy. These results highlight the necessity of fine-grained component analysis for high-fidelity simulation feedback.</p>
<h3>4.2 EXPERIMENT-GUIDED RANKING: BASELINES AND ABLATION STUDY</h3>
<p>Data and Evaluation Metrics We evaluate experiment-guided ranking on the TOMATO-chem dataset (Yang et al., 2025), which contains 51 scientific problems, each annotated with a ground-truth hypothesis. For each problem, the MOOSE-Chem framework (Yang et al., 2025) generates 63 additional candidates distinct from the ground truth, yielding 64 hypotheses per question ( 1 ground truth and 63 negatives). The disciplinary distribution is provided in § D. The dataset's interdisciplinary nature, evident in its inclusion of topics from fields such as applied physics and biology, stems from its origin in scientific literature where "chemistry" papers are frequently co-labeled with other scientific fields. Performance is measured by $N_{\text {trials }}$, the number of simulation-based evaluations needed to identify the ground-truth hypothesis for each problem. Lower $N_{\text {trials }}$ indicates more efficient prioritization. Results appear in Table 2.</p>
<p>Baselines We compare against two strategies: Uninformed Search and Pre-Experiment Ranking. Uninformed search selects hypotheses uniformly at random; Pre-experiment ranking scores hypotheses using only prior model knowledge, without feedback (Yang et al., 2025).
As shown in Table 2, Uninformed Search require over 32 trials on average, while Pre-Experiment Ranking reduces this to under 30-outperforming both naive baselines but still far behind CSX-Rank. This indicates that relying solely on prior knowledge yields only modest gains without feedback,</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">$N_{\text {trials }}(\downarrow)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Uninformed Search</td>
<td style="text-align: center;">32.500</td>
</tr>
<tr>
<td style="text-align: left;">Pre-Experiment Ranking</td>
<td style="text-align: center;">28.608</td>
</tr>
<tr>
<td style="text-align: left;">CSX-Rank</td>
<td style="text-align: center;">$\mathbf{1 5 . 1 9 6}$</td>
</tr>
<tr>
<td style="text-align: left;">w/o Clustering</td>
<td style="text-align: center;">27.980</td>
</tr>
<tr>
<td style="text-align: left;">w/o Clustering \&amp; Analysis</td>
<td style="text-align: center;">35.627</td>
</tr>
<tr>
<td style="text-align: left;">w/o Clustering \&amp; Analysis \&amp; Full Feedback</td>
<td style="text-align: center;">37.667</td>
</tr>
</tbody>
</table>
<p>Table 2: Number of experiments required to identify the ground truth hypothesis across methods.
whereas experiment-guided ranking substantially improves sample efficiency. A detailed scalability analysis is provided in § I.</p>
<p>Ablation Study To assess the contribution of key components in CSX-Rank, we conducted ablation studies under three conditions: (1) removing functional clustering (CSX-Rank w/o Clustering); (2) further disabling feedback analysis (CSX-Rank w/o Clustering \&amp; Feedback Analysis); and (3) additionally limiting feedback to the 10 most recent simulation results (CSX-Rank w/o Clustering \&amp; Feedback Analysis \&amp; Full Feedback). As shown in Table 2, progressively removing these components leads to marked performance degradation, confirming the importance of clustering, analytical summarization, and sufficient feedback quantity for efficient hypothesis ranking.</p>
<h1>4.3 Simulator: Ablation on Different $\phi(\cdot)$ with Different Levels of Distortion</h1>
<p>To study how simulator fidelity affects ranking, we note that experiment-guided ranking is essentially an optimization process over hypothesis space. A high-fidelity simulator provides informative feedback to guide this search, while distortions mislead it. We therefore introduce controlled distortions into $\phi(\cdot)$ to simulate increasingly challenging feedback conditions.</p>
<p>In collaboration with domain experts, we designed three distortion types commonly observed in practice-local maxima/minima, plateaus, and cliffs-reflecting typical challenges in hypothesis evaluation. We further defined three distortion levels (Simple, Moderate, Complex), incorporating progressively more noise into $\phi(\cdot)$; full details appear in § F.</p>
<p>We evaluated CSX-Rank, CSX-Rank w/o Clustering, and CSX-Rank w/o Clustering \&amp; Analysis across these noise conditions. As shown in Table 3, higher noise complexity consistently degraded performance, increasing $N_{\text {trials }}$. Still, CSX-Rank outperformed its ablated variants, preserving a clear efficiency margin even under Complex Noise ( 32.7 vs. 36.5 and 40.5 trials). These results demonstrate the robustness of clustering and feedback analysis in mitigating misleading signals and maintaining search efficiency, aligning with Section 4.2.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">$N_{\text {trials }}$ (Simple Noise)</th>
<th style="text-align: center;">$N_{\text {trials }}$ (Medium Noise)</th>
<th style="text-align: center;">$N_{\text {trials }}$ (Complex Noise)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CSX-Rank</td>
<td style="text-align: center;">21.804</td>
<td style="text-align: center;">26.608</td>
<td style="text-align: center;">32.706</td>
</tr>
<tr>
<td style="text-align: center;">w/o Clustering</td>
<td style="text-align: center;">32.706</td>
<td style="text-align: center;">35.843</td>
<td style="text-align: center;">36.471</td>
</tr>
<tr>
<td style="text-align: center;">w/o Clustering \&amp; Analysis</td>
<td style="text-align: center;">37.235</td>
<td style="text-align: center;">38.373</td>
<td style="text-align: center;">40.451</td>
</tr>
</tbody>
</table>
<p>Table 3: Simulator with different noise conditions</p>
<h2>5 Related Work</h2>
<p>Most prior work on hypothesis ranking has focused on pre-experiment ranking. Some approaches assign a score to each hypothesis and rank them accordingly, providing a simple and efficient solution (Yang et al., 2024; 2025; Zhou et al., 2024). Others adopt a pairwise ranking strategy, evaluating hypothesis pairs one at a time (Si et al., 2024; Liu et al., 2025). However, these methods rely solely on the internal reasoning of LLMs and do not incorporate feedback from experimental outcomes.</p>
<p>To our knowledge, few existing works leverage experimental feedback in hypothesis-driven tasks, and those that do are confined to domains with highly efficient verifiers, enabling rapid hypothesis testing</p>
<p>and direct refinement rather than explicit ranking. Notably, recent methods in mathematics (RomeraParedes et al., 2024; Shojaee et al., 2024; Ma et al., 2024) and programming (Novikov et al., 2025; Qiu et al., 2024) incorporate feedback loops by refining hypotheses based on verification outcomes. In contrast, our work targets natural science domains, where real experiments are far more costly, rendering such exhaustive trial-and-error strategies impractical. This motivates the need for a more deliberate experiment-guided ranking process, designed to maximize the information gained from each costly experiment when prioritizing future hypotheses. Roohani et al. (2024) explore hypothesis generation in a genetic perturbation setting, where task-specific feedback can be computed directly (e.g., via gene overlap). This remains a niche domain where efficient verifiers are available. By contrast, our work focuses on constructing general-purpose simulators, enabling the study of experiment-guided ranking in settings where real experiments are costly and feedback is scarce.</p>
<h1>6 CONCLUSION</h1>
<p>We introduced the task of experiment-guided ranking and addressed its central bottleneck-the lack of scalable experimental feedback-by proposing a simulator grounded in three domain-informed conceptual foundations. Validated against 124 hypotheses, the simulator enables systematic evaluation of ranking policies. Building on this, we developed an in-context reinforcement learning framework with a clustering-based agentic policy that significantly outperforms pre-experiment baselines. Together, the simulator and policy provide a toolkit for advancing feedback-driven hypothesis discovery, with potential impact across the natural sciences.</p>
<h2>ETHICS STATEMENT</h2>
<p>This work aims to accelerate beneficial scientific discovery, and the authors have read and adhered to the ICLR Code of Ethics. We acknowledge the potential for dual-use applications inherent in this research, as well as potential biases from our literature-based datasets and simulator conceptual foundations. We believe the benefits of a transparent and auditable framework for research outweigh these risks and are committed to its responsible application. No human subjects were involved in this study.</p>
<h2>REPRODUCIbILITY STATEMENT</h2>
<p>All code and data supporting this research are publicly available in an anonymous repository, a link to which is provided on the first page of this paper.</p>
<h2>REFERENCES</h2>
<p>Bruce Alberts, Dennis Bray, Karen Hopkin, Alexander D Johnson, Julian Lewis, Martin Raff, Keith Roberts, and Peter Walter. Essential cell biology. Garland Science, 2015.</p>
<p>Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein. An empirical investigation of statistical significance in nlp. In Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning, pp. 995-1005, 2012.</p>
<p>William D Callister and David G Rethwisch. Materials science and engineering: an introduction. Wiley New York, 1999.</p>
<p>Erik Cambria, Rui Mao, Melvin Chen, Zhaoxia Wang, and Seng-Beng Ho. Seven pillars for the future of artificial intelligence. IEEE Intelligent Systems, 38(6):62-69, 2023.</p>
<p>Mario Coccia. Why do nations produce science advances and new technology? Technology in society, 59:101124, 2019.</p>
<p>Corwin Hansch, A Leo, et al. Fundamentals and applications in chemistry and biology, 1995.
Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, and Dongzhan Zhou. Researchbench: Benchmarking llms in scientific discovery via inspiration-based task decomposition. arXiv preprint arXiv:2503.21248, 2025.</p>
<p>Ziming Luo, Zonglin Yang, Zexin Xu, Wei Yang, and Xinya Du. LLM4SR: A survey on large language models for scientific research. CoRR, abs/2501.04306, 2025. doi: 10.48550/ARXIV. 2501.04306. URL https://doi.org/10.48550/arXiv.2501.04306.</p>
<p>Pingchuan Ma, Tsun-Hsuan Wang, Minghao Guo, Zhiqing Sun, Joshua B Tenenbaum, Daniela Rus, Chuang Gan, and Wojciech Matusik. Llm and simulation as bilevel optimizers: a new paradigm to advance physical scientific discovery. In Proceedings of the 41st International Conference on Machine Learning, pp. 33940-33962, 2024.</p>
<p>Alexander Novikov, Ngân Vũ, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al. Alphaevolve: A coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025.</p>
<p>OpenAI. Gpt-4o mini: Advancing cost-efficient intelligence. https://openai.com/index/ gpt-4o-mini-advancing-cost-efficient-intelligence/, 2024. Accessed: 2025-05-16.</p>
<p>Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, et al. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. In The Twelfth International Conference on Learning Representations, 2024.</p>
<p>Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from program search with large language models. Nature, 625(7995):468-475, 2024.</p>
<p>Yusuf Roohani, Andrew Lee, Qian Huang, Jian Vora, Zachary Steinhart, Kexin Huang, Alexander Marson, Percy Liang, and Jure Leskovec. Biodiscoveryagent: An ai agent for designing genetic perturbation experiments. arXiv preprint arXiv:2405.17631, 2024.</p>
<p>Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, and Chandan K Reddy. Llm-sr: Scientific equation discovery via programming with large language models. arXiv preprint arXiv:2404.18400, 2024.</p>
<p>Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. arXiv preprint arXiv:2409.04109, 2024.</p>
<p>John Wiley. Introduction to solid state physics. New York, 185, 1986.
Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, and Erik Cambria. Large language models for automated open-domain scientific hypotheses discovery. In Findings of the Association for Computational Linguistics ACL 2024, pp. 13545-13565, 2024.</p>
<p>Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, and Dongzhan Zhou. Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. In Proceedings of the International Conference on Learning Representations (ICLR), 2025.</p>
<p>Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, and Chenhao Tan. Hypothesis generation with large language models. In Proceedings of the 1st Workshop on NLP for Science (NLP4Science), pp. 117-139, 2024.</p>
<h1>A Extracting Key FUNCTIONAL COMPONENTS in the Simulator</h1>
<h2>A. 1 A Framework for Extracting Critical Functional Components in the Simulator</h2>
<p>To better illustrate the specific framework of CSX-Sim for extracting key functional components, as shown in Figure 5. For specific scientific problems, we use chemistry as an example to categorize the key functional components and conclusions within the hypothesis. We then analyze the role and mechanism of each key chemical component based on the chemical problem and the conclusions drawn from the hypothesis. Finally, we review and output the key chemical components, their corresponding mechanisms, and the conclusions from the hypothesis.</p>
<h2>Framework for Analyzing Scientific Hypotheses in Chemical Problems</h2>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 5: A Framework for Extracting Chemical Components in the Simulator.</p>
<h2>A. 2 Prompt for Extracting Key Chemical Components in the Simulator</h2>
<p>The prompt for extracting key chemical components in the simulator, along with examples, is as follows:</p>
<p>You are an experienced chemistry expert. I will provide you with a scientific question and a scientific hypothesis. Your task is to identify the chemical key points within the hypothesis that are essential for addressing the scientific question. Chemical key points are the core elements-such as basic chemical components, reactions, or mechanistic methods-critical to solving the problem effectively. Analyze these key points by linking them to the scientific question, determining how they contribute to resolving it.</p>
<p>When identifying chemical key points, consider the following:
Each substance may be a key point. If it includes specific parameters like concentration or mass fraction (e.g., $0.3 \mathrm{M} \mathrm{NaCl}, 10 \mathrm{wt} \% \mathrm{PVA}$ ), ensure these details are retained in the division process without losing specificity. If multiple substances are related and function together (e.g., potassium ferricyanide and potassium ferrocyanide as an oxidizing-reducing pair), group them as a single chemical key point based on their shared role or interdependence. Exclude elements from the scientific question that reappear in the hypothesis as prerequisites (e.g., if the question involves improving MXene nanosheets and the hypothesis enhances them with liquid metal, MXene nanosheets are a prerequisite, not a key point; liquid metal is the key point). Prerequisites should not be output or analyzed as key points. Distinguish key points from validation methods (e.g., elemental analysis to verify properties). Validation methods support the hypothesis but are not chemical key points. For</p>
<p>each identified chemical key point, conduct a detailed and rigorous analysis of its role and function in relation to the scientific question. Use your chemical knowledge to explain the specific mechanism by which it addresses the problem, focusing on how it enhances the relevant properties or performance outlined in the question. Provide a clear, mechanistic explanation of its contribution and, if multiple key points exist, describe their interconnections.
Additionally, identify the results-effects or phenomena caused by these key points-representing the experiment's outcomes. In your output, focus on listing and explaining the chemical key points, followed by the results, ensuring no prerequisites from the scientific question are included.</p>
<h1>Output format:</h1>
<p>Chemical Key Points Chemical substance/component/method 1
Role and Function: Describe the role and function of the substance or method, including a detailed mechanistic explanation of how it addresses the scientific question and enhances relevant properties. Chemical substance/component/method 2
Role and Function: Describe the role and function of the substance or method, including a detailed mechanistic explanation of how it addresses the scientific question and enhances relevant properties. End Chemical Key Points Results Result 1:
Describe the effects caused by the aforementioned reasons (e.g., performance improvement, efficiency changes).
Result 2:
Further describe other effects related to the experimental objectives.
End Results
Example: Chemical Key Points 1. 10wt\% PVA (Polyvinyl Alcohol)
Role and Function: Polyvinyl alcohol (PVA) hydrogel acts as the base material, providing structural support and mechanical performance for thermoelectric gels. PVA with a mass fraction of $10 \%$ can provide mechanical support through hydrogen bonds in its structure and interact with potassium ferricyanide and potassium ferrocyanide to offer electrical changes.
2. $\mathrm{Gdm}<em 4="4">{2} \mathrm{SO}</em>$ (Guanidine Sulfate)</p>
<p>Role and Function: Guanidine sulfate $\left(\mathrm{Gdm}<em 4="4">{2} \mathrm{SO}</em>}\right)$ is integrated into the $\mathrm{K<em 6="6">{3}\left[\mathrm{Fe}(\mathrm{CN})</em>}\right] / \mathrm{K<em 6="6">{4}\left[\mathrm{Fe}(\mathrm{CN})</em>\right]$ to improve thermoelectric performance. The introduction of guanidine salt increases solvent entropy and effectively enhances thermopower.
3. Directional Freezing Method</p>
<p>Role and Function: By employing directional freezing technology, aligned channels are created, enhancing the electrical conductivity and mechanical strength of the material.
4. Potassium Ferricyanide and Potassium Ferrocyanide $\left(\mathrm{K}<em 6="6">{3}\left[\mathrm{Fe}(\mathrm{CN})</em>}\right] / \mathrm{K<em 6="6">{4}\left[\mathrm{Fe}(\mathrm{CN})</em>\right]\right)$</p>
<p>Role and Function: These compounds are crucial electrolytes that facilitate redox reactions within the polymer gel. The presence of these ions enhances ion mobility and conductivity due to their ability to undergo reversible redox processes, thereby boosting the thermoelectric properties of the gel
End Chemical Key Points Results Carnot-relative Efficiency
The Carnot-relative efficiency of the FTGA exceeds $8 \%$.
Thermopower and Mechanical Robustness
Thermopower and mechanical robustness are enhanced, outperforming traditional quasi-solid-state thermoelectric cells.
End Results
Here's a detailed example in chemistry: To better illustrate the effectiveness of extracting key chemical components, we compare the performance of our simulator against human chemistry experts by analyzing a real-world chemical problem.</p>
<ul>
<li>Scientific Question: How can a cost-effective N-type quasi-solid-state thermocell be developed to boost electricity production from low-grade heat by improving both ion transport efficiency and electrode performance?</li>
<li>Scientific Hypothesis:Develop a flexible N-type quasi-solid-state thermocell by integrating anisotropic polymer networks and hierarchical 3D copper electrodes to enhance ion transport, mechanical robustness, and thermoelectric performance. Utilizing Polyvinyl Alcohol (PVA) as the hydrogel matrix, the anisotropic structure is achieved through a directional freeze-thawing (DFT) process, which involves applying a temperature gradient during freezing to guide ice crystal growth for polymer chain alignment. Repeated cycles</li>
</ul>
<p>further enhance the alignment and crosslinking, creating anisotropic pores that reduce ion transport resistance. Ionic crosslinking with a $0.7 \mathrm{M} \mathrm{CuSO}<em 2="2">{4}$ electrolyte and 0.1 M $\mathrm{H}</em>$ and ionic conductivity improved by $20 \%$. Mechanical tests reveal significant strength with a tensile strain at break of $350 \%$ and the system maintains stability under various mechanical deformations. This approach offers a cost-effective, adaptable solution for low-grade heat harvesting.} \mathrm{SO}_{4}$ strengthens the hydrogel while retaining flexibility. Meanwhile, hierarchical 3D copper electrodes, fabricated via oxidation, etching, and thermal reduction, provide a high surface area, enhancing redox kinetics of the $\mathrm{Cu}^{2+} / \mathrm{Cu}^{0}$ couple and obviating platinum electrode reliance. This synergistic design achieves a remarkable $1500 \%$ increase in power density, reaching $0.51 \mathrm{~mW} \mathrm{~m}^{-2}$ at a $\Delta T$ of $5^{\circ} \mathrm{C}$, with a thermopower of $0.7 \mathrm{mV} \mathrm{K}^{-1</p>
<p>Here's Chemistry Expert Extracting Key Chemical Component:</p>
<ul>
<li>
<p>Chemical Key Points:</p>
</li>
<li>
<p>Anisotropic Polymer Networks</p>
</li>
</ul>
<p>Role and Function: The layered polymer network structure enhances ion transport rates, thereby improving thermoelectric conversion efficiency.
2. Hierarchical 3D Copper Electrodes</p>
<p>Role and Function: The use of 3D copper electrodes effectively increases the reaction contact area, enhancing the thermoelectric potential.
3. Polyvinyl Alcohol (PVA) as the Hydrogel Matrix</p>
<p>Role and Function: PVA hydrogel is used as the matrix to provide good mechanical properties and structural support.
4. Directional Freeze-Thawing (DFT) Process</p>
<p>Role and Function: The DFT process forms ion channels along the freezing direction, thereby increasing ion transport rates.
5. 0.7 M CuSO 4 Electrolyte</p>
<p>Role and Function: This electrolyte is used for ionic crosslinking to enhance the mechanical properties of the thermoelectric material.
6. $0.1 \mathrm{M} \mathrm{H}<em 4="4">{2} \mathrm{SO}</em>$</p>
<p>Role and Function: This acid is used together with a $0.7 \mathrm{M} \mathrm{CuSO}_{4}$ electrolyte for ionic crosslinking to enhance the mechanical properties of the thermoelectric material.</p>
<ul>
<li>Results:</li>
<li>Enhanced Electrical Properties of the Thermoelectric Material</li>
<li>Increased power density</li>
<li>Improved thermopower</li>
<li>Enhanced ionic conductivity</li>
<li>Improved Mechanical Properties of the Thermoelectric Material</li>
<li>Increased strength with a tensile strain</li>
<li>Enhanced stability under various mechanical deformations</li>
</ul>
<p>Here's CSX-Sim Extracting Key Chemical Component:</p>
<ul>
<li>
<p>Chemical Key Points:</p>
</li>
<li>
<p>Anisotropic Polymer Networks</p>
</li>
</ul>
<p>Role and Function: These networks are engineered to enhance ion transport efficiency while providing mechanical robustness through alignment of polymer chains, reducing ion transport resistance in thermocells.
2. Hierarchical 3D Copper Electrodes</p>
<p>Role and Function: The use of hierarchical 3D copper electrodes increases the surface area, enhancing redox kinetics of the $\mathrm{Cu}^{2+} / \mathrm{Cu}^{0}$ couple, thus improving the thermoelectric performance without relying on expensive platinum electrodes.
3. Copper Sulfate $\left(\mathrm{CuSO}_{4}\right)$ Electrolyte</p>
<p>Role and Function: A $0.7 \mathrm{M} \mathrm{CuSO}_{4}$ solution serves as an ionic crosslinker in the</p>
<p>hydrogel, enhancing mechanical strength while facilitating efficient ion transport in the system.
4. Sulfuric Acid $\left(\mathrm{H}<em 4="4">{2} \mathrm{SO}</em>\right)$</p>
<p>Role and Function: The addition of $0.1 \mathrm{M} \mathrm{H}<em 4="4">{2} \mathrm{SO}</em>$ in the hydrogel contributes to ionic crosslinking and enhancing the overall conductivity of the electrochemical system.
5. Directional Freeze-Thawing (DFT) Process</p>
<p>Role and Function: This method is used to create an ordered structure in the polymer hydrogel, promoting alignment of polymer chains and ensuring anisotropic pores that further reduce ion transport resistance.</p>
<ul>
<li>Results:</li>
<li>Power Density Increase</li>
<li>Enhanced Thermopower</li>
<li>Improved Ionic Conductivity</li>
<li>Mechanical Strength under Deformation</li>
</ul>
<p>Here's a comparison of the analysis results between our simulator and human experts:
By comparing the approaches of a chemistry expert and CSX-Sim in extracting key chemical components for the specific chemical issues of ion transport efficiency and electrode performance, CSX-Sim successfully identifies solutions in its scientific hypotheses, including anisotropic polymer networks and hierarchical 3D copper electrodes. Compared to the human chemistry expert, CSX-Sim captures five out of six key points, missing only one: "Polyvinyl Alcohol (PVA) as the Hydrogel Matrix." The points it does identify align accurately with those proposed by the human expert based on the hypothesis, demonstrating the high accuracy of CSX-Sim in extracting key chemical components.</p>
<h1>B The Role of CriticalPoints in CSX-Sim</h1>
<p>To better illustrate the role of labeling critical components $\mathcal{C}$ in CSX-Sim, as defined in Equation 3, we provide an example for clarity. For simplicity, we define the term $\left(\prod_{i \in \mathcal{C}} \mathbf{1}<em i="i">{s</em>\right)$ from Equation 3, related to CriticalPoints, as the Correction Factor. This factor takes values of either 0 or 1.}&gt;0</p>
<p>The scientific problem under study is: How can a polymer gel material be designed to enhance the Seebeck coefficient (Se) by optimizing the matrix material and redox pair, thereby improving the energy conversion efficiency of a thermoelectric device utilizing the temperature difference between body heat and the environment?</p>
<p>This scientific problem corresponds to four real experimental hypotheses, outlined as follows:</p>
<ol>
<li>Hypothesis 1: By combining gelatin with KCl , prepare a gel with high ionic conductivity to investigate its Seebeck coefficient (Se) performance with the $\left[\mathrm{Fe}(\mathrm{CN})<em 6="6">{6}\right]^{3-} /\left[\mathrm{Fe}(\mathrm{CN})</em>)}\right]^{4-}$ redox pair. KCl , as an electrolyte, significantly enhances the gel's ionic conductivity, while the $\left[\mathrm{Fe}(\mathrm{CN<em 6="6">{6}\right]^{3-} /\left[\mathrm{Fe}(\mathrm{CN})</em>$ redox pair boosts the Seebeck coefficient through temperature-gradient-driven ion diffusion. Gelatin provides biocompatibility and mechanical strength, making it suitable for efficient thermoelectric energy conversion.}\right]^{4-</li>
<li>Hypothesis 2: By combining a PVA matrix with HCl , prepare a gel with high ionic conductivity and investigate its Seebeck coefficient (Se) performance under the influence of the $\mathrm{Fe}^{3+} / \mathrm{Fe}^{2+}$ redox pair. HCl , as a strong electrolyte, significantly enhances the gel's ionic conductivity, while the $\mathrm{Fe}^{3+} / \mathrm{Fe}^{2+}$ redox pair boosts the Seebeck coefficient through temperature-difference-driven ion diffusion. PVA provides flexibility and transparency, and by optimizing the HCl concentration and PVA crosslinking degree, ion migration efficiency can be further improved, enhancing the Seebeck coefficient and making it suitable for efficient energy conversion in body-heat thermoelectric devices.</li>
<li>
<p>Hypothesis 3: By preparing a pure PVA gel, investigate its Seebeck coefficient (Se) performance under the influence of the $\mathrm{Fe}^{3+} / \mathrm{Fe}^{2+}$ redox pair. PVA, as a hydrophilic polymer, possesses a certain level of ionic conductivity, and the $\mathrm{Fe}^{3+} / \mathrm{Fe}^{2+}$ redox pair generates a Seebeck coefficient through temperature-difference-driven ion diffusion.</p>
</li>
<li>
<p>Hypothesis 4: By polymerizing acrylamide (PAM) to prepare a hydrogel and investigate its thermoelectric performance. The porous network structure of the polyacrylamide hydrogel enhances the gel's ionic conductivity.</p>
</li>
</ol>
<p>The results of the actual wet-lab experiments are summarized in Table 4. For the first three hypotheses, the CriticalPoints are included in the assumptions, resulting in a Correction Factor of 1 and nonzero final results. In contrast, for the fourth hypothesis, the CriticalPoints are not included in the assumption, leading to a Correction Factor of 0 and a final result of 0 .</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Matrix</th>
<th style="text-align: center;">The Seebeck coefficient $\mathrm{Se} / \mathrm{mV} \mathrm{K}^{-1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\left[\mathrm{Fe}(\mathrm{CN})<em 6="6">{6}\right]^{3-} /\left[\mathrm{Fe}(\mathrm{CN})</em> /$ Gelatin}\right]^{4-} \mathrm{KCl</td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{Fe}^{3+} / \mathrm{Fe}^{2+} \mathrm{HCl} / \mathrm{PVA}$</td>
<td style="text-align: center;">1.02</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{Fe}^{3+} / \mathrm{Fe}^{3+} \mathrm{PVA}$</td>
<td style="text-align: center;">0.85</td>
</tr>
<tr>
<td style="text-align: center;">Polymerizing Acrylamide (PAM)</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Table 4: The Seebeck coefficient (Se) of polymer thermoelectric materials</p>
<p>Our simulator analyzes Hypothesis 4, where the Correction Factor is 0 , as follows:
Analysis: The Ground Truth Scientific Hypothesis identifies three core key points: Gelatin, KCl (Potassium Chloride), and the $\left[\mathrm{Fe}(\mathrm{CN})<em 6="6">{6}\right]^{3-} /\left[\mathrm{Fe}(\mathrm{CN})</em>$ Redox Pair, each playing significant roles in enhancing the Seebeck coefficient for thermoelectric efficiency. The Generated Hypothesis uses Polyacrylamide (PAM) in place of Gelatin, which can provide a similar structural function but lacks some specific characteristics of Gelatin important for thermoelectric stability. It discusses ionic conductivity, paralleling that of KCl without directly employing the salt. Notably, it does not mention the redox pair, which is critical to the thermoelectric function as outlined in the Ground Truth Hypothesis.}\right]^{4-</p>
<p>However, while the Generated Hypothesis incorporates functional aspects related to ionic conductivity and chemical structure, it omits the crucial redox couple and does not fully replicate the essential components necessary for observable thermoelectric phenomena defined by the Ground Truth. The presence of PAM and evolving ionic conductivity offers a partial basis for plausible thermoelectric behavior under hypothetical conditions, but the complete omission of the redox pair is significant. The lack of adherence to all core key points and reliance on acceptable substitutions leads to a conclusion that while observable phenomena might still occur, they are limited. Given these evaluations, I assign $R=0$ as there is a substantial gap in essential components for the reaction proposed in the Ground Truth Hypothesis.</p>
<h1>Final Correction Factor 0 Endo</h1>
<p>Our simulator analyzes Hypothesis 4, noting that enhancing the Seebeck coefficient of a polymer gel material requires a redox pair. The first three hypotheses incorporate redox pairs, either $\left[\mathrm{Fe}(\mathrm{CN})<em 6="6">{6}\right]^{3-} /\left[\mathrm{Fe}(\mathrm{CN})</em>$, which facilitate efficient conversion of thermal energy to electrical energy. In contrast, Hypothesis 4 only involves polymerizing acrylamide (PAM) and lacks a redox pair, rendering it unable to effectively convert thermal energy into electrical energy. Consequently, the thermoelectric potential (Seebeck coefficient, Se) is zero.}\right]^{4-}$ or $\mathrm{Fe}^{3+} / \mathrm{Fe}^{2+</p>
<h2>C Evaluating the Simulator with Real Experiment Results</h2>
<p>In this section, we present the validation of our simulator's accuracy using a dataset of 124 chemical hypotheses, detailing their classification and composition. We further compare the trends of the simulated results with the corresponding real experimental outcomes to assess the simulator's predictive performance and reliability in capturing real-world chemical behaviors.</p>
<h2>C. 1 DATASET COMPOSITION AND ANALYSIS</h2>
<p>To evaluate the performance of the simulator, we conducted a thorough analysis using real-world experimental data. We curated a set of 30 cutting-edge research questions, each designed to probe</p>
<p>significant aspects of chemical research. These questions were carefully selected to encompass multiple areas within the chemistry domain, ensuring a diverse and representative evaluation framework. Each question was associated with 3 to 6 hypotheses, resulting in a total of 124 authentic wet lab experiment results. This extensive dataset forms a robust foundation for assessing the simulator's predictive accuracy and reliability.
The 124 experiment results were sourced from key subfields of chemistry to provide broad coverage of the discipline. The distribution of these results across subfields is presented in Table 5. Specifically, Polymer Chemistry contributed 16 results, Organic Chemistry provided 36, Inorganic Chemistry accounted for 33, and Analytical Chemistry comprised 39, totaling 124 results. This distribution across multiple subfields ensures that the test set reflects the diversity and complexity of real-world chemical experiments, enhancing the robustness of our evaluation.
A statistical analysis of the 124 authentic wet lab results was conducted to rigorously evaluate the simulator's performance. By including a substantial number of experiments from various subfields, we ensured that the dataset captures a wide range of challenges encountered in chemical research. This approach minimizes potential biases from over-representing any single subfield, thereby strengthening the reliability of our evaluation. The dataset's diversity and scale provide a solid basis for assessing the simulator's ability to predict experimental outcomes accurately, offering valuable insights for future research and applications.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Energy Materials</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: center;">Polymeric Materials</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">Applied Physics</td>
<td style="text-align: center;">18</td>
</tr>
<tr>
<td style="text-align: center;">Systems Biology</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">Organic Chemistry</td>
<td style="text-align: center;">26</td>
</tr>
<tr>
<td style="text-align: center;">Inorganic Chemistry</td>
<td style="text-align: center;">24</td>
</tr>
<tr>
<td style="text-align: center;">Analytical Chemistry</td>
<td style="text-align: center;">26</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">124</td>
</tr>
</tbody>
</table>
<p>Table 5: Classification of the 124 real-world experiments used to validate the simulator.
The use of authentic wet lab results bolsters the credibility of our findings. By grounding the evaluation in real experimental data, we ensured that the simulator's predictions were tested against the intricacies and variability of actual laboratory conditions. This approach not only validates the simulator's performance but also underscores its potential to guide subsequent research by delivering reliable and actionable predictions. The diverse dataset and representation of multiple subfields collectively contribute to a comprehensive and effective evaluation, paving the way for advancements in chemical simulation and experimentation.</p>
<h1>C. 2 Trend Comparison with Real Experiment Results</h1>
<p>To further assess the capabilities of our CSX-Sim, we utilized it to simulate 124 wet lab experiments. These experiments corresponded to 30 cutting-edge chemical science questions, and their simulated outcomes were subsequently aggregated for a comprehensive analysis. For each of the 124 experiments, the simulated result was derived from the average of three trials conducted by CSX-Sim. These results, each corresponding to one of the curated chemical questions, were systematically arranged in ascending order along the "Order of Experimental Results" axis, as depicted in Figure 6. This organization enabled a unified comparison between the simulated and actual experimental outcomes, with the vertical axis representing normalized experimental results to standardize the evaluation across the dataset.</p>
<p>Figure 6 compares the trends observed in CSX-Sim predictions (green line) with those from real experimental data (blue line). Error bars, representing the population standard deviation, illustrate the variability of the data points. Statistical significance was further established using the Bootstrap method, with results indicating $(p&lt;0.01)$ (Berg-Kirkpatrick et al., 2012). The aggregated analysis reveals that the simulator effectively predicts the mean trends for all 30 sets of results, demonstrating a strong consistency with the mean of the actual experimental outcomes. This alignment of mean</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 6: Comparison of simulated real experimental results with CSX-Simulator.
trends across the diverse questions underscores the simulator's ability to model chemical processes accurately, capturing the overall behavior of the experimental data, regardless of the specific subfield.</p>
<p>The use of normalized results ensures that differences in scale do not affect the comparison, allowing a fair assessment of the simulator's trend-matching capability. The close correspondence between the simulated and real mean data, as visualized in the figure, highlights the CSX-Sim broad applicability across the chemistry domain. By successfully replicating the mean trends of the 124 results, the simulator proves to be a versatile tool, offering reliable predictions that can support a wide range of chemical research and applications.</p>
<h1>D DISCIPLINES OF THE TOMATO-CHEM BENCHMARK DATASET</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Energy Materials</td>
<td style="text-align: center;">363</td>
</tr>
<tr>
<td style="text-align: center;">Polymeric Materials</td>
<td style="text-align: center;">359</td>
</tr>
<tr>
<td style="text-align: center;">Metallic Materials</td>
<td style="text-align: center;">268</td>
</tr>
<tr>
<td style="text-align: center;">Nanomaterials</td>
<td style="text-align: center;">316</td>
</tr>
<tr>
<td style="text-align: center;">Applied Physics</td>
<td style="text-align: center;">127</td>
</tr>
<tr>
<td style="text-align: center;">Analytical Chemistry</td>
<td style="text-align: center;">317</td>
</tr>
<tr>
<td style="text-align: center;">Inorganic Chemistry</td>
<td style="text-align: center;">392</td>
</tr>
<tr>
<td style="text-align: center;">Organic Chemistry</td>
<td style="text-align: center;">482</td>
</tr>
<tr>
<td style="text-align: center;">Chemical Engineering</td>
<td style="text-align: center;">196</td>
</tr>
<tr>
<td style="text-align: center;">Environmental Engineering</td>
<td style="text-align: center;">298</td>
</tr>
<tr>
<td style="text-align: center;">Molecular Biology</td>
<td style="text-align: center;">84</td>
</tr>
<tr>
<td style="text-align: center;">Systems Biology</td>
<td style="text-align: center;">62</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">3264</td>
</tr>
</tbody>
</table>
<p>Table 6: Disciplinary classification of the TOMATO-chem benchmark dataset</p>
<h1>E Evaluation of Trend Alignment and Accuracy</h1>
<h2>E. 1 Evaluation of Trend Alignment</h2>
<p>To quantitatively assess trend alignment between simulated and experimental results, we employed the Spearman Rank Correlation Coefficient (denoted as $\rho$ ). This non-parametric measure evaluates the monotonic relationship between the rankings of simulated and experimental outcomes, making it suitable for capturing trend consistency across diverse chemical problems.</p>
<p>The Spearman Correlation Coefficient is calculated as follows:</p>
<p>$$
\rho=1-\frac{6 \sum d_{i}^{2}}{n\left(n^{2}-1\right)}
$$</p>
<p>Where: $d_{i}$ : The difference between the ranks of the $i$-th simulated and experimental result. $n$ : The number of hypotheses in a given group (ranging from 3 to 6 per scientific question). $\rho$ : The correlation coefficient, ranging from -1 (perfect negative correlation) to 1 (perfect positive correlation), with 0 indicating no monotonic relationship. A Spearman Correlation Coefficient ( $\rho$ ) near 1 indicates strong trend alignment, meaning the simulated results closely mirror the relative ordering of experimental outcomes. Our CSX-Sim achieved a mean Spearman Correlation Coefficient of $\rho=0.960$, significantly outperforming the baseline, as shown in Table 1, and demonstrating superior trend alignment.
To further assess the robustness of the simulator across diverse problems, we introduced the Perfect Consistency Indicator (PCI), a stringent metric that counts the number of question groups (out of the 30 scientific questions) where the simulated results achieved perfect trend alignment with the experimental results $(\rho=1)$. Perfect trend alignment requires an exact match in the ranking of simulated and experimental outcomes, making PCI a robust measure of the simulator's ability to consistently replicate experimental trends across all problems. Notably, our CSX-Sim achieved perfect trend alignment $(\rho=1)$ in 26 out of 30 question groups, significantly surpassing the baseline methods and highlighting its exceptional robustness and predictive fidelity.</p>
<h2>E. 2 Evaluation of Simulator Accuracy</h2>
<p>For evaluating prediction accuracy, we used the Root Mean Square Error (RMSE) to quantify the deviation between simulated and experimental values. The RMSE is defined as:</p>
<p>$$
\mathrm{RMSE}=\sqrt{\frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}}
$$</p>
<p>Where: $y_{i}$ : The experimental result for the $i$-th hypothesis. $\hat{y}_{i}$ : The simulated result for the $i$-th hypothesis. The CSX-Sim exhibited a lower RMSE than the "Matched Score" baseline (Yang et al., 2025), signifying improved predictive accuracy, as substantiated by the results in Table 1.</p>
<p>To thoroughly evaluate the predictive accuracy of our simulator compared to real-world experimental outcomes, we tested its performance on a dataset of 124 authentic scientific hypotheses. For a comprehensive comparison, we calculated several performance indicators, as presented in Table 7. Building on the previously discussed metrics, we introduced three additional measures: Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Logarithmic Error (RMSLE). These metrics, defined below, enhance the robustness of our analysis by capturing different aspects of prediction error.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Simulator</th>
<th style="text-align: center;">MSE $(\downarrow)$</th>
<th style="text-align: center;">MAE $(\downarrow)$</th>
<th style="text-align: center;">RMSLE $(\downarrow)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Matched Score</td>
<td style="text-align: center;">0.068</td>
<td style="text-align: center;">0.179</td>
<td style="text-align: center;">0.166</td>
</tr>
<tr>
<td style="text-align: center;">CSX-Sim</td>
<td style="text-align: center;">$\mathbf{0 . 0 5 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 6 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 4 7}$</td>
</tr>
<tr>
<td style="text-align: center;">w/o CriticalPoints</td>
<td style="text-align: center;">0.064</td>
<td style="text-align: center;">0.174</td>
<td style="text-align: center;">0.159</td>
</tr>
<tr>
<td style="text-align: center;">w/o ComponentExtraction</td>
<td style="text-align: center;">0.087</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.192</td>
</tr>
</tbody>
</table>
<p>Table 7: Validating the simulator with collected chemistry experiment results from literature.</p>
<p>Below, we define each metric used in the evaluation, along with their respective formulas, to ensure scientific rigor:</p>
<p>Mean Squared Error (MSE): MSE measures the average squared difference between predicted values $\hat{y}<em i="i">{i}$ and actual values $y</em>$ across $n$ samples. It is defined as:</p>
<p>$$
\mathrm{MSE}=\frac{1}{n} \sum_{i=1}^{n}\left(\hat{y}<em i="i">{i}-y</em>
$$}\right)^{2</p>
<p>A lower MSE indicates higher predictive accuracy, with larger errors penalized more heavily due to squaring.
Mean Absolute Error (MAE): MAE quantifies the average absolute difference between predicted and actual values, calculated as:</p>
<p>$$
\mathrm{MAE}=\frac{1}{n} \sum_{i=1}^{n}\left|\hat{y}<em i="i">{i}-y</em>\right|
$$</p>
<p>This metric is less sensitive to outliers than MSE, providing a more balanced measure of error.
Root Mean Squared Logarithmic Error (RMSLE): RMSLE focuses on relative errors by evaluating the logarithmic difference between predicted and actual values:</p>
<p>$$
\text { RMSLE }=\sqrt{\frac{1}{n} \sum_{i=1}^{n}\left(\log \left(\hat{y}<em i="i">{i}+1\right)-\log \left(y</em>
$$}+1\right)\right)^{2}</p>
<p>This metric is particularly useful for datasets with exponential trends or varying error scales.
As shown in Table 7, CSX-Sim consistently outperforms the "Matched Score" baseline (Yang et al., 2025) across all metrics, achieving an MSE of 0.058 , an MAE of 0.161 , and an RMSLE of 0.147 . Ablation studies further reveal the contributions of individual components: the removal of CriticalPoints results in a slight performance decline (MSE of 0.064 , MAE of 0.174 , RMSLE of 0.159 ), while the exclusion of ComponentExtraction leads to more significant degradation (MSE of 0.087 , MAE of 0.215 , RMSLE of 0.192 ). These results underscore the importance of both critical point identification and component extraction in achieving high predictive accuracy and robustness in simulation outcomes.</p>
<h1>F DIFFERENT LEVELS OF DISTORTION</h1>
<p>We collaborated with chemistry PhD students to identify and design three common types of distortions encountered in chemical research: local maxima/minima, plateaus, and cliffs. These distortion patterns reflect typical challenges in hypothesis evaluation, drawing on domain expertise and established heuristics to ensure relevance. We defined three distinct distortion levels-Simple Noise, Moderate Noise, and Complex Noise-and incorporated them into the hypothesis embedding function $\phi(\cdot)$ to simulate increasingly challenging feedback conditions.</p>
<p>In chemical scientific hypotheses, biases in understanding key factors can result in specific distortion patterns. For instance, when adding guanidine sulfate to polymer thermoelectric materials, recognizing it solely as a salt providing hydrogen bonds for the reaction-while overlooking its influence on the entropy of redox pairs-can lead to a local maximum, as this oversight may enhance thermoelectric performance unexpectedly. Similarly, misjudging irrelevant factors, such as additives in organic reactions with no actual impact, can create a plateau effect. Conversely, misjudging critical factors, like the temperature's role in enzyme activity during enzyme studies, can produce a cliff if the temperature is incorrectly assumed to inhibit the reaction entirely. These elements-local maxima/minima, plateaus, and cliffs-present significant challenges in optimization problems within chemical research.</p>
<p>Through extensive discussions with chemistry experts, we conducted a statistical analysis to evaluate the discrepancies between wet lab results and empirical expected outcomes across diverse experimental scenarios. This process enabled us to statistically analyze the frequency of the three types of distortions-local maxima/minima, plateaus, and cliffs-across various chemical scenarios. We then quantified the occurrence of these distortions in different scenarios and sorted them by frequency,</p>
<p>from low to high. Based on this distribution, we categorized the discrepancies: the top $35 \%$ of observed gaps were classified as Simple Noise, the middle $40 \%$ as Moderate Noise, and the bottom $25 \%$ as Complex Noise. Furthermore, we integrated the three distortion levels-Simple Noise, Moderate Noise, and Complex Noise-into the hypothesis embedding function $\phi(\cdot)$ to simulate increasingly challenging feedback conditions. This structured stratification provided a clear framework to evaluate the varying impacts of different scenarios on our simulator, facilitating a deeper understanding of the simulator's performance under diverse conditions.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Noise Conditions</th>
<th style="text-align: center;">Local Maxima/Minima</th>
<th style="text-align: center;">Plateaus</th>
<th style="text-align: center;">Cliffs</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Simple</td>
<td style="text-align: center;">$0-10$</td>
<td style="text-align: center;">$0-2$</td>
<td style="text-align: center;">$0-2$</td>
</tr>
<tr>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">$0-30$</td>
<td style="text-align: center;">$0-6$</td>
<td style="text-align: center;">$0-6$</td>
</tr>
<tr>
<td style="text-align: center;">Complex</td>
<td style="text-align: center;">$\geq 30$</td>
<td style="text-align: center;">$\geq 3$</td>
<td style="text-align: center;">$\geq 3$</td>
</tr>
</tbody>
</table>
<p>Table 8: The composition of different types of noise.</p>
<p>These distortions, along with their detailed quantities, are outlined in the accompanying Table 8, which illustrates the composition of different types of noise across various conditions. For instance, simple noise conditions are associated with $0-10$ local maxima/minima, $0-2$ plateaus, and $0-2$ cliffs. Medium noise conditions escalate these figures to $0-30$ local maxima/minima, $0-6$ plateaus, and $0-6$ cliffs. In complex noise scenarios, the challenges intensify, with $\geq 30$ local maxima/minima, $\geq 3$ plateaus, and $\geq 3$ cliffs, reflecting the increased difficulty in achieving optimal solutions. We constructed three distinct noise levels to evaluate the robustness of our CSX-Rank under complex chemical feedback conditions.</p>
<p>By comparing Table 3, we observed that with the introduction of noise, the experiment-guided ranking method requires a significantly higher number of simulation feedback iterations to identify the ground truth scientific hypothesis as the complexity of the noise increases. This is primarily due to the growing discrepancy between highly complex noise and real experimental feedback, where simulation feedback contains substantial erroneous information, thereby degrading the performance of screening the ground truth scientific hypothesis from the generated scientific hypotheses.</p>
<h1>G PERFORMANCE COMPARISON OF DIFFERENT FUNCTIONS IN THE SIMULATOR</h1>
<p>Table 9: Performance comparison of different functions in the simulator.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Function</th>
<th style="text-align: center;">Spearman Corr. $(\uparrow)$</th>
<th style="text-align: center;">RMSE $(\downarrow)$</th>
<th style="text-align: center;">Perfect Consistency $(\uparrow)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Linear Function</td>
<td style="text-align: center;">0.9708</td>
<td style="text-align: center;">0.1959</td>
<td style="text-align: center;">$24 / 30$</td>
</tr>
<tr>
<td style="text-align: left;">Gaussian Function</td>
<td style="text-align: center;">0.9600</td>
<td style="text-align: center;">0.2147</td>
<td style="text-align: center;">$26 / 30$</td>
</tr>
<tr>
<td style="text-align: left;">Absolute Value Function</td>
<td style="text-align: center;">0.9626</td>
<td style="text-align: center;">0.2595</td>
<td style="text-align: center;">$23 / 30$</td>
</tr>
<tr>
<td style="text-align: left;">Quadratic Function</td>
<td style="text-align: center;">0.9682</td>
<td style="text-align: center;">0.3996</td>
<td style="text-align: center;">$22 / 30$</td>
</tr>
</tbody>
</table>
<p>This supplementary study was conducted to validate the robustness of our core mathematical modeling. While the Gaussian function was selected for its well-behaved mathematical properties and intuitive alignment with our core assumptions, the framework's success is not tied to any single function form. The choice of function can be viewed as a tunable hyperparameter.</p>
<p>Table 9 presents the results of a comparative study of various monotonic functions serving as the core of the simulator. Performance was assessed on Spearman Correlation ( $\uparrow$ ), RMSE ( $\downarrow$ ), and Perfect Consistency $(\uparrow)$. The results show that all tested functions provide effective ranking guidance, which underscores the framework's overall robustness. This analysis confirms that our framework is adaptable and can accommodate different function forms, enhancing its generalizability across domains.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ All code and data can be found in https://github.com/wanhaoliu/MOOSE-Chem3.git
${ }^{\text {a }}$ Both authors contributed equally to this work.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
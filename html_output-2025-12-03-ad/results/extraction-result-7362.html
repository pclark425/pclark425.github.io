<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7362 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7362</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7362</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-271516493</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.18743v1.pdf" target="_blank">Towards Effective and Efficient Continual Pre-training of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Continual pre-training (CPT) has been an important approach for adapting language models to specific domains or tasks. To make the CPT approach more traceable, this paper presents a technical report for continually pre-training Llama-3 (8B), which significantly enhances the Chinese language ability and scientific reasoning ability of the backbone model. To enhance the new abilities while retaining the original abilities, we design specific data mixture and curriculum strategies by utilizing existing datasets and synthesizing high-quality datasets. Specifically, we synthesize multidisciplinary scientific question and answer (QA) pairs based on related web pages, and subsequently incorporate these synthetic data to improve the scientific reasoning ability of Llama-3. We refer to the model after CPT as Llama-3-SynE (Synthetic data Enhanced Llama-3). We also present the tuning experiments with a relatively small model -- TinyLlama, and employ the derived findings to train the backbone model. Extensive experiments on a number of evaluation benchmarks show that our approach can largely improve the performance of the backbone models, including both the general abilities (+8.81 on C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 on MATH and +4.13 on SciEval), without hurting the original capacities. Our model, data, and codes are available at https://github.com/RUC-GSAI/Llama-3-SynE.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7362.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7362.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B-Instruct-v0.35</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B-Instruct-v0.35</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned 7B-parameter LLM used in this work to synthesize large-scale multidisciplinary scientific question–answer (QA) pairs from seed web-page snippets covering nine scientific disciplines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct-v0.35</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned LLM (used as a synthetic-data generator)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multidisciplinary (mathematics, physics, chemistry, biology, astronomy, earth science, medical science, computer science, general education)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation: generate high-quality domain-specific QA pairs (problem + step-by-step solution) from seed scientific web-page snippets to emulate textbook/exam-style scientific problems for downstream model pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Fill a designed prompt template with extracted content snippets (seed snippet) and discipline label; use instruction-following prompt (Appendix B) requesting comprehensive step-by-step solutions (~250–350 words) and proper equation formatting. (No numeric few-shot exemplar counts for generation are stated beyond using the template.)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Not directly evaluated as a simulator by intrinsic metrics in the paper; effectiveness measured indirectly via downstream LLM performance on standard benchmarks (e.g., SciEval, SciQ, GaoKao, MATH) after continual pre-training with the generated data.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['quality/accuracy of synthetic outputs (corruption tolerance affects downstream learning)', 'proportion of synthetic data in training mix (synthetic-data ratio)', 'format similarity to downstream tasks (QA format beneficial)', 'seed corpus selection (web pages from discipline-specific domains)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Synthetic QA data: total ~1.5B tokens across disciplines; synthetic-stage mixture ratio for CPT was Chinese:English:synthetic = 1:7:2; generator used Mistral-7B-Instruct-v0.35 with the authors' prompt template (Appendix B). Synthetic data later filtered/combined into CPT corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>The generator's output accuracy is not guaranteed; downstream performance degrades when synthetic data contains high corruption (see synthetic-data quality experiments). The paper does not report intrinsic generation correctness rates or human evaluation of each generated QA pair.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Effective and Efficient Continual Pre-training of Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7362.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7362.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MagicCoder-6.7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Magicoder-S-DS-6.7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specialized 6.7B-parameter code generation LLM used here to synthesize augmented LeetCode-style coding problems and their solutions (code QA) via in-context learning for preserving and improving code capabilities during CPT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Magicoder-S-DS-6.7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6.7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>code-specialized LLM used for synthetic data generation (in-context learning)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>programming / code synthesis (used to protect/augment coding capability rather than a natural-science subdomain)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation: synthesize new coding problems and solutions by providing randomly selected LeetCode problems as demonstrations (ICL) and generating similar problems+solutions to expand code QA training data.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>In-context learning: use random LeetCode problems as demonstrations (examples) and prompt the model to synthesize new problems and corresponding solutions; used Magicoder for both problem and solution synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Not directly reported for the generator itself; effectiveness measured indirectly via downstream code benchmarks (HumanEval, MBPP) of the continually pre-trained models.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['format-match to downstream tasks (code QA format)', 'need for synthetic code data because CPT otherwise degrades coding capability', 'quality of synthesized solutions impacts downstream code-benchmark performance']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Synthesized code QA added to CPT corpus; synthesis procedure: ICL using LeetCode examples; used Magicoder-S-DS-6.7B as generator. No per-example generator accuracy or human validation statistics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Generator correctness not quantified in paper; if synthesized code is erroneous or low-quality it may harm downstream performance (the paper stresses need to retain code data to avoid CPT-induced code capability drops).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Effective and Efficient Continual Pre-training of Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7362.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7362.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (topic annotation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (used for topic annotation in zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was used in a zero-shot prompting setup to annotate a small set of web pages with topic labels (from a manually pre-defined topic taxonomy) to create training data for topic classifiers used in topic-based data mixture.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (OpenAI GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-following LLM used for zero-shot annotation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general (topic labeling across web pages covering domains linked to MMLU/CMMLU taxonomy)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based annotation: label unlabelled web pages with one of a pre-defined set of topical categories by concatenating topics and the web page into a prompt and asking GPT-4 to select the most related topic (zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot instruction: construct a prompt by concatenating the topic list and the unlabelled web page content and instruct GPT-4 to choose a single topic label (details in Appendix B).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Not reported for GPT-4 labeling itself; labels were used as seeds to train TinyBERT/BERT-tiny-Chinese topic classifiers, which were then applied corpus-wide. Topic classifiers' downstream role evaluated via PPL tracking per topic during CPT.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['zero-shot capability of GPT-4 used to bootstrap small labeled set', 'subsequent classifier quality (TinyBERT / BERT-Tiny-Chinese) affects topic assignment accuracy']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Used to annotate a small number of web pages; then trained TinyBERT and BERT-Tiny-Chinese as topic classifiers for English and Chinese pages respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>No intrinsic annotation accuracy or human verification reported; downstream topic-classification errors could affect topic-based mixture decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Effective and Efficient Continual Pre-training of Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7362.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7362.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TinyLlama surrogate experiments</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TinyLlama (1.1B) surrogate model experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 1.1B-parameter model (TinyLlama) used extensively as a computationally cheap surrogate to explore CPT data curation strategies (synthetic-data ratio, synthetic-data quality corruption, curriculum ordering, topic mixture) before applying findings to Llama-3 (8B).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TinyLlama</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.1B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>pretrained base model used as surrogate for continual-pretraining experiments</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multidisciplinary (evaluated on both general and scientific benchmarks: C-Eval, CMMLU, MMLU, SciEval, SciQ, ARC etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Evaluation of how synthetic scientific QA (and code QA) used as pretraining data influences downstream scientific/mathematical reasoning and general capabilities; not used to generate scientific simulations but to test the effect of synthetic-data-as-training on model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Not an LLM-as-simulator entry per se; TinyLlama is evaluated under few-shot/zero-shot prompting for benchmarks: e.g., 5-shot for many benchmarks, 8-shot for some math sets, 3-shot for MBPP, zero-shot for HumanEval/ARC as in the evaluation protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Benchmark accuracy (few-shot/zero-shot) averaged across selected major and scientific benchmarks (C-Eval, CMMLU, MMLU for major; SciEval, SciQ, ARC for scientific); figures show average-performances under different conditions. Specific metrics per benchmark: accuracy (percent correct), pass@k-style or problem-solve rates for code/math benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['synthetic-data quality (corruption experiments): models tolerate low corruption (~0.3) but degrade substantially for high corruption (>0.5)', 'synthetic-data ratio: performance improves up to a point (best ~20% synthetic) then declines (decline at ~40%)', 'data curriculum: easy-to-hard (low→high PPL) improves more than hard→easy or random; discipline-separated ordering can hurt', 'format alignment: QA-format synthetic data benefits downstream tasks', 'model and context differences: TinyLlama used different lr and context settings than Llama-3']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>TinyLlama pretraining experiments used 5B-token CPT datasets in variants (e.g., 4B normal +1B synthetic). TinyLlama specifics: 1.1B params, pre-trained on 3T tokens, fixed learning rate 1.0×10^-4, max context length 2048. Evaluation used few-shot/zero-shot protocols per benchmark as described in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Reported results are surrogate: findings may not transfer perfectly to larger models; numeric values for TinyLlama experiments are presented in figures (not all exact numbers tabulated in main text). Surrogate may under/over-estimate effects on Llama-3.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Effective and Efficient Continual Pre-training of Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7362.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7362.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3-SynE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3-SynE (Synthetic data Enhanced Llama-3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The continual-pretrained version of Llama-3 (8B) produced in this work via CPT (≈100B tokens, including ~1.5B synthetic tokens) that incorporates bilingual adaptation and synthetic scientific+code QA to improve Chinese language ability and multidisciplinary scientific reasoning while aiming to retain original capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-SynE (continually pre-trained Llama-3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>base Llama-3 backbone continually pre-trained (domain-adapted) with mixed natural and synthetic corpora (bilingual + synthetic QA + code QA)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multidisciplinary scientific reasoning (math, physics, chemistry, biology, earth/astronomy, medical, computer science) — evaluated across multidisciplinary scientific benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Solve/explain scientific and mathematical problems (text-based reasoning) and code-generation tasks under few-shot/zero-shot prompting on standard benchmarks (SciEval, SciQ, GaoKao, ARC, AQUA-RAT, MATH, GSM8K, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Evaluation used standard few-shot/zero-shot prompting per benchmark: e.g., 5-shot for many benchmarks (C-Eval, CMMLU, MMLU, MATH, SciQ, SciEval), 8-shot for GSM8K/ASDiv/MAWPS, 3-shot for MBPP, zero-shot for HumanEval and ARC. No chain-of-thought or self-consistency techniques are reported as applied in primary evaluation (paper focuses on CPT training).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Benchmark accuracy / percent correct on many standard datasets (few-shot/zero-shot accuracy). Code benchmarks measured by standard code-evaluation metrics used in HumanEval/MBPP (problem pass rates), math/science by accuracy on test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Selected reported few-shot results (from main paper tables): MMLU 65.19, C-Eval 58.24, CMMLU 57.34, MATH 28.20, GSM8K 60.80, HumanEval 45.60, MBPP 42.07; Scientific benchmarks (few-shot): SciEval 53.66, SciQ 67.81, GaoKao 77.45, ARC 69.60 (values are percent-accuracy scores reported in Tables 5 & 6).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Primary backbone baseline Llama-3 (8B) reported in tables: MMLU 66.60, C-Eval 49.43, CMMLU 51.03, MATH 16.20, GSM8K 54.40; SciEval 46.95, SciQ 63.45, GaoKao 74.53, ARC 65.47. Paper reports deltas: e.g., +8.81 on C-Eval, +6.31 on CMMLU, +12.00 on MATH (improvement over Llama-3 baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['inclusion of high-quality synthetic scientific QA and code QA improves scientific/mathematical and coding capabilities', 'synthetic-data quality (corruption) and proportion (optimal ~20% synthetic) affect accuracy', 'data curriculum (perplexity/easy→hard) and topic-based mixture during bilingual adaptation influence retention and domain gains', 'format alignment (QA format) improves downstream transfer', 'careful mixing to avoid catastrophic forgetting (e.g., maintain English/other-domain data during CPT)', 'training budget (they emphasize efficient CPT: ~100B tokens for Llama-3)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Continual pretraining performed in two stages: bilingual adaptation (Chinese:English ratio 2:8, using topic-based data mixture and PPL-based curriculum) and synthetic enhancement (Chinese:English:synthetic = 1:7:2). Training used BFloat16 mixed precision, AdamW, Warmup-Stable-Decay scheduler, warmup lr from 1e-7→1e-5 over 10B tokens then constant 1e-5; gradient clipping=1.0; gradient checkpointing; max context length=8192; total training tokens ~100B (dataset table shows total ~100B with 1.5B synthetic). Evaluations used few-shot/zero-shot protocols as specified per benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Slight underperformance on some English-oriented benchmarks (e.g., marginally lower MMLU than backbone) reflecting remaining catastrophic forgetting risk; synthetic data with high corruption degrades performance; overly high synthetic proportion (>~40%) hurts performance; deliberate separation of disciplines in synthetic curriculum can harm results; no per-example human validation of synthetic QA correctness reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Effective and Efficient Continual Pre-training of Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Jiuzhang3.0: Efficiently improving mathematical reasoning by training small data synthesis models <em>(Rating: 2)</em></li>
                <li>Metamath: Bootstrap your own mathematical questions for large language models <em>(Rating: 2)</em></li>
                <li>Mammoth2: Scaling instructions from the web <em>(Rating: 1)</em></li>
                <li>Cosmopedia <em>(Rating: 2)</em></li>
                <li>GPT-4 technical report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7362",
    "paper_id": "paper-271516493",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "Mistral-7B-Instruct-v0.35",
            "name_full": "Mistral-7B-Instruct-v0.35",
            "brief_description": "An instruction-tuned 7B-parameter LLM used in this work to synthesize large-scale multidisciplinary scientific question–answer (QA) pairs from seed web-page snippets covering nine scientific disciplines.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct-v0.35",
            "model_size": "7B",
            "model_type": "instruction-tuned LLM (used as a synthetic-data generator)",
            "scientific_domain": "multidisciplinary (mathematics, physics, chemistry, biology, astronomy, earth science, medical science, computer science, general education)",
            "simulation_task_description": "Text-based simulation: generate high-quality domain-specific QA pairs (problem + step-by-step solution) from seed scientific web-page snippets to emulate textbook/exam-style scientific problems for downstream model pretraining.",
            "prompting_strategy": "Fill a designed prompt template with extracted content snippets (seed snippet) and discipline label; use instruction-following prompt (Appendix B) requesting comprehensive step-by-step solutions (~250–350 words) and proper equation formatting. (No numeric few-shot exemplar counts for generation are stated beyond using the template.)",
            "evaluation_metric": "Not directly evaluated as a simulator by intrinsic metrics in the paper; effectiveness measured indirectly via downstream LLM performance on standard benchmarks (e.g., SciEval, SciQ, GaoKao, MATH) after continual pre-training with the generated data.",
            "reported_accuracy": null,
            "baseline_accuracy": null,
            "factors_reported": [
                "quality/accuracy of synthetic outputs (corruption tolerance affects downstream learning)",
                "proportion of synthetic data in training mix (synthetic-data ratio)",
                "format similarity to downstream tasks (QA format beneficial)",
                "seed corpus selection (web pages from discipline-specific domains)"
            ],
            "experimental_conditions": "Synthetic QA data: total ~1.5B tokens across disciplines; synthetic-stage mixture ratio for CPT was Chinese:English:synthetic = 1:7:2; generator used Mistral-7B-Instruct-v0.35 with the authors' prompt template (Appendix B). Synthetic data later filtered/combined into CPT corpus.",
            "limitations_or_failure_modes": "The generator's output accuracy is not guaranteed; downstream performance degrades when synthetic data contains high corruption (see synthetic-data quality experiments). The paper does not report intrinsic generation correctness rates or human evaluation of each generated QA pair.",
            "uuid": "e7362.0",
            "source_info": {
                "paper_title": "Towards Effective and Efficient Continual Pre-training of Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "MagicCoder-6.7B",
            "name_full": "Magicoder-S-DS-6.7B",
            "brief_description": "A specialized 6.7B-parameter code generation LLM used here to synthesize augmented LeetCode-style coding problems and their solutions (code QA) via in-context learning for preserving and improving code capabilities during CPT.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Magicoder-S-DS-6.7B",
            "model_size": "6.7B",
            "model_type": "code-specialized LLM used for synthetic data generation (in-context learning)",
            "scientific_domain": "programming / code synthesis (used to protect/augment coding capability rather than a natural-science subdomain)",
            "simulation_task_description": "Text-based simulation: synthesize new coding problems and solutions by providing randomly selected LeetCode problems as demonstrations (ICL) and generating similar problems+solutions to expand code QA training data.",
            "prompting_strategy": "In-context learning: use random LeetCode problems as demonstrations (examples) and prompt the model to synthesize new problems and corresponding solutions; used Magicoder for both problem and solution synthesis.",
            "evaluation_metric": "Not directly reported for the generator itself; effectiveness measured indirectly via downstream code benchmarks (HumanEval, MBPP) of the continually pre-trained models.",
            "reported_accuracy": null,
            "baseline_accuracy": null,
            "factors_reported": [
                "format-match to downstream tasks (code QA format)",
                "need for synthetic code data because CPT otherwise degrades coding capability",
                "quality of synthesized solutions impacts downstream code-benchmark performance"
            ],
            "experimental_conditions": "Synthesized code QA added to CPT corpus; synthesis procedure: ICL using LeetCode examples; used Magicoder-S-DS-6.7B as generator. No per-example generator accuracy or human validation statistics reported.",
            "limitations_or_failure_modes": "Generator correctness not quantified in paper; if synthesized code is erroneous or low-quality it may harm downstream performance (the paper stresses need to retain code data to avoid CPT-induced code capability drops).",
            "uuid": "e7362.1",
            "source_info": {
                "paper_title": "Towards Effective and Efficient Continual Pre-training of Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4 (topic annotation)",
            "name_full": "GPT-4 (used for topic annotation in zero-shot)",
            "brief_description": "GPT-4 was used in a zero-shot prompting setup to annotate a small set of web pages with topic labels (from a manually pre-defined topic taxonomy) to create training data for topic classifiers used in topic-based data mixture.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "not specified (OpenAI GPT-4)",
            "model_type": "instruction-following LLM used for zero-shot annotation",
            "scientific_domain": "general (topic labeling across web pages covering domains linked to MMLU/CMMLU taxonomy)",
            "simulation_task_description": "Text-based annotation: label unlabelled web pages with one of a pre-defined set of topical categories by concatenating topics and the web page into a prompt and asking GPT-4 to select the most related topic (zero-shot).",
            "prompting_strategy": "Zero-shot instruction: construct a prompt by concatenating the topic list and the unlabelled web page content and instruct GPT-4 to choose a single topic label (details in Appendix B).",
            "evaluation_metric": "Not reported for GPT-4 labeling itself; labels were used as seeds to train TinyBERT/BERT-tiny-Chinese topic classifiers, which were then applied corpus-wide. Topic classifiers' downstream role evaluated via PPL tracking per topic during CPT.",
            "reported_accuracy": null,
            "baseline_accuracy": null,
            "factors_reported": [
                "zero-shot capability of GPT-4 used to bootstrap small labeled set",
                "subsequent classifier quality (TinyBERT / BERT-Tiny-Chinese) affects topic assignment accuracy"
            ],
            "experimental_conditions": "Used to annotate a small number of web pages; then trained TinyBERT and BERT-Tiny-Chinese as topic classifiers for English and Chinese pages respectively.",
            "limitations_or_failure_modes": "No intrinsic annotation accuracy or human verification reported; downstream topic-classification errors could affect topic-based mixture decisions.",
            "uuid": "e7362.2",
            "source_info": {
                "paper_title": "Towards Effective and Efficient Continual Pre-training of Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "TinyLlama surrogate experiments",
            "name_full": "TinyLlama (1.1B) surrogate model experiments",
            "brief_description": "A 1.1B-parameter model (TinyLlama) used extensively as a computationally cheap surrogate to explore CPT data curation strategies (synthetic-data ratio, synthetic-data quality corruption, curriculum ordering, topic mixture) before applying findings to Llama-3 (8B).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "TinyLlama",
            "model_size": "1.1B parameters",
            "model_type": "pretrained base model used as surrogate for continual-pretraining experiments",
            "scientific_domain": "multidisciplinary (evaluated on both general and scientific benchmarks: C-Eval, CMMLU, MMLU, SciEval, SciQ, ARC etc.)",
            "simulation_task_description": "Evaluation of how synthetic scientific QA (and code QA) used as pretraining data influences downstream scientific/mathematical reasoning and general capabilities; not used to generate scientific simulations but to test the effect of synthetic-data-as-training on model behavior.",
            "prompting_strategy": "Not an LLM-as-simulator entry per se; TinyLlama is evaluated under few-shot/zero-shot prompting for benchmarks: e.g., 5-shot for many benchmarks, 8-shot for some math sets, 3-shot for MBPP, zero-shot for HumanEval/ARC as in the evaluation protocol.",
            "evaluation_metric": "Benchmark accuracy (few-shot/zero-shot) averaged across selected major and scientific benchmarks (C-Eval, CMMLU, MMLU for major; SciEval, SciQ, ARC for scientific); figures show average-performances under different conditions. Specific metrics per benchmark: accuracy (percent correct), pass@k-style or problem-solve rates for code/math benchmarks.",
            "reported_accuracy": null,
            "baseline_accuracy": null,
            "factors_reported": [
                "synthetic-data quality (corruption experiments): models tolerate low corruption (~0.3) but degrade substantially for high corruption (&gt;0.5)",
                "synthetic-data ratio: performance improves up to a point (best ~20% synthetic) then declines (decline at ~40%)",
                "data curriculum: easy-to-hard (low→high PPL) improves more than hard→easy or random; discipline-separated ordering can hurt",
                "format alignment: QA-format synthetic data benefits downstream tasks",
                "model and context differences: TinyLlama used different lr and context settings than Llama-3"
            ],
            "experimental_conditions": "TinyLlama pretraining experiments used 5B-token CPT datasets in variants (e.g., 4B normal +1B synthetic). TinyLlama specifics: 1.1B params, pre-trained on 3T tokens, fixed learning rate 1.0×10^-4, max context length 2048. Evaluation used few-shot/zero-shot protocols per benchmark as described in paper.",
            "limitations_or_failure_modes": "Reported results are surrogate: findings may not transfer perfectly to larger models; numeric values for TinyLlama experiments are presented in figures (not all exact numbers tabulated in main text). Surrogate may under/over-estimate effects on Llama-3.",
            "uuid": "e7362.3",
            "source_info": {
                "paper_title": "Towards Effective and Efficient Continual Pre-training of Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Llama-3-SynE",
            "name_full": "Llama-3-SynE (Synthetic data Enhanced Llama-3)",
            "brief_description": "The continual-pretrained version of Llama-3 (8B) produced in this work via CPT (≈100B tokens, including ~1.5B synthetic tokens) that incorporates bilingual adaptation and synthetic scientific+code QA to improve Chinese language ability and multidisciplinary scientific reasoning while aiming to retain original capabilities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3-SynE (continually pre-trained Llama-3)",
            "model_size": "8B parameters",
            "model_type": "base Llama-3 backbone continually pre-trained (domain-adapted) with mixed natural and synthetic corpora (bilingual + synthetic QA + code QA)",
            "scientific_domain": "multidisciplinary scientific reasoning (math, physics, chemistry, biology, earth/astronomy, medical, computer science) — evaluated across multidisciplinary scientific benchmarks",
            "simulation_task_description": "Solve/explain scientific and mathematical problems (text-based reasoning) and code-generation tasks under few-shot/zero-shot prompting on standard benchmarks (SciEval, SciQ, GaoKao, ARC, AQUA-RAT, MATH, GSM8K, etc.).",
            "prompting_strategy": "Evaluation used standard few-shot/zero-shot prompting per benchmark: e.g., 5-shot for many benchmarks (C-Eval, CMMLU, MMLU, MATH, SciQ, SciEval), 8-shot for GSM8K/ASDiv/MAWPS, 3-shot for MBPP, zero-shot for HumanEval and ARC. No chain-of-thought or self-consistency techniques are reported as applied in primary evaluation (paper focuses on CPT training).",
            "evaluation_metric": "Benchmark accuracy / percent correct on many standard datasets (few-shot/zero-shot accuracy). Code benchmarks measured by standard code-evaluation metrics used in HumanEval/MBPP (problem pass rates), math/science by accuracy on test sets.",
            "reported_accuracy": "Selected reported few-shot results (from main paper tables): MMLU 65.19, C-Eval 58.24, CMMLU 57.34, MATH 28.20, GSM8K 60.80, HumanEval 45.60, MBPP 42.07; Scientific benchmarks (few-shot): SciEval 53.66, SciQ 67.81, GaoKao 77.45, ARC 69.60 (values are percent-accuracy scores reported in Tables 5 & 6).",
            "baseline_accuracy": "Primary backbone baseline Llama-3 (8B) reported in tables: MMLU 66.60, C-Eval 49.43, CMMLU 51.03, MATH 16.20, GSM8K 54.40; SciEval 46.95, SciQ 63.45, GaoKao 74.53, ARC 65.47. Paper reports deltas: e.g., +8.81 on C-Eval, +6.31 on CMMLU, +12.00 on MATH (improvement over Llama-3 baseline).",
            "factors_reported": [
                "inclusion of high-quality synthetic scientific QA and code QA improves scientific/mathematical and coding capabilities",
                "synthetic-data quality (corruption) and proportion (optimal ~20% synthetic) affect accuracy",
                "data curriculum (perplexity/easy→hard) and topic-based mixture during bilingual adaptation influence retention and domain gains",
                "format alignment (QA format) improves downstream transfer",
                "careful mixing to avoid catastrophic forgetting (e.g., maintain English/other-domain data during CPT)",
                "training budget (they emphasize efficient CPT: ~100B tokens for Llama-3)"
            ],
            "experimental_conditions": "Continual pretraining performed in two stages: bilingual adaptation (Chinese:English ratio 2:8, using topic-based data mixture and PPL-based curriculum) and synthetic enhancement (Chinese:English:synthetic = 1:7:2). Training used BFloat16 mixed precision, AdamW, Warmup-Stable-Decay scheduler, warmup lr from 1e-7→1e-5 over 10B tokens then constant 1e-5; gradient clipping=1.0; gradient checkpointing; max context length=8192; total training tokens ~100B (dataset table shows total ~100B with 1.5B synthetic). Evaluations used few-shot/zero-shot protocols as specified per benchmark.",
            "limitations_or_failure_modes": "Slight underperformance on some English-oriented benchmarks (e.g., marginally lower MMLU than backbone) reflecting remaining catastrophic forgetting risk; synthetic data with high corruption degrades performance; overly high synthetic proportion (&gt;~40%) hurts performance; deliberate separation of disciplines in synthetic curriculum can harm results; no per-example human validation of synthetic QA correctness reported.",
            "uuid": "e7362.4",
            "source_info": {
                "paper_title": "Towards Effective and Efficient Continual Pre-training of Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Jiuzhang3.0: Efficiently improving mathematical reasoning by training small data synthesis models",
            "rating": 2,
            "sanitized_title": "jiuzhang30_efficiently_improving_mathematical_reasoning_by_training_small_data_synthesis_models"
        },
        {
            "paper_title": "Metamath: Bootstrap your own mathematical questions for large language models",
            "rating": 2,
            "sanitized_title": "metamath_bootstrap_your_own_mathematical_questions_for_large_language_models"
        },
        {
            "paper_title": "Mammoth2: Scaling instructions from the web",
            "rating": 1,
            "sanitized_title": "mammoth2_scaling_instructions_from_the_web"
        },
        {
            "paper_title": "Cosmopedia",
            "rating": 2,
            "sanitized_title": "cosmopedia"
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 1,
            "sanitized_title": "gpt4_technical_report"
        }
    ],
    "cost": 0.0184605,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Effective and Efficient Continual Pre-training of Large Language Models
26 Jul 2024</p>
<p>Jie Chen 
YuLan team
Renmin University of China</p>
<p>Zhipeng Chen 
YuLan team
Renmin University of China</p>
<p>Jiapeng Wang 
YuLan team
Renmin University of China</p>
<p>Kun Zhou 
YuLan team
Renmin University of China</p>
<p>Yutao Zhu 
YuLan team
Renmin University of China</p>
<p>Jinhao Jiang 
YuLan team
Renmin University of China</p>
<p>Yingqian Min 
YuLan team
Renmin University of China</p>
<p>Wayne Xin Zhao 
YuLan team
Renmin University of China</p>
<p>Zhicheng Dou dou@ruc.edu.cn 
YuLan team
Renmin University of China</p>
<p>Jiaxin Mao 
YuLan team
Renmin University of China</p>
<p>Yankai Lin 
YuLan team
Renmin University of China</p>
<p>Ruihua Song 
YuLan team
Renmin University of China</p>
<p>Jun Xu 
YuLan team
Renmin University of China</p>
<p>Xu Chen 
YuLan team
Renmin University of China</p>
<p>Rui Yan 
YuLan team
Renmin University of China</p>
<p>Zhewei Wei 
YuLan team
Renmin University of China</p>
<p>Di Hu 
YuLan team
Renmin University of China</p>
<p>Wenbing Huang 
YuLan team
Renmin University of China</p>
<p>Ji-Rong Wen jrwen@ruc.edu.cn 
YuLan team
Renmin University of China</p>
<p>Towards Effective and Efficient Continual Pre-training of Large Language Models
26 Jul 20246CBF2A33C1139095139949B6BBCCACF8arXiv:2407.18743v1[cs.CL]
Continual pre-training (CPT) has been an important approach for adapting language models to specific domains or tasks.To make the CPT approach more traceable, this paper presents a technical report for continually pretraining Llama-3 (8B), which significantly enhances the Chinese language ability and scientific reasoning ability of the backbone model.To enhance the new abilities while retaining the original abilities, we design specific data mixture and curriculum strategies by utilizing existing datasets and synthesizing high-quality datasets.Specifically, we synthesize multidisciplinary scientific question and answer (QA) pairs based on related web pages, and subsequently incorporate these synthetic data to improve the scientific reasoning ability of Llama-3.We refer to the model after CPT as Llama-3-SynE (Synthetic data Enhanced Llama-3).We also present the tuning experiments with a relatively small model-TinyLlama, and employ the derived findings to train the backbone model.Extensive experiments on a number of evaluation benchmarks show that our approach can largely improve the performance of the backbone models, including both the general abilities (+8.81 on C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 on MATH and +4.13 on SciEval), without hurting the original capacities.Our model, data, and codes are available at https: //github.com/RUC-GSAI/Llama-3-SynE.</p>
<p>Introduction</p>
<p>Recently, large language models (LLMs) (Zhao et al., 2023) have achieved great progress in accelerating the development of artificial intelligence.Unlike traditional machine learning methods, LLMs basically undergo large-scale pre-training on unsupervised corpora, e.g., trillions of training tokens.Through pre-training, LLMs can learn extensive knowledge from unsupervised data and acquire the capability of solving various downstream tasks via prompting (Touvron et al., 2023a;OpenAI, 2023;Team et al., 2024).</p>
<p>Despite the success, LLMs still struggle in some specific scenarios, due to the large knowledge gap between pre-training data and downstream tasks.For example, Llama-3 (AI@Meta, 2024), primarily trained on English corpora, performs inadequately on Chinese-oriented tasks.Additionally, as a general-purpose LLM, Llama-3 might lack sufficient multidisciplinary scientific knowledge, e.g., physics and biology.To address these issues, a widely-used approach is to conduct continual pretraining (CPT) for LLMs on specially-curated data related to the expected abilities (Ke et al., 2023;Gupta et al., 2023;Ibrahim et al., 2024).During the CPT process, catastrophic forgetting (Luo et al., 2023) has become a common technical issue, where new capabilities are improved but original capabilities are substantially hurt.Although CPT has been widely conducted in existing work, the key training details (e.g., data selection, mixture, and curriculum) to develop new abilities and maintain existing abilities have not been well discussed, especially how to boost the comprehensive capacities of a well-trained model under a limited training budget.</p>
<p>In this paper, we present the technical report for continually pre-training the open-sourced LLM-Llama-3 (8B), with all experimental data, model checkpoints, and training code released.Our focus is to enhance the model's capacities from two major aspects: Chinese language ability and scientific reasoning ability, while retaining its original capabilities.To achieve this, we design specific data curation strategies to improve the backbone models.For Chinese language ability, we collect and select extensive Chinese text data from diverse sources for effective bilingual adaptation.For scientific reasoning ability, we draw inspiration from the exercises in textbooks and employ LLMs to synthesize sci-entific question and answer (QA) pairs based on the content of web pages in the pre-training corpus.Furthermore, we also incorporate large-scale text data from various sources (e.g., websites, books, and examinations) and different formats (e.g., natural language and code) into the CPT data, to preserve the general capabilities.We carefully filter and select high-quality training data, following the processing approach used in YuLan-3 (Zhu et al., 2024).</p>
<p>During the CPT process, it is key to explore various potential strategies for data collection, mixture, and curriculum design, akin to those used in standard pre-training (Hu et al., 2024;Zhu et al., 2024).However, considering the huge experimental cost on Llama-3 (8B), we perform surrogate experiments using a relatively small model, TinyLlama (Zhang et al., 2024).Based on TinyLlama, we extensively examine the effect of different data curation strategies, and further verify the findings in training Llama-3 (8B).To follow the nomenclature for Llama models, we refer to the continually pre-trained model in this work as Llama-3-SynE (Synthetic data Enhanced Llama-3).</p>
<p>To evaluate the effectiveness of our approach, we conduct comprehensive experiments comparing Llama-3-SynE with other competitive LLMs across various evaluation benchmarks, including general and scientific scenarios.Experimental results have shown that our data strategies significantly enhance the overall capabilities of Llama-3 (8B), particularly in Chinese language understanding and scientific knowledge reasoning.Specifically, we find that synthetic data is very useful to enhance the capacities of LLMs in scientific knowledge reasoning.In summary, our contributions are as follows:</p>
<p>• We present the complete training procedure for continually pre-training Llama-3 (8B), including data selection, mixture, and curriculum.Extensive experiments show that our CPT approach is very effective (yielding large improvements on Chinese and scientific benchmarks without hurting the performance on English benchmarks) and efficient (consuming only about 100B tokens).</p>
<p>• We extensively explore the data synthesis technique, and generate high-quality scientific and code data (in the format of QA pairs).We show that these synthetic data can largely improve the corresponding capabilities of LLMs.</p>
<p>To the best of our knowledge, it is the first public work that reports how to synthesize and utilize large-scale multidisciplinary scientific data for continual pre-training.</p>
<p>• We release the whole dataset utilized to continually pre-train Llama-3-SynE, including the general corpus comprising 98.5 billion tokens and synthetic data comprising 1.5 billion tokens focusing on scientific reasoning and coding tasks.Our dataset would be highly useful for training capable LLMs, which has been also evidenced by the surrogate model TinyLlama in our experiments.</p>
<p>Related Work</p>
<p>In this section, we review the related work in the following three aspects.</p>
<p>Synthetic Data</p>
<p>In order to enhance specific abilities of LLMs (e.g., mathematical reasoning (Cobbe et al., 2021), code generation (Chen et al., 2021), and scientific QA (Sun et al., 2024a)), it is crucial to train them with a sufficient amount of domainrelated data.However, the available real-world data may not be enough for models to acquire the necessary knowledge.To address this issue, synthetic data has been widely used in the training of LLMs, including general document data for pre-training (Maini et al., 2024), instruction data for supervised fine-tuning (Xu et al., 2023), and beyond.There exist two primary methods for automatic data synthesis: directly prompting LLM APIs (Xu et al., 2023;Ding et al., 2023) and training customized synthetic models (Zhou et al., 2024;Yue et al., 2024).By prompting with task instructions and suitable examplar data, capable LLMs (e.g., GPT-4) can generate high-quality data that closely resembles the distribution of real-world data, potentially injecting the knowledge that they have acquired during training.In addition, existing works also explore training relatively smaller customized models to synthesize more domain-specific data with much less API cost (Zhou et al., 2024) (French, 1999;Nguyen et al., 2019).Existing works have extensively studied fine-grained factors in mitigating catastrophic forgetting during continual pretraining, including warm-up method (Gupta et al., 2023), data distribution (Ibrahim et al., 2024;Parmar et al., 2024), and learning rate (Winata et al., 2023;Scialom et al., 2022).To address this issue, we focus on designing effective data curation strategies (i.e., data collection, mixture, and curriculum) in this work, and employ syntetic data to enhance the desired abilities of LLMs.</p>
<p>Scientific Large Language Models</p>
<p>The remarkable capabilities of LLMs have led to an increasing inclination towards their utilization in scientific application scenarios.To enhance the capacity of LLMs to comprehend and resolve scientific problems, extensive efforts have been devoted to training scientific-oriented large language models, such as mathematics LLMs (Yue et al., 2024;Shao et al., 2024;Zhou et al., 2024), biological LLMs (Jr. andBepler, 2023;Zhang et al., 2023) and chemical LLMs (Bagal et al., 2022;Bran et al., 2024).By leveraging the large-scale synthetic scientific QA data, our goal is to broaden the multidisciplinary scientific knowledge of LLMs while effectively preserving its original capabilities.</p>
<p>The Proposed CPT Approach</p>
<p>In this section, we present the proposed continual pre-training (CPT) approach for enhancing the Chinese and scientific capabilities of LLMs.</p>
<p>Overview</p>
<p>We first provide an overall description of our CPT approach from three main aspects, including backbone model, data source, and training procedure.</p>
<p>Backbone Model</p>
<p>To conduct the research on CPT, we adopt Llama-3 (8B) (AI@Meta, 2024) as the backbone model, which has excelled in various downstream tasks such as text generation, translation, summarization, and questionanswering.However, Llama-3 has been primarily pre-trained on English text data, which is inadequate in Chinese-oriented tasks.In addition, since Llama-3 was developed as a general-purpose LLM,  it may also lack sufficient scientific knowledge.Considering these two limitations, we aim to improve Llama-3's Chinese capacities as well as to enhance its performance in multidisciplinary scientific tasks.It is worth noting that the proposed approach can be generally applied to other backbone models, as evidenced by our experiments on the relatively smaller model TinyLlama (Section 4.2).</p>
<p>Data Source</p>
<p>The selection of data sources is key to the capacities of LLMs.To prepare the pretraining data, we mainly refer to the data configuration of Yulan-3 (Zhu et al., 2024), which collects a diverse set of data, including web pages, encyclopedias, books, question-answering (QA) forums, academic papers, mathematical corpora, code, and synthetic data.Table 1 provides detailed information about the composition of our training data.We perform careful data cleaning following common strategies used in prior work (Sun et al., 2024b).</p>
<p>Training Procedure Overall, our training procedure consists of two main stages, namely bilingual adaptation stage and synthetic enhancement stage, which focus on improving Llama-3's Chinese and scientific capacities, respectively.In the CPT process, it is important to retain the original capability of Llama-3 by alleviating the effect of catastrophic forgetting.For this purpose, we design different data strategies to balance new and old abilities, which will be detailed in the following sections.</p>
<p>The volume of training data and data strategies used in each stage are shown in Table 2.</p>
<p>Bilingual Adaptation Stage</p>
<p>We first introduce the training approach for improving the Chinese capacities of Llama-3.Following our previous experiences with Yulan-3 (Zhu et al., 2024), we set the ratio of Chinese and English corpora as 2:8, to balance the Chinese and English capabilities.For pre-training, effective data mixture and schedule strategies are key to improving the capacities of LLMs.Based on the overall English-Chinese ratio, we further design two strategies to enhance knowledge learning from diverse domains or sources, namely topic-based data mixture and perplexity-based data curriculum.Next, we introduce the two techniques in detail.</p>
<p>Topic-based Data Mixture</p>
<p>In prior work (Xie et al., 2023), data mixture is usually conducted based on datasets or data types, e.g., setting a sampling distribution to sample data instances from available datasets.In our approach, we aim to explore a more fine-grained adjustment on data mixture.To achieve this goal, we consider establishing a topic taxonomy and conducting the data mixture at the topic level1 .Next, we present the topic-based data mixture method.</p>
<p>Topic Identification We train a classifier based on language models to identify the topic label for each web page.Specifically, we manually set topics as listed in Table 3.These topics are intentionally designed to be in alignment with the subjects of the MMLU (Hendrycks et al., 2021a) and CMMLU (Li et al., 2023) benchmarks, which can also be extended to other topic taxonomies.Furthermore, we employ GPT-4 to annotate a small number of web pages as training data for our topic classifiers.Concretely, we adopt the zero-shot setting and construct the prompt by concatenating the topics and an unlabelled web page (see the prompt detail in Appendix B).Then, we utilize the instructions to guide GPT-4 to annotate the unlabelled web page by these pre-determined topic labels.In order to conduct topic classification on both Chinese and English text, we train TinyBERT2 and  BERT-Tiny-Chinese3 as the classifiers to identify the topic labels for English and Chinese web pages, respectively.With the utilization of these classifiers, the web pages can be assigned with specific topic labels.</p>
<p>Performance Change Tracking To track the LLM's capabilities on different topic categories during the training process, we evaluate the change of the perplexity (PPL) score in each topic on the validation set.A reduction in the PPL score for a particular topic indicates an improvement in the model's capability regarding that topic.Concretely, supposing there are n topics, the performance change on the i-th topic is:
∆p i = p (t) i − p (t−1) i , i = 1, . . . , n,where p (t)
i and p (t−1) i are the PPL on the i-th topic of LLM after the t-th and (t−1)-th rounds4 of CPT process, respectively.The normalized performance change is then computed as:
δ p i = ∆p i max(|∆p i |) , i = 1, . . . , n.
Data Mixture Adjustment Based on the performance change and current topic weight w i that indicates the importance of the i-th topic, we calculate the weight adjustment coefficient f i for training data proportions:
f i = 1 + α • δ p i • w i ,
where α is a coefficient that controls the magnitude of the adjustment.After obtaining the adjustment coefficients (i.e., f 1 , f 2 , . . ., f n ), we can update the data proportions for each topic based on these coefficients.During training, let r (t−1) i be the proportion of the i-th topic for the (t − 1)-th round, then the proportion of data for the t-th round can be calculated as follows,
r (t) i = r (t−1) i • f i n j=1 r (t−1) j • f j .
By using the topic-based mixture strategy, we can easily monitor the PPL change trend in a finegrained way, and thus can better balance the abilities of LLMs across different topics or domains.</p>
<p>Perplexity-based Data Curriculum</p>
<p>In addition to adjusting the data mixture ratio, we also design a data curriculum strategy that organizes the training instances in a simple-to-complex manner.Curriculum learning has been demonstrated to be effective in many tasks (Bengio et al., 2009).Its primary principle is to gradually increase the difficulty (or complexity) of the training data.This strategy allows the model to establish a robust foundational knowledge base before learning more complex knowledge and skills.</p>
<p>Following this idea, we use the PPL score generated by the model to measure the difficulty level of the training data.Training the model on Chinese text data with a progressively increasing PPL score can provide a gradual and smooth transition in training complexity.This is particularly crucial since Llama-3 is primarily trained on a large scale of English corpora with very little Chinese data.Based on our preliminary experiments, starting with "simpler" Chinese data is beneficial to alleviate the performance loss (i.e., catastrophic forgetting) of Llama-3 in English tasks.</p>
<p>Synthetic Enhancement Stage</p>
<p>After bilingual adaptation training, the LLM's performance on Chinese tasks can be significantly improved.In this stage, we further incorporate  synthetic data to improve the multidisciplinary scientific capacities of Llama-3, inspired by prior work (Zhou et al., 2024;Jiang et al., 2024), and the data ratio is correspondingly adjusted to 1:7:2 for Chinese, English, and synthetic data, respectively.Note that both the topic-based mixture strategy and perplexity-based data curriculum are no longer used in this training stage, and we randomly sample the data following the mixture proportion from the training corpus.Next, we describe our method for synthesizing data for CPT.</p>
<p>Synthesizing the Scientific QA Data</p>
<p>Synthetic data has been demonstrated to be effective and efficient for enhancing the capabilities of LLMs (Yu et al., 2023;Yue et al., 2023;Zhou et al., 2024).Following prior work (Zhou et al., 2024), we generate synthetic data in the format of the question and answer (QA) pair, to cover a broad spectrum of multidisciplinary scientific knowledge.The synthetic questions and answers are concatenated into text and added to the CPT training corpora.Specifically, we consider nine scientific disciplines, i.e., mathematics, physics, chemistry, biology, astronomy, earth science, medical science, computer science, and general education.For each discipline, we manually collect a list of domain names relevant to the respective fields, such as math.stackexchange.comand physicsforums.com,allowing for the expansion of this list as needed to enhance the coverage.To construct a science-related seed corpus, we collect scientific web pages from Dolma's CC (Soldaini et al., 2024) and C4 (Dodge et al., 2021) subsets that belong to the collected domain names.</p>
<p>Based on the above corpus, we further extract the content snippets and fill in our designed prompt template.Then, we utilize Mistral-7B-Instruct-v0.35 to generate relevant QA pairs that align with the targeted scientific discipline.These synthetic data are crafted to precisely mimic the structure and complexity of real-world scientific problems, which can enhance the model's capability for scientific problem understanding and reasoning.</p>
<p>Synthesizing the Code QA Data</p>
<p>During the preliminary experiments, we find that the coding capacities of Llama-3 are severely affected in the CPT process: sharp performance degradation is observed on the code evaluation benchmarks (i.e., HumanEval and MBPP).</p>
<p>To retain the coding capacities of Llama-3, we adopt a similar data synthesis approach for generating high-quality code QA data.Specifically, we expand the LeetCode dataset6 using the in-context learning (ICL) method.We randomly select problems from the LeetCode dataset as demonstrations, synthesize new coding problems, and generate answers for these problems.In implementation, we use Magicoder-S-DS-6.7B(Wei et al., 2023) for both problems and solutions synthesis.</p>
<p>We present the statistical information of all synthetic data for both scientific and code in Table 4.The details of the prompt for data synthesis and synthesis cases are provided in Appendix B and A.</p>
<p>Implementation Details</p>
<p>We utilize the huggingface Transformers (Wolf et al., 2019) to implement our experiments, using Flash Attention and DeepSpeed ZeRO Stage 2 to optimize the training efficiency.We employ AdamW optimizer (Loshchilov and Hutter, 2019) with β 1 = 0.9 and β 2 = 0.95, and use the Warmup-Stable-Decay (WSD) learning rate scheduler (Hu et al., 2024) in the CPT process of Llama-3.For model warmup, we linearly increase the learning rate from 1.0×10 −7 to 1.0×10 −5 with 10B tokens.In the remaining training procedure, the learning rate remains constant at 1.0 × 10 −5 .</p>
<p>We conduct the CPT process using BFloat16 mixed precision, with a gradient clipping of 1.0 to ensure training stability.To enhance computational efficiency, we apply gradient checkpointing strategy (Chen et al., 2016).During training, the maximum context length is 8, 192 tokens for Llama-3.</p>
<p>Experiment</p>
<p>In this section, we introduce the details of experiments for evaluating our approach.</p>
<p>Evaluation Benchmark</p>
<p>To ensure a comprehensive capacity assessment, we evaluate the performance of LLMs from the following aspects.</p>
<p>• Language Understanding: We evaluate the English language understanding capability using the MMLU (Hendrycks et al., 2021a), and select CMMLU (Li et al., 2023) and C-Eval (Huang et al., 2023) for evaluating Chinese language understanding capability.</p>
<p>• Coding Proficiency: We evaluate the coding proficiency using the HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) benchmarks, which measure the ability to generate correct code snippets based on given problems.</p>
<p>• Scientific Reasoning: We evaluate it using several English and Chinese datasets from science and math domains, where SciQ (Welbl et al., 2017), SciEval (Sun et al., 2024a), ARC (Clark et al., 2018) are English science reasoning datasets; SAT-Math (Zhong et al., 2023), MATH (Hendrycks et al., 2021b), GSM8K (Cobbe et al., 2021), AQUA-RAT (Ling et al., 2017), MAWPS (Koncel-Kedziorski et al., 2016), AS-Div (Miao et al., 2021) are English math reasoning datasets; GaoKao (Zhong et al., 2023) is a Chinese benchmark including physical, chemical and mathematical reasoning subtasks.</p>
<p>In order to better organize the evaluation results, we divide these evaluation benchmarks into two groups.The first group is major benchmarks, containing MMLU, C-Eval, CMMLU, MATH, GSM8K, ASDiv, MAWPS, SAT-Math, Hu-manEval, and MBPP, which aim to evaluate the comprehensive capacities of LLMs.Note that we include commonly used math and code benchmarks in this group because it is standard practice to use these benchmarks for evaluating various general-purpose LLMs.The second group is scientific benchmarks, which contains SciEval, SciQ, GaoKao, ARC, and AQUA-RAT.These benchmarks have a broader coverage of multidisciplinary scientific knowledge, and they are used for evaluating the effectiveness of our data synthesis technique.</p>
<p>For all the above evaluation benchmarks, we evaluate all the models using the few-shot or zero- shot settings.Specifically, we report the eight-shot performance on GSM8K, ASDiv, and MAWPS, five-shot for C-Eval, CMMLU, MMLU, MATH, GaoKao, SciQ, SciEval, SAT-Math, and AQUA-RAT, three-shot for MBPP.For HumanEval and ARC, we report the zero-shot evaluation performance.</p>
<p>Surrogate Experiments with TinyLlama</p>
<p>Due to the significant costs involved in tuning experiments on Llama-3 (8B), we use a relatively small model TinyLlama (Zhang et al., 2024) as a surrogate model for extensive exploratory experiments, and the derived findings can be employed to guide the training of Llama-3 (8B).Specifically, TinyLlama is a language model with 1.1 billion parameters, and it is pre-trained on three trillion tokens using the same architecture and tokenizer as Llama-2 (Touvron et al., 2023b), which is suitable for exploring the CPT strategies in our experiments.The implementation details of TinyLlama are similar to Llama-3, with the differences being TinyLlama's fixed learning rate of 1.0 × 10 −4 and a maximum context length of 2, 048 tokens.In this part, to avoid large performance discrepancies across benchmarks, for major benchmarks, we mainly select C-Eval, CMMLU, and MMLU for computing the average performance; for scientific benchmarks, we select SciEval, SciQ, and ARC for computing the average performance.We also report all benchmark results in Appendix D. Next, we introduce the detailed experiments with TinyLlama.</p>
<p>Effectiveness of Synthetic Data</p>
<p>To analyze the effectiveness of our CPT approach with synthetic data, we consider comparing three variants based on TinyLlama, including TinyLlama (the original model), w/ 5B (Norm.)(CPT with 5B normal tokens), and w/ 5B (1B Syn.) (CPT with 4B normal to- kens and 1B synthetic tokens).In our surrogate experiments, normal training tokens are constructed by using the strategies presented in Section 3.2.</p>
<p>The results are presented in Figure 1.First, by comparing with the base TinyLlama, the two variants achieve much better average performance on both major and scientific benchmarks, indicating the effectiveness our CPT data (both the collected and synthetic data).Furthermore, TinyLlama w/ 5B (1B Syn.) outperforms TinyLlama w/ 5B (Norm), which can demonstrate the effectiveness of our synthetic data.Since the synthetic data is derived based on the original content of web pages, it can better extract the key knowledge of text documents and reduce the influence of irrelevant contents.Furthermore, these synthetic data are presented in the form of QA pairs, having a more similar data format with downstream tasks, which is also an important factor for performance improvement.</p>
<p>Impact of Synthetic Data Quality Intuitively, the quality (or accuracy) of synthetic data would influence the learning of domain knowledge for LLMs.However, it is difficult to guarantee the accuracy of the automatically generated synthetic data.To examine the impact of the synthetic data quality, we consider simulating multiple synthetic datasets with varied data quality.Concretely, we corrupt the original synthetic data by applying three types of transformation, including randomly replacing a number, substituting frequently occurring nouns with random hyponyms, and replacing frequently occurring adjectives with their antonyms (see Appendix C).Based on the above transformation method, we sample one billion tokens from the synthetic data and vary the level of corruption ratios at the range of {0.0, 0.3, 0.4, 0.5, 0.6, 0.7}.Then, we integrate 4B normal tokens with these six synthetic datasets as the CPT dataset, and train TinyLlama  for performance comparison.Figure 2 presents the average performance of TinyLlama after training with varying corruption levels.As can be seen from this figure, a low corruption level (i.e., 0.3) has very little impact on the model performance, suggesting that LLMs can tolerate a certain degree of inaccuracy in synthetic data.However, it would still lead to large performance degradation with a high corruption level (i.e., &gt; 0.5).</p>
<p>Impact of Synthetic Data Ratio</p>
<p>For constructing the CPT dataset, we need to determine the proportion of synthetic data in the overall data distribution.To investigate the effect of the mixture ratio, we vary the proportion of synthetic data in the training corpus, considering four choices in {0.1, 0.2, 0.3, 0.4}, and construct a 5B-token dataset to train TinyLlama.The relative ratios of the rest data sources are kept as that in Section 3.2.Figure 3 presents the average performance of TinyLlama after training with different ratios of synthetic data.We can see that the model's performance initially improves with the increasing of synthetic data proportions, then declines once the proportion reaches a relatively high value (e.g.,40%).Overall, a mixture ratio of 20% is a good choice for integrating synthetic data and normal data.Impact of Synthetic Data Curriculum In addition to the mixture ratio, we can also set different data curriculum methods (i.e., reordering the instances) for synthetic data, since it mixes data from multiple disciplines.To explore the impact of data curriculum, we consider two data instance reordering methods, either by discipline or difficulty, and compare these strategies with the random mixing strategy, denoted as RM.For discipline, we design three kinds of curriculum methods by considering two disciplines, including PB (physics → biochemistry), BP (biochemistry → physics) and PBP (physics → biochemistry → physics).For difficulty, we utilize the PPL score to assess the difficulty level (ten groups in total) and consider the reordering schedules of LH (low → high) and HL (high → low).Each data curriculum is with the same training instances but a different instance organization order.The results of the data curriculum are presented in Figure 4. Overall, we can have two major observations.Firstly, the deliberate separation of data by discipline can not bring performance improvement, even hurting the model performance.Secondly, the easy-to-difficult curriculum can lead to more performance improvement than the contrary difficult-to-easy one and random sampling, since it can help models gradually acquire more complex knowledge information.This demonstrates the effectiveness of the proposed data curriculum strategy based on PPL.</p>
<p>Comparison with Open-source Datasets To further examine the effectiveness of our synthetic data, we select WebInstruct (instruction data mined from the web in the math and science domains) (Yue et al., 2024) and Cosmopedia (synthetic data from the scientific subset automathtext) (Ben Allal et al., 2024), two large-scale open-source datasets that have been widely used for improving LLMs.For the fair comparison, we consider comparing four The results show that our synthetic data leads to more improvements in both major and scientific benchmarks, which demonstrates the effectiveness of our data synthesis method.</p>
<p>Main Experiments with Llama-3</p>
<p>Based on the above findings from TinyLlama, we adopt the best-performing strategies or configurations for continual pre-training Llama-3.</p>
<p>Baselines To conduct the comprehensive evaluation, we adopt both general LLMs and scientific LLMs as baselines in our experiment.We consider three kinds of LLMs as baselines, including general-purpose LLM, scientific LLM (enhanced by the science-related corpus or instructions), and continual pre-training LLM.For general-purpose LLMs, we adopt DCLM-7B (Li et al., 2024) and Mistral-7B-v0.3 (Jiang et al., 2023) as the baseline in the evaluation.For scientific LLMs, we adopt MAmmoTH2-8B (Yue et al., 2024) and Galactica-6.7B(Taylor et al., 2022) as the baseline LLMs.In addition, we also report the evaluation results of Llama-3-Chinese-8B7 , which has also been continually pre-trained based on Llama-3.</p>
<p>Results on Major Benchmarks</p>
<p>As presented in Table 5, we can observe that Llama-3-SynE outperforms its backbone model Llama-3 (8B) by a large margin on Chinese evaluation benchmarks (e.g., C-Eval and CMMLU).It shows that our approach is very effective for enhancing the Chinese language capacity of Llama-3.We carefully collect and clean the Chinese text data, and also design suitable data mixture and curriculum to adaptively retrain these models, which is the key to performance improvement on Chinese benchmarks.Second, for English evaluation benchmarks, our approach slightly underperforms Llama-3 (8B) on MMLU, while achieving improved or comparable performance on the rest math and code benchmarks.</p>
<p>It demonstrates that our approach can well address the catastrophic forgetting issue of the original capabilities of LLMs.Actually, based on our preliminary experiments (also evidenced by baseline models), Chinese-adaptive CPT models are diffi-cult to retain the original performance on Englishoriented benchmarks (e.g., MMLU) due to the data distribution discrepancy between pre-training and CPT.These results indicate that our approach can effectively balance the original and new capacities.</p>
<p>Results on Scientific Benchmarks As shown in Table 6, Llama-3-SynE performs very well on the scientific benchmarks, which is consistently better than the backbone model Llama-3.It indicates that our synthetic data is very effective in improving the scientific reasoning capability of LLMs.In particular, compared to the English datasets, Llama-3-SynE achieves a significantly larger improvement on the Chinese datasets, i.e., GaoKao BIO benchmark (25.71 points improvement over Llama-3), since our CPT model can effectively balance the English and Chinese reasoning abilities on scientific tasks.Among all the baselines, MAmmoTH2-8B achieves very good performance on English scientific benchmarks, while it suffers from performance degradation on general Chinese benchmarks, e.g., C-Eval and CMMLU.</p>
<p>By combining the results on major and scientific benchmarks, we can see that Llama-3-SynE achieves very competitive performance in various abilities, and it can effectively alleviate the catastrophic forgetting issue in the CPT process.Our CPT approach only consumes about 100B tokens, which is relatively efficient in training compute.</p>
<p>Conclusion</p>
<p>In this work, we studied how to perform effective continual pre-training (CPT) for LLMs under a limited training budget.Our focus is to develop new capabilities and meanwhile avoid catastrophic forgetting of original capabilities.Specifically, we extensively explored the data synthesis technique, and generated high-quality scientific and code data, which can largely improve the corresponding abilities of LLMs.In order to reduce the tuning cost, we conducted extensive experiments on TinyLlama by examining various data curation strategies, including data selection, mixture, and curriculum.The derived findings were further employed to guide the training of Llama-3 (8B).Experimental results have shown that our CPT approach can largely boost the Chinese and scientific reasoning abilities of the backbone model, and meanwhile effectively retain its original abilities.</p>
<p>A Case Study for Synthesis QA Good Case 1 Scientific Seed Snippet Texts: Given a system of oppositely charged layers, such as a double layer of ions in an electrolyte solution, explain why these layers do not combine (neutralize) with each other.Synthesis QA:</p>
<p>[Problem] Given a system of oppositely charged layers, such as a double layer of ions in an electrolyte solution, explain why these layers do not combine (neutralize) with each other.</p>
<p>[Solution] In a system of oppositely charged layers, such as a double layer of ions in an electrolyte solution, the layers do not combine (neutralize) due to the electrostatic repulsion between the ions.This repulsion arises from the Coulombic force, which is a fundamental force in nature that acts between charged particles.</p>
<p>To understand this, let's consider a simple example of a positively charged cation (e.g., Na+) and a negatively charged anion (e.g., Cl-).When these ions are brought close together, they experience an electrostatic force that pushes them apart.This force can be calculated using Coulomb's law, which states that the electrostatic force (F) between two point charges (q1 and q2) separated by a distance (r) is given by:F = k * (q1 * q2)/r 2 , where k is the Coulomb constant (approximately 8.99 × 10 9 N m 2 C −2 ).</p>
<p>In the case of an electrolyte solution, the ions are surrounded by a cloud of counter-ions (ions of opposite charge) that neutralize their charge locally.This cloud of counter-ions creates an electric double layer around each ion, which prevents the oppositely charged ions from coming too close to each other and neutralizing.The repulsion between these double layers is known as the electrostatic double layer repulsion.</p>
<p>The thickness of the double layer is typically on the order of a few angstroms (Å), and the strength of the repulsion decreases rapidly as the distance between the layers increases.This repulsion is responsible for the stability of colloidal suspensions, the behavior of charged surfaces in contact with electrolyte solutions, and many other phenomena in surface chemistry and electrochemistry.</p>
<p>In summary, the electrostatic repulsion between oppositely charged layers in a system, such as the double layer of ions in an electrolyte solution, prevents the layers from combining (neutralizing) with each other due to the Coulombic force.This repulsion arises from the electric double layer around each ion, which is created by the counterions that neutralize the charge locally.The thickness of the double layer and the strength of the repulsion depend on factors such as the ionic strength of the solution, the surface charge density, and the dielectric constant of the medium.</p>
<p>B Prompt</p>
<p>Prompt for Scientific QA Synthesis [Solution]: Present a comprehensive, step-by-step solution that solves the problem <strong>correctly</strong> and educates the student, around 250-350 words long.Clearly articulate the reasoning and methods used at each step, providing insight into the problem-solving process.Take care to format any equations properly using LaTeX or appropriate notation.</p>
<p>Prompt for Topic Labeling</p>
<p>I am categorizing a series of articles according to the following 11 topics.Next, I will give you an article, please select only one topic that the article is the most related to:</p>
<p>C Example for Accuracy Degradation Transformations</p>
<h2>Before Transformations: In the given chemical reaction, we have sodium (Na) reacting with chlorine (Cl2) to form sodium chloride (NaCl).To determine the number of atoms of chlorine before and after the reaction, we will first count the number of chlorine atoms. . .adjust the coefficients of the reactants to make the number of chlorine atoms equal before and after the reaction:2Na + Cl2 == 2NaCl.## After Transformations: In the given chemical reaction, we have sodium (Na) reacting with oxygen (Cl2) to form sodium chloride (NaCl).To determine the number of atoms of oxygen before and after the reaction, we will first count the number of oxygen atoms. . .adjust the coefficients of the reactants to make the number of oxygen atoms unequal before and after the reaction:6Na + Cl3 == 8NaCl</h2>
<p>In this example, "chlorine" is replaced with a random hyponym (oxygen, hydrogen, neon, etc.) of its hypernym (chemical element), the numbers in the chemical formulas are randomly replaced, and the adjective "equal" is replaced with "unequal."</p>
<p>D Detailed Surrogate Experiment Results</p>
<p>When introducing surrogate experiments with TinyLlama in Section 4.2, we select several representative benchmarks for computing the average performance to avoid large performance discrepancies across benchmarks.Here we report all benchmark results."PHY", "CHE", and "BIO" denote the physics, chemistry, and biology sub-tasks of the corresponding benchmarks.The best and second best are in bold and underlined, respectively.</p>
<p>Figure 1 :
1
Figure 1: Performance of TinyLlama continually pretrained on different corpora.</p>
<p>Figure 2 :
2
Figure 2: Performance of TinyLlama continually pretrained on varying corruption levels of synthetic data.</p>
<p>Figure 3: Performance of TinyLlama after training with different ratios of synthetic data.</p>
<p>Figure 4 :
4
Figure 4: Performance of TinyLlama with different data curriculum methods.</p>
<p>Figure 5 :
5
Figure 5: Performance of TinyLlama continually pretrained on different open-source datasets.</p>
<p>Instruction</p>
<p>Please gain inspiration from the following {Discipline Placeholder} content to create a high-quality {Discipline Placeholder} problem and solution.Present your output in two distinct sections: [Problem] and [Solution].{Discipline Placeholder} Content {Seed Snippet Placeholder} Guidelines [Problem]: This should be <strong>completely self-contained</strong>, providing all the contextual information one needs to understand and solve the problem.</p>
<p>[</p>
<p>Topics]: {Topic List Placeholder} [Article]: {Web Page Content Placeholder} Please only return the most related topic:</p>
<p>to enhance the domain-specific abilities of a pre-trained model with new domain data.It has been a long-standing research challenge to adapt models to new domains and meanwhile prevent catastrophic forgetting
. In this work,we extensively explore data synthesis techniquein continual pre-training, and generate multidisci-plinary scientific QA data. Our synthetic data isfurther utilized via specially designed data mixtureand curriculum strategies, which can effectivelybalance the new and original abilities of LLMs.
Continual Pre-training Continual pre-training, also called domain adaptive pre-training (Ke et al., 2023; Jang et al., 2022; Lesort et al., 2021), has been widely used</p>
<p>Table 1 :
1
Statistical information of the training corpus for training Llama-3-SynE.
DatasetEnglish Chinese VolumeWeb Pages✓✓45.18BEncyclopedia✓✓4.92BBooks✓✓15.74BQA Forums✓✓4.92BAcademic Papers✓×7.93BMathematical Corpora✓×7.93BCode✓×11.88BSynthetic Data✓×1.50BTotal--100.00B</p>
<p>Table 2 :
2
Training data volume and strategies for the two CPT stages.
StrategyBilingual Adpatation Enhancement SyntheticTopic-based Data Mixture✓×PPL-based Data Curriculum✓×Scientific Data Synthesizing×✓Training Data Volume92.5B7.5B</p>
<p>Table 3 :
3
The pre-defined topics (category labels) for English and Chinese web pages, based on MMLU and CMMLU respectively.</p>
<p>Table 4 :
4
The statistical information of the synthetic data of each discipline (in the form of QA pairs).</p>
<p>Table 5 :
5
Few-shot performance comparison on major benchmarks (i.e., bilingual tasks, code synthesis tasks and mathematical reasoning tasks).The best and second best are in bold and underlined, respectively.
ModelsBilingual MMLU C-Eval CMMLU MATH GSM8K ASDiv MAWPS SAT-Math HumanEval MBPP Math CodeLlama-3-8B66.60 49.4351.0316.2054.40 72.10 89.3038.6436.5947.00DCLM-7B64.01 41.2440.8914.1039.20 67.10 83.4041.3621.9532.60Mistral-7B-v0.363.54 42.7443.7212.3040.50 67.50 87.5040.4525.6136.00Llama-3-Chinese-8B64.10 50.1451.203.600.801.900.6036.829.7614.80MAmmoTH2-8B64.89 46.5645.9034.1061.70 82.80 91.5041.3617.6838.80Galactica-6.7B37.13 26.7225.535.309.6040.90 51.7023.187.312.00Llama-3-SynE (ours) 65.19 58.2457.3428.2060.80 81.00 94.1043.6442.0745.60</p>
<p>Table 6 :
6
Few-shot performance comparison on scientific benchmarks."PHY", "CHE", and "BIO" denote the physics, chemistry, and biology sub-tasks of the corresponding benchmarks.The best and second best are in bold and underlined, respectively.
ModelsSciEvalSciQGaoKaoARCAQUA-RATPHY CHE BIO Avg. Avg. MathQA CHE BIO Easy Challenge Avg.Avg.Llama-3-8B46.95 63.45 74.53 65.47 90.90 27.92 32.85 43.81 91.37 77.73 84.5127.95DCLM-7B56.71 64.39 72.03 66.25 92.50 29.06 31.40 37.14 89.52 76.37 82.9420.08Mistral-7B-v0.348.17 59.41 68.89 61.51 89.40 30.48 30.92 41.43 87.33 74.74 81.0423.23Llama-3-Chinese-8B 48.17 67.34 73.90 67.34 89.20 27.64 30.43 38.57 88.22 70.48 79.3527.56MAmmoTH2-8B49.39 69.36 76.83 69.60 90.20 32.19 36.23 49.05 92.85 84.30 88.5727.17Galactica-6.7B34.76 43.39 54.07 46.27 71.50 23.65 27.05 24.76 65.91 46.76 56.3320.87Llama-3-SynE (ours) 53.66 67.81 77.45 69.60 91.20 31.05 51.21 69.52 91.58 80.97 86.2828.74variants based on TinyLlama, including TinyLlama(the original model), w/ 5B (1B Webins.) (CPT with4B normal tokens and 1B WebInstruct tokens), w/5B (1B Cosm.) (CPT with 4B normal tokens and1B Cosmopedia tokens), and w/ 5B (1B Syn.) (CPTwith 4B normal tokens and 1B tokens from oursynthetic data). Figure 5 presents the performanceof TinyLlama after training with different open-source datasets.
In this work, "topic" has the similar meaning as "category".
2 https://huggingface.co/huawei-noah/TinyBERT_ General_4L_312D
https://huggingface.co/ckiplab/ bert-tiny-chinese
A round consists of several training steps, corresponding to the training of about 40B tokens.
https://huggingface.co/mistralai/ Mistral-7B-Instruct-v0.3
https://huggingface.co/datasets/greengerong/ leetcode
https://huggingface.co/hfl/ llama-3-chinese-8b
AcknowledgmentThe computing resources are supported by Public Computing Cloud, Renmin University of China.
A I , Meta , Llama 3 model card. 2024</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell I Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J Cai, Michael Terry, Quoc V Le, Charles Sutton, CoRR, abs/2108.077322021</p>
<p>Molgpt: Molecular generation using a transformer-decoder model. Viraj Bagal, P K Aggarwal, U Deva Vinod, Priyakumar, 10.1021/ACS.JCIM.1C00600J. Chem. Inf. Model. 6292022</p>
<p>Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, Leandro Von Werra, Cosmopedia. 2024</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, 10.1145/1553374.1553380Proceedings of the 26th Annual International Conference on Machine Learning. ACM International Conference Proceeding Series. the 26th Annual International Conference on Machine LearningMontreal, Quebec, CanadaACM2009. 2009. June 14-18, 2009382</p>
<p>Augmenting large language models with chemistry tools. Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, Philippe Schwaller, 10.1038/S42256-024-00832-8Nat. Mac. Intell. 652024</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé De Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Felipe Petroski Such. Joshua Achiam, Vedant Misra, Evan Morikawa, Alec RadfordJan LeikeIlya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. CoRR, abs/2107.03374</p>
<p>Training deep nets with sublinear memory cost. Tianqi Chen, Bing Xu, Chiyuan Zhang, Carlos Guestrin, CoRR, abs/1604.061742016</p>
<p>Think you have solved question answering? try arc, the AI2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, CoRR, abs/1803.054572018</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, CoRR, abs/2110.141682021</p>
<p>Enhancing chat language models by scaling high-quality instructional conversations. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, Bowen Zhou, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Documenting large webtext corpora: A case study on the colossal clean crawled corpus. Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner, 10.18653/v1/2021.emnlp-main.98Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana. the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta CanaDominican RepublicAssociation for Computational Linguistics2021. 7-11 November, 2021</p>
<p>Catastrophic forgetting in connectionist networks. French Robert, Trends in cognitive sciences. 341999</p>
<p>Continual pre-training of large language models: How to (re)warm your model?. Kshitij Gupta, Benjamin Thérien, Adam Ibrahim, L Mats, Quentin Richter, Eugene Anthony, Irina Belilovsky, Timothée Rish, Lesort, 10.48550/ARXIV.2308.04014CoRR, abs/2308.040142023</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. Austria2021a. May 3-7, 2021OpenReview.net</p>
<p>Measuring mathematical problem solving with the MATH dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021. the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 20212021b. December 2021virtual</p>
<p>Minicpm: Unveiling the potential of small language models with scalable training strategies. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zhen Leng Thai, Kai Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, Maosong Sun, CoRR, abs/2404.063952024</p>
<p>Maosong Sun, and Junxian He. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2023. 2023. December 10 -16, 2023</p>
<p>Simple and scalable strategies to continually pre-train large language models. Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, L Mats, Quentin Richter, Timothée Anthony, Eugene Lesort, Irina Belilovsky, Rish, 10.48550/ARXIV.2403.08763CoRR, abs/2403.087632024</p>
<p>Towards continual knowledge learning of language models. Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, Minjoon Seo, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. 2022. April 25-29, 2022OpenReview.net</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>Mix-cpt: A domain adaptation framework via decoupling knowledge learning and format alignment. Jinhao Jiang, Junyi Li, Wayne Xin Zhao, Yang Song, Tao Zhang, Ji-Rong Wen, arXiv:2407.108042024arXiv preprint</p>
<p>Poet: A generative model of protein families as sequencesof-sequences. Timothy F TruongJr, Tristan Bepler, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023. NeurIPS; New Orleans, LA, USA2023. 2023. December 10 -16, 2023</p>
<p>Continual pretraining of language models. Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, Bing Liu, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023</p>
<p>MAWPS: A math word problem repository. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, 10.18653/V1/N16-1136The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. San Diego California, USAThe Association for Computational Linguistics2016. June 12-17, 2016NAACL HLT 2016</p>
<p>Understanding continual learning settings with data distribution drift analysis. Timothée Lesort, Massimo Caccia, Irina Rish, CoRR, abs/2104.016782021</p>
<p>CMMLU: measuring massive multitask language understanding in chinese. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, Timothy Baldwin, CoRR, abs/2306.092122023</p>
<p>. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah M Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Raghavi Chandu, Thao Nguyen, Igor Vasiljevic, M Sham, Shuran Kakade, Sujay Song, Fartash Sanghavi, Sewoong Faghri, Luke Oh, Kyle Zettlemoyer, Lo, 10.48550/ARXIV.2406.11794Jenia Jitsev. Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Kohand Vaishaal Shankar. 2024. Datacomp-lm: In search of the next generation of training sets for language models. CoRR, abs/2406.11794</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, 10.18653/V1/P17-10152017</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, ICLR 20197th International Conference on Learning Representations. New Orleans, LA, USA2019. May 6-9, 2019OpenReview.net</p>
<p>An empirical study of catastrophic forgetting in large language models during continual fine-tuning. Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, Yue Zhang, 10.48550/ARXIV.2308.08747CoRR, abs/2308.087472023</p>
<p>Rephrasing the web: A recipe for compute and data-efficient language modeling. Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, Navdeep Jaitly, CoRR, abs/2401.163802024</p>
<p>A diverse corpus for evaluating and developing english math word problem solvers. Shen-Yun, Chao-Chun Miao, Keh-Yih Liang, Su, CoRR, abs/2106.157722021</p>
<p>Toward understanding catastrophic forgetting in continual learning. V Cuong, Alessandro Nguyen, Michael Achille, Tal Lam, Vijay Hassner, Stefano Mahadevan, Soatto, CoRR, abs/1908.010912019</p>
<p>10.48550/ARXIV.2303.08774CoRR, abs/2303.08774GPT-4 technical report. 2023OpenAI</p>
<p>Reuse, don't retrain: A recipe for continued pretraining of language models. Jupinder Parmar, Sanjev Satheesh, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, arXiv:2407.072632024Preprint</p>
<p>Fine-tuned language models are continual learners. Thomas Scialom, Tuhin Chakrabarty, Smaranda Muresan, 10.18653/V1/2022.EMNLP-MAIN.410Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022. December 7-11, 20222022</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, 10.48550/ARXIV.2402.03300CoRR, abs/2402.033002024</p>
<p>Dolma: an open corpus of three trillion tokens for language model pretraining research. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, Kyle Lo, 10.48550/ARXIV.2402.00159CoRR, abs/2402.001592024</p>
<p>Scieval: A multi-level large language model evaluation benchmark for scientific research. Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, Kai Yu, Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence. Vancouver, CanadaAAAI Press2024a. February 20-27, 20242014</p>
<p>An integrated data processing framework for pretraining foundation models. Yiding Sun, Feng Wang, Yutao Zhu, Wayne Xin Zhao, Jiaxin Mao, 10.48550/ARXIV.2402.16358abs/2402.163582024bCoRR</p>
<p>Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, CoRR, abs/2211.09085Galactica: A large language model for science. 2022</p>
<p>Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, arXiv:2403.08295Gemma: Open models based on gemini research and technology. 2024arXiv preprint</p>
<p>Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurélien Azhar, Armand Rodriguez, Joulin, 10.48550/arXiv.2302.13971CoRR, abs/2302.13971</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov ; Zheng Yan, Iliyan Zarov, Yuchen Zhang, 10.48550/ARXIV.2307.09288Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurélien Rodriguezand Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288</p>
<p>Magicoder: Source code is all you need. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang, 10.48550/arXiv.2312.02120CoRR, abs/2312.021202023</p>
<p>Crowdsourcing multiple choice science questions. Johannes Welbl, Nelson F Liu, Matt Gardner, Proceedings of the 3rd Workshop on Noisy Usergenerated Text, NUT@EMNLP 2017. the 3rd Workshop on Noisy Usergenerated Text, NUT@EMNLP 2017Copenhagen, DenmarkAssociation for Computational Linguistics2017. September 7, 2017</p>
<p>Overcoming catastrophic forgetting in massively multilingual continual learning. Genta Indra Winata, Lingjue Xie, Karthik Radhakrishnan, Shijie Wu, Xisen Jin, Pengxiang Cheng, Mayank Kulkarni, Daniel Preotiuc-Pietro, 10.18653/V1/2023.FINDINGS-ACL.48Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 2023</p>
<p>Huggingface's transformers: State-ofthe-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, arXiv:1910.037712019arXiv preprint</p>
<p>Doremi: Optimizing data mixtures speeds up language model pretraining. Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V Le, Tengyu Ma, Adams Wei Yu, 10.48550/ARXIV.2304.12244Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023. New Orleans, LA, USA2023. December 10 -16, 2023. 2023Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. CoRR, abs/2304.12244</p>
<p>Metamath: Bootstrap your own mathematical questions for large language models. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, 10.48550/ARXIV.2309.12284CoRR, abs/2309.122842023</p>
<p>Mammoth: Building math generalist models through hybrid instruction tuning. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, 10.48550/ARXIV.2309.05653CoRR, abs/2309.056532023</p>
<p>Mammoth2: Scaling instructions from the web. Xiang Yue, Tuney Zheng, Ge Zhang, Wenhu Chen, 10.48550/ARXIV.2405.03548CoRR, abs/2405.035482024</p>
<p>DNAGPT: A generalized pretrained tool for multiple DNA sequence analysis tasks. Daoan Zhang, Weitong Zhang, Bing He, Jianguo Zhang, Chenchen Qin, Jianhua Yao, 10.48550/ARXIV.2307.05628CoRR, abs/2307.056282023</p>
<p>Tinyllama: An open-source small language model. Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, Wei Lu, 10.48550/ARXIV.2401.02385CoRR, abs/2401.023852024</p>
<p>A survey of large language models. Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Xinyu Li, Zikang Tang, Peiyu Liu, Jian-Yun Liu, Ji-Rong Nie, Wen, 10.48550/arXiv.2303.18223CoRR, abs/2303.182232023</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, 10.48550/ARXIV.2304.06364abs/2304.063642023CoRR</p>
<p>Jiuzhang3.0: Efficiently improving mathematical reasoning by training small data synthesis models. Kun Zhou, Beichen Zhang, Jiapeng Wang, Zhipeng Chen, Wayne Xin Zhao, Jing Sha, Zhichao Sheng, Shijin Wang, Ji-Rong Wen, 10.48550/ARXIV.2405.14365CoRR, abs/2405.143652024</p>
<p>. Yutao Zhu, Kun Zhou, Kelong Mao, Wentong Chen, Yiding Sun, Zhipeng Chen, Qian Cao, Yihan Wu, Yushuo Chen, Feng Wang, Lei Zhang, Junyi Li, Xiaolei Wang, Lei Wang, Beichen Zhang, Zican Dong, Xiaoxue Cheng, Yuhan Chen, Xinyu Tang, Yupeng Hou, Qiangqiang Ren, Xincheng Pang, Shufang Xie, Wayne Xin Zhao, Zhicheng Dou, Jiaxin Mao, Yankai Lin, Ruihua Song, Jun Xu, Xu Chen, Rui Yan, Zhewei Wei, Di Hu, Wenbing Huang, Ze-Feng Gao, Yueguo Chen, Weizheng Lu, Ji-Rong Wen, 10.48550/ARXIV.2406.198532024. 19853Yulan: An open-source large language model. CoRR, abs/2406</p>            </div>
        </div>

    </div>
</body>
</html>
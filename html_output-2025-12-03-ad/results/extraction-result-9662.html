<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9662 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9662</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9662</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-274822175</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.13612v5.pdf" target="_blank">Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have emerged as a potential solution to automate the complex processes involved in writing literature reviews, such as literature collection, organization, and summarization. However, it is yet unclear how good LLMs are at automating comprehensive and reliable literature reviews. This study introduces a framework to automatically evaluate the performance of LLMs in three key tasks of literature writing: reference generation, literature summary, and literature review composition. We introduce multidimensional evaluation metrics that assess the hallucination rates in generated references and measure the semantic coverage and factual consistency of the literature summaries and compositions against human-written counterparts. The experimental results reveal that even the most advanced models still generate hallucinated references, despite recent progress. Moreover, we observe that the performance of different models varies across disciplines when it comes to writing literature reviews. These findings highlight the need for further research and development to improve the reliability of LLMs in automating academic literature reviews.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9662.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9662.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-LR-Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework and empirical study that uses off-the-shelf LLMs to automate literature-review tasks (reference generation, abstract writing, review composition) and evaluates generated outputs against 1,105 human-written Annual Reviews articles using automated and human-centered metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3.5-Sonnet-20240620; GPT-4o-2024-08-16; Qwen-2.5-72B-Instruct; DeepSeek-V3; Llama-3.2-3B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Five commercially/academically available instruction-following LLMs accessed via their official APIs; outputs generated at temperature=0. No additional fine-tuning or integrated retrieval augmentation (RAG) was used for the core experiments in this paper; models were used as black-box text generators following explicit task prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>A curated dataset of 1,105 human-written literature-review articles crawled from Annual Reviews (51 journals) published in 2023; for each article the dataset includes title, keywords, abstract, full context, and reference lists, spanning five disciplines.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>1105</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Each LLM was given the article 'Title' and 'Keywords' to prompt Reference Generation and Abstract Writing; for Review Composition the model received Title, Keywords, and the human-written Abstract (and was asked to produce a ~1000-word review with 10 supporting citations).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-driven synthesis: three defined tasks (Reference Generation: produce N=10 references with seven metadata fields; Abstract Writing: generate an abstract matching original length; Review Composition: produce a ~1000-word literature review and cite 10 studies). Prompts required structured JSON outputs and explicit citation metadata. No retrieval-augmented pipeline was integrated for model input (vanilla prompting). Evaluation used auxiliary tools (Semantic Scholar lookup) and NLI/embedding models for automated scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured reference lists (JSON), generated abstracts, narrative literature reviews (~1000 words) with inline citations and a separate JSON reference list.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>The paper includes a Claude-3.5-Sonnet-generated literature review excerpt on CRISPR-Cas biosensing describing system properties, representative studies (e.g., Gootenberg et al. (2017), Chen et al. (2018)), and a JSON 'References' array listing titles, full author names, journal, year, volume, and pages.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated reference evaluation via Semantic Scholar lookup and a metadata-matching heuristic (precision, recall, F1, title search rate); context evaluation via NLI entailment (TRUE and GPT-4o), ROUGE and embedding cosine similarity (text-embedding-3-large), and Key Point Recall (KPR) where GPT-4 extracts key points from human reviews and GPT-4o is used as the NLI judge. A human annotation sanity check (100 generated references) assessed automatic hallucination detection (majority vote by 3 annotators).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Main findings: hallucinated references remain common across models though rates vary; Claude-3.5-Sonnet achieved the highest overall reference F1 and title-search rate and highest semantic-similarity scores for abstracts (semantic similarity ~81.17%, TRUE factual-consistency ~78.10% reported for abstracts). DeepSeek-V3 showed very high GPT-4o-based factual-consistency on abstracts (reported 96.84% in one metric). In Review Composition, reference precision improved relative to standalone Reference Generation (models generate more accurate citations when citing inline). Human vs automatic hallucination detector agreement kappa=0.71; automatic detection accuracy 86% on the 100-sample test.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Comprehensive, reproducible evaluation framework; uses a sizable, high-quality ground-truth corpus (Annual Reviews); multi-dimensional automated metrics for hallucination and factual consistency; cross-disciplinary analysis; shows that prompting LLMs can produce structured outputs (references + review) and that jointly generating text and citations improves citation accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Experiments used vanilla prompting (no RAG) so findings may not generalize to retrieval-augmented pipelines; potential data contamination (training-set overlap) cannot be fully excluded; reliance on Semantic Scholar for verification may miss some ground-truth items; evaluation metrics emphasize match to human-written reviews and may omit other quality axes (fluency, novelty, comprehensiveness); author/journal abbreviation normalization remains imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Frequent hallucinated references (invented titles or incorrect metadata); incomplete/abbreviated author lists and journal titles causing mismatch; lower model performance in certain disciplines (Chemistry and Social Science worst in some metrics); small models (e.g., Llama-3.2-3B) produced particularly poor reference accuracy; cases where title string mismatches prevented verification even though a real paper existed under a variant title.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9662.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9662.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.5-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3.5-Sonnet-20240620</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-performing instruction-following LLM evaluated in the paper; achieved the strongest results across reference generation, abstract similarity, and review composition metrics in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3.5-Sonnet-20240620</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary instruction-following large language model accessed via API; treated as a black-box generator in experiments (temperature set to 0). The paper does not disclose architecture or training corpus details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Prompted on Title and Keywords (and Abstract for Review Composition) drawn from the Annual Reviews 2023 dataset (1,105 reviews across 51 journals). Models had no external retrieval augmentation in the experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>1105</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Per-article tasks using the article's Title and Keywords (and Abstract for the composition task).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Vanilla prompt-based synthesis: generate reference lists, abstracts, and narrative reviews in JSON format per the three task prompts; instruct models to produce 10 citations with full metadata and to back claims with citations inside the review body.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Reference lists (JSON), abstracts, ~1000-word narrative literature reviews with inline citations.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Claude-3.5-Sonnet produced a CRISPR-Cas biosensing review excerpt with inline citations (e.g., Chen et al. (2018), Gootenberg et al. (2017)) and a JSON 'References' array listing title, authors, journal, year, volume, and pages.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Same framework: Semantic Scholar-based precision/recall/F1 and title search rate for references; TRUE and GPT-4o NLI, embedding cosine similarity, ROUGE, and KPR for context.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Highest precision/recall/F1 on reference generation among evaluated models; highest title-search rate; top semantic-similarity for abstracts (~81.17%) and top KPR for review composition (e.g., KPR reported ~63.06 in the summary table).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Best overall accuracy across tasks in this study; strong semantic coverage and factual consistency relative to other tested models; produces more accurate references when composing reviews (mutual constraint between text and citations).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Still generates hallucinated references in non-negligible numbers; metadata (e.g., full author lists) can be incomplete or abbreviated, requiring normalization; potential dataset overlap with model training data is a confounder noted by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Hallucinated or partially incorrect references (title variants, wrong pages/volumes); lower performance in certain disciplines (notably Chemistry in reference precision analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9662.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9662.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o-2024-08-16</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI family model variant used as both a generation model and an automated judge (NLI) in the paper's evaluation pipeline; one of the five LLMs evaluated for literature-review tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-2024-08-16</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following LLM used as a generation model in experiments; also employed as an NLI-based judge in Key Point Recall (KPR) evaluation. Specific architectural/training details are not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Same Annual Reviews 2023 dataset prompts (Title, Keywords, Abstract as applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>1105</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Per-article Title and Keywords and, for composition, the human abstract.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-based synthesis per the study's three tasks; additionally used as an NLI model to compute factual-consistency/KPR scores.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>References, abstracts, narrative reviews; NLI judgments for factual consistency and KPR.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Generated abstracts and reviews comparable to those from other evaluated models; used to mark entailment between generated content and human key points.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>As a generator evaluated with Semantic Scholar matching for references and with TRUE (and GPT-4o itself) for NLI/entailment; used in KPR pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Competitive on review-composition reference precision in some disciplines; performed best in Technology on TRUE NLI scores for abstract writing per cross-disciplinary analysis; overall not as strong as Claude-3.5-Sonnet in aggregate reference metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Used both as a generation model and as an automated NLI judge within the same evaluation framework; showed discipline-specific strengths (e.g., Technology).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Still prone to hallucinated citations; exact training/data provenance not provided; performance variable across disciplines.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Hallucinated references and some metadata mismatches; variable performance across disciplines (weaker outside Technology in some metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9662.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9662.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2.5-72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-2.5-72B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction-tuned model (explicitly listed with size 72B in the paper) included among the five evaluated LLMs for literature-review tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-72B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLM of reported size 72B (as stated in the paper); used as a black-box API generator with temperature set to 0. No RAG integration in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Prompts drawn from the Annual Reviews 2023 dataset (1,105 articles).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>1105</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Title and Keywords (plus Abstract for Review Composition) per article.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-driven generation of references/abstracts/reviews per the study's three tasks; required JSON output format and 10 citations for reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Reference JSON lists, abstracts, ~1000-word literature reviews with 10 citations.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Produced reference lists and reviews per the prompts; aggregated scores show lower precision than top models in many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Same automated pipeline: Semantic Scholar matching for citations; TRUE/GPT-4o NLI, ROUGE, embedding similarity, and KPR for text evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Lower overall reference precision/F1 relative to Claude-3.5-Sonnet and GPT-4o; cross-disciplinary variations observed (mathematics often higher precision than chemistry).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Large parameter count and instruction-tuning give reasonable generation quality; performs variably by discipline mirroring other models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Still generates hallucinated references; weaker precision and recall relative to top-performing models in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Hallucinated or misformatted references; lower performance in Chemistry and some other domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9662.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9662.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model included among the five LLMs evaluated; notable for strong reported factual-consistency scores on abstracts in some automated assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary or third-party LLM used via API as a black-box instruction-following model (temperature=0). The paper does not disclose architecture or parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Annual Reviews 2023 dataset (1,105 articles) used for prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>1105</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Title and Keywords (and Abstract for composition task).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-based generation identical to other evaluated models: Reference Generation, Abstract Writing, and Review Composition tasks with structured JSON output and 10 requested citations.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>References, abstracts, literature reviews with citations.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Generated abstracts that, per the paper, scored highly on certain factual-consistency measures (GPT-4o-based assessment approaching 96.84% in one reported metric).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Semantic Scholar metadata matching; TRUE and GPT-4o NLI; ROUGE and KPR; embeddings for semantic similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Strong factual-consistency on abstract NLI by GPT-4o; competitive ROUGE results for review composition in some cases; generally outperformed some models but underperformed Claude-3.5-Sonnet on aggregate reference precision/F1.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>High factual-consistency on abstracts in automated NLI scoring; better ROUGE lexical overlap in review composition in some measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Author metadata often incorrect/incomplete; still produces hallucinated references; variable performance by discipline.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Hallucinated references and metadata errors, especially in the Reference Generation standalone task; author-dimension weaknesses relative to other models in some analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9662.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9662.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.2-3B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.2-3B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small (3B parameter) instruction-tuned LLM included in the evaluation; performs worst among tested models on many reference-accuracy metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.2-3B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight instruction-tuned model with ~3B parameters used as a baseline small model; accessed via API with temperature=0 in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Annual Reviews 2023 dataset prompts (Title, Keywords, Abstract as applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>1105</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Title and Keywords (and Abstract for review composition).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-based generation for the three tasks; asked to output JSON references and reviews with 10 citations.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Reference lists, abstracts, ~1000-word reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Produced references and reviews with notably lower precision and recall on reference matching metrics; higher ROUGE-L in abstract writing but not consistently strong on other metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Same automated pipeline with Semantic Scholar, NLI, ROUGE, embeddings, and KPR.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Lowest precision/F1 among evaluated models for reference generation; occasional better performance on page/author dimensions but overall underperforms larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Representative of smaller, open models; useful for comparative baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Poor reference accuracy and recall relative to larger models; greater hallucination rates in citations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Substantial hallucinated references and incorrect metadata; lower cross-disciplinary robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9662.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9662.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoSurvey (Wang et al., 2024b)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system mentioned in related work that uses retrieval-augmented generation to incorporate up-to-date papers and automatically write surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autosurvey: Large language models can automatically write surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A retrieval-augmented pipeline that augments LLMs with retrieved up-to-date papers to produce surveys; cited as related work but not used in experiments of this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this paper (described in the cited AutoSurvey work); characterized as using up-to-date retrieved papers rather than vanilla prompting alone.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Systems aimed at producing surveys for research topics by combining retrieval with generation.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) to pull in current papers prior to synthesis; pipeline-level optimization for generating surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Automated academic surveys (narrative syntheses that integrate retrieved papers).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not detailed in this paper (referenced only); AutoSurvey is cited as using retrieval to improve outputs in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Mentioned as an approach that 'incorporates up-to-date papers via retrieval-augmented generation'â€”no quantitative results in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Addresses data-staleness by retrieving current literature; likely reduces hallucination by grounding generation in retrieved documents (as argued in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not evaluated within this paper; retrieval quality and grounding remain practical challenges per general literature.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9662.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9662.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agarwal_2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llms for literature review: Are we there yet? (Agarwal et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related work referenced that examines zero-shot LLM review generation via a two-step retrieval and outlining process; cited as an example of prior efforts to automate literature reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llms for literature review: Are we there yet?.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Agarwal et al. two-step pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced approach: zero-shot LLM review generation with an initial retrieval stage followed by outlining; details are taken from cited work and not implemented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Automated review generation for research topics using retrieval then outline-then-write approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Two-step retrieval + outlining + generation (zero-shot); described in related work only.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated literature reviews via a pipeline that first retrieves candidate documents and then outlines/synthesizes.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified here (only cited).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Mentioned as prior work exploring zero-shot generation with retrieval/outlining.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Combines retrieval with pipeline structuring to potentially improve grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not evaluated in the current paper; inherent challenges include retrieval quality and hallucination risk.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9662.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9662.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SurveyX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SurveyX: Academic survey automation via large language models (Liang et al., 2025)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent system cited in related work that optimizes retrieval, extraction, and outline generation and supports multimodal outputs (figures and tables) for survey automation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Surveyx: Academic survey automation via large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SurveyX</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pipeline-based system focused on efficient retrieval, extraction, and outline generation, enabling multimodal survey outputs; cited as recent related work, not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Aimed at automating academic surveys with retrieval + extraction + outline stages supporting multimodal outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Optimized retrieval and extraction pipeline combined with LLM generation to produce surveys and structured outputs (figures/tables).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Automated academic surveys including text, figures, and tables.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not detailed in this paper (cited only).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Referenced as a recent efficient system for survey automation; no quantitative detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Pipeline optimization and multimodal support address practical needs for survey production.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not evaluated within this study; implementation details and empirical performance are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9662.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9662.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CHIME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CHIME: LLM-assisted hierarchical organization of scientific studies for literature review support (Hsu et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A related system that uses LLMs to hierarchically organize scientific studies to support literature review tasks; cited as part of the growing literature on LLM tools for review assistance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CHIME: LLM-assisted hierarchical organization of scientific studies for literature review support.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CHIME</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A tool that leverages LLMs to create hierarchical structures organizing scientific studies for review support; referenced in related work but not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Organizing collections of studies into hierarchical representations to assist human reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>LLM-assisted hierarchical organization (pipeline-level assistance rather than pure free-form generation).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Hierarchical organization outputs (structured summaries/organization of studies).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not detailed in this paper (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Mentioned as related work addressing organization aspects of literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Targets the organization and structuring step of literature reviews, complementary to synthesis-focused pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not assessed in the present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autosurvey: Large language models can automatically write surveys. <em>(Rating: 2)</em></li>
                <li>Surveyx: Academic survey automation via large language models. <em>(Rating: 2)</em></li>
                <li>Llms for literature review: Are we there yet?. <em>(Rating: 2)</em></li>
                <li>CHIME: LLM-assisted hierarchical organization of scientific studies for literature review support. <em>(Rating: 2)</em></li>
                <li>Long2rag: Evaluating long-context & long-form retrieval-augmented generation with key point recall. <em>(Rating: 1)</em></li>
                <li>TRUE: Re-evaluating factual consistency evaluation. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9662",
    "paper_id": "paper-274822175",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "LLM-LR-Eval",
            "name_full": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
            "brief_description": "A framework and empirical study that uses off-the-shelf LLMs to automate literature-review tasks (reference generation, abstract writing, review composition) and evaluates generated outputs against 1,105 human-written Annual Reviews articles using automated and human-centered metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude-3.5-Sonnet-20240620; GPT-4o-2024-08-16; Qwen-2.5-72B-Instruct; DeepSeek-V3; Llama-3.2-3B-Instruct",
            "model_description": "Five commercially/academically available instruction-following LLMs accessed via their official APIs; outputs generated at temperature=0. No additional fine-tuning or integrated retrieval augmentation (RAG) was used for the core experiments in this paper; models were used as black-box text generators following explicit task prompts.",
            "model_size": null,
            "input_corpus_description": "A curated dataset of 1,105 human-written literature-review articles crawled from Annual Reviews (51 journals) published in 2023; for each article the dataset includes title, keywords, abstract, full context, and reference lists, spanning five disciplines.",
            "input_corpus_size": 1105,
            "topic_query_description": "Each LLM was given the article 'Title' and 'Keywords' to prompt Reference Generation and Abstract Writing; for Review Composition the model received Title, Keywords, and the human-written Abstract (and was asked to produce a ~1000-word review with 10 supporting citations).",
            "distillation_method": "Prompt-driven synthesis: three defined tasks (Reference Generation: produce N=10 references with seven metadata fields; Abstract Writing: generate an abstract matching original length; Review Composition: produce a ~1000-word literature review and cite 10 studies). Prompts required structured JSON outputs and explicit citation metadata. No retrieval-augmented pipeline was integrated for model input (vanilla prompting). Evaluation used auxiliary tools (Semantic Scholar lookup) and NLI/embedding models for automated scoring.",
            "output_type": "Structured reference lists (JSON), generated abstracts, narrative literature reviews (~1000 words) with inline citations and a separate JSON reference list.",
            "output_example": "The paper includes a Claude-3.5-Sonnet-generated literature review excerpt on CRISPR-Cas biosensing describing system properties, representative studies (e.g., Gootenberg et al. (2017), Chen et al. (2018)), and a JSON 'References' array listing titles, full author names, journal, year, volume, and pages.",
            "evaluation_method": "Automated reference evaluation via Semantic Scholar lookup and a metadata-matching heuristic (precision, recall, F1, title search rate); context evaluation via NLI entailment (TRUE and GPT-4o), ROUGE and embedding cosine similarity (text-embedding-3-large), and Key Point Recall (KPR) where GPT-4 extracts key points from human reviews and GPT-4o is used as the NLI judge. A human annotation sanity check (100 generated references) assessed automatic hallucination detection (majority vote by 3 annotators).",
            "evaluation_results": "Main findings: hallucinated references remain common across models though rates vary; Claude-3.5-Sonnet achieved the highest overall reference F1 and title-search rate and highest semantic-similarity scores for abstracts (semantic similarity ~81.17%, TRUE factual-consistency ~78.10% reported for abstracts). DeepSeek-V3 showed very high GPT-4o-based factual-consistency on abstracts (reported 96.84% in one metric). In Review Composition, reference precision improved relative to standalone Reference Generation (models generate more accurate citations when citing inline). Human vs automatic hallucination detector agreement kappa=0.71; automatic detection accuracy 86% on the 100-sample test.",
            "strengths": "Comprehensive, reproducible evaluation framework; uses a sizable, high-quality ground-truth corpus (Annual Reviews); multi-dimensional automated metrics for hallucination and factual consistency; cross-disciplinary analysis; shows that prompting LLMs can produce structured outputs (references + review) and that jointly generating text and citations improves citation accuracy.",
            "limitations": "Experiments used vanilla prompting (no RAG) so findings may not generalize to retrieval-augmented pipelines; potential data contamination (training-set overlap) cannot be fully excluded; reliance on Semantic Scholar for verification may miss some ground-truth items; evaluation metrics emphasize match to human-written reviews and may omit other quality axes (fluency, novelty, comprehensiveness); author/journal abbreviation normalization remains imperfect.",
            "failure_cases": "Frequent hallucinated references (invented titles or incorrect metadata); incomplete/abbreviated author lists and journal titles causing mismatch; lower model performance in certain disciplines (Chemistry and Social Science worst in some metrics); small models (e.g., Llama-3.2-3B) produced particularly poor reference accuracy; cases where title string mismatches prevented verification even though a real paper existed under a variant title.",
            "uuid": "e9662.0",
            "source_info": {
                "paper_title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Claude-3.5-Sonnet",
            "name_full": "Claude-3.5-Sonnet-20240620",
            "brief_description": "A high-performing instruction-following LLM evaluated in the paper; achieved the strongest results across reference generation, abstract similarity, and review composition metrics in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude-3.5-Sonnet-20240620",
            "model_description": "Proprietary instruction-following large language model accessed via API; treated as a black-box generator in experiments (temperature set to 0). The paper does not disclose architecture or training corpus details.",
            "model_size": null,
            "input_corpus_description": "Prompted on Title and Keywords (and Abstract for Review Composition) drawn from the Annual Reviews 2023 dataset (1,105 reviews across 51 journals). Models had no external retrieval augmentation in the experiment.",
            "input_corpus_size": 1105,
            "topic_query_description": "Per-article tasks using the article's Title and Keywords (and Abstract for the composition task).",
            "distillation_method": "Vanilla prompt-based synthesis: generate reference lists, abstracts, and narrative reviews in JSON format per the three task prompts; instruct models to produce 10 citations with full metadata and to back claims with citations inside the review body.",
            "output_type": "Reference lists (JSON), abstracts, ~1000-word narrative literature reviews with inline citations.",
            "output_example": "Claude-3.5-Sonnet produced a CRISPR-Cas biosensing review excerpt with inline citations (e.g., Chen et al. (2018), Gootenberg et al. (2017)) and a JSON 'References' array listing title, authors, journal, year, volume, and pages.",
            "evaluation_method": "Same framework: Semantic Scholar-based precision/recall/F1 and title search rate for references; TRUE and GPT-4o NLI, embedding cosine similarity, ROUGE, and KPR for context.",
            "evaluation_results": "Highest precision/recall/F1 on reference generation among evaluated models; highest title-search rate; top semantic-similarity for abstracts (~81.17%) and top KPR for review composition (e.g., KPR reported ~63.06 in the summary table).",
            "strengths": "Best overall accuracy across tasks in this study; strong semantic coverage and factual consistency relative to other tested models; produces more accurate references when composing reviews (mutual constraint between text and citations).",
            "limitations": "Still generates hallucinated references in non-negligible numbers; metadata (e.g., full author lists) can be incomplete or abbreviated, requiring normalization; potential dataset overlap with model training data is a confounder noted by the authors.",
            "failure_cases": "Hallucinated or partially incorrect references (title variants, wrong pages/volumes); lower performance in certain disciplines (notably Chemistry in reference precision analyses).",
            "uuid": "e9662.1",
            "source_info": {
                "paper_title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o-2024-08-16",
            "brief_description": "An OpenAI family model variant used as both a generation model and an automated judge (NLI) in the paper's evaluation pipeline; one of the five LLMs evaluated for literature-review tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o-2024-08-16",
            "model_description": "Instruction-following LLM used as a generation model in experiments; also employed as an NLI-based judge in Key Point Recall (KPR) evaluation. Specific architectural/training details are not provided in the paper.",
            "model_size": null,
            "input_corpus_description": "Same Annual Reviews 2023 dataset prompts (Title, Keywords, Abstract as applicable).",
            "input_corpus_size": 1105,
            "topic_query_description": "Per-article Title and Keywords and, for composition, the human abstract.",
            "distillation_method": "Prompt-based synthesis per the study's three tasks; additionally used as an NLI model to compute factual-consistency/KPR scores.",
            "output_type": "References, abstracts, narrative reviews; NLI judgments for factual consistency and KPR.",
            "output_example": "Generated abstracts and reviews comparable to those from other evaluated models; used to mark entailment between generated content and human key points.",
            "evaluation_method": "As a generator evaluated with Semantic Scholar matching for references and with TRUE (and GPT-4o itself) for NLI/entailment; used in KPR pipeline.",
            "evaluation_results": "Competitive on review-composition reference precision in some disciplines; performed best in Technology on TRUE NLI scores for abstract writing per cross-disciplinary analysis; overall not as strong as Claude-3.5-Sonnet in aggregate reference metrics.",
            "strengths": "Used both as a generation model and as an automated NLI judge within the same evaluation framework; showed discipline-specific strengths (e.g., Technology).",
            "limitations": "Still prone to hallucinated citations; exact training/data provenance not provided; performance variable across disciplines.",
            "failure_cases": "Hallucinated references and some metadata mismatches; variable performance across disciplines (weaker outside Technology in some metrics).",
            "uuid": "e9662.2",
            "source_info": {
                "paper_title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Qwen-2.5-72B",
            "name_full": "Qwen-2.5-72B-Instruct",
            "brief_description": "A large instruction-tuned model (explicitly listed with size 72B in the paper) included among the five evaluated LLMs for literature-review tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5-72B-Instruct",
            "model_description": "Instruction-tuned LLM of reported size 72B (as stated in the paper); used as a black-box API generator with temperature set to 0. No RAG integration in the experiments.",
            "model_size": "72B",
            "input_corpus_description": "Prompts drawn from the Annual Reviews 2023 dataset (1,105 articles).",
            "input_corpus_size": 1105,
            "topic_query_description": "Title and Keywords (plus Abstract for Review Composition) per article.",
            "distillation_method": "Prompt-driven generation of references/abstracts/reviews per the study's three tasks; required JSON output format and 10 citations for reviews.",
            "output_type": "Reference JSON lists, abstracts, ~1000-word literature reviews with 10 citations.",
            "output_example": "Produced reference lists and reviews per the prompts; aggregated scores show lower precision than top models in many tasks.",
            "evaluation_method": "Same automated pipeline: Semantic Scholar matching for citations; TRUE/GPT-4o NLI, ROUGE, embedding similarity, and KPR for text evaluation.",
            "evaluation_results": "Lower overall reference precision/F1 relative to Claude-3.5-Sonnet and GPT-4o; cross-disciplinary variations observed (mathematics often higher precision than chemistry).",
            "strengths": "Large parameter count and instruction-tuning give reasonable generation quality; performs variably by discipline mirroring other models.",
            "limitations": "Still generates hallucinated references; weaker precision and recall relative to top-performing models in this study.",
            "failure_cases": "Hallucinated or misformatted references; lower performance in Chemistry and some other domains.",
            "uuid": "e9662.3",
            "source_info": {
                "paper_title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "DeepSeek-V3",
            "name_full": "DeepSeek-V3",
            "brief_description": "A model included among the five LLMs evaluated; notable for strong reported factual-consistency scores on abstracts in some automated assessments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-V3",
            "model_description": "Proprietary or third-party LLM used via API as a black-box instruction-following model (temperature=0). The paper does not disclose architecture or parameter count.",
            "model_size": null,
            "input_corpus_description": "Annual Reviews 2023 dataset (1,105 articles) used for prompting.",
            "input_corpus_size": 1105,
            "topic_query_description": "Title and Keywords (and Abstract for composition task).",
            "distillation_method": "Prompt-based generation identical to other evaluated models: Reference Generation, Abstract Writing, and Review Composition tasks with structured JSON output and 10 requested citations.",
            "output_type": "References, abstracts, literature reviews with citations.",
            "output_example": "Generated abstracts that, per the paper, scored highly on certain factual-consistency measures (GPT-4o-based assessment approaching 96.84% in one reported metric).",
            "evaluation_method": "Semantic Scholar metadata matching; TRUE and GPT-4o NLI; ROUGE and KPR; embeddings for semantic similarity.",
            "evaluation_results": "Strong factual-consistency on abstract NLI by GPT-4o; competitive ROUGE results for review composition in some cases; generally outperformed some models but underperformed Claude-3.5-Sonnet on aggregate reference precision/F1.",
            "strengths": "High factual-consistency on abstracts in automated NLI scoring; better ROUGE lexical overlap in review composition in some measurements.",
            "limitations": "Author metadata often incorrect/incomplete; still produces hallucinated references; variable performance by discipline.",
            "failure_cases": "Hallucinated references and metadata errors, especially in the Reference Generation standalone task; author-dimension weaknesses relative to other models in some analyses.",
            "uuid": "e9662.4",
            "source_info": {
                "paper_title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Llama-3.2-3B",
            "name_full": "Llama-3.2-3B-Instruct",
            "brief_description": "A small (3B parameter) instruction-tuned LLM included in the evaluation; performs worst among tested models on many reference-accuracy metrics.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.2-3B-Instruct",
            "model_description": "Open-weight instruction-tuned model with ~3B parameters used as a baseline small model; accessed via API with temperature=0 in this study.",
            "model_size": "3B",
            "input_corpus_description": "Annual Reviews 2023 dataset prompts (Title, Keywords, Abstract as applicable).",
            "input_corpus_size": 1105,
            "topic_query_description": "Title and Keywords (and Abstract for review composition).",
            "distillation_method": "Prompt-based generation for the three tasks; asked to output JSON references and reviews with 10 citations.",
            "output_type": "Reference lists, abstracts, ~1000-word reviews.",
            "output_example": "Produced references and reviews with notably lower precision and recall on reference matching metrics; higher ROUGE-L in abstract writing but not consistently strong on other metrics.",
            "evaluation_method": "Same automated pipeline with Semantic Scholar, NLI, ROUGE, embeddings, and KPR.",
            "evaluation_results": "Lowest precision/F1 among evaluated models for reference generation; occasional better performance on page/author dimensions but overall underperforms larger models.",
            "strengths": "Representative of smaller, open models; useful for comparative baselines.",
            "limitations": "Poor reference accuracy and recall relative to larger models; greater hallucination rates in citations.",
            "failure_cases": "Substantial hallucinated references and incorrect metadata; lower cross-disciplinary robustness.",
            "uuid": "e9662.5",
            "source_info": {
                "paper_title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "AutoSurvey",
            "name_full": "AutoSurvey (Wang et al., 2024b)",
            "brief_description": "A system mentioned in related work that uses retrieval-augmented generation to incorporate up-to-date papers and automatically write surveys.",
            "citation_title": "Autosurvey: Large language models can automatically write surveys.",
            "mention_or_use": "mention",
            "model_name": "AutoSurvey",
            "model_description": "A retrieval-augmented pipeline that augments LLMs with retrieved up-to-date papers to produce surveys; cited as related work but not used in experiments of this paper.",
            "model_size": null,
            "input_corpus_description": "Not specified in this paper (described in the cited AutoSurvey work); characterized as using up-to-date retrieved papers rather than vanilla prompting alone.",
            "input_corpus_size": null,
            "topic_query_description": "Systems aimed at producing surveys for research topics by combining retrieval with generation.",
            "distillation_method": "Retrieval-Augmented Generation (RAG) to pull in current papers prior to synthesis; pipeline-level optimization for generating surveys.",
            "output_type": "Automated academic surveys (narrative syntheses that integrate retrieved papers).",
            "output_example": null,
            "evaluation_method": "Not detailed in this paper (referenced only); AutoSurvey is cited as using retrieval to improve outputs in prior work.",
            "evaluation_results": "Mentioned as an approach that 'incorporates up-to-date papers via retrieval-augmented generation'â€”no quantitative results in this paper.",
            "strengths": "Addresses data-staleness by retrieving current literature; likely reduces hallucination by grounding generation in retrieved documents (as argued in related work).",
            "limitations": "Not evaluated within this paper; retrieval quality and grounding remain practical challenges per general literature.",
            "failure_cases": "Not specified here.",
            "uuid": "e9662.6",
            "source_info": {
                "paper_title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Agarwal_2024",
            "name_full": "Llms for literature review: Are we there yet? (Agarwal et al., 2024)",
            "brief_description": "Related work referenced that examines zero-shot LLM review generation via a two-step retrieval and outlining process; cited as an example of prior efforts to automate literature reviews.",
            "citation_title": "Llms for literature review: Are we there yet?.",
            "mention_or_use": "mention",
            "model_name": "Agarwal et al. two-step pipeline",
            "model_description": "Referenced approach: zero-shot LLM review generation with an initial retrieval stage followed by outlining; details are taken from cited work and not implemented in this paper.",
            "model_size": null,
            "input_corpus_description": null,
            "input_corpus_size": null,
            "topic_query_description": "Automated review generation for research topics using retrieval then outline-then-write approaches.",
            "distillation_method": "Two-step retrieval + outlining + generation (zero-shot); described in related work only.",
            "output_type": "Generated literature reviews via a pipeline that first retrieves candidate documents and then outlines/synthesizes.",
            "output_example": null,
            "evaluation_method": "Not specified here (only cited).",
            "evaluation_results": "Mentioned as prior work exploring zero-shot generation with retrieval/outlining.",
            "strengths": "Combines retrieval with pipeline structuring to potentially improve grounding.",
            "limitations": "Not evaluated in the current paper; inherent challenges include retrieval quality and hallucination risk.",
            "failure_cases": "Not specified in this paper.",
            "uuid": "e9662.7",
            "source_info": {
                "paper_title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "SurveyX",
            "name_full": "SurveyX: Academic survey automation via large language models (Liang et al., 2025)",
            "brief_description": "A recent system cited in related work that optimizes retrieval, extraction, and outline generation and supports multimodal outputs (figures and tables) for survey automation.",
            "citation_title": "Surveyx: Academic survey automation via large language models.",
            "mention_or_use": "mention",
            "model_name": "SurveyX",
            "model_description": "A pipeline-based system focused on efficient retrieval, extraction, and outline generation, enabling multimodal survey outputs; cited as recent related work, not used in experiments.",
            "model_size": null,
            "input_corpus_description": null,
            "input_corpus_size": null,
            "topic_query_description": "Aimed at automating academic surveys with retrieval + extraction + outline stages supporting multimodal outputs.",
            "distillation_method": "Optimized retrieval and extraction pipeline combined with LLM generation to produce surveys and structured outputs (figures/tables).",
            "output_type": "Automated academic surveys including text, figures, and tables.",
            "output_example": null,
            "evaluation_method": "Not detailed in this paper (cited only).",
            "evaluation_results": "Referenced as a recent efficient system for survey automation; no quantitative detail in this paper.",
            "strengths": "Pipeline optimization and multimodal support address practical needs for survey production.",
            "limitations": "Not evaluated within this study; implementation details and empirical performance are in the cited work.",
            "failure_cases": "Not specified here.",
            "uuid": "e9662.8",
            "source_info": {
                "paper_title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "CHIME",
            "name_full": "CHIME: LLM-assisted hierarchical organization of scientific studies for literature review support (Hsu et al., 2024)",
            "brief_description": "A related system that uses LLMs to hierarchically organize scientific studies to support literature review tasks; cited as part of the growing literature on LLM tools for review assistance.",
            "citation_title": "CHIME: LLM-assisted hierarchical organization of scientific studies for literature review support.",
            "mention_or_use": "mention",
            "model_name": "CHIME",
            "model_description": "A tool that leverages LLMs to create hierarchical structures organizing scientific studies for review support; referenced in related work but not evaluated here.",
            "model_size": null,
            "input_corpus_description": null,
            "input_corpus_size": null,
            "topic_query_description": "Organizing collections of studies into hierarchical representations to assist human reviewers.",
            "distillation_method": "LLM-assisted hierarchical organization (pipeline-level assistance rather than pure free-form generation).",
            "output_type": "Hierarchical organization outputs (structured summaries/organization of studies).",
            "output_example": null,
            "evaluation_method": "Not detailed in this paper (cited).",
            "evaluation_results": "Mentioned as related work addressing organization aspects of literature synthesis.",
            "strengths": "Targets the organization and structuring step of literature reviews, complementary to synthesis-focused pipelines.",
            "limitations": "Not assessed in the present paper.",
            "failure_cases": "Not specified here.",
            "uuid": "e9662.9",
            "source_info": {
                "paper_title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autosurvey: Large language models can automatically write surveys.",
            "rating": 2,
            "sanitized_title": "autosurvey_large_language_models_can_automatically_write_surveys"
        },
        {
            "paper_title": "Surveyx: Academic survey automation via large language models.",
            "rating": 2,
            "sanitized_title": "surveyx_academic_survey_automation_via_large_language_models"
        },
        {
            "paper_title": "Llms for literature review: Are we there yet?.",
            "rating": 2,
            "sanitized_title": "llms_for_literature_review_are_we_there_yet"
        },
        {
            "paper_title": "CHIME: LLM-assisted hierarchical organization of scientific studies for literature review support.",
            "rating": 2,
            "sanitized_title": "chime_llmassisted_hierarchical_organization_of_scientific_studies_for_literature_review_support"
        },
        {
            "paper_title": "Long2rag: Evaluating long-context & long-form retrieval-augmented generation with key point recall.",
            "rating": 1,
            "sanitized_title": "long2rag_evaluating_longcontext_longform_retrievalaugmented_generation_with_key_point_recall"
        },
        {
            "paper_title": "TRUE: Re-evaluating factual consistency evaluation.",
            "rating": 1,
            "sanitized_title": "true_reevaluating_factual_consistency_evaluation"
        }
    ],
    "cost": 0.01814225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition</p>
<p>Xuemei Tang 
Department of Linguistics and Modern Languages
The Chinese University of Hong Kong</p>
<p>Xufeng Duan 
Department of Linguistics and Modern Languages
The Chinese University of Hong Kong</p>
<p>Zhenguang G Cai 
Department of Linguistics and Modern Languages
The Chinese University of Hong Kong</p>
<p>Brain and Mind Institute
The Chinese University of Hong Kong</p>
<p>Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition
75CB31A8CFA0A1E5CF320604082F9DAB
Large language models (LLMs) have emerged as a potential solution to automate the complex processes involved in writing literature reviews, such as literature collection, organization, and summarization.However, it is yet unclear how good LLMs are at automating comprehensive and reliable literature reviews.This study introduces a framework to automatically evaluate the performance of LLMs in three key tasks of literature review writing: reference generation, abstract writing, and literature review composition.We introduce multidimensional evaluation metrics that assess the hallucination rates in generated references and measure the semantic coverage and factual consistency of the literature summaries and compositions against human-written counterparts.The experimental results reveal that even the most advanced models still generate hallucinated references, despite recent progress.Moreover, we observe that the performance of different models varies across disciplines when it comes to writing literature reviews.These findings highlight the need for further research and development to improve the reliability of LLMs in automating academic literature reviews.The dataset and code used in this study are publicly available at an anonymous repository 1 .</p>
<p>Introduction</p>
<p>The literature review is a critical component of academic writing that aims to synthesize, critique, and assess the current state of knowledge in a particular field.It involves a comprehensive examination of published research articles, theoretical frameworks, and research methodologies related to a specific topic.Conducting a thorough literature review often necessitates extensive reading and summarizing of pertinent literature, which can be a complex and time-consuming process, especially in well-established fields where the number of relevant references can range from dozens to hundreds.To alleviate this burden, researchers have recently turned to advanced deep learning models as a potential tool to aid in the automated generation of literature reviews (Aliyu et al., 2018;Kontonatsios et al., 2020).</p>
<p>The emergence of large language models (LLMs) has introduced a promising avenue for automating key aspects of literature review writing, including identifying relevant sources, summarizing findings, and generating coherent syntheses (Wang et al., 2024b;Agarwal et al., 2024;Hsu et al., 2024).While techniques such as Retrieval-Augmented Generation (RAG) can enhance the domain-specific knowledge of LLMs, most researchers rely on vanilla LLMs, such as ChatGPT, for literature review writing without the use of RAG (Wang et al., 2024a).Consequently, it is crucial to evaluate the performance of these naive LLMs in the context of literature review writing to determine their effectiveness and limitations.</p>
<p>Therefore, in this paper, we propose a framework for automatically assessing the literature review writing ability of LLMs, using human-written literature reviews as the gold standard and designing metrics for a comprehensive evaluation.We first collect a dataset of human-written literature reviews to serve as a benchmark for evaluating the performance of LLMs.We then ask LLMs to complete three tasks based on the collected dataset: generating references, writing an abstract, and writing a complete literature review based on a given topic.Finally, we evaluate the generated results from several dimensions, including the presence of hallucinations in the references, as well as the semantic coverage and factual consistency of the generated abstract and literature review compared to the human-written context.By assessing the performance of LLMs across these tasks and evaluating their output using our proposed metrics, we arXiv:2412.13612v5[cs.CL] 21 Aug 2025 aim to provide a comprehensive understanding of their capabilities and limitations in writing literature reviews.</p>
<p>Our contribution can be summarized as follows.</p>
<p>â€¢ First, we propose a framework for automatically evaluating the literature review writing ability of LLMs, without requiring any human involvement.This framework encompasses multiple stages, including the compilation of a literature review dataset construction, the collection of LLM-generated output, and the evaluation of LLM performance.</p>
<p>â€¢ Second, we collect 1,105 literature reviews from 51 journals across 5 disciplines as the ground truth.We then design three tasks for accessing LLMs in literature writing: reference generation, abstract writing, and literature composition on a given topic.</p>
<p>â€¢ Then, we evaluate the generated results of LLMs from multiple perspectives, including the hallucination rate in generated references, factual consistency, and semantic coverage compared to human-written content.</p>
<p>â€¢ Finally, we assess five LLMs using the proposed framework.By analyzing the experimental results, we find that hallucinated references remain a prevalent issue for current LLMs.Furthermore, the performance of LLMs in writing literature reviews varies across different disciplines.</p>
<p>Related Work</p>
<p>Recent studies have explored LLMs for literature review generation.For example, Wang et al. (2024b) proposed AutoSurvey, which incorporates up-to-date papers via retrieval-augmented generation.Agarwal et al. (2024) examined zero-shot LLM review generation using a two-step retrieval and outlining process.More recently, Liang et al. (2025) presented SurveyX, an efficient system that optimizes retrieval, extraction, and outline generation, supporting multimodal outputs such as figures and tables.Additionally, recent efforts to evaluate literature review generation by LLMs have increasingly focused on assessing hallucinations in reference citations.For instance, Chelli et al. (2024) analyzed hallucination rates in 11 systematic reviews on shoulder rotator cuff pathology generated by ChatGPT, GPT-4, and Bard, finding Bard exhibited significantly higher hallucination rates.Similarly, Agrawal et al. (2024)</p>
<p>Methodology</p>
<p>In this section, we propose a framework for evaluating LLMs' literature review writing ability.The framework, as shown in Figure 1, consists of three main stages: dataset construction and task design for evaluation, collection LLM-generated output, and assessment of the generated output.</p>
<p>Dataset Construction</p>
<p>Assessing the ability to write literature reviews is a challenging task, as evaluating the quality of content is inherently complex.In this paper, we use human-written reviews as the gold standard, which simplifies the evaluation process to some extent.As illustrated in Figure 1, we first collect publicly available information of literature reviews (i.e., the title, authors, abstract, and keywords) from the Annual Reviews website (https: //www.annualreviews.org/).Annual Reviews, an independent nonprofit publisher, produces 51 review journals spanning various scientific disciplines.Invited experts write comprehensive, authoritative reviews that synthesize and summarize the most significant primary research literature in their field, providing a valuable resource for researchers to stay current with the latest developments.We crawl all articles published in 2023, including their title, keywords, abstracts, contents, and references, and then clean them to create the experimental dataset.</p>
<p>Then, the dataset D is the article set from 51 journals, D = {p 0 , ..., p i , ..., p M }, where M represents the number of articles.Each article p i = {t i , w i , a i , c i , R i }, where t i , w i , a i , c i , R i represent the title, keywords, abstract, context, and reference set R i = {r 1 , ..., r k , ..., r K }, and K represents the length of the reference set.</p>
<p>Task Design</p>
<p>Since literature review writing primarily involves the collection and synthesis of relevant research, we design three independent tasks as follows to evaluate LLMs' capabilities in different aspects of literature review writing.</p>
<p>â€¢ Reference Generation: Given the article title t i and keywords w i , ask LLMs to find the N most relevant studies R g i = {r g 1 , ..., r g n , ..., r g N } to the research topic.Each citation study must include 7 metadata elements: title, authors, journal, year, volumes, first page, and last page, r g n = {T, A, J, Y, V, F P, LP }.In this task, we evaluate whether LLMs can recommend reliable references based on the given topic.Note that these references are not reused in later tasks.</p>
<p>â€¢ Abstract Writing: Given an article title t i and its associated keywords w i , the LLMs are prompted to generate an abstract a g i that aligns with the research topic.The length of the generated abstract is constrained to match that of the original.This task serves as a proxy for literature review planning, as abstracts often outline the key components of a study-such as its objectives, methods, and covered subtopics-which are also critical in structuring comprehensive literature reviews.By evaluating the model's ability to generate coherent and topic-relevant abstracts, we assess its potential to assist researchers in the early planning stages of literature review writing.</p>
<p>â€¢ Review Composition: Given the article t i and keywords w i , and abstract a i , ask LLMs to write a short literature review c g i according to the research topic provided in the title, keywords, and abstract.To facilitate evaluation and accommodate computational budget constraints, the length of each literature review is limited to approximately 1000 words.LLMs also need to back up claims by citing relevent studies R g i = {r g 1 , ..., r g n , ..., r g N } (with a total of N citations in the literature review).These citations are newly generated to support the content of the review.In this task, we evaluate whether LLMs can write a high-quality literature review and cite truth studies.</p>
<p>Three task prompts are shown in Appendix Table 5.</p>
<p>Evaluation Metrics</p>
<p>Based on the type of generated text, we divide the evaluation of the model's results into two parts: first, the hallucination rate of the references generated by LLMs, and second, a comparison of the generated context with human-written results, including two dimensions: factual consistency and semantic coverage.</p>
<p>Reference hallucination evaluation metrics.Given that LLMs are trained on vast corpora, including academic sources, we aim to evaluate whether they can generate true references.In this section, we introduce the calculation process of the reference precision P recison, reference recall Recall, F 1, and title search rate S t for each LLM.A higher precision metric indicates a lower hallucination rate.A higher recall indicates that the LLM-generated references cover more of the ground-truth citations used by human authors, reflecting a better ability to identify key prior work relevant to the topic.</p>
<p>For each article p i âˆˆ D, each LLM generates N references R g i = {r g 1 , ..., r g n , ..., r g N } in both Reference Generation and Review Composition tasks, each r g n and includes 7 elements.Each element corresponding to a state label represents whether it is accurate or not {e d } 6</p>
<p>d=0 , e d = 1 or 0. Next, we describe how to obtain {e d } 6</p>
<p>d=0 .First, we use the generated titles T and the first author in A as the queries and search them separately from external academic search engines.This results in two sets of candidate articles, Z t and Z a respectively, We then merge the two sets and remove duplicates to obtain the final candidate set Z = {z 1 , ..., z j , ..., z J }. Subsequently, we compare the generated r g n with the article z j from candidate sets Z.For example, if the title of a candidate article z j matches the title of r g n , then e 0 = 1.Finally, we find the best candidate article based on the sum of {e d } 6</p>
<p>d=0 , and the one with the largest sum is the best candidate article z j of r g n .Then, we compare the alignment degree between the generated reference r g n and the best-matching candidate article z j to determine whether r g n corresponds to a real article (as shown in Eq. 1).We consider r g n to be reliable under either of the following two conditions:</p>
<p>Title-based matching: If the title T is correct (i.e., e 0 = 1), and at least one other metadata element (e.g., author, journal, year, etc.) also matches, the reference is deemed reliable.To allow for minor variations, we consider the title to be correct if it achieves a match rate of at least 80% with the ground-truth title-a threshold determined through human evaluation.</p>
<p>Metadata-based matching: If the title T is incorrect (i.e., e 0 = 0), we still consider the reference reliable if at least three of the remaining metadata elements (author, journal, year, volume, first page, last page) match those of a real article.This allows us to identify true references even when the title is noisy or incomplete.
True(r g n ) = ï£± ï£´ ï£² ï£´ ï£³ 1 if e0 = 1 and 6 i=1 ei â‰¥ 1 or e0 = 0 and 6 i=1 ei â‰¥ 3 0 otherwise (1)
For each paper p i in the dataset, we compute the reference precision of the LLM-generated references, denoted as Precision(p i ), as defined in Eq. 2. We then obtain the overall Precision score for each LLM by averaging Precision(p i ) across all papers in the dataset, as shown in Eq. 3.</p>
<p>P recision (p
i ) = 1 N N n=0
True(r g n )</p>
<p>(2)
P reicison = 1 M M i=0 P recison(pi)(3)
Precision is measured by comparing the LLMgenerated references with external academic databases.We also evaluate Recall by comparing the references generated by the LLM with those cited in the human-written original articles.The key difference between precision and recall in our setting lies in the candidate set Z: for precision, Z is constructed from external academic search results, whereas for recall, Z consists of the references actually cited in the human-written articles.</p>
<p>Based on precision and recall, we further compute the F1 score, which serves as a harmonic mean to reflect the overall accuracy of the reference generation.</p>
<p>Additionally, the title is intuitively the most critical element in determining the faithfulness of a generated reference.In the work of Agrawal et al. (2024), ground-truth labels were assigned based on results returned by the Bing Search API.Inspired by their approach, we also calculate the title search score for each LLM to estimate how many generated titles correspond to real publications.
St = 1 M N M N s (n) p i (4) s (n) p i = r g n âˆˆR g i ï£± ï£´ ï£² ï£´ ï£³ 1 if the T âˆˆ r g n has return value from external Scholar API, 0 otherwise (5)
Here, s (n) p i indicates whether the generated title in reference r g n for paper p i can be found using an external academic search engine.This metric helps estimate the proportion of references with verifiable titles among the total generated references.</p>
<p>Context evaluation metrics.In our study, we use the human-written article as the gold truth and then evaluate LLM-generated context from factual consistency and semantic coverage aspects.The resemblance of natural language inference (NLI) to factual consistency evaluation has led to utilizing NLI models for measuring factual consistency (Gao et al., 2023).Encouraged by previous works, we also use the NLI method to evaluate the factual consistency between LLMs generated and humanwritten text.For example, we calculate the NLI score Entail p i between the original article abstract a i and the LLM-generated abstract a g i as follows.
Entailp i = Î¸ NLI (a g i , ai) = 1 if a g i entails ai, 0 otherwise (6)
where Î¸ N LI denotes the NLI model.Finally, we obtain the NLI score Entail for each model according to Eq 7.
Entail = 1 M M i=0 Entailp i (7)
Additionally, we use commonly employed semantic similarity metrics and Key Point Recall (KPR) to calculate the semantic coverage between the context generated by LLMs and human-written context.Specifically, for the Abstract Writing task, we apply cosine similarity and the ROUGE metric for semantic coverage evaluation.For the Review Composition task, we use the ROUGE metric and KPR to measure the semantic coverage of the literature review generated by the LLMs relative to human-written content.KPR, first proposed by Qi et al. (2024), is a metric designed to evaluate the effectiveness of LLMs in utilizing RAG for long documents.Since humanwritten literature reviews are lengthy and difficult to compare directly, we adopt the KPR method to measure the extent to which LLM-generated content covers the key points in human-written literature reviews.Specifically, we first use GPT-4 to extract q key points X i = [x i 1 , x i 2 , ..., x iq ] from the human-written literature review c i , and then calculate the coverage of these key points by the model-generated literature review as Eq. 8.
KP R = 1 M M i=0 xâˆˆX i Î¸ NLI (x, c g i ) |Xi| (8)
where c g i denotes the literature review generated by LLMs.</p>
<p>Finally, we also concatenate key points and compute the ROUGE metric between the key points and c g i .</p>
<p>Experiments</p>
<p>Experimental Settings</p>
<p>Dataset.We collect 1,105 literature review articles published in 2023 from the Annual Reviews website.The distribution of articles across journals is shown in Appendix B, Figure 5. LLMs Selection.We evaluate five LLMs: Claude-3.5-Sonnet-20240620,GPT-4o-2024-08-16, Qwen-2.5-72B-Instruct,DeepSeek-V3, and Llama-3.2-3B-Instruct.All model outputs were generated via their official APIs with temperature set to 0 for consistency.</p>
<p>In Reference Generation and Review Composition tasks, we set N as 10, each model generates 10 references.For the generated reference evaluation, we use Semantic Scholar as the external database.Recently, LLMs-as-judges has become more common (Chen et al., 2024;Zheng et al., 2023;Shangyu et al., 2024).So, for the Abstract Writing task, we employ TRUE (Honovich et al., 2022), along with GPT-4o as NLI models for evaluating factual consistency in context; to compute semantic similarity, we use text-embedding-3-large to convert texts into embeddings.For the Review Composition task, we employ GPT-4o as the NLI model in Eq 8, and set q as 10.</p>
<p>Main Results</p>
<p>We present the results for the three tasks in Table 1, 2, and 3.The performance of different models on each task is analyzed as follows.</p>
<p>Results for Reference Generation.As shown in Table 1, Claude-3.5-Sonnetachieves the highest F 1 score and S t , while Llama-3.2-3Bperforms the worst on both metrics.Notably, Claude-3.5-Sonnetalso achieves the highest precision and recall, indicating that it not only generates more correct references overall but also shares the highest overlap with those cited in the human-written article.When evaluating the author dimension of LLMgenerated references, we consider the reference to match in this dimension if the first author is correctly matched.Applying this criterion results in a 1-3% increase in F1 scores across all models.This suggests that generating complete and accurate author lists remains a major challenge for LLMs.</p>
<p>We further conduct a year-wise analysis of the correctly generated references, as illustrated in Figure 2. The results reveal that the majority of accurate citations produced by the models are concentrated in the period between 2010 and 2020, a trend consistent across nearly all LLMs evaluated in this task.</p>
<p>Results for Abstract Writing.As shown in Table 2, Claude-3.5-Sonnetachieves the best overall performance across most evaluation metrics.It generates abstracts with the highest average semantic similarity to human-written ones (81.17%) and shows strong factual consistency, achieving a TRUE score of 78.10%.DeepSeek-V3 also performs well in factual consistency, with the highest GPT-4o-based assessment score (96.84%).In contrast, Llama-3.2-3Bobtains the highest ROUGE-L score but does not show clear advantages on other metrics.These results highlight the importance of using multiple evaluation metrics to comprehensively assess the diverse outputs of LLMs.</p>
<p>Results for Review Composition.As shown in Table 3, compared to the Reference Generation task, all LLMs demonstrate a significant increase in Precision when generating references within the Review Composition task.Prior research indicates that grounding generated text with real external citations can effectively reduce hallucination rates (Gao et al., 2023).Consistently, our experiments reveal that when LLMs generate references alongside the literature review, the accuracy of these references improves markedly.This suggests a mutual constraint between the generated references and the review text, leading to enhanced overall reliability.</p>
<p>On the other hand, Claude-3.5-Sonnetachieves the highest performance on the KPR metric, indicating that its generated literature reviews recall the greatest number of claims from the human-written versions.Meanwhile, the literature reviews produced by DeepSeek-V3 excel on the ROUGE metrics, demonstrating stronger overlap with reference texts in terms of lexical similarity.</p>
<p>Analyze LLM-Generated References from Different Dimensions</p>
<p>In both Reference Generation and Review Composition tasks, we ask LLMs to generate references.The overall performance was discussed in the previous section.In this section, we provide a detailed across all dimensions in the Reference Generation task.Additionally, the accuracy of reference generation in the Reference Generation task for Claude-3.5-Sonnet,GPT-4o, and Qwen-2.5-72Bfollows a consistent trend across all dimensions, with the highest accuracy observed in the title dimension.Accuracy for journal name, page, and author is also relatively high.However, DeepSeek-V3 performs worse in the author dimension compared to the other dimensions.In contrast, Llama-3.2 demonstrates higher accuracy in the page and author dimensions than in other dimensions.However, overall, Llama-3.2-3Bdoes not exhibit a competitive advantage in reference generation accuracy.Next, we examine the accuracy of LLMgenerated references across various dimensions in Review Composition, as shown in Figure 3(b), and comparing it with Figure 3(a), we observe improvements across all dimensions for Claude-3.5-Sonnet,DeepSeek-V3, GPT-4o, and Qwen-2.5-72B,with particularly obvious gains in the author dimension.The possible reason is that, in the generated text, the LLMs tend to cite the first author's name, which may lead the models to place more emphasis on this dimension.Notably, the accuracy of DeepSeek-V3 and GPT-4o in certain dimensions approaches or even exceeds that of Claude-3.5-Sonnet.However, the performance of LLaMA-3.2-3Bremains suboptimal.</p>
<p>Cross-Disciplinary Analysis</p>
<p>In this section, we compare the performance of LLMs across different disciplines.</p>
<p>in Technology.</p>
<p>We then present bar charts in Figures 4, which illustrate the performance of different models across various tasks and disciplines.</p>
<p>First, we observe that in the Reference Generation task, as shown in Figure 4(a), almost all models exhibit the highest precision in the Mathematics discipline and the lowest precision in the Chemistry discipline.To validate these differences, we conduct one-way ANOVA tests for each LLM across five disciplines.Significant differences are found for all models except Llama-3.2-3B.Detailed ANOVA results are reported in Appendix G.</p>
<p>Secondly, as shown in Figure 4(b), the NLI scores evaluated by TRUE in the Abstract Writing task indicate that all models perform the worst in Social Science.GPT-4o performs best in Technology, while Claude 3.5-Sonnet achieves the highest performance in Biology.One-way ANOVA tests reveal significant differences across disciplines for all models.See Appendix G for detailed results.Thirdly, we examine the references precision of each model across five disciplines in the Review Composition task, as illustrated in Figure 4(c).It is evident that the precision of Claude 3.5 Sonnet, DeepSeek-V3, and GPT-4o is significantly higher than that of Qwen-2.5-72B and LLaMA-3.2-3Bacross all disciplines.Furthermore, Claude 3.5 Sonnet, DeepSeek-V3, and Qwen-2.5-72Bexhibit the highest precision in Mathematics, while GPT-4o performs best in Social Science.ANOVA tests confirm significant differences across disciplines for all models (see Appendix G).</p>
<p>Finally, we observe the KPR metric across different disciplines in Review Composition, as shown in Figure 4(d).The results from the figure indicate that the differences between models-Claude- 3.5-Sonnet, DeepSeek-V3, GPT-4o, and Qwen-2.5-72B-arenot significant across various disciplines, a finding that is also supported by statistical tests (see Appendix G).</p>
<p>Citation Frequency and Precision Across Disciplines.Additionally, we report statistics on the citation frequency of correctly generated references by LLMs in the Reference Generation task, as shown in Table 4, using Claude-3.5 and DeepSeek-V3 as examples.The data indicates that the references generated by the LLMs are highly cited, which might be due to their frequent presence in online sources, making them more likely to appear in the LLMs training datasets.As a result, LLMs tend to generate more accurate metadata (e.g., author, year) for these well-known references.</p>
<p>Furthermore, when analyzing different disciplines, we observe that the Mathematics discipline has the highest precision, and the relevant references generated for Mathematics also have the highest citation count.We compute the correlation between citation precision and average citation counts, finding that the correlation coefficient for Claude-3.5 is 0.4, and for DeepSeek-V3 it is 0.51, indicating a positive relationship between the two.</p>
<p>Human Evaluation</p>
<p>To evaluate the reliability of our automatic assessment method for identifying hallucinated refer-ences, we conduct a comparative analysis involving 100 LLM-generated references.These references were assessed by three annotators and the final manual results were obtained by majority vote.The results demonstrated a kappa agreement of 0.71 between the automatic and human assessments, signifying a relatively high level of consistency and supporting the reliability of our method.Furthermore, when using human assessment results as the gold standard, the automatic assessment method achieved an accuracy of 86%, further validating its effectiveness.</p>
<p>Conclusion</p>
<p>In this paper, we present a framework to assess the literature review writing abilities of LLMs.This framework includes three tasks designed to evaluate LLMs' literature review writing capabilities.The generated outputs are then evaluated from multiple dimensions using various tools, such as Semantic Scholar and NLI models, focusing on aspects like hallucination rate, semantic coverage, and factual consistency compared to humanwritten texts.Finally, we analyze the performance of LLMs in writing literature reviews from the perspective of different academic disciplines.</p>
<p>Limitations</p>
<p>In this paper, we evaluate the ability of LLMs to write literature reviews.However, several limitations remain: First, instead of evaluating the generated reviews from conventional perspectives such as fluency or topic coverage, we primarily compare LLMgenerated results with human-written ones.As such, our current evaluation metrics may not be comprehensive.In the future, we plan to incorporate additional aspects of review quality to improve the completeness of our evaluation.These may include the coverage of cited works (i.e., whether the review offers a comprehensive overview of the relevant field) and the coherence of the overall structure (i.e., whether the review is organized in a way that facilitates information-seeking).</p>
<p>Second, there is a possibility that our test data overlaps with the training data of the LLMs.When we initiated this study in August 2024, the dataset from the Annual Reviews website had not yet been updated to include 2024 articles, so we relied on the complete 2023 dataset.To mitigate potential data leakage, we plan to deploy a leaderboard on Hugging Face to continuously evaluate the performance of various LLMs in literature review writing, with real-time updates to the test dataset.However, due to the rapid iteration of LLMs, data leakage cannot be completely ruled out.That said, our experimental results-particularly those related to reference generation-show that all models still perform poorly.If data contamination were present, the actual scores would likely be lower than those reported.This reinforces, rather than undermines, our conclusion that significant challenges remain in using LLMs for literature review generation.</p>
<p>Additionally, when processing LLM-generated outputs, we frequently encountered abbreviated author names and journal titles.Although we have taken care to address these issues thoroughly (see Appendix E), minor discrepancies may still exist.</p>
<p>Finally, to verify the precision of LLMgenerated references, we primarily used Semantic Scholar as our auxiliary tool.Although we also experimented with Google Scholar, its lack of an accessible API led us to rely on the freely available Semantic Scholar API for consistency and ease of access.However, this may have resulted in incomplete reference retrieval.</p>
<p>B Data Distribution</p>
<p>Statistics of the dataset are shown in Figure 5.</p>
<p>C Comparison of LLM-Cited and</p>
<p>Human-cited References from Different Dimensions</p>
<p>We provide a more detailed comparison of the LLM-cited and human-cited references across various dimensions.As shown in Figure 6, for Reference Generation, we observe that the overlap rate is higher in the "Title" and other numerical dimensions, while the overlap rates for the "Journal" and "Author" dimensions are relatively lower.For Review Composition, Claude-3.5-Sonnetand GPT-4o exhibit a higher overlap rate on the "Author" dimension compared to Reference Generation.This trend is consistent with the findings in Figure 6, as the citation of author names in the literature for Review Composition leads to the generation of more accurate author information.</p>
<p>D Discussion</p>
<p>We select four LLMs for task evaluation and find that Claude-3.5-Sonnetoutperforms DeepSeek-V3, GPT-4o, Qwen-2.5-72B, and Llama-3.2-3Bacross all three tasks, particularly excelling in the task of generating accurate references.This advantage is likely influenced by the training data of each model.Additionally, we observed that each model has different strengths across disciplines.Overall, for the reference generation task, nearly all models perform better in Mathematics, while their performance is weaker in Chemistry and Technology.However, when writing abstracts, all models exhibit the lowest factual consistency in Social Science, as indicated by the entailment scores, compared to human-written texts.</p>
<p>When comparing the references generated by the models in Reference Generation and Review Composition, we find that in Review Composition, nearly all models generate more accurate references.This suggests that LLMs cite references during the writing process, which improves the authenticity of the references.Moreover, the inclusion of the first author's name in the generated context also enhances the accuracy of the author dimension.</p>
<p>In the future, we will deploy this evaluation framework on Hugging Face to enable real-time assessments of newly released LLMs.</p>
<p>E Data Processing Strategy</p>
<p>Author name and journal title variations often pose challenges when aligning LLM-generated references with articles from Semantic Scholar.To address this, we adopt the following normalization strategies:</p>
<p>Author names.When comparing author names between LLM-generated references and candidate articles, if an exact match is not found (e.g., "John Smith"), we consider common variants such as "Smith, John", "Smith, J.", or "J.Smith" to account for different citation formats.</p>
<p>Journal titles.For journal names like Journal of Chemical Physics, we incorporate standard abbreviation forms (e.g., J. Chem.Phys.) based on widely used abbreviation conventions.Nonetheless, certain non-standard or ambiguous cases may still be unmatched.</p>
<p>F LLM Output Example</p>
<p>Here is one example of the output generated by Claude-3.5-Sonnetfor the literature review composition task.</p>
<p>{"Literature Review": "CRISPR-Cas systems have emerged as a powerful tool in the field of biosensing, offering unprecedented capabilities for detecting nucleic acids, proteins, and small molecules.This review explores the technical developments and application prospects of CRISPRbased biosensing strategies, highlighting their potential to revolutionize diagnostics and molecular detection.The core biochemical properties of CRISPR-Cas systems provide the foundation for their application in biosensing.These properties include diverse reaction temperatures, programmability in design, high reaction efficiency, and recognition specificity.Chen et al. (2018) demonstrated the versatility of CRISPR-Cas9 in detecting DNA targets across a wide temperature range, from 20Â°C to 60Â°C, enabling its use in various environmental conditions.The programmability of CRISPR systems allows for the easy design of guide RNAs (gRNAs) to target specific sequences, as shown by Gootenberg et al. (2017) in their development of the SHERLOCK (Specific High-sensitivity Enzymatic Reporter unLOCKing) platform for nucleic acid detection.The high reaction efficiency of CRISPR-Cas systems contributes to their sensitivity in biosensing applications.Li et al. ( 2019) reported a CRISPR-Cas12a-based assay capable of detecting attomolar concentrations of DNA targets, demonstrating the potential for ultrasensitive detection.The recognition specificity of CRISPR systems allows for precise discrimination between closely related sequences, as evidenced by the work of Myhrvold et al. ( 2018), who developed a CRISPR-based diagnostic platform capable of distinguishing between Zika and Dengue virus strains with single-base resolution.Recent efforts have focused on improving these core properties to enhance the performance of CRISPR-based biosensors.For example, Harrington et al. ( 2018) engineered Cas12a variants with improved activity and specificity, expanding the range of targetable sequences and reducing off-target effects.Similarly, Gootenberg et al. (2018) developed an enhanced version of the SHERLOCK system, incorporating lateral flow readouts and multiplexing capabilities, further improving its utility in point-of-care diagnostics.One of the key areas of technical development in CRISPR-based biosensing is improving sensitivity and quantification capabilities.Wang et al. (2020) introduced a CRISPR-Cas12a-based digital PCR method that achieved attomolar sensitivity and precise quantification of target nucleic acids.This approach combines the specificity of CRISPR recognition with the quantitative power of digital PCR, opening new avenues for highly sensitive and accurate molecular diagnostics.Multiplex detection is another important aspect of CRISPR biosensing development.Ackerman et al. (2020) demonstrated a CRISPR-Cas13-based multiplexed assay capable of simultaneously detecting and distinguishing multiple SARS-CoV-2 variants, showcasing the potential of CRISPR technology in rapid and accurate pathogen identification.The ability to detect multiple targets in a single reaction not only saves time and resources but also provides more comprehensive diagnostic information.Efforts to develop convenient one-pot assays have led to simplified and more user-friendly CRISPR-based detection methods.Joung et al. ( 2020) reported a one-pot, CRISPR-Cas12-based assay for SARS-CoV-2 detection that could be performed at a single temperature, eliminating the need for complex thermal cycling equipment.This advancement brings CRISPR-based diagnostics closer to pointof-care applications, where simplicity and ease of use are crucial.The creation of advanced sensors has further expanded the capabilities of CRISPRbased biosensing.Bruch et al. ( 2019) developed a CRISPR-Cas13a-based electrochemical sensor for nucleic acid detection, demonstrating the potential for integrating CRISPR technology with electronic readout systems.This approach offers the possibility of developing portable, electronic CRISPRbased diagnostic devices for rapid and sensitive detection in resource-limited settings.While CRISPRbased biosensing has shown great promise, there are still obstacles to its widespread commercial application.These include challenges in sample preparation, the need for standardization across different CRISPR systems and assay formats, and regulatory considerations for diagnostic use.Addressing these issues will be crucial for the successful translation of CRISPR biosensing technology from the laboratory to real-world applications.Looking ahead, the future of CRISPR-based biosensing appears bright, with numerous opportunities for further development and application.Potential directions include the integration of CRISPR biosensors with microfluidic devices for automated sample processing and analysis, the development of CRISPR-based wearable sensors for continuous health monitoring, and the application of machine learning algorithms to improve the design and performance of CRISPR biosensing systems.In conclusion, CRISPR-based biosensing strategies represent a rapidly evolving field with immense potential to transform molecular detection and diagnostics.As researchers continue to refine and expand upon the core capabilities of CRISPR systems, we can expect to see increasingly sensitive, specific, and versatile biosensing applications emerge, with far-reaching implications for healthcare, environmental monitoring, and beyond.","References": [{"title": "CRISPR-Cas12a target binding unleashes indiscriminate singlestranded DNase activity", "authors": "Chen, Janice S., Ma, Enbo, Harrington, Lucas B., Da Costa, Maria, Tian, Xiang, Palefsky, Joel M.,</p>
<p>Figure 1 :
1
Figure 1: Illustration of the evaluation framework.</p>
<p>Figure 2 :
2
Figure 2: Distribution of LLM-generated true references over years.</p>
<p>Figure 3: Radar chart of the accuracy of LLM-generated references across various dimensions.</p>
<p>Figure 4: Three tasks evaluation scores across different disciplines.</p>
<p>Figure 5 :
5
Figure 5: Statistics of dataset.</p>
<p>Figure 6 :
6
Figure 6: Radar chart of the accuracy of LLM-generated references with human-written references in the original article.</p>
<p>Table 1 :
1
The experimental results of the five LLMs in Reference Generation."S t " refers to the title search rate as defined in Eq 4, while "P" represents the Precision, "R" denotes the Recall."first author" refers to when evaluating the accuracy of references, the author dimension only comparing the first author.
First, based on</p>
<p>Table 2 :
2
Compare the performance of four LLMs on Abstract Writing.
Review CompositionModelsReferencesLiterature ReviewStâ†‘PRF1PR F1(first author) KPRâ†‘ ROUGE-1â†‘ ROUGE-2â†‘ ROUGE-Lâ†‘Qwen-2.5-72B 40.02 28.91 17.36 21.69 33.64 18.3123.71338.8229.959.0115.14Llama-3.2-3B21.78 4.86 8.28 6.12 7.28 8.447.8229.0728.077.7715.46DeepSeek-V362.29 52.81 26.79 35.55 55.38 27.3036.5756.0235.6510.4017.46GPT-4o60.05 50.62 27.88 35.96 54.16 28.8637.6559.1830.789.7215.54Claude-3.5-Sonnet 66.43 59.06 31.90 41.42 63.06 33.2543.5462.3228.598.9014.41</p>
<p>Table 3 :
3
The experimental results of the four LLMs in Review Composition."S
DisciplineCitation Count DeepSeek Claude DeepSeek Claude PrecisionBiology76367855.5558.00Mathematics2288198460.0062.22Physics89465247.6256.19Chemistry1334107943.1443.80Social Science 1321115146.8056.70Technology90474844.8849.01
t " refers to the title retrieval rate as defined in Eq 4. While "P" represents the Precision, "R" denotes the Recall."KPR" means the Key Point Recall rate."firstauthor" refers to when evaluating the accuracy of references, the author dimension only comparing the first author.</p>
<p>Table 4 :
4
Average citation counts and reference precision across disciplines.</p>
<p>https://anonymous.4open.science/r/Eval_LLM_ LR-C657
Ethics StatementThe human evaluations conducted in this study were carried out by members of the research team.No personal or sensitive information was collected, and all participants were fully informed of the purpose of the evaluation.Therefore, the study does not raise any ethical concerns.A Prompts for TasksPromptContent Prompt 1Imagine you are an experienced academic researcher with access to a vast library of scientific literature.I would like you to find the 10 studies that are most relevant to the research topic provided in the "Title" and the "Keywords" below.Please cite the studies according to the following JSON format.There is no need to provide any explanation before or after the JSON output.Ensure that the "authors" field lists the names of all authors and not exceeding 10 authors, and that there are no duplicate author names nor abbreviations such as "et al.".{ "References": [ { "title": "", "authors": "", "journal": "", "year": "", "volumes": "", "first page": "", "last page": "", } ] } Title: title Keywords: keywords Prompt 2Imagine you are an experienced academic researcher with access to a vast library of scientific literature.I would like you to write an abstract according to the research topic provided in the "Title" and the "Keywords" below.Please write the abstract for about xx words, according to the JSON format as follows.There is no need to provide any explanation before or after the JSON output.{"Abstract": ""} Title: title Keywords: keywords Prompt 3Imagine you are an experienced academic researcher with access to a vast library of scientific literature.I would like you to write a literature review according to the research topic provided in the "Title", "Abstract" and "Keywords" below.The literature review should be about 1000 words long.I would like you to back up claims by citing previous studies (with a total of 10 citations in the literature review).The output should be in JSON format as follows: { "Literature Review": "xxx", "References": [ { "title": "", "authors": "", "journal": "", "year": "", "volumes": "", "first page": "", "last page": "", } ] } The "Literature Review" field should be about 1000 words.The "References" field is a list of 10 references, and ensures that the "authors" field lists the names of all authors and not exceeding 10 authors, and that there are no duplicate author names nor abbreviations such as "et al.".Title: title Keywords: keywords Abstract: abstract
Shubham Agarwal, Gaurav Sahu, Abhay Puri, H Issam, Laradji, D J Krishnamurthy, Jason Dvijotham, Laurent Stanley, Christopher Charlin, Pal, 10.48550/arXiv.2412.15249arXiv:2412.15249ArXiv:2412.15249Llms for literature review: Are we there yet?. 2024</p>
<p>Do language models know when they're hallucinating references?. Ayush Agrawal, Mirac Suzgun, Lester Mackey, Adam Tauman, Kalai , 2024</p>
<p>The canonical model of structure for data extraction in systematic reviews of scientific research articles. Muhammad Bello Aliyu, Rahat Iqbal, Anne James, 10.1109/SNAMS.2018.85548962018 Fifth International Conference on Social Networks Analysis, Management and Security (SNAMS). 2018</p>
<p>Reference hallucination score for medical artificial intelligence chatbots: Development and usability study. Mohamad-Hani Fadi Aljamaan, Ibraheem Temsah, Ayman Altamimi, Amr Al-Eyadhy, Khalid Jamal, Alhasan, A Tamer, Mohamed Mesallam, Khalid H Farahat, Malki, 10.2196/54345Company: JMIR Medical Informatics Distributor: JMIR Medical Informatics Institution: JMIR Medical Informatics Label: JMIR Medical Informatics publisher. Toronto, CanadaJMIR Publications Inc202412e54345</p>
<p>Exploring the boundaries of reality: Investigating the phenomenon of artificial intelligence hallucination in scientific writing through chatgpt references. Anirudh Sai, ; Athaluri, Manoj V S R Krishna, Vineel Kesapragada, Tirth Yarlagadda, Rama Dave, Siri Tulasi, Duddumpudi, 10.7759/cureus.37432Sandeep Varma Manthena. 202315e37432</p>
<p>Hallucination rates and reference accuracy of chatgpt and bard for systematic reviews: Comparative analysis. MikaÃ«l Chelli, Jules Descamps, Vincent LavouÃ©, Christophe Trojani, Michel Azar, Marcel Deckert, Jean-Luc Raynier, Gilles Clowez, Pascal Boileau, Caroline Ruetsch-Chelli, 10.2196/53164Journal of Medical Internet Research. 26e531642024</p>
<p>Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, Benyou Wang, 10.48550/arXiv.2402.10669arXiv:2402.10669ArXiv:2402.10669Humans or llms as the judge? a study on judgement biases. 2024</p>
<p>Enabling large language models to generate text with citations. Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen, 10.48550/arXiv.2305.14627arXiv:2305.14627ArXiv:2305.146272023</p>
<p>TRUE: Re-evaluating factual consistency evaluation. Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, Yossi Matias, 10.18653/v1/2022.naacl-main.287Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United States2022Association for Computational Linguistics</p>
<p>CHIME: LLM-assisted hierarchical organization of scientific studies for literature review support. Chao-Chun, Erin Hsu, Jenna Bransom, Bailey Sparks, Chenhao Kuehl, David Tan, Lucy Wadden, Aakanksha Wang, Naik, 10.18653/v1/2024.findings-acl.8Findings of the Association for Computational Linguistics: ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>Using a neural network-based feature extraction method to facilitate citation screening for systematic reviews. Georgios Kontonatsios, Sally Spencer, Peter Matthew, Ioannis Korkontzelos, 10.1016/j.eswax.2020.100030Expert Systems with Applications: X. 61000302020</p>
<p>Xun Liang, Jiawei Yang, Yezhaohui Wang, Chen Tang, Zifan Zheng, Shichao Song, Zehao Lin, Yebin Yang, Simin Niu, Hanyu Wang, Bo Tang, Feiyu Xiong, Keming Mao, Zhiyu Li, 10.48550/arXiv.2502.14776arXiv:2502.14776ArXiv:2502.14776Surveyx: Academic survey automation via large language models. 2025</p>
<p>Long2rag: Evaluating long-context &amp; long-form retrieval-augmented generation with key point recall. Zehan Qi, Rongwu Xu, Zhijiang Guo, Cunxiang Wang, Hao Zhang, Wei Xu, 10.18653/v1/2024.findings-emnlp.279Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Efuf: Efficient fine-grained unlearning framework for mitigating hallucinations in multimodal large language models. Xing Shangyu, Zhao Fei, An Wu Zhen, Chen Tuo, Li Weihao, Zhang Chunhui, Dai Jianbing, Xinyu, 2024</p>
<p>Evaluating large language models on academic literature understanding and review: An empirical study among early-stage scholars. Jiyao Wang, Haolong Hu, Zuyuan Wang, Song Yan, 10.1145/3613904.3641917Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing SystemsHonolulu HI USAACM2024aYouyu Sheng, and Dengbo He</p>
<p>Autosurvey: Large language models can automatically write surveys. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, Yue Zhang ; Lianmin, Wei-Lin Zheng, Ying Chiang, Siyuan Sheng, Zhanghao Zhuang, Yonghao Wu, Zi Zhuang, Zhuohan Lin, Dacheng Li, Eric P Li, Hao Xing, Joseph E Zhang, Ion Gonzalez, Stoica, 10.48550/arXiv.2306.05685arXiv:2406.10252ArXiv:2306.05685Judging llm-as-a-judge with mt-bench and chatbot arena. 2024b. 2023</p>
<p>CRISPR-Cas13-based electrochemical biosensing of viral RNA: Application to detection of SARS-CoV-2. Jennifer A Doudna, Jonathan S Gootenberg, Omar O Abudayyeh, Jeong Lee, Wook, Essletzbichler, Patrick, Aaron J Dy, Joung, Julia, Verdine, Vanessa, Donghia, Nina, Nichole M Daringer, Catherine A Freije, Li, Suwei, Cheng, Qingmei, Wang, Jianming, Li, Xiaoyu, Zhang, Zhiwei, Gao, Shan, Cao, Rong, Zhao, Guoping, Jin ; Wang, Myhrvold, Cameron, Catherine A Freije, Jonathan S Gootenberg, Omar O Abudayyeh, Hayden C Metsky, Ann F Durbin, Max J Kellner, Amanda L Tan, Lauren M Paul, Leda A Parham, Lucas B Harrington, Paez-Espino, David, Staahl, T Brett, Janice S Chen, Ma, Enbo, Nikos C Kyrpides, Jennifer A Doudna, Jonathan S Gootenberg, Omar O Abudayyeh, Max J Kellner, Joung, Julia, James J Collins, Feng ; Zhang, Wang, Xiaoxia, Zhong, Minjie, Liu, Yue, Ma, Pengfei, Dang, Lei, Meng, Qing, Wan, Wanying, Ma, Xiaowei, Liu, Jing, Guohua ; Yang, Julia Joung, Ladha, Alim, Saito, Makoto, Kim, Nam-Gyun, Ann E Woolley, Segel, Michael, Barretto, P J Robert, Ranu, Antonija, Rhiannon K Macrae, Faure, Cheri M Guilhem ; Ackerman, Myhrvold, Cameron, Thakku, G Shiv, Catherine A Freije, Hayden C Metsky, David K Yang, Simon H Ye, Chloe K Boehm, Kosoko-Thoroddsen, F Tinna-Solveig, Jared Kehe, Scalable and robust SARS-CoV-2 testing in an academic center. Bruch, Richard, Baaske, Johannes, Chatelle, Claire, Meirich, Maren, Madlener, Sibylle, Weber, Wilfried, Dincer, Can, Urban, Gerald A.Field-deployable viral diagnostics using CRISPR-Cas13. journal": "Nature Biotechnology", "year": "2020", "volumes": "38", "first page": "927", "last page": "931"}]} G ANOVA Test Results We report the p-values from one-way ANOVA tests across disciplines for each model and task: â€¢ Reference Generation task: Claude-3.5-Sonnet (p&lt;.0001), DeepSeek-V3(p&lt;.0001), GPT-4o(p&lt;.0001), Qwen-2.5-72B (p&lt;.0001), Llama-3.2-3B (p=0.065</p>
<p>Qwen2.5-72B (p&lt;.001Abstract Writing task (NLI scores): Claude-3.5-Sonnet(p&lt;.0001), DeepSeek-V3(p&lt;0.05). Llama-3.2-3B (p&lt;.0001</p>
<p>Claude-3.5-Sonnet(p&lt;.0001), DeepSeek-V3(p&lt;.0001). Qwen-2.5- 72B(p&lt;.0001Review Composition (Reference accuracy. Llama-3.2-3B(p&lt;.001</p>
<p>DeepSeek-V3 (p=0.23), GPT-4o. Claude-3.5-Sonnet (p=0.46). Review Composition (KPR metric. p=0.18), Qwen-2.5-72B (p=0.10), and Llama-3.2-3B (p&lt;0.001</p>            </div>
        </div>

    </div>
</body>
</html>
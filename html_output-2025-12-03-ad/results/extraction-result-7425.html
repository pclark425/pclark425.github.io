<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7425 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7425</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7425</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-271924212</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.12055v1.pdf" target="_blank">Aligning (Medical) LLMs for (Counterfactual) Fairness</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have emerged as promising solutions for a variety of medical and clinical decision support applications. However, LLMs are often subject to different types of biases, which can lead to unfair treatment of individuals, worsening health disparities, and reducing trust in AI-augmented medical tools. Aiming to address this important issue, in this study, we present a new model alignment approach for aligning LLMs using a preference optimization method within a knowledge distillation framework. Prior to presenting our proposed method, we first use an evaluation framework to conduct a comprehensive (largest to our knowledge) empirical evaluation to reveal the type and nature of existing biases in LLMs used for medical applications. We then offer a bias mitigation technique to reduce the unfair patterns in LLM outputs across different subgroups identified by the protected attributes. We show that our mitigation method is effective in significantly reducing observed biased patterns. Our code is publicly available at \url{https://github.com/healthylaife/FairAlignmentLLM}.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7425.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7425.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero/Few-shot/CoT (Q‑Pain, Meditron)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot, Few-shot and Chain-of-Thought prompting (effect on Q‑Pain task for Meditron)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports that the prompt presentation (zero-shot, few-shot, chain-of-thought) changes measured counterfactual fairness for the Q‑Pain pain-management QA task; Meditron (7B) showed increased demographic sensitivity under Few‑Shot and Chain‑of‑Thought prompting with statistically significant differences in specific subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Aligning (Medical) LLMs for (Counterfactual) Fairness</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Meditron</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source clinically fine-tuned LLM (medical pretraining/fine-tuning described in paper citations).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Q‑Pain (pain management vignette QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Vignette-based clinical QA asking whether to give pain medication (closed yes/no), repeated across rotated patient demographics to test counterfactual fairness.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Closed-ended binary question (yes/no) presented under different prompting strategies: Zero‑Shot, Few‑Shot, Chain‑of‑Thought</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Comparisons between Base (Zero‑Shot), Base (Few‑Shot with a few exemplars), and Base (CoT with stepwise reasoning). Each vignette repeated with randomized demographic fields; token probability distributions used to infer changes in probability of answering 'no'.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average maximum probability difference between any two demographic groups (minimax fairness metric) and Welch's ANOVA for statistical testing</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Meditron exhibited statistically significant disparities: significant discrepancies under Welch's ANOVA (p ≤ 0.05) in CNC and ANC tasks with Few‑Shot prompting, and in the CNC task with Chain‑of‑Thought; exact magnitude per prompt not reported in text (see Fig. 3 for plotted values).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Base (Zero‑Shot) prompting (used as reference in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Reported increase in demographic disparity for Meditron under Few‑Shot (CNC, ANC) and under CoT (CNC) relative to Base (Zero‑Shot); exact numerical absolute changes not specified in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Models evaluated as 'Base' (no alignment) under Zero‑Shot, Few‑Shot, and CoT prompting; demographic rotation (race/gender) applied to vignettes; similarity of answers measured via sentence embeddings for preference generation in other experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Welch's ANOVA reported significant discrepancies with p ≤ 0.05 for the noted tasks/prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Aligning (Medical) LLMs for (Counterfactual) Fairness', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7425.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7425.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Closed‑form (Treatment Recommendation, Llama 3‑OpenBioLLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Closed-ended yes/no treatment-recommendation prompts (imaging/referral) for Llama 3‑OpenBioLLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using closed-form yes/no prompts for treatment recommendation enabled token-probability based fairness measurements; Llama 3‑OpenBioLLM showed large demographic disparities in imaging recommendations in this format.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Aligning (Medical) LLMs for (Counterfactual) Fairness</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3‑OpenBioLLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Domain-focused open-source Llama‑3 variant fine-tuned for biomedical/clinical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Treatment Recommendation (referral and imaging)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a short patient-case summary, the model answers whether to refer to specialist and whether to order advanced imaging (CT/MRI/ultrasound); closed-form yes/no responses used to measure demographic impact.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Closed-ended binary JSON-style response (yes/no) designed to allow extraction of token probability of 'yes' or 'no'</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>question type</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Prompts ask two binary questions with discrete yes/no outputs; authors used token probability of 'yes' to compute maximum differences across demographic rotations for each question.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Maximum difference in predicted probability (yes/no) across demographic subgroups (per question)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Llama 3‑OpenBioLLM produced >15% probability differences in 6 of 10 imaging-referral questions, with the largest single-question discrepancy being 35% (between demographic subgroups).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Prompts formulated to elicit closed-form yes/no answers; demographic fields (race/gender) rotated; probabilities derived from token logits.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>No global statistically significant discrepancies reported for Referral or Imaging at dataset level in text, but large per-question disparities observed (numbers reported as counts and maximums).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Aligning (Medical) LLMs for (Counterfactual) Fairness', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7425.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7425.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Likert (Triage, Llama 3‑OpenBioLLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Likert-scale (1–5) agreement prompts for triage judgments (effect on Llama 3‑OpenBioLLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Triage prompts required the model to rate agreement with a statement on a 1–5 Likert scale; Llama 3‑OpenBioLLM showed statistically significant differences in category distributions across demographic groups in this presentation format.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Aligning (Medical) LLMs for (Counterfactual) Fairness</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3‑OpenBioLLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Domain-focused Llama‑3 variant fine-tuned for healthcare/life-sciences use cases.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Triage (perception and assessment) Likert rating</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Models are shown triage vignette and asked to rate agreement with a statement on a 1–5 Likert scale (1 strongly disagree to 5 strongly agree); distributions compared across demographics to detect bias.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Likert-scale rating task (ordinal categorical output, 1–5)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>question type</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Because outputs are non-binary, the authors run each prompt multiple times to estimate distribution over ratings; stacked bar charts compared proportion of ratings per demographic group; Pearson Chi‑Squared tests applied.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Distributional differences across Likert categories per demographic group; Pearson Chi‑Squared test for association</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Llama 3‑OpenBioLLM exhibited statistically significant biases for the majority of demographic pairs (Pearson Chi‑Squared p ≤ 0.05). Gemma 2 and Meditron also showed significant differences in some comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Prompts included randomized [race] and [gender] placeholders; multiple runs per vignette to estimate rating distributions; statistical testing via Pearson Chi‑Squared.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Pearson Chi‑Squared reported significant differences (p ≤ 0.05) for Llama 3‑OpenBioLLM across most demographic pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Aligning (Medical) LLMs for (Counterfactual) Fairness', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7425.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7425.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alignment × Prompting (CoT best when aligned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of combining model alignment (preference optimization/LoRA fine-tuning) with prompting style (Zero‑Shot, Few‑Shot, CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>After applying the proposed preference‑optimization alignment (SimPO + LoRA) the authors compared aligned models under Zero‑Shot, Few‑Shot and Chain‑of‑Thought prompts and found CoT combined with alignment produced the largest reductions in measured demographic disparities for several models (notably Llama 3.1 and Gemma 2).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Aligning (Medical) LLMs for (Counterfactual) Fairness</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3.1; Gemma 2 (examples emphasized)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama 3.1 (8B) general-purpose Llama‑3 family model; Gemma 2 (9B) general-purpose open model; both evaluated before and after alignment via SimPO+LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Llama 3.1: 8B; Gemma 2: 9B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Q‑Pain, Treatment Recommendation, and Triage (same evaluation suite)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Set of clinical vignette QA tasks used to measure counterfactual fairness across demographic rotations; tasks include closed binary and Likert formats.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Compared Base (Zero‑Shot / Few‑Shot / CoT) vs Aligned (Zero‑Shot / Few‑Shot / CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Alignment pipeline: generate preference dataset (teacher answers + demographic red‑teaming), rank candidate responses by semantic similarity to teacher answer, fine‑tune Target LLM via SimPO using LoRA. Evaluations repeated across prompting strategies (Zero/Few/CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average maximum probability difference between demographic groups (for binary tasks) and distributional differences for Likert tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Alignment reduced observed disparities across all models/tasks; Llama 3.1 and Gemma 2 showed the most pronounced improvements after alignment. CoT prompting generally outperformed Zero‑Shot and Few‑Shot when combined with alignment (numerical reductions shown in figures but specific absolute percentages not enumerated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Unaligned Base models under the same prompting strategies</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Consistent decrease in maximum demographic difference after alignment vs baseline; for Meditron statistical significance of prior disparities disappeared (no longer significant by Welch's ANOVA). Exact absolute improvements per model/task/prompt are plotted in figures but not fully enumerated in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Teacher model: Gemini (commercial) used to produce reference answers; Gecko embeddings for semantic similarity; preference dataset ~1,500 queries from EquityMedQA-derived set; LoRA low‑rank fine‑tuning with SimPO objective.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Authors report Meditron 'no longer shows significant biases under Welch's ANOVA' after alignment; other per-model p‑values not explicitly listed in text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Aligning (Medical) LLMs for (Counterfactual) Fairness', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7425.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7425.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Closed vs Non‑binary prompt measurement note</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of problem presentation (closed binary vs open/non-binary) on measurement methodology and sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors note that closed-ended binary prompts allow direct use of token probability distributions to infer demographic effects with fewer runs, whereas non-binary Likert prompts require multiple runs and distribution estimation; the choice of prompt format therefore affects sensitivity and the measurement pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Aligning (Medical) LLMs for (Counterfactual) Fairness</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies to all evaluated models (methodological note)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied to Llama 3.1, Gemma 2, Meditron, Llama 3‑OpenBioLLM in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (7B–9B as evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Methodological: binary Q‑Pain & Treatment Recommendation vs Likert Triage</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Shows how prompt output modality (binary vs ordinal) changes how model outputs are captured and statistically tested for demographic disparities.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Binary closed‑form vs Likert ordinal outputs</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / output modality</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>For binary closed prompts the authors read token probabilities (single run) to compute differences; for non-binary Likert outputs they ran prompts multiple times to estimate response distribution across categories and applied chi‑squared tests.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Measurement sensitivity (enables per‑question probability differences vs distributional comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Binary format permitted direct probability differences (used to report >15% and up to 35% differences in some cases); non-binary format required repeated sampling and distributional tests which revealed significant demographic associations (chi‑squared p ≤ 0.05) for some models.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Binary prompts: single-run token probability inspection; Likert prompts: repeated runs to recover empirical distribution; statistical tests chosen accordingly (Welch's ANOVA for probability outputs; Pearson Chi‑Squared for Likert distributions).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Aligning (Medical) LLMs for (Counterfactual) Fairness', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Language models are few‑shot learners <em>(Rating: 2)</em></li>
                <li>Q‑pain: a question answering dataset to measure social bias in pain management <em>(Rating: 2)</em></li>
                <li>A toolbox for surfacing health equity harms and biases in large language models <em>(Rating: 2)</em></li>
                <li>Simpo: Simple preference optimization with a reference‑free reward <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7425",
    "paper_id": "paper-271924212",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Zero/Few-shot/CoT (Q‑Pain, Meditron)",
            "name_full": "Zero-shot, Few-shot and Chain-of-Thought prompting (effect on Q‑Pain task for Meditron)",
            "brief_description": "The paper reports that the prompt presentation (zero-shot, few-shot, chain-of-thought) changes measured counterfactual fairness for the Q‑Pain pain-management QA task; Meditron (7B) showed increased demographic sensitivity under Few‑Shot and Chain‑of‑Thought prompting with statistically significant differences in specific subtasks.",
            "citation_title": "Aligning (Medical) LLMs for (Counterfactual) Fairness",
            "mention_or_use": "use",
            "model_name": "Meditron",
            "model_description": "Open-source clinically fine-tuned LLM (medical pretraining/fine-tuning described in paper citations).",
            "model_size": "7B",
            "task_name": "Q‑Pain (pain management vignette QA)",
            "task_description": "Vignette-based clinical QA asking whether to give pain medication (closed yes/no), repeated across rotated patient demographics to test counterfactual fairness.",
            "problem_format": "Closed-ended binary question (yes/no) presented under different prompting strategies: Zero‑Shot, Few‑Shot, Chain‑of‑Thought",
            "format_category": "prompt style",
            "format_details": "Comparisons between Base (Zero‑Shot), Base (Few‑Shot with a few exemplars), and Base (CoT with stepwise reasoning). Each vignette repeated with randomized demographic fields; token probability distributions used to infer changes in probability of answering 'no'.",
            "performance_metric": "Average maximum probability difference between any two demographic groups (minimax fairness metric) and Welch's ANOVA for statistical testing",
            "performance_value": "Meditron exhibited statistically significant disparities: significant discrepancies under Welch's ANOVA (p ≤ 0.05) in CNC and ANC tasks with Few‑Shot prompting, and in the CNC task with Chain‑of‑Thought; exact magnitude per prompt not reported in text (see Fig. 3 for plotted values).",
            "baseline_performance": "Base (Zero‑Shot) prompting (used as reference in the paper)",
            "performance_change": "Reported increase in demographic disparity for Meditron under Few‑Shot (CNC, ANC) and under CoT (CNC) relative to Base (Zero‑Shot); exact numerical absolute changes not specified in main text.",
            "experimental_setting": "Models evaluated as 'Base' (no alignment) under Zero‑Shot, Few‑Shot, and CoT prompting; demographic rotation (race/gender) applied to vignettes; similarity of answers measured via sentence embeddings for preference generation in other experiments.",
            "statistical_significance": "Welch's ANOVA reported significant discrepancies with p ≤ 0.05 for the noted tasks/prompts.",
            "uuid": "e7425.0",
            "source_info": {
                "paper_title": "Aligning (Medical) LLMs for (Counterfactual) Fairness",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Closed‑form (Treatment Recommendation, Llama 3‑OpenBioLLM)",
            "name_full": "Closed-ended yes/no treatment-recommendation prompts (imaging/referral) for Llama 3‑OpenBioLLM",
            "brief_description": "Using closed-form yes/no prompts for treatment recommendation enabled token-probability based fairness measurements; Llama 3‑OpenBioLLM showed large demographic disparities in imaging recommendations in this format.",
            "citation_title": "Aligning (Medical) LLMs for (Counterfactual) Fairness",
            "mention_or_use": "use",
            "model_name": "Llama 3‑OpenBioLLM",
            "model_description": "Domain-focused open-source Llama‑3 variant fine-tuned for biomedical/clinical tasks.",
            "model_size": "8B",
            "task_name": "Treatment Recommendation (referral and imaging)",
            "task_description": "Given a short patient-case summary, the model answers whether to refer to specialist and whether to order advanced imaging (CT/MRI/ultrasound); closed-form yes/no responses used to measure demographic impact.",
            "problem_format": "Closed-ended binary JSON-style response (yes/no) designed to allow extraction of token probability of 'yes' or 'no'",
            "format_category": "question type",
            "format_details": "Prompts ask two binary questions with discrete yes/no outputs; authors used token probability of 'yes' to compute maximum differences across demographic rotations for each question.",
            "performance_metric": "Maximum difference in predicted probability (yes/no) across demographic subgroups (per question)",
            "performance_value": "Llama 3‑OpenBioLLM produced &gt;15% probability differences in 6 of 10 imaging-referral questions, with the largest single-question discrepancy being 35% (between demographic subgroups).",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Prompts formulated to elicit closed-form yes/no answers; demographic fields (race/gender) rotated; probabilities derived from token logits.",
            "statistical_significance": "No global statistically significant discrepancies reported for Referral or Imaging at dataset level in text, but large per-question disparities observed (numbers reported as counts and maximums).",
            "uuid": "e7425.1",
            "source_info": {
                "paper_title": "Aligning (Medical) LLMs for (Counterfactual) Fairness",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Likert (Triage, Llama 3‑OpenBioLLM)",
            "name_full": "Likert-scale (1–5) agreement prompts for triage judgments (effect on Llama 3‑OpenBioLLM)",
            "brief_description": "Triage prompts required the model to rate agreement with a statement on a 1–5 Likert scale; Llama 3‑OpenBioLLM showed statistically significant differences in category distributions across demographic groups in this presentation format.",
            "citation_title": "Aligning (Medical) LLMs for (Counterfactual) Fairness",
            "mention_or_use": "use",
            "model_name": "Llama 3‑OpenBioLLM",
            "model_description": "Domain-focused Llama‑3 variant fine-tuned for healthcare/life-sciences use cases.",
            "model_size": "8B",
            "task_name": "Triage (perception and assessment) Likert rating",
            "task_description": "Models are shown triage vignette and asked to rate agreement with a statement on a 1–5 Likert scale (1 strongly disagree to 5 strongly agree); distributions compared across demographics to detect bias.",
            "problem_format": "Likert-scale rating task (ordinal categorical output, 1–5)",
            "format_category": "question type",
            "format_details": "Because outputs are non-binary, the authors run each prompt multiple times to estimate distribution over ratings; stacked bar charts compared proportion of ratings per demographic group; Pearson Chi‑Squared tests applied.",
            "performance_metric": "Distributional differences across Likert categories per demographic group; Pearson Chi‑Squared test for association",
            "performance_value": "Llama 3‑OpenBioLLM exhibited statistically significant biases for the majority of demographic pairs (Pearson Chi‑Squared p ≤ 0.05). Gemma 2 and Meditron also showed significant differences in some comparisons.",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Prompts included randomized [race] and [gender] placeholders; multiple runs per vignette to estimate rating distributions; statistical testing via Pearson Chi‑Squared.",
            "statistical_significance": "Pearson Chi‑Squared reported significant differences (p ≤ 0.05) for Llama 3‑OpenBioLLM across most demographic pairs.",
            "uuid": "e7425.2",
            "source_info": {
                "paper_title": "Aligning (Medical) LLMs for (Counterfactual) Fairness",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Alignment × Prompting (CoT best when aligned)",
            "name_full": "Effect of combining model alignment (preference optimization/LoRA fine-tuning) with prompting style (Zero‑Shot, Few‑Shot, CoT)",
            "brief_description": "After applying the proposed preference‑optimization alignment (SimPO + LoRA) the authors compared aligned models under Zero‑Shot, Few‑Shot and Chain‑of‑Thought prompts and found CoT combined with alignment produced the largest reductions in measured demographic disparities for several models (notably Llama 3.1 and Gemma 2).",
            "citation_title": "Aligning (Medical) LLMs for (Counterfactual) Fairness",
            "mention_or_use": "use",
            "model_name": "Llama 3.1; Gemma 2 (examples emphasized)",
            "model_description": "Llama 3.1 (8B) general-purpose Llama‑3 family model; Gemma 2 (9B) general-purpose open model; both evaluated before and after alignment via SimPO+LoRA.",
            "model_size": "Llama 3.1: 8B; Gemma 2: 9B",
            "task_name": "Q‑Pain, Treatment Recommendation, and Triage (same evaluation suite)",
            "task_description": "Set of clinical vignette QA tasks used to measure counterfactual fairness across demographic rotations; tasks include closed binary and Likert formats.",
            "problem_format": "Compared Base (Zero‑Shot / Few‑Shot / CoT) vs Aligned (Zero‑Shot / Few‑Shot / CoT)",
            "format_category": "prompt style",
            "format_details": "Alignment pipeline: generate preference dataset (teacher answers + demographic red‑teaming), rank candidate responses by semantic similarity to teacher answer, fine‑tune Target LLM via SimPO using LoRA. Evaluations repeated across prompting strategies (Zero/Few/CoT).",
            "performance_metric": "Average maximum probability difference between demographic groups (for binary tasks) and distributional differences for Likert tasks",
            "performance_value": "Alignment reduced observed disparities across all models/tasks; Llama 3.1 and Gemma 2 showed the most pronounced improvements after alignment. CoT prompting generally outperformed Zero‑Shot and Few‑Shot when combined with alignment (numerical reductions shown in figures but specific absolute percentages not enumerated in text).",
            "baseline_performance": "Unaligned Base models under the same prompting strategies",
            "performance_change": "Consistent decrease in maximum demographic difference after alignment vs baseline; for Meditron statistical significance of prior disparities disappeared (no longer significant by Welch's ANOVA). Exact absolute improvements per model/task/prompt are plotted in figures but not fully enumerated in main text.",
            "experimental_setting": "Teacher model: Gemini (commercial) used to produce reference answers; Gecko embeddings for semantic similarity; preference dataset ~1,500 queries from EquityMedQA-derived set; LoRA low‑rank fine‑tuning with SimPO objective.",
            "statistical_significance": "Authors report Meditron 'no longer shows significant biases under Welch's ANOVA' after alignment; other per-model p‑values not explicitly listed in text.",
            "uuid": "e7425.3",
            "source_info": {
                "paper_title": "Aligning (Medical) LLMs for (Counterfactual) Fairness",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Closed vs Non‑binary prompt measurement note",
            "name_full": "Effect of problem presentation (closed binary vs open/non-binary) on measurement methodology and sensitivity",
            "brief_description": "The authors note that closed-ended binary prompts allow direct use of token probability distributions to infer demographic effects with fewer runs, whereas non-binary Likert prompts require multiple runs and distribution estimation; the choice of prompt format therefore affects sensitivity and the measurement pipeline.",
            "citation_title": "Aligning (Medical) LLMs for (Counterfactual) Fairness",
            "mention_or_use": "use",
            "model_name": "applies to all evaluated models (methodological note)",
            "model_description": "Applied to Llama 3.1, Gemma 2, Meditron, Llama 3‑OpenBioLLM in experiments.",
            "model_size": "various (7B–9B as evaluated)",
            "task_name": "Methodological: binary Q‑Pain & Treatment Recommendation vs Likert Triage",
            "task_description": "Shows how prompt output modality (binary vs ordinal) changes how model outputs are captured and statistically tested for demographic disparities.",
            "problem_format": "Binary closed‑form vs Likert ordinal outputs",
            "format_category": "prompt style / output modality",
            "format_details": "For binary closed prompts the authors read token probabilities (single run) to compute differences; for non-binary Likert outputs they ran prompts multiple times to estimate response distribution across categories and applied chi‑squared tests.",
            "performance_metric": "Measurement sensitivity (enables per‑question probability differences vs distributional comparisons)",
            "performance_value": "Binary format permitted direct probability differences (used to report &gt;15% and up to 35% differences in some cases); non-binary format required repeated sampling and distributional tests which revealed significant demographic associations (chi‑squared p ≤ 0.05) for some models.",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Binary prompts: single-run token probability inspection; Likert prompts: repeated runs to recover empirical distribution; statistical tests chosen accordingly (Welch's ANOVA for probability outputs; Pearson Chi‑Squared for Likert distributions).",
            "statistical_significance": null,
            "uuid": "e7425.4",
            "source_info": {
                "paper_title": "Aligning (Medical) LLMs for (Counterfactual) Fairness",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Language models are few‑shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Q‑pain: a question answering dataset to measure social bias in pain management",
            "rating": 2,
            "sanitized_title": "qpain_a_question_answering_dataset_to_measure_social_bias_in_pain_management"
        },
        {
            "paper_title": "A toolbox for surfacing health equity harms and biases in large language models",
            "rating": 2,
            "sanitized_title": "a_toolbox_for_surfacing_health_equity_harms_and_biases_in_large_language_models"
        },
        {
            "paper_title": "Simpo: Simple preference optimization with a reference‑free reward",
            "rating": 1,
            "sanitized_title": "simpo_simple_preference_optimization_with_a_referencefree_reward"
        }
    ],
    "cost": 0.01595725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Aligning (Medical) LLMs for (Counterfactual) Fairness
22 Aug 2024</p>
<p>Raphael Poulain rpoulain@udel.edu 
University of Delaware</p>
<p>Hamed Fayyaz fayyaz@udel.edu 
University of Delaware</p>
<p>Rahmatollah Beheshti 
University of Delaware</p>
<p>Pier Giuseppe Sessa 
University of Delaware</p>
<p>Cassidy Hardin 
University of Delaware</p>
<p>Surya Bhupatiraju 
University of Delaware</p>
<p>Léonard Hussenot 
University of Delaware</p>
<p>Thomas Mesnard 
University of Delaware</p>
<p>Shahri- Ari Bobak 
University of Delaware</p>
<p>Alexandre Ramé 
University of Delaware</p>
<p>Johan Ferret 
University of Delaware</p>
<p>Peter Liu 
University of Delaware</p>
<p>Pouya Tafti 
University of Delaware</p>
<p>Abe Friesen 
University of Delaware</p>
<p>Michelle Casbon 
University of Delaware</p>
<p>Sabela Ramos 
University of Delaware</p>
<p>Ravin Kumar 
University of Delaware</p>
<p>Charline Le Lan 
University of Delaware</p>
<p>Sammy Jerome 
University of Delaware</p>
<p>An- Ton Tsitsulin 
University of Delaware</p>
<p>Nino Vieillard 
University of Delaware</p>
<p>Piotr Stanczyk 
University of Delaware</p>
<p>Sertan Girgin 
University of Delaware</p>
<p>Nikola Momchev 
University of Delaware</p>
<p>Matt Hoffman 
University of Delaware</p>
<p>Shantanu Thakoor 
University of Delaware</p>
<p>Jean-Bastien Grill 
University of Delaware</p>
<p>Behnam Neyshabur 
University of Delaware</p>
<p>Olivier Bachem 
University of Delaware</p>
<p>Alanna Walton 
University of Delaware</p>
<p>Aliaksei Severyn 
University of Delaware</p>
<p>Alicia Par- Rish 
University of Delaware</p>
<p>Aliya Ahmad 
University of Delaware</p>
<p>Allen Hutchison 
University of Delaware</p>
<p>Alvin Abdagic 
University of Delaware</p>
<p>Amanda Carl 
University of Delaware</p>
<p>Amy Shen 
University of Delaware</p>
<p>Andy Brock 
University of Delaware</p>
<p>Andy Coenen 
University of Delaware</p>
<p>Anthony Laforge 
University of Delaware</p>
<p>Antonia Paterson 
University of Delaware</p>
<p>Ben Bastian 
University of Delaware</p>
<p>Bilal Piot 
University of Delaware</p>
<p>Bo Wu 
University of Delaware</p>
<p>Brandon Royal 
University of Delaware</p>
<p>Charlie Chen 
University of Delaware</p>
<p>Chintu Ku- Mar 
University of Delaware</p>
<p>Chris Perry 
University of Delaware</p>
<p>Chris Welty 
University of Delaware</p>
<p>Christopher A Choquette- Choo 
University of Delaware</p>
<p>Danila Sinopalnikov 
University of Delaware</p>
<p>David Weinberger 
University of Delaware</p>
<p>Dimple Vijaykumar 
University of Delaware</p>
<p>Dominika Rogozińska 
University of Delaware</p>
<p>Dustin Herbison 
University of Delaware</p>
<p>Elisa Bandy 
University of Delaware</p>
<p>Emma Wang 
University of Delaware</p>
<p>Eric Noland 
University of Delaware</p>
<p>Erica Moreira 
University of Delaware</p>
<p>Evan Senter 
University of Delaware</p>
<p>Evgenii Eltyshev 
University of Delaware</p>
<p>Francesco Visin 
University of Delaware</p>
<p>Gabriel Rasskin 
University of Delaware</p>
<p>Gary Wei 
University of Delaware</p>
<p>Glenn Cameron 
University of Delaware</p>
<p>Gus Martins 
University of Delaware</p>
<p>Hadi Hashemi 
University of Delaware</p>
<p>Hanna Klimczak-Plucińska 
University of Delaware</p>
<p>Harleen Batra 
University of Delaware</p>
<p>Harsh Dhand 
University of Delaware</p>
<p>Ivan Nardini 
University of Delaware</p>
<p>Jacinda Mein 
University of Delaware</p>
<p>Jack Zhou 
University of Delaware</p>
<p>James Svensson 
University of Delaware</p>
<p>Jeff Stanway 
University of Delaware</p>
<p>Jetha Chan 
University of Delaware</p>
<p>Jin Peng Zhou 
University of Delaware</p>
<p>Joana Carrasqueira 
University of Delaware</p>
<p>Joana Iljazi 
University of Delaware</p>
<p>Jocelyn Becker 
University of Delaware</p>
<p>Joe Fernandez 
University of Delaware</p>
<p>Joost Van Amersfoort 
University of Delaware</p>
<p>Josh Gordon 
University of Delaware</p>
<p>Josh Lipschultz 
University of Delaware</p>
<p>Josh Newlan 
University of Delaware</p>
<p>Ju Yeong Ji 
University of Delaware</p>
<p>Kareem Mohamed 
University of Delaware</p>
<p>Kartikeya Badola 
University of Delaware</p>
<p>Kat Black 
University of Delaware</p>
<p>Katie Milli- Can 
University of Delaware</p>
<p>Keelin Mcdonell 
University of Delaware</p>
<p>Kelvin Nguyen 
University of Delaware</p>
<p>Kiranbir Sodhia 
University of Delaware</p>
<p>Kish Greene 
University of Delaware</p>
<p>Lars Lowe Sjoesund 
University of Delaware</p>
<p>Lauren Usui 
University of Delaware</p>
<p>Laurent Sifre 
University of Delaware</p>
<p>Lena Heuermann 
University of Delaware</p>
<p>Leticia Lago 
University of Delaware</p>
<p>Lilly Mcnealus 
University of Delaware</p>
<p>Baldini Livio 
University of Delaware</p>
<p>Logan Soares 
University of Delaware</p>
<p>Lucas Kilpatrick 
University of Delaware</p>
<p>Luciano Dixon 
University of Delaware</p>
<p>Machel Martins 
University of Delaware</p>
<p>Manvinder Reid 
University of Delaware</p>
<p>Mark Singh 
University of Delaware</p>
<p>Martin Iverson 
University of Delaware</p>
<p>Mat VellosoMateo Görner 
University of Delaware</p>
<p>Matt Wirth 
University of Delaware</p>
<p>Matt Davidow 
University of Delaware</p>
<p>Matthew Miller 
University of Delaware</p>
<p>Matthew Rahtz 
University of Delaware</p>
<p>Meg Watson 
University of Delaware</p>
<p>Mehran Risdal 
University of Delaware</p>
<p>Michael Kazemi 
University of Delaware</p>
<p>Ming Moynihan 
University of Delaware</p>
<p>Minsuk Zhang 
University of Delaware</p>
<p>Minwoo Kahng 
University of Delaware</p>
<p>Mofi Park 
University of Delaware</p>
<p>Mohit Rahman 
University of Delaware</p>
<p>Natalie Khatwani 
University of Delaware</p>
<p>Nenshad Dao 
University of Delaware</p>
<p>Nesh Bardoliwalla 
University of Delaware</p>
<p>Neta Devanathan 
University of Delaware</p>
<p>Nilay Dumai 
University of Delaware</p>
<p>Oscar Chauhan 
University of Delaware</p>
<p>Pankil Wahltinez 
University of Delaware</p>
<p>Parker Botarda 
University of Delaware</p>
<p>Paul Barnes 
University of Delaware</p>
<p>Paul Barham 
University of Delaware</p>
<p>Pengchong Michel 
University of Delaware</p>
<p>Petko Jin 
University of Delaware</p>
<p>Phil Georgiev 
University of Delaware</p>
<p>Pradeep Cul- Liton 
University of Delaware</p>
<p>Ramona Kuppala 
University of Delaware</p>
<p>Ramona Comanescu 
University of Delaware</p>
<p>Reena Merhej 
University of Delaware</p>
<p>Reza Ardeshir Jana 
University of Delaware</p>
<p>Rishabh Rokni 
University of Delaware</p>
<p>Ryan Agarwal 
University of Delaware</p>
<p>Samaneh Mullins 
University of Delaware</p>
<p>Sara Saadat 
University of Delaware</p>
<p>Sarah Mc Carthy 
University of Delaware</p>
<p>Sébastien M R Perrin 
University of Delaware</p>
<p>Sebas- Tian Arnold 
University of Delaware</p>
<p>Shengyang Krause 
University of Delaware</p>
<p>Shruti Dai 
University of Delaware</p>
<p>Shruti Garg 
University of Delaware</p>
<p>Sue Sheth 
University of Delaware</p>
<p>Susan Ronstrom 
University of Delaware</p>
<p>Timothy Chan 
University of Delaware</p>
<p>Ting Jordan 
University of Delaware</p>
<p>Tom Yu 
University of Delaware</p>
<p>Tom Eccles 
University of Delaware</p>
<p>Tomas Hennigan 
University of Delaware</p>
<p>Tulsee Kocisky 
University of Delaware</p>
<p>Vihan Doshi 
University of Delaware</p>
<p>Vikas Jain 
University of Delaware</p>
<p>Vilobh Yadav 
University of Delaware</p>
<p>Vishal Meshram 
University of Delaware</p>
<p>Warren Dharmadhikari 
University of Delaware</p>
<p>Wei Barkley 
University of Delaware</p>
<p>Wenming Wei 
University of Delaware</p>
<p>Woohyun Ye 
University of Delaware</p>
<p>Woosuk Han 
University of Delaware</p>
<p>Xiang Kwon 
University of Delaware</p>
<p>Zhe Xu 
University of Delaware</p>
<p>Zhitao Shen 
University of Delaware</p>
<p>Zichuan Gong 
University of Delaware</p>
<p>Victor Wei 
University of Delaware</p>
<p>Phoebe Cotruta 
University of Delaware</p>
<p>Anand Kirk 
University of Delaware</p>
<p>Minh Rao 
University of Delaware</p>
<p>Ludovic Giang 
University of Delaware</p>
<p>Tris Peran 
University of Delaware</p>
<p>Eli Warkentin 
University of Delaware</p>
<p>Joelle Collins 
University of Delaware</p>
<p>Zoubin Barral 
University of Delaware</p>
<p>Raia Ghahramani 
University of Delaware</p>
<p>Aligning (Medical) LLMs for (Counterfactual) Fairness
22 Aug 20241EF2A803FCCE2778366291275FEE6951arXiv:2408.12055v1[cs.CL]
Large Language Models (LLMs) have emerged as promising solutions for a variety of medical and clinical decision support applications.However, LLMs are often subject to different types of biases, which can lead to unfair treatment of individuals, worsening health disparities, and reducing trust in AI-augmented medical tools.Aiming to address this important issue, in this study, we present a new model alignment approach for aligning LLMs using a preference optimization method within a knowledge distillation framework.Prior to presenting our proposed method, we first use an evaluation framework to conduct a comprehensive (largest to our knowledge) empirical evaluation to reveal the type and nature of existing biases in LLMs used for medical applications.We then offer a bias mitigation technique to reduce the unfair patterns in LLM outputs across different subgroups identified by the protected attributes.We show that our mitigation method is effective in significantly reducing observed biased patterns.Our code is publicly available at https://github.com/healthylaife/FairAlignmentLLM.</p>
<p>I. INTRODUCTION</p>
<p>The burgeoning field of Large Language Models (LLMs) has generated immense excitement across various applications, including in business, education, and healthcare.In healthcare, LLMs hold the promise of revolutionizing clinical decisionmaking processes and enablers of AI generalists [1].With their capacity for natural language understanding, they have shown promise in applications like summarizing medical notes, answering patient questions, and generating discharge letters [2].Their potential to assist in clinical decision support (CDS), through tasks such as disease diagnosis, patient triage, and treatment planning, is particularly noteworthy [3,4].</p>
<p>Despite their potential benefits, the application of LLMs in medicine raises significant concerns about the responsible development and deployment of those, including the concerns about the potential for biased and unfair treatment of individuals [5,6,7,8,9].The high-stakes nature of clinical decisionmaking necessitates a critical examination of the fairness of LLMs in these domains.Existing studies have documented the presence of biases in LLMs, across various medical scenarios and protected groups [10,11,12].</p>
<p>These biases can be sourced from various stages of model development, including the training data, training procedure, and inference mechanism [13].For instance, a common source of biased behavior of LLMs (similar to other ML models [14,15,16]) relates to learning spurious relationships between the protected attributes (e.g., race, ethnicity, gender) and the desired health outcomes, leading to underperformance for his-torically marginalized populations and potentially exacerbating existing health disparities.</p>
<p>In general, methods for mitigating bias patterns in ML models can be categorized into pre-, in-, and post-processing categories [17].For LLMs, a better way [18] of categorizing such methods relates to the same three sources of biases highlighted above and includes: (a) data-related (modifying the original data to reflect/represent less biased data), (b) model parameter-related (changing the parameters of LLMs via gradient-based updates or by adding new regularisation or loss function), and (c) inference-based (modifying the behavior of inference, like the weights or decoding behavior).</p>
<p>Each of these categories of bias mitigation methods has its own limitations.Specifically, both data-related and model parameter-related methods often have scalability issues, as both types generally require access to auxiliary datasets that are hard to collect/generate at a large scale and can be the source of new biases.Model parameter-related methods also often require access to model parameters and extensive computing resources for training.Moreover, inference-based methods face challenges of maintaining output diversity and relying on the performance of auxiliary classification methods.</p>
<p>Focusing on medical LLMs, in this study we present a new model parameter-related approach that aligns the Target LLM (i.e., the LLM that we aim to improve) using a preference optimization method within a knowledge distillation framework.Our approach uses a teacher LLM to create a preference dataset to ensure that the Target (student) LLM's responses, even when prompted with questions containing sensitive attributes, closely match the unbiased answers provided by the teacher model.We demonstrate the effectiveness of this technique through our evaluation framework.</p>
<p>Prior to presenting our mitigation approach, we first conduct a large-scale analysis across multiple clinical datasets and tasks, evaluating a diverse range of general-purpose and clinically focused LLMs.This allows us to rigorously demonstrate the extent of bias in LLMs in various medical applications and pinpoint specific tasks and patient populations at risk.In our empirical analysis, we use a comprehensive framework for evaluating the fairness of LLMs in clinical applications.While several prior studies have followed a similar path to study the bias patterns in medical LLMs, the scale of our empirical analysis (across dimensions of LLM types, protected attributes, and prompting methods) is larger than those.</p>
<p>This way, our contributions can be formulated as follows:</p>
<p>• We present an evaluation framework to conduct a comprehensive evaluation to quantify social biases in LLMs used in medical applications.We extensively analyze multiple LLM types, datasets, and prompting techniques, demonstrating existing fairness challenges in medical LLMs.</p>
<p>• We propose a mitigation technique for fairness concerns using model alignment techniques, in a knowledge distillation framework.We show that our mitigation method is effective in reducing observed biased patterns.</p>
<p>II. COMPREHENSIVE EVALUATION OF BIAS PATTERNS</p>
<p>We first run an extensive series of experiments to show the type of bias patterns that LLMs show when used in medical tasks.In this study, we adopt a common way to conceptualize fairness in ML models (including LLMs), which is through the lens of counterfactual fairness [19].Counterfactual fairness examines how model predictions change when sensitive attributes are altered (for simplicity, we refer to counterfactual fairness as fairness).This way of framing fairness is the basis of the "red-teaming" strategies to examine bias patterns in medical LLMs [20,12].</p>
<p>To comprehensively assess the bias patterns, we follow our comprehensive evaluation framework.Leveraging standardized question-answering (QA) datasets [11,21,10], we employ a red-teaming strategy to systematically rotate patient demographics (e.g., race, ethnicity, gender) within each clinical query, allowing us to construct realistic scenarios.By quantifying discrepancies in LLM responses across these demographic groups for each question, we can identify and measure bias.This approach enables us to identify discrepancies in LLM responses attributable to demographic factors, providing a quantitative measure of bias.Our framework further examines the influence of model architecture and prompting techniques on bias manifestation by analyzing responses across generalpurpose and clinical-focused open-source LLMs, as well as through zero-shot, few-shot, and Chain of Thought prompting.Figure 1 shows our evaluation framework, consisting of three dimensions of datasets (studied scenarios), LLMs, and prompting techniques.</p>
<p>A. Dimension 1: Datasets</p>
<p>To assess and quantify the social biases encoded within LLMs, we leverage clinical QA datasets using vignettes.Clinical vignettes serve as standardized narratives depicting specific patient presentations within the healthcare domain.These narratives typically include a defined set of clinical features and symptoms, with the aim of simulating realistic clinical scenarios for controlled evaluation.Notably, we evaluated social biases in LLMs' answers to clinical questions using vignettes from three angles: pain management [11] through the Q-Pain dataset, treatment recommendations [21], and patient triage [10].To effectively assess the extent to which demographics impact LLMs' responses, we run each vignette multiple times while randomly rotating the vignettes' patient demographics and perform this process for all three datasets.All vignettes are carefully designed such that the studied sensitive attributes (gender and race) are neutral with respect to the outcomes of interest (for example, treatment of appendicitis patients).We briefly present the three datasets in the Appendix.</p>
<p>B. Dimension 2: LLMs</p>
<p>We examine both general-purpose LLMs and open-source clinical LLMs to gain insights into the potential benefits and drawbacks of domain-specific fine-tuning to mitigate bias in clinical settings.</p>
<p>• General-purpose: Llama 3.1 (8B) [22] and Gemma 2 (9B) [23] • Domain-focused: Meditron (7B) [24] and Llama 3-OpenBioLLM [25] (8B) This selection of LLMs, with different architectures and (pre-)training data, allows us to assess the potential benefits of certain architectures and domain-specific fine-tuning for clinical tasks.</p>
<p>C. Dimension 3: Prompting Strategies</p>
<p>Prompting methods can play a pivotal role in enhancing the capabilities of LLMs [26].We investigate different prompting and reasoning techniques to explore how these models engage with complex tasks and queries.Specifically, we use the following three techniques: zero-shot (no prior examples or guidance), few-shot [27] (provides a few examples to guide the LLMs), and Chain of Thought [28], which extends fewshot prompting by providing step-by-step explanations of the answers to enhance the model's reasoning capabilities and further improves the accuracy and interoperability of the LLM's answers.</p>
<p>Since only Q-Pain [11] provides detailed explanations for each sample case, we investigate prompt engineering on this dataset and we employ zero-shot prompting for the other datasets.We provide further details on the prompting process in the Appendix.</p>
<p>D. Bias Evaluation</p>
<p>To quantify potential social biases in LLM responses, we use the following statistical framework.For the Q-Pain (pain management) and treatment recommendation tasks, where LLM outputs were binary (yes/no for medication or referral), we employ a framework centered around the concept of minimax fairness [29], aiming to minimize the maximum disparity between different demographic groups.Rather than focusing on equalizing outcomes across all demographic groups (group fairness), minimax fairness prioritizes the worst-performing group, ensuring that no group population is systematically disadvantaged [30].We report the maximum probability difference between any two demographic groups for each question in the datasets.We report the average maximum differences across all questions.</p>
<p>We also use Welch's ANOVA tests to determine if there are statistically significant discrepancies in the output probabilities.This non-parametric approach is robust to violations of the</p>
<p>E. Results of Empirical Evaluation</p>
<p>We report the observed patterns across the three datasets.To prevent "fairness gerrymandering" [31], we report combined results for gender and race.Additionally, we explored the influence of different prompting techniques on fairness for the Q-Pain dataset.</p>
<p>a) Q-Pain: We first evaluated the impact of the rotating demographics on Q-Pain's vignettes [11] and report the average maximum difference in each question between two subgroups in Figure 3 (blue hues).We compare three prompting techniques, as indicated by: Base (Zero-Shot), Base (Few-Shot), and Base (CoT), where Base refers to the base LLM, without any modification to it.Meditron, a medical LLM, seems to be more sensitive to changes in demographics than other LLMs, on average.Notably, we have found significant discrepancies (under Welch's ANOVA test with a p-value ≤ 0.05) in the CNC (Chronic Non-Cancer) and ANC (Acute Non-Cancer) tasks with Few-Shot Prompting, and in the CNC task with Chain of Thought.This suggests the potential for bias amplification in clinically-tuned models.In addition, some tasks appeared to be more prone to bias than others, which is helpful in identifying potential prejudices in real-world applications.For example, the impact of rotating demographics is larger for the CNC task than for the other tasks for almost all models.</p>
<p>b) Treatment Recommendation: We assessed the biases in the context of treatment recommendations, where given a summary of a patient case, the models were asked whether the patient should be referred to a specialist and whether it was necessary to perform advanced medical imaging.Similar to the Q-Pain dataset, we computed the maximum difference in probabilities for closed-ended responses (yes/no) across demographic subgroups.We report the results in Figure 4 (blue bars).While we found no statistically significant discrepancies between any pairs of demographics at a global level for either the Referral or Imaging, we found differences in the probability outputs of more than 15% in 6 of the 10 imaging referral questions for Llama 3-OpenBioLLM, with the biggest difference being 35%, and 2 out of 10 for both Llama 3.1 and Gemma 2. While Meditron showed the most biased generations in the Q-Pain task, it showed no alarming results on this dataset, with the biggest discrepancy being less than 2%.c) Triage: We have also investigated biases in a task designed to evaluate nurses' perception of patients [10], which is particularly critical in triage.Here, the LLMs were asked about their agreement to a statement given a specific case.The models were specifically asked to answer on a 1-5 Likert scale.We report the results of our experiment on this task in Figure 5 (top row).The stacked bar charts visually depict the proportion of ratings (1-5) assigned to each demographic group.A similar distribution of rating categories for each demographic group would indicate unbiased model behavior.Deviations from this ideal suggest potential biases in the models' assessments.We have found that Llama 3-OpenBioLLM exhibited statistically significant biases for the majority of demographic pairs, as shown by a Pearson Chi-Squared test (Figure 6), with a p-value ≤ 0.05.Additionally, we have found significant differences in the results of Gemma 2 and Meditron.</p>
<p>Clinically fine-tuned models showed the most prevalent biases in almost all tasks, pushing for more scrutiny in the development of such specialized LLMs.These findings highlight the potential for biases in medical decisions based on demographic factors, emphasizing the need for robust fairness evaluation and mitigation.</p>
<p>III. ALIGNING MEDICAL LLMS TO IMPROVE FAIRNESS</p>
<p>To address bias patterns seen in medical LLMs, we propose a novel model alignment technique centered on improving fairness from preference datasets.We leverage a teacher model to serve as a reference point, and guide the Target LLM towards fairer decision-making through a Preference Optimization (PO) method.The teacher model serves as a gold standard, responding to medical queries.Before presenting the detailed steps of our method, we present a very brief background on aligning LLMs.</p>
<p>A. A Short Background on LLM Alignment</p>
<p>The core process to reduce unwanted (such as dangerous or biased) outcomes in pre-trained LLMs involves aligning the LLM outcomes with the desired values.Supervised finetuning (SFT) based on human feedback (the gold standard for alignment) is not scalable and can introduce new biases.Accordingly, alternative approaches are used to 'generalize' from finite human preferences, often in the form of reinforcement learning (RL) approaches.When learning directly from human preferences, the process is commonly referred to as RL from human feedback (RLHF), and when learning from a strong (teacher) LLM to scale human preferences, the process is referred to as RL from AI feedback (RLAIF, a knowledge distillation process).</p>
<p>The most common method to implement the RL framework has been PPO (Proximal Policy Optimization) [32] to work with preferences.Due to the complex nature and instability of PPO-based method, various types of PO algorithms have been presented in the literature, including DPO [33], ORPO [34], and SimPO [35].Per the Bradley-Terry model [36] for converting pairs (of preferences) to Elo scores (i.e., assigning numerical values to the responses), existing PO methods have an objective in the form of:
max π θ E (x,yw,y l )∼D [Ψ(p * (y w &gt; y l |x))] − τ D KL (π θ ||π ref ),
where p * is the human preference, π ref and π θ refer to the 'policy' encoded by the existing and the updated LLMs, respectively; τ is a hyperparameter, (x, y w , y l ) ∈ D is the tuple containing the prompt x, the winning response y w , and the losing response y l drawn from a preference dataset D, and D KL shows the KL divergence.A 'policy' that is being optimized here refers to the 'action' that the LLM takes (i.e., picking winning versus losing).Importantly, replacing Ψ with different functions yields different PO techniques, like, Ψ = Logit() and Ψ = I() yield DPO [33] and IPO [37], respectively.These methods require a reference policy, which can increase the computational requirements.</p>
<p>Simple Preference Optimization (SimPO) offers another variation by introducing a simpler reward function without requiring a reference policy.The SimPO objective is:
a = β |yw| log π θ (y w |x), b = β |y l | log π θ (y l |x), L SimP O (π θ ) = −E (x,yw,y l )∼D [log σ (a − b − γ)] ,(1)
where γ is the target reward margin term, β a tradeoff hyperparameter, and σ is the sigmoid function.By learning directly from preference pairs, SimPO effectively aligns models with desired outcomes.However, constructing fair preference datasets remains a challenge.</p>
<p>B. Proposed Method</p>
<p>At a high level, our proposed method prompts a teacher model with a medical query, generating a reference answer.The same query goes through a Random Prompt Modifier unit, which injects sensitive attributes (e.g., demographics) into the question, acting as a red teaming agent.The Target LLM is then prompted to answer the modified query twice, producing two candidate answers (Answer 1 and 2 in the figure).The candidate answers are ranked based on their closeness in semantics to the reference answer, generating a preference dataset.The preferences are then used to align the LLM through a PO method.Figure 2 (left panel) shows the three steps in our proposed aligning technique, including (1) data generation, (2) preference ranking, and (3) model alignment.</p>
<p>a) Data Generation: First, we introduce a methodology to create a set of counterfactual questions, to be used later in the preference-ranking stage.From a set of neutral (with no information about the patient demographics) questions, we ask a teacher LLM to add demographic information to the question, while keeping the original meaning intact.The sensitive attributes (gender, race, and ethnicity) to be added are randomly chosen from a curated list, that is, not chosen by the LLM itself.This ensures that this random process is truly random and not influenced by the LLM's own potential biases.The teacher model could be a larger, pre-trained LLM, or the Target Model itself within an agentic workflow [38].</p>
<p>This process leaves us with pairs of similar questions.While the presence of demographics in the question differs within the pair, the underlying question remains the same, and the answers should thus be the same in both scenarios.Deviations from this can indicate a violation of counterfactual fairness.The right panel in Figure 2 shows an example scenario.</p>
<p>In medical scenarios, it is pivotal to carefully check the reference queries to ensure that the scenarios have no justifiable differences from one demographic to the other.For example for pain management, the race of the patient should not change their treatment.However, in treating pregnancy complications, males should be excluded due to the biological impossibility.This approach allows for a direct comparison of LLM responses under varying demographic conditions while controlling for the underlying medical query.</p>
<p>b) Preference Ranking: The two generated answers are ranked during the preference ranking phase.We first leverage the teacher model, which acts as a reference point.We start by asking the reference question (the one without sensitive information) from the teacher model and note the resulting answer as the reference answer.In parallel, each modified question (containing explicit referral to sensitive attributes) is presented to the target LLM and asked to answer in two different ways, resulting in two candidate responses.To determine how close in meaning the candidate answers are to the reference answers, we first extract the sentence embeddings produced by a text embeddings model specifically pre-trained for Semantic Textual Similarity [39].Then, we calculate the cosine similarity between the two candidate answers and the reference answer.The response exhibiting the highest  similarity to the teacher's response is then deemed as the winning answer, while the other response is considered as the losing answer.For each medical query x, we then obtain a tuple (x, y w , y l ), where y w is the winning answer and y l is the losing answer.By iterating this process across all queries, we construct a preference dataset D = {(x i , y wi , y li )} N i=1 , where N is the total number of queries.</p>
<p>c) Model Alignment: Finally, we use SimPO [35], as described in Eq. 1, to fine-tune the target LLM based on the newly constructed preference dataset D. To reduce the overall computational constraint of fine-tuning such models and preventing overfitting, we fine-tune the models using Low-Rank Adaptation (LoRA) [40,41].LoRA decomposes the weight matrix update, ∆W , into the product of two smaller matrices, A and B: ∆W = AB, where ∆W ∈ R d×k , A ∈ R d×r , and B ∈ R r×k .Because r ≪ min(d, k), LoRA reduces the number of trainable parameters.By preserving the original weights and introducing low-rank matrices, LoRA accelerates training, reduces memory consumption, and helps prevent catastrophic forgetting.</p>
<p>C. Results of Proposed Alignment Method</p>
<p>To evaluate the effectiveness of our proposed mitigation technique, we fine-tuned the LLMs previously evaluated in our empirical evaluation and applied our evaluation framework.This comparative analysis allows us to quantify our method's impact on reducing bias in LLM outputs.Throughout our experiments, we have used Gemini [42], a commercial LLM, as our teacher model, and Gecko [43], as our text embeddings model for semantic text similarity.Lastly, we used clinical questions derived from the EquityMedQA dataset [44], a collection of seven datasets containing both human-authored by 80 medical experts and AI-generated medical queries, designed to elicit biased responses from LLMs as a basis for our preference dataset.After curating the datasets for our particular use case, we were left with a dataset of about 1,500 queries.</p>
<p>a) Q-Pain: We report our mitigation results on the Q-Pain dataset in Figure 3 (brown hues).Similar to the previously discussed results from the base models, we report the results with our proposed mitigation method with three prompting techniques: Aligned (Zero-Shot), Aligned (Few-Shot), and Aligned (CoT).This process allows us to compare our mitigation technique to the base models as well as the impact of the prompting techniques.</p>
<p>Our results indicate that our proposed alignment technique effectively mitigates bias, as evidenced by reduced maximum differences compared to baseline models.Notably, Meditron no longer shows significant biases under our Welch's ANOVA test for any prompting strategy.Additionally, Llama 3.1 and Gemma 2 exhibit the best improvement when aligned, showcasing minimal discrepancies in the probabilities between subgroups for almost all tasks and prompting techniques.While the magnitude of improvement differs between models and tasks, the overall improvement is consistently observed for all models and tasks.A more granular analysis reveals that the effectiveness of our mitigation method varies across different tasks and model combinations.For instance, Gemma 2 showed its worst improvements on the Post Op (postoperative) task, the same task where Llama 3.1 saw the greatest improvement.Although CoT prompting generally outperforms both Zero-Shot and Few-Shot prompting, the combined use of the alignment and CoT yields the most promising results, advocating for a multi-faceted approach to mitigate bias in clinical LLMs.</p>
<p>b) Treatment Recommendation: As shown in Figure 4 (orange bars), our mitigation technique reduced bias in both imaging and referral tasks within the Treatment Recommendation dataset.This is evidenced by a consistent decrease in maximum difference across all models relative to their base counterparts.Notably, Llama 3-OpenBioLLM and Gemma 2 demonstrated particularly pronounced bias reductions.c) Triage: Consistent with our previous findings, our mitigation technique greatly reduced social biases on this task as shown by the bottom row in Figure 5.This is particularly visible for Llama 3.1, Llama 3-OpenBioLLM and Gemma 2. These baseline models exhibited statistically significant biases during our red-teaming test, as depicted in Figure 6.These biases were effectively mitigated through the application of our proposed method.While our alignment method effectively reduced the statement rating disparities between models, residual differences in rating distributions remain.However, statistical tests indicate these discrepancies are likely attributable to random variation rather than the models' systematic biases.d) Impact of the Teacher Model: To study the degree to which the choice of the teacher model (its power and biases) may impact the performance of our mitigation method, we run a similar series of experiments while having the same Target Model also act as the teacher model (acting as the teacher and student agent).Following such an agentic workflow, for each tested LLM, we applied our mitigation method and then performed our evaluation framework.
&amp; 1 &amp; $ 1 &amp; 3 R V W 2 S 7DVN 0D['LIIHUHQFH 0RGHO *HPPD &amp; 1 &amp; $ 1 &amp; 3 R V W 2 S 7DVN 0RGHO /ODPD &amp; 1 &amp; $ 1 &amp; 3 R V W 2 S 7DVN 0RGHO /ODPD2SHQ%LR//0 &amp; 1 &amp; $ 1 &amp; 3 R V W 2 S 7DVN 0RGHO 0HGLWURQ %DVH=HUR6KRW %DVH)HZ6KRW %DVH&amp;R7 $OLJQHG=HUR6KRW $OLJQHG)HZ6KRW $OLJQHG&amp;R7
We report the results of our experiments in Figures 7, 8, and 9.These results show a consistent ability to reduce observed bias patterns, compared to using a separate stronger teacher model in our method (only slightly lower, but still significant as shown in Figure 10).These experiments also show that mitigating fairness using our knowledge distillation framework can be achieved even when the teacher model has a limited capacity to generate unbiased outcomes (i.e., when the Target model generates biased patterns as a student).</p>
<p>e) Impact on Target LLM's Performance: To evaluate the trade-off between bias reduction and model performance, we assessed the quality of LLM-generated responses on a subset of 500 PubMedQA questions [45].PubMedQA is a popular benchmarking dataset collected from biomedical abstracts indexed on the PubMed platform.As shown in Figure 11, we found no major decrease in the LLMs' performance (accuracy) after using our alignment technique.</p>
<p>IV. RELATED WORK</p>
<p>While our work builds upon a rich body of existing research, here, we focus on two key areas related to our work.</p>
<p>a) Medical LLMs: The emergence of LLMs has precipitated a paradigm shift in numerous domains, including healthcare.General-purpose LLMs (e.g., Claude [46], Llama [22], Gemini [42], and GPT-4), often trained on medical text as well, have been applied to tasks such as generating discharge documents [47], converting clinical narratives into structured data with standard format [48], and education [49].In addition to general-purpose LLMs, specialized models for medical applications are present.For instance, Med-PaLM [50] extends Google's PaLM [51], developed with prompttuning for medical queries, and Palmyra-Med [52] leverages a custom medical dataset.Additionally, Meta's Llama [53] has been adapted in various ways, including Meditron [24] which extends pre-training on a curated medical corpus.While these models show promise, their potential biases remain a critical area of investigation.</p>
<p>Assessing bias in LLMs is crucial for responsible deployment in medical applications.Prior research has employed various methods, including specialized datasets like Q-Pain [11] and comparative studies against human experts [54,12].Similarly, Pfohl et al. [44] proposed a new framework and dataset to assess LLMs' bias and fairness against human ratings and evaluated Med-PaLM on the proposed dataset.Furthermore, Zack et al. [10] evaluated whether GPT-4 encodes racial and gender biases and explored how these biases
<em>HPPD_%DVH /ODPD_%DVH 2SHQ%LR//0_%DVH 0HGLWURQ_%DVH </em>HPPD_$OLJQHG /ODPD_$OLJQHG 2SHQ%LR//0_$OLJQHG 0HGLWURQ_$OLJQHG /LNHUW6FDOH5DWLQJ $ V LD Q ) $ V LD % OD F Q 0 % OD F N ) LV N 0 + S D Q LF ) + LV S D Q LF 0 : K LW H ) : K LW H 0 $ V LD Q ) $ V LD % OD F Q 0 % OD F N ) LV N 0 + S D Q LF ) + LV S D Q LF 0 : K LW H ) : K LW H 0 $ V LD Q ) $ V LD % OD F Q 0 % OD F N ) LV N 0 + S D Q LF ) + LV S D Q LF 0 : K LW H ) : K LW H 0 $ V LD Q ) $ V LD % OD F Q 0 % OD F N ) LV N 0 + S D Q LF ) + LV S D Q LF 0 : K LW H ) : K LW H 0 $ V LD Q ) $ V LD % OD F Q 0 % OD F N ) LV N 0 + S D Q LF ) + LV S D Q LF 0 : K LW H ) : K LW H 0 $ V LD Q ) $ V LD % OD F Q 0 % OD F N ) LV N 0 + S D Q LF ) + LV S D Q LF 0 : K LW H ) : K LW H 0 $ V LD Q ) $ V LD % OD F Q 0 % OD F N ) LV N 0 + S D Q LF ) + LV S D Q LF 0 : K LW H ) : K LW H 0 $ V LD Q ) $ V LD % OD F Q 0 % OD F N ) LV N 0 + S D Q LF ) + LV S D Q LF 0 : K LW H ) : K LW H 0
Fig. 5. Results on the Triage dataset on a Likert Scale.The LLMs were presented with patient summaries and statements and were asked to rate their agreement with the statement.1:Strongly disagree with the statement.5:Strongly agree.</p>
<p>might affect medical education, diagnosis, treatment planning, and patient assessment.Reported findings highlight the potential for biased LLMs to perpetuate stereotypes and lead to inaccurate clinical reasoning [55].While these efforts provide valuable insights, a comprehensive framework is needed to evaluate fairness across diverse LLM applications and mitigate potential biases.b) Model Alignment: Given the limitations of pre-trained LLMs in accurately following human instructions, researchers have explored various techniques to enhance their alignment with human preferences [56].Recent advances in model alignment [33,57,37] have shown promising results in aligning LLMs' generations to human preferences.Although such alignment techniques have been used to improve factuality [58] or reduce toxicity [59], they often overlook the fundamental problem of bias embedded within the models themselves.Specifically, in the clinical domain, a study by Manathunga and Hettigoda [60] used a parameter and dataefficient solution for fine-tuning GPT 3.5 for medical questionanswering.Moreover, Han et.al [61] fine-tuned medical LLMs (Medalpaca [62], and Meditron [24]) on demonstrations of general safety [63], and medical safety.However, these studies have not investigated the issue of counterfactual fairness within medical LLMs</p>
<p>V. DISCUSSION</p>
<p>This study demonstrates the effectiveness of the proposed method in mitigating bias in LLMs applied to clinical tasks.Our proposed bias evaluation framework enabled a rigorous assessment of model performance, revealing consistent reductions in maximum difference across various models and datasets.Our bias evaluation framework and our mitigation technique represent a critical step toward ensuring the fairness and reliability of AI-driven clinical systems.</p>
<p>By aligning the Target LLM within our PO framework, our technique can ensure that the generated responses are consistent and fair when presented with prompts containing sensitive information.This approach is particularly relevant in clinical settings, where unbiased predictions are crucial for accurate diagnosis, treatment planning, and patient care.Furthermore, our technique can be easily integrated into existing clinical LLM workflows, making it a practical and effective solution for mitigating bias in healthcare applications.</p>
<p>While we rank both candidate answers according to their semantic similarity to the reference answer, it is important to note that this does not inherently imply a fairness disparity between the two.Both responses may be considered fair (or unfair), with the winning answer simply aligning closer to the reference answer.Through our extensive experiments, we demonstrate that even without knowing the exact fairer answer, our method can help mitigate fairness concerns.Such an unsupervised approach enables scalable application of our method.</p>
<p>Although our method yielded improvements across the board, some of the model-task combinations seemed to perform better than others, highlighting potential areas to further maximize bias mitigation.Additionally, our findings show the impact of teacher model strength in our bias mitigation approach.A stronger teacher model leads to greater bias reduction and better knowledge preservation than using the Target LLM itself.</p>
<p>Our study is limited in a few ways.First, we focused on counterfactual fairness, which, while relevant to many practical scenarios, does not encompass all facets of bias.Moreover, our method (in its current form) relies on opensource LLMs with access to model parameters, which might be restrictive in working with closed-source models.However, we expect a similar method where the Target LLM is kept frozen and alignment is performed through soft prompting layers would achieve comparable results.Lastly, while we focus on addressing fairness concerns within computational settings, real-world implementation requires careful consideration of ethical implications and potential unintended consequences beyond the scope of this study.</p>
<p>APPENDIX</p>
<p>A. Prompting Strategies</p>
<p>In this study, we have examined how zero-shot, few-shot, and Chain of Thought prompting methods affect LLMs and their potential biases in healthcare applications.</p>
<p>a) Zero-shot: Zero-shot prompting is a common prompting approach for guiding large language models (LLMs) on new tasks.It involves providing the LLM with clear instructions and a brief prompt, rather than extensive additional data.The prompt sets the context and desired outcome for the LLM, allowing it to leverage its existing knowledge and understanding of language to complete the task.While not as powerful as tailored prompting techniques, zero-shot prompting offers a convenient way to expand the capabilities of LLMs without a heavy investment in data or training time.</p>
<p>b) Few-shot: Few-shot prompting is a technique that builds upon zero-shot prompting for guiding large language models (LLMs) on new tasks.While zero-shot prompting relies solely on clear instructions and a brief prompt, few-shot prompting goes a step further.It provides the LLM with a few real-world examples alongside the prompt.These examples help the LLM grasp the nuances of the task and improve its performance compared to zero-shot prompting.While requiring slightly more data than zero-shot, few-shot prompting offers a good balance between efficiency and effectiveness, making it a popular choice for various LLM applications.c) Chain of Thought: Chain-of-thought (CoT) prompting is an advanced prompting technique that aims to improve the reasoning capabilities of large language models (LLMs).Unlike zero-shot or few-shot prompting, which focus on providing instructions and examples, CoT prompting encourages the LLM to explain its thought process.It achieves this by incorporating prompts that ask the LLM to break down its reasoning steps into a series of logical explanations.While CoT prompting offers significant advantages, it can also be computationally more expensive compared to simpler prompting techniques.However, its ability to improve transparency and potentially accuracy makes it a valuable tool for tasks where understanding the LLM's reasoning is crucial.</p>
<p>B. Tasks and Prompt Examples</p>
<p>a) Q-Pain: Q-Pain is a vignette-based dataset for assessing bias in medical QA in the context of pain management [11].The dataset is split into 5 different medical contexts related to pain management.In all cases, we asked the models whether the patients should receive pain management medication or not.Because this is a closed question, we can use the probability distribution of each word in the sequence to infer the impact of the demographics on the probability of answering no to the question, that is, refusing to administrate a pain medication.This allows us to directly evaluate the impact of the demographic changes without the need to run the experiments a significant amount of times.Here, we report sample prompts for the three different prompting techniques using the Q-Pain dataset.</p>
<p>Zero  [10] proposed a vignette-based dataset containing 8 different challenging cases to simulate a triage scenario where the LLMs would need to make decisions based on their perception of the patients' pain and illness severity.The LLMs would then be shown a statement related to the case and asked about their agreement with the statement on a Likert scale (from 1 to 5, where 1 means the model strongly disagrees and 5 means the model strongly agrees).Each of the 22 questions relates to one of the following five categories: perception of patient dishonesty, perception of patient understanding, perception of relationships, treatment decisions regarding pain, and other treatment decisions.Because the questions are non-binary by default, we cannot use the probabilities to infer the true token probability distribution.Therefore, we run each prompt multiple times and infer the underlying distribution from the results.As an example, we report a sample prompt used to query the LLMs on this task: You will be given a situation and a statement.You will then be asked to rate your agreement with the statement on a scale of 1 to 5, where 1 is strongly disagree and 5 is strongly agree.Situation: A 48-year-old [race] [gender] presents to the emergency department with abdominal pain and 4 days of constipation.Workup reveals a mass in [subject] right colon for which [subject] undergoes a hemicolectomy.The operation is uncomplicated and, after awakening from anesthesia, the patient is sent to recover on the surgical floor.You assess the patient on postoperative day 1 and the patient reports extreme unbearable pain, saying that [subject] did not sleep at all through the night.However, it was reported that [subject] did sleep intermittently through the night and whenever [subject] woke up all [subject] asked for is "stronger pain medicine."Statement: This patient is exaggerating their level of pain. 1. Strongly Disagree 2. Disagree 3. Neutral 4. Agree 5. Strongly Agree Select the option that best describes your decision.c) Treatment Recommendation: Additionally, we use the vignettes from NEJM Healer [21] presented for evaluating social bias in treatment recommendation [10].In this task, the models would be given a short summary of a case and would then be asked whether the patient would need to be referred to a specialist or if any advanced medical imaging was needed, as defined by CT scans, MRIs, or abdominal ultrasounds.Similar to the method used for Q-Pain, we design the questions to be of closed form so we can leverage the tokens' probabilities to evaluate the differences in the likelihood of answering yes to the questions with the rotating demographics.As an example, we report a sample prompt used to query the LLMs on this task:</p>
<p>You are a master diagnostician with extensive clinical expertise and knowledge.I will present a very brief summary of the case and I would like you to produce the following
V L D Q ) $ V L D Q 0 % O D F N ) % O D F N 0 + L V S D Q L F ) + L V S D Q L F 0 : K L W H ) : K L W H 0 'HPRJUDSKLFV $ V L D Q ) $ V L D Q 0 % O D F N ) % O D F N 0 + L V S D Q L F ) + L V S D Q L F 0 : K L W H ) : K L W H 0 'HPRJUDSKLFV $ V L D Q ) $ V L D Q 0 % O D F N ) % O D F N 0 + L V S D Q L F ) + L V S D Q L F 0 : K L W H ) : K L W H 0 'HPRJUDSKLFV $ V L D Q ) $ V L D Q 0 % O D F N ) % O D F N 0 + L V S D Q L F ) + L V S D Q L F 0 : K L W
Fig. 2 .
2
Fig. 2. Left panel: Proposed pipeline for fairness-aware model alignment in three steps.The non-shaded yellow-blocks area describes the dataset generation process, and the blue hatched area is the preference ranking step, where both candidate answers are compared to the reference answer.The red arrow denotes the alignment process through Preference Optimization (PO).Right panel: Example of the generation process for the preference dataset.</p>
<p>Fig. 3 .Fig. 4 .
34
Fig. 3. Results on the Q-Pain dataset.The bars represent the average maximum difference probability of denying the pain treatment between two subgroups for each question.The error bars show the standard deviation.CNC: Chronic Non Cancer, ANC: Acute Non Cancer, Post Op: Postoperative</p>
<p>Fig. 9 .Fig. 10 G e m m a 2 LFig. 11 .
910211
Fig.9.Results on the Triage dataset on a Likert Scale when the Target LLM acts as its own teacher.The LLMs were presented with patient summaries and statements and were asked to rate their agreement with the statement.1:Strongly disagree with the statement.5:Strongly agree.</p>
<p>1) Would you recommend this patient to a specialist?Say yes only if there is an urgent need 2) Would you recommend this patient for advanced medical imaging (CT, MRI, or abdominal ultrasound)?Enter your response in a json format as "Specialist Recommendation":true/false, "Advanced Medical Imaging Recommendation":true/false.Here is the case summary: A 54-year-old obese [race] [gender] presents with recurrent severe, burning, central, non-exertional, chest pain that is worse supine and radiates to [subject] back and neck.
*HPPD_%DVH/ODPD <em>%DVH/ODPD 2SHQ%LR//0</em>%DVH0HGLWURQ_%DVH$
VI. ACKNOWLEDGEMENTSOur study was partially supported by the NIH award U54-GM104941 and a computing credit award from Amazon Web Services (AWS).
Towards generalist biomedical ai. Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, 10.1056/aioa2300138NEJM AI. 2836-9386132024</p>
<p>Clinical text summarization: Adapting large language models can outperform human experts. Dave Van Veen, Cara Van Uden, Louis Blankemeier, Jean-Benoit Delbrouck, Asad Aali, Christian Bluethgen, Anuj Pareek, Malgorzata Polacin, Eduardo Pontes Reis, Anna Seehofnerova, 2023Research Square</p>
<p>Leveraging Large Language Models for Decision Support in Personalized Oncology. Manuela Benary, David Xing, Max Wang, Dominik Schmidt, Georg Soll, Mani Hilfenhaus, Christian Nassir, Maren Sigler, Ulrich Knödler, Dieter Keller, Ulrich Beule, Ulf Keilholz, Damian T Leser, Rieke, 10.1001/jamanetworkopen.2023.43689JAMA Network Open. 2574-380561111 2023</p>
<p>Foundation models for generalist medical artificial intelligence. Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein, Harlan M Abad, Jure Krumholz, Eric J Leskovec, Pranav Topol, Rajpurkar, Nature. 61679562023</p>
<p>Foundational Principles of Ophthalmic Imaging, DC Algorithmic Interpretation Working Group of the Collaborative Community for Ophthalmic Imaging Foundation, Washington, and William H Maisel. Considerations for addressing bias in artificial intelligence for health equity. Michelle E Michael D Abràmoff, Nilsa Tarver, Sylvia Loyo-Berrios, Danton Trujillo, Ziad Char, Malvina B Obermeyer, Eydelman, NPJ digital medicine. 611702023</p>
<p>Sources of bias in artificial intelligence that perpetuate healthcare disparities-a global review. Leo Anthony Celi, Jacqueline Cellini, Marie-Laure Charpignon, Edward Christopher Dee, Franck Dernoncourt, Rene Eber, William Greig Mitchell, Lama Moukheiber, Julian Schirmer, Julia Situ, Joseph Paguio, Joel Park, Judy Gichoya Wawira, Seth Yao, Data, 10.1371/journal.pdig.0000022PLOS Digital Health. 132022</p>
<p>Artificial intelligence and health inequities in primary care: a systematic scoping review and framework. Mark Alexander D'elia, Sarah Gabbay, Ciara Rodgers, Elisa Kierans, Irum Jones, Adele Durrani, Lucy Thomas, Frith, 10.1136/fmch-2022-001670Family Medicine and Community Health. 2305-6983102022Suppl 1</p>
<p>Treating health disparities with artificial intelligence. Shalmali Irene Y Chen, Marzyeh Joshi, Ghassemi, Nature medicine. 2612020</p>
<p>Bias in ai-based models for medical applications: challenges and mitigation strategies. Mirja Mittermaier, Joseph C Marium M Raza, Kvedar, Digital Medicine. 611132023</p>
<p>Assessing the potential of gpt-4 to perpetuate racial and gender biases in health care: a model evaluation study. The Lancet Digital Health. Travis Zack, Eric Lehman, Mirac Suzgun, Jorge A Rodriguez, Leo Anthony Celi, Judy Gichoya, Dan Jurafsky, Peter Szolovits, David W Bates, Raja-Elie E Abdulnour, 20246</p>
<p>Q-pain: a question answering dataset to measure social bias in pain management. Cécile Logé, Emily Ross, David Yaw, Amoah Dadey, Saahil Jain, Adriel Saporta, Andrew Y Ng, Pranav Rajpurkar, arXiv:2108.017642021arXiv preprint</p>
<p>Large language models propagate race-based medicine. Jenna C Jesutofunmi A Omiye, Simon Lester, Veronica Spichak, Roxana Rotemberg, Daneshjou, NPJ Digital Medicine. 611952023</p>
<p>Tackling bias in pre-trained language models: Current trends and under-represented societies. Gillian Vithya Yogarajan, Te Dobbie, Rostam J Taka Keegan, Neuwirth, 2023</p>
<p>Associations of longitudinal bmi percentile classification patterns in early childhood with neighborhood-level social determinants of health. Mehak Gupta, T Thao-Ly, Félice Phan, Daniel Lê-Scherban, H Timothy Eckrich, Rahmatollah Bunnell, Beheshti, 10.1101/2023.06.08.23291145medRxiv. 2023</p>
<p>Improving fairness in ai models on electronic health records: The case for federated learning methods. Raphael Poulain, Mirza Farhan Bin, Rahmatollah Tarek, Beheshti, 10.1145/3593013.3594102Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, FAccT '23. the 2023 ACM Conference on Fairness, Accountability, and Transparency, FAccT '23New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Graph transformers on EHRs: Better representation improves downstream performance. Raphael Poulain, Rahmatollah Beheshti, The Twelfth International Conference on Learning Representations. 2024</p>
<p>A Survey on Bias and Fairness in Machine Learning. Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, Aram Galstyan, 10.1145/3457607ACM Comput. Surv. 0360-03005462021</p>
<p>Tackling bias in pre-trained language models: Current trends and under-represented societies. Gillian Vithya Yogarajan, Te Dobbie, Rostam J Taka Keegan, Neuwirth, 2023</p>
<p>Counterfactual fairness. Advances in neural information processing systems. Matt J Kusner, Joshua Loftus, Chris Russell, Ricardo Silva, 201730</p>
<p>Red teaming large language models in medicine: Real-world insights on model behavior. medRxiv. Crystal Tin-Tin, Hodan Chang, Haiwen Farah, Gui, Justin Shawheen, Charbel Rezaei, Ye-Jean Bou-Khalil, Akshay Park, Swaminathan, Akaash Jesutofunmi A Omiye, Akash Kolluri, Chaurasia, 2024</p>
<p>Deliberate practice at the virtual bedside to improve clinical reasoning. E Raja-Elie, Andrew S Abdulnour, Daniel Parsons, Jeffrey Muller, Eric J Drazen, Joseph Rubin, Rencic, 10.1056/nejme2204540New England Journal of Medicine. 0028-4793386202022</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Gemma 2: Improving open language models at a practical size. Gemma Team, Morgane Riviere, Shreya Pathak, D Hadsell, Jeanine Sculley, Anca Banks, Slav Dragan, Oriol Petrov, Jeff Vinyals, Demis Dean, Koray Hassabis, Clement Kavukcuoglu, Elena Farabet, Sebastian Buchatskaya, Noah Borgeaud, Armand Fiedel, Kathleen Joulin, Robert Kenealy, Alek Dadashi, Andreev, 2024</p>
<p>Meditron-70b: Scaling medical pretraining for large language models. Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, arXiv:2311.160792023arXiv preprint</p>
<p>Openbiollms: Advancing open-source large language models for healthcare and life sciences. Malaikannan Sankarasubbu, Ankit Pal, 2024</p>
<p>Efficient prompting methods for large language models: A survey. Kaiyan Chang, Songcheng Xu, Chenglong Wang, Yingfeng Luo, Xiao Tong, Jingbo Zhu, 2024</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Minimax group fairness: Algorithms and experiments. Emily Diana, Wesley Gill, Michael Kearns, Krishnaram Kenthapadi, Aaron Roth, 10.1145/3461702.3462523Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, AIES '21. the 2021 AAAI/ACM Conference on AI, Ethics, and Society, AIES '21New York, NY, USAAssociation for Computing Machinery2021</p>
<p>Minimax pareto fairness: A multi objective perspective. Natalia Martinez, Martin Bertran, Guillermo Sapiro, International conference on machine learning. PMLR2020</p>
<p>Preventing fairness gerrymandering: Auditing and learning for subgroup fairness. Michael Kearns, Seth Neel, Aaron Roth, Zhiwei Steven Wu, Proceedings of the 35th International Conference on Machine Learning. Jennifer Dy, Andreas Krause, the 35th International Conference on Machine LearningPMLR10-15 Jul 201880Proceedings of Machine Learning Research</p>
<p>Proximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, 2017</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, Chelsea Finn, 2023</p>
<p>Orpo: Monolithic preference optimization without reference model. Jiwoo Hong, Noah Lee, James Thorne, 2024</p>
<p>Simpo: Simple preference optimization with a reference-free reward. Yu Meng, Mengzhou Xia, Danqi Chen, arXiv:2405.147342024arXiv preprint</p>
<p>Rank analysis of incomplete block designs: I. the method of paired comparisons. Ralph Allan, Bradley , Milton E Terry, Biometrika. 393/41952</p>
<p>A general theoretical paradigm to understand learning from human preferences. Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, Rémi Munos, 2023</p>
<p>A survey on llm-based agents: Common workflows and reusable llm-profiled components. Xinzhe Li, 2024</p>
<p>Evolution of semantic similarity-a survey. Dhivya Chandrasekaran, Vijay Mago, 10.1145/3440755ACM Computing Surveys. 1557-7341542February 2021</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>Yang Lin, Xinyu Ma, Xu Chu, Yujie Jin, Zhibang Yang, Yasha Wang, Hong Mei, arXiv:2404.09610Lora dropout as a sparsity regularizer for overfitting control. 2024arXiv preprint</p>
<p>Gemini: A family of highly capable multimodal models. Gemini Team, 2024</p>
<p>Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher, Karthik Duddu, Gustavo Hernandez Abrego, Weiqiang Shi, Nithi Gupta, Aditya Kusupati, Prateek Jain, Siddhartha Reddy Jonnalagadda, Ming-Wei Chang, Iftekhar Naim, Versatile text embeddings distilled from large language models. Gecko2024</p>
<p>Stephen R Pfohl, Heather Cole-Lewis, Rory Sayres, Darlene Neal, Mercy Asiedu, Awa Dieng, Nenad Tomasev, Mamunur Qazi, Shekoofeh Rashid, Negar Azizi, Liam G Rostamzadeh, Leo Anthony Mccoy, Yun Celi, Mike Liu, Alanna Schaekermann, Alicia Walton, Chirag Parrish, Preeti Nagpal, Akeiylah Singh, Philip Dewitt, Sushant Mansfield, Katherine Prakash, Alan Heller, Christopher Karthikesalingam, Semturs, Greg Corrado, Yossi Matias, Jamila Smith-Loud, Ivor Horn, and Karan Singhal. A toolbox for surfacing health equity harms and biases in large language models. 2024</p>
<p>Pubmedqa: A dataset for biomedical research question answering. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, Xinghua Lu, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)2019</p>
<p>. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado ; Tom Henighan, Tristan Hume, Samuel R Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mccandlish, Tom Brown, Jared Kaplan, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly,2022Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott JohnstonConstitutional ai: Harmlessness from ai feedback</p>
<p>Chatgpt-4 generates orthopedic discharge documents faster than humans maintaining comparable quality: a pilot study of 6 cases. Guillermo Sanchez Rosenberg, Martin Magnéli, Niklas Barle, Andreas Marc Michael G Kontakis, Matthias Müller, Max Wittauer, Cyrus Gordon, Brodén, Acta Orthopaedica. 951522024</p>
<p>Fhir-gpt enhances health interoperability with large language models. Yikuan Li, Hanyin Wang, Yoshihisa Halid Z Yerebakan, Yuan Shinagawa, Luo, NEJM AI. 23003012024</p>
<p>Nur Azlina Mohamed Mokmin and Nurul Anwar Ibrahim. The evaluation of chatbot as a tool for health literacy education among undergraduate students. Education and Information Technologies. 202126</p>
<p>Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Nature. 62079722023</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, Yaguang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham ; Bryan, Parker Richter, Alex Riley, Aurko Castro Ros, Brennan Roy, Rajkumar Saeta, Renee Samuel, Ambrose Shelby, Daniel Slone, David R Smilkov, Daniel So, Simon Sohn, Dasha Tokumine, Vijay Valter, Kiran Vasudevan, Xuezhi Vodrahalli, Pidong Wang, Zirui Wang, Tao Wang, John Wang, Yuhuai Wieting, Kelvin Wu, Yunhan Xu, Linting Xu, Pengcheng Xue, Jiahui Yin, Qiao Yu, Steven Zhang, Ce Zheng, Weikang Zheng, Denny Zhou, Zhou, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,2023Slav Petrovand Yonghui Wu. Palm 2 technical report</p>
<p>Writer Engineering team. Palmyra-Large Parameter Autoregressive Language Model. 2023</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>The accuracy and potential racial and ethnic biases of gpt-4 in the diagnosis and triage of health conditions: Evaluation study. Naoki Ito, Sakina Kadomatsu, Mineto Fujisawa, Kiyomitsu Fukaguchi, Ryo Ishizawa, Naoki Kanda, Daisuke Kasugai, Mikio Nakajima, Tadahiro Goto, Yusuke Tsugawa, JMIR Medical Education. 9e475322023</p>
<p>Hamed Fayyaz, and Rahmatollah Beheshti. Bias patterns in the application of llms for clinical decision support: A comprehensive study. Raphael Poulain, arXiv:2404.151492024arXiv preprint</p>
<p>Aligning large language models with human: A survey. Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, Qun Liu, arXiv:2307.129662023arXiv preprint</p>
<p>Kto: Model alignment as prospect theoretic optimization. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, Douwe Kiela, 2024</p>
<p>Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D Manning, Chelsea Finn, arXiv:2311.08401Fine-tuning language models for factuality. 2023arXiv preprint</p>
<p>Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan K Kummerfeld, Rada Mihalcea, arXiv:2401.01967A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity. 2024arXiv preprint</p>
<p>Aligning large language models for clinical tasks. Supun Manathunga, Isuru Hettigoda, arXiv:2309.028842023arXiv preprint</p>
<p>Towards safe large language models for medicine. Tessa Han, Aounon Kumar, Chirag Agarwal, Himabindu Lakkaraju, ICML 2024 Workshop on Models of Human Feedback for AI Alignment. 2024</p>
<p>Medalpaca-an open-source collection of medical conversational ai models and training data. Tianyu Han, Lisa C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander Löser, Daniel Truhn, Keno K Bressem, arXiv:2304.082472023arXiv preprint</p>
<p>Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions. Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Röttger, Dan Jurafsky, Tatsunori Hashimoto, James Zou, arXiv:2309.078752023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
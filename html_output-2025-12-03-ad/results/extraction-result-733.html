<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-733 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-733</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-733</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-dc030c2e55b266c029356a54bb444b7d9b1f2abc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/dc030c2e55b266c029356a54bb444b7d9b1f2abc" target="_blank">StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow</a></p>
                <p><strong>Paper Venue:</strong> The Web Conference</p>
                <p><strong>Paper TL;DR:</strong> This paper investigates a new problem of systematically mining question-code pairs from Stack Overflow and proposes a novel Bi-View Hierarchical Neural Network which can capture both the programming content and the textual context of a code snippet to make a prediction.</p>
                <p><strong>Paper Abstract:</strong> Stack Overflow (SO) has been a great source of natural language questions and their code solutions (i.e., question-code pairs), which are critical for many tasks including code retrieval and annotation. In most existing research, question-code pairs were collected heuristically and tend to have low quality. In this paper, we investigate a new problem of systematically mining question-code pairs from Stack Overflow (in contrast to heuristically collecting them). It is formulated as predicting whether or not a code snippet is a standalone solution to a question. We propose a novel Bi-View Hierarchical Neural Network which can capture both the programming content and the textual context of a code snippet (i.e., two views) to make a prediction. On two manually annotated datasets in Python and SQL domain, our framework substantially outperforms heuristic methods with at least 15% higher F1 and accuracy. Furthermore, we present StaQC (Stack Overflow Question-Code pairs), the largest dataset to date of ~148K Python and ~120K SQL question-code pairs, automatically mined from SO using our framework. Under various case studies, we demonstrate that StaQC can greatly help develop data-hungry models for associating natural language with programming language</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e733.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e733.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heuristic pairing mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Heuristic question-code pairing mismatch (Select-First / Select-All / concat)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mismatch caused by heuristic methods that pair natural language question titles with code snippets in multi-code answer posts, producing low-precision or low-recall QC pairs because many snippets are not standalone solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>StaQC mining framework (heuristic baselines vs. BiV-HNN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline for mining question-code (QC) pairs from Stack Overflow; compares naive heuristics (Select-First, Select-All) against learned models (BiV-HNN and variants) to identify standalone code solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>Stack Overflow question title / answer post text</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Stack Overflow answer code snippets (Python, SQL)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete/incorrect pairing (low precision / low recall)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Heuristic rules that pair a question title with either the first code block, every code block, or a concatenation of all code blocks fail because answer posts often contain multiple code snippets playing different roles (standalone solution, input-output demo, partial step, reminder). These heuristics therefore (a) include non-solution snippets as solutions (low precision) and/or (b) miss actual standalone solutions that are not the first snippet (low recall).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data collection / dataset construction (pairing natural language to code)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical comparison against manually annotated ground truth (human annotation of per-snippet labels) and quantitative evaluation of heuristics vs. learned models using precision/recall/F1/accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Standard binary-class metrics: precision, recall, F1, accuracy computed on held-out human-annotated test sets; explicit comparison of heuristics' metrics to BiV-HNN. Example numbers: Select-First Python F1=0.607, accuracy=0.663; Select-All Python F1=0.642, accuracy=0.472; BiV-HNN Python F1=0.841, accuracy=0.843.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Heuristics produced much lower-quality QC pairs that would degrade downstream models; quantified as >15% absolute lower F1/accuracy compared to BiV-HNN in the paper's experiments (e.g., ~0.24 absolute accuracy gap between Select-All and BiV-HNN in Python). Poor pairing reduces usefulness of mined datasets for training NL↔code models.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Very common in the studied corpora: 44.66% of Python and 34.35% of SQL accepted 'how-to-do-it' answer posts contain more than one code snippet, making heuristics frequently mispair.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Simplistic heuristics that assume a one-to-one mapping between question and a single code block; answer posts are composite and code snippets can serve multiple non-solution roles; accepted-answer-level verification does not provide per-snippet labels.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Train a learned classifier (Bi-View Hierarchical Neural Network, BiV-HNN) that uses both textual context and code content to predict whether each snippet is a standalone solution; combine multiple model variants and consensus heuristics to improve precision.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Substantial improvements: BiV-HNN outperformed heuristics by >15% absolute in F1/accuracy; combined model (agreement of three models) labeled 69.2% of Python code blocks with F1=0.916 and accuracy=0.911, and 78.7% of SQL blocks with F1=0.943 and accuracy=0.926.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Natural language + code dataset mining / machine learning for code</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e733.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e733.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Label granularity gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accepted-answer vs per-snippet label granularity mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The natural language verification (accepted answer check) exists at the answer-post level, not per code snippet, so datasets constructed from accepted answers lack per-snippet ground-truth labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>StaQC mining framework (annotation and model training)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mining system relies on per-snippet binary labels (standalone solution vs not) but Stack Overflow only provides accepted-answer signals at the post level; addresses this by manual annotation and learned prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>Stack Overflow accepted-answer indicator (post-level)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>multiple code snippets within one accepted answer post</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / missing labels</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Stack Overflow's accepted-answer flag verifies an entire answer post but does not indicate which of multiple embedded code snippets are actual standalone solutions. This prevents direct reliable pairing of natural language questions to individual snippets without additional labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>dataset labeling / ground-truth availability</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Observation of Stack Overflow metadata (accepted answer flag) combined with manual annotation effort showing need for per-snippet labels; empirical annotation process (two annotators per snippet, Cohen's kappa reported).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Annotation statistics: around 85% of code snippets were labeled by annotators; inter-annotator agreement measured by Cohen's kappa (≈0.658 Python, 0.691 SQL). Proportion of multi-snippet accepted answers: 44.66% Python, 34.45% SQL.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Without per-snippet labels, heuristics are forced to make incorrect assumptions leading to noisy training data; manual annotation and learned labeling improved dataset quality and downstream model performance (e.g., StaQC yields improved CODE-NN MRR).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Highly prevalent because many accepted answers contain multiple code blocks (see percentages above); majority of accepted answers cannot be unambiguously used for QC pairing without per-snippet labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Stack Overflow's accepted-answer mechanism is coarse-grained; authors of answers include multiple snippets with diverse roles without explicit markup of role.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Human annotation (dual-annotator with agreement threshold) to create labeled training data; train predictive models (BiV-HNN) to infer per-snippet labels; combine models for higher-confidence automated labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Manual annotation provided training/validation/test sets (thousands of examples) enabling BiV-HNN to achieve F1 up to 0.841 Python and 0.888 SQL; enabled creation of large-scale StaQC (147,546 Python, 119,519 SQL pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Data curation for ML / software engineering corpora</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e733.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e733.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Non-solution snippet roles</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-solution roles of code snippets (input-output demo, reminder, partial step)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Code snippets in answer posts often play roles other than being a standalone solution: demonstrations of input/output, small examples, reminders, or steps of a larger multi-step solution, creating a semantics mismatch with natural language question descriptions when heuristically paired.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>StaQC mining framework / BiV-HNN classifier</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The classifier must distinguish code snippets that are true standalone solutions from those that are examples, demos, or fragments; uses textual context and code content to infer role.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>Answer post textual context (surrounding sentences)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Code snippet fragments (examples, demos, partial implementations)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous role / incomplete specification</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>A code snippet may be runnable but not constitute a full solution (e.g., C2 in Figure 1 is an input-output demo showing expected I/O; C4 is a reminder). Simple matching between question and snippet can therefore misclassify demos or partial steps as full solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>interpretation of code snippet role within answer post</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual annotation of per-snippet labels; model error analysis showing false positives where partial snippets were predicted as solutions; qualitative examples from posts.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Annotation labels (binary) and classifier error statistics: more than half of model mistakes were false positives (predicting non-solution as solution). Evaluation metrics include per-class precision/recall and overall F1; examples include BiV-HNN precision=0.808, recall=0.876 (Python).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>False positives introduce noisy QC pairs into dataset, harming downstream models; error analysis showed correcting false positives often requires broader context (entire post), and current per-snippet independent modeling can mislabel fragments as full solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common — multi-role snippets are frequent in multi-code posts (44.66% Python, 34.35% SQL); among model errors, >50% were false positives linked to these non-solution roles.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Answer authors include multiple snippets with different communicative intents; natural language cues for role may be dispersed across the post and not present in the title; heuristics and per-snippet-local models lack global post-level reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Modeling both textual context and code content (BiV-HNN) to capture signals such as surrounding sentences that introduce or comment on snippets; consider combining models and using consensus to increase precision; future work suggests modeling sequences of snippets jointly to capture multi-step solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>BiV-HNN reduced false-positive errors relative to heuristics (quantified improvements in F1 and accuracy). Combined-model consensus achieved much higher precision/F1 on the subset it labels (e.g., Python F1=0.916 on 69.2% of blocks). However, residual false positives remain, and authors identify joint-sequence modeling as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Software engineering / dataset curation for NL↔code</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e733.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e733.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parsability/runability gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parsability/runability mismatch of code snippets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large fraction of Stack Overflow code snippets are not directly parsable or runnable, so verifying snippet correctness via execution is often infeasible, causing reliance on semantic and contextual methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>StaQC mining framework (dataset verification considerations)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>When mining QC pairs, the system cannot rely on executing snippets for validation because many snippets are incomplete or contain interactive console markers; must use language+code analysis instead.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>Question title and surrounding answer text</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>incomplete / interactive / partial code snippets (Python, SQL)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>implementation incompleteness (non-runnable / unparsable code)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Prior work reports that around 74% of Python and 88% of SQL snippets on SO are not directly parsable or runnable; hence automatic verification by running code is not a reliable method to determine whether a snippet is a standalone solution.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>validation step (runnable-execution-based verification)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Reference to prior analyses of Stack Overflow snippet parsability and the authors' observation that executing snippets is infeasible; use of AST-based token replacement to regularize code but not to execute.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Quoted statistics from prior work: ~74% of Python and ~88% of SQL snippets not directly parsable/runnable (cited prior studies [21,59] in paper). The authors additionally applied tokenization and AST-based replacement to ease sparsity but did not run most snippets.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Prevents use of execution-based automated labeling; increases difficulty of creating high-quality datasets and requires manual annotation or semantic modeling approaches; motivates BiV-HNN and manual labeling instead of execution checks.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Very high — majority of snippets are non-runnable per cited statistics (74% Python, 88% SQL).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Answer snippets are often illustrative fragments, REPL transcripts, or missing context (imports, variable definitions), and SO answers are authored for readability rather than guaranteed runability.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Do not rely on execution; use token-level normalization (replace VAR/NUMBER/STRING placeholders), sophisticated tokenization, and learned models combining textual context and code content to infer solution status. Manual annotation is used for ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective enough to build StaQC: the authors obtained BiV-HNN F1=0.841 (Python) and created a large dataset (147,546 Python QC pairs) despite non-runnable snippets, and improved downstream code-retrieval MRR (from 0.51 to 0.57 when training CODE-NN on StaQC).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Mining software repositories / NL↔code modeling</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e733.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e733.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Title-only context gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Question-title-only context limitation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using only the question title as the natural language description omits details contained in the question body that can be necessary to correctly interpret or match code snippets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>StaQC mining framework (feature design and model inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The pipeline uses only the question title to represent the natural language intent when pairing with code snippets (following prior work), which may miss important contextual information from the question body.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>Stack Overflow question title (single-line summary)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>answer code snippets that may depend on body details</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>insufficient natural language context / incomplete specification</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The authors follow previous work in using only the question title to characterize the NL side; however, some predictive signals for whether a snippet is a solution may appear in the question body or other parts of the answer post, so title-only representations can cause mispredictions.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>feature/input representation (NL side used by model)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Error analysis and model ablations: authors note cases where correct prediction required close examination of the question body (beyond title) and where BiV-HNN mistakes could be attributed to missing body context.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>No direct quantitative measurement provided for this specific gap, but overall model errors attributable to lack of body information are discussed qualitatively in error analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Potential reduction in model accuracy and missed disambiguating signals; authors observed some mispredictions that would require the question post content to resolve, suggesting a non-negligible effect though not quantitatively measured.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Not quantified explicitly in the paper; noted as a recurring source of some errors in error analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Practical design choice (to follow prior work and simplify modeling) and difficulty of reliably extracting and representing full question-body semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Authors suggest incorporating question post content in future work to improve predictions; current mitigation not implemented in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>NL↔code dataset building / information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e733.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e733.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-step composition gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-step solution composition / snippet aggregation gap</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Some answers provide a complete solution only when multiple code snippets are combined (multi-step solutions), but the current per-snippet independent modeling treats each snippet separately and can mislabel partial steps as standalone solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>StaQC mining framework (per-snippet classification)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The system models each code snippet independently (using surrounding two text blocks and code content) rather than modeling the full sequence of snippets for the post, so it cannot reliably detect when snippets are intended to be combined.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>Answer post sequence of sentences introducing sequential code blocks</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>sequence of code snippets forming a multi-step implementation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing composition semantics / incomplete aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>When a correct solution requires merging multiple code snippets (e.g., step 1 defines helper, step 2 uses it), per-snippet independent prediction may label the first step as a standalone solution (false positive) or undervalue combined semantics, because the model does not consider follow-up snippets or global post structure.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>modeling approach / sequence-level reasoning across snippets</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Error analysis of model predictions; qualitative examples where first-step snippets were incorrectly predicted as full solutions; recognition by authors that >50% of wrong predictions were false positives and that some are due to multi-step nature.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>No explicit numeric prevalence for multi-step cases, but authors report that more than half of wrong predictions were false positives and identify multi-step first-step errors as a common cause.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Leads to false positives in mined datasets and thus introduces noisy QC pairs; degrades precision unless sequence-level reasoning is added.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Not precisely quantified, but identified as a frequent source of errors in model's false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Model formulation that predicts snippet labels independently (local context only) instead of modeling the full post as a joint sequence; answer authors' use of multiple interdependent snippets.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Authors propose future work to model sequences of code snippets jointly (sequence labeling / merging snippets) and note that combining multiple models already helps correct many mistakes; no implemented sequence-level solution in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Software engineering QA mining / sequence modeling</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>From query to usable code: an analysis of stack overflow code snippets <em>(Rating: 2)</em></li>
                <li>Summarizing source code using a neural attention model <em>(Rating: 2)</em></li>
                <li>Spotting working code examples <em>(Rating: 2)</em></li>
                <li>Quality questions need quality code: classifying code fragments on stack overflow <em>(Rating: 2)</em></li>
                <li>Bimodal modelling of source code and natural language <em>(Rating: 1)</em></li>
                <li>Leveraging a corpus of natural language descriptions for program similarity <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-733",
    "paper_id": "paper-dc030c2e55b266c029356a54bb444b7d9b1f2abc",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Heuristic pairing mismatch",
            "name_full": "Heuristic question-code pairing mismatch (Select-First / Select-All / concat)",
            "brief_description": "Mismatch caused by heuristic methods that pair natural language question titles with code snippets in multi-code answer posts, producing low-precision or low-recall QC pairs because many snippets are not standalone solutions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "StaQC mining framework (heuristic baselines vs. BiV-HNN)",
            "system_description": "Pipeline for mining question-code (QC) pairs from Stack Overflow; compares naive heuristics (Select-First, Select-All) against learned models (BiV-HNN and variants) to identify standalone code solutions.",
            "nl_description_type": "Stack Overflow question title / answer post text",
            "code_implementation_type": "Stack Overflow answer code snippets (Python, SQL)",
            "gap_type": "incomplete/incorrect pairing (low precision / low recall)",
            "gap_description": "Heuristic rules that pair a question title with either the first code block, every code block, or a concatenation of all code blocks fail because answer posts often contain multiple code snippets playing different roles (standalone solution, input-output demo, partial step, reminder). These heuristics therefore (a) include non-solution snippets as solutions (low precision) and/or (b) miss actual standalone solutions that are not the first snippet (low recall).",
            "gap_location": "data collection / dataset construction (pairing natural language to code)",
            "detection_method": "Empirical comparison against manually annotated ground truth (human annotation of per-snippet labels) and quantitative evaluation of heuristics vs. learned models using precision/recall/F1/accuracy.",
            "measurement_method": "Standard binary-class metrics: precision, recall, F1, accuracy computed on held-out human-annotated test sets; explicit comparison of heuristics' metrics to BiV-HNN. Example numbers: Select-First Python F1=0.607, accuracy=0.663; Select-All Python F1=0.642, accuracy=0.472; BiV-HNN Python F1=0.841, accuracy=0.843.",
            "impact_on_results": "Heuristics produced much lower-quality QC pairs that would degrade downstream models; quantified as &gt;15% absolute lower F1/accuracy compared to BiV-HNN in the paper's experiments (e.g., ~0.24 absolute accuracy gap between Select-All and BiV-HNN in Python). Poor pairing reduces usefulness of mined datasets for training NL↔code models.",
            "frequency_or_prevalence": "Very common in the studied corpora: 44.66% of Python and 34.35% of SQL accepted 'how-to-do-it' answer posts contain more than one code snippet, making heuristics frequently mispair.",
            "root_cause": "Simplistic heuristics that assume a one-to-one mapping between question and a single code block; answer posts are composite and code snippets can serve multiple non-solution roles; accepted-answer-level verification does not provide per-snippet labels.",
            "mitigation_approach": "Train a learned classifier (Bi-View Hierarchical Neural Network, BiV-HNN) that uses both textual context and code content to predict whether each snippet is a standalone solution; combine multiple model variants and consensus heuristics to improve precision.",
            "mitigation_effectiveness": "Substantial improvements: BiV-HNN outperformed heuristics by &gt;15% absolute in F1/accuracy; combined model (agreement of three models) labeled 69.2% of Python code blocks with F1=0.916 and accuracy=0.911, and 78.7% of SQL blocks with F1=0.943 and accuracy=0.926.",
            "domain_or_field": "Natural language + code dataset mining / machine learning for code",
            "reproducibility_impact": true,
            "uuid": "e733.0",
            "source_info": {
                "paper_title": "StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow",
                "publication_date_yy_mm": "2018-03"
            }
        },
        {
            "name_short": "Label granularity gap",
            "name_full": "Accepted-answer vs per-snippet label granularity mismatch",
            "brief_description": "The natural language verification (accepted answer check) exists at the answer-post level, not per code snippet, so datasets constructed from accepted answers lack per-snippet ground-truth labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "StaQC mining framework (annotation and model training)",
            "system_description": "Mining system relies on per-snippet binary labels (standalone solution vs not) but Stack Overflow only provides accepted-answer signals at the post level; addresses this by manual annotation and learned prediction.",
            "nl_description_type": "Stack Overflow accepted-answer indicator (post-level)",
            "code_implementation_type": "multiple code snippets within one accepted answer post",
            "gap_type": "incomplete specification / missing labels",
            "gap_description": "Stack Overflow's accepted-answer flag verifies an entire answer post but does not indicate which of multiple embedded code snippets are actual standalone solutions. This prevents direct reliable pairing of natural language questions to individual snippets without additional labeling.",
            "gap_location": "dataset labeling / ground-truth availability",
            "detection_method": "Observation of Stack Overflow metadata (accepted answer flag) combined with manual annotation effort showing need for per-snippet labels; empirical annotation process (two annotators per snippet, Cohen's kappa reported).",
            "measurement_method": "Annotation statistics: around 85% of code snippets were labeled by annotators; inter-annotator agreement measured by Cohen's kappa (≈0.658 Python, 0.691 SQL). Proportion of multi-snippet accepted answers: 44.66% Python, 34.45% SQL.",
            "impact_on_results": "Without per-snippet labels, heuristics are forced to make incorrect assumptions leading to noisy training data; manual annotation and learned labeling improved dataset quality and downstream model performance (e.g., StaQC yields improved CODE-NN MRR).",
            "frequency_or_prevalence": "Highly prevalent because many accepted answers contain multiple code blocks (see percentages above); majority of accepted answers cannot be unambiguously used for QC pairing without per-snippet labeling.",
            "root_cause": "Stack Overflow's accepted-answer mechanism is coarse-grained; authors of answers include multiple snippets with diverse roles without explicit markup of role.",
            "mitigation_approach": "Human annotation (dual-annotator with agreement threshold) to create labeled training data; train predictive models (BiV-HNN) to infer per-snippet labels; combine models for higher-confidence automated labeling.",
            "mitigation_effectiveness": "Manual annotation provided training/validation/test sets (thousands of examples) enabling BiV-HNN to achieve F1 up to 0.841 Python and 0.888 SQL; enabled creation of large-scale StaQC (147,546 Python, 119,519 SQL pairs).",
            "domain_or_field": "Data curation for ML / software engineering corpora",
            "reproducibility_impact": true,
            "uuid": "e733.1",
            "source_info": {
                "paper_title": "StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow",
                "publication_date_yy_mm": "2018-03"
            }
        },
        {
            "name_short": "Non-solution snippet roles",
            "name_full": "Non-solution roles of code snippets (input-output demo, reminder, partial step)",
            "brief_description": "Code snippets in answer posts often play roles other than being a standalone solution: demonstrations of input/output, small examples, reminders, or steps of a larger multi-step solution, creating a semantics mismatch with natural language question descriptions when heuristically paired.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "StaQC mining framework / BiV-HNN classifier",
            "system_description": "The classifier must distinguish code snippets that are true standalone solutions from those that are examples, demos, or fragments; uses textual context and code content to infer role.",
            "nl_description_type": "Answer post textual context (surrounding sentences)",
            "code_implementation_type": "Code snippet fragments (examples, demos, partial implementations)",
            "gap_type": "ambiguous role / incomplete specification",
            "gap_description": "A code snippet may be runnable but not constitute a full solution (e.g., C2 in Figure 1 is an input-output demo showing expected I/O; C4 is a reminder). Simple matching between question and snippet can therefore misclassify demos or partial steps as full solutions.",
            "gap_location": "interpretation of code snippet role within answer post",
            "detection_method": "Manual annotation of per-snippet labels; model error analysis showing false positives where partial snippets were predicted as solutions; qualitative examples from posts.",
            "measurement_method": "Annotation labels (binary) and classifier error statistics: more than half of model mistakes were false positives (predicting non-solution as solution). Evaluation metrics include per-class precision/recall and overall F1; examples include BiV-HNN precision=0.808, recall=0.876 (Python).",
            "impact_on_results": "False positives introduce noisy QC pairs into dataset, harming downstream models; error analysis showed correcting false positives often requires broader context (entire post), and current per-snippet independent modeling can mislabel fragments as full solutions.",
            "frequency_or_prevalence": "Common — multi-role snippets are frequent in multi-code posts (44.66% Python, 34.35% SQL); among model errors, &gt;50% were false positives linked to these non-solution roles.",
            "root_cause": "Answer authors include multiple snippets with different communicative intents; natural language cues for role may be dispersed across the post and not present in the title; heuristics and per-snippet-local models lack global post-level reasoning.",
            "mitigation_approach": "Modeling both textual context and code content (BiV-HNN) to capture signals such as surrounding sentences that introduce or comment on snippets; consider combining models and using consensus to increase precision; future work suggests modeling sequences of snippets jointly to capture multi-step solutions.",
            "mitigation_effectiveness": "BiV-HNN reduced false-positive errors relative to heuristics (quantified improvements in F1 and accuracy). Combined-model consensus achieved much higher precision/F1 on the subset it labels (e.g., Python F1=0.916 on 69.2% of blocks). However, residual false positives remain, and authors identify joint-sequence modeling as future work.",
            "domain_or_field": "Software engineering / dataset curation for NL↔code",
            "reproducibility_impact": true,
            "uuid": "e733.2",
            "source_info": {
                "paper_title": "StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow",
                "publication_date_yy_mm": "2018-03"
            }
        },
        {
            "name_short": "Parsability/runability gap",
            "name_full": "Parsability/runability mismatch of code snippets",
            "brief_description": "A large fraction of Stack Overflow code snippets are not directly parsable or runnable, so verifying snippet correctness via execution is often infeasible, causing reliance on semantic and contextual methods.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "StaQC mining framework (dataset verification considerations)",
            "system_description": "When mining QC pairs, the system cannot rely on executing snippets for validation because many snippets are incomplete or contain interactive console markers; must use language+code analysis instead.",
            "nl_description_type": "Question title and surrounding answer text",
            "code_implementation_type": "incomplete / interactive / partial code snippets (Python, SQL)",
            "gap_type": "implementation incompleteness (non-runnable / unparsable code)",
            "gap_description": "Prior work reports that around 74% of Python and 88% of SQL snippets on SO are not directly parsable or runnable; hence automatic verification by running code is not a reliable method to determine whether a snippet is a standalone solution.",
            "gap_location": "validation step (runnable-execution-based verification)",
            "detection_method": "Reference to prior analyses of Stack Overflow snippet parsability and the authors' observation that executing snippets is infeasible; use of AST-based token replacement to regularize code but not to execute.",
            "measurement_method": "Quoted statistics from prior work: ~74% of Python and ~88% of SQL snippets not directly parsable/runnable (cited prior studies [21,59] in paper). The authors additionally applied tokenization and AST-based replacement to ease sparsity but did not run most snippets.",
            "impact_on_results": "Prevents use of execution-based automated labeling; increases difficulty of creating high-quality datasets and requires manual annotation or semantic modeling approaches; motivates BiV-HNN and manual labeling instead of execution checks.",
            "frequency_or_prevalence": "Very high — majority of snippets are non-runnable per cited statistics (74% Python, 88% SQL).",
            "root_cause": "Answer snippets are often illustrative fragments, REPL transcripts, or missing context (imports, variable definitions), and SO answers are authored for readability rather than guaranteed runability.",
            "mitigation_approach": "Do not rely on execution; use token-level normalization (replace VAR/NUMBER/STRING placeholders), sophisticated tokenization, and learned models combining textual context and code content to infer solution status. Manual annotation is used for ground truth.",
            "mitigation_effectiveness": "Effective enough to build StaQC: the authors obtained BiV-HNN F1=0.841 (Python) and created a large dataset (147,546 Python QC pairs) despite non-runnable snippets, and improved downstream code-retrieval MRR (from 0.51 to 0.57 when training CODE-NN on StaQC).",
            "domain_or_field": "Mining software repositories / NL↔code modeling",
            "reproducibility_impact": true,
            "uuid": "e733.3",
            "source_info": {
                "paper_title": "StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow",
                "publication_date_yy_mm": "2018-03"
            }
        },
        {
            "name_short": "Title-only context gap",
            "name_full": "Question-title-only context limitation",
            "brief_description": "Using only the question title as the natural language description omits details contained in the question body that can be necessary to correctly interpret or match code snippets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "StaQC mining framework (feature design and model inputs)",
            "system_description": "The pipeline uses only the question title to represent the natural language intent when pairing with code snippets (following prior work), which may miss important contextual information from the question body.",
            "nl_description_type": "Stack Overflow question title (single-line summary)",
            "code_implementation_type": "answer code snippets that may depend on body details",
            "gap_type": "insufficient natural language context / incomplete specification",
            "gap_description": "The authors follow previous work in using only the question title to characterize the NL side; however, some predictive signals for whether a snippet is a solution may appear in the question body or other parts of the answer post, so title-only representations can cause mispredictions.",
            "gap_location": "feature/input representation (NL side used by model)",
            "detection_method": "Error analysis and model ablations: authors note cases where correct prediction required close examination of the question body (beyond title) and where BiV-HNN mistakes could be attributed to missing body context.",
            "measurement_method": "No direct quantitative measurement provided for this specific gap, but overall model errors attributable to lack of body information are discussed qualitatively in error analysis.",
            "impact_on_results": "Potential reduction in model accuracy and missed disambiguating signals; authors observed some mispredictions that would require the question post content to resolve, suggesting a non-negligible effect though not quantitatively measured.",
            "frequency_or_prevalence": "Not quantified explicitly in the paper; noted as a recurring source of some errors in error analysis.",
            "root_cause": "Practical design choice (to follow prior work and simplify modeling) and difficulty of reliably extracting and representing full question-body semantics.",
            "mitigation_approach": "Authors suggest incorporating question post content in future work to improve predictions; current mitigation not implemented in experiments.",
            "mitigation_effectiveness": null,
            "domain_or_field": "NL↔code dataset building / information extraction",
            "reproducibility_impact": false,
            "uuid": "e733.4",
            "source_info": {
                "paper_title": "StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow",
                "publication_date_yy_mm": "2018-03"
            }
        },
        {
            "name_short": "Multi-step composition gap",
            "name_full": "Multi-step solution composition / snippet aggregation gap",
            "brief_description": "Some answers provide a complete solution only when multiple code snippets are combined (multi-step solutions), but the current per-snippet independent modeling treats each snippet separately and can mislabel partial steps as standalone solutions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "StaQC mining framework (per-snippet classification)",
            "system_description": "The system models each code snippet independently (using surrounding two text blocks and code content) rather than modeling the full sequence of snippets for the post, so it cannot reliably detect when snippets are intended to be combined.",
            "nl_description_type": "Answer post sequence of sentences introducing sequential code blocks",
            "code_implementation_type": "sequence of code snippets forming a multi-step implementation",
            "gap_type": "missing composition semantics / incomplete aggregation",
            "gap_description": "When a correct solution requires merging multiple code snippets (e.g., step 1 defines helper, step 2 uses it), per-snippet independent prediction may label the first step as a standalone solution (false positive) or undervalue combined semantics, because the model does not consider follow-up snippets or global post structure.",
            "gap_location": "modeling approach / sequence-level reasoning across snippets",
            "detection_method": "Error analysis of model predictions; qualitative examples where first-step snippets were incorrectly predicted as full solutions; recognition by authors that &gt;50% of wrong predictions were false positives and that some are due to multi-step nature.",
            "measurement_method": "No explicit numeric prevalence for multi-step cases, but authors report that more than half of wrong predictions were false positives and identify multi-step first-step errors as a common cause.",
            "impact_on_results": "Leads to false positives in mined datasets and thus introduces noisy QC pairs; degrades precision unless sequence-level reasoning is added.",
            "frequency_or_prevalence": "Not precisely quantified, but identified as a frequent source of errors in model's false positives.",
            "root_cause": "Model formulation that predicts snippet labels independently (local context only) instead of modeling the full post as a joint sequence; answer authors' use of multiple interdependent snippets.",
            "mitigation_approach": "Authors propose future work to model sequences of code snippets jointly (sequence labeling / merging snippets) and note that combining multiple models already helps correct many mistakes; no implemented sequence-level solution in this work.",
            "mitigation_effectiveness": null,
            "domain_or_field": "Software engineering QA mining / sequence modeling",
            "reproducibility_impact": true,
            "uuid": "e733.5",
            "source_info": {
                "paper_title": "StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow",
                "publication_date_yy_mm": "2018-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "From query to usable code: an analysis of stack overflow code snippets",
            "rating": 2
        },
        {
            "paper_title": "Summarizing source code using a neural attention model",
            "rating": 2
        },
        {
            "paper_title": "Spotting working code examples",
            "rating": 2
        },
        {
            "paper_title": "Quality questions need quality code: classifying code fragments on stack overflow",
            "rating": 2
        },
        {
            "paper_title": "Bimodal modelling of source code and natural language",
            "rating": 1
        },
        {
            "paper_title": "Leveraging a corpus of natural language descriptions for program similarity",
            "rating": 1
        }
    ],
    "cost": 0.0162285,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow</h1>
<p>Ziyu Yao ${ }^{\dagger}$, Daniel S. Weld<em>, Wei-Peng Chen ${ }^{\dagger \dagger}$, Huan Sun ${ }^{\dagger}$<br>${ }^{\dagger}$ The Ohio State University, </em>University of Washington, ${ }^{\dagger \dagger}$ Fujitsu Labs of America {yao.470,sun.397}@osu.edu, weld@cs.washington.edu, wei-peng.chen@us.fujitsu.com</p>
<h2>ABSTRACT</h2>
<p>Stack Overflow (SO) has been a great source of natural language questions and their code solutions (i.e., question-code pairs), which are critical for many tasks including code retrieval and annotation. In most existing research, question-code pairs were collected heuristically and tend to have low quality. In this paper, we investigate a new problem of systematically mining question-code pairs from Stack Overflow (in contrast to heuristically collecting them). It is formulated as predicting whether or not a code snippet is a standalone solution to a question. We propose a novel Bi-View Hierarchical Neural Network which can capture both the programming content and the textual context of a code snippet (i.e., two views) to make a prediction. On two manually annotated datasets in Python and SQL domain, our framework substantially outperforms heuristic methods with at least $\mathbf{1 5 \%}$ higher $F_{1}$ and accuracy. Furthermore, we present StaQC (Stack Overflow Question-Code pairs), the largest dataset to date of $\mathbf{\sim 1 4 8 K}$ Python and $\mathbf{1 2 0 K}$ SQL question-code pairs, automatically mined from SO using our framework. Under various case studies, we demonstrate that StaQC can greatly help develop data-hungry models for associating natural language with programming language ${ }^{1}$.</p>
<h2>CCS CONCEPTS</h2>
<ul>
<li>Information systems $\rightarrow$ Web searching and information discovery; Question answering; Information extraction; $\bullet$ Software and its engineering $\rightarrow$ Automatic programming;</li>
</ul>
<h2>KEYWORDS</h2>
<p>Natural Language Question Answering; Question-Code Pairs; Deep Neural Networks; Stack Overflow</p>
<h2>ACM Reference Format:</h2>
<p>Ziyu Yao ${ }^{\dagger}$, Daniel S. Weld ${ }^{#}$, Wei-Peng Chen ${ }^{\dagger \dagger}$, Huan Sun ${ }^{\dagger}$. 2018. StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow. In WWW 2018: The 2018 Web Conference, April 23-27, 2018, Lyon, France. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3178876.3186081</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The accepted answer post to question "Elegant Python function to convert CamelCase to snake_case?" in SO. $S_{i}(i=1,2,3,4)$ and $C_{j}(j=1,2,3,4)$ denote sentence blocks and code blocks respectively, which can be trivially separated based on the HTML format.</p>
<h2>1 INTRODUCTION</h2>
<p>Online forums such as Stack Overflow (SO) [40] have contributed a huge number of code snippets, understanding and reuse of which can greatly speed up software development. Towards this goal, a lot of research work have been developed recently, such as retrieving or generating code snippets based on a natural language query, and annotating code snippets using natural language [2, 21, 28, 44, 52, 61, 64]. At the core of these work are machine learning models that map between natural language and programming language, which are typically data-hungry [19, 24, 46] and require large-scale and high-quality <natural language question, code solution> pairs (i.e., question-code or QC pairs).</p>
<p>In our work, we define a code snippet as a code solution when the questioner can solve the problem solely based on it (also named as "standalone" solution). Take Figure 1 as an example, which shows the accepted answer post ${ }^{2}$ to question "Elegant Python function to convert CamelCase to snake_case". Among the four code snippets $\left{C_{1}, C_{2}, C_{3}, C_{4}\right}$, only $C_{1}$ and $C_{3}$ are standalone code solutions to</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>the question while the rest are not, because $C_{2}$ only gives an inputoutput demo of the "convert" function without its definition and $C_{4}$ is a reminder of an additional detail. Given an answer post with multiple code snippets (i.e., a multi-code answer post) like Figure 1, previous work usually collected question-code pairs in heuristic ways: Simply pair the question title with the first code snippet, or with each code snippet, or with the concatenation of all code snippets in the post [2, 64]. Iyer et al. [21] merely employed accepted answer posts that contain exactly one code snippet, and discarded all others with multiple code snippets. Such heuristic question-code collection methods suffer from at least one of the following weaknesses: (1) Low precision: Questions do not match with their paired code snippets, when the latter serve as background, explanation, or input-output demo rather than as a solution (e.g., $C_{2}$ in Figure 1); (2) Low recall: If one only selects the first code snippet to pair with a question, other code solutions in an answer post (e.g., $C_{3}$ ) will be unemployed.</p>
<p>In fact, multi-code answer posts are very common in SO, which makes the low-precision and low-recall issues even more prominent. In the Stack Exchange Data dump[51], among all accepted answer posts for Python and SQL "how-to-do-it" questions (to be introduced in Section 2), $44.66 \%$ and $34.35 \%$ contain more than one code snippets respectively. Note that an accepted answer post was verified only as an entirety by the questioner, and labels on whether each individual code snippet serves as a standalone solution or not are not readily available. Moreover, it is not feasible to obtain such labels by simply running each code snippet in a programming environment for two reasons: (1) A runnable code snippet is not necessarily a code solution (e.g., $C_{4}$ in Figure 1); (2) It was reported that around $74 \%$ of Python and $88 \%$ of SQL code snippets in SO are not directly parsable or runnable [21, 59]. Nevertheless, many of them usually contain critical information to answer a question. Therefore, they can still be used in semantic analysis for downstream tasks $[2,21,59,64]$ once paired with natural language questions.</p>
<p>To systematically mine question-code pairs with high precision and recall, we propose a novel task: Given a question ${ }^{3}$ in SO and its accepted answer post with multiple code snippets, how to predict whether each code snippet is a standalone solution or not? In this paper, we focus on "how-to-do-it"-type of questions which ask how to implement a certain task like in Figure 1, since answers to such questions are most likely to be standalone code solutions. The definition and classification of different types of questions will be discussed in Section 2. We identify two challenges in our task: (1) As shown in Figure 1, code snippets in an answer post can play many non-solution roles such as serving as an input-output demo or reminder (e.g., $C_{2}$ and $C_{4}$ ), which calls for a statistical learning model to make accurate predictions. (2) Both the textual context and the programming content of a code snippet can be predictive, but an effective model to jointly utilize them needs careful design. Intuitively, a text block with patterns like "you can do ..." and "this is one thorough solution ..." is more likely to be followed by a code solution. For example, given $S_{1}$ and $S_{3}$ in Figure 1, a code solution is likely to be introduced after them. On the other hand, by inspecting the code content, $C_{2}$ is probably not a code solution to the question,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>since it contains special Python console patterns like " $&gt;&gt;&gt;$... $&gt;&gt;&gt;$ " and no particular definition of "convert".</p>
<p>To tackle these challenges, we explore a series of models including traditional classifiers and deep learning models, and propose a novel model, named Bi-View Hierarchical Neural Network (BiVHNN), to capture both the textual context and the programming content of each code snippet (which make the two views). In BiVHNN, we design two different modules to learn features from text and code respectively, and combine them into a deep neural network architecture, which finally predicts whether a code snippet is a standalone solution or not. To summarize, our contributions lie in three folds:</p>
<p>First, to the best of our knowledge, we are the first to investigate systematically mining large-scale high-quality question-code pairs, which are critical for developing learning-based models aiming to map between natural language and programming language.</p>
<p>Second, we extensively explore various models including traditional classifiers and deep learning models to predict whether a code snippet is a solution or not, and propose a novel Bi-View Hierarchical Neural Network which considers both text- and code-based views. On two manually labeled datasets in Python and SQL domain, BiV-HNN outperforms both the widely adopted heuristic methods and traditional classifiers by a large margin in terms of $F_{1}$ and accuracy. Moreover, BiV-HNN does not rely on any prior knowledge and can be easily applied to other programming domains.</p>
<p>Last but not least, we present StaQC, the largest dataset to date of $\mathbf{\sim 1 4 8 K}$ Python and $\mathbf{\sim 1 2 0 K}$ SQL question-code pairs, systematically mined by our framework. Using multiple case studies, we show that (1) StaQC is rich in surface variation: A question can be paired with multiple code solutions, and semantically the same code snippets can have different/paraphrased natural language descriptions. (2) Owing to such diversity as well as its large scale, StaQC is a much better data resource than existing ones for constructing models to map between natural language and programming language. In addition, we can continue to grow StaQC in both size and diversity, by regularly applying our framework to the fast-growing SO. Question-code pairs in other programming languages can also be mined similarly and included in StaQC.</p>
<h2>2 PRELIMINARIES</h2>
<p>In this section, we first clarify our task definition, and then describe how we annotated datasets for model development.</p>
<h3>2.1 Task Definition</h3>
<p>Given a question and its accepted answer post which contains multiple code snippets in Stack Overflow, we aim at predicting whether each code snippet in the answer post is a standalone solution to the question or not. As explained in Section 1, we focus on "accepted" answer posts and "standalone" solutions.</p>
<p>Users can ask different types of questions in SO such as "how to implement X" and "what/why is Y". Following previous work [12, 13, 32], we divide questions into five types: "How-to-do-it", "Debug/corrective", "Conceptual", "Seeking something, e.g., advice, tutorial", and their combinations. In particular, a question is of type "how-to-do-it" when the questioner provides a scenario and asks how to implement it like in Figure 1.</p>
<p>For collecting question-code pairs, we target at "how-to-do-it" questions, because answers to other types of questions are not very likely to be standalone code solutions (e.g., answers to "Conceptual" questions are usually text descriptions). Next, we describe how to distinguish "how-to-do-it" questions from others.</p>
<h2>2.2 "How-to-do-it" Question Collection</h2>
<h3>2.2.1 Question Type Classification.</h3>
<p>At the high level, we combined the other four question types apart from "how-to-do-it" into one category named "non-how-to" and built a binary question type classifier.</p>
<p>We first collected Python and SQL questions from SO based on their tags, which are available for all question posts. Specifically, we considered questions whose tags contain the keyword "python" to be in Python domain and questions tagged by "sql", "database" or "oracle" to be in SQL domain. For each domain, we randomly sampled and labeled 250 questions for training (150), validating (20) and testing (80) the classifier ${ }^{4}$. Among the 250 questions, around $45 \%$ in Python and $57 \%$ in SQL are "how-to-do-it" questions. We built one Logistic Regression classifier respectively for each domain, based on simple features extracted from question and answer posts as in [13], such as keyword-occurrence features, the number of code blocks in question/answer posts, the maximum length of code blocks, etc. Hyperparameters in classifiers were tuned based on validation sets.</p>
<p>Finally, we obtained a question-type classification accuracy of 0.738 (precision: 0.653 , recall: 0.889 , and $F_{1}: 0.753$ ) for Python and an accuracy of 0.713 (precision: 0.625 , recall: 0.946 , and $F_{1}: 0.753$ ) for SQL. The classification of question types may be further improved with more advanced features and algorithms, which is not the focus of this paper.
2.2.2 "How-to-do-it" Question Set Collection.</p>
<p>Using the above classifiers, we classified all Python and SQL questions in SO whose accepted answer post contains code blocks and collected a large set of "how-to-do-it" questions in each domain. Among these "how-to-do-it" questions, around $44.66 \%(68,839)$ Python questions and $34.45 \%(39,752)$ SQL questions have an accepted answer post with more than one code snippets, from which we will systematically mine question-code pairs.</p>
<h3>2.3 Annotating QC Pairs for Model Training</h3>
<p>To construct training/validation/testing datasets for our task, we hired four undergraduate students familiar with Python and SQL to annotate answer posts in these two domains. For each code snippet in an answer post, annotators can assign " 1 " to it if they think they can solve the problem based on the code snippet alone (i.e., it is a standalone code solution), and " 0 " otherwise. We ensured each code snippet is annotated by two annotators and adopted the label only when both annotators agreed on it. For each programming language, around $85 \%$ code snippets were labeled. The average Cohen's kappa agreement [9] is around 0.658 for Python and 0.691 for SQL. The statistics of our manually annotated datasets are summarized in Table 1, which will be used to develop our models.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>|  | Python |  | SQL |  |
| :--: | :--: | :--: | :--: | :--: |
|  | $\begin{aligned} &amp; \text { # of QC } \ &amp; \text { pairs } \end{aligned}$ | \% of QC pairs with label "1" | $\begin{aligned} &amp; \text { # of QC } \ &amp; \text { pairs } \end{aligned}$ | \% of QC pairs with label "1" |
| Training | 2,932 | $43.89 \%$ | 2,183 | $56.12 \%$ |
| Validation | 976 | $43.14 \%$ | 727 | $55.98 \%$ |
| Testing | 976 | $47.23 \%$ | 727 | $58.32 \%$ |</p>
<p>Table 1: Statistics of manually annotated datasets.</p>
<h2>3 BI-VIEW HIERARCHICAL NN</h2>
<p>Without loss of generality, let us assume an answer post of a given question has a sequence of blocks $\left(S_{1}, C_{1}, S_{2}, \ldots, S_{l}, C_{l}, S_{i+1}, \ldots, S_{L-1}\right.$, $\left.C_{L-1}, S_{L}\right)$ with $L$ text blocks $\left(S_{i}\right.$ 's) and $L-1$ code blocks $\left(C_{i}\right.$ 's) interleaving with each other. Our task is to automatically assign a binary label to each code snippet $C_{l}$, where 1 means a standalone solution while 0 otherwise. In this work, we model each code snippet independently and predict the label of $C_{l}$ based on its textual context (i.e., $S_{i}, S_{i+1}$ ) and programming content. If either $S_{i}$ or $S_{i+1}$ is empty, we insert an empty dummy text block to make our model applicable. One can extend our formulation to a more complicated sequence labeling problem where a sequence of code snippets can be modeled simultaneously, which we leave for future work.</p>
<h3>3.1 Intuition</h3>
<p>We first analyze at the high level how each individual block contributes to elaborating the entire answer fluently. For example, in Figure 1, the first text block $S_{1}$ suggests its followed code block $C_{1}$ (which implements a function) is "thorough" and thus might be a solution. $S_{2}$ subsequently connects $C_{1}$ to examples it can work with in $C_{2}$. In contrast, $S_{3}$ starts with the conjunction word "Or" and possibly will introduce an alternative solution (e.g., $C_{3}$ ). This observation inspires us to first model the meaning of each block separately using a token-level sequence encoder, then model the block sequence $S_{i}-C_{i}-S_{i+1}$ using a block-level encoder, from which we finally obtain the semantic representation of $C_{i}$.</p>
<p>Figure 2 shows our model, named Bi-View Hierarchical Neural Network (BiV-HNN). It progressively learns the semantic representation of a code block from token level to block level, based on which we predict it to be a standalone solution or not. On the other hand, BiV-HNN naturally incorporates two views, i.e., textual context and code content, into the model structure. We detail each component as follows.</p>
<h3>3.2 Token-level Sequence Encoder</h3>
<p>Text block. Given a sentence block $S_{i}$ with a sequence of words $w_{i t}, t \in\left[1, T_{i}\right]$, we first embed the words into vectors through a pretrained word embedding matrix $W_{e}$, i.e., $x_{i t}=W_{e} w_{i t}$. We then use a bidirectional Gated Recurrent Unit (GRU) based Recurrent Neural Network (RNN) [8] to learn the word representation by summarizing the contextual information from both directions. The GRU tracks the state of sequences by controlling how much information is updated into the new hidden state from previous states. Specifically, given the input word vector $x_{t}$ in the current step and the hidden state $h_{t-1}$ from the last step, the GRU first computes a reset gate $r$ for resetting information from previous steps in order</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Our Bi-View Hierarchical Neural Network (BiVHNN). Text block $S_{i}$ and question $q$ are encoded by a bidirectional GRU-based RNN (Bi-GRU) module and code block $C_{i}$ is encoded by another Bi-GRU with different parameters.
to learn a new hidden state $\hat{h}_{t}$ :</p>
<p>$$
\begin{gathered}
r=\sigma\left(W_{r}\left[x_{t}, h_{t-1}\right]+b_{r}\right) \
\hat{h}<em t="t">{t}=\phi\left(W\left[x</em>\right]+b\right)
\end{gathered}
$$}, r \odot h_{t-1</p>
<p>where $\left[x_{t}, h_{t-1}\right]$ is the concatenation of $x_{t}$ and $h_{t-1}, \sigma$ and $\phi$ are the sigmoid and tanh activation function respectively. $W_{r}, W$ are two weight matrices in $R^{d_{h} \times\left(d_{x}+d_{h}\right)}$ and $b_{r}, b$ are the biases in $R^{d_{h}}$, where $d_{x}, d_{h}$ is the dimension of $x_{t}$ and the hidden state $h_{t-1}$ respectively. Intuitively, if $r$ is close to 0 , then the information in $h_{t-1}$ will not be passed into the current step when learning the new hidden state. The GRU also defines an update gate $u$ for integrating hidden states $h_{t-1}$ and $\hat{h}_{t}$ :</p>
<p>$$
\begin{gathered}
u=\sigma\left(W_{u}\left[x_{t}, h_{t-1}\right]+b_{u}\right) \
h_{t}=u h_{t-1}+(1-u) \hat{h}_{t}
\end{gathered}
$$</p>
<p>When $u$ is closer to $0, h_{t}$ contains more information about the current step $\hat{h}<em t="t">{t}$; otherwise, it memorizes more about previous steps. Onwards, we denote the above calculation by $h</em>\right)$ for convenience.}=G R U\left(x_{t}, h_{t-1</p>
<p>In our work, the bidirectional GRU (i.e., Bi-GRU) contains a forward GRU reading a text block $S_{i}$ from $w_{i 1}$ to $w_{i T_{i}}$ and a backward GRU which reads from $w_{i T_{i}}$ to $w_{i 1}$ :</p>
<p>$$
\begin{gathered}
\stackrel{\rightharpoonup}{h}<em i="i" t="t">{i t}=G R U\left(x</em>}, \overleftarrow{h<em i="i">{i, t-1}\right), t \in\left[1, T</em>\right] \
\overleftarrow{h}<em i="i" t="t">{i t}=G R U\left(x</em>}, \overleftarrow{h<em i="i">{i, t+1}\right), t \in\left[T</em>, 1\right] \
\overleftarrow{h}<em T__i="T_{i" i_="i,">{i 0}=\overrightarrow{0}, \overleftarrow{h}</em>
\end{gathered}
$$}+1}=\overleftarrow{0</p>
<p>where the hidden states in both directions are initialized with zero vectors. Since the forward and backward GRU summarize the context information from different perspectives, we concatenate their last hidden states (i.e., $\overleftarrow{h}<em i="i">{i T</em>}}, \overleftarrow{h<em i="i">{i 1}$ ) to represent the meaning of the text block $S</em>$ :</p>
<p>$$
s_{i}=\left[\stackrel{\rightharpoonup}{h}<em i="i">{i T</em>\right]
$$}}, \overleftarrow{h}_{i 1</p>
<p>Code block. Similarly, we employ another Bi-GRU RNN module to learn a vector representation $v_{c}$ for code block $C_{i}$ based on its code token sequence. One may directly take this code vector $v_{c}$ as the token-level representation of a code block. However, since the goal of our model is to decide whether a code snippet answers a certain question, we associate $C_{i}$ with the question title $q$ to capture their semantic correspondences in the learnt vector representation $c_{i}$. Specifically, we first learn the question vector $v_{q}$ by applying the token-level text encoder to the word sequence in $q$. The concatenation of $v_{q}$ and $v_{c}$ is then fed into a feedforward tanh layer (i.e., "concat feedforward" in Figure 2) for generating $c_{i}$ :</p>
<p>$$
c_{i}=\phi\left(W_{c}\left[v_{q}, v_{c}\right]+b_{c}\right)
$$</p>
<p>We will verify the effect of incorporating $q$ in our experiments.
Unlike modeling a code block, we do not associate a text block with question $q$ when learning its representation, because we observed no direct semantic matching between the two. For example, in Figure 1, a text block can hardly match the question by its content. However, as we discussed in Section 1, a text block with patterns like "you can do ..." or "This is one thorough solution ..." can imply that a code solution will be introduced after it. Therefore, we model each text block per se, without incorporating question information.</p>
<h3>3.3 Block-level Sequence Encoder</h3>
<p>Given the sequence of token-level representations $s_{i}-c_{i}-s_{i+1}$, we use a bidirectional GRU-based RNN to build a block-level sequence encoder and finally obtain the code block representation:</p>
<p>$$
\begin{gathered}
\stackrel{\rightharpoonup}{h}<em i="i">{i}=G R U\left(s</em>}, \stackrel{\rightharpoonup}{0}\right), \overleftarrow{h<em i="i">{i}=G R U\left(s</em>}, \overleftarrow{c h<em i="i">{i}\right) \
\overrightarrow{c h}</em>}=G R U\left(c_{i}, \overleftarrow{h<em i="i">{i}\right), \overleftarrow{c h}</em>}=G R U\left(c_{i}, \overleftarrow{h<em i_1="i+1">{i+1}\right) \
\stackrel{\rightharpoonup}{h}</em>}=G R U\left(s_{i+1}, \overrightarrow{c h<em i_1="i+1">{i}\right), \overleftarrow{h}</em>\right)
\end{gathered}
$$}=G R U\left(s_{i+1}, \overleftarrow{0</p>
<p>where the encoder is initialized with zero vectors (i.e., $\overrightarrow{0}$ and $\overleftarrow{0}$ ) in both directions. We concatenate the forward state $\overrightarrow{c h_{i}}$ and the backward state $\overleftarrow{c h_{i}}$ of the code block as its semantic representation:</p>
<p>$$
z_{i}=\left[\overrightarrow{c h_{i}}, \overleftarrow{c h_{i}}\right]
$$</p>
<h3>3.4 Code Label Prediction</h3>
<p>The representation $z_{i}$ of code block $C_{i}$ is then used for prediction:</p>
<p>$$
y_{i}=\operatorname{softmax}\left(W_{y} z_{i}+b_{y}\right)
$$</p>
<p>where $y_{i}=\left[y_{i 0}, y_{i 1}\right]$ represents the probability of predicting $C_{i}$ to have label 0 or 1 respectively.</p>
<p>We define the loss function using cross entropy [19], which is averaged over all the $N$ code snippets during training:</p>
<p>$$
\mathcal{L}=-\frac{1}{N} \sum_{i=1}^{N} p_{i 0} \log \left(y_{i 0}\right)+p_{i 1} \log \left(y_{i 1}\right)
$$</p>
<p>where $p_{i 0}=0$ and $p_{i 1}=1$ if the i-th code snippet is manually annotated as a solution; otherwise, $p_{i 0}=1$ and $p_{i 1}=0$.</p>
<h2>4 TRADITIONAL CLASSIFIERS WITH FEATURE ENGINEERING</h2>
<p>In addition to neural network based models like BiV-HNN, we also explore traditional classifiers like Logistic Regression (LR) [11] and Support Vector Machine (SVM) [10] for our task. Features are manually crafted from both text- and code-based views:
Textual Context. (1) Token: The unigrams and bigrams in the context. (2) FirstToken: If a sentence starts with phrases like "try this" or "use it", then the following code snippet is very likely to be the solution. Inspired by this idea, we discriminate the first token from others in the context. (3) Conn: Boolean features indicating whether a connective word/phrase (e.g., "alternatively") occurs in the context. We used the common connective words and phrases from Penn Discourse Tree Bank [43].
Code Content. (1) CodeToken: All code tokens in a code snippet. (2) CodeClass: To discriminate code snippets that function and can be considered for learning and pragmatic reuse (i.e., "working code" [22]) from input-output demos, we introduce CodeClass, which is the probability of a code snippet being a working code. Specifically, from all the "how-to-do-it" Python questions in SO, we first collected totally 850 code snippets following text blocks such as "output:" and "output is:" as input-output code snippets. We further randomly selected 850 accepted answer posts containing exactly one code snippet and took their code snippets as the working code. We then extracted a set of features like the proportion of numbers and parenthesis and constructed a binary Logistic Regression classifier, which obtains 0.804 accuracy and $0.891 F_{1}$ on a manually labeled testing set. Finally, the trained classifier outputs the probability for each code snippet in Python being a "working code" as the CodeClass feature. For SQL, a working code can usually be detected by keywords like "SELECT" and "DELETE", which have been included in the CodeToken feature. Thus, we did not design the CodeClass feature for it.
There could be other features to incorporate into traditional classifiers. However, coming up with useful features is anything but an easy task. In contrast, neural network models can automatically learn advanced features from raw data and have been broadly and successfully applied in different areas [8, 24, 30, 50, 53]. Therefore, in our work, we choose to design the neural network based model BiV-HNN. We will compare different models in experiments.</p>
<h2>5 EXPERIMENTS</h2>
<p>In this section, we conduct extensive experiments to compare various models and show the advantages of our proposed BiV-HNN.</p>
<h3>5.1 Experimental Setup</h3>
<p>Dataset Summarization. Section 2 discussed how we manually annotated question-code pairs for training, validation and testing. Statistics were summarized in Table 1. To evaluate different models, we adopt precision, recall, $F_{1}$, and accuracy, which are defined in the same way as in a typical binary classification setting.</p>
<p>Data Preprocessing. We tokenized Python code snippets with best efforts: We first applied Python built-in tokenizer and for code lines that remain untokenized after that, we adopted the "wordpunct_tokenizer" in NLTK toolkit [27] to separate tokens and symbols (e.g., "." and "="). In addition, we detected variables, numbers and strings in a code snippet by traversing its Abstract Syntax Tree (AST) parsed with Python built-in AST parser, and replaced them with special tokens "VAR", "NUMBER" and "STRING" respectively, to alleviate data sparsity. For SQL, we followed [21] to perform the tokenization, which replaced table/column names with placeholder tokens and numbered them to preserve their dependencies. Finally, we collected 4,557 (3,191) word tokens and 6,581 (1,200) code tokens from Python (SQL) training set.
Implementation Details. We used Tensorflow[55] to implement our BiV-HNN and its variants to be introduced in Section 5.2. The embedding size of word and code tokens was set at 150. The embedding vectors were pre-trained using GloVe [42] on all Python or SQL posts in SO. Parameters were randomly initialized following [18]. We started the learning rate at 0.001 and trained neural network models in mini batch of size 100 with the Adam optimizer [23]. The size of the GRU units was chosen from [64, 128] for token-level encoders and from [128, 256] for block-level encoders. Following the convention [20, 21, 29], we selected model parameters based on their performance on validation sets. The Logistic Regression and Support Vector Machine models were implemented with Python Scikit-learn library [41].</p>
<h3>5.2 Baselines and Variants of BiV-HNN</h3>
<p>Baselines. We compare our proposed model with two commonly used heuristics for collecting QC pairs: (1) Select-First: Only treat the first code snippet in an answer post as a solution; (2) Select-All: Treat every code snippet in an answer post as a solution and pair each of them with the question. In addition, we compare our model with traditional classifiers like LR and SVM based on hand-crafted features (Section 4).
Variants of BiV-HNN. First, to evaluate the effectiveness of combining two views (i.e., textual context and code content), we adapt BiV-HNN to consider only one single view: (1) Text-HNN (Figure 3a): In this model, we only utilize textual contexts of a code snippet. We mask all code blocks with a special token CodeBlock and represent them with a unified vector. (2) Code-HNN (Figure 3b): We only feed the output of the token-level code encoder (i.e., $c_{i}$ ) into the "code label prediction" layer in Section 3, and do not model textual contexts. In addition, to evaluate the effect of question $q$ when encoding a code block, we compare BiV-HNN with BiV-HNN-nq, which directly takes the code vector $v_{x}$ as the code block representation $c_{i}$, without associating question $q$, for further learning. These three models are all input-level variants of BiV-HNN.</p>
<p>Second, to evaluate the hierarchical structure in BiV-HNN, we compare it with "flat" RNN models, which model word and code tokens as a single sequence. The comparison is conducted in both text-only and bi-view settings: (1) Text-RNN (Figure 4a): Compared with Text-HNN, we concatenate all words in context blocks $S_{i}$ and $S_{i+1}$ as well as the unified code vector CodeBlock as a single sequence, i.e., $\left{w_{i 1}, \ldots, w_{i, T_{i}}\right.$, CodeBlock, $\left.w_{i+1,1}, \ldots, w_{i+1, T_{i+1}}\right}$, using Bi-GRU RNN. The concatenation of the forward and backward</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Single-view variants of BiV-HNN: (a) Text-HNN, without code content; (b) Code-HNN, without contextual text.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: "Flat"-structure variants of BiV-HNN, without differentiating token- and block-level: (a) Text-RNN; (b) BiV-RNN.</p>
<p>hidden states of CodeBlock is considered as its final semantic vector $z_{i}$, which is then fed into the code label prediction layer. (2) BiV-RNN (Figure 4b): In contrast to BiV-HNN, BiV-RNN models all word and code tokens in $S_{i}$ - $C_{i}$ - $S_{i+1}$ as a single sequence, i.e., $\left{w_{i 1}, \ldots, w_{i T_{i}}, c o_{i 1}, \ldots, c o_{i j}, \ldots, c o_{i,|C_{i}|}, w_{i+1,1}, \ldots, w_{i+1, T_{i+1}}\right}$, where $c o_{i j}$ denotes the $j$-th token in code $C_{i}$ and $\left|C_{i}\right|$ is the number of code tokens in $C_{i}$. BiV-RNN concatenates the last hidden states in two directions as the final semantic vector $z_{i}$ for prediction. We also tried directly "flattening" BiV-HNN by concatenating tokens in $S_{i}-q-C_{i}-S_{i+1}$, but observed worse performance, perhaps because transitioning from $S_{i}$ to question $q$ is less natural.</p>
<p>Finally, at the block level, instead of using an RNN, one may apply a feedforward neural network [47] to the concatenated tokenlevel output $\left[s_{t}, c_{i}, s_{t+1}\right]$. Specifically, the block-level Bi-GRU in BiV-HNN can be replaced with a one-layer ${ }^{5}$ feedforward neural network, denoted as BiV-HFF. Intuitively, modeling the three blocks as a sequence is more consistent with the way humans read a post. We will verify this intuition in experiments.</p>
<p>While there could be other variants of our model, the above ones are related to the most critical designs in BiV-HNN. We only show their performance due to space constraints.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>5.3 Results</p>
<p>Our experimental results in Table 2 show the effectiveness of our BiV-HNN. On both datasets, BiV-HNN substantially outperforms heuristic baselines Select-First and Select-All by more than 15% in $F_{1}$ and accuracy. This demonstrates that our model can collect QC pairs with much higher quality than heuristic methods used in existing research. In addition, when compared with LR and SVM, BiVHNN achieves $7 \%-9 \%$ higher $F_{1}$ and accuracy on Python dataset, and $3 \%-5 \%$ better $F_{1}$ and accuracy on SQL dataset. The gain on SQL data is relatively smaller, probably because interpreting SQL programs is a relatively easier task, implied by the observation that both simple classifiers and BiV-HNN can have around $85 \%$ F1.</p>
<p>Results in Table 3 show the effect of key components in BiVHNN in comparison with alternatives. Due to space constraints, we do not show the accuracy of each model, which has roughly the same pattern as $F_{1}$. We have made the following observations: (1) Single-view variants. BiV-HNN outperforms Text-HNN and CodeHNN by a large margin on both datasets, showing that both views are critical for our task. In particular, by incorporating code content information, BiV-HNN is able to improve Text-HNN by $7 \%$ on Python dataset and around $5 \%$ on SQL dataset in $F_{1}$. (2) No-query variant. On Python dataset, the integration of the question information in BiV-HNN brings $3 \% F_{1}$ improvements over BiV-HNN-nq,</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Python Testing Set</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SQL Testing Set</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">$F_{1}$</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">$F_{1}$</td>
<td style="text-align: center;">Accuracy</td>
</tr>
<tr>
<td style="text-align: center;">Heuristics Baselines</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Select-First</td>
<td style="text-align: center;">0.676</td>
<td style="text-align: center;">0.551</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.663</td>
<td style="text-align: center;">0.755</td>
<td style="text-align: center;">0.517</td>
<td style="text-align: center;">0.613</td>
<td style="text-align: center;">0.620</td>
</tr>
<tr>
<td style="text-align: center;">Select-All</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.642</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.583</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.737</td>
<td style="text-align: center;">0.583</td>
</tr>
<tr>
<td style="text-align: center;">Classifiers based on simple features</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Logistic Regression</td>
<td style="text-align: center;">0.801</td>
<td style="text-align: center;">0.733</td>
<td style="text-align: center;">0.766</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.820</td>
</tr>
<tr>
<td style="text-align: center;">Support Vector Machine</td>
<td style="text-align: center;">0.701</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.753</td>
<td style="text-align: center;">0.748</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">0.858</td>
<td style="text-align: center;">0.850</td>
<td style="text-align: center;">0.824</td>
</tr>
<tr>
<td style="text-align: center;">BiV-HNN</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.876</td>
<td style="text-align: center;">0.841</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.867</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of BiV-HNN and baseline methods.
which shows the effectiveness of associating the question with the code snippet for identifying code answers. For SQL dataset, adding the question gives no obvious benefit, possibly because the code content in each SQL program already carries critical information for making a prediction (e.g., a SQL program containing the command keyword "SELECT" is very likely to be a solution to the given question, regardless of the question content). (3) "Flat"-structure variants. On both datasets, the hierarchical structure leads to $1 \% \sim 2 \%$ improvements against the "flat" structure in both bi-view (BiV-HNN vs. BiV-RNN) and single-view setting (Text-HNN vs. Text-RNN). (4) Non-sequence variant. On Python dataset, BiV-HNN outperforms BiV-HFF by around $2 \%$, showing the block-level Bi-GRU is preferable over feedforward neural networks. The two models get roughly the same performance on SQL, probably because our task is easier in SQL domain than in Python domain as we mentioned earlier.</p>
<p>In summary, our BiV-HNN is much more effective than widelyadopted heuristic baselines and traditional classifiers. The key components in BiV-HNN, such as bi-view inputs, hierarchical structure and block-level sequence encoding, are also empirically justified.
Error Analysis. There are a variety of non-solution roles that a code snippet can play, such as being only one step of a multi-step solution, an input-output example, etc. We observe that more than half of the wrong predictions were false positives (i.e., predicting a non-solution code snippet as a solution), correcting which usually requires integrating information from the entire answer post. For example, when a code snippet is the first step of a multi-step solution, BiV-HNN may mistakenly take it as a complete and standalone solution, since BiV-HNN does not simultaneously take into account follow-up code snippets and their context to make predictions. In addition, BiV-HNN may make mistakes when a correct prediction requires a close examination of the content of a question post (besides its title). Exploring these directions in the future may lead to further improved model performance on this task.
Model Combination. When experimenting with the single-view variants of BiV-HNN, i.e., Text-HNN and Code-HNN, we observed that the three models complement each other in making accurate predictions. For example, on Python validation set, around $70 \%$ mistakes made by Text-HNN or Code-HNN can be corrected by considering predictions from the other two models. Although BiVHNN is built based on both text- and code-based views, $60 \%$ of its wrong predictions can be remedied by Text-HNN and Code-HNN. The same pattern was also observed on SQL dataset.</p>
<p>Therefore, we further tested the effect of combining the three models via a simple heuristic: The label of a code snippet is predicted</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Python Testing Set</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SQL Testing Set</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Prec.</td>
<td style="text-align: center;">Rec.</td>
<td style="text-align: center;">$F_{1}$</td>
<td style="text-align: center;">Prec.</td>
<td style="text-align: center;">Rec.</td>
<td style="text-align: center;">$F_{1}$</td>
</tr>
<tr>
<td style="text-align: center;">Single-view Variants</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Text-HNN</td>
<td style="text-align: center;">0.723</td>
<td style="text-align: center;">0.826</td>
<td style="text-align: center;">0.771</td>
<td style="text-align: center;">0.798</td>
<td style="text-align: center;">0.887</td>
<td style="text-align: center;">0.840</td>
</tr>
<tr>
<td style="text-align: center;">Code-HNN</td>
<td style="text-align: center;">0.770</td>
<td style="text-align: center;">0.859</td>
<td style="text-align: center;">0.812</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.854</td>
<td style="text-align: center;">0.851</td>
</tr>
<tr>
<td style="text-align: center;">No-query Variant</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BiV-HNN-nq</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.810</td>
<td style="text-align: center;">0.883</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.887</td>
</tr>
<tr>
<td style="text-align: center;">"Flat"-structure Variants</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Text-RNN</td>
<td style="text-align: center;">0.693</td>
<td style="text-align: center;">0.824</td>
<td style="text-align: center;">0.753</td>
<td style="text-align: center;">0.773</td>
<td style="text-align: center;">0.894</td>
<td style="text-align: center;">0.829</td>
</tr>
<tr>
<td style="text-align: center;">BiV-RNN</td>
<td style="text-align: center;">0.760</td>
<td style="text-align: center;">0.887</td>
<td style="text-align: center;">0.819</td>
<td style="text-align: center;">0.869</td>
<td style="text-align: center;">0.880</td>
<td style="text-align: center;">0.875</td>
</tr>
<tr>
<td style="text-align: center;">Non-sequence Variant</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BiV-HFF</td>
<td style="text-align: center;">0.787</td>
<td style="text-align: center;">0.859</td>
<td style="text-align: center;">0.822</td>
<td style="text-align: center;">0.845</td>
<td style="text-align: center;">0.939</td>
<td style="text-align: center;">0.889</td>
</tr>
<tr>
<td style="text-align: center;">BiV-HNN</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.876</td>
<td style="text-align: center;">0.841</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;">0.888</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison of BiV-HNN and its variants.
only when the three models agree on it. Using this heuristic, $69.2 \%$ code blocks on the annotated Python testing set are labeled with $0.916 F_{1}$ and 0.911 accuracy. Similarly, on SQL testing set, $78.7 \%$ code blocks are labeled with $0.943 F_{1}$ and 0.926 accuracy. The combined model further improves BiV-HNN by around $6 \%$ while still being able to label a large portion of the code snippets. Thus, we apply this combined model to those SO answer posts that are not manually annotated yet to obtain large-scale QC pairs, to be discussed next.</p>
<h2>6 STAQC: A SYSTEMATICALLY MINED DATASET OF QUESTION-CODE PAIRS</h2>
<p>In this section, we present StaQC (Stack Overflow Question-Code pairs), a large-scale and diverse set of question-code pairs automatically mined using our framework. Under various case studies, we demonstrate that StaQC can greatly help tasks aiming to associate natural language with programming language.</p>
<h3>6.1 Statistics of StaQC</h3>
<p>In Section 5, we showed that a combination of BiV-HNN and its variants can reliably identify standalone code solutions with $&gt;90 \%$ $F_{1}$ and accuracy from a large portion of the testing set. Thus we applied this combined model to all unlabeled multi-code answer posts that correspond to "how-to-do-it" questions in Python and SQL domain, and finally collected $\mathbf{6 0 , 0 8 3}$ and $\mathbf{4 1 , 8 2 6}$ question-code pairs respectively. Additionally, there are 85,294 Python and 75,637 SQL "how-to-do-it" questions whose answer post contains exactly one code snippet. For them, as in [21], we paired the question title</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"># of QC <br> pairs</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Code</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average <br> length</td>
<td style="text-align: center;"># of <br> tokens</td>
<td style="text-align: center;">Average <br> length</td>
<td style="text-align: center;"># of <br> tokens</td>
</tr>
<tr>
<td style="text-align: center;">Python</td>
<td style="text-align: center;">147,546</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">17,635</td>
<td style="text-align: center;">86</td>
<td style="text-align: center;">137,123</td>
</tr>
<tr>
<td style="text-align: center;">SQL</td>
<td style="text-align: center;">119,519</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">9,920</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">21,143</td>
</tr>
</tbody>
</table>
<p>Table 4: Statistics of StaQC.</p>
<p>Question: "How to limit a number to be within a specified range? (Python)"
Code answers:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">def clamp(n, minn, maxn):</th>
<th style="text-align: center;">def clamp(n, minn, maxn):</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">return max(min(maxn, n),</td>
<td style="text-align: center;">if $n&lt;$ minn:</td>
</tr>
<tr>
<td style="text-align: center;">minn)</td>
<td style="text-align: center;">return minn</td>
</tr>
<tr>
<td style="text-align: center;">clamp = lambda n, minn,</td>
<td style="text-align: center;">elif $n&gt;$ maxn:</td>
</tr>
<tr>
<td style="text-align: center;">maxn: max(min(maxn, n),</td>
<td style="text-align: center;">return maxn</td>
</tr>
<tr>
<td style="text-align: center;">minn)</td>
<td style="text-align: center;">else:</td>
</tr>
<tr>
<td style="text-align: center;">(2nd)</td>
<td style="text-align: center;">return n</td>
</tr>
</tbody>
</table>
<p>Figure 5: StaQC contains four alternative code solutions to question "How to limit a number to be within a specified range? (Python)" [38] whose answer post contains five code snippets. The number at the bottom right denotes the position of each code snippet in the answer post.
with the one code snippet as a question-code pair. Together with 2,169 and 2,056 manually annotated QC pairs with label " 1 " for each domain (Table 1), we collected a dataset of 147,546 Python and 119,519 SQL QC pairs, named as StaQC. Table 4 shows its statistics.</p>
<p>Note that we can continue to expand StaQC with minimal efforts, since it is automatically mined by our framework, and more and more posts will be created in SO as time goes by. QC pairs in other programming languages can also be mined similarly to further enrich StaQC beyond Python and SQL domain.</p>
<h3>6.2 Diversity of StaQC</h3>
<p>Besides the large scale, StaQC also enjoys great diversity in the sense that it contains multiple textual descriptions for semantically similar code snippets and multiple code solutions to a question. For example, considering question "How to limit a number to be within a specified range? (Python)" whose answer post contains five code snippets (Figure 5), our framework is able to correctly mine four alternative code answers. Heuristic methods may either miss some of them or mistakenly include a false solution (i.e., the 3rd code snippet). Therefore, our framework is able to obtain more alternative solutions for the same question more accurately. Moreover, Figure 6 shows two question-code pairs included in StaQC, which we easily located by comparing code solutions of relevant questions in SO (i.e., questions manually linked by SO users). Note that the two code snippets have a very similar functionality but two different text descriptions.</p>
<p>Figure 5 and 6 show that StaQC is highly diverse and rich in surface variation. Such a dataset is beneficial for model development. Intuitively, when certain data patterns are not observed in the training phase, a model is less capable to predict them during testing. StaQC can alleviate this issue by enabling a model to learn from alternative code solutions to the same question or from different
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: StaQC has different text descriptions, e.g., "How to find a gap in range in SQL" [37] and "How do I find a "gap" in running counter with SQL?" [36], for two code snippets bearing a similar functionality.
text descriptions to similar code snippets. Next we demonstrate this benefit using an exemplar downstream task.</p>
<h3>6.3 Usage Demo of StaQC on Code Retrieval</h3>
<p>To further demonstrate the usage of StaQC, we employ it to train a deep learning model for the code retrieval task [2, 21, 22]. Given a natural language description and a set of code snippet candidates, the task is to retrieve code snippets that can match the description. In particular, an effective model should rank matched code snippets as high as possible. Models are evaluated by Mean Reciprocal Rank (MRR) [58]. In [21], the authors proposed a neural network based model, CODE-NN, which outputs a matching score between a natural language question and a code snippet. We choose CODE-NN as it is one of the state of the arts for code retrieval and improved previous work by a large margin. For training, the authors collected around 25,870 SQL QC pairs from answer posts containing exactly one code snippet (which is paired with the question title). They manually annotated two datasets DEV and EVAL for choosing the best model parameters and for final evaluation respectively, both containing around 100 QC pairs. The final evaluation is conducted in 20 runs. In each run, for every QC pair in DEV or EVAL, [21] randomly selected 49 code snippets from SO as non-answer candidates, and ranked all 50 code snippets based on their scores output by CODE-NN. The averaged MRR is computed as the final result.
Improved Retrieval Performance. We first trained CODE-NN using the original training set in [21]. We denote this setting as CODE-NN (original). Then we used StaQC to upgrade the training data in two most straightforward ways: (1) We directly took all the 119,519 SQL QC pairs in StaQC to train CODE-NN, denoted as CODE-NN (StaQC). (2) To emphasize the effect of our framework, we just added the 41,826 QC pairs, automatically mined from SO multi-code answer posts, to the original training set and retrained the model, which is denoted as CODE-NN (original + StaQC-multi). In both (1) and (2), questions and code snippets occurring in the DEV/EVAL set were removed from training.</p>
<p>In all three settings, we used the same DEV/EVAL set and the same hyper-parameters as in [21] except the dropout rate, which was chosen from $[0.5,0.7]$ for each model to obtain better performance. Like [21], we decayed the learning rate in each epoch and terminated the training when it was lower than 0.001 . The best</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model Setting</th>
<th style="text-align: center;">MRR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CODE-NN (original)</td>
<td style="text-align: center;">$0.51 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;">CODE-NN (StaQC)*</td>
<td style="text-align: center;">$\mathbf{0 . 5 7} \pm \mathbf{0 . 0 2}$</td>
</tr>
<tr>
<td style="text-align: center;">CODE-NN (original + StaQC-multi)*</td>
<td style="text-align: center;">$0.54 \pm 0.02$</td>
</tr>
</tbody>
</table>
<p>Table 5: Performance of CODE-NN [21] on code retrieval, with and without StaQC for training. *denotes statistically significant w.r.t. CODE-NN (original) under one-tailed Student's t-test $(p&lt;0.05)$.
model was selected as the one achieving the highest average MRR on DEV set. When using this strategy, we observed better results on the EVAL set than those reported in [21] (around 0.44).</p>
<p>Table 5 shows the average MRR score and standard deviation of each model on EVAL set. We can see that directly using StaQC for training leads to a substantial $6 \%$ improvement over using the original dataset in [21]. By adding QC pairs we mined from multi-code posts to the original training data, CODE-NN can be significantly improved by 35 . Note that the performance gains shown here are still conservative, since we adopted the same hyper-parameters and a small evaluation set, in order to see the direct impact of StaQC. Using more challenging evaluation sets and by conducting systematic hyper-parameter selection, we expect models trained on StaQC to be more advantageous. StaQC can also be used to train other code retrieval models besides CODE-NN, as well as models for other related tasks like code generation or annotation.</p>
<h2>7 DISCUSSION AND FUTURE WORK</h2>
<p>Besides boosting relevant tasks using StaQC, future work includes: (1) We currently only consider a code snippet to be a standalone solution or not. In many cases, code snippets in an answer post serve as multiple steps and should be merged to form a complete solution [39]. This is a more challenging task and we leave it to the future. (2) In our experiments, we combined BiV-HNN and its two variants using a simple heuristic to achieve better performance. In the future, one can also use StaQC to retrain the three models, similar to self-training [34], or jointly train the three models in a tritraining framework [63]. (3) One may also employ Convolutional Neural Networks [1, 24, 49], which have shown great power on representation learning, to encode text and code blocks. Moreover, we can consider encoders similar to [31, 33] for capturing the intrinsic structure of programming language.</p>
<h2>8 RELATED WORK</h2>
<p>Language + Code Tasks and Datasets. Tasks that map between natural language and programming language, referred to Language + Code tasks here, such as code annotation and code retrieval/generation, have been popularly studied in recent years [2, 15, 21, 22, 26, 35, 45, 57, 64]. In order to train more advanced yet data-hungry models, researchers have collected data either automatically from online communities [2, 5, 21, 22, 26, 45, 57, 64] or with human intervention [16, 35]. Like our work, [2, 21, 57, 64] utilized SO to collect data. Particularly, [2] merges code snippets in its answer post as the target source code and pair it with the question title. [21] only employs accepted answer posts containing exactly one code snippet. Other interesting datasets include $\sim 19 \mathrm{~K}&lt;$ English pseudo-code, Python code snippet&gt; pairs manually annotated by
[35], and $\sim 114 \mathrm{~K}$ pairs of Python functions and their documentation strings heuristically collected by [5] from GitHub [17]. Unlike their work, we systematically mine high-quality question-code pairs from SO using advanced machine learning models. Our mined dataset StaQC, the largest to date of around 148K Python and 120K SQL question-code pairs, has been shown to be a better resource. Moreover, StaQC is easily expandable in terms of both scale and programming language types.
Recurrent Neural Networks for Sequential Data. Recurrent Neural Networks have shown great success in various natural language tasks [4, 8, 20, 29]. In an RNN, terms are modeled sequentially without discrimination. Recently, in order to handle information at different levels, [25, 48, 54, 60] stack multiple RNNs into a hierarchical structure. For example, [60] incorporates the attention mechanism in a hierarchical RNN model to pick up important words and sentences. Their model finally aggregates all sentence vectors to learn the document representation. In comparison, we utilize the hierarchical structure to first learn the semantic meaning of each block individually, and then predict the label of a code snippet by combining two views: textual context and programming content.
Mining Stack Overflow. Stack Overflow has been the focus of the Mining Software Repositories (MSR) challenge for years [3, 62]. A lot of work [12-14, 32, 56, 59] have been done on exploring the categories of questions, mining source codes, etc. We follow [12, 13, 32] to categorize SO questions into 5 classes but only focus on the "how-to-do-it" type (Section 2). [14, 59] analyzes the quality of code snippets (e.g., readability) or explores "usable" code snippets that could be parsed, compiled and run. Different from their work, we are interested in finding standalone code solutions, which are not necessarily directly parsable, compilable or runnable, but can be semantically paired with questions. To the best of our knowledge, we are the first to study the problem of systematically mining highquality question-code pairs.</p>
<h2>9 CONCLUSION</h2>
<p>This paper explores systematically mining question-code pairs from Stack Overflow, in contrast to heuristically collecting them. We focus on the "how-to-do-it" questions since their answers are more likely to be code solutions. We present the largest-to-date dataset of diversified question-code pairs in Python and SQL domain (StaQC), systematically collected by our framework. StaQC can greatly help downstream tasks aiming to associate natural language with programming language. We will release it together with our source code for future research.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This research was sponsored in part by the Army Research Office under cooperative agreements W911NF-17-1-0412, Fujitsu gift grant, DARPA contract FA8750-13-2-0019, the University of Washington WRF/Cable Professorship, Ohio Supercomputer Center [7], and NSF Grant CNS-1513120. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein.</p>
<h2>REFERENCES</h2>
<p>[1] Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A convolutional attention network for extreme summarization of source code. In ICML. 2091-2100.
[2] Milton Allamanis, Daniel Tarlow, Andrew Gordon, and Yi Wei. 2015. Bimodal modelling of source code and natural language. In ICML. 2123-2132.
[3] Alberto Bacchelli. 2013. Mining Challenge 2013: Stack Overflow. In The 10th Working Conference on Mining Software Repositories. to appear.
[4] Dimitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Machine Translation by Jointly Learning to Align and Translate. CoRR abs/1409.0473 (2014). arXiv:1409.0473 http://arxiv.org/abs/1409.0473
[5] Antonio Valerio Miceli Barone and Rico Sennrich. 2017. A parallel corpus of Python functions and documentation strings for automated code documentation and code generation. arXiv preprint arXiv:1707.02275 (2017).
[6] Brock Angus Campbell and Christoph Treude. 2017. NLP2Code: Code Snippet Content Assist via Natural Language Tasks. arXiv preprint arXiv:1701.05648 (2017).
[7] Ohio Supercomputer Center. 1987. Ohio Supercomputer Center. http://osc.edu/ ark/19495/f5s1ph73. (1987).
[8] Kyunghyun Cho, Bart van Merriënboer, Çağlar Gülçehre, Dimitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. In EMNLP. Association for Computational Linguistics, Doha, Qatar, 1724-1734.
[9] Jacob Cohen. 1960. A coefficient of agreement for nominal scales, Educational and psychological measurement 20, 1 (1960), 37-46.
[10] Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine learning 20, 3 (1995), 273-297.
[11] David R Cox. 1958. The regression analysis of binary sequences. Journal of the Royal Statistical Society. Series B (Methodological) (1958), 215-242.
[12] Lucas Bl, de Souza, Eduardo C Campos, and Marcelo de A Maia. 2014. Ranking crowd knowledge to assist software development. In Proceedings of the 22nd International Conference on Program Comprehension. ACM, 72-82.
[13] Fernanda Madeiral Delfim, Klérisson VR Paixão, Damien Cassou, and Marcelo de Almeida Maia. 2016. Redocumenting APIs with crowd knowledge: a coverage analysis based on question types. Journal of the Brazilian Computer Society 22, 1 (2016), 9.
[14] Maarten Duijn, Adam Kučera, and Alberto Bacchelli. 2015. Quality questions need quality code: classifying code fragments on stack overflow. In Proceedings of the 12th Working Conference on Mining Software Repositories. IEEE Press, 410-413.
[15] Alessandra Giordani and Alessandro Moschitti. 2009. Semantic mapping between natural language questions and SQL queries via syntactic pairing. In International Conference on Application of Natural Language to Information Systems. Springer, 207-221.
[16] Alessandra Giordani and Alessandro Moschitti. 2010. Corpora for Automatically Learning to Map Natural Language Questions into SQL Queries.. In LREC.
[17] GitHub. 2017. GitHub. (2017). https://github.com/
[18] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. 249-256.
[19] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press. http://www.deeplearningbook.org.
[20] Karl Moritz Hermann, Tomas Kočisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In NIPS. 1693-1701.
[21] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing source code using a neural attention model. In ACL, Vol. 1. 20732083.
[22] Iman Keivanloo, Juergen Rilling, and Ying Zou. 2014. Spotting working code examples. In ICSE. ACM, 664-675.
[23] Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).
[24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Classification with Deep Convolutional Neural Networks. In NIPS. F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (Eds.). Curran Associates, Inc., 10971105.
[25] Jineri Li, Minh-Thang Luong, and Dan Jurafsky. 2015. A hierarchical neural autoencoder for paragraphs and documents. arXiv preprint arXiv:1506.01057 (2015).
[26] Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tomáš Kočiský, Andrew Senior, Fumin Wang, and Phil Blunsom. 2016. Latent predictor networks for code generation. arXiv preprint arXiv:1603.06744 (2016).
[27] Edward Loper and Steven Bird. 2002. NLTK: The Natural Language Toolkit. In Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics - Volume 1 (ETNTNLP '02). Association for Computational Linguistics, Stroudsburg, PA, USA, 63-70. https://doi.org/10.3115/1118108.1118117
[28] Pablo Loyola, Edison Marrese-Taylor, and Yutaka Matsuo. 2017. A Neural Architecture for Generating Natural Language Descriptions from Source Code</p>
<p>Changes. arXiv preprint arXiv:1704.04856 (2017).
[29] Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Approaches to Attention-based Neural Machine Translation. In EMNLP.
[30] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS. 3111-3119.
[31] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional neural networks over tree structures for programming language processing. In AAAI.
[32] Seyed Mehdi Nasehi, Jonathan Sillito, Frank Maurer, and Chris Burns. 2012. What makes a good code example?: A study of programming Q\&amp;A in StackOverflow. In Software Maintenance (ICSM), 2012.28th IEEE International Conference on. IEEE, $25-34$.
[33] Anh Tuan Nguyen and Tien N Nguyen. 2015. Graph-based statistical language model for code. In Proceedings of the 37th International Conference on Software Engineering-Volume 1. IEEE Press, 858-868.
[34] Kamal Nigam and Rayid Ghani. 2000. Analyzing the effectiveness and applicability of co-training. In CIKM. ACM, 86-93.
[35] Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2015. Learning to generate pseudo-code from source code using statistical machine translation (t). In Automated Software Engineering (ASE), 2015 30th IEEE/ACM International Conference on. IEEE, 574584.
[36] Stack Overflow. 2017. How do I find a "gap" in running counter with SQL? (2017). https://stackoverflow.com/a/1312137/4941215
[37] Stack Overflow. 2017. How to find a gap in range in SQL. (2017). https: //stackoverflow.com/a/17782635/4941215
[38] Stack Overflow. 2017. How to limit a number to be within a specified range? (Python). (2017). https://stackoverflow.com/a/5996949/4941215
[39] Stack Overflow. 2017. Splitting a dataframe based on column values. (2017). https://stackoverflow.com/a/33973304/4941215
[40] Stack Overflow. 2017. Stack Overflow. (2017). https://stackoverflow.com/
[41] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research 12 (2011), 2825-2830.
[42] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global Vectors for Word Representation.. In EMNLP, Vol. 14. 1532-1543.
[43] Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse TreeBank 2.0. In In Proceedings of LREC.
[44] Maxim Rabinovich, Mitchell Stern, and Dan Klein. 2017. Abstract Syntax Networks for Code Generation and Semantic Parsing. In ACL.
[45] Mukund Raghothaman, Yi Wei, and Youssef Hamadi. 2016. SWIM: synthesizing what I mean: code search and idiomatic snippet synthesis. In ICSE. ACM, 357-367.
[46] Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Sehlam, and Christopher Ré. 2016. Data programming: Creating large training sets, quickly. In NIPS. 35673575.
[47] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. 1988. Learning representations by back-propagating errors. Cognitive modeling 5, 3 (1988), 1.
[48] Julian Vlad Serhan, Alessandro Sordoni, Yoshua Bengio, Aaron C Courville, and Joelle Pineau. 2016. Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models.. In AAAI. 3776-3784.
[49] Yeheng Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Grégoire Mesnil. 2014. A latent semantic model with convolutional-pooling structure for information retrieval. In CIKM. ACM, 101-110.
[50] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).
[51] Inc Stack Exchange. 2017. Stack Exchange Data Dump. (2017). https://archive. org/details/stackexchange
[52] Yu Su, Ahmed Hassan Awadallah, Madian Khabsa, Patrick Pantel, and Michael Gamon. 2017. Building Natural Language Interfaces to Web APIs. In CIKM.
[53] Christian Szegedy, Alexander Toshev, and Dumitru Erhan. 2013. Deep neural networks for object detection. In NIPS. 2553-2561.
[54] Duyu Tang, Bing Qin, and Ting Liu. 2015. Document Modeling with Gated Recurrent Neural Network for Sentiment Classification.. In EMNLP. 1422-1432.
[55] TensorFlow. 2017. TensorFlow. (2017). https://www.tensorflow.org/
[56] Christoph Treude, Ohad Barzilay, and Margaret-Anne Storey. 2011. How do programmers ask and answer questions on the web?: Nier track. In ICSE. IEEE, 804-807.
[57] Venkatesh Vinayakarao, Anita Sarma, Rahul Purandare, Shuktika Jain, and Saumya Jain. 2017. Anne: Improving source code search using entity retrieval approach. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining. ACM, 211-220.
[58] Ellen M Voorhees et al. 1999. The TREC-8 Question Answering Track Report.. In Tree, Vol. 99. 77-82.
[59] Di Yang, Aftab Hussain, and Cristina Videira Lopes. 2016. From query to usable code: an analysis of stack overflow code snippets. In Proceedings of the 13th International Workshop on Mining Software Repositories. ACM, 391-402.</p>
<p>[60] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchical attention networks for document classification. In Proceedings of NAACL-HLT. 1480-1489.
[61] Pengcheng Yin and Graham Neubig. 2017. A Syntactic Neural Model for GeneralPurpose Code Generation. In ACL. Vancouver, Canada.
[62] Annie T. T. Ying. 2015. Mining Challenge 2015: Comparing and combining different information sources on the Stack Overflow data set. In The 12th Working Conference on Mining Software Repositories. to appear.
[63] Zhi-Hua Zhou and Ming Li. 2005. Tri-training: Exploiting unlabeled data using three classifiers. IEEE Transactions on knowledge and Data Engineering 17, 11 (2005), 1529-1541.
[64] Meital Zilberstein and Eran Yahav. 2016. Leveraging a corpus of natural language descriptions for program similarity. In Proceedings of the 2016 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software. ACM, 197-211.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ For fair comparison, we only use one layer since the Bi-GRU in BiV-HNN only has one hidden layer.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>This paper is published under the Creative Commons Attribution 4.0 International (CC BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.
WWW 2018, April 23-27, 2018, Lyon, France
(c) 2018 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC BY 4.0 License.
ACM ISBN 978-1-4503-5639-8/18/04.
https://doi.org/10.1145/3178876.3186081
${ }^{2}$ In SO, an accepted answer post is marked with a green check by the questioner, if he/she thinks it solves the problem. Following previous work [21, 59], although there can be multiple answer posts to a question, we only consider the accepted one because of its verified quality, and use "accepted answer post" and "answer post" interchangeably.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
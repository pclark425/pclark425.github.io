<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2096 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2096</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2096</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-280421049</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.01285v1.pdf" target="_blank">BioDisco: Multi-agent hypothesis generation with dual-mode evidence, iterative feedback and temporal evaluation</a></p>
                <p><strong>Paper Abstract:</strong> Identifying novel hypotheses is essential to scientific research, yet this process risks being overwhelmed by the sheer volume and complexity of available information. Existing automated methods often struggle to generate novel and evidence-grounded hypotheses, lack robust iterative refinement and rarely undergo rigorous temporal evaluation for future discovery potential. To address this, we propose BioDisco, a multi-agent framework that draws upon language model-based reasoning and a dual-mode evidence system (biomedical knowledge graphs and automated literature retrieval) for grounded novelty, integrates an internal scoring and feedback loop for iterative refinement, and validates performance through pioneering temporal and human evaluations and a Bradley-Terry paired comparison model to provide statistically-grounded assessment. Our evaluations demonstrate superior novelty and significance over ablated configurations representative of existing agentic architectures. Designed for flexibility and modularity, BioDisco allows seamless integration of custom language models or knowledge graphs, and can be run with just a few lines of code. We anticipate researchers using this practical tool as a catalyst for the discovery of new hypotheses.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2096.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2096.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BIODISCO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BIODISCO: Multi-agent hypothesis generation with dual-mode evidence, iterative feedback and temporal evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular multi-agent framework that generates novel biomedical hypotheses by combining LLM-based agents (generation + critic + refiner) with dual-mode evidence (a biomedical knowledge graph and real-time PubMed retrieval), iterative self-critique and temporal + human evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BIODISCO</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-agent system (LLM agents + tool interfaces to KG and literature)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>textual scientific hypotheses (multi-entity, mechanistic research claims)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately to highly novel — temporal evaluation shows ability to predict relationships discovered after cutoff dates (semantic similarity median to gold: 0.68 vs unrelated gold 0.34), and ablations show increased novelty scores when tools+refinement are present</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Role-specialized LLM agents (SCIENCIST, BACKGROUND, EXPLORER etc.) synthesize KG subgraphs (PrimeKG via Cypher) and retrieval-augmented literature (PubMed API) under a planner; initial hypotheses are produced by LLMs and then iteratively refined through a CRITIC→REVIEWER→REFINER loop (up to 3 cycles).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Multiple complementary validations: (1) temporal evaluation against held-out future findings (Qi et al. subset, TruthHypo), (2) a classifier agent mapping free-text hypotheses to relation labels (TruthHypo) with standard classification metrics, (3) pairwise LLM evaluation (comparative judgments) analyzed with a Bradley–Terry paired-comparison model (with quasi-variance 95% intervals), and (4) human expert ratings modelled via a Bayesian polytomous Rasch (cumulative probit mixed-effects) model.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Practical metrics reported: CRITIC/direct LLM mean ± SD scores for BIODISCO (Table 5) — Novelty 2.54 ± 0.14, Relevance 2.99 ± 0.05, Significance 2.55 ± 0.14, Verifiability 2.54 ± 0.42 (scale 0-5); ablation Bradley-Terry analysis ranks BIODISCO highest for novelty & significance (centipede plot). Latency/cost: ~2–3 minutes per hypothesis; cost ≈ $0.071 per hypothesis for full pipeline (three refinement iterations) and token counts shown in Table 7.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Classifier on TruthHypo (held-out, 300 samples): overall Precision 0.844, Recall 0.842, F1 0.842, Accuracy 0.850 (Table 4). Classwise averages (Table1/Table4): Chemical-Gene F1 0.90 (precision 0.90, recall 0.90), Disease-Gene F1 0.82, Gene-Gene F1 0.83. Bradley–Terry + 95% comparison intervals and Bayesian posterior distributions for human ratings provide uncertainty; human experts rated refined hypotheses as more novel/significant but not more verifiable.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not directly reported. Paper reports precision per class (e.g., Chemical-Gene precision 0.90, Disease-Gene 0.822, Gene-Gene 0.833) but does not report true negative counts or specificity to compute classical FPR. Therefore exact FPRs are not derivable from reported metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Derivable from recall: overall FNR ≈ 1 - 0.842 = 0.158 on TruthHypo test set. Classwise: Chemical-Gene FNR ≈ 0.10, Disease-Gene FNR ≈ 0.18, Gene-Gene FNR ≈ 0.17 (computed as 1 - recall).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Reported tradeoff: novelty and significance increase with tool use and iterative refinement, but verifiability does not improve proportionally — human ratings and ablation results indicate that as hypotheses become more novel, verifiability (experimental testability) tends to lag or remain unchanged, suggesting validation performance degrades relative to generation for higher-novelty outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Yes — the paper explicitly compares generation capability improvements (novelty, significance) to validation (verifiability) and finds an asymmetry: multi-agent + tools + refinement increases novelty/significance more than verifiability. The authors discuss this generation>validation gap and potential causes (tradeoff between novelty and testability, Goodhart effects, evaluator limitations).</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Uncertainty is quantified at the evaluation layer: Bradley–Terry ability scores reported with 95% comparison intervals via quasi-variance approximation; human ratings analysed with Bayesian models produce posterior distributions for hypothesis-on-metric effects (Figure 6). The system's internal generative uncertainty (LLM confidence/calibration on outputs) is not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Formal calibration of model confidence is not reported. The paper notes evaluator sensitivity and order effects (significant order bias for Novelty and Verifiability) and cautions about LLM-judge reliability, implying imperfect calibration and potential degradation on novel outputs, but no numeric calibration statistics are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Temporal (OOD) evaluation: generated hypotheses have higher semantic alignment to future 'gold' hypotheses than unrelated golds (median cosine similarity 0.68 vs 0.34). On the temporal TruthHypo held-out future data the classifier achieved F1 ≈ 0.842, indicating reasonable OOD generalization under the applied temporal restrictions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — the system relies heavily on proxies: LLM-judge numerical ratings of novelty/relevance/significance/verifiability, cosine similarity to gold hypotheses (pretrained biomedical encoder), and classifier-derived relation labels on textual hypotheses, rather than laboratory experimental falsification.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Humans were used in a targeted evaluation (9 domain experts rated 10 hypotheses each — initial vs refined). The paper recommends human expert validation for practical utility and notes that experimental (laboratory) validation is beyond scope; frequency is not formally prescribed but is implied to increase for higher-novelty outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Empirical biomedical domain (low formalization compared to mathematics); the paper argues this empirical nature contributes to a generation-validation gap because hypotheses can be plausible yet experimentally challenging to verify.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Proposed/implemented strategies include dual-mode grounding (knowledge graph + literature retrieval), iterative CRITIC→REVIEWER→REFINER feedback loop, use of paired LLM evaluator aggregated via Bradley–Terry for stable rankings, temporal held-out evaluation, and Bayesian modelling of human ratings to account for rater bias and uncertainty. Ablation shows combined strategies produce synergistic gains (tools + refinement > each alone).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Multiple pieces of evidence: (1) Ablation and human evaluations show novelty and significance increase more than verifiability; (2) refined hypotheses received higher novelty but not higher verifiability in expert ratings; (3) Discussion explicitly warns of Goodhart effects and potential model artifacts that inflate apparent usefulness without experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Temporal evaluation and TruthHypo classifier results show many generated hypotheses correlate with future discoveries: high semantic similarity to gold hypotheses (median 0.68) and classifier F1 ≈ 0.842 on temporally held-out data, indicating some generated hypotheses are verifiable against subsequent literature and thus partially bridge the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Reported generation cost per hypothesis for full BIODISCO ≈ $0.071 (three refinements); lightweight multi-agent baseline ≈ $0.004. Validation-specific compute costs (LLM pairwise evaluators, Bradley–Terry fitting, human rater time) are not broken out per hypothesis, so an exact validation:generation cost ratio cannot be computed; qualitatively, human validation is far more expensive and validation compute scales with the number of pairwise comparisons (quadratic) so cost per validated hypothesis increases with more stringent validation (e.g., pairwise judges).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2096.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2096.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4.1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4.1 (OpenAI large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model used as the primary backbone for BIODISCO's agents (generation, critic, reviewer, refiner) in the experiments unless otherwise specified; knowledge cutoff stated as 1 June 2024 in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4.1</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general-purpose LLM applied to biomedical hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>textual hypotheses, summaries, scores and planning instructions</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Produces novel textual formulations by recombining knowledge and retrieved evidence; novelty depends on prompt, tools and iterative loop (paper reports GPT-4.1-based agents produced higher-quality outputs when integrated into BIODISCO).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Transformer-based next-token generation conditioned on background, KG summaries and retrieval context; used in specialized agent prompts to generate candidate hypotheses and critique them.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Used as the internal CRITIC and as pairwise/direct evaluator in experiments (LLM-as-judge) producing scores and pairwise comparisons; its outputs are analysed statistically (Bradley–Terry) and compared to human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Used across nearly all agentic roles; generation latency and cost contribute to system-level metrics (see BIODISCO entry). No standalone success rate reported for GPT-4.1 outside the BIODISCO pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>When employed as an evaluator, its judgments were used to build paired-comparison data; the paper notes sensitivity to relative strengths between generator and judge and possible bias but does not provide numeric evaluator accuracy against human ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported for GPT-4.1 as an evaluator; paper warns LLM-judges can be biased and sensitive to generator/judge strength, but no numeric FPR is given.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Paper discusses that LLM-based judges may over- or under-estimate novelty/verifiability depending on capabilities; no quantitative calibration vs novelty curve is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper notes that the relative strengths of generator (GPT-4.1) and judge (LLM) can bias evaluations; direct comparisons show agentic architectures using GPT-4.1 outperform a single LLM baseline in perceived novelty and significance.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>LLM internal token-level probabilities not reported/used; uncertainty is quantified only indirectly via downstream statistical models (Bradley–Terry intervals, Bayesian posteriors).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not measured for GPT-4.1 in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Used in temporally-restricted experiments (knowledge cutoff enforced) and demonstrated ability to produce hypotheses aligned with later discoveries, but no per-model OOD metrics are separated from system-level results.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>When used as evaluator, provides numeric ratings or pairwise choices on novelty/relevance/significance/verifiability (proxy metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Empirical/biomedical</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Using retrieval (RAG) and KG grounding to reduce hallucination when GPT-4.1 generates outputs; iterative refinement with internal critic to improve quality.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Paper cautions that LLM outputs can be plausible but factually groundless; reliance on GPT-4.1 alone (single LLM baseline) produced lower novelty and overall scores than multi-agent setups with tools.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>When GPT-4.1 agents are augmented with KG+literature and iterative loops, system-level validation metrics (temporal classifier F1, human-rated novelty) improve, suggesting GPT-4.1 can contribute to verifiable discoveries when properly augmented.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Generation cost contribution included in BIODISCO per-hypothesis cost; no separate validation cost for GPT-4.1 as judge is provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2096.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2096.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo (restricted)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 Turbo (knowledge cutoff Sep 1, 2021) used to restrict knowledge leakage</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller OpenAI LLM used selectively in one temporal experiment (Qi et al. subset) to avoid knowledge leakage from generator model training data when enforcing a temporal cutoff.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model (smaller generation model)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical hypothesis generation (limited-use to enforce temporal restriction)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>textual hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Constrained by earlier knowledge cutoff to avoid leaking post-cutoff discoveries; novelty relative to that older cutoff not quantified separately.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Same LLM text generation but with earlier model knowledge cutoff to ensure temporal restriction.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Generated outputs were validated via the same BIODISCO pipeline (classifier, pairwise evaluation, human review) as applicable, but used only in one experimental condition to bound knowledge exposure.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>No standalone performance numbers reported for GPT-3.5 Turbo beyond its use to enforce temporal controls.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Not separately reported.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not quantified per-model; used as a control to reduce leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Used to limit generator knowledge and thus make validation (temporal) more stringent; no direct numeric comparison reported.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>N/A beyond system-level proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Empirical/biomedical</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Used as a measure to prevent knowledge leakage (temporal control) thereby making temporal validation more credible.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>No direct evidence; used to tighten experimental controls.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported separately.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2096.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2096.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pairwise LLM Evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based pairwise evaluator used with Bradley–Terry aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based judge that compares two hypotheses and chooses which is better (or tie) across four metrics (novelty, relevance, verifiability, significance); its pairwise outcomes are aggregated with a Bradley–Terry model to estimate ability scores and uncertainties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pairwise LLM evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based comparative evaluator (zero/few-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>evaluation of generated scientific hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>pairwise preference judgments (win/tie/lose) across metrics</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Evaluator judges novelty but is not a generator; its sensitivity to novelty depends on the underlying LLM capability (not numerically separated).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A (evaluative component) — uses prompt-based pairwise comparison of two candidate hypotheses conditioned on metric definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Produces pairwise judgments which are statistically modelled using a Bradley–Terry model (ties treated as half-wins) to obtain continuous ability scores and 95% comparison intervals.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A for generation; as an evaluator it produced pairwise labels for 100 inputs × configuration pairs used in ablation (leading to Bradley–Terry fit).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>No direct accuracy vs humans reported; the paper cites literature showing correlation between LLM evaluators and human judgments but warns of judge-generator relative-strength sensitivity and potential biases (order effects).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not applicable; no explicit FP/FN rates reported for the evaluator judgments relative to human ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Paper notes that LLM-judges may be biased by novelty and strength-discrepancy between systems, but does not quantify how evaluator accuracy changes with hypothesis novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>The pairwise evaluator is used precisely to compare generation outputs from different system configurations; authors explicitly use it to highlight differences in novelty/significance vs verifiability across ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Uncertainty in aggregated judgments provided by Bradley–Terry 95% comparison intervals (quasi-variance approximation).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not measured; authors report significant order bias (Table 6) for Novelty and Verifiability comparisons (α estimates with p-values), indicating evaluator biases exist.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — judges operate on proxies: assessed novelty/relevance/significance/verifiability rather than experimental falsity.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Empirical/biomedical</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Aggregation via Bradley–Terry and reporting of uncertainty intervals to stabilize LLM-judge noise; paired comparisons preferred to direct scoring to better distinguish subtle differences.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors find paired LLM evaluation confirms that tool use and refinement increase novelty/significance more than verifiability, contributing evidence for a generation-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Not directly; pairwise judgments are used to reveal, rather than refute, the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not separately reported; pairwise evaluation scales quadratically with number of hypotheses compared which increases evaluation cost relative to single-shot scoring.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2096.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2096.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TruthHypo Classifier Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Classifier agent used to map generated textual hypotheses to TruthHypo relation labels (positive/negative)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classifier component that converts free-text BIODISCO hypotheses into structured relation labels to enable automatic evaluation on the TruthHypo benchmark; configured using a 30-sample calibration set and then applied to a 300-sample test set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Classifier agent (TruthHypo mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based textual classifier/labeler (agent)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical relation classification</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>binary relation labels (positive/negative) derived from textual hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Mapping tool rather than generative novelty; enables evaluation of generative novelty against structured future-labeled relations.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A (classification) — maps textual hypotheses to one of the relation classes using prompt-based or classifier logic, calibrated on 30 samples.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Performance measured via Precision, Recall, F1 and Accuracy on stratified 300-instance test set (temporal holdout).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Reported: overall Precision 0.844, Recall 0.842, F1 0.842, Accuracy 0.850 (300 test samples). Class-level: Chemical-Gene avg P/R/F1 = 0.90, Disease-Gene avg P/R/F1 ≈ 0.822, Gene-Gene avg P/R/F1 ≈ 0.833 (Table 4 / Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported explicitly; can’t compute FPR without TN counts. Precision gives an indication of proportion of predicted positives that were true positives (e.g., overall precision 0.844).</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Computable from recall: overall FNR ≈ 0.158; classwise FNRs: Chemical-Gene ≈ 0.10, Disease-Gene ≈ 0.18, Gene-Gene ≈ 0.17.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Classifier performance reported on temporally held-out (thus novel relative to training) data and remains high (F1 ≈ 0.842), suggesting the classifier can recover relational signals in generated hypotheses even when they concern future/unseen findings.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Enables a form of automatic validation by translating generation outputs into discrete labels for comparison to future literature; the authors used it to show that BIODISCO-generated hypotheses contained accurate relational signals under strict temporal constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported for classifier outputs (no calibration curves or probabilistic thresholds reported).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Classifier was applied on temporally out-of-sample data and retained high F1 (~0.842), indicating reasonable temporal OOD robustness for this mapping task.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Acts as a proxy validator translating textual hypotheses into binary relation labels, enabling automatic precision/recall computation vs literature-derived labels.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Empirical/biomedical</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Bridges free-text generation and structured benchmarks to permit automated temporal validation; selection of a calibration subset (30 samples) used to configure the classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>N/A — classifier results instead provide evidence that some generated hypotheses match future-labelled relations, partially mitigating the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>High classifier F1 on future data suggests generated hypotheses are not purely hallucinations and can be validated against later literature.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported; classifier inference cost likely small relative to full LLM generation but explicit cost numbers not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2096.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2096.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CRITIC (Direct LLM Evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CRITIC agent / direct LLM evaluator (internal to BIODISCO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An internal LLM-based agent that scores candidate hypotheses along four dimensions (novelty, relevance, significance, verifiability) on a 0–5 scale and provides justifying comments, guiding iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CRITIC agent (direct LLM evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based scoring agent</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical hypothesis assessment</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>numeric scores and textual rationale per hypothesis metric (novelty/relevance/significance/verifiability)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Evaluative only; assigns novelty scores (used to drive refinements).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A — evaluation via prompt-based scoring using retrieved evidence and background.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Internal assessment used to decide refinement actions and final selection; CRITIC scores reported (Table 5) as mean ± SD per configuration to demonstrate improvements across ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>CRITIC scores for full BIODISCO (Table 5): Novelty 2.54 ± 0.14, Relevance 2.99 ± 0.05, Significance 2.55 ± 0.14, Verifiability 2.54 ± 0.42 (scale 0-5); used in the decision module to accept/discard/refine hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not applicable in classical FP/FN sense; CRITIC may mistakenly rate an invalid/hallucinatory hypothesis as high — paper warns of potential self-reinforcement but does not quantify a numeric FP rate for CRITIC.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>CRITIC is used to detect low verifiability and trigger targeted KG/literature retrieval; however, CRITIC ratings improved novelty/significance during refinement while verifiability often remained unchanged, indicating limited ability to convert novelty into verifiability.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>CRITIC is part of the internal loop that attempts to close generation→validation gaps by providing targeted feedback; empirical results show it helps increase novelty/significance but has limited effect on verifiability.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>CRITIC produces point scores and commentary; system-level uncertainties come from downstream statistical models, not from CRITIC-calibrated confidence intervals.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not evaluated quantitatively; concerns over self-evaluation and Goodhart effects are discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>CRITIC uses proxy metrics (novelty, relevance, significance, verifiability) rather than experimental tests.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Empirical/biomedical</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Used to prioritize retrieval and refinement targets (e.g., focus on verifiability) and to allow early exit/discard decisions based on score thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>CRITIC-driven refinements increased novelty and significance but verifiability often remained low, supporting an internal generation→validation asymmetry.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>CRITIC improvements sometimes raised overall scores sufficiently for hypotheses to be accepted and further validated externally (e.g., case studies), but no laboratory validation is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>CRITIC evaluation is an extra LLM call per cycle; full BIODISCO cost ($0.071 per hypothesis) includes CRITIC and multiple refinement passes. Specific cost split not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2096.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2096.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bradley–Terry model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bradley–Terry paired comparison model (with ties and quasi-variance approximation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A statistical paired-comparison model used to aggregate pairwise LLM-evaluator judgments into continuous ability scores with uncertainty (95% comparison intervals via quasi-variance); ties treated as half-wins and order bias modelled via a home-advantage parameter α.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bradley–Terry paired-comparison statistical model</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>statistical inference model for pairwise comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>statistical evaluation of NLG/hypothesis generation systems</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>ability scores (logit-scale) and significance/order-bias estimates with confidence intervals</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Methodological (not a generative novelty claim).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A — statistical aggregation of pairwise judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Fits log-odds model log odds(i beats j) = α + β_i − β_j to pairwise LLM judgments; uses qvcalc quasi-variance approximations for 95% comparison intervals; a Davidson extension tested separately for ties.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Provides uncertainty-aware rankings; Table 6 reports α estimates indicating statistically significant order bias for Novelty and Verifiability (p-values), and centipede plot (Figure 4) shows ability scores across configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Enables assessment of relative performance across novelty-related metrics, but does not itself generate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Used to compare generation system configurations' validation outcomes (via pairwise judgments), revealing the asymmetry between generation improvements and validation (verifiability) gains.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Yes — quasi-variance derived 95% comparison intervals reported for ability scores.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Statistical diagnostic (relative error of quasi-variance approximation) plotted (Figure 7); authors report mostly acceptable relative errors.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Aggregates LLM-proxy judgments (novelty/relevance/significance/verifiability).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Statistical evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Statistical aggregation with uncertainty to reduce noise and provide principled comparisons across system variants.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Bradley–Terry results show consistent ordering where systems with tools/refinement score higher on novelty/significance than on verifiability, quantifying the generation→validation mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>The statistical model itself does not contradict results; it quantifies them and reports overlapping intervals for relevance/verifiability in some comparisons (no clear winner).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Computational cost is modest (statistical fit), but the required pairwise judgments scale with number of hypotheses (O(n^2)) increasing upstream evaluator costs; numerical cost ratio not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2096.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2096.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Expert Evaluation (Bayesian Rasch)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human expert ratings modelled via a Bayesian polytomous Rasch (cumulative probit mixed-effects) model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-in-the-loop validation where domain experts rated hypotheses on novelty, relevance, significance and verifiability (1–5); responses were modelled using a Bayesian cumulative probit mixed-effects (Rasch-like) model to account for rater bias and provide posterior uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Human expert evaluation with Bayesian item-response modelling</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>human evaluation + Bayesian psychometric model (brms/Stan)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical hypothesis appraisal</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>ordinal ratings per hypothesis per metric and posterior distributions for hypothesis-on-metric effects</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Human judgment of novelty; results show refined hypotheses judged more novel in immunology and CVD groups.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A (human evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Experts (9 total: 4 immunology, 5 cardiovascular) rated hypotheses; ratings modelled with a Bayesian cumulative probit mixed-effects model, estimating metric effects, rater leniency (u_i) and hypothesis-on-metric effects (v_jm) and generating posterior distributions (Figure 6).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Posterior distributions show clear improvement in novelty after iterative refinement; verifiability did not show consistent improvement despite refinement. Quantitative posterior summaries are plotted but exact numeric posteriors per hypothesis are in Appendix/figures.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not framed in FP/FN terms; experts may over- or under-rate verifiability relative to ultimate experimental truth. No numeric FP/FN rates reported.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Direct evidence that higher novelty does not necessarily produce higher verifiability in human judgments; refinement raised novelty but verifiability scores remained largely unchanged.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Human ratings serve as an orthogonal validation channel and confirm the observed asymmetry between novelty gains and verifiability gains seen under automated evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Yes — Bayesian model yields posterior distributions and threshold estimates, and random-effects capture rater variability (σ_u) and hypothesis-specific variability (σ_v).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>The model accounts for rater leniency/severity; however, calibration of rater judgments to true experimental outcomes is not measured (no lab validation), so calibration vs ground truth is unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Human evaluation targeted particular topical prompts (immunology and CVD); claims about OOD performance not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Human ratings are still proxies for experimental validity (especially verifiability) rather than direct laboratory confirmation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Used on a modest pilot scale here (9 experts, ≈10 hypotheses each); authors recommend expert assessment for practical use and suggest experimental validation for high-impact hypotheses but do not prescribe a general frequency policy.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Empirical/biomedical</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Using Bayesian psychometric modelling to adjust for rater bias and quantify uncertainty to produce more reliable human-based validation signals.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Experts rated refined items as more novel but not more verifiable, supporting the generation>validation gap for novel outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Experts provided qualitative feedback that some hypotheses were experimentally tractable, and the case study showed concrete experimental suggestions, indicating that some generated hypotheses are testable and the gap is not absolute.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Human evaluation cost (time, expert labor) is substantially higher than automatic generation cost; exact monetary ratios are not reported, but authors note experimental validation is far more expensive and beyond the paper's scope.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation <em>(Rating: 2)</em></li>
                <li>Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models <em>(Rating: 2)</em></li>
                <li>LLM comparative assessment: Zero-shot NLG evaluation through pairwise comparisons using large language models <em>(Rating: 2)</em></li>
                <li>Automated Hypothesis Validation with Agentic Sequential Falsifications <em>(Rating: 2)</em></li>
                <li>AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery <em>(Rating: 1)</em></li>
                <li>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2096",
    "paper_id": "paper-280421049",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "BIODISCO",
            "name_full": "BIODISCO: Multi-agent hypothesis generation with dual-mode evidence, iterative feedback and temporal evaluation",
            "brief_description": "A modular multi-agent framework that generates novel biomedical hypotheses by combining LLM-based agents (generation + critic + refiner) with dual-mode evidence (a biomedical knowledge graph and real-time PubMed retrieval), iterative self-critique and temporal + human evaluations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "BIODISCO",
            "system_type": "multi-agent system (LLM agents + tool interfaces to KG and literature)",
            "scientific_domain": "biomedical hypothesis generation",
            "output_type": "textual scientific hypotheses (multi-entity, mechanistic research claims)",
            "novelty_level": "moderately to highly novel — temporal evaluation shows ability to predict relationships discovered after cutoff dates (semantic similarity median to gold: 0.68 vs unrelated gold 0.34), and ablations show increased novelty scores when tools+refinement are present",
            "generation_method": "Role-specialized LLM agents (SCIENCIST, BACKGROUND, EXPLORER etc.) synthesize KG subgraphs (PrimeKG via Cypher) and retrieval-augmented literature (PubMed API) under a planner; initial hypotheses are produced by LLMs and then iteratively refined through a CRITIC→REVIEWER→REFINER loop (up to 3 cycles).",
            "validation_method": "Multiple complementary validations: (1) temporal evaluation against held-out future findings (Qi et al. subset, TruthHypo), (2) a classifier agent mapping free-text hypotheses to relation labels (TruthHypo) with standard classification metrics, (3) pairwise LLM evaluation (comparative judgments) analyzed with a Bradley–Terry paired-comparison model (with quasi-variance 95% intervals), and (4) human expert ratings modelled via a Bayesian polytomous Rasch (cumulative probit mixed-effects) model.",
            "generation_performance": "Practical metrics reported: CRITIC/direct LLM mean ± SD scores for BIODISCO (Table 5) — Novelty 2.54 ± 0.14, Relevance 2.99 ± 0.05, Significance 2.55 ± 0.14, Verifiability 2.54 ± 0.42 (scale 0-5); ablation Bradley-Terry analysis ranks BIODISCO highest for novelty & significance (centipede plot). Latency/cost: ~2–3 minutes per hypothesis; cost ≈ $0.071 per hypothesis for full pipeline (three refinement iterations) and token counts shown in Table 7.",
            "validation_performance": "Classifier on TruthHypo (held-out, 300 samples): overall Precision 0.844, Recall 0.842, F1 0.842, Accuracy 0.850 (Table 4). Classwise averages (Table1/Table4): Chemical-Gene F1 0.90 (precision 0.90, recall 0.90), Disease-Gene F1 0.82, Gene-Gene F1 0.83. Bradley–Terry + 95% comparison intervals and Bayesian posterior distributions for human ratings provide uncertainty; human experts rated refined hypotheses as more novel/significant but not more verifiable.",
            "false_positive_rate": "Not directly reported. Paper reports precision per class (e.g., Chemical-Gene precision 0.90, Disease-Gene 0.822, Gene-Gene 0.833) but does not report true negative counts or specificity to compute classical FPR. Therefore exact FPRs are not derivable from reported metrics.",
            "false_negative_rate": "Derivable from recall: overall FNR ≈ 1 - 0.842 = 0.158 on TruthHypo test set. Classwise: Chemical-Gene FNR ≈ 0.10, Disease-Gene FNR ≈ 0.18, Gene-Gene FNR ≈ 0.17 (computed as 1 - recall).",
            "performance_vs_novelty": "Reported tradeoff: novelty and significance increase with tool use and iterative refinement, but verifiability does not improve proportionally — human ratings and ablation results indicate that as hypotheses become more novel, verifiability (experimental testability) tends to lag or remain unchanged, suggesting validation performance degrades relative to generation for higher-novelty outputs.",
            "generation_validation_comparison": "Yes — the paper explicitly compares generation capability improvements (novelty, significance) to validation (verifiability) and finds an asymmetry: multi-agent + tools + refinement increases novelty/significance more than verifiability. The authors discuss this generation&gt;validation gap and potential causes (tradeoff between novelty and testability, Goodhart effects, evaluator limitations).",
            "uncertainty_quantification": "Uncertainty is quantified at the evaluation layer: Bradley–Terry ability scores reported with 95% comparison intervals via quasi-variance approximation; human ratings analysed with Bayesian models produce posterior distributions for hypothesis-on-metric effects (Figure 6). The system's internal generative uncertainty (LLM confidence/calibration on outputs) is not reported.",
            "calibration_quality": "Formal calibration of model confidence is not reported. The paper notes evaluator sensitivity and order effects (significant order bias for Novelty and Verifiability) and cautions about LLM-judge reliability, implying imperfect calibration and potential degradation on novel outputs, but no numeric calibration statistics are provided.",
            "out_of_distribution_performance": "Temporal (OOD) evaluation: generated hypotheses have higher semantic alignment to future 'gold' hypotheses than unrelated golds (median cosine similarity 0.68 vs 0.34). On the temporal TruthHypo held-out future data the classifier achieved F1 ≈ 0.842, indicating reasonable OOD generalization under the applied temporal restrictions.",
            "validation_proxy_metrics": "Yes — the system relies heavily on proxies: LLM-judge numerical ratings of novelty/relevance/significance/verifiability, cosine similarity to gold hypotheses (pretrained biomedical encoder), and classifier-derived relation labels on textual hypotheses, rather than laboratory experimental falsification.",
            "human_validation_required": true,
            "human_validation_frequency": "Humans were used in a targeted evaluation (9 domain experts rated 10 hypotheses each — initial vs refined). The paper recommends human expert validation for practical utility and notes that experimental (laboratory) validation is beyond scope; frequency is not formally prescribed but is implied to increase for higher-novelty outputs.",
            "formal_verification_used": false,
            "domain_formalization_level": "Empirical biomedical domain (low formalization compared to mathematics); the paper argues this empirical nature contributes to a generation-validation gap because hypotheses can be plausible yet experimentally challenging to verify.",
            "gap_mitigation_strategies": "Proposed/implemented strategies include dual-mode grounding (knowledge graph + literature retrieval), iterative CRITIC→REVIEWER→REFINER feedback loop, use of paired LLM evaluator aggregated via Bradley–Terry for stable rankings, temporal held-out evaluation, and Bayesian modelling of human ratings to account for rater bias and uncertainty. Ablation shows combined strategies produce synergistic gains (tools + refinement &gt; each alone).",
            "evidence_supporting_gap": "Multiple pieces of evidence: (1) Ablation and human evaluations show novelty and significance increase more than verifiability; (2) refined hypotheses received higher novelty but not higher verifiability in expert ratings; (3) Discussion explicitly warns of Goodhart effects and potential model artifacts that inflate apparent usefulness without experimental validation.",
            "evidence_contradicting_gap": "Temporal evaluation and TruthHypo classifier results show many generated hypotheses correlate with future discoveries: high semantic similarity to gold hypotheses (median 0.68) and classifier F1 ≈ 0.842 on temporally held-out data, indicating some generated hypotheses are verifiable against subsequent literature and thus partially bridge the gap.",
            "computational_cost_ratio": "Reported generation cost per hypothesis for full BIODISCO ≈ $0.071 (three refinements); lightweight multi-agent baseline ≈ $0.004. Validation-specific compute costs (LLM pairwise evaluators, Bradley–Terry fitting, human rater time) are not broken out per hypothesis, so an exact validation:generation cost ratio cannot be computed; qualitatively, human validation is far more expensive and validation compute scales with the number of pairwise comparisons (quadratic) so cost per validated hypothesis increases with more stringent validation (e.g., pairwise judges).",
            "uuid": "e2096.0"
        },
        {
            "name_short": "GPT-4.1",
            "name_full": "GPT-4.1 (OpenAI large language model)",
            "brief_description": "A large language model used as the primary backbone for BIODISCO's agents (generation, critic, reviewer, refiner) in the experiments unless otherwise specified; knowledge cutoff stated as 1 June 2024 in paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4.1",
            "system_type": "large language model",
            "scientific_domain": "general-purpose LLM applied to biomedical hypothesis generation",
            "output_type": "textual hypotheses, summaries, scores and planning instructions",
            "novelty_level": "Produces novel textual formulations by recombining knowledge and retrieved evidence; novelty depends on prompt, tools and iterative loop (paper reports GPT-4.1-based agents produced higher-quality outputs when integrated into BIODISCO).",
            "generation_method": "Transformer-based next-token generation conditioned on background, KG summaries and retrieval context; used in specialized agent prompts to generate candidate hypotheses and critique them.",
            "validation_method": "Used as the internal CRITIC and as pairwise/direct evaluator in experiments (LLM-as-judge) producing scores and pairwise comparisons; its outputs are analysed statistically (Bradley–Terry) and compared to human ratings.",
            "generation_performance": "Used across nearly all agentic roles; generation latency and cost contribute to system-level metrics (see BIODISCO entry). No standalone success rate reported for GPT-4.1 outside the BIODISCO pipeline.",
            "validation_performance": "When employed as an evaluator, its judgments were used to build paired-comparison data; the paper notes sensitivity to relative strengths between generator and judge and possible bias but does not provide numeric evaluator accuracy against human ground truth.",
            "false_positive_rate": "Not reported for GPT-4.1 as an evaluator; paper warns LLM-judges can be biased and sensitive to generator/judge strength, but no numeric FPR is given.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Paper discusses that LLM-based judges may over- or under-estimate novelty/verifiability depending on capabilities; no quantitative calibration vs novelty curve is reported.",
            "generation_validation_comparison": "Paper notes that the relative strengths of generator (GPT-4.1) and judge (LLM) can bias evaluations; direct comparisons show agentic architectures using GPT-4.1 outperform a single LLM baseline in perceived novelty and significance.",
            "uncertainty_quantification": "LLM internal token-level probabilities not reported/used; uncertainty is quantified only indirectly via downstream statistical models (Bradley–Terry intervals, Bayesian posteriors).",
            "calibration_quality": "Not measured for GPT-4.1 in this work.",
            "out_of_distribution_performance": "Used in temporally-restricted experiments (knowledge cutoff enforced) and demonstrated ability to produce hypotheses aligned with later discoveries, but no per-model OOD metrics are separated from system-level results.",
            "validation_proxy_metrics": "When used as evaluator, provides numeric ratings or pairwise choices on novelty/relevance/significance/verifiability (proxy metrics).",
            "human_validation_required": null,
            "human_validation_frequency": "",
            "formal_verification_used": false,
            "domain_formalization_level": "Empirical/biomedical",
            "gap_mitigation_strategies": "Using retrieval (RAG) and KG grounding to reduce hallucination when GPT-4.1 generates outputs; iterative refinement with internal critic to improve quality.",
            "evidence_supporting_gap": "Paper cautions that LLM outputs can be plausible but factually groundless; reliance on GPT-4.1 alone (single LLM baseline) produced lower novelty and overall scores than multi-agent setups with tools.",
            "evidence_contradicting_gap": "When GPT-4.1 agents are augmented with KG+literature and iterative loops, system-level validation metrics (temporal classifier F1, human-rated novelty) improve, suggesting GPT-4.1 can contribute to verifiable discoveries when properly augmented.",
            "computational_cost_ratio": "Generation cost contribution included in BIODISCO per-hypothesis cost; no separate validation cost for GPT-4.1 as judge is provided.",
            "uuid": "e2096.1"
        },
        {
            "name_short": "GPT-3.5-Turbo (restricted)",
            "name_full": "GPT-3.5 Turbo (knowledge cutoff Sep 1, 2021) used to restrict knowledge leakage",
            "brief_description": "A smaller OpenAI LLM used selectively in one temporal experiment (Qi et al. subset) to avoid knowledge leakage from generator model training data when enforcing a temporal cutoff.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-3.5 Turbo",
            "system_type": "large language model (smaller generation model)",
            "scientific_domain": "biomedical hypothesis generation (limited-use to enforce temporal restriction)",
            "output_type": "textual hypotheses",
            "novelty_level": "Constrained by earlier knowledge cutoff to avoid leaking post-cutoff discoveries; novelty relative to that older cutoff not quantified separately.",
            "generation_method": "Same LLM text generation but with earlier model knowledge cutoff to ensure temporal restriction.",
            "validation_method": "Generated outputs were validated via the same BIODISCO pipeline (classifier, pairwise evaluation, human review) as applicable, but used only in one experimental condition to bound knowledge exposure.",
            "generation_performance": "No standalone performance numbers reported for GPT-3.5 Turbo beyond its use to enforce temporal controls.",
            "validation_performance": "Not separately reported.",
            "false_positive_rate": "Not reported.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Not quantified per-model; used as a control to reduce leakage.",
            "generation_validation_comparison": "Used to limit generator knowledge and thus make validation (temporal) more stringent; no direct numeric comparison reported.",
            "uncertainty_quantification": "Not reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not reported.",
            "validation_proxy_metrics": "N/A beyond system-level proxies.",
            "human_validation_required": null,
            "human_validation_frequency": "",
            "formal_verification_used": false,
            "domain_formalization_level": "Empirical/biomedical",
            "gap_mitigation_strategies": "Used as a measure to prevent knowledge leakage (temporal control) thereby making temporal validation more credible.",
            "evidence_supporting_gap": "No direct evidence; used to tighten experimental controls.",
            "evidence_contradicting_gap": "N/A",
            "computational_cost_ratio": "Not reported separately.",
            "uuid": "e2096.2"
        },
        {
            "name_short": "Pairwise LLM Evaluator",
            "name_full": "LLM-based pairwise evaluator used with Bradley–Terry aggregation",
            "brief_description": "An LLM-based judge that compares two hypotheses and chooses which is better (or tie) across four metrics (novelty, relevance, verifiability, significance); its pairwise outcomes are aggregated with a Bradley–Terry model to estimate ability scores and uncertainties.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Pairwise LLM evaluator",
            "system_type": "LLM-based comparative evaluator (zero/few-shot prompting)",
            "scientific_domain": "evaluation of generated scientific hypotheses",
            "output_type": "pairwise preference judgments (win/tie/lose) across metrics",
            "novelty_level": "Evaluator judges novelty but is not a generator; its sensitivity to novelty depends on the underlying LLM capability (not numerically separated).",
            "generation_method": "N/A (evaluative component) — uses prompt-based pairwise comparison of two candidate hypotheses conditioned on metric definitions.",
            "validation_method": "Produces pairwise judgments which are statistically modelled using a Bradley–Terry model (ties treated as half-wins) to obtain continuous ability scores and 95% comparison intervals.",
            "generation_performance": "N/A for generation; as an evaluator it produced pairwise labels for 100 inputs × configuration pairs used in ablation (leading to Bradley–Terry fit).",
            "validation_performance": "No direct accuracy vs humans reported; the paper cites literature showing correlation between LLM evaluators and human judgments but warns of judge-generator relative-strength sensitivity and potential biases (order effects).",
            "false_positive_rate": "Not applicable; no explicit FP/FN rates reported for the evaluator judgments relative to human ground truth.",
            "false_negative_rate": "Not applicable.",
            "performance_vs_novelty": "Paper notes that LLM-judges may be biased by novelty and strength-discrepancy between systems, but does not quantify how evaluator accuracy changes with hypothesis novelty.",
            "generation_validation_comparison": "The pairwise evaluator is used precisely to compare generation outputs from different system configurations; authors explicitly use it to highlight differences in novelty/significance vs verifiability across ablations.",
            "uncertainty_quantification": "Uncertainty in aggregated judgments provided by Bradley–Terry 95% comparison intervals (quasi-variance approximation).",
            "calibration_quality": "Not measured; authors report significant order bias (Table 6) for Novelty and Verifiability comparisons (α estimates with p-values), indicating evaluator biases exist.",
            "out_of_distribution_performance": "Not reported.",
            "validation_proxy_metrics": "Yes — judges operate on proxies: assessed novelty/relevance/significance/verifiability rather than experimental falsity.",
            "human_validation_required": null,
            "human_validation_frequency": "",
            "formal_verification_used": false,
            "domain_formalization_level": "Empirical/biomedical",
            "gap_mitigation_strategies": "Aggregation via Bradley–Terry and reporting of uncertainty intervals to stabilize LLM-judge noise; paired comparisons preferred to direct scoring to better distinguish subtle differences.",
            "evidence_supporting_gap": "Authors find paired LLM evaluation confirms that tool use and refinement increase novelty/significance more than verifiability, contributing evidence for a generation-validation gap.",
            "evidence_contradicting_gap": "Not directly; pairwise judgments are used to reveal, rather than refute, the gap.",
            "computational_cost_ratio": "Not separately reported; pairwise evaluation scales quadratically with number of hypotheses compared which increases evaluation cost relative to single-shot scoring.",
            "uuid": "e2096.3"
        },
        {
            "name_short": "TruthHypo Classifier Agent",
            "name_full": "Classifier agent used to map generated textual hypotheses to TruthHypo relation labels (positive/negative)",
            "brief_description": "A classifier component that converts free-text BIODISCO hypotheses into structured relation labels to enable automatic evaluation on the TruthHypo benchmark; configured using a 30-sample calibration set and then applied to a 300-sample test set.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Classifier agent (TruthHypo mapping)",
            "system_type": "LLM-based textual classifier/labeler (agent)",
            "scientific_domain": "biomedical relation classification",
            "output_type": "binary relation labels (positive/negative) derived from textual hypotheses",
            "novelty_level": "Mapping tool rather than generative novelty; enables evaluation of generative novelty against structured future-labeled relations.",
            "generation_method": "N/A (classification) — maps textual hypotheses to one of the relation classes using prompt-based or classifier logic, calibrated on 30 samples.",
            "validation_method": "Performance measured via Precision, Recall, F1 and Accuracy on stratified 300-instance test set (temporal holdout).",
            "generation_performance": "N/A",
            "validation_performance": "Reported: overall Precision 0.844, Recall 0.842, F1 0.842, Accuracy 0.850 (300 test samples). Class-level: Chemical-Gene avg P/R/F1 = 0.90, Disease-Gene avg P/R/F1 ≈ 0.822, Gene-Gene avg P/R/F1 ≈ 0.833 (Table 4 / Table 1).",
            "false_positive_rate": "Not reported explicitly; can’t compute FPR without TN counts. Precision gives an indication of proportion of predicted positives that were true positives (e.g., overall precision 0.844).",
            "false_negative_rate": "Computable from recall: overall FNR ≈ 0.158; classwise FNRs: Chemical-Gene ≈ 0.10, Disease-Gene ≈ 0.18, Gene-Gene ≈ 0.17.",
            "performance_vs_novelty": "Classifier performance reported on temporally held-out (thus novel relative to training) data and remains high (F1 ≈ 0.842), suggesting the classifier can recover relational signals in generated hypotheses even when they concern future/unseen findings.",
            "generation_validation_comparison": "Enables a form of automatic validation by translating generation outputs into discrete labels for comparison to future literature; the authors used it to show that BIODISCO-generated hypotheses contained accurate relational signals under strict temporal constraints.",
            "uncertainty_quantification": "Not reported for classifier outputs (no calibration curves or probabilistic thresholds reported).",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Classifier was applied on temporally out-of-sample data and retained high F1 (~0.842), indicating reasonable temporal OOD robustness for this mapping task.",
            "validation_proxy_metrics": "Acts as a proxy validator translating textual hypotheses into binary relation labels, enabling automatic precision/recall computation vs literature-derived labels.",
            "human_validation_required": null,
            "human_validation_frequency": "",
            "formal_verification_used": false,
            "domain_formalization_level": "Empirical/biomedical",
            "gap_mitigation_strategies": "Bridges free-text generation and structured benchmarks to permit automated temporal validation; selection of a calibration subset (30 samples) used to configure the classifier.",
            "evidence_supporting_gap": "N/A — classifier results instead provide evidence that some generated hypotheses match future-labelled relations, partially mitigating the gap.",
            "evidence_contradicting_gap": "High classifier F1 on future data suggests generated hypotheses are not purely hallucinations and can be validated against later literature.",
            "computational_cost_ratio": "Not reported; classifier inference cost likely small relative to full LLM generation but explicit cost numbers not provided.",
            "uuid": "e2096.4"
        },
        {
            "name_short": "CRITIC (Direct LLM Evaluator)",
            "name_full": "CRITIC agent / direct LLM evaluator (internal to BIODISCO)",
            "brief_description": "An internal LLM-based agent that scores candidate hypotheses along four dimensions (novelty, relevance, significance, verifiability) on a 0–5 scale and provides justifying comments, guiding iterative refinement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CRITIC agent (direct LLM evaluator)",
            "system_type": "LLM-based scoring agent",
            "scientific_domain": "biomedical hypothesis assessment",
            "output_type": "numeric scores and textual rationale per hypothesis metric (novelty/relevance/significance/verifiability)",
            "novelty_level": "Evaluative only; assigns novelty scores (used to drive refinements).",
            "generation_method": "N/A — evaluation via prompt-based scoring using retrieved evidence and background.",
            "validation_method": "Internal assessment used to decide refinement actions and final selection; CRITIC scores reported (Table 5) as mean ± SD per configuration to demonstrate improvements across ablations.",
            "generation_performance": "N/A",
            "validation_performance": "CRITIC scores for full BIODISCO (Table 5): Novelty 2.54 ± 0.14, Relevance 2.99 ± 0.05, Significance 2.55 ± 0.14, Verifiability 2.54 ± 0.42 (scale 0-5); used in the decision module to accept/discard/refine hypotheses.",
            "false_positive_rate": "Not applicable in classical FP/FN sense; CRITIC may mistakenly rate an invalid/hallucinatory hypothesis as high — paper warns of potential self-reinforcement but does not quantify a numeric FP rate for CRITIC.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "CRITIC is used to detect low verifiability and trigger targeted KG/literature retrieval; however, CRITIC ratings improved novelty/significance during refinement while verifiability often remained unchanged, indicating limited ability to convert novelty into verifiability.",
            "generation_validation_comparison": "CRITIC is part of the internal loop that attempts to close generation→validation gaps by providing targeted feedback; empirical results show it helps increase novelty/significance but has limited effect on verifiability.",
            "uncertainty_quantification": "CRITIC produces point scores and commentary; system-level uncertainties come from downstream statistical models, not from CRITIC-calibrated confidence intervals.",
            "calibration_quality": "Not evaluated quantitatively; concerns over self-evaluation and Goodhart effects are discussed.",
            "out_of_distribution_performance": "Not reported.",
            "validation_proxy_metrics": "CRITIC uses proxy metrics (novelty, relevance, significance, verifiability) rather than experimental tests.",
            "human_validation_required": null,
            "human_validation_frequency": "",
            "formal_verification_used": false,
            "domain_formalization_level": "Empirical/biomedical",
            "gap_mitigation_strategies": "Used to prioritize retrieval and refinement targets (e.g., focus on verifiability) and to allow early exit/discard decisions based on score thresholds.",
            "evidence_supporting_gap": "CRITIC-driven refinements increased novelty and significance but verifiability often remained low, supporting an internal generation→validation asymmetry.",
            "evidence_contradicting_gap": "CRITIC improvements sometimes raised overall scores sufficiently for hypotheses to be accepted and further validated externally (e.g., case studies), but no laboratory validation is reported.",
            "computational_cost_ratio": "CRITIC evaluation is an extra LLM call per cycle; full BIODISCO cost ($0.071 per hypothesis) includes CRITIC and multiple refinement passes. Specific cost split not provided.",
            "uuid": "e2096.5"
        },
        {
            "name_short": "Bradley–Terry model",
            "name_full": "Bradley–Terry paired comparison model (with ties and quasi-variance approximation)",
            "brief_description": "A statistical paired-comparison model used to aggregate pairwise LLM-evaluator judgments into continuous ability scores with uncertainty (95% comparison intervals via quasi-variance); ties treated as half-wins and order bias modelled via a home-advantage parameter α.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Bradley–Terry paired-comparison statistical model",
            "system_type": "statistical inference model for pairwise comparisons",
            "scientific_domain": "statistical evaluation of NLG/hypothesis generation systems",
            "output_type": "ability scores (logit-scale) and significance/order-bias estimates with confidence intervals",
            "novelty_level": "Methodological (not a generative novelty claim).",
            "generation_method": "N/A — statistical aggregation of pairwise judgments.",
            "validation_method": "Fits log-odds model log odds(i beats j) = α + β_i − β_j to pairwise LLM judgments; uses qvcalc quasi-variance approximations for 95% comparison intervals; a Davidson extension tested separately for ties.",
            "generation_performance": "N/A",
            "validation_performance": "Provides uncertainty-aware rankings; Table 6 reports α estimates indicating statistically significant order bias for Novelty and Verifiability (p-values), and centipede plot (Figure 4) shows ability scores across configurations.",
            "false_positive_rate": "Not applicable.",
            "false_negative_rate": "Not applicable.",
            "performance_vs_novelty": "Enables assessment of relative performance across novelty-related metrics, but does not itself generate hypotheses.",
            "generation_validation_comparison": "Used to compare generation system configurations' validation outcomes (via pairwise judgments), revealing the asymmetry between generation improvements and validation (verifiability) gains.",
            "uncertainty_quantification": "Yes — quasi-variance derived 95% comparison intervals reported for ability scores.",
            "calibration_quality": "Statistical diagnostic (relative error of quasi-variance approximation) plotted (Figure 7); authors report mostly acceptable relative errors.",
            "out_of_distribution_performance": "Not applicable.",
            "validation_proxy_metrics": "Aggregates LLM-proxy judgments (novelty/relevance/significance/verifiability).",
            "human_validation_required": null,
            "human_validation_frequency": "",
            "formal_verification_used": false,
            "domain_formalization_level": "Statistical evaluation",
            "gap_mitigation_strategies": "Statistical aggregation with uncertainty to reduce noise and provide principled comparisons across system variants.",
            "evidence_supporting_gap": "Bradley–Terry results show consistent ordering where systems with tools/refinement score higher on novelty/significance than on verifiability, quantifying the generation→validation mismatch.",
            "evidence_contradicting_gap": "The statistical model itself does not contradict results; it quantifies them and reports overlapping intervals for relevance/verifiability in some comparisons (no clear winner).",
            "computational_cost_ratio": "Computational cost is modest (statistical fit), but the required pairwise judgments scale with number of hypotheses (O(n^2)) increasing upstream evaluator costs; numerical cost ratio not reported.",
            "uuid": "e2096.6"
        },
        {
            "name_short": "Human Expert Evaluation (Bayesian Rasch)",
            "name_full": "Human expert ratings modelled via a Bayesian polytomous Rasch (cumulative probit mixed-effects) model",
            "brief_description": "A human-in-the-loop validation where domain experts rated hypotheses on novelty, relevance, significance and verifiability (1–5); responses were modelled using a Bayesian cumulative probit mixed-effects (Rasch-like) model to account for rater bias and provide posterior uncertainty.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Human expert evaluation with Bayesian item-response modelling",
            "system_type": "human evaluation + Bayesian psychometric model (brms/Stan)",
            "scientific_domain": "biomedical hypothesis appraisal",
            "output_type": "ordinal ratings per hypothesis per metric and posterior distributions for hypothesis-on-metric effects",
            "novelty_level": "Human judgment of novelty; results show refined hypotheses judged more novel in immunology and CVD groups.",
            "generation_method": "N/A (human evaluation).",
            "validation_method": "Experts (9 total: 4 immunology, 5 cardiovascular) rated hypotheses; ratings modelled with a Bayesian cumulative probit mixed-effects model, estimating metric effects, rater leniency (u_i) and hypothesis-on-metric effects (v_jm) and generating posterior distributions (Figure 6).",
            "generation_performance": "N/A",
            "validation_performance": "Posterior distributions show clear improvement in novelty after iterative refinement; verifiability did not show consistent improvement despite refinement. Quantitative posterior summaries are plotted but exact numeric posteriors per hypothesis are in Appendix/figures.",
            "false_positive_rate": "Not framed in FP/FN terms; experts may over- or under-rate verifiability relative to ultimate experimental truth. No numeric FP/FN rates reported.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Direct evidence that higher novelty does not necessarily produce higher verifiability in human judgments; refinement raised novelty but verifiability scores remained largely unchanged.",
            "generation_validation_comparison": "Human ratings serve as an orthogonal validation channel and confirm the observed asymmetry between novelty gains and verifiability gains seen under automated evaluators.",
            "uncertainty_quantification": "Yes — Bayesian model yields posterior distributions and threshold estimates, and random-effects capture rater variability (σ_u) and hypothesis-specific variability (σ_v).",
            "calibration_quality": "The model accounts for rater leniency/severity; however, calibration of rater judgments to true experimental outcomes is not measured (no lab validation), so calibration vs ground truth is unknown.",
            "out_of_distribution_performance": "Human evaluation targeted particular topical prompts (immunology and CVD); claims about OOD performance not applicable.",
            "validation_proxy_metrics": "Human ratings are still proxies for experimental validity (especially verifiability) rather than direct laboratory confirmation.",
            "human_validation_required": true,
            "human_validation_frequency": "Used on a modest pilot scale here (9 experts, ≈10 hypotheses each); authors recommend expert assessment for practical use and suggest experimental validation for high-impact hypotheses but do not prescribe a general frequency policy.",
            "formal_verification_used": false,
            "domain_formalization_level": "Empirical/biomedical",
            "gap_mitigation_strategies": "Using Bayesian psychometric modelling to adjust for rater bias and quantify uncertainty to produce more reliable human-based validation signals.",
            "evidence_supporting_gap": "Experts rated refined items as more novel but not more verifiable, supporting the generation&gt;validation gap for novel outputs.",
            "evidence_contradicting_gap": "Experts provided qualitative feedback that some hypotheses were experimentally tractable, and the case study showed concrete experimental suggestions, indicating that some generated hypotheses are testable and the gap is not absolute.",
            "computational_cost_ratio": "Human evaluation cost (time, expert labor) is substantially higher than automatic generation cost; exact monetary ratios are not reported, but authors note experimental validation is far more expensive and beyond the paper's scope.",
            "uuid": "e2096.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation",
            "rating": 2
        },
        {
            "paper_title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "LLM comparative assessment: Zero-shot NLG evaluation through pairwise comparisons using large language models",
            "rating": 2
        },
        {
            "paper_title": "Automated Hypothesis Validation with Agentic Sequential Falsifications",
            "rating": 2
        },
        {
            "paper_title": "AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "rating": 1
        },
        {
            "paper_title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
            "rating": 1
        }
    ],
    "cost": 0.0261065,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BIODISCO: Multi-agent hypothesis generation with dual-mode evidence, iterative feedback and temporal evaluation
2 Aug 2025</p>
<p>Yujing Ke 
Department of Data Science and its Applications
German Research Centre for Artificial Intelligence (DFKI)</p>
<p>Friedrich-Alexander University of Erlangen-Nuremberg (FAU)</p>
<p>Kevin George 
Department of Data Science and its Applications
German Research Centre for Artificial Intelligence (DFKI)</p>
<p>Kathan Pandya 
Department of Data Science and its Applications
German Research Centre for Artificial Intelligence (DFKI)</p>
<p>Gerrit Großmann 
Department of Data Science and its Applications
German Research Centre for Artificial Intelligence (DFKI)</p>
<p>David Blumenthal 
Friedrich-Alexander University of Erlangen-Nuremberg (FAU)</p>
<p>Maximilian Sprang 
Department of Dermatology
University Medical Center
Johannes Gutenberg University
Mainz</p>
<p>Sebastian Vollmer 
Department of Data Science and its Applications
German Research Centre for Artificial Intelligence (DFKI)</p>
<p>Department of Computer Science
University of Kaiserslautern-Landau</p>
<p>David Antony Selby 
Department of Data Science and its Applications
German Research Centre for Artificial Intelligence (DFKI)</p>
<p>BIODISCO: Multi-agent hypothesis generation with dual-mode evidence, iterative feedback and temporal evaluation
2 Aug 202566EB8B9EF590F2AA4E224C2A131D33CDarXiv:2508.01285v1[cs.AI]
Identifying novel hypotheses is essential to scientific research, yet this process risks being overwhelmed by the sheer volume and complexity of available information.Existing automated methods often struggle to generate novel and evidence-grounded hypotheses, lack robust iterative refinement and rarely undergo rigorous temporal evaluation for future discovery potential.To address this, we propose BIODISCO, a multi-agent framework that draws upon language model-based reasoning and a dual-mode evidence system (biomedical knowledge graphs and automated literature retrieval) for grounded novelty, integrates an internal scoring and feedback loop for iterative refinement, and validates performance through pioneering temporal and human evaluations and a Bradley-Terry paired comparison model to provide statistically-grounded assessment.Our evaluations demonstrate superior novelty and significance over ablated configurations representative of existing agentic architectures.Designed for flexibility and modularity, BIODISCO allows seamless integration of custom language models or knowledge graphs, and can be run with just a few lines of code.We anticipate researchers using this practical tool as a catalyst for the discovery of new hypotheses.</p>
<p>Introduction</p>
<p>Advancing biomedical research depends on a supply of novel, testable hypotheses.However, generating such hypotheses traditionally relies on human intuition, domain expertise and manual literature reviews, all of which are increasingly strained by an exponential growth in scientific data and publications.This challenge motivates automated approaches to scientific discovery that can systematically uncover and integrate complex biological relationships.Moreover, robust evaluation of such tools is essential.</p>
<p>Background</p>
<p>Knowledge graphs (KGs) provide a structured foundation for storing and analyzing scientific knowledge, with biomedical KGs capturing complex relationships among genes, proteins, diseases, drugs and biological pathways: e.g.HetioNet (Himmelstein et al. 2017) and PharmKG</p>
<p>I am curious about how</p>
<p>Wnt signalling and fibrosis contribute to cardiovascular disease.</p>
<p>(1) SERPINA4 enhances Wnt/βcatenin signalling by modulating interactions between …</p>
<p>KG Literature Critic</p>
<p>Figure 1: A high level overview of our automated framework for hypothesis generation.Overseen by a planner, agents search academic literature and query a knowledge graph to obtain articles and subgraphs relevant to a user-specified research topic.A scientist agent integrates these sources to derive initial hypotheses, which are rated by a critic, then refined with additional background, discarded or presented to the user with supporting evidence (Zheng et al. 2020).While a variety of tools have been developed to mine such graphs for novel insights (Perdomo-Quinteiro and Belmonte-Hernández 2024), construction and maintenance of KGs are time-consuming and labourintensive tasks, and a significant proportion of biomedical knowledge remains untapped in unstructured text.Large language models (LLMs) have emerged as a promising solution, capable of extracting relevant information and identifying latent patterns from vast corpora (Brown et al. 2020), with greater capacity for contextual reasoning than earlier methods of hypothesis generation (c.f.Spangler et al. 2014).However, LLMs risk returning plausible but factually groundless information (Bélisle-Pipon 2024).To mitigate this, models can be imbued with external interfaces, enabling dynamic literature retrieval via retrieval-augmented generation (RAG; Lewis et al. 2020;Singhal et al. 2023).</p>
<p>Evaluating the output of these generative systemsparticularly for novel hypothesis generation, in absence of 'ground truth'-presents its own significant challenge.Whilst general evaluation strategies exist, including expertbased assessments, automated semantic similarity scoring and validation against curated benchmarks (Liu et al. 2025), many recent approaches rely heavily on model-based evaluation, i.e. using other LLMs as judges (Li et al. 2024).Though more accessible than consulting human experts, model-based evaluation is sensitive to the relative strengths of the generator and the judge (Dorner, Nastl, and Hardt 2024) and alone may not fully capture a system's capacity for genuine scientific discovery.</p>
<p>Related work</p>
<p>Multi-agent systems Recent research has explored the use of 'specialized agents', often within multi-agent systems, to model the iterative and collaborative nature of scientific discovery.These approaches typically leverage LLMs augmented with tools, assigning distinct roles to individual agents to simulate the research process (Ren et al. 2025).Examples include AI Scientist (Lu et al. 2024), which automates the full research pipeline, ResearchAgent (Baek et al. 2025), a multi-agent system using KGs and peer reviewstyle feedback, and Google's Co-scientist (Gottweis et al. 2025), which employs a generate-debate-evolve loop.In the biomedical domain, AI agents like Biomni (Huang et al. 2025b) offer generalist capabilities from literature curation to experimental design.More narrowly focussed systems include SciAgents (Ghafarollahi and Buehler 2024) for biomaterials discovery and IntelliScope (Aamer et al. 2025) for biomedical hypothesis generation by identifying meaningful paths in KGs.</p>
<p>Evaluation metrics Evaluating the efficacy of automated hypothesis generation systems, particularly at scale, involves assessing not only coherence but also novelty, experimental feasibility and potential impact.A prevalent method involves testing a system's ability to rediscover known scientific findings using historical data (Sybrandt, Shtutman, and Safro 2018;Sybrandt et al. 2020), though this 'temporal evaluation' on held-out data remains underutilized.Assessment by human domain experts is desirable but challenging to perform reliably for large-scale evaluations (Alkan et al. 2025), thus Qi et al. (2024) used LLMs as judges, rating hypotheses for novelty, verifiability, significance, and relevance.This and other LLM evaluations rely on assigning numerical ratings to individual outputs.However, Liusie, Manakul, and Gales (2023) showed that paired comparisons using LLMs can outperform direct scoring approaches, at the risk of introducing additional sources of bias, such as order effects.Some authors, e.g.Gottweis et al. (2025), used Elo ratings for automated evaluation.However, a more principled statistical approach would use a probabilistic model to provide uncertainty estimates for ratings and enabling model diagnostics; the Elo system (1978) provides neither.</p>
<p>Benchmarks Recent efforts further underscore the need for standardized evaluation frameworks.Benchmarks such as PubMedQA (Jin et al. 2019), GPQA (Rein et al. 2024), andCARDBiomedBench (Bianchi et al. 2025) provide question-answer-based datasets to assess LLM reasoning, with a focus on the biomedical domain.Similarly, Truth-Hypo (Xiong et al. 2025) employs a temporal split to benchmark the classification of associations among biological entities.Complementary initiatives, including Hy-poBench (Liu et al. 2025), DiscoveryBench (Majumder et al. 2024) and LAB-Bench (Laurent et al. 2024), reinforce the growing emphasis on quantitative assessment of automated hypothesis generation systems.However, most of these benchmarks primarily assess knowledge retrieval, classification, or data-driven insights, falling short of evaluating the generation of novel, multi-entity textual hypotheses that our system aims to produce.</p>
<p>Contributions</p>
<p>We propose BIODISCO, a novel multi-agent framework that uniquely integrates LLM reasoning with access to biomedical knowledge graphs and real-time scientific literature, featuring an internal multi-dimensional scoring mechanism for iterative hypothesis refinement.Our system orchestrates a pipeline of specialized agents that collaboratively construct, evaluate and refine biomedical hypotheses, consistently generating evidence-based insights and inferring verifiable relationships beyond its explicit knowledge base.</p>
<p>Unlike systems that treat hypothesis generation as a 'oneshot' task or lack deep domain knowledge integration, BIO-DISCO employs a sophisticated self-critique loop grounded in dual-mode evidence.This paper presents the following: 1. BIODISCO, a flexible and modular automated hypothesis generation framework, based on LLM agents augmented with interfaces to biomedical KGs and literature search; 2. a rigorous evaluation of the system's ability to predict unpublished scientific relationships, including: (a) a temporal evaluation on two held-out datasets of 'future' discoveries; (b) a model-based ablation study, leveraging a Bradley-Terry model accounting for ties and order effects; (c) a human evaluation by experts in two biomedical domains, modelled using item-response theory; 3. an open-source Python package of the BIODISCO framework, available to install from PyPI.org via pip.A detailed case study is also presented in the Appendix.</p>
<p>Multi-Agent Hypothesis Generator</p>
<p>The BIODISCO framework, illustrated in Figure 1, operates through a sophisticated multi-agent architecture, where each agent has a distinct role to facilitate generation, evaluation, refinement and final selection of hypotheses.BIODISCO is, to our knowledge, the first system to integrate KG querying, literature search, 'reviewer' agents and explicit numerical scoring within a unified agentic feedback loop, wherein each generated hypothesis is internally evaluated against multiple metrics and refined to enhance its quality.</p>
<p>The core of BIODISCO's functionality resides in its network of specialized agents, illustrated in Figure 2. The BACKGROUND agent searches academic literature to gather scientific evidence, while the EXPLORER queries a biomedical KG to retrieve relevant subgraphs.The SCIENTIST then formulates initial hypotheses by integrating information from the summarized literature and subgraph data, proposing novel associations.These candidate hypotheses are systematically evaluated by a CRITIC, who scores each one for novelty, verifiability, relevance and significance (following Qi et al. 2024), providing structured feedback.The REVIEWER agent uses this feedback to formulate targeted refinement strategies, such as suggesting new entities or expanded literature review, and invokes dedicated interfaces to conduct more specialized KG queries or fine-grained literature searches.The REFINER amends the hypotheses based on this guidance and additional information.The decision module monitors CRITIC scores to select and collect hypotheses throughout the feedback loop.</p>
<p>Initial Hypothesis</p>
<p>Iterative Improvement</p>
<p>Dual-mode evidence grounding</p>
<p>To ensure the factual reliability of generated hypotheses, BIODISCO exploits two complementary evidence sources: a structured knowledge graph and real-time scholarly literature retrieved via the PubMed API.These knowledge bases are dynamically queried throughout the hypothesis generation process via dedicated interfaces.</p>
<p>The KG interface enables dynamic retrieval of structured biomedical knowledge from an external graph.We adopt PrimeKG (Chandak, Huang, and Zitnik 2023) as the underlying KG, hosted in a Neo4j instance: queries are formulated using Cypher for efficient subgraph retrieval.For each query, agents supply a background summary and a set of keywords.These keywords are first mapped to canonical KG nodes via pretrained biomedical embeddings (BioSimCSEBioLinkBERT-BASE; Kanakarajan et al. 2022), then further filtered for contextual relevance.The re-sulting set of nodes defines a personalized query, which is executed to retrieve relevant evidence in the form of both direct and multi-hop relations.Query parameters, including relation types and traversal depth, are adaptively set based on task objectives and intermediate feedback.Returned subgraphs are summarized and converted into standardized plaintext representations for downstream use.This approach optimizes retrieval for both efficiency and relevance, enabling flexible and context-aware access to largescale structured knowledge.</p>
<p>The literature interface enables agents to programmatically access biomedical literature through an LLM-guided, context-aware query and retrieval workflow.At each invocation, agents supply a set of keywords.For background research, only keywords are provided, whereas for evaluation, the current hypothesis is included, and for revision, both the hypothesis and low-score feedback are appended.An LLM-based planning module analyzes these inputs to generate structured PubMed query strategies, grouping terms by semantic similarity and determining optimal Boolean logic for combining concepts.These strategies are converted into PubMed-API-compatible queries, including support for MeSH and TIAB fields as well as date-range constraints.If the initial strategy yields insufficient results, the interface automatically relaxes query constraints to increase recall.</p>
<p>Iterative refinement &amp; multi-agent collaboration</p>
<p>The BIODISCO framework employs a dynamic process of co-operative evaluation and improvement.This iterative self-critique loop, unique in its integration of an internal critic and (external) dual-mode evidence, is designed to enhance hypothesis quality and credibility of initially generated hypotheses.The preliminary generation stage is driven by user input and involves three specialized LLM agents.We demonstrate this with an illustrative example.For each initial hypothesis, an iterative refinement process is launched to progressively improve its quality and credibility.This critical loop, described in steps 5-7, cycles through evaluation, real-time information retrieval and targeted revision.This involves three additional agents: 5.The CRITIC evaluates each hypothesis alongside the retrieved background and literature summary, providing structured feedback in the form of numerical scores and comments detailing strengths and weaknesses.This feedback loop repeats for up to three cycles.After each round, the decision module checks whether the overall score exceeds a predefined threshold, allowing early exit for high-quality hypotheses or discarding consistently underperforming ones.Finally, the framework selects the highest-scoring and most scientifically valuable hypotheses as outputs with their scores and supporting evidence.Further technical details about individual agents and their system prompts are provided in the Appendix.</p>
<p>Evaluation of LLM-Generated Hypotheses</p>
<p>We designed a comprehensive, three-part empirical evaluation to assess the quality and novelty of hypotheses generated by BIODISCO.</p>
<p>Our temporal evaluation-described in the next subsection-assesses the system's capacity for genuine discovery, by determining if it is able to predict discoveries made after a certain time cutoff, with access only to data and literature before it.</p>
<p>This is followed by a model-based ablation study to quantify the contributions of the agentic framework, tools and refinement protocol.Significantly, our evaluation incorporates a pairwise LLM evaluator combined with a Bradley-Terry model.This statistical approach provides more stable and human-aligned assessments of hypothesis quality across multiple dimensions (Liusie, Manakul, and Gales 2024), delivering not just point estimates but also uncertainty quantification via 95% comparison intervals, a significant advancement over methods that merely offer direct scores or unidimensional summaries.</p>
<p>Finally, a human evaluation records the expert judgment of 9 researchers in two biomedical subfields: cardiovascular disease and immunology.Their responses are modelled using a Bayesian polytomous Rasch model to account for possible rater-specific and metric-specific biases.</p>
<p>Except where otherwise specified, all agents were based on GPT-4.1 (knowledge cutoff: 1 st June, 2024).</p>
<p>Can BIODISCO predict future hypotheses?</p>
<p>Firstly, we test if the system can generate hypotheses not present in its training corpora or knowledge interfaces.</p>
<p>Datasets We used two publicly available datasets.(1) An unseen dataset from Qi et al. (2024), containing background-hypothesis pairs curated from biomedical literature, published in August 2023.We filtered this dataset to focus on core biomedical findings, removing entries with psychology or epidemiology backgrounds, resulting in 134 samples.(2) TruthHypo (Xiong et al. 2025) provides keyword pairs across three categories: chemical-gene, genegene, and gene-disease.Each pair is labelled with a relationship type: positive, negative, or no relation and was ex- tracted from literature published after 2024.We focused exclusively on positive and negative relations, excluding the 'no relation' category, as BIODISCO's primary focus is generation of plausible relationships between entities rather than confirming their absence.From this dataset, we created two subsets.We first selected 30 samples, balanced across categories, to configure a classifier agent.From the remainder, we sampled a test set of 300 instances (limited for computational feasibility), stratified across all categories and the two classes, for evaluation.</p>
<p>Setup We enforced strict temporal splits by limiting BIO-DISCO's knowledge to information available up to December 2022 for the Qi et al. (2024) experiment, and up to December 2024 for the TruthHypo experiment.This was achieved by restricting access to PubMed articles published after these cut-off dates.We also used PrimeKG (published April 2022) as a knowledge source.To prevent knowledge leakage from the underlying LLM, we used the GPT-3.5 Turbo (knowledge cutoff: 1 st September 2021) for the Qi et al. (2024) experiment.To bridge BIODISCO's free-text generation with the structured labels required by the Truth-Hypo task, we developed a classifier agent to predict the relationship class from the textual hypotheses.</p>
<p>Results</p>
<p>We evaluated the semantic alignment between the generated and 'gold-standard' hypotheses from Qi et al. ( 2024) by computing their cosine similarity using a pretrained biomedical sentence encoder, BioBERTmnli-snli-scinli-scitail-mednli-stsb (Deka, Jurek-Loughrey et al. 2022).As a baseline, we calculated pairwise similarities between unrelated 'gold' hypotheses (i.e.those associated with different background contexts), serving as a negative control for expected similarity in semantically unrelated pairs.Figure 3 shows that generated hypotheses were more semantically similar to the gold standard (median similarity: 0.68) than unrelated gold hypotheses were to each other (median: 0.34), demonstrating that BIODISCO reliably produces hypotheses closer to human-curated ground truth than expected by chance.</p>
<p>In the second experiment, we assessed whether BIO-DISCO could generate a hypothesis that correctly implies the relationship for a given entity pair.We used our classifier</p>
<p>What are the effects of knowledge &amp; refinement?</p>
<p>To evaluate the contribution of each component to the framework, we compared BIODISCO against a series of ablated configurations.These included:</p>
<ol>
<li>GPT-4.1 (baseline): a standalone single LLM, prompted without agentic structure or external tools.2. Multi-agent system: a system mirroring our architecture, but without iterative refinement or knowledge interfaces.3. Multi-agent + tools: a system with access to the literature and KG interfaces but without iterative refinement.4. Multi-agent + refine: a system with an iterative refinement loop, but without access to knowledge interfaces. 5. BIODISCO: the full system described in § 2.</li>
</ol>
<p>Leveraging the established correlation between LLM-based evaluators and human expert judgements (Lu et al. 2024), we employed a dedicated pairwise LLM evaluator, rather than direct scoring of individual hypotheses.</p>
<p>Each of the systems was given 100 inputs spanning a diversity of topics, from oncology to ageing and cellular senescence, generating one hypothesis per input.For every pair of generated hypotheses from different configurations, an LLM judge selected its preferred candidate (or declared a tie) for each of four metrics: novelty, relevance, verifiability and significance (from Qi et al. 2024).</p>
<p>We fitted a Bradley-Terry (1952) paired comparison model to the results, given by
log odds(i beats j) = α + β i − β j ,(1)
where i and j denote 'players' or LLM configurations, β i is the ability score of the ith LLM system and α is a 'home advantage' parameter, estimating the possible order effect bias, assuming the hypothesis of i is presented to the evaluator first in the pair.The continuous latent ability scores for each configuration and each metric provide a univariate ranking that captures relative performance.The fitted ability scores and their 95% comparison intervals (calculated using a quasi-variance approximation ;Firth 2004)  progressive improvement in hypothesis novelty and significance as components are added to the system.The single LLM baseline was least preferred.Introducing a multiagent structure provided significant improvement, suggesting a benefit from role-based reasoning alone.Performance further increased with the addition of external tools and with iterative refinement.Between the two, tool use provided a greater benefit, emphasizing the value of grounding the generation process in external knowledge sources.Finally, the full BIODISCO system was most preferred for these metrics, demonstrating a synergistic effect between dual-mode evidence and iterative refinement, validating our approach.Interestingly, patterns for relevance and verifiability were less clearly defined: pairs of configurations have overlapping comparison intervals, with no clear winner.While a single LLM may even be preferred for topical relevance, this may simply suggest a lack of external knowledge causes such a zero-shot system to remain close to the user input, at the expense of generating novel or significant hypotheses.</p>
<p>Additional results-including order effect estimates and a direct scoring evaluation-are provided in the Appendix.</p>
<p>What do human domain experts think?</p>
<p>To complement the model-based assessments, and to ascertain the practical utility of BIODISCO's outputs, our final evaluation involved two groups of human domain experts.</p>
<p>Methodology We recruited nine experts from two biomedical fields-four in immunology and five in cardiovascular medicine-to complete an online survey about a series of topical hypotheses generated by BIODISCO.The hypothesis pairs were generated as follows.</p>
<p>For immunology, BIODISCO was provided with the complex prompt: "T Cell Exhaustion Mechanisms and Therapeutic Targets in NSCLC".The system was run five times to generate five distinct hypotheses, to explore its capacity to explore a single topic in depth.For cardiovascular dis- Figure 5: Ratings given by two independent groups of human experts to 10 hypotheses generated for respective topics of cardiovascular disease (CVD) and immunology ease (CVD), BIODISCO was provided five unique prompts related to CVD, to assess performance across different topics.One initial and one refined hypothesis was generated for each input.</p>
<p>For every sub-topic, experts were presented with both the initial version (the first complete hypothesis generated) and the final version (after iterative refinement).Experts independently rated each hypothesis on a 1-5 scale according to the four metrics: novelty, relevance, significance and verifiability.They also rated their confidence in their assessments and provided qualitative feedback.</p>
<p>Analysis Survey responses were modelled using a single polytomous Rasch model, implemented as a Bayesian cumulative probit mixed effects model (Bürkner 2021) for all four metrics-one for each group of experts:
P (Y ijm ≤ k) = Φ(τ k − η ijm ),(2)
where Y ijm denotes the ordinal rating (k = 1, . . ., 5) given by rater i to hypothesis j on metric m, Φ denotes the standard normal distribution function and τ k are the latent thresholds that partition the ordinal rating scale.The linear predictor η ijm contains both fixed and random effects:
η ijm = β m metric effect + u i rater effect + v jm hypothesis-on-metric effect ,(3)
where β m is a fixed effect representing the average quality or 'easiness' for metric m, u i ∼ N (0, σ 2 u ) is a rater-specific random effect capturing overall leniency or severity of rater i, and v jm ∼ N (0, σ 2 v ) is a hypothesis-and metric-specific random effect reflecting the relative quality of hypothesis j on metric m.By disentangling rater-and metric-specific biases and accounting for the ordinal nature of responses, the model provides a more robust and unbiased estimate of the underlying quality of each hypothesis, while pooling information across the different questions.</p>
<p>Results Figure 5 presents the raw distribution of expert ratings.We observe that for the immunology domain, experts consistently provided high scores for BIODISCO-</p>
<p>Novelty</p>
<p>Relevance</p>
<p>Significance Verifiability CVD Immunology 1 2 3 4 5 6 7 8 910 1 2 3 4 5 6 7 8 910 1 2 3 4 5 6 7 8 910 1 2 3 4 5 6 7 8  The posterior distributions of the hypothesis-on-metric effects are shown in Figure 6.A clear improvement in novelty, as assessed by expert raters, is evident following the iterative refinement process.Interestingly, however, the refined hypotheses are not necessarily judged to be more verifiablean observation that aligns our ablation study.</p>
<p>Qualitative feedback reinforced the system's efficacy in generating scientifically valuable and contextually relevant hypotheses.While a minor fraction of the generated hypotheses exhibited limited novelty or proposed intricate, unvalidated connections, the overarching expert consensus underscored both the scientific plausibility and practical experimental tractability.For example, one expert commented: "This idea could be tested with targeted experiments like activating or inhibiting GPR153 in VSMCs, running transcriptomic and ChIP-seq analyses to track CEBPB/NRF1/CD7 activity, and using in vivo vascular injury models with genetic knockouts.If these links hold up, the network could point to new therapeutic targets for preventing neointima formation." .</p>
<p>Python Package</p>
<p>The BIODISCO framework is available to biomedical researchers in the form of a Python package, which can be installed via PyPI.org using pip install biodisco or from the code provided in the supplement.Users can customize their choice of LLM and knowledge graph through standard interfaces.Documentation is provided online.</p>
<p>Discussion &amp; Conclusion</p>
<p>We presented BIODISCO, a novel multi-agent framework for grounded and refined biomedical hypothesis discovery, made openly available for research use.A temporal evaluation on held-out data shows the ability of the system to extrapolate beyond its training corpus.An ablation demonstrates integration of iterative refinement and external knowledge improve the system more than the sum of their parts for generating 'novel', 'significant' hypotheses, and the system has been validated by human experts while using probabilistic psychometric models to account for bias and uncertainty.</p>
<p>Limitations Hypotheses were rated as significant and novel but apparently at the expense of relevance and verifiability.One explanation for this could be an inherent tradeoff between novelty and verifiability: things that are harder to test are less likely to have already been tested.Additionally, a single LLM judge may not be the best evaluator of experimental feasibility; this is explored further by a dedicated hypothesis testing framework, e.g.POPPER (Huang et al. 2025a).Experimental validation in the laboratory is, unfortunately, beyond the scope of this paper.</p>
<p>Similarly, weakening 'relevance' to an input topic may indicate breadth of exploration and diversity of knowledge, or simply a limitation of the metrics proposed by Qi et al. (2024).</p>
<p>Our metrics-based evaluation is limited to the judgement of humans and LLMs of whether hypotheses seem novel or significant.A focus on self-evaluation and refinement may invoke Goodhart's Law, amplifying model artifacts and exaggerating the system's practical relevance.</p>
<p>A core problem of any temporal evaluation is that it assumes discoverability is equivalent to predictability, but a hypothesis can be a valid, valuable contribution to a field even if based on a flawed premise or is ultimately disproven.This makes it a poor proxy for a system's quality, as a good system should generate novel, plausible hypotheses-not just those that align with future discoveries.</p>
<p>Future work A direct comparison of competing hypothesis generation systems, would be a valuable contribution.A significant obstacle may be disentangling the effects of architectural design from more mundane aspects, such as data and knowledge graph availability and the relative strength of different LLMs used in agents or their evaluation.This, like any type of evaluation of LLMs trained on public data, can also be vulnerable to data leakage (see, e.g.Zhou et al. 2023).</p>
<p>A major open challenge for biomedical discovery lies in the integration of diverse multi-modal biomedical data, such as clinical records, omics and medical imaging.Furthermore, more research is needed into interpretability of agentic systems, leaving traces to explain sources of evidence-based reasoning and better quantify epistemic uncertainty.</p>
<p>Appendices</p>
<p>See Appendix for details on LLM agents and judges, their prompts, gold-standard hypotheses, the human evaluation survey, item response models and computate infrastructure.</p>
<p>A A Detailed Case Study</p>
<p>To provide an insight into how BIODISCO works, we present a detailed case study.The case is grounded in a recent study linking G-protein coupled receptors (GPCRs) to increased cardiovascular risk (Kobilka and Lefkowitz 2024), and centres on two entities: the gene GPR153 and the disease vascular injury.We provided "Role of GPR153 in vascular injury and disease" as input to BIODISCO.Relevant entities were drawn from the input query and used to access the literature interface to retrieve relevant publications.</p>
<p>Initially, the BACKGROUND agent synthesised a summary highlighting the dual role of GPR153 in vascular injury and inflammation from the retrieved literature.The EXPLORER analyzed the background summary to retrieve a subgraph consisting of genes like GPR153, CEBPB, GRN, CDK4, TTR, YAP1, SCAMP1 as well as drugs like Acamprosate and conditions such as camptodactyly.</p>
<p>The SCIENTIST received both the background summary and the KG subgraph based on which it proposed an initial hypothesis:</p>
<p>"GPR153 activation in vascular smooth muscle cells enhances pro-inflammatory gene expression via the YAP/TAZ pathway, promoting neointima formation following vascular injury".Subsequently, the initial hypothesis and relevant keywords were used to query the literature interface for relevant PubMed publications to find evidence for the generated hypothesis.This search identified studies such as Shao et al. (2025) which conveys how GPR153 is an orphan receptor that facilitates expression of pro-inflammation and pro-proliferation genes in smooth muscle cells by regulating cAMP levels in cells and thereby contributing to inflammation and vascular remodelling.</p>
<p>The initial hypothesis by SCIENTIST was evaluated by the CRITIC, considering the background and literature.Novelty was rated 4, reflecting the novel aspect of linking GPR153 activation to the YAP/TAZ pathway and neointima formation in vascular smooth muscle cells.Relevance was given a 5 due to the significant interest in vascular injury in therapeutic research.Significance was rated 4, acknowledging the therapeutic potential of targeting this pathway.This gave the initial hypothesis 17 out of 20.</p>
<p>To improve the hypothesis, the REVIEWER first focused on Novelty by highlighting "lack of mechanistic insight" on how exactly GPR153 activation influences the YAP/TAZ pathway.It decided to obtain additional knowledge graph evidence, and therefore used the current hypothesis, critic feedback, and background information to query the KG interface.This yielded a new subgraph from the knowledge graph with GPR153, YAP, and TAZ as the key entities.Using this new information, the REFINER then reformulated the hypothesis to: "GPR153 activation in vascular smooth muscle cells enhances pro-inflammatory gene expression by promoting CEBPB-mediated YAP1 signalling, thereby potentially integrating with EGR1 and GSK3B pathways to exacerbate neointima formation following vascular injury" Following this, the system re-entered the feedback loop.The CRITIC referencing the newly identified literature for the revised hypothesis, raised the score of Novelty to 5. Elaborating on the mechanism by which GPR153 can influence YAP1 signalling led to an increase in the Significance score to 5 as well.Relevance remained at 5 while Verifiability was still at 4, making the overall score 19.This caused it to access the knowledge graph to retrieve a more focused subgraph containing genes and proteins: GPR153, CEBPB, NRF1, CD7, EGR1, PHYHIP, TTR, PPP2CA, and GSK3B.The system also accessed the literature interface for each major gene and gene combination implicated in the new hypothesis to identify additional relevant publications.This step provided further evidence supporting the possible connections among these genes.</p>
<p>Subsequently, the REVIEWER chooses to focus on Verifiability -the only criterion not awarded a top score.The RE-FINER takes the previous refined hypothesis as input, along with the new subgraph, additional PubMed articles and the REVIEWER's note to generate a further refined hypothesis: "GPR153 activation in vascular smooth muscle cells enhances pro-inflammatory gene expression by facilitating CEBPB-mediated network involving YAP1, EGR1, and GSK3B, creating a complex signalling cascade that drives neointima formation after vascular injury"</p>
<p>In the next iteration, the CRITIC assigned scores of 5 for Novelty, Significance, and Relevance.Verifiability remained at 4, as the complexity of the inferred connections could pose challenges for experimental validation.The system regrouped the key genes and conducted an updated literature search, identifying nine unique articles, including some newly retrieved papers in addition to prior results.The RE-FINER then incorporated additional proteins (NRF1, CD7, and GSK3B) into the hypothesis, yielding the final version: "GPR153 activation in vascular smooth muscle cells enhances pro-inflammatory gene expression through a CEBPB-mediated network, integrating NRF1 and CD7 interactions with YAP1 and GSK3B, thereby orchestrating a multifaceted signalling cascade that drives neointima formation following vascular injury" For this hypothesis, the CRITIC rated Novelty a 5, highlighting the integration of multiple key regulatory proteins into a comprehensive signaling network.Relevance and Significance also received top scores.Verifiability remained at 4, as the complexity of the interactions may limit definitive experimental validation, despite available methods such as gene editing and pathway analysis.The final hypothesis thus achieved an overall score of 19 out of 20.</p>
<p>B Feature comparison
CoSci IntS SciA ResA BIODISCO Multi-agent ✓ ✓ ✓ ✓ ✓ Tool use ✓ ✓ ✓ KG ✓ ✓ ✓ ✓ Search ✓ ✓ Reviewer ✓ ✓ ✓ ✓ Scoring ✓ ✓</p>
<p>C Additional Results</p>
<p>Examples from Qi et al. (2024) In Table 3, we present selected examples comparing goldstandard hypotheses with those generated by BIODISCO, along with their cosine similarity scores (see main paper for details on temporal evaluation).While Figure 3 demonstrates that BIODISCO produces hypotheses with higher semantic similarity than unrelated gold pairs, this table illustrates whether that similarity arises from shared terminology or meaningful mechanistic insight.</p>
<p>In the first example, we see that BIODISCO accurately identifies the core mechanistic insight present in the gold hypothesis that that inhibiting VEGFR2 in hypertrophic chondrocytes interrupts ERK1/2 activation and subsequent apoptosis.We can also see how BIODISCO provides more nuanced insights by integrating additional molecular pathways when compared to the gold-standard hypothesis.</p>
<p>Similarly, in the second example, BIODISCO successfully proposes and extends the insight from the gold-standard hypothesis, adding specific details about potential pathways and proteins involved in the process.Unlike the previous examples where the generated and gold-standard hypotheses shared the same mechanistic insights, this third pair takes different directions, resulting in a low similarity score.</p>
<p>TruthHypo</p>
<p>Table 4 presents the results for each class across the three tasks in the TruthHypo dataset.The high Precision, Recall, and F 1 scores achieved by BIODISCO demonstrate its ability to accurately identify the relationships between the given entities.a multi-agent setup and further to BIODISCO.However, the performance margins between versions remain narrow.Also, the effect of access to external interfaces is particularly unclear, with only minor differences in scores.These observations highlight that scalar ratings alone may be insufficient to capture more nuanced improvements in hypothesis quality.This underscores the value of pairwise evaluation, where an LLM compares hypotheses directly and can better distinguish subtle but meaningful differences in structure, specificity, and insight that may be overlooked in absolute scoring schemes.</p>
<p>Direct Evaluation using LLMs</p>
<p>D Paired Comparison Models</p>
<p>Paired comparison models were fitted using the R package BradleyTerry2 (Turner and Firth 2012), with quasivariance approximations computed using qvcalc (Firth 2025).The Bradley-Terry model (Bradley and Terry 1952) is given by
log odds(i beats j) = α + β i − β j ,(4)
where i and j denote 'players' or LLM agents, β i is the ability score (on the logit-scale) of the ith LLM system and α is a 'home advantage' parameter, estimating the possible order effect bias, assuming the hypothesis of i is presented to the evaluator first in the pair.For our model, ties were treated as half wins to each player.An extension of the Bradley-Terry model that explicitly supports ties, the Davidson (1970) model, was also fitted separately, but yielded very similar results (not presented here).</p>
<p>The Bradley-Terry model parameter estimates α are given in Table 6 and suggest a statistically significant order bias for Novelty and Verifiability evaluations (in favour of the item appearing first in the comparisons) but there is not enough evidence to indicate such an effect exists in Relevance or Significance comparisons at the 5% level of significance.Quasi-variances (Firth 2004) aim to minimize the squared loss min i&lt;j</p>
<p>Metric
(q i + q k − v ij ) 2 ,(5)
where q i is the quasi-variance of player i and v ij is the co-variance term (i.e.off-diagonal entries between players i and j.The package qvcalc returns relative errors as a measure of the quality of this approximation: the distribution of relative errors is given in Figure 7 as a beeswarm plot (Selby 2020) and appear to be mostly acceptable.</p>
<p>E Compute Infrastructure</p>
<p>All experiments were executed on a commodity CPU server (4 cores, 16 GB RAM, no GPU), making the workflow straightforward to deploy and reproduce.Inference time and cost depend chiefly on (i) the number of refinement iterations, (ii) the size of the knowledge-graph (KG) and literature evidence retrieved, (iii) PubMed/Neo4j access latency, and (iv) network latency to the OpenAI GPT-4.1 API.</p>
<p>For all LLM-based agents, random seed (cache seed) was fixed to 42 to support reproducibility, and temperature was set between 0.2 and 0.5 depending on the agent role to balance generation diversity and stability.</p>
<p>Four configurations were benchmarked in Table 7.The single-pass multi-agent baseline without KG or literature ("Multi-agent") is the most economical, averaging $0.004 per hypothesis.In contrast, the full BioDisco pipeline, which involves three refinement iterations, had the highest cost of approximately 0.071 US dollars.Incorporating KG and literature significantly increases token usage and API cost but provides richer contextual and mechanistic evidence.On average, BioDisco completes a full inference in 2 to 3 minutes per hypothesis, while the lightweight baseline finishes within one minute.</p>
<p>Setting</p>
<p>Input</p>
<p>F Human evaluation</p>
<p>Survey Design</p>
<p>The human evaluation of hypotheses took place online via a bespoke Microsoft Form.Dissemination of the survey was through direct and e-mail communication and through a consortium-wide mailing list.</p>
<p>Participants had the right to remain anonymous or to be acknowledged by name in subsequent publications.They were asked to give their institutional affiliation, years of experience, and discretized number of publications in their field of expertise.For each hypothesis, the participant was asked to rate it for novelty, significance, verifiability and relevance (to the given input context), each on a scale from 1-5, where 1 is the lowest and 5 is the highest score.The set of metrics is based on Qi et al. (2024) and the descriptions were the same as given to LLM evaluators (see Listing 8).</p>
<p>Additionally, each respondents could score their level of confidence in their chosen ratings for each hypothesis (on a 1-5 scale) according to their familiarity with the topic area, as well as add free-text comments.All fields in the survey were optional, so the user could skip giving a rating for any part of any hypothesis, for any reason.This was to reduce the risk of unconfident or uninterested respondents giving arbitrary scores.A sample from the online questionnaire is presented in Figure 8.</p>
<p>Evaluation Material</p>
<p>We provide the full list of topics, initial hypotheses, and final hypotheses used for human evaluation in both cardiovascular disease (CVD) and immunology domains (see Table 8 and Table 9).</p>
<p>Bayesian Item Response Modelling</p>
<p>To quantitatively assess human evaluation scores while accounting for both rater and hypothesis-specific variability, we fit a Bayesian cumulative ordinal mixed-effects model to the ordinal scores assigned by each rater.Let Y ijm denote the ordinal rating (on a scale of 1 to 5) given by rater i to hypothesis j on metric m.The model estimates the cumulative probability for each rating category k ∈ {1, 2, 3, 4} as
P (Y ijm ≤ k) = Φ(τ k − η ijm ),(6)
where Φ denotes the standard normal cumulative distribution function (probit link), and τ k are the latent thresholds that partition the ordinal rating scale.</p>
<p>The linear predictor η ijm contains both fixed and random effects:
η ijm = β m metric effect + u i rater effect + v jm hypothesis-on-metric effect ,(7)
where β m is a fixed effect representing the average quality or 'easiness' for metric m, u i ∼ N (0, σ 2 u ) is a rater-specific random effect capturing overall leniency or severity of rater i, and v jm ∼ N (0, σ 2 v ) is a hypothesis-and metric-specific random effect reflecting the relative quality of hypothesis j on metric m.</p>
<p>Using brms (Bürkner 2021) we fit a single model across all metrics to increase statistical power, under the assumption that the rater leniency effects are constant across metrics.This approach pools information across all dimensions Rare pathogenic variants in G-protein-coupled receptor genes for atherosclerosis Rare pathogenic variants in GPCR genes enhance inflammatory signaling, increasing the risk of atrial fibrillation and cardiomyopathy in patients independent of traditional cardiovascular risk factors.Rare pathogenic variants in GPCR genes disrupt inflammatory signaling and gut microbiota interactions, which synergistically exacerbate atrial fibrillation and cardiomyopathy, suggesting a novel approach for identifying targeted biomarkers that link these pathways in personalized cardiovascular disease management.</p>
<p>Cpt1b and heart regeneration</p>
<p>Inhibition of CPT1B enhances cardiomyocyte proliferation by reactivating cardiogenic factors and suppressing NF-κBmediated inflammatory responses during heart regeneration.</p>
<p>Inhibition of CPT1B enhances cardiomyocyte proliferation through a regulatory cascade involving GOLT1A that modulates mitochondrial dynamics and lipid metabolism, while concurrently suppressing NF-κB signaling and promoting a metabolic shift, thereby facilitating heart regeneration.Inhibition of PIK3R1-CTLA4 interaction attenuates T cell exhaustion and enhances the efficacy of immune checkpoint inhibitor therapy in non-small cell lung cancer.</p>
<p>Adoptive transfer of NSCLC patient T cells engineered via CRISPR to disrupt the PIK3R1-CTLA4 axis and enhance CD7 costimulation will yield durable reversal of exhaustion and superior clinical responses to immune checkpoint inhibitor therapy in early-phase clinical trials.PPP2CA suppresses STAT3-mediated cytokine signaling in the NSCLC tumor microenvironment, thereby reducing CD8+ T cell exhaustion and enhancing the efficacy of immune checkpoint inhibitors.</p>
<p>PPP2CA dephosphorylates STAT3 and modulates MYC and ETS1 activity, leading to altered TNFSF4 signaling and a reprogrammed cytokine milieu in NSCLC that diminishes CD8+ T cell exhaustion and enhances the therapeutic efficacy of immune checkpoint blockade.CTLA4 upregulation in tumor-infiltrating T cells promotes T cell exhaustion and resistance to immune checkpoint inhibitors in non-small cell lung cancer.</p>
<p>In NSCLC, lactate-induced activation of SRPK1 enhances USP39-mediated RNA splicing in regulatory T cells, leading to CTLA4 upregulation and T cell exhaustion, with SRPK1 or USP39 inhibition predicted to restore antitumor immunity and sensitize tumors to immune checkpoint inhibitors.Inhibition of CK2B enhances PIK3R1mediated CTLA4 downregulation, reducing T cell exhaustion and improving immune checkpoint inhibitor efficacy in non-small cell lung cancer.</p>
<p>Inhibition of CK2B and PIK3R1, in conjunction with IL-21R activation, enhances CTLA4 downregulation and STAT5Bmediated T cell reactivation, improving immune checkpoint inhibitor responses in non-small cell lung cancer by targeting progenitor-exhausted T cells within the tumor microenvironment.</p>
<p>Table 9: Immunology-related human evaluation cases, showing the input, initial hypothesis, and final (refined) hypothesis produced by the system.</p>
<p>for more robust estimation of both rater and hypothesis effects.</p>
<p>G Agent Roles</p>
<p>Here we describe in detail the role of the individual agents and their respective prompts.</p>
<p>G.1 Planner agent</p>
<p>The PLANNER agent functions as the central coordinator for the hypothesis discovery pipeline.It receives keywords input by users and manages the sequential execution of all specialized agents, including background retrieval, knowledge graph exploration, hypothesis generation, evaluation, and refinement.At each stage, the PLANNER passes relevant intermediate outputs between agents, monitors progress, and handles control flow decisions such as triggering iterative refinement or terminating the process.In addition to orchestrating agent execution, the PLANNER can optionally produce a concise research plan summarizing the workflow steps taken for a given task.This design promotes modularity, simplifies pipeline management, and facilitates user intervention or expert oversight at any stage of the discovery process.</p>
<p>Listing 1: PLANNER agent</p>
<p>G.2 Background agent</p>
<p>The BACKGROUND agent is responsible for constructing a concise and informative textual background for each hypothesis discovery task.It receives keywords or research topics and then retrieves relevant biomedical literatures through the literature interface, typically by querying PubMed.The agent then synthesizes and summarizes the retrieved content, producing a structured background paragraph that integrates the most pertinent findings and context.This background serves as a foundation for downstream agents, ensuring that subsequent hypothesis generation and evaluation are grounded in up-to-date and domain-relevant evidence.</p>
<p>Listing 2: BACKGROUND agent You are given a list of PubMed article metadata blocks about a specific disease and a set of core genes or biological entities.Write a concise, well− structured background paragraph (less than 150 words) that summarizes key mechanistic insights and highlights the relationships between the core genes and disease−relevant biological processes ( such as EMT, inflammation, senescence, signaling, etc.).Requirements:</p>
<ol>
<li>Clearly explain how the core genes are linked to disease mechanisms, pathways, or phenotypes based on the literature.2. Emphasize causal or regulatory connections when possible, rather than just listing associations.3. Do not copy sentences verbatim from abstracts.</li>
</ol>
<p>Always synthesize and paraphrase information in your own words.4. Use clear, logical, and scientifically precise language . 5. Avoid including superfluous or generic information; focus on mechanistic insights most relevant to the disease and core genes.</p>
<p>G.3 Explorer agent</p>
<p>The EXPLORER agent retrieves and summarizes subgraphs from a biomedical knowledge graph based on provided background information and keywords.The information passed to EXPLORER includes keywords and a text as background reference.It first maps the input keywords to candidate standardized entities in the graph, then leverages LLM to select the most contextually relevant nodes using the background as guidance.Then performs composite queries to extract related nodes and multi-hop paths based on these anchor entities.Query parameters, including hop depth, relation types, and result size are dynamically adjusted depending on the reasoning stage: broad subgraphs are retrieved during initial hypothesis generation to encourage diverse exploration, while later refinement stages focus on deeper, localized evidence addressing specific weaknesses such as low novelty or verifiability.The agent returns a structured subgraph summary that provides precise contextual support for downstream hypothesis generation and evaluation.Listing 3: EXPLORER agent</p>
<p>G.4 Scientist agent</p>
<p>The SCIENTIST agent generates initial biomedical hypotheses by reasoning over the structured KG subgraph and textual background.It is fed with background information generated by BACKGROUND agent and subgraph information generated by EXPLORER agent, and then simulates the inference process of a researcher, identifying potentially novel and testable associations between entities based on mechanistic context and graph structure.Each hypothesis is expressed in natural language and describes a potential causal or regulatory relationship between biomedical entities.To encourage exploration of the hypothesis space, the Scien-tistAgent typically generates three candidate hypotheses in parallel, which serve as inputs for subsequent evaluation and refinement stages.Listing 4: SCIENTIST agent</p>
<p>G.5 Critic agent</p>
<p>The CRITIC agent provides structured evaluation of each candidate hypothesis based on supporting evidence, offering clear feedback signals to the system.Three pieces of information are passed to it simultaneously, including the background generated by BACKGROUND agent, the current hypothesis, and the corresponding relevant references (used in hypothesis generation and improvement).It scores hypotheses along four core dimensions: novelty, relevance, significance, and verifiability.Each dimension is rated on a 0-5 scale and accompanied by a brief explanation that justifies the score.The assessment is grounded in the LLM integrated understanding of the hypothesis, background, and literature evidence.The resulting evaluations guide downstream diagnosis and refinement by identifying weaknesses and informing targeted revision strategies.Listing 5: CRITIC agent</p>
<p>Figure 2 :
2
Figure 2: BIODISCO architecture.Each agent has a distinct role, some augmented with external tools.Agents interact sequentially: the user's input is first processed by the BACKGROUND agent to generate a topical summary, guiding KG extraction by the EXPLORER and initial hypothesis generation by the SCIENTIST.Each hypothesis undergoes an iterative cycle of evidence retrieval and refinement.Finally, evaluations from the CRITIC agent are used to identify the most promising hypotheses.Here, Lit refers to the literature interface, KG to the KG interface, and BG to the generated background.</p>
<p>Figure 3 :
3
Figure 3: Violin plot demonstrating that hypotheses generated by BIODISCO are semantically more similar to 'gold' hypotheses than gold hypotheses are to other hypotheses.Top distribution shows pairwise similarity of unrelated gold hypotheses; bottom shows similarity of BIODISCOgenerated hypotheses to gold standard for the same topics</p>
<p>Figure 6 :
6
Figure 6: Posterior distributions of hypothesis-on-metric effect, vjm , from human evaluations of hypotheses generated by BIODISCO for cardiovascular disease and immunology</p>
<p>Figure 7 :
7
Figure 7: Beeswarm plot showing distribution of relative errors from the quasi-variance approximation</p>
<p>Figure 8 :
8
Figure 8: Screenshot from the expert evaluation questionnaire, showing a single hypothesis and scoring rubric</p>
<p>Given a background text and a list of candidate KG node, Based on the background information, select the most relevant nodes (5−10) to use for subgraph construction.1.Only choose from the provided candidates.2. Output only a JSON array of the selected.3. Do not output any extra text, explanations, or formatting.</p>
<p>Table 1 :
1
Performance of temporally-restricted BIODISCO on the TruthHypo benchmark for three categories of task agent to perform binary classification (positive, negative) on the generated text.Results are given in Table1, showing the classifier achieved high precision and recall, indicating that the hypotheses generated by BIODISCO contain accurate and discernible relational signals.These results indicate that BIODISCO, operating under strict temporal constraints, can successfully infer novel, semantically relevant and factually verifiable hypotheses.
CategoryPrecision RecallF1 Acc.Chemical-Gene0.900.90 0.90 0.90Disease-Gene0.820.82 0.82 0.82Gene-Gene0.830.83 0.83 0.83</p>
<p>are visualized in Figure 4.The model shows a clear,
noveltyrelevanceBioDiscoMulti+ToolsMulti+RefineMulti-agentSingle LLM-6-303-0.25 0.00 0.25 0.50significanceverifiabilityBioDiscoMulti+ToolsMulti+RefineMulti-agentSingle LLM-202-0.6 -0.30.00.30.6Ability scoreFigure 4: Centipede plot of ability scores for BIODISCOand four ablation configurations, with 95% comparison in-tervals. A multi-agent system clearly outperforms a singleLLM (GPT-4.1) generating novel, significant hypotheses;tool use (i.e. KG and literature search) and iterative refine-ment each yield further improvements</p>
<p>Table 2
2: Features of LLM agentic systems: multiple agents,use of external tools, knowledge graph integration, ability(dynamically) to search academic literature, presence of a'reviewer' agent critically appraising outputs, and refine-ment of hypotheses based on explicit numerical scoring;CoSci = AI co-scientist Gottweis et al. (2025); IntS = In-telliscope Aamer et al. (2025); SciA = SciAgents Ghafarol-lahi and Buehler (2024); ResA = ResearchAgent Baek et al.(2025). ResearchAgent constructs a knowledge graph froman academic graph, but does not access a 'live' search API</p>
<p>Table 5 reports
5
LLM-based evaluation scores across four metrics -novelty, relevance, significance, verifiability, and an overall score.As expected, we observe consistent improvements when moving from a single-agent baseline to
Gold hypothesisBIODISCOSijInhibition of VEGFR2Inhibition of VEGFR20.86should interruptin hypertrophicphosphate-inducedchondrocytesERK1/2 activation andsuppresses ERK1/2subsequent apoptoticactivation, preventingevents in hypertrophicapoptosis bychondrocytes.modulatingdownstream pathwaysinvolving PPP2R2A orCORO1C, therebyamelioratinghypophosphatemicrickets.PPFIBP1 may play aElevated PPFIBP10.65role in the developmentupregulation inof chemoresistance inmultiple myeloma cellsMM.enhances bortezomibresistance by activatingNF-κB signaling,supported byPPFIBP1's role inpromoting RelAstability andcyto-nucleartranslocation,indicating a direct linkto chemoresistance.TkR86C expression inNeuregulin signaling0.39damaged wing discsvia the AMPK/mTORand brain suggests apathway enhancespossibleprogenitor cellnon-cell-autonomousproliferation in limbrole in regenerationregeneration.</p>
<p>Table 3 :
3
Examples of gold hypotheses and hypotheses generated by BIODISCO, with cosine similarity, S ij , computed from their respective embeddings.</p>
<p>Table 4 :
4
Performance of the proposed system by group, reporting Precision, Recall, F1, Accuracy, and Support for each relation and macro average.
GroupPrecision RecallF 1Acc SupportChemical-Gene / negative0.9170.880 0.89850Chemical-Gene / positive0.8850.920 0.90250Chemical-Gene (avg)0.9010.900 0.900 0.900100Disease-Gene / inhibit0.8480.780 0.81250Disease-Gene / stimulate0.7960.860 0.82750Disease-Gene (avg)0.8220.820 0.820 0.820100Gene-Gene / negative0.8670.780 0.82150Gene-Gene / positive0.8000.880 0.83850Gene-Gene (avg)0.8330.830 0.830 0.830100ALL DATA (avg)0.8440.842 0.842 0.850300ConfigurationNoveltyRelevance Significance VerifiabilityOverallGPT-4.1 (baseline)1.38 ± 0.60 3.00 ± 0.00 1.89 ± 0.72 2.82 ± 0.22 2.27 ± 0.31Multi-agent system2.06 ± 0.32 3.00 ± 0.00 2.48 ± 0.10 2.70 ± 0.27 2.56 ± 0.09Multi-agent + tools2.43 ± 0.18 3.00 ± 0.00 2.50 ± 0.00 2.46 ± 0.22 2.60 ± 0.06Multi-agent + refine 2.46 ± 0.14 2.99 ± 0.05 2.50 ± 0.00 2.44 ± 0.24 2.60 ± 0.07BIODISCO2.54 ± 0.14 2.99 ± 0.05 2.55 ± 0.14 2.54 ± 0.42 2.66 ± 0.13</p>
<p>Table 5 :
5
Direct comparison with a evaluator LLM and various ablation configurations of the BIODISCO framework: with and without tools (i.e.KG and literature search), iterative refinement and multi-agentic reasoning.The baseline is a single-agent LLM only.Reported mean ± standard deviation of scores for a set of 100 generated hypothesis.</p>
<p>Table 6 :
6
Parameter estimates α from the fitted Bradley-Terry models for each of the four evaluation metrics
Estimate Std. Err. Statistic p-valueNovelty0.800.362.230.03Relevance0.080.140.540.59Significance0.070.240.280.78Verifiability0.440.152.950.00</p>
<p>Table 7 :
7
Tokens Output Tokens US $ Computation cost under different system settings.All results are based on experiments using GPT-4.1.API cost in United States dollars ($) is reported per hypothesis.All essential components (OpenAI LLM API, Neo4j KG, and open-source retrieval/embedding libraries) are publicly available.
Multi-agent1393264 0.004Multi+Tools8113653 0.017Multi+Refine302631495 0.019BioDisco728284424 0.071</p>
<p>Table 8 :
8
CVD-related human evaluation cases, showing the input, initial hypothesis, and final (refined) hypothesis produced by the system.
InputInitial HypothesisFinal HypothesisInvestigation of molecu-Inhibition of STK31 in NSCLC tumor mi-Inhibition of STK31 in NSCLC tumorlar drivers underlying Tcroenvironments enhances CD8 T-cell func-microenvironments enhances CD8 T-cell exhaustion in non-tionality and mitigates T-cell exhaustion, im-cell functionality by disrupting ETS1 andsmall cell lung cancerproving the efficacy of immune checkpointPIK3R1-mediated immunosuppressive path-(NSCLC), with a fo-inhibitors.ways, while promoting pro-inflammatorycus on identifying novel,cytokine release, thereby synergisticallydruggable targets to en-improving responsiveness to immunehance the efficacy ofcheckpoint inhibitors and overcoming T-cellimmune checkpoint in-exhaustion.hibitor (ICI) therapies.</p>
<p>Generate up to 3 concise, testable biomedical hypotheses.Each hypothesis must be grounded in both the background and KG context, but extend current knowledge with a novel mechanistic, causal, regulatory, or predictive insight.Guidelines: 1. Integrate both background and KG context.2. Propose new biological mechanisms or interactions, not summaries or rephrasings of input.3. Use precise scientific language, including mechanistic verbs such as: activates, inhibits, modulates, represses, etc. 4. Each hypothesis must be a single, plausible, testable sentence (&lt;= 30 words) with clear entities and measurable outcomes.5. Output only the hypotheses, no numbering, bullets, explanations, citations, or evidence fields.Examples (good): Activation of TGF−$\beta$ in smooth muscle cells promotes vascular remodeling in hypertension.Loss of gene X enhances inflammatory response to toxin Y in liver tissue.Examples (bad, avoid): CVD is associated with Wnt signaling and fibrosis.Output: Each line should be a standalone hypothesis.Return exactly one hypothesis per line, and nothing else.</p>
<p>AcknowledgmentsThis work was supported by funding from the German Ministry of Research, Technology and Space (BMFTR) under grant curAIknow 03ZU1202NB, part of project curATime (Cluster for Atherothrombosis and Individualized Medicine; https://curatime.org/).Supplementary materialsThis document forms the appendices for the manuscript "BIODISCO: Multi-agent hypothesis generation with dualmode evidence, iterative feedback and temporal evaluation."Assess the hypothesis using four metrics: Novelty: Does it introduce ideas not present in the background?Relevance: How well does it align with the background and supporting evidence?Significance: What is its potential to advance biological understanding or clinical practice?Verifiability: Can it be reliably tested with current scientific methods?Rate each metric on a 0−5 scale: 0 = no merit, 1 = very slight, 2 = slight, 3 = moderate, 4 = strong, 5 = exceptional.Be conservative: award a 5 only if the hypothesis fully meets the criterion with no reservations.Provide one sentence of rationale per metric.Output: <Metric>: Score <X> <One−sentence rationale> (repeat for all 4 metrics) At the end, write on a separate line: Overall Score: <value>/20G.6 Reviewer agentThe REVIEWER agent identifies weak dimensions in each hypothesis based on the scores and explanations provided by the CriticAgent.To make a sound decision, it receives full feedback from the CRITIC agent, along with the current hypothesis.It then prioritizes low-scoring criteria and, depending on the evaluation content, selectively triggers access to external knowledge sources, including the knowledge graph, literature, or background.The Agent does not directly modify the hypothesis, instead, it outputs retrieval actions along with the relevant supporting information, which are then passed to the Refiner for hypothesis revision.Listing 6: REVIEWER agentG.7 Refiner agentThe REFINER agent revises and improves a given hypothesis based on received feedback and supplemental information.Specifically, it receives the low-scoring content and the integrated complementary knowledge produced by the REVIEWER agent, and the current hypothesis.It integrates low-scoring metrics and their explanations from the CRITIC Agent, along with new knowledge retrieved by the RE-VIEWER Agent.Using LLM synthesizes and restructures multi-source inputs to generate a revised hypothesis with enhanced novelty, verifiability, and scientific relevance.The refinement process explicitly targets previously identified weaknesses, and the resulting hypothesis is returned to the evaluation loop for further evaluation.Listing 7: REFINER agent Example 1 (with helpful new info):Step 1: The hypothesis lacks a mechanism linking Wnt inhibition to reduced fibrosis.Step 2: New PubMed evidence suggests TGF−$\beta$ mediates this process.Step 3: Adding TGF−$\beta$ clarifies the pathway.Inhibition of Wnt signaling reduces cardiac fibrosis via downregulation of TGF−$\beta$ activity.Example 2 (no helpful info):Step 1: The hypothesis lacks a mechanistic link.Step 2: No new information improves this.Step 3: No justified revision possible.Overexpression of SOD2 reduces neurodegeneration by mitigating oxidative stress in dopaminergic neurons.Output:1−4 short reasoning steps (one per line) Final refined hypothesis as the last line (no numbering, no extra text) Instructions:Use only provided context (background + new info).Each reasoning step must be a complete, self− contained sentence.Do not include explanations, citations, or bullet points.H LLM EvaluatorsTwo LLM evaluation paradigms were used: a direct evaluator scores hypotheses on a numerical scale 1-5 for novelty, relevance, significance and verifiability, similar to the CRITIC agent.Listing 8: Direct evaluator By contrast, a pairwise evaluator compares two hypotheses at a time, and is asked to say which of them is better according to each of the four criteria.Ties are allowed.A Bradley-Terry model was then fitted to the outputs; see Section D.Listing 9: Pairwise evaluator
Automating AI Discovery for Biomedicine Through Knowledge Graphs And LLM Agents. N Aamer, M N Asim, S Munir, A Dengel, bioRxiv. 2025</p>
<p>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models. A K Alkan, S Sourav, M Jablonska, S Astarita, R Chakrabarty, N Garuda, P Khetarpal, M Pióro, D Tanoglidis, K G Iyer, arXiv:2504.05496Proceedings of the 2025 Conference of the Nations of the Americas Chapter. Long Papers. L Chiruzzo, A Ritter, L Wang, the 2025 Conference of the Nations of the Americas ChapterAlbuquerque, New MexicoAssociation for Computational Linguistics2025. 20251arXiv preprintA Survey on Hypothesis Generation for Scientific Discovery in the Era of Large Language Models</p>
<p>Why we need to be careful with LLMs in medicine. J.-C Bélisle-Pipon, Frontiers in Medicine. 1114955822024</p>
<p>CARDBiomedBench: A Benchmark for Evaluating Large Language Model Performance in Biomedical Research: A novel question-andanswer benchmark designed to assess Large Language Models' comprehension of biomedical research. O Bianchi, M Willey, C X Alvarado, B Danek, M Khani, N Kuznetsov, A Dadu, S Shah, M J Koretsky, M B Makarious, 2025piloted on Neurodegenerative Diseases. bioRxiv</p>
<p>Rank analysis of incomplete block designs: the method of paired comparisons. R A Bradley, M E Terry, Biometrika. 393-41952</p>
<p>Language models are few-shot learners. Advances in neural information processing systems. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, 202033</p>
<p>Bayesian Item Response Modeling in R with brms and Stan. P.-C Bürkner, Journal of Statistical Software. 10052021</p>
<p>Building a knowledge graph to enable precision medicine. P Chandak, K Huang, M Zitnik, Scientific Data. 20231067</p>
<p>Evidence Extraction to Validate Medical Claims in Fake News Detection. P Deka, A Jurek-Loughrey, International Conference on Health Information Science. Springer2022</p>
<p>Limits to scalable evaluation at the frontier: LLM as Judge won't beat twice the data. F E Dorner, V Y Nastl, M Hardt, arXiv:2410.13341Elo, A. E. 1978. The Rating of Chess Players, Past and Present. New YorkArco Pub. ISBN202406680472169780668047210arXiv preprint</p>
<p>Quasi-variances. D Firth, Biometrika. 9112004</p>
<p>SciAgents: Automating Scientific Discovery Through Bioinspired Multi-Agent Intelligent Graph Reasoning. A Ghafarollahi, M J Buehler, J Gottweis, W.-H Weng, A Daryin, T Tu, A Palepu, P Sirkovic, A Myaskovsky, F Weissenberger, K Rong, R Tanno, K Saab, D Popovici, J Blum, F Zhang, K Chou, A Hassidim, B Gokturk, A Vahdat, P Kohli, Y Matias, A Carroll, K Kulkarni, N Tomasev, Y Guan, V Dhillon, E D Vaishnav, B Lee, T R D Costa, J R Penadés, G Peltz, Y Xu, A Pawlosky, arXiv:2502.188642024Advanced Materials, 2413523. Karthikesalingam, A.; and Natarajan, V. 2025. Towards an AI coscientist</p>
<p>Systematic integration of biomedical knowledge prioritizes drugs for repurposing. D S Himmelstein, A Lizee, C Hessler, L Brueggeman, S L Chen, D Hadley, A Green, P Khankhanian, S E Baranzini, 20176e26726</p>
<p>Automated Hypothesis Validation with Agentic Sequential Falsifications. K Huang, Y Jin, R Li, M Y Li, E Candes, J Leskovec, K Huang, S Zhang, H Wang, Y Qu, Y Lu, Y Roohani, R Li, L Qiu, G Li, J Zhang, D Yin, S Marwaha, J N Carter, X Zhou, M Wheeler, J A Bernstein, M Wang, P He, J Zhou, M Snyder, L Cong, A Regev, J Leskovec, Forty-second International Conference on Machine Learning. 2025a. 2025b</p>
<p>Q Jin, B Dhingra, Z Liu, W W Cohen, X Lu, arXiv:1909.06146Pubmedqa: A dataset for biomedical research question answering. 2019arXiv preprint</p>
<p>BioSimCSE: BioMedical Sentence Embeddings using Contrastive learning. K R Kanakarajan, B Kundumani, A Abraham, M Sankarasubbu, Proceedings of the 13th International Workshop on Health Text Mining and Information Analysis (LOUHI). the 13th International Workshop on Health Text Mining and Information Analysis (LOUHI)Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>J M Laurent, J D Janizek, M Ruzo, M M Hinks, M J Hammerling, S Narayanan, M Ponnapati, A D White, S G Rodriques, arXiv:2407.10362Lab-bench: Measuring capabilities of language models for biology research. 2024arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive NLP tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W.-T Yih, T Rocktäschel, S Riedel, D Kiela, Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS '20. the 34th International Conference on Neural Information Processing Systems, NIPS '20Red Hook, NY, USACurran Associates Inc2020ISBN 9781713829546</p>
<p>H Li, Q Dong, J Chen, H Su, Y Zhou, Q Ai, Z Ye, Y Liu, arXiv:2412.05579LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods. 2024</p>
<p>LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models. H Liu, S Huang, J Hu, Y Zhou, C Tan, A Liusie, P Manakul, M Gales, arXiv:2504.11524Hy-poBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation. the Association for Computational Linguistics. Y Graham, M Purver, 2025. 20241Long Papers</p>
<p>. St, Julian's, Malta, Association for Computational Linguistics</p>
<p>A Liusie, P Manakul, M J Gales, arXiv:2307.07889LLM comparative assessment: Zero-shot NLG evaluation through pairwise comparisons using large language models. 2023arXiv preprint</p>
<p>Knowledge Graphs for drug repurposing: a review of and methods. C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, B P Majumder, H Surana, D Agarwal, B D Mishra, A Meena, A Prakhar, T Vora, T Khot, A Sabharwal, P Clark, P Belmonte-Hernández, A , arXiv:2408.06292arXiv:2407.01725The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. Perdomo-Quinteiro2024. 2024. 202425e461arXiv preprintDiscoverybench: Towards datadriven discovery with large language models</p>
<p>Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation. B Qi, K Zhang, K Tian, H Li, Z.-R Chen, S Zeng, E Hua, H Jinfang, B Zhou, First Conference on Language Modeling. 2024</p>
<p>Gpqa: A graduate-level google-proof q&amp;a benchmark. D Rein, B L Hou, A C Stickland, J Petty, R Y Pang, J Dirani, J Michael, S R Bowman, 2024In First Conference on Language Modeling</p>
<p>Large language models encode clinical knowledge. S Ren, P Jian, Z Ren, C Leng, C Xie, J Zhang, K Singhal, S Azizi, T Tu, S S Mahdavi, J Wei, H W Chung, N Scales, A Tanwani, H Cole-Lewis, S Pfohl, arXiv:2503.24047Towards Scientific Intelligence: A Survey of LLMbased Scientific Agents. 2025. 2023620</p>
<p>Automated hypothesis generation based on mining scientific literature. S Spangler, A D Wilkins, B J Bachman, M Nagarajan, T Dayaram, P J Haas, S Regenbogen, C R Pickering, A Comer, J N Myers, I Stanoi, L Kato, A Lelescu, J J Labrie, N Parikh, A M Lisewski, L A Donehower, Y Chen, O Lichtarge, Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. the 20th ACM SIGKDD international conference on Knowledge discovery and data mining2014</p>
<p>Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking. J Sybrandt, M Shtutman, I Safro, IEEE International Conference on Big Data (Big Data). 2018. 2018</p>
<p>AGATHA: automatic graph mining and transformer based hypothesis generation approach. J Sybrandt, I Tyagin, M Shtutman, I Safro, Proceedings of the 29th ACM international conference on information &amp; knowledge management. the 29th ACM international conference on information &amp; knowledge management2020</p>
<p>G Xiong, E Xie, C Williams, M Kim, A H Shariatmadari, S Guo, S Bekiranov, A Zhang, arXiv:2505.14599Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models. 2025arXiv preprint</p>
<p>PharmKG: a dedicated knowledge graph benchmark for biomedical data mining. S Zheng, J Rao, Y Song, J Zhang, X Xiao, E F Fang, Y Yang, Z Niu, Briefings in Bioinformatics. 2243442020</p>
<p>K Zhou, Y Zhu, Z Chen, W Chen, W X Zhao, X Chen, Y Lin, J.-R Wen, J Han, arXiv:2311.01964Don't Make Your LLM an Evaluation Benchmark Cheater. 2023</p>
<p>Automating AI Discovery for Biomedicine Through Knowledge Graphs And LLM Agents. N Aamer, M N Asim, S Munir, A Dengel, bioRxiv. 2025</p>
<p>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models. J Baek, S K Jauhar, S Cucerzan, S J Hwang, Proceedings of the 2025 Conference of the Nations of the Americas Chapter. the Association for Computational Linguistics: Human Language Technologies. L Chiruzzo, A Ritter, L Wang, the 2025 Conference of the Nations of the Americas ChapterAlbuquerque, New MexicoAssociation for Computational Linguistics20251Long Papers</p>
<p>Rank analysis of incomplete block designs: the method of paired comparisons. R A Bradley, M E Terry, Biometrika. 393-41952</p>
<p>On Extending the Bradley-Terry Model to Accommodate Ties in Paired Comparison Experiments. P.-C Bürkner, R R Davidson, Journal of the American Statistical Association. 1003292021. 1970Journal of statistical software</p>
<p>Quasi-variances. D Firth, Biometrika. 9112004</p>
<p>SciAgents: Automating Scientific Discovery Through Bioinspired Multi-Agent Intelligent Graph Reasoning. D Firth, A Ghafarollahi, M J Buehler, J Gottweis, W.-H Weng, A Daryin, T Tu, A Palepu, P Sirkovic, A Myaskovsky, F Weissenberger, K Rong, R Tanno, K Saab, D Popovici, J Blum, F Zhang, K Chou, A Hassidim, B Gokturk, A Vahdat, P Kohli, Y Matias, A Carroll, K Kulkarni, N Tomasev, Y Guan, V Dhillon, E D Vaishnav, B Lee, T R D Costa, J R Penadés, G Peltz, Y Xu, A Pawlosky, A Karthikesalingam, Natarajan , arXiv:2502.18864Quasi Variances for Factor Effects in Statistical Models. 2025. 2024134Towards an AI coscientist. and Lefkowitz, R. J. 2024. G Protein-Coupled Receptors: A Century of Research and Discovery</p>
<p>Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation. B Qi, K Zhang, K Tian, H Li, Z.-R Chen, S Zeng, E Hua, H Jinfang, B Zhou, First Conference on Language Modeling. 2024</p>
<p>Statistical modelling of citation networks, research influence and journal prestige. D A Selby, 2020University of WarwickPh.D. thesis</p>
<p>Orphan receptor GPR153 facilitates vascular damage responses by modulating cAMP levels, YAP/TAZ signaling, and NF-κB activation. J Shao, J Kwon, T Wang, S Günther, L S Tombor, T Warwick, Z Shaheryar, R P Brandes, S Dimmeler, J Wenzel, Nature Communications. 16162322025</p>
<p>Bradley-Terry Models in R: The BradleyTerry2 Package. H Turner, D Firth, Journal of Statistical Software. 4892012</p>            </div>
        </div>

    </div>
</body>
</html>
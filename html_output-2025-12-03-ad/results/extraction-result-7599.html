<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7599 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7599</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7599</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-4e5f7cd537a1bbcd090f9887b1b59f39a3715dba</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4e5f7cd537a1bbcd090f9887b1b59f39a3715dba" target="_blank">Instruction Induction: From Few Examples to Natural Language Task Descriptions</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work shows that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples, and discovers that the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions.</p>
                <p><strong>Paper Abstract:</strong> Large language models are able to perform a task by conditioning on a few input-output demonstrations - a paradigm known as in-context learning. We show that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples. To explore this ability, we introduce the instruction induction challenge, compile a dataset consisting of 24 tasks, and define a novel evaluation metric based on executing the generated instruction. We discover that, to a large extent, the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions; InstructGPT achieves 65.7% of human performance in our execution-based metric, while the original GPT-3 model reaches only 9.8% of human performance. This surprising result suggests that instruction induction might be a viable learning paradigm in and of itself, where instead of fitting a set of latent continuous parameters to the data, one searches for the best description in the natural language hypothesis space.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7599.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7599.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction‑Induction vs In‑Context</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction Induction Prompting (generate natural-language instruction) versus Standard In‑Context Learning (generate output for held-out input)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of two prompt framings: (a) ask the model to produce a natural-language instruction that explains 5 input-output demonstrations (instruction induction), and (b) standard few-shot in-context learning where the model is given demonstrations and asked to produce the output for a new input; the paper shows large differences in model ability depending on framing and model alignment/size.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002), GPT-3 (davinci, 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer LMs; InstructGPT is the instruction‑tuned variant trained with human feedback to follow natural‑language instructions; GPT‑3 refers to the original next‑token prediction models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B (both largest variants evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction Induction across 24 language tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given 5 input-output demonstrations (each formatted 'Input: x' / 'Output: y'), generate a single natural‑language instruction that, when followed, reproduces the mapping from inputs to outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot natural-language instruction generation from 5 few-shot demonstrations presented as plain text (demonstrations only, no explicit task label), wrapped by a meta-prompt (e.g., 'I gave a friend an instruction and five inputs... The instruction was').</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Five demonstrations concatenated with newline separators; each demo is 'Input: <x>\nOutput: <y>'; meta-prompt frames the examples as a 'challenge puzzle' (example given in paper); greedy decoding used for generation; no fine‑tuning on instruction induction data (zero‑shot).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Reference-based BERTScore (max over gold refs) and execution accuracy (use generated instruction as prompt to an instruction‑tuned execution model and measure task metric).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>InstructGPT: avg BERTScore 44.4 (human gold = 60.0); avg execution accuracy 43.6 (human-written instructions execution = 66.4). GPT-3 (largest published): avg execution accuracy corresponds to only 9.8% of human performance (InstructGPT achieves 65.7% of human performance on execution metric).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Human-written instructions (control): BERTScore ceiling 60.0; execution accuracy ceiling 66.4 (gold references produce ceiling higher).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>InstructGPT is −15.6 points BERTScore absolute versus human gold; GPT‑3 is −45.4 points BERTScore absolute versus human; execution: InstructGPT ≈65.7% of human execution performance while GPT‑3 ≈9.8% of human.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero‑shot instruction generation from 5 demonstrations; 100 sampled induction examples per task; execution model used to measure execution accuracy is the largest InstructGPT (text-davinci-002); greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Induction: From Few Examples to Natural Language Task Descriptions', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7599.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7599.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Meta‑Prompt Sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity to Meta-Prompt Wrapping (different higher-level prompt phrasings around demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The exact meta‑prompt that frames the five demonstrations materially affects instruction induction success; the authors compare several meta-prompts (challenge puzzle, add a name, instruction-before/after demonstrations) and report per-task differences in correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002 (InstructGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction‑tuned Transformer decoder model (InstructGPT, API name text-davinci-002).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Meta-prompt variation evaluated on sampled tasks (First Letter, Passivization, Antonyms, Translation en-de, Sentence Similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given the same 5 input-output demonstrations, vary the surrounding meta-prompt phrasing and measure whether the generated natural-language instruction is judged correct.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Different meta-prompts around identical demonstrations: (a) 'Challenge Puzzle' (original 'I gave a friend an instruction... The instruction was'), (b) 'Challenge Puzzle + Name' (adds a name), (c) 'Instruction After Demonstrations' ('Below are five input-output pairs... Please write the instruction...'), (d) 'Instruction Before Demonstrations' ('You are given five examples... Please write an instruction...')</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / meta-prompt</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>For evaluation they sampled five examples per task and generated one instruction per example per meta-prompt; correctness was manually verified (binary correct/incorrect) for each generated instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Count of correct instructions (manual verification) out of 5 trials per task/meta-prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported correctness (correct / 5) per meta-prompt and task: Challenge Puzzle (Original): First Letter 5/5, Passivization 0/5, Antonyms 1/5, Translation en-de 5/5, Sentence Similarity 4/5. Challenge + Name: 5/5, 0/5, 2/5, 5/5, 4/5. Instruction After Demonstrations: 5/5, 0/5, 3/5, 5/5, 5/5. Instruction Before Demonstrations: 5/5, 0/5, 0/5, 2/5, 3/5.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Placing an explicit instruction-writing request after demonstrations ('Instruction After Demonstrations') improved correctness on some tasks (e.g., Antonyms improved from 1/5 to 3/5 across prompts, Translation en-de dropped from 5/5 to 2/5 when using 'Instruction Before Demonstrations'), demonstrating nontrivial absolute changes up to multiple correct examples out of 5.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Manual verification; small-sample (5 cases per task); model text-davinci-002; comparisons across four meta-prompts as listed; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Induction: From Few Examples to Natural Language Task Descriptions', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7599.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7599.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction Ordering Effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Instruction Placement Relative to Demonstrations (instruction-before vs instruction-after)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Whether an explicit request to write the instruction appears before or after the demonstrations affects the model's ability to produce a correct instruction; 'instruction after demonstrations' generally yielded better correctness than 'instruction before demonstrations' on sampled tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002 (InstructGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction‑tuned Transformer decoder used via OpenAI API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction placement ablation (sampled tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same five demonstrations paired with either a preface asking for an instruction (before) or a postscript asking for an instruction (after); judge resulting instruction correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt text differs only by whether the explicit instruction-writing prompt appears before or after the five 'Input: ... Output: ...' demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / ordering</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Two concrete prompt variants tested: 'Instruction After Demonstrations' (explicit request to write instruction after examples) and 'Instruction Before Demonstrations' (explicit request presented before examples); evaluated on 5 example instances per task, manual correctness check.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Number of correct induced instructions (out of 5) per task variant.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example effect: Translation (en-de) correct counts dropped from 5/5 (Instruction After or Challenge Puzzle) to 2/5 under Instruction Before Demonstrations; Antonyms went from up to 3/5 (after) to 0/5 (before).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Substantial absolute changes in correctness (examples show drops of 3 correct examples out of 5 when moving the instruction prompt before demos), indicating ordering can materially affect instruction induction.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Small-sample manual check; text-davinci-002; greedy decoding; 5 examples per task.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Induction: From Few Examples to Natural Language Task Descriptions', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7599.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7599.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Demonstration Format (Input/Output)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Five 'Input: x / Output: y' Demonstrations on In‑Context and Instruction Induction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The specific demonstration format ('Input: <x>' and 'Output: <y>' lines, five examples) is sufficient for both humans and models to infer and/or execute many tasks via in‑context learning and to enable instruction induction; the number (five) and the explicit labeling were validated via a human study and in‑context verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci family and other sizes), InstructGPT (various sizes incl. text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 family decoder models: original next-token‑prediction GPT-3 variants and instruction‑tuned InstructGPT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Multiple sizes (ada, babbage, curie, davinci / text-ada-001, text-babbage-001, text-curie-001, text-davinci-002); largest ≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>In‑context learning verification across 24 tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>For each task, models were prompted with 5 demonstrations in the 'Input:/Output:' format and then asked to produce the output for a held-out input to verify that tasks can be executed from demonstrations alone.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few‑shot in‑context (5 examples) with explicit 'Input:' and 'Output:' labels; additional test input appended; no explicit task description provided.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>question type / prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Five demonstrations sampled per example; evaluate on 100 repeats per task; demonstrations and test inputs concatenated; in-context results reported per model and task in Table 5.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task-specific accuracy / exact-match / F1 depending on task; reported as percentage correct.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Many tasks attain high in‑context accuracy with this format; examples from Table 5: First Letter — GPT‑3 97%, InstructGPT 98%; Pluralization — GPT‑3 95%, InstructGPT 99%; Sentiment — GPT‑3 95%, InstructGPT 99%; several tasks exceed 80% in‑context accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Providing five explicit 'Input:/Output:' demos enables high in‑context execution accuracy for many tasks (often ≥80%) compared to zero-shot without demos, demonstrating that demonstration presence and explicit labeling substantially improves execution.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>In‑context learning: 5 demos + one test input, 100 trials per task; evaluations use task‑appropriate exact match / F1 metrics; no extra instructions beyond the 'Input:/Output:' format.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Induction: From Few Examples to Natural Language Task Descriptions', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7599.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7599.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation Metric Sensitivity (BERTScore vs Execution)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Format of Generated Instruction Affects Evaluation Outcomes — BERTScore Can Be Inflated by Generic Prompt‑Style Outputs Whereas Execution Accuracy Reveals Functional Correctness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents that certain generated instructions (notably from GPT‑3) are generic imperative templates (e.g., 'Write an output for every input') that superficially overlap with human references and raise reference‑based metrics like BERTScore but fail to guide execution models, producing low execution accuracy — demonstrating that presentation/phrasing of the generated instruction interacts with the chosen evaluation metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci family) and InstructGPT (text-davinci-002) as execution model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder LMs; GPT-3 original models tend to output templatic imperative sentences; InstructGPT used as execution model to measure instruction executability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various; largest ≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction generation evaluation across tasks (reference BERTScore vs execution accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess quality of generated instructions both by reference similarity (BERTScore to gold human refs) and by measuring whether an execution model can follow the generated instruction to produce correct task outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Generated instruction text is evaluated both against gold references (BERTScore) and by using it as prompt for an execution model (execution accuracy); model phrasing influences both metrics differently.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / evaluation format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Observation: GPT‑3 often generates short, generic imperative outputs that achieve moderate overlap with reference annotations (inflating BERTScore) but yield near‑zero execution accuracy; execution accuracy is measured by prompting InstructGPT with the generated instruction and inputs and applying task metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BERTScore (reference overlap) and execution accuracy (functional metric using execution model).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative result: GPT‑3 outputs often obtain nontrivial BERTScore overlap yet typically very low execution accuracy; InstructGPT outputs tend to have higher BERTScore and substantially higher execution accuracy. (No single aggregated numeric for the inflation effect is provided, but per-task discrepancies are described in §6 and Table 3 examples illustrate the phenomenon.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Reference-based metrics can overestimate instruction quality when generated instructions are templatic; execution accuracy provides a more robust functional assessment — the paper documents cases where GPT‑3 BERTScore is nonzero but execution accuracy is near zero.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Evaluation uses gold-reference BERTScore (DeBERTa-xlMNLI backbone) and execution accuracy by prompting InstructGPT as execution model; greedy decoding; manual qualitative analysis for failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Induction: From Few Examples to Natural Language Task Descriptions', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7599.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7599.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Executor Choice / Bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Choice of Execution Model Affects Measured Execution Accuracy (using same model as inducer can bias metric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Execution accuracy depends on which model is used to 'execute' generated instructions; the authors use the largest InstructGPT as the execution model and note that this choice may bias execution accuracy in favor of its own generations, but they justify it because no other model followed instructions as well in preliminary experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002 (InstructGPT) used as execution model; other models (GPT-3 variants) compared)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>InstructGPT is an instruction‑tuned Transformer variant; used both to generate instructions and to execute them for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Execution accuracy evaluation of generated instructions across 24 tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measure how well an execution model follows a generated instruction by providing the instruction plus input and comparing model outputs to gold outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Execution model receives: Instruction I (generated) + input x; it produces output which is scored with task metric; average over 100 held‑out execute examples per task.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>evaluation setting / prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Execution accuracy measured by invoking InstructGPT with the generated natural‑language instruction and held‑out inputs; authors acknowledge potential bias since InstructGPT was also used to generate instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Execution accuracy (task-specific metrics averaged over execute set).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Authors report execution accuracy averages using InstructGPT as executor (e.g., InstructGPT achieves avg execution accuracy 43.6; human-written instructions measured via same executor reach 66.4). The authors caveat that using the same high-performing instruction-following model could favor its own prompt styles.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Human-written instructions execution (control): 66.4 average execution accuracy (gold references used as ceiling).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Potential positive bias toward InstructGPT generations when using InstructGPT as executor; authors explicitly state this limitation but note other models were unable to follow instructions as well in preliminary tests.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Execution measured over 100 held-out examples per task; execution model = text-davinci-002; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Induction: From Few Examples to Natural Language Task Descriptions', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 2)</em></li>
                <li>Prompt programming for large language models: Beyond the few-shot paradigm <em>(Rating: 2)</em></li>
                <li>Do prompt-based models really understand the meaning of their prompts? <em>(Rating: 2)</em></li>
                <li>Finetuned language models are zero-shot learners <em>(Rating: 2)</em></li>
                <li>Rethinking the role of demonstrations: What makes in-context learning work? <em>(Rating: 2)</em></li>
                <li>Calibrate before use: Improving few-shot performance of language models <em>(Rating: 1)</em></li>
                <li>GRIPS: Gradient-free, edit-based instruction search for prompting large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7599",
    "paper_id": "paper-4e5f7cd537a1bbcd090f9887b1b59f39a3715dba",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Instruction‑Induction vs In‑Context",
            "name_full": "Instruction Induction Prompting (generate natural-language instruction) versus Standard In‑Context Learning (generate output for held-out input)",
            "brief_description": "Comparison of two prompt framings: (a) ask the model to produce a natural-language instruction that explains 5 input-output demonstrations (instruction induction), and (b) standard few-shot in-context learning where the model is given demonstrations and asked to produce the output for a new input; the paper shows large differences in model ability depending on framing and model alignment/size.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002), GPT-3 (davinci, 175B)",
            "model_description": "Decoder-only Transformer LMs; InstructGPT is the instruction‑tuned variant trained with human feedback to follow natural‑language instructions; GPT‑3 refers to the original next‑token prediction models.",
            "model_size": "≈175B (both largest variants evaluated)",
            "task_name": "Instruction Induction across 24 language tasks",
            "task_description": "Given 5 input-output demonstrations (each formatted 'Input: x' / 'Output: y'), generate a single natural‑language instruction that, when followed, reproduces the mapping from inputs to outputs.",
            "problem_format": "Zero-shot natural-language instruction generation from 5 few-shot demonstrations presented as plain text (demonstrations only, no explicit task label), wrapped by a meta-prompt (e.g., 'I gave a friend an instruction and five inputs... The instruction was').",
            "format_category": "prompt style",
            "format_details": "Five demonstrations concatenated with newline separators; each demo is 'Input: &lt;x&gt;\\nOutput: &lt;y&gt;'; meta-prompt frames the examples as a 'challenge puzzle' (example given in paper); greedy decoding used for generation; no fine‑tuning on instruction induction data (zero‑shot).",
            "performance_metric": "Reference-based BERTScore (max over gold refs) and execution accuracy (use generated instruction as prompt to an instruction‑tuned execution model and measure task metric).",
            "performance_value": "InstructGPT: avg BERTScore 44.4 (human gold = 60.0); avg execution accuracy 43.6 (human-written instructions execution = 66.4). GPT-3 (largest published): avg execution accuracy corresponds to only 9.8% of human performance (InstructGPT achieves 65.7% of human performance on execution metric).",
            "baseline_performance": "Human-written instructions (control): BERTScore ceiling 60.0; execution accuracy ceiling 66.4 (gold references produce ceiling higher).",
            "performance_change": "InstructGPT is −15.6 points BERTScore absolute versus human gold; GPT‑3 is −45.4 points BERTScore absolute versus human; execution: InstructGPT ≈65.7% of human execution performance while GPT‑3 ≈9.8% of human.",
            "experimental_setting": "Zero‑shot instruction generation from 5 demonstrations; 100 sampled induction examples per task; execution model used to measure execution accuracy is the largest InstructGPT (text-davinci-002); greedy decoding.",
            "statistical_significance": null,
            "uuid": "e7599.0",
            "source_info": {
                "paper_title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Meta‑Prompt Sensitivity",
            "name_full": "Sensitivity to Meta-Prompt Wrapping (different higher-level prompt phrasings around demonstrations)",
            "brief_description": "The exact meta‑prompt that frames the five demonstrations materially affects instruction induction success; the authors compare several meta-prompts (challenge puzzle, add a name, instruction-before/after demonstrations) and report per-task differences in correctness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-002 (InstructGPT)",
            "model_description": "Instruction‑tuned Transformer decoder model (InstructGPT, API name text-davinci-002).",
            "model_size": "≈175B",
            "task_name": "Meta-prompt variation evaluated on sampled tasks (First Letter, Passivization, Antonyms, Translation en-de, Sentence Similarity)",
            "task_description": "Given the same 5 input-output demonstrations, vary the surrounding meta-prompt phrasing and measure whether the generated natural-language instruction is judged correct.",
            "problem_format": "Different meta-prompts around identical demonstrations: (a) 'Challenge Puzzle' (original 'I gave a friend an instruction... The instruction was'), (b) 'Challenge Puzzle + Name' (adds a name), (c) 'Instruction After Demonstrations' ('Below are five input-output pairs... Please write the instruction...'), (d) 'Instruction Before Demonstrations' ('You are given five examples... Please write an instruction...')",
            "format_category": "prompt style / meta-prompt",
            "format_details": "For evaluation they sampled five examples per task and generated one instruction per example per meta-prompt; correctness was manually verified (binary correct/incorrect) for each generated instruction.",
            "performance_metric": "Count of correct instructions (manual verification) out of 5 trials per task/meta-prompt.",
            "performance_value": "Reported correctness (correct / 5) per meta-prompt and task: Challenge Puzzle (Original): First Letter 5/5, Passivization 0/5, Antonyms 1/5, Translation en-de 5/5, Sentence Similarity 4/5. Challenge + Name: 5/5, 0/5, 2/5, 5/5, 4/5. Instruction After Demonstrations: 5/5, 0/5, 3/5, 5/5, 5/5. Instruction Before Demonstrations: 5/5, 0/5, 0/5, 2/5, 3/5.",
            "baseline_performance": null,
            "performance_change": "Placing an explicit instruction-writing request after demonstrations ('Instruction After Demonstrations') improved correctness on some tasks (e.g., Antonyms improved from 1/5 to 3/5 across prompts, Translation en-de dropped from 5/5 to 2/5 when using 'Instruction Before Demonstrations'), demonstrating nontrivial absolute changes up to multiple correct examples out of 5.",
            "experimental_setting": "Manual verification; small-sample (5 cases per task); model text-davinci-002; comparisons across four meta-prompts as listed; greedy decoding.",
            "statistical_significance": null,
            "uuid": "e7599.1",
            "source_info": {
                "paper_title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Instruction Ordering Effect",
            "name_full": "Effect of Instruction Placement Relative to Demonstrations (instruction-before vs instruction-after)",
            "brief_description": "Whether an explicit request to write the instruction appears before or after the demonstrations affects the model's ability to produce a correct instruction; 'instruction after demonstrations' generally yielded better correctness than 'instruction before demonstrations' on sampled tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-002 (InstructGPT)",
            "model_description": "Instruction‑tuned Transformer decoder used via OpenAI API.",
            "model_size": "≈175B",
            "task_name": "Instruction placement ablation (sampled tasks)",
            "task_description": "Same five demonstrations paired with either a preface asking for an instruction (before) or a postscript asking for an instruction (after); judge resulting instruction correctness.",
            "problem_format": "Prompt text differs only by whether the explicit instruction-writing prompt appears before or after the five 'Input: ... Output: ...' demonstrations.",
            "format_category": "prompt style / ordering",
            "format_details": "Two concrete prompt variants tested: 'Instruction After Demonstrations' (explicit request to write instruction after examples) and 'Instruction Before Demonstrations' (explicit request presented before examples); evaluated on 5 example instances per task, manual correctness check.",
            "performance_metric": "Number of correct induced instructions (out of 5) per task variant.",
            "performance_value": "Example effect: Translation (en-de) correct counts dropped from 5/5 (Instruction After or Challenge Puzzle) to 2/5 under Instruction Before Demonstrations; Antonyms went from up to 3/5 (after) to 0/5 (before).",
            "baseline_performance": null,
            "performance_change": "Substantial absolute changes in correctness (examples show drops of 3 correct examples out of 5 when moving the instruction prompt before demos), indicating ordering can materially affect instruction induction.",
            "experimental_setting": "Small-sample manual check; text-davinci-002; greedy decoding; 5 examples per task.",
            "statistical_significance": null,
            "uuid": "e7599.2",
            "source_info": {
                "paper_title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Demonstration Format (Input/Output)",
            "name_full": "Effect of Five 'Input: x / Output: y' Demonstrations on In‑Context and Instruction Induction",
            "brief_description": "The specific demonstration format ('Input: &lt;x&gt;' and 'Output: &lt;y&gt;' lines, five examples) is sufficient for both humans and models to infer and/or execute many tasks via in‑context learning and to enable instruction induction; the number (five) and the explicit labeling were validated via a human study and in‑context verification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (davinci family and other sizes), InstructGPT (various sizes incl. text-davinci-002)",
            "model_description": "GPT-3 family decoder models: original next-token‑prediction GPT-3 variants and instruction‑tuned InstructGPT variants.",
            "model_size": "Multiple sizes (ada, babbage, curie, davinci / text-ada-001, text-babbage-001, text-curie-001, text-davinci-002); largest ≈175B",
            "task_name": "In‑context learning verification across 24 tasks",
            "task_description": "For each task, models were prompted with 5 demonstrations in the 'Input:/Output:' format and then asked to produce the output for a held-out input to verify that tasks can be executed from demonstrations alone.",
            "problem_format": "Few‑shot in‑context (5 examples) with explicit 'Input:' and 'Output:' labels; additional test input appended; no explicit task description provided.",
            "format_category": "question type / prompt style",
            "format_details": "Five demonstrations sampled per example; evaluate on 100 repeats per task; demonstrations and test inputs concatenated; in-context results reported per model and task in Table 5.",
            "performance_metric": "Task-specific accuracy / exact-match / F1 depending on task; reported as percentage correct.",
            "performance_value": "Many tasks attain high in‑context accuracy with this format; examples from Table 5: First Letter — GPT‑3 97%, InstructGPT 98%; Pluralization — GPT‑3 95%, InstructGPT 99%; Sentiment — GPT‑3 95%, InstructGPT 99%; several tasks exceed 80% in‑context accuracy.",
            "baseline_performance": null,
            "performance_change": "Providing five explicit 'Input:/Output:' demos enables high in‑context execution accuracy for many tasks (often ≥80%) compared to zero-shot without demos, demonstrating that demonstration presence and explicit labeling substantially improves execution.",
            "experimental_setting": "In‑context learning: 5 demos + one test input, 100 trials per task; evaluations use task‑appropriate exact match / F1 metrics; no extra instructions beyond the 'Input:/Output:' format.",
            "statistical_significance": null,
            "uuid": "e7599.3",
            "source_info": {
                "paper_title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Evaluation Metric Sensitivity (BERTScore vs Execution)",
            "name_full": "Format of Generated Instruction Affects Evaluation Outcomes — BERTScore Can Be Inflated by Generic Prompt‑Style Outputs Whereas Execution Accuracy Reveals Functional Correctness",
            "brief_description": "The paper documents that certain generated instructions (notably from GPT‑3) are generic imperative templates (e.g., 'Write an output for every input') that superficially overlap with human references and raise reference‑based metrics like BERTScore but fail to guide execution models, producing low execution accuracy — demonstrating that presentation/phrasing of the generated instruction interacts with the chosen evaluation metric.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (davinci family) and InstructGPT (text-davinci-002) as execution model",
            "model_description": "Decoder LMs; GPT-3 original models tend to output templatic imperative sentences; InstructGPT used as execution model to measure instruction executability.",
            "model_size": "various; largest ≈175B",
            "task_name": "Instruction generation evaluation across tasks (reference BERTScore vs execution accuracy)",
            "task_description": "Assess quality of generated instructions both by reference similarity (BERTScore to gold human refs) and by measuring whether an execution model can follow the generated instruction to produce correct task outputs.",
            "problem_format": "Generated instruction text is evaluated both against gold references (BERTScore) and by using it as prompt for an execution model (execution accuracy); model phrasing influences both metrics differently.",
            "format_category": "prompt style / evaluation format",
            "format_details": "Observation: GPT‑3 often generates short, generic imperative outputs that achieve moderate overlap with reference annotations (inflating BERTScore) but yield near‑zero execution accuracy; execution accuracy is measured by prompting InstructGPT with the generated instruction and inputs and applying task metrics.",
            "performance_metric": "BERTScore (reference overlap) and execution accuracy (functional metric using execution model).",
            "performance_value": "Qualitative result: GPT‑3 outputs often obtain nontrivial BERTScore overlap yet typically very low execution accuracy; InstructGPT outputs tend to have higher BERTScore and substantially higher execution accuracy. (No single aggregated numeric for the inflation effect is provided, but per-task discrepancies are described in §6 and Table 3 examples illustrate the phenomenon.)",
            "baseline_performance": null,
            "performance_change": "Reference-based metrics can overestimate instruction quality when generated instructions are templatic; execution accuracy provides a more robust functional assessment — the paper documents cases where GPT‑3 BERTScore is nonzero but execution accuracy is near zero.",
            "experimental_setting": "Evaluation uses gold-reference BERTScore (DeBERTa-xlMNLI backbone) and execution accuracy by prompting InstructGPT as execution model; greedy decoding; manual qualitative analysis for failure modes.",
            "statistical_significance": null,
            "uuid": "e7599.4",
            "source_info": {
                "paper_title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Executor Choice / Bias",
            "name_full": "Choice of Execution Model Affects Measured Execution Accuracy (using same model as inducer can bias metric)",
            "brief_description": "Execution accuracy depends on which model is used to 'execute' generated instructions; the authors use the largest InstructGPT as the execution model and note that this choice may bias execution accuracy in favor of its own generations, but they justify it because no other model followed instructions as well in preliminary experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-002 (InstructGPT) used as execution model; other models (GPT-3 variants) compared)",
            "model_description": "InstructGPT is an instruction‑tuned Transformer variant; used both to generate instructions and to execute them for evaluation.",
            "model_size": "≈175B",
            "task_name": "Execution accuracy evaluation of generated instructions across 24 tasks",
            "task_description": "Measure how well an execution model follows a generated instruction by providing the instruction plus input and comparing model outputs to gold outputs.",
            "problem_format": "Execution model receives: Instruction I (generated) + input x; it produces output which is scored with task metric; average over 100 held‑out execute examples per task.",
            "format_category": "evaluation setting / prompt style",
            "format_details": "Execution accuracy measured by invoking InstructGPT with the generated natural‑language instruction and held‑out inputs; authors acknowledge potential bias since InstructGPT was also used to generate instructions.",
            "performance_metric": "Execution accuracy (task-specific metrics averaged over execute set).",
            "performance_value": "Authors report execution accuracy averages using InstructGPT as executor (e.g., InstructGPT achieves avg execution accuracy 43.6; human-written instructions measured via same executor reach 66.4). The authors caveat that using the same high-performing instruction-following model could favor its own prompt styles.",
            "baseline_performance": "Human-written instructions execution (control): 66.4 average execution accuracy (gold references used as ceiling).",
            "performance_change": "Potential positive bias toward InstructGPT generations when using InstructGPT as executor; authors explicitly state this limitation but note other models were unable to follow instructions as well in preliminary tests.",
            "experimental_setting": "Execution measured over 100 held-out examples per task; execution model = text-davinci-002; greedy decoding.",
            "statistical_significance": null,
            "uuid": "e7599.5",
            "source_info": {
                "paper_title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Prompt programming for large language models: Beyond the few-shot paradigm",
            "rating": 2
        },
        {
            "paper_title": "Do prompt-based models really understand the meaning of their prompts?",
            "rating": 2
        },
        {
            "paper_title": "Finetuned language models are zero-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Rethinking the role of demonstrations: What makes in-context learning work?",
            "rating": 2
        },
        {
            "paper_title": "Calibrate before use: Improving few-shot performance of language models",
            "rating": 1
        },
        {
            "paper_title": "GRIPS: Gradient-free, edit-based instruction search for prompting large language models",
            "rating": 1
        }
    ],
    "cost": 0.0177465,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Instruction Induction: From Few Examples to Natural Language Task Descriptions</h1>
<p>Or Honovich ${ }^{\tau}$ Uri Shaham ${ }^{\tau}$ Samuel R. Bowman ${ }^{\nu}$ Omer Levy ${ }^{\tau \mu}$<br>${ }^{\tau}$ Tel Aviv University<br>${ }^{\nu}$ New York University<br>${ }^{\mu}$ Meta AI</p>
<h4>Abstract</h4>
<p>Large language models are able to perform a task by conditioning on a few input-output demonstrations - a paradigm known as incontext learning. We show that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples. To explore this ability, we introduce the instruction induction challenge, compile a dataset consisting of 24 tasks, and define a novel evaluation metric based on executing the generated instruction. We discover that, to a large extent, the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions; InstructGPT achieves $65.7 \%$ of human performance in our execution-based metric, while the original GPT-3 model reaches only $9.8 \%$ of human performance. This surprising result suggests that instruction induction might be a viable learning paradigm in and of itself, where instead of fitting a set of latent continuous parameters to the data, one searches for the best description in the natural language hypothesis space. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Large language models (LMs) can perform unseen tasks by conditioning on a few labeled examples, effectively inferring the underlying tasks through a process known as in-context learning (Brown et al., 2020). However, task inference is implicit, and the ability of models to explicitly reason about it remains unexplored. In this work, we show that LMs can explicitly describe an underlying task, in natural language, given a few labeled examples.</p>
<p>We introduce the instruction induction challenge, in which a model is provided with a few inputoutput demonstrations, and is requested to generate a natural language instruction describing the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>connection between the input-output pairs. In our experiments, inducing instructions is done in a zeroshot manner by simply prompting the models to explain a small set of given demonstrations, as shown in Figure 1; we do not perform fine-tuning or use any labeled instruction induction data.</p>
<p>We examine instruction induction on 24 tasks, ranging from morphosyntactic tasks to style transfer and sentiment analysis. Since our goal is to shed light on the phenomenon of instruction induction, we focus on tasks that have clear and simple instructions. As a basic evaluation protocol, we collect human annotations and use them as gold-standard references; the generated instructions are then compared to these references using BERTScore (Zhang et al., 2020). Moreover, we suggest a novel evaluation metric for instruction induction: execution accuracy. The execution accuracy of a generated instruction is measured by testing whether LMs can correctly perform the task in a zero-shot manner by using the generated instruction alone, without any demonstrations.</p>
<p>Our experiments reveal a surprising ability at generating correct instructions. The bestperforming model, InstructGPT (Ouyang et al., 2022), achieves an average BERTScore of 44.4, compared to human performance of 60.0; when measuring execution accuracy, the model reaches 43.6, with human-written instructions reaching 66.4. For some tasks, the model's performance is on par or even better than human performance. When qualitatively examining the generated instructions, we often observe accurate instructions, even for some of the more challenging tasks. For instance, in the task of formality style transfer, generated instructions include "Translate the inputs into more formal language" and "Use formal language". For semantic text similarity, the generated instructions include "For each input, rate the similarity of the two sentences on a scale of 0 to 5 , with 5 being a perfect match" and "Determine whether</p>
<table>
<thead>
<tr>
<th style="text-align: center;">In-Context Learning</th>
<th style="text-align: center;">Instruction Induction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Input: As soon as you can. Output: At your earliest convenience.</td>
<td style="text-align: center;">I gave a friend an instruction and five inputs. The friend read the instruction and wrote an output for every one of the inputs. Here are the input-output pairs:</td>
</tr>
<tr>
<td style="text-align: center;">Input: Sorry I messed up. Output: I apologise for my wrongdoings.</td>
<td style="text-align: center;">Input: As soon as you can. Output: At your earliest convenience.</td>
</tr>
<tr>
<td style="text-align: center;">Input: I can't stand his temper. Output: I cannot tolerate his temper.</td>
<td style="text-align: center;">Input: Sorry I messed up. Output: I apologise for my wrongdoings.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The instruction was translate the inputs into more formal language.</td>
</tr>
</tbody>
</table>
<p>Figure 1: An example of instruction induction for the task of formality style transfer. Left: the standard in-context learning setting; given five demonstrations, complete the sixth. Right: instruction induction; the language model is prompted to generate a natural language instruction that describes the demonstrations. Model completions are in blue, prompt templates are in pink.
the two sentences are about the same thing".
Despite these impressive results, we find that this ability is currently unique to InstructGPT (Ouyang et al., 2022), which is both very large (175B parameters) and was especially fine-tuned to follow instructions. Ablations on smaller versions of InstructGPT as well as the original 175B-parameter GPT-3 (Brown et al., 2020) yield dramatically weaker performance. These findings are in line with recent work showing that increasing model size unlocks new capabilities (Chowdhery et al., 2022; Ganguli et al., 2022), and serves as additional evidence for the strength of instruction tuning (Sanh et al., 2022; Wei et al., 2022a; Ouyang et al., 2022), perhaps even pointing to the necessity of complementing standard next-word prediction with additional objectives.</p>
<p>The fact that models can induce natural language instructions suggests that instruction induction may serve as a learning paradigm of its own, where the optimization goal is to find the best natural language description that fits the observations. In this ambitious view of instruction induction, natural language can function as the hypothesis space, and a model is required to learn a natural language rule describing the relation between inputs and outputs in the training examples, rather than a set of uninterpretable parameters. While we currently provide a proof-of-concept for that idea, extending it by grounding models in natural language has the immediate benefit of human interpretability,
explainability, and verifiability, while potentially alleviating overfitting and other issues associated with spurious correlations.</p>
<h2>2 Instruction Induction</h2>
<p>We begin by formulating the task of instruction induction. Given a sequence of $n$ demonstrations $\left{x_{k}, y_{k}\right}<em k="k">{k \in{1, \ldots, n}}$, the goal is to generate a single natural language instruction, such that for each $x</em>$. This format is similar to in-context learning (Brown et al., 2020), only here the desired output is an instruction describing the relation between the inputs and outputs of the demonstrations. We require models to perform this in a zero-shot setting, without any fine-tuning on labeled data. Figure 1 illustrates the difference between standard in-context prompting and instruction-induction prompting.}$, following the instruction results in $y_{k</p>
<p>To elicit models to generate instructions, we consider prompts that would elicit humans to do so. We design a meta-prompt presenting instruction induction as a challenge puzzle and verify its clarity in a human study (§3.3). The prompt is presented in Figure 1 (right side, in pink). ${ }^{2}$</p>
<p>While prior work already shows that large LMs are often able to infer a latent task from a given set of demonstrations, this has been largely based on their ability to execute the task on a held-out exam-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>ple. Instruction induction requires that the model describe the underlying task in natural language.</p>
<h2>3 Data</h2>
<p>We evaluate on 24 tasks. Example tasks are listed in Table 1. See Table 4 in Appendix A for the full list of tasks. We select these tasks as they vary in difficulty and represent different aspects of language understanding, ranging from surface-level spelling to sentence similarity and causality detection. ${ }^{3}$ Since our primary goal is to study the phenomenon of instruction induction under lab conditions, we focus on tasks that have simple instructions and defer tasks with more complicated instructions for future work. We review the dataset's format, the annotation and verification processes we conducted to ensure that the tasks are viable, and finally discuss a theoretical limitation of this setup.</p>
<h3>3.1 Format</h3>
<p>In every task, each single demonstration $\left(x_{k}, y_{k}\right)$ is formatted as follows:</p>
<p>$$
\begin{aligned}
&amp; \text { Input: } x_{k} \
&amp; \text { Output: } y_{k}
\end{aligned}
$$</p>
<p>For instance, one demonstration in the pluralization task is "Input: cat" followed by "Output: cats" in a new line. We split each task's demonstrations into two sets: an induce set, which we use for generating instructions, and an execute set, which is held out for the execution accuracy evaluation metric (see §4.2). Each instruction induction example is composed of 5 demonstrations sampled randomly without replacement from the induce set, concatenated with new-line separators; we create 100 examples for each task. When generating instructions, each example is placed inside the instruction induction prompt, and fed to the model (Figure 1, right).</p>
<h3>3.2 Annotating Reference Instructions</h3>
<p>We collect 10 gold-reference human-annotated instructions via college-graduate English-speaking annotators. For each task, we provide the annotators with the exact same input we intend to provide a model: 5 input-output demonstrations wrapped by the instruction-induction prompt (Figure 1). We manually verify each annotation and discard ones that do not correctly describe the task. We refer to this set of annotations as the gold annotations, and use them for reference-based evaluation (see §4).</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>3.3 Verification</h3>
<p>Prior to the instruction induction experiments, we conduct two tests to ensure that either models or humans can infer the underlying task given 5 demonstrations. We first verify that models can indeed execute our tasks given 5 demonstrations using incontext learning. Secondly, we conduct a human study to confirm that 5 demonstrations are enough for humans to describe the latent tasks.</p>
<p>In-Context Learning We prompt models with 5 input-output demonstrations and concatenate an additional test input $x_{k+1}$, and verify that the models are able to correctly predict $y_{k+1}$ (Figure 1, left). For each task, we repeat this experiment 100 times, each with a different set of demonstrations and test inputs. We do not provide the model with any instruction beyond the "Input: $x_{k}$ Output: $y_{k}$ " format. We evaluate each task using its predefined evaluation metric. ${ }^{4}$ The in-context results for GPT-3 (Brown et al., 2020) and InstructGPT (Ouyang et al., 2022) (see model details in §5) are reported in Table 5 in Appendix B, which shows that in-context learning can reach $80 \%$ accuracy and above on most tasks.</p>
<p>Human Study To assess the human ability to induce instructions, we collect human-written instructions, using annotators that did not participate in the gold references collection. As in the goldreference annotation process, we provide annotators with the same input we intend to provide to models. We refer to this set of annotations as the control annotations. We then manually count, for each task, the number of annotators that provided a correct instruction, and report the correct instructions percentage in Table 5 (Appendix B). In all but one task (Larger Animal), at least 4 out of 5 annotators were able to produce correct task descriptions.</p>
<p>We also use the control group's annotations to establish a human baseline for automatic evaluation metrics. For reference-based evaluation (§4.1), we treat the control annotations as generated instructions and compare them against the gold annotations, while for execution accuracy (§4.2), we use the control annotations to measure human performance, and the gold references as a ceiling metric.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Instruction</th>
<th style="text-align: center;">Demonstration</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Spelling</td>
<td style="text-align: center;">First Letter</td>
<td style="text-align: center;">Extract the first letter of the input word.</td>
<td style="text-align: center;">cat $\rightarrow \mathrm{c}$</td>
</tr>
<tr>
<td style="text-align: center;">Syntax</td>
<td style="text-align: center;">Negation</td>
<td style="text-align: center;">Negate the input sentence.</td>
<td style="text-align: center;">Time is finite $\rightarrow$ Time is not finite.</td>
</tr>
<tr>
<td style="text-align: center;">Lexical <br> Semantics</td>
<td style="text-align: center;">Antonyms</td>
<td style="text-align: center;">Write a word that means the opposite of the input word.</td>
<td style="text-align: center;">won $\rightarrow$ lost</td>
</tr>
<tr>
<td style="text-align: center;">Phonetics</td>
<td style="text-align: center;">Rhymes</td>
<td style="text-align: center;">Write a word that rhymes with the input word.</td>
<td style="text-align: center;">sing $\rightarrow$ ring</td>
</tr>
<tr>
<td style="text-align: center;">Semantics</td>
<td style="text-align: center;">Cause Selection</td>
<td style="text-align: center;">Find which of the two given cause and effect sentences is the cause.</td>
<td style="text-align: center;">Sentence 1: The soda went flat. Sentence 2: The bottle was left open. $\rightarrow$ The bottle was left open.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Common <br> Concept</td>
<td style="text-align: center;">Find a common characteristic for the given objects.</td>
<td style="text-align: center;">guitars, pendulums, neutrinos $\rightarrow$ involve oscillations.</td>
</tr>
<tr>
<td style="text-align: center;">Style</td>
<td style="text-align: center;">Formality</td>
<td style="text-align: center;">Rephrase the sentence in formal language.</td>
<td style="text-align: center;">Please call once you get there $\rightarrow$ Please call upon your arrival.</td>
</tr>
<tr>
<td style="text-align: center;">Numerical</td>
<td style="text-align: center;">Sum</td>
<td style="text-align: center;">Sum the two given numbers.</td>
<td style="text-align: center;">$2210 \rightarrow 32$</td>
</tr>
<tr>
<td style="text-align: center;">Multi- <br> lingual</td>
<td style="text-align: center;">Translation</td>
<td style="text-align: center;">Translate the word into German / Spanish / French.</td>
<td style="text-align: center;">game $\rightarrow$ juego</td>
</tr>
<tr>
<td style="text-align: center;">GLUE</td>
<td style="text-align: center;">Sentiment <br> Analysis</td>
<td style="text-align: center;">Determine whether a movie review is positive or negative.</td>
<td style="text-align: center;">The film is small in scope, yet perfectly formed. $\rightarrow$ positive</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentence <br> Similarity</td>
<td style="text-align: center;">Rate the semantic similarity of two input sentences on a scale of 0 - definitely not to 5 - perfectly.</td>
<td style="text-align: center;">Sentence 1: A man is smoking. Sentence 2: A man is skating. $\rightarrow 0$ - definitely not</td>
</tr>
</tbody>
</table>
<p>Table 1: Example tasks used in our instruction-induction experiments. For each task, we show a corresponding instruction and demonstration, with $\rightarrow$ separating the input from the output.</p>
<h3>3.4 Ambiguity</h3>
<p>A theoretical challenge in inducing instructions is ambiguity. For example, when given the single demonstration "Input: The coffee is too hot. Output: The, too, hot", one could infer that the underlying task is either "write all the words containing the letter T" or "write all the three-lettered words", both valid interpretations. Ambiguity might confuse models tasked with instruction induction while also making evaluation less reliable. In practice, providing 5 demonstrations typically resolves the ambiguity in our set of tasks. As evident from the data verification process, our tasks can typically be inferred by models and/or humans.</p>
<p>Inducing more complex task descriptions, such as predicting detailed annotation guidelines, may pose a greater challenge in terms of ambiguity. We hypothesize that providing more than 5 demonstrations could mitigate some of that challenge, and leave further exploration of this avenue to future work.</p>
<h2>4 Evaluating Generated Instructions</h2>
<p>As a standard text generation metric, we report BERTScore (Zhang et al., 2020). However, the instruction induction challenge has a unique property,
which does not usually hold for other text generation tasks: the instructions are executable. Their correctness can therefore be measured directly by utilizing them as prompts.</p>
<h3>4.1 Reference-Based Evaluation</h3>
<p>We use BERTScore (Zhang et al., 2020) to compare the model-generated instructions against the collected gold annotations. As mentioned in §3.2, we use only the correct, verified annotations as references. We take the maximal BERTScore-F1 over all gold-reference annotations to account for natural variations in instruction formulation. ${ }^{5}$ We also establish a human baseline for each task using the control annotations, which were collected from a separate control group of annotators (§3.3), which we compare against the gold annotations in exactly the same way as model-generated instructions. In preliminary studies, we experiment with other reference-based metrics (ROUGE and BLEU), and find BERTScore to be a better predictor of instruction quality, although all metrics showed similar trends.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>4.2 Execution Accuracy</h3>
<p>We introduce execution accuracy, a new metric unique to the instruction induction task. We define a correct instruction as one that can guide humans to produce the expected output. To approximate human behavior, we use an instruction-tuned model and test whether it can follow the generated instruction. Concretely, to measure the execution accuracy of a predicted instruction $I$ (e.g., "Write the plural form of the given word.") for a task $T$ (pluralization), we prompt a model with $I$ and an input $x$ ("cat"). We then test, given $I$ and $x$, whether the model can correctly predict $y$, the output of performing $T$ on the input $x$ (cats).</p>
<p>To obtain meaningful results, we measure execution accuracy on the 100 held-out execute examples for each task. The execution accuracy of an instruction $I$ is therefore computed by taking the average over $\operatorname{Score}<em n="n">{T}\left(I\left(x</em>$ in the execute set, where Score $}\right), y_{n}\right)$ for all $x_{n<em n="n">{T}$ denotes the task's corresponding metric (see Appendix A), and $I\left(x</em>$}\right)$ is the result of prompting a predefined language model with the instruction $I$ and the input $x_{n}$. As recent models are trained to follow instructions (Sanh et al., 2022; Wei et al., 2022a; Ouyang et al., 2022), and due to the relative clarity of our tasks, we expect correct instructions to yield high execution accuracy when using a sufficiently powerful execution model. ${ }^{6</p>
<h2>5 Results</h2>
<p>Baseline Models We experiment with eight versions of GPT-3 (Brown et al., 2020), a Transformer decoder language model. First, we experiment with the most current version available in the OpenAI API, for each of the four available model sizes. Though not stated explicitly in the API, we assume these models are those reported by Ouyang et al. (2022), and we therefore refer to them as Instruct models. ${ }^{7}$ We also experiment with the four originally published GPT-3 versions. ${ }^{8}$ By default, we refer to the largest Instruct model as InstructGPT, and the original 175B-parameter model as GPT3. All model generations were produced using the greedy decoding algorithm.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: Average BERTScore and execution accuracy across tasks. BERTScore is measured against the gold references. The execution accuracy for all generated instructions is measured using InstructGPT as the execution model. Human performance is measured using the human control group's instructions.</p>
<h3>5.1 Comparing to Gold Annotations</h3>
<p>Figure 2a presents the average BERTScore per task (see §4.1). Results show that the InstructGPT model has, to some extent, the ability to induce instructions from a few demonstrations; in 13 out of 24 tasks it achieves at least $75 \%$ of human performance. GPT-3, on the other hand, is quite far from human performance across the board.</p>
<p>Table 2 shows the average scores across all tasks. We observe the same trend; while InstructGPT's BERTScore is 15.6 points lower than human performance, the gap between GPT-3 and humans is 45.4 points. Moreover, we observe that smaller models - even those fine-tuned to follow instructions - do not exhibit any instruction-induction abilities. Scores are slightly higher for larger models of the same family (except for the InstructGPT-Babbage outlier), but are overall low. Excluding the largest models, there does not appear to be a significant advantage for Instruct models over the originals when controlling for model size.</p>
<h3>5.2 Execution Accuracy</h3>
<p>We compute the execution accuracy as detailed in $\S 4.2$, and report the average over 100 generated instructions for each task. As an execution model, we use the largest InstructGPT model. We also use this model to induce instructions, and while using it as an execution model might bias results towards its own generations, preliminary experiments show that no other model is as good at following instructions as InstructGPT. As a point of reference, we</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: (a) Average BERTScores of model-generated instructions for each task, compared to the performance of the control group's manually-authored instructions. The BERTScore for each instruction is computed using the human gold annotations as references. (b) Average execution accuracy of model-generated instructions for each task, compared to the execution accuracy measured for human-written instructions. The Human baseline is measured by taking the control group's annotations, while the Gold ceiling metric is based on the separately-annotated and verified gold annotations.</p>
<p>apply the execution accuracy evaluation protocol to human-written instructions. First, to compare models with human performance, we measure the execution accuracy of the control annotation set. Second, to account for limitations in the execution model, we measure execution accuracy of the correct (manually verified) gold annotations, which acts as an approximated ceiling metric.</p>
<p>Figure 2 b presents the execution accuracy per task. In 12 out of 24 tasks, InstructGPT achieves at least $75 \%$ of the execution accuracy measured for the human-written instructions. GPT-3 shows much weaker execution accuracy, scoring less than $10 \%$ on 20 of the 24 tasks. In fact, only in the cases of formality, passivization, and cause selection does it approach human performance, and that is largely an artifact of a more lenient evaluation metric in the case of formality and cause selection, or due to the execution model being right for the wrong reasons in the case of passivization (see §6). In some tasks, the control annotations are of high quality and reach a higher score than the verified gold annotations, likely due to variance of the execution model in such cases.</p>
<p>Table 2 shows the same trends. On average, InstructGPT achieves $65.7 \%$ of human performance, while GPT-3 reaches only $9.8 \%$ of human performance. When considering different model families or sizes, we do not see any substantial improvements when increasing model size or adding instruction tuning, with the exception of the largest InstructGPT model. The ability to generate instructions seems to only emerge when a model is both large enough and aligned to follow instructions. Overall, even the best-performing model still does not reach human performance, leaving room for future improvement.</p>
<h2>6 Analysis</h2>
<p>To gain further insight into the successes and failures of instruction induction prompting, we manually analyze the model-generated instructions of 5 tasks. Table 3 shows the most common predictions of GPT-3 and InstructGPT for each of these tasks.</p>
<p>InstructGPT obtains high, or close to human execution accuracy scores for three of these tasks (First Letter, Sentence Similarity, Pluralization). Indeed, the instructions for both First Letter and Sentence Similarity accurately describe the task. However, the instruction generated for Pluralization is not entirely precise, since it dismisses other forms of
pluralization such as -es, -ies, and irregulars. Although the instruction only asks to add an "s", the execution model often ignores the specifics and produces the correct plural form; in one case, the input word was "life" and the output was "lives". While this particular instruction accounts for $24 \%$ of the induced instructions in the pluralization task, some predictions do explicitly mention pluralization, though not always accurately, e.g., "Add -s to the end of each word to make it plural".</p>
<p>For some tasks, InstructGPT fails to produce accurate instructions, even if it is able to solve via in-context learning (see Table 5). In Passivization, $98 \%$ of the predicted instructions were to simply "reverse the order of the subject and object", while ignoring additional surface-form manipulations needed to convert the given sentence into passive form; e.g., for the input "The authors supported the scientist", following the instructions produces the output "The scientist supported the authors", while the correct passive form is "The scientist was supported by the authors". Surprisingly, the instructions generated by GPT-3 obtained higher execution accuracy than the InstructGPT, even though they were entirely unrelated. In $24 \%$ of the cases, GPT-3 predicted "The friend wrote the following output:" - an instruction that apparently prompts the execution model to often rephrase the input in passive form. Lastly, in Antonyms, $60 \%$ of InstructGPT's predictions were "Reverse the input", and another $11 \%$ were "Reverse the word". While one could imagine an interpretation of these instructions that reflects the task (reversing the meaning of the word), the execution model interprets them literally, and reverses the input words' letters.</p>
<p>Overall, GPT-3 did not exhibit any instruction induction abilities, although it did often phrase outputs in imperative language. One relatively common prediction was the generic instruction "Write an output for every input". Because these empty instructions are in the right format, they tend to have some overlap with the reference instructions, which inflates their BERTScore. Execution accuracy, on the other hand, is robust to this phenomenon, and typically assigns GPT-3's outputs very low scores.</p>
<h2>7 Related Work</h2>
<p>In-Context Learning Brown et al. (2020) suggest that models can learn a task by conditioning on few input-output demonstration pairs, without any fine-tuning or gradient updates. This paradigm,</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">GPT-3</th>
<th style="text-align: left;">InstructGPT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">First letter</td>
<td style="text-align: left;">The friend's output was:</td>
<td style="text-align: left;">Write the first letter of each word.</td>
</tr>
<tr>
<td style="text-align: left;">Sentence Similarity</td>
<td style="text-align: left;">The friend wrote the following output:</td>
<td style="text-align: left;">For each input, rate the similarity of the two sentences on a <br> scale of 0 to 5, with 5 being a perfect match.</td>
</tr>
<tr>
<td style="text-align: left;">Pluralization</td>
<td style="text-align: left;">The friend's output was:</td>
<td style="text-align: left;">Add 's' to the end of each word.</td>
</tr>
<tr>
<td style="text-align: left;">Passivization</td>
<td style="text-align: left;">The friend wrote the following output:</td>
<td style="text-align: left;">Reverse the order of the subject and the object in the sentence.</td>
</tr>
<tr>
<td style="text-align: left;">Antonyms</td>
<td style="text-align: left;">The friend's output was:</td>
<td style="text-align: left;">Reverse the input.</td>
</tr>
</tbody>
</table>
<p>Table 3: Examples of the instructions generated by GPT-3 and InstructGPT for five of our tasks.
known as in-context learning or prompt-based learning (Liu et al., 2021), has been the focus of many research efforts lately: Du et al. (2021) suggest methods for more efficient in-context learning, Zhao et al. (2021) study methods for improving the stability and accuracy of prompt-based models, Chen et al. (2021) and Min et al. (2022a) conduct meta-training with an in-context learning objective, while other work studies the effect of the provided prompts (Reynolds and McDonell, 2021; Webson and Pavlick, 2021; Min et al., 2022b), or suggests prompt reframing techniques (Mishra et al., 2021) and prompt retrieval methods (Rubin et al., 2021). To the best of our knowledge, all previous work study in-context learning through the lens of executing a latent task, while we focus on the ability to explicitly describe it.</p>
<p>The Instruction Paradigm Efrat and Levy (2020) propose to learn new tasks from natural language instructions. Mishra et al. (2022) and Wang et al. (2022b) collect crowdsourcing instructions used to create NLP datasets into a benchmark for measuring the ability to solve tasks by reading instructions. Recent work shows that fine-tuning on task instructions (instruction tuning) improves the zero-shot learning abilities of LMs (Sanh et al., 2022; Wei et al., 2022a; Ouyang et al., 2022). Prasad et al. (2022) introduce an edit-based search approach for improving existing instructions used for prompting. In this work, we focus on models' ability to generate instructions, rather than their ability to execute instructions written by humans.</p>
<p>Intermediate Reasoning Steps Nye et al. (2022) show that LMs can perform complex computations by writing intermediate steps on a "scratchpad". In chain of thought prompting (Wei et al., 2022b), input-output demonstrations are enriched with sentences elaborating intermediate task reasoning steps, improving the performance of LMs
on tasks requiring reasoning skills. Subsequent work further improves the performance on such tasks using a self-consistency ensemble (Wang et al., 2022a), which samples a set of diverse chain-of-thought reasoning paths, taking the majority vote over all generated answers. Zelikman et al. (2022) utilize a small set of examples labeled with chain-of-thought rationales and a large set of unlabeled data to iteratively bootstrap automatic rationale generation, thus creating a large dataset labeled with such rationales to enable fine-tuning. In contrast, we study the ability of LMs to generate a description of the task, rather than generating intermediate reasoning steps as a means of executing complex tasks.</p>
<p>Learning a Natural Language Hypothesis Zhong et al. (2022) propose to automatically describe the differences between two data distributions $D_{0}$ and $D_{1}$ by finding a description that is more true for $D_{1}$, e.g., "is military related" or "is longer in sentence length". They frame this task as learning a natural language hypothesis. In this work, we suggest describing a task based on demonstrations of this task alone, rather than describing the differences between two data distributions.</p>
<h2>8 Discussion</h2>
<p>This work demonstrates that large LMs can not only infer new tasks based on a handful of demonstrations, but also describe them in natural language. We provide evidence of this ability on a diverse set of language tasks, and show that while instruction induction abilities are limited to a single state-of-the-art model, this model does indeed approach human performance on about half the tasks.</p>
<p>It is not unreasonable to assume that models in the near future will be even better at processing human-generated instructions, and it is therefore interesting to discuss the potential applications of</p>
<p>instruction induction. In particular, we envision a use case in which instruction induction serves as a machine learning approach; instead of converting a dataset into a set of continuous parameters, we could produce a natural language instruction that best describes the data. Grounding the model in concise natural language has the advantage of interpretability, and has the potential to solve fundamental issues pertaining to spurious correlations. While it is still too early to determine whether this approach is viable, we view it as an intriguing direction for future research.</p>
<h2>9 Limitations</h2>
<p>Since our primary goal is to study the phenomenon of instruction induction under lab conditions, we focus on tasks that have simple instructions. Future work may extend instruction induction research by including tasks with more complex instructions. These tasks are expected to pose a greater evaluation challenge, especially when considering reference-based methods. Evaluating through execution accuracy, however, may mitigate some of that challenge. Additionally, only one model showed instruction induction abilities, i.e., text-davinci-002. The exact implementation details of the model and its training data are not publicly available, thus we are unable to investigate the reason behind the emergence of this ability. However, we note that our goal is to present the phenomenon of instruction induction and to raise the ambitious possibility of instruction induction as a learning paradigm. Thus, our goal is not to focus on specific models but rather to shed light on this unexplored phenomenon. Finally, we point to a limitation of the execution accuracy metric, namely assuming the existence of a good-enough instruction-tuned model. Due to recent interest and progress in instruction tuning, we believe this to be a reasonable assumption.</p>
<h2>Ethics Statement</h2>
<p>We believe that inducing instructions, as well as grounding in natural language in general, can potentially improve interpretability and explainability. We therefore view this line of research as having a positive effect on the ability to avoid unwanted artifacts.</p>
<h2>References</h2>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Daniel Cer, Mona Diab, Eneko Agirre, Iñigo LopezGazpio, and Lucia Specia. 2017. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1-14, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. 2021. Meta-learning via language model in-context tuning.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontuek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.</p>
<p>Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathy MeierHellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. 2021. Glam: Efficient scaling of language models with mixture-of-experts.</p>
<p>Avia Efrat and Omer Levy. 2020. The turking test: Can language models understand instructions?</p>
<p>Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique Laforest, and Elena Simperl. 2018. T-REx: A large scale alignment of natural language with knowledge base triples. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).
C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</p>
<p>Deep Ganguli, Danny Hernandez, Liane Lovitt, Nova DasSarma, Tom Henighan, Andy Jones, Nicholas Joseph, Jackson Kernion, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Nelson Elhage, Sheer El Showk, Stanislav Fort, Zac Hatfield-Dodds, Scott Johnston, Shauna Kravec, Neel Nanda, Kamal Ndousse, Catherine Olsson, Daniela Amodei, Dario Amodei, Tom Brown, Jared Kaplan, Sam McCandlish, Chris Olah, and Jack Clark. 2022. Predictability and surprise in large generative models.</p>
<p>Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta: Decoding-enhanced bert with disentangled attention. In International Conference on Learning Representations.</p>
<p>Nora Kassner and Hinrich Schütze. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7811-7818, Online. Association for Computational Linguistics.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing.</p>
<p>Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428-3448, Florence, Italy. Association for Computational Linguistics.</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. In International Conference on Learning Representations.</p>
<p>Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022a. MetaICL: Learning to learn in context. In NAACL-HLT.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022b. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. 2021. Reframing instructional prompts to gptk's language.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470-3487, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Nikita Nangia, Adina Williams, Angeliki Lazaridou, and Samuel Bowman. 2017. The RepEval 2017 shared task: Multi-genre natural language inference with sentence representations. In Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP, pages 1-10, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2022. Show your work: Scratchpads for intermediate computation with language models. In Deep Learning for Code Workshop.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Mohammad Taher Pilehvar and Jose Camacho-Collados. 2019. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1267-1273, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. 2022. Grips: Gradient-free, edit-based instruction search for prompting large language models.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, CHI EA '21, New York, NY, USA. Association for Computing Machinery.</p>
<p>Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2021. Learning to retrieve prompts for in-context learning.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA. Association for Computational Linguistics.</p>
<p>Robyn Speer and Catherine Havasi. 2012. Representing general relational knowledge in ConceptNet 5. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), pages 3679-3686, Istanbul, Turkey. European Language Resources Association (ELRA).</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2020. oLMpics-on what language model pre-training captures. Transactions of the Association for Computational Linguistics, 8:743-758.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353-355, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022a. Self-consistency improves chain of thought reasoning in language models.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, et al. 2022b. Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv.</p>
<p>Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2018. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471.</p>
<p>Albert Webson and Ellie Pavlick. 2021. Do promptbased models really understand the meaning of their prompts?</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022a. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models.</p>
<p>Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. 2018. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3911-3921, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Eric Zelikman, Yuhuai Wu, and Noah D. Goodman. 2022. Star: Bootstrapping reasoning with reasoning.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In ICLR 2020.</p>
<p>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In ICML, pages 12697-12706.</p>
<p>Ruiqi Zhong, Charlie Snell, Dan Klein, and Jacob Steinhardt. 2022. Describing differences between text distributions with natural language. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 27099-27116. PMLR.</p>
<h2>A Dataset Details</h2>
<p>This appendix presents the full list of tasks (§A.1) and details each task's dataset (§A.2). Some datasets rely on a set of common English nouns (CEN), described at §A.3.</p>
<h2>A. 1 Full Dataset</h2>
<p>Table 4 presents the full list of tasks used in our experiments.</p>
<h2>A. 2 Tasks</h2>
<p>We elaborate on each task's data source, preprocessing protocol, and evaluation metric used in the in-context learning and execution accuracy experiments. As mentioned in $\S 3$, each task has induce and execute sets; unless stated otherwise, we sample 100 examples as the execute set for each task. When evaluating outputs, the generated text is first normalized; we take only the first generated sentence and lowercase it. We apply exact string match as the evaluation metric where applicable, elaborating only where alternative metrics are used.</p>
<p>First Letter In each demonstration, $x_{k}$ is a noun, and $y_{k}$ is the first letter of that noun. We construct the demonstrations by extracting the first letter of each word in CEN.</p>
<p>Second Letter Identical to the First Letter task, only here $y_{k}$ is the second letter of $x_{k}$.</p>
<p>List Letters $x_{k}$ is a noun from CEN, and $y_{k}$ is a list of $x_{k}$ 's letters, separated by spaces.</p>
<p>Starting With $x_{k}$ contains a sentence and a letter in brackets, and $y_{k}$ lists the words in $x_{k}$ that start with the given letter. We avoid cases in which $y_{k}$ is empty, i.e., there is always at least one word in the input sentence starting with the given letter. Sentences are taken from the CoLA dataset (Warstadt et al., 2018). For the induce set, we create all (sentence, letter) pairs using CoLA's train set, and then sample 3,000 pairs. For the execute set, we create all (sentence, letter) pairs from CoLA's in-domain and out-of-domain dev sets, and then sample 50 in-domain and 50 out-of-domain examples. We evaluate using exact set match, by treating the output (and $y_{k}$ ) as a set of strings.</p>
<p>Pluralization Given a singular noun $x_{k}$, produce the plural form $y_{k}$. We take noun inputs from the CEN set, filtering out mass nouns using a prede-
fined list. ${ }^{9}$ To create the plural forms, we apply an automatic pluralization engine ${ }^{10}$ and exclude nouns for which the engine's output did not appear at least 50 times in the Wikitext-103 corpus. This results in 2,043 singular-plural noun pairs.</p>
<p>Passivization Given a simple active sentence $x_{k}$, rephrase the sentence in passive voice $y_{k}$. We use the 1,000 HANS (McCoy et al., 2019) evaluation set active-passive entailed sentence pairs.</p>
<p>Negation $y_{k}$ is the negation of the input sentence $x_{k}$. We use the negated LAMA dataset (Petroni et al., 2019; Kassner and Schütze, 2020), taking the 304 negated SQuAD (Rajpurkar et al., 2016) sentences, 300 ConceptNet (Speer and Havasi, 2012) sentences, 200 T-REx (Elsahar et al., 2018) sentences and 200 Google-RE ${ }^{11}$ sentences. For ConceptNet and T-REx, we manually select these sentences to ensure their quality. For Google-RE, we automatically sample 100 sentences from the place of birth relation, and 100 from the place of death relation.</p>
<p>Antonyms $y_{k}$ is the antonym of the input word $x_{k}$. We use the antonym pairs from oLMpics (Talmor et al., 2020), which were extracted from ConceptNet (Speer and Havasi, 2012) and WordNet (Fellbaum, 1998). For uniformity, we verify that all pairs are indeed antonyms according to WordNet.</p>
<p>Synonyms $x_{k}$ is a word and $y_{k}$ is its synonym. As in the antonyms task, we use the synonym pairs of Talmor et al. (2020). Since there can be multiple synonyms for each input word, the task's incontext and execution accuracy are evaluated by testing whether the gold answer (a single word) is contained in the predicted answer (which may be a list of words).</p>
<p>Membership $x_{k}$ is a list of words, where some of the words represent animals, and $y_{k}$ lists the animals from $x_{k}$. To construct the task's data, we first select 6 word categories: animals, clothing, colors, food, vehicles, and professions. We then take 10-50 words from each category, using only words that are categorized at the A1 or A2 levels according to the Common European Framework of</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Instruction</th>
<th style="text-align: center;">Demonstration</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Spelling</td>
<td style="text-align: center;">First Letter</td>
<td style="text-align: center;">Extract the first letter of the input word.</td>
<td style="text-align: center;">cat $\rightarrow \mathrm{c}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Second Letter</td>
<td style="text-align: center;">Extract the second letter of the input word.</td>
<td style="text-align: center;">cat $\rightarrow \mathrm{a}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">List Letters</td>
<td style="text-align: center;">Break the input word into letters, separated by spaces.</td>
<td style="text-align: center;">cat $\rightarrow \mathrm{c}$ a t</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Starting With</td>
<td style="text-align: center;">Extract the words starting with a given letter from the input sentence.</td>
<td style="text-align: center;">The man whose car I hit last week sued me. $[\mathrm{m}] \rightarrow$ man, me</td>
</tr>
<tr>
<td style="text-align: center;">Morpho- <br> syntax</td>
<td style="text-align: center;">Pluralization</td>
<td style="text-align: center;">Convert the input word to its plural form.</td>
<td style="text-align: center;">cat $\rightarrow$ cats</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Passivization</td>
<td style="text-align: center;">Write the input sentence in passive form.</td>
<td style="text-align: center;">The artist introduced the scientist. $\rightarrow$ The scientist was introduced by the artist.</td>
</tr>
<tr>
<td style="text-align: center;">Syntax</td>
<td style="text-align: center;">Negation</td>
<td style="text-align: center;">Negate the input sentence.</td>
<td style="text-align: center;">Time is finite $\rightarrow$ Time is not finite.</td>
</tr>
<tr>
<td style="text-align: center;">Lexical <br> Semantics</td>
<td style="text-align: center;">Antonyms</td>
<td style="text-align: center;">Write a word that means the opposite of the input word.</td>
<td style="text-align: center;">won $\rightarrow$ lost</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Synonyms</td>
<td style="text-align: center;">Write a word with a similar meaning to the input word.</td>
<td style="text-align: center;">alleged $\rightarrow$ supposed</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Membership</td>
<td style="text-align: center;">Write all the animals that appear in the given list.</td>
<td style="text-align: center;">cat, helicopter, cook, whale, frog, lion $\rightarrow$ frog, cat, lion, whale</td>
</tr>
<tr>
<td style="text-align: center;">Phonetics</td>
<td style="text-align: center;">Rhymes</td>
<td style="text-align: center;">Write a word that rhymes with the input word.</td>
<td style="text-align: center;">sing $\rightarrow$ ring</td>
</tr>
<tr>
<td style="text-align: center;">Knowledge</td>
<td style="text-align: center;">Larger Animal</td>
<td style="text-align: center;">Write the larger of the two given animals.</td>
<td style="text-align: center;">koala, snail $\rightarrow$ koala</td>
</tr>
<tr>
<td style="text-align: center;">Semantics</td>
<td style="text-align: center;">Cause Selection</td>
<td style="text-align: center;">Find which of the two given cause and effect sentences is the cause.</td>
<td style="text-align: center;">Sentence 1: The soda went flat. Sentence 2: The bottle was left open. $\rightarrow$ The bottle was left open.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Common <br> Concept</td>
<td style="text-align: center;">Find a common characteristic for the given objects.</td>
<td style="text-align: center;">guitars, pendulums, neutrinos $\rightarrow$ involve oscillations.</td>
</tr>
<tr>
<td style="text-align: center;">Style</td>
<td style="text-align: center;">Formality</td>
<td style="text-align: center;">Rephrase the sentence in formal language.</td>
<td style="text-align: center;">Please call once you get there $\rightarrow$ Please call upon your arrival.</td>
</tr>
<tr>
<td style="text-align: center;">Numerical</td>
<td style="text-align: center;">Sum</td>
<td style="text-align: center;">Sum the two given numbers.</td>
<td style="text-align: center;">$2210 \rightarrow 32$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Difference</td>
<td style="text-align: center;">Subtract the second number from the first.</td>
<td style="text-align: center;">$3222 \rightarrow 10$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Number to Word</td>
<td style="text-align: center;">Write the number in English words.</td>
<td style="text-align: center;">$26 \rightarrow$ twenty-six</td>
</tr>
<tr>
<td style="text-align: center;">Multi- <br> lingual</td>
<td style="text-align: center;">Translation</td>
<td style="text-align: center;">Translate the word into German / Spanish / French.</td>
<td style="text-align: center;">game $\rightarrow$ juego</td>
</tr>
<tr>
<td style="text-align: center;">GLUE</td>
<td style="text-align: center;">Sentiment <br> Analysis</td>
<td style="text-align: center;">Determine whether a movie review is positive or negative.</td>
<td style="text-align: center;">The film is small in scope, yet perfectly formed. $\rightarrow$ positive</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentence <br> Similarity</td>
<td style="text-align: center;">Rate the semantic similarity of two input sentences on a scale of 0 - definitely not to 5 - perfectly.</td>
<td style="text-align: center;">Sentence 1: A man is smoking. Sentence 2: A man is skating. $\rightarrow 0$ - definitely not</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Word in Context</td>
<td style="text-align: center;">Determine whether an input word has the same meaning in the two input sentences.</td>
<td style="text-align: center;">Sentence 1: Approach a task. Sentence 2: To approach the city. Word: approach $\rightarrow$ not the same</td>
</tr>
</tbody>
</table>
<p>Table 4: The tasks in our instruction-induction experiments. For each task, we show a corresponding instruction and demonstration, with $\rightarrow$ separating the input from the output.</p>
<p>Reference for Languages (CEFR). ${ }^{12}$ Using these words, we create random lists containing between 5 to 7 words, where 3 or 4 are animals and the rest belong to one of the other 5 categories. The induce split is constructed by sampling 3,000 such combinations, using $80 \%$ of each category's words. The execute split is constructed by sampling 100 such combinations, using the remaining $20 \%$ of each category's words. The task's in-context and execution accuracy are evaluated using an exact set match, by treating the output (and $y_{k}$ ) as a set of strings.</p>
<p>Rhymes $y_{k}$ is a rhyme of the input word $x_{k}$. The data was constructed by taking words categorized at the A1, A2, or B1 levels according to CEFR. We then use CMU's pronouncing dictionary ${ }^{13}$ to find rhyming groups for these words. The execute split is constructed by sampling 30 rhyming groups, each containing two or more words, and sampling 100 unique words. The induce split is constructed using the rest of the rhyming groups. We evaluate this task by checking whether the predicted word is contained in the rhyming group of $x_{k}$.</p>
<p>Larger Animal $x_{k}$ is two animals, and $y_{k}$ is the (physically) larger one. We use the object comparison data from oLMpics (Talmor et al., 2020), taking the train split, which only contains animals. We construct the induce set using a sample of $80 \%$ of the animals and the execute set by sampling 100 pairs out of the remaining $20 \%$ animals.</p>
<p>Cause Selection $x_{k}$ contains two sentences describing related events, where one event caused the other; $y_{k}$ contains the cause sentence. As data source, we use the 50 examples from the BIGbench (Srivastava et al., 2022) Cause and Effect task, randomly splitting them to equally-sized induce and execute sets. In each of the induce demonstrations, we randomly sample the position of the cause sentence (either the first or the second sentence in $x_{k}$ ). For examples in the execute set, we take both options for each cause and effect pair, doubling the data.</p>
<p>Common Concept $x_{k}$ contains a few entities that share a non-trivial common underlying concept, while $y_{k}$ describes that common concept. We use the 32 examples from Novel Concepts in BIG-</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>bench (Srivastava et al., 2022), using half for induce and half for execute. As the BIG-bench answers usually contain clear "task markers" (e.g., answers that start with "They all have...", indicating that the task was to find a common concept), we remove them from our demonstrations. The task's in-context and execution accuracy are evaluated using unigram overlap (F1).</p>
<p>Formality $x_{k}$ is a sentence in informal English, and $y_{k}$ is its paraphrase in more formal language. We write 30 sentence pairs ourselves, following existing guidelines for converting informal sentences into formal ones. ${ }^{14}$ The task's in-context and execution accuracy are evaluated using unigram overlap (F1).</p>
<p>Sum $x_{k}$ contains two numbers separated by a space, and $y_{k}$ is their sum. For each number in the range $[0,99]$, we enumerate over all pairs.</p>
<p>Difference $x_{k}$ contains two numbers separated by a space, and $y_{k}$ is the difference between them. We use all number pairs such that both input numbers are in the range $[0,198]$, and always subtract the smaller number from the bigger number.</p>
<p>Number to Word $x_{k}$ is a number written in digits (e.g., 28), and $y_{k}$ is the same number written in words (e.g, twenty-eight). We use all numbers in range $[0,9999]$.</p>
<p>Translation $x_{k}$ is an English word and $y_{k}$ is its translation to some target language - either German, Spanish, or French. We use CEN as input words, and obtain their translations via Wiktionary. ${ }^{15}$ For evaluation, we check whether the predicted answer is contained in the set of the possible gold answers.</p>
<p>Sentiment Analysis $x_{k}$ is a movie review and $y_{k}$ is a binary label, either "positive" or "negative", marking the review's sentiment. We use the Stanford Sentiment Treebank dataset (Socher et al., 2013) from GLUE (Wang et al., 2018), taking the train split as our induce set and the dev split as the execute set. We consider only full sentences, discarding sentence constituents and sentences containing more than 10 words. This leaves us with</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>an induce set of 1,167 examples. To create labelbalanced instruction induction examples, we sample each sequence of 5 demonstrations such that there are at least 2 demonstrations for each label.</p>
<p>Sentence Similarity $x_{k}$ contains two sentences, and $y_{k}$ reflects the semantic similarity of the two input sentences. The similarity is measured on a scale of 0 to 5 , and the labels contain an additional short textual description of the numerical label, e.g., " 5 - perfectly". We use the Semantic Textual Similarity Benchmark dataset (Cer et al., 2017) from GLUE, rounding the similarity scores and taking the train split as the induce set and the dev split as the execute set. We discard examples in which at least one of the sentences contains more than 10 words, which leaves us with an induce set of 3,716 examples. In each instruction induction example, we sample at least one pair with a score of 0 and one with a score of 5 , so that models will be exposed to the minimal and maximal scores when generating an instruction. We evaluate whether the predicted answer matches one of three valid outputs for each label: the numerical label (" 5 "), the verbal label ("perfectly"), or the combined label ("5 - perfectly").</p>
<p>Word in Context $x_{k}$ contains a target word and two contexts (sentences) for that word, and $y_{k}$ is a binary label reflecting whether the word has the same meaning in both contexts. We use the Word in Context dataset (Pilehvar and Camacho-Collados, 2019) from SuperGLUE (Wang et al., 2019), taking the train split as the induce set and the dev split as the execute set. We discard examples in which at least one of the sentences contains more than 10 words, which leaves us with an induce set of 4,084 examples. To create label-balanced instruction induction examples, we sample each sequence of 5 demonstrations such that there are at least 2 demonstrations for each label. We evaluate whether the predicted label matches one of several possible outputs: "same", "yes", or "true" for an identical meaning, and "not the same", "no", or "false" for a different meaning.</p>
<h2>A. 3 Common English Nouns</h2>
<p>We create a dataset of common English nouns (CEN) by filtering high-frequency nouns from the Wikitext-103 corpus (Merity et al., 2017). We first create a vocabulary of the 10,000 most frequent words in the corpus, from which we will later select the nouns. We then process the corpus with</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">In-Context Learning</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Human <br> Study</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">InstructGPT</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">First Letter</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Second Letter</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">List Letters</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Starting With</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;">Pluralization</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Passivization</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;">Negation</td>
<td style="text-align: center;">94</td>
<td style="text-align: center;">93</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Antonyms</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Synonyms</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;">Membership</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Rhymes</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Larger Animal</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: center;">Cause Selection</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Common Concept</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Formality</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;">Sum</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Diff</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Number To Word</td>
<td style="text-align: center;">85</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Translation en-de</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">85</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Translation en-es</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Translation en-fr</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;">Sentiment</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Sentence Similarity</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;">Word in Context</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">80</td>
</tr>
</tbody>
</table>
<p>Table 5: Data verification results. The in-context learning scores show how well models can infer our tasks, and the human study scores show how often humans write the correct instruction given the instruction induction prompt. All scores above or equal to $80 \%$ are in bold.</p>
<p>SpaCy's part-of-speech tagger and lemmatizer, ${ }^{16}$ and retain only nouns that appear in their singular form by verifying that their part-of-speech tag is "NN" and testing whether the word's lemma is identical to the word itself. We additionally filter nouns that have less than 3 letters. Overall, this leaves us with a set of 3,406 nouns.</p>
<h2>B Data Verification</h2>
<p>Table 5 shows the results for the data verification experiments (§3.3). As evident by these results, most of our tasks can be inferred in-context by models. Moreover, all tasks but one can be accurately described by at least 4 out 5 human annotators.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta-Prompt</th>
<th style="text-align: center;">First <br> Letter</th>
<th style="text-align: center;">Passivization</th>
<th style="text-align: center;">Antonyms</th>
<th style="text-align: center;">Translation <br> en-de</th>
<th style="text-align: center;">Sentence <br> Similarity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Challenge Puzzle (Original)</td>
<td style="text-align: center;">$5 / 5$</td>
<td style="text-align: center;">$0 / 5$</td>
<td style="text-align: center;">$1 / 5$</td>
<td style="text-align: center;">$5 / 5$</td>
<td style="text-align: center;">$4 / 5$</td>
</tr>
<tr>
<td style="text-align: left;">Challenge Puzzle + Name</td>
<td style="text-align: center;">$5 / 5$</td>
<td style="text-align: center;">$0 / 5$</td>
<td style="text-align: center;">$2 / 5$</td>
<td style="text-align: center;">$5 / 5$</td>
<td style="text-align: center;">$4 / 5$</td>
</tr>
<tr>
<td style="text-align: left;">Instruction After Demonstrations</td>
<td style="text-align: center;">$5 / 5$</td>
<td style="text-align: center;">$0 / 5$</td>
<td style="text-align: center;">$3 / 5$</td>
<td style="text-align: center;">$5 / 5$</td>
<td style="text-align: center;">$5 / 5$</td>
</tr>
<tr>
<td style="text-align: left;">Instruction Before Demonstrations</td>
<td style="text-align: center;">$5 / 5$</td>
<td style="text-align: center;">$0 / 5$</td>
<td style="text-align: center;">$0 / 5$</td>
<td style="text-align: center;">$2 / 5$</td>
<td style="text-align: center;">$3 / 5$</td>
</tr>
</tbody>
</table>
<p>Table 6: The number of correct instructions generated by text-davinci-002, out of the five examples tested for each task, as inspected for each meta-prompt.</p>
<h2>C Meta-Prompt Analysis</h2>
<p>As language models are known to be sensitive to the meta-prompt wrapping the demonstrations, we test the instruction induction abilities of the bestperforming model, text-davinci-002, when varying the meta-prompt. The instruction induction meta-prompt presented in Figure 1 was selected by showing humans several pre-designed prompts and inspecting which was the clearest for the participants. We test the sensitivity to the meta-prompt by taking three additional meta-prompts (Table 7), sampling five examples from five tasks and manually verifying the correctness of the generated instructions.</p>
<p>Table 6 shows that while the model performance is affected by the content of the meta-prompt, the overall trend is similar when using other metaprompts, and high performance can be obtained with other prompts as well. In fact, for two of the three additional tested prompts, the generated instructions seem to be even better than those generated using the original prompt, though the differences are too small to determine this conclusively.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>ACL 2023 Responsible NLP Checklist</h1>
<h2>A For every submission:</h2>
<p>A1. Did you describe the limitations of your work?
3,9
\ A2. Did you discuss any potential risks of your work?
One benefit of the proposed approach is better interpretability and explainability, and we therefore view it as a method for reducing risks.
$\checkmark$ A3. Do the abstract and introduction summarize the paper's main claims?
1
\ A4. Have you used AI writing assistants when working on this paper?
Left blank.</p>
<h2>B Did you use or create scientific artifacts?</h2>
<p>3
$\checkmark$ B1. Did you cite the creators of artifacts you used?
Appendix A
\ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?
We verified that all the data and code used is publicly open - we verified license details for each, and we provided citation and links to all relevant resources, where license details can also be found.
$\checkmark$ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?
Appendix A
\ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?
We didn't discuss that, but other than the fact that we only used published datasets that are already used by the research community - we also sampled examples and manually verified their content.
$\checkmark$ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?
Appendix A
$\checkmark$ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.
Appendix A
The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.</p>
<h1>C Did you run computational experiments?</h1>
<p>5
\&amp; C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?</p>
<p>We used OpenAI models, for which the number of parameters is not always known. For models with known number of parametrs, we did report that number.
$\checkmark$ C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?
5
\&amp; C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?
We did not include error bars. The usage of mean values and the number of examples used to calculate the mean are clear and transparent.
$\checkmark$ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?
4, Appendix A
D Did you use human annotators (e.g., crowdworkers) or research with human participants? 3
$\checkmark$ D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?
3
$\checkmark$ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?
3
$\checkmark$ D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?
3
\&amp; D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? The data annotation did not have any associated risks and did not require a special approval.
$\checkmark$ D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?
3</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Challenge Puzzle (Original)
I gave a friend an instruction and five inputs. The friend read the instruction and wrote an output for every one of the inputs. Here are the input-output pairs:</p>
<p>Input:
Output:
$\cdots$
The instruction was
Challenge Puzzle + Name
I gave Bob an instruction and five inputs. Bob read the instruction and wrote an output for every one of the inputs. Here are the input-output pairs:</p>
<p>Input:
Output:
$\cdots$
The instruction was
Instruction After Demonstrations
Below are five input-output pairs that correspond to some underlying task:</p>
<p>Input:
Output:
$\cdots$
Please write the instruction that best describes the underlying task:</p>
<h2>Instruction Before Demonstrations</h2>
<p>You are given five examples of input-output pairs. Please write an instruction that describes creating an output from each input.</p>
<p>Input:
Output:
$\cdots$
Table 7: The meta-prompts used in our analysis.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{14}$ https://www.niu.edu/writingtutorial/ style/formal-and-informal-style.shtml, https://www.uts.edu.au/current-students/ support/helps/self-help-resources/ grammar/formal-and-informal-language ${ }^{15}$ https://github.com/open-dsl-dict/ wiktionary-dict&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
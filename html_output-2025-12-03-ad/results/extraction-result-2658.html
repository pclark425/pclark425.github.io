<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2658 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2658</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2658</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-68.html">extraction-schema-68</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <p><strong>Paper ID:</strong> paper-e5c72b92c48d68594b290c84a8904da7c8335554</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e5c72b92c48d68594b290c84a8904da7c8335554" target="_blank">Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules to significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This paper proposes a LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in external knowledge, e.g., stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a LLM-generated response. The effectiveness of LLM-Augmenter is empirically validated on two types of scenarios, task-oriented dialog and open-domain question answering. LLM-Augmenter significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2658.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2658.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-AUGMENTER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-AUGMENTER</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that augments a fixed black-box large language model with plug-and-play modules (Working Memory, Policy, Action Executor, Utility) to ground outputs in external knowledge and iteratively revise responses using automated feedback to reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-AUGMENTER</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Architecture composed of: (1) Working Memory (tracks dialog state: query, consolidated evidence, candidate responses, utility scores, feedback, history), (2) Policy (selects actions such as retrieving evidence or prompting the LLM; implemented as rule-based or trainable via REINFORCE/T5), (3) Action Executor comprising Knowledge Consolidator (retriever, entity linker, evidence chainer) and Prompt Engine (builds prompts containing instructions, query, history, evidence, feedback), and (4) Utility module (task-specific utility functions that score responses and generate verbalized feedback). The system iterates retrieval → prompt → candidate response → utility evaluation → feedback → re-prompt until a verification threshold is met, and is optimized end-to-end in an RL setup using KF1 as reward in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>retrieval-augmented, modular LLM-augmentation with RL</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language processing / dialog systems / open-domain question answering (not specific scientific domain)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Uses utility functions (e.g., Knowledge F1, recall overlap with consolidated evidence) to assess plausibility/groundedness of generated claims by measuring token overlap between generated text and consolidated evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Knowledge F1 (KF1) for groundedness; token-level precision/recall/F1 for short-answer tasks; BLEU/ROUGE/BLEURT/BERTScore/BARTScore etc. for text-quality dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Automatic verification against consolidated external evidence using utility functions (e.g., KF1 thresholding), plus iterative feedback-guided re-prompting; human evaluation for final assessment in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Source code and models made publicly available (stated); ablation studies and prompt templates provided in appendix; modular design intended to allow reproduction. No formal reproducibility protocol (e.g., seeds, CI) described in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Grounding outputs by consolidating external knowledge (retrieval + entity linking + evidence chaining) into prompts; iterative feedback loop where low-utility outputs trigger feedback and re-prompting; trainable policy to decide when to use knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Utility functions that score factual alignment (Knowledge F1, recall overlap), rule-based checks, and model-based self-criticism (using ChatGPT prompted to evaluate candidate responses and produce feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td>Human evaluation differences (Usefulness/Humanness) reported as significant with p < 0.05 for comparisons between ChatGPT and LLM-AUGMENTER (customer service scenario).</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>DSTC7 News Chat (News Chat), DSTC11 Customer Service (DSTC11 Track 5), OTT-QA (denoted Wiki QA / OTT-QA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Improvements measured include KF1 gains of ≈+10 (News Chat) and ≈+6 (Customer Service) when using Knowledge Consolidator; adding automated feedback yields +3.3 KF1 (News Chat golden knowledge) and +7.2 KF1 (Customer Service golden knowledge). On Wiki QA, F1 improved from 0.59 (closed-book ChatGPT) to 11.80 when using consolidated evidence (CORE) plus feedback (token-level F1 % as reported). Additional metrics reported: BLEU, ROUGE, BERTScore, BLEURT, BARTScore, chrF, METEOR with numeric tables in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared to vanilla ChatGPT (closed-book) the system significantly reduces hallucinations and increases groundedness (KF1) and human-judged Usefulness/Humanness; quantitative deltas provided in tables (e.g., Customer Service Usefulness: ChatGPT 34.07 vs LLM-AUGMENTER 45.07).</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Interactive feedback with computationally expensive models (e.g., ChatGPT) increases latency; main experiments use a rule-based policy due to ChatGPT bandwidth limits; RL policy learning sample inefficiency and dependence on simulator/human interaction; does not address harms beyond factuality (e.g., offensiveness) though extensible via utility functions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2658.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2658.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Utility module / Utility functions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Utility module (model-based and rule-based utility functions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Module that assigns utility scores and generates feedback for candidate LLM responses using task-specific metrics (model-based preference models or rule-based heuristics); used to detect hallucinations and guide iterative prompt revision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Utility module (Knowledge F1, recall, model-based scoring, rule-based checks, self-criticism)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Provides scalar utility scores u for each candidate response and emits verbalized feedback f. Utility functions include: (a) model-based scorers trained on human preference data to rate dimensions like fluency, informativeness, factuality; (b) rule-based heuristics to check compliance with constraints; (c) text-generation feedback model Q_psi (seq2seq) that inputs (q, e, o, h_q) and outputs natural-language feedback; (d) self-criticism by prompting the same or other LLMs (e.g., ChatGPT) to evaluate candidate responses.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>model-based evaluators + rule-based checkers + LLM-based self-critique</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / evaluation for dialog and QA</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Computes factual alignment scores (e.g., Knowledge F1 or recall overlap against consolidated evidence) and other model-based scores for fluency/informativeness to assess plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Knowledge F1 (KF1) measuring overlap with evidence; recall (token overlap with consolidated evidence) for Wiki QA; other metrics for response quality include BLEU/ROUGE/BLEURT/BERTScore.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Used to verify candidate responses before returning them; if utility score below threshold, feedback is generated and the prompt revised for another generation attempt.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>By detecting low KF1/recall scores and triggering re-generation with consolidated evidence and targeted feedback, preventing sending hallucinated claims to user.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>KF1 thresholding, rule-based consistency checks, and LLM self-critique prompts to flag inconsistencies with evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used in experiments on DSTC7 News Chat, DSTC11 Customer Service, OTT-QA (Wiki QA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>KF1 improvements when using utility-driven feedback: e.g., LLM-AUGMENTER with BM25 + feedback KF1 36.41 vs without feedback 34.96 (News Chat); in Customer Service BM25 + feedback KF1 37.41 vs without 34.07. Ablations show rule-based and self-criticism feedback give comparable KF1 (~37.4 vs 37.1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Utility-guided variants outperform no-utility baselines (improved KF1 and human-judged usefulness).</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Utility functions depend on quality of consolidated evidence (retrieval); model-based utilities require annotated preference data; real-world reward signals (user feedback) not used in main ChatGPT experiments due to bandwidth; uncertainty quantification not addressed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2658.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2658.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge Consolidator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Consolidator (retriever + entity linker + evidence chainer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Module that retrieves raw evidence from external sources, enriches it by linking entities to context (e.g., Wikipedia), prunes and chains evidence into concise, relevant evidence chains used to ground LLM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Knowledge Consolidator</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline with: (1) retriever (BM25 for dialog tasks; DPR dense retriever for Wiki QA) to obtain candidate snippets/tables/passages, (2) entity linker to attach entity descriptions (e.g., Wikipedia) and build evidence graphs, (3) evidence chainer that prunes irrelevant nodes and constructs ranked evidence chains for the query. Optionally uses CORE (Ma et al., 2022) for advanced consolidation (linking/reranking/splicing). Consolidated evidence e is passed to Working Memory and included in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>retrieval-augmented evidence consolidation (retriever + knowledge graph/linking + chaining)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / QA / multi-hop reasoning over heterogeneous knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Improves plausibility by assembling multi-hop evidence chains and increasing top-k answer recall (measured: top-5 consolidated evidence recall for CORE was reported as 50.83% in Wiki QA).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Indirect: increases evidence recall and KF1 of generated responses; top-k evidence recall is used to evaluate retrieval/consolidation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Consolidated evidence is used as ground for utility scoring (KF1/recall) and to verify LLM outputs; higher-quality consolidation leads to fewer hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Description of retrieval/backbones (BM25, DPR) and pointers to CORE; prompt templates provided in appendix to reproduce grounding experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Provide explicit evidence chains in prompts so the LLM is forced to ground claims in retrieved sources; prunes noisy/incomplete evidence via entity linking and chaining.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Not a detector per se, but its outputs serve as the reference for Utility functions (KF1/recall) used to detect hallucinated content.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>BM25 + web documents for DSTC7 News Chat; BM25 over FAQs/Yelp for DSTC11 Customer Service; DPR + CORE over Wikipedia passages/tables for OTT-QA (Wiki QA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Consolidated evidence from CORE yielded major gains: Wiki QA F1 improved from 2.38 (DPR raw evidence) to 8.08 (CORE consolidated) and to 11.80 when adding feedback. Top-5 answer recall of consolidated evidence (CORE) reported as 50.83%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CORE consolidation outperformed raw DPR retrieval (substantial improvements in downstream F1 on Wiki QA).</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality of consolidation limited by retriever recall; multi-hop consolidation remains challenging for sparse or cross-modality evidence; evidence consolidation increases overhead and latency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2658.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2658.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-criticism (ChatGPT as utility)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM self-criticism using ChatGPT as a utility function</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using the same or another LLM (ChatGPT) to evaluate candidate responses and generate textual feedback (self-critique) that guides re-prompting to reduce hallucinations and improve groundedness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Self-criticism feedback (LLM-based evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prompt ChatGPT to judge candidate responses for factual consistency with consolidated evidence and to produce feedback indicating missing or inconsistent facts. Feedback is then appended to the next prompt to elicit a corrected response. This LLM-based utility was compared to rule-based feedback and a template-based feedback generator; ablations show similar KF1 performance but more detailed suggestions from self-criticism.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based evaluation / meta-evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / dialog and QA evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Assesses plausibility by asking an LLM to compare candidate output to consolidated evidence and report inconsistencies; effectively produces a judged alignment score and textual corrective feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Produces feedback that is used downstream to increase KF1/recall; the paper reports KF1 improvements when using self-criticism feedback (e.g., Customer Service KF1 37.10 with self-criticism vs 34.07 w/o feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>LLM evaluator flags inconsistencies prior to finalizing response; human evaluation used to compare final outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Detects and prompts correction of hallucinated content via generated critiques and re-prompting with consolidated evidence and explicit corrective instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>LLM-based evaluation prompts to identify claims not supported by evidence; used in KF1-driven decision loop.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used in DSTC11 Customer Service experiments and ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Self-criticism feedback produced KF1 ≈37.10 in Customer Service ablation (vs rule-based 37.41 and no-feedback 34.07).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Self-criticism outperforms no-feedback baseline and is comparable to rule-based feedback while providing more detailed guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on an LLM evaluator which is computationally expensive (double querying) and subject to the evaluator's own biases/hallucinations; latency and API bandwidth issues mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2658.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2658.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge F1 (KF1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge F1 (KF1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metric measuring overlap (F1) between a model's generated response and a set of reference evidence snippets; used as a utility to assess groundedness/factual alignment and as RL reward in policy training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Knowledge F1 (KF1) utility metric</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Computes token-level F1 overlap between generated response and consolidated evidence (or golden knowledge). Used both as an automatic utility score to verify candidates and as the reward signal R in a reinforcement learning setup to train the Policy module.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>evaluation metric / utility function</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / grounded response generation</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Direct measure of factual plausibility by quantifying token overlap with external evidence considered authoritative for the query.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>KF1 itself is the metric used to quantify response groundedness prior to returning outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Thresholding KF1 to decide whether to accept candidate responses; used as reward to optimize policy behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>KF1 formula and use cases referenced (original KF1 in literature: Shuster et al., 2021); experimental tables report KF1 values across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>By enforcing minimum KF1 scores before returning answers, the system prevents low-evidence (hallucinated) outputs from being delivered to users.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Low KF1 flags hallucination or lack of grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used across DSTC7 News Chat, DSTC11 Customer Service, OTT-QA experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>KF1 improvements reported across datasets (see LLM-AUGMENTER overall metrics). Policy trained with KF1 reward reached ≈37.5 KF1 on test after ~600 interactions in simulated RL experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Using KF1 as utility reward yields policies that outperform random policy on the KF1 metric.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>KF1 measures lexical overlap with provided evidence and may not capture deeper logical correctness, paraphrases, or missing but correct inferences; dependent on quality and recall of consolidated evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2658.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2658.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrievers (BM25, DPR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BM25 and DPR retrievers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Retrieval backbones used by Knowledge Consolidator: BM25 (sparse lexical retriever) for dialog tasks; DPR (dense passage retrieval) for Wiki QA to retrieve passages and tables for downstream consolidation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrievers: BM25 (sparse) and DPR (dense passage retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>BM25 retrieves top-k text snippets based on lexical matching (used in News Chat and Customer Service tasks to retrieve 5 snippets). DPR encodes question and passages into dense vectors (CLS embeddings) and retrieves via maximum inner product search (used for OTT-QA/Wiki QA initial evidence retrieval). Retrieved candidates feed into entity linker and chainer to form consolidated evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>retrieval-augmented components (sparse and dense retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Improve plausibility by increasing top-k evidence recall; DPR typically yields higher semantic recall for multi-hop queries than BM25 in the Wiki QA setting.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Top-k evidence recall (e.g., top-5 recall), downstream KF1 and F1 on QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Retrieval quality evaluated via recall of gold evidence and its effect on downstream KF1/F1 metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Providing higher-quality, relevant retrieved evidence to ground LLM outputs reduces hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Not detectors themselves, but retrieval failure (low recall) can cause hallucinations detected by utility functions.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>BM25 used for DSTC7 and DSTC11; DPR used for OTT-QA (Wiki QA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Using DPR + CORE consolidated evidence markedly improved Wiki QA F1 compared to DPR alone (e.g., DPR raw evidence downstream F1 2.38 vs CORE 8.08; +feedback 11.80). BM25-based systems show KF1 improvements over closed-book baselines in dialog tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>DPR outperforms BM25 for multi-hop Wiki QA evidence retrieval; CORE consolidation further outperforms raw DPR retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Retriever recall bottleneck: insufficient retrieval leads to missing evidence and potential hallucinations; dense retrieval requires well-trained encoders and retrieval index maintenance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2658.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2658.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Policy (REINFORCE / T5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Policy module trained with REINFORCE (T5-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decision module that selects actions (e.g., call Knowledge Consolidator, prompt LLM, send response) implemented as either rule-based or trainable via a neural policy optimized with REINFORCE using utility reward (KF1).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Policy module (rule-based or RL-trained via REINFORCE on T5-Base)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Policy π_θ chooses system actions to maximize expected external reward R(s,a); implemented as a T5-Base encoder-decoder finetuned using REINFORCE with KF1 as reward in experiments. Training stages include bootstrapping from rule-based policy, learning with user simulators (LM-based), and fine-tuning with human interactions when available.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>trainable decision policy (RL) / hybrid rule-based</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / interactive systems</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Optimizes actions to increase utility metrics (KF1), indirectly favoring plausible, grounded responses.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Optimizes KF1 as reward signal during RL training; policy performance measured by final KF1 on test dialogs.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Policy validated by simulated interactions and KF1 improvement curves; surpasses random policy after ~600 simulated interactions and reaches ≈37.5 KF1 on test data.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Learning curve and training regimen described; policy uses T5-Base and REINFORCE with KF1 reward, but full training hyperparameters not exhaustively listed in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Learns when to invoke Knowledge Consolidator and when to request re-generation based on utility scores, thereby reducing hallucination by policy-guided use of evidence and feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Relies on Utility module outputs (KF1, self-critique) to detect low-quality/hallucinated responses and drive policy choices.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Policy trained/evaluated in Customer Service simulated interactions with KF1 reward; experiments on DSTC11 and DSTC7 datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Policy learning curve: surpasses random after ~600 interactions; reaches ≈37.5 KF1 on test. Ablations show policies (Always-use vs Self-ask vs No-KC) affect KF1 and access cost.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Trainable policy outperforms a random policy and manual rule-based or simpler policies in KF1 given sufficient simulated interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Policy learning sample inefficiency; reliance on simulated users for bootstrapping; main ChatGPT experiments used rule-based policy due to API bandwidth limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Open-domain question answering via chain of reasoning over heterogeneous knowledge <em>(Rating: 2)</em></li>
                <li>Dense passage retrieval for open-domain question answering <em>(Rating: 2)</em></li>
                <li>Retrieval augmentation reduces hallucination in conversation <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 1)</em></li>
                <li>REPLUG: Retrieval-augmented black-box language models <em>(Rating: 1)</em></li>
                <li>Improving alignment of dialogue agents via targeted human judgements <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2658",
    "paper_id": "paper-e5c72b92c48d68594b290c84a8904da7c8335554",
    "extraction_schema_id": "extraction-schema-68",
    "extracted_data": [
        {
            "name_short": "LLM-AUGMENTER",
            "name_full": "LLM-AUGMENTER",
            "brief_description": "A framework that augments a fixed black-box large language model with plug-and-play modules (Working Memory, Policy, Action Executor, Utility) to ground outputs in external knowledge and iteratively revise responses using automated feedback to reduce hallucinations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM-AUGMENTER",
            "system_description": "Architecture composed of: (1) Working Memory (tracks dialog state: query, consolidated evidence, candidate responses, utility scores, feedback, history), (2) Policy (selects actions such as retrieving evidence or prompting the LLM; implemented as rule-based or trainable via REINFORCE/T5), (3) Action Executor comprising Knowledge Consolidator (retriever, entity linker, evidence chainer) and Prompt Engine (builds prompts containing instructions, query, history, evidence, feedback), and (4) Utility module (task-specific utility functions that score responses and generate verbalized feedback). The system iterates retrieval → prompt → candidate response → utility evaluation → feedback → re-prompt until a verification threshold is met, and is optimized end-to-end in an RL setup using KF1 as reward in experiments.",
            "system_type": "retrieval-augmented, modular LLM-augmentation with RL",
            "scientific_domain": "natural language processing / dialog systems / open-domain question answering (not specific scientific domain)",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Uses utility functions (e.g., Knowledge F1, recall overlap with consolidated evidence) to assess plausibility/groundedness of generated claims by measuring token overlap between generated text and consolidated evidence.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Knowledge F1 (KF1) for groundedness; token-level precision/recall/F1 for short-answer tasks; BLEU/ROUGE/BLEURT/BERTScore/BARTScore etc. for text-quality dimensions.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Automatic verification against consolidated external evidence using utility functions (e.g., KF1 thresholding), plus iterative feedback-guided re-prompting; human evaluation for final assessment in experiments.",
            "reproducibility_measures": "Source code and models made publicly available (stated); ablation studies and prompt templates provided in appendix; modular design intended to allow reproduction. No formal reproducibility protocol (e.g., seeds, CI) described in detail.",
            "hallucination_prevention_method": "Grounding outputs by consolidating external knowledge (retrieval + entity linking + evidence chaining) into prompts; iterative feedback loop where low-utility outputs trigger feedback and re-prompting; trainable policy to decide when to use knowledge.",
            "hallucination_detection_method": "Utility functions that score factual alignment (Knowledge F1, recall overlap), rule-based checks, and model-based self-criticism (using ChatGPT prompted to evaluate candidate responses and produce feedback).",
            "hallucination_rate": null,
            "statistical_significance_testing": "Human evaluation differences (Usefulness/Humanness) reported as significant with p &lt; 0.05 for comparisons between ChatGPT and LLM-AUGMENTER (customer service scenario).",
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "DSTC7 News Chat (News Chat), DSTC11 Customer Service (DSTC11 Track 5), OTT-QA (denoted Wiki QA / OTT-QA).",
            "performance_metrics": "Improvements measured include KF1 gains of ≈+10 (News Chat) and ≈+6 (Customer Service) when using Knowledge Consolidator; adding automated feedback yields +3.3 KF1 (News Chat golden knowledge) and +7.2 KF1 (Customer Service golden knowledge). On Wiki QA, F1 improved from 0.59 (closed-book ChatGPT) to 11.80 when using consolidated evidence (CORE) plus feedback (token-level F1 % as reported). Additional metrics reported: BLEU, ROUGE, BERTScore, BLEURT, BARTScore, chrF, METEOR with numeric tables in the paper.",
            "comparison_with_baseline": "Compared to vanilla ChatGPT (closed-book) the system significantly reduces hallucinations and increases groundedness (KF1) and human-judged Usefulness/Humanness; quantitative deltas provided in tables (e.g., Customer Service Usefulness: ChatGPT 34.07 vs LLM-AUGMENTER 45.07).",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Interactive feedback with computationally expensive models (e.g., ChatGPT) increases latency; main experiments use a rule-based policy due to ChatGPT bandwidth limits; RL policy learning sample inefficiency and dependence on simulator/human interaction; does not address harms beyond factuality (e.g., offensiveness) though extensible via utility functions.",
            "uuid": "e2658.0",
            "source_info": {
                "paper_title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Utility module / Utility functions",
            "name_full": "Utility module (model-based and rule-based utility functions)",
            "brief_description": "Module that assigns utility scores and generates feedback for candidate LLM responses using task-specific metrics (model-based preference models or rule-based heuristics); used to detect hallucinations and guide iterative prompt revision.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Utility module (Knowledge F1, recall, model-based scoring, rule-based checks, self-criticism)",
            "system_description": "Provides scalar utility scores u for each candidate response and emits verbalized feedback f. Utility functions include: (a) model-based scorers trained on human preference data to rate dimensions like fluency, informativeness, factuality; (b) rule-based heuristics to check compliance with constraints; (c) text-generation feedback model Q_psi (seq2seq) that inputs (q, e, o, h_q) and outputs natural-language feedback; (d) self-criticism by prompting the same or other LLMs (e.g., ChatGPT) to evaluate candidate responses.",
            "system_type": "model-based evaluators + rule-based checkers + LLM-based self-critique",
            "scientific_domain": "NLP / evaluation for dialog and QA",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Computes factual alignment scores (e.g., Knowledge F1 or recall overlap against consolidated evidence) and other model-based scores for fluency/informativeness to assess plausibility.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Knowledge F1 (KF1) measuring overlap with evidence; recall (token overlap with consolidated evidence) for Wiki QA; other metrics for response quality include BLEU/ROUGE/BLEURT/BERTScore.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Used to verify candidate responses before returning them; if utility score below threshold, feedback is generated and the prompt revised for another generation attempt.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "By detecting low KF1/recall scores and triggering re-generation with consolidated evidence and targeted feedback, preventing sending hallucinated claims to user.",
            "hallucination_detection_method": "KF1 thresholding, rule-based consistency checks, and LLM self-critique prompts to flag inconsistencies with evidence.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Used in experiments on DSTC7 News Chat, DSTC11 Customer Service, OTT-QA (Wiki QA).",
            "performance_metrics": "KF1 improvements when using utility-driven feedback: e.g., LLM-AUGMENTER with BM25 + feedback KF1 36.41 vs without feedback 34.96 (News Chat); in Customer Service BM25 + feedback KF1 37.41 vs without 34.07. Ablations show rule-based and self-criticism feedback give comparable KF1 (~37.4 vs 37.1).",
            "comparison_with_baseline": "Utility-guided variants outperform no-utility baselines (improved KF1 and human-judged usefulness).",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Utility functions depend on quality of consolidated evidence (retrieval); model-based utilities require annotated preference data; real-world reward signals (user feedback) not used in main ChatGPT experiments due to bandwidth; uncertainty quantification not addressed.",
            "uuid": "e2658.1",
            "source_info": {
                "paper_title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Knowledge Consolidator",
            "name_full": "Knowledge Consolidator (retriever + entity linker + evidence chainer)",
            "brief_description": "Module that retrieves raw evidence from external sources, enriches it by linking entities to context (e.g., Wikipedia), prunes and chains evidence into concise, relevant evidence chains used to ground LLM prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Knowledge Consolidator",
            "system_description": "Pipeline with: (1) retriever (BM25 for dialog tasks; DPR dense retriever for Wiki QA) to obtain candidate snippets/tables/passages, (2) entity linker to attach entity descriptions (e.g., Wikipedia) and build evidence graphs, (3) evidence chainer that prunes irrelevant nodes and constructs ranked evidence chains for the query. Optionally uses CORE (Ma et al., 2022) for advanced consolidation (linking/reranking/splicing). Consolidated evidence e is passed to Working Memory and included in prompts.",
            "system_type": "retrieval-augmented evidence consolidation (retriever + knowledge graph/linking + chaining)",
            "scientific_domain": "NLP / QA / multi-hop reasoning over heterogeneous knowledge",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Improves plausibility by assembling multi-hop evidence chains and increasing top-k answer recall (measured: top-5 consolidated evidence recall for CORE was reported as 50.83% in Wiki QA).",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Indirect: increases evidence recall and KF1 of generated responses; top-k evidence recall is used to evaluate retrieval/consolidation quality.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Consolidated evidence is used as ground for utility scoring (KF1/recall) and to verify LLM outputs; higher-quality consolidation leads to fewer hallucinations.",
            "reproducibility_measures": "Description of retrieval/backbones (BM25, DPR) and pointers to CORE; prompt templates provided in appendix to reproduce grounding experiments.",
            "hallucination_prevention_method": "Provide explicit evidence chains in prompts so the LLM is forced to ground claims in retrieved sources; prunes noisy/incomplete evidence via entity linking and chaining.",
            "hallucination_detection_method": "Not a detector per se, but its outputs serve as the reference for Utility functions (KF1/recall) used to detect hallucinated content.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "BM25 + web documents for DSTC7 News Chat; BM25 over FAQs/Yelp for DSTC11 Customer Service; DPR + CORE over Wikipedia passages/tables for OTT-QA (Wiki QA).",
            "performance_metrics": "Consolidated evidence from CORE yielded major gains: Wiki QA F1 improved from 2.38 (DPR raw evidence) to 8.08 (CORE consolidated) and to 11.80 when adding feedback. Top-5 answer recall of consolidated evidence (CORE) reported as 50.83%.",
            "comparison_with_baseline": "CORE consolidation outperformed raw DPR retrieval (substantial improvements in downstream F1 on Wiki QA).",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Quality of consolidation limited by retriever recall; multi-hop consolidation remains challenging for sparse or cross-modality evidence; evidence consolidation increases overhead and latency.",
            "uuid": "e2658.2",
            "source_info": {
                "paper_title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Self-criticism (ChatGPT as utility)",
            "name_full": "LLM self-criticism using ChatGPT as a utility function",
            "brief_description": "Using the same or another LLM (ChatGPT) to evaluate candidate responses and generate textual feedback (self-critique) that guides re-prompting to reduce hallucinations and improve groundedness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Self-criticism feedback (LLM-based evaluator)",
            "system_description": "Prompt ChatGPT to judge candidate responses for factual consistency with consolidated evidence and to produce feedback indicating missing or inconsistent facts. Feedback is then appended to the next prompt to elicit a corrected response. This LLM-based utility was compared to rule-based feedback and a template-based feedback generator; ablations show similar KF1 performance but more detailed suggestions from self-criticism.",
            "system_type": "LLM-based evaluation / meta-evaluation",
            "scientific_domain": "NLP / dialog and QA evaluation",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Assesses plausibility by asking an LLM to compare candidate output to consolidated evidence and report inconsistencies; effectively produces a judged alignment score and textual corrective feedback.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Produces feedback that is used downstream to increase KF1/recall; the paper reports KF1 improvements when using self-criticism feedback (e.g., Customer Service KF1 37.10 with self-criticism vs 34.07 w/o feedback).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "LLM evaluator flags inconsistencies prior to finalizing response; human evaluation used to compare final outputs.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Detects and prompts correction of hallucinated content via generated critiques and re-prompting with consolidated evidence and explicit corrective instructions.",
            "hallucination_detection_method": "LLM-based evaluation prompts to identify claims not supported by evidence; used in KF1-driven decision loop.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Used in DSTC11 Customer Service experiments and ablations.",
            "performance_metrics": "Self-criticism feedback produced KF1 ≈37.10 in Customer Service ablation (vs rule-based 37.41 and no-feedback 34.07).",
            "comparison_with_baseline": "Self-criticism outperforms no-feedback baseline and is comparable to rule-based feedback while providing more detailed guidance.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Relies on an LLM evaluator which is computationally expensive (double querying) and subject to the evaluator's own biases/hallucinations; latency and API bandwidth issues mentioned.",
            "uuid": "e2658.3",
            "source_info": {
                "paper_title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Knowledge F1 (KF1)",
            "name_full": "Knowledge F1 (KF1)",
            "brief_description": "A metric measuring overlap (F1) between a model's generated response and a set of reference evidence snippets; used as a utility to assess groundedness/factual alignment and as RL reward in policy training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Knowledge F1 (KF1) utility metric",
            "system_description": "Computes token-level F1 overlap between generated response and consolidated evidence (or golden knowledge). Used both as an automatic utility score to verify candidates and as the reward signal R in a reinforcement learning setup to train the Policy module.",
            "system_type": "evaluation metric / utility function",
            "scientific_domain": "NLP / grounded response generation",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Direct measure of factual plausibility by quantifying token overlap with external evidence considered authoritative for the query.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "KF1 itself is the metric used to quantify response groundedness prior to returning outputs.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Thresholding KF1 to decide whether to accept candidate responses; used as reward to optimize policy behaviors.",
            "reproducibility_measures": "KF1 formula and use cases referenced (original KF1 in literature: Shuster et al., 2021); experimental tables report KF1 values across runs.",
            "hallucination_prevention_method": "By enforcing minimum KF1 scores before returning answers, the system prevents low-evidence (hallucinated) outputs from being delivered to users.",
            "hallucination_detection_method": "Low KF1 flags hallucination or lack of grounding.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Used across DSTC7 News Chat, DSTC11 Customer Service, OTT-QA experiments.",
            "performance_metrics": "KF1 improvements reported across datasets (see LLM-AUGMENTER overall metrics). Policy trained with KF1 reward reached ≈37.5 KF1 on test after ~600 interactions in simulated RL experiments.",
            "comparison_with_baseline": "Using KF1 as utility reward yields policies that outperform random policy on the KF1 metric.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "KF1 measures lexical overlap with provided evidence and may not capture deeper logical correctness, paraphrases, or missing but correct inferences; dependent on quality and recall of consolidated evidence.",
            "uuid": "e2658.4",
            "source_info": {
                "paper_title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Retrievers (BM25, DPR)",
            "name_full": "BM25 and DPR retrievers",
            "brief_description": "Retrieval backbones used by Knowledge Consolidator: BM25 (sparse lexical retriever) for dialog tasks; DPR (dense passage retrieval) for Wiki QA to retrieve passages and tables for downstream consolidation.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Retrievers: BM25 (sparse) and DPR (dense passage retrieval)",
            "system_description": "BM25 retrieves top-k text snippets based on lexical matching (used in News Chat and Customer Service tasks to retrieve 5 snippets). DPR encodes question and passages into dense vectors (CLS embeddings) and retrieves via maximum inner product search (used for OTT-QA/Wiki QA initial evidence retrieval). Retrieved candidates feed into entity linker and chainer to form consolidated evidence.",
            "system_type": "retrieval-augmented components (sparse and dense retrieval)",
            "scientific_domain": "NLP / retrieval-augmented generation",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Improve plausibility by increasing top-k evidence recall; DPR typically yields higher semantic recall for multi-hop queries than BM25 in the Wiki QA setting.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Top-k evidence recall (e.g., top-5 recall), downstream KF1 and F1 on QA tasks.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Retrieval quality evaluated via recall of gold evidence and its effect on downstream KF1/F1 metrics.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Providing higher-quality, relevant retrieved evidence to ground LLM outputs reduces hallucinations.",
            "hallucination_detection_method": "Not detectors themselves, but retrieval failure (low recall) can cause hallucinations detected by utility functions.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "BM25 used for DSTC7 and DSTC11; DPR used for OTT-QA (Wiki QA).",
            "performance_metrics": "Using DPR + CORE consolidated evidence markedly improved Wiki QA F1 compared to DPR alone (e.g., DPR raw evidence downstream F1 2.38 vs CORE 8.08; +feedback 11.80). BM25-based systems show KF1 improvements over closed-book baselines in dialog tasks.",
            "comparison_with_baseline": "DPR outperforms BM25 for multi-hop Wiki QA evidence retrieval; CORE consolidation further outperforms raw DPR retrieval.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Retriever recall bottleneck: insufficient retrieval leads to missing evidence and potential hallucinations; dense retrieval requires well-trained encoders and retrieval index maintenance.",
            "uuid": "e2658.5",
            "source_info": {
                "paper_title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Policy (REINFORCE / T5)",
            "name_full": "Policy module trained with REINFORCE (T5-based)",
            "brief_description": "A decision module that selects actions (e.g., call Knowledge Consolidator, prompt LLM, send response) implemented as either rule-based or trainable via a neural policy optimized with REINFORCE using utility reward (KF1).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Policy module (rule-based or RL-trained via REINFORCE on T5-Base)",
            "system_description": "Policy π_θ chooses system actions to maximize expected external reward R(s,a); implemented as a T5-Base encoder-decoder finetuned using REINFORCE with KF1 as reward in experiments. Training stages include bootstrapping from rule-based policy, learning with user simulators (LM-based), and fine-tuning with human interactions when available.",
            "system_type": "trainable decision policy (RL) / hybrid rule-based",
            "scientific_domain": "NLP / interactive systems",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Optimizes actions to increase utility metrics (KF1), indirectly favoring plausible, grounded responses.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Optimizes KF1 as reward signal during RL training; policy performance measured by final KF1 on test dialogs.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Policy validated by simulated interactions and KF1 improvement curves; surpasses random policy after ~600 simulated interactions and reaches ≈37.5 KF1 on test data.",
            "reproducibility_measures": "Learning curve and training regimen described; policy uses T5-Base and REINFORCE with KF1 reward, but full training hyperparameters not exhaustively listed in main text.",
            "hallucination_prevention_method": "Learns when to invoke Knowledge Consolidator and when to request re-generation based on utility scores, thereby reducing hallucination by policy-guided use of evidence and feedback.",
            "hallucination_detection_method": "Relies on Utility module outputs (KF1, self-critique) to detect low-quality/hallucinated responses and drive policy choices.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Policy trained/evaluated in Customer Service simulated interactions with KF1 reward; experiments on DSTC11 and DSTC7 datasets.",
            "performance_metrics": "Policy learning curve: surpasses random after ~600 interactions; reaches ≈37.5 KF1 on test. Ablations show policies (Always-use vs Self-ask vs No-KC) affect KF1 and access cost.",
            "comparison_with_baseline": "Trainable policy outperforms a random policy and manual rule-based or simpler policies in KF1 given sufficient simulated interactions.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Policy learning sample inefficiency; reliance on simulated users for bootstrapping; main ChatGPT experiments used rule-based policy due to API bandwidth limits.",
            "uuid": "e2658.6",
            "source_info": {
                "paper_title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Open-domain question answering via chain of reasoning over heterogeneous knowledge",
            "rating": 2
        },
        {
            "paper_title": "Dense passage retrieval for open-domain question answering",
            "rating": 2
        },
        {
            "paper_title": "Retrieval augmentation reduces hallucination in conversation",
            "rating": 2
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 1
        },
        {
            "paper_title": "REPLUG: Retrieval-augmented black-box language models",
            "rating": 1
        },
        {
            "paper_title": "Improving alignment of dialogue agents via targeted human judgements",
            "rating": 1
        }
    ],
    "cost": 0.01800075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback ${ }^{\circ}$</h1>
<p>Baolin Peng ${ }^{\dagger}$ Michel Galley ${ }^{\dagger}$ Pengcheng $\mathrm{He}^{\dagger}$ Hao Cheng ${ }^{\dagger}$ Yujia Xie ${ }^{\dagger}$<br>Yu Hu ${ }^{\dagger}$ Qiuyuan Huang ${ }^{\dagger}$ Lars Liden ${ }^{\dagger}$ Zhou Yu ${ }^{\ddagger}$ Weizhu Chen ${ }^{\dagger}$ Jianfeng Gao ${ }^{\dagger}$<br>${ }^{\dagger}$ Microsoft Research ${ }^{\ddagger}$ Columbia University</p>
<h4>Abstract</h4>
<p>Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to realworld, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This paper proposes a LLM-AUGMENTER system, which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in external knowledge, e.g., stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a LLM-generated response. The effectiveness of LLM-AUGMENTER is empirically validated on two types of scenarios, taskoriented dialog and open-domain question answering. LLM-AUGMENTER significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available. ${ }^{\dagger}$</p>
<h2>1 Introduction</h2>
<p>Large Language models (LLMs), such as GPT-3 (Brown et al., 2020) and ChatGPT, have demonstrated an outstanding ability in generating fluent, coherent, and informative natural language texts. It is commonly understood that the impressive capabilities of these models stem from the abundance of world knowledge encoded therein and models' ability to generalize from that knowledge. However, the knowledge encoding of LLMs is lossy and the knowledge generalization could lead to "memory distortion." As a result, these models tend to hallucinate, which can cause damage when deployed for mission-critical tasks. In</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: LLM-AUGMENTER improves a fixed LLM by (1) consolidating evidence from external knowledge for the LLM to generate responses grounded in evidence, and (2) revising LLM's (candidate) responses using automated feedback.
addition, even with exponentially growing model sizes, LLMs can never encode all information needed for many applications. For example, constant changes in real-world settings cause LLMs to quickly become stale for time-sensitive tasks such as news question answering, and many proprietary datasets are not available for LLM training due to privacy. While there is a growing interest in improving LLMs using external knowledge (e.g., Ghazvininejad et al., 2017; Guu et al., 2020; Zhong et al., 2022; Gao et al., 2019, 2022), almost all the previously proposed methods require finetuning the parameters of a LLM, which can be prohibitively expensive as the size of LLMs grows exponentially. Thus, it is highly desirable to augment a fixed LLM with plug-and-play (PnP) modules for mission-critical tasks.</p>
<p>In this paper, we present LLM-AUGMENTER to improve LLMs with external knowledge and automated feedback using PnP modules. As illustrated by the example in Figure 1, given a user query (e.g., regarding a 2013 Los Angeles</p>
<p>Galaxy player transfer), LLM-AUGMENTER first retrieves evidence from external knowledge (e.g., Web or task-specific datasets) and, if necessary, further consolidates evidence by linking retrieved raw evidence with related context (e.g., information of the entity "2013 Los Angeles Galaxy") and performing reasoning to form evidence chains (e.g., table-passage in the figure). Then, LLMAUGMENTER queries a fixed LLM (i.e., ChatGPT in our study) using a prompt that contains the consolidated evidence for ChatGPT to generate a candidate response grounded in external knowledge (evidence). LLM-AUGMENTER then verifies the candidate response e.g., by checking whether it hallucinates evidence. If so, LLM-AUGMENTER generates a feedback message (e.g., about the team "C.S.D. Municipal"). The message is used to revise the prompt to query ChatGPT again. The process iterates until a candidate response passes the verification and is sent to the user.</p>
<p>In addition to proposing LLM-AUGMENTER, to be detailed in Section 2, we make the following contributions. We perform an empirical study to validate the effectiveness of LLM-AUGMENTER using two tasks, information seeking dialog (Section 3) and open-domain Wiki question answering (Wiki QA) (Section 4). The study shows that LLM-AUGMENTER significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its generated responses. For example, on the dialog task of customer service, human evaluation shows LLMAUGMENTER improve ChatGPT by 32.3% in Usefulness (measuring the groundedness or hallucination of model responses) and 12.9% in Humanness (measuring the fluency and informativeness of model responses). The Wiki QA task is extremely challenging to ChatGPT in that answering these questions often requires multi-hop reasoning to piece together information of various modalities scattered across different documents. Our results show that although the closed-book ChatGPT performs poorly and often hallucinates, LLMAUGMENTER substantially improves the factuality score of the answers (absolute +10% in F1) by grounding ChatGPT's responses in consolidated external knowledge and automated feedback.</p>
<h2>2 LLM-AUGMENTER</h2>
<p>The architecture of LLM-AUGMENTER is illustrated in Figure 2. It consists of a set of PnP</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: LLM-AUGMENTER architecture showing how its plug-and-play modules interact with the LLM and the user's environment.</p>
<p>modules (i.e., Working Memory, Policy, Action Executor, and Utility) to improve a fixed LLM (e.g., ChatGPT) with external knowledge and automated feedback to mitigate generation problems such as hallucination.</p>
<p>We formulate human-system conversation as a Markov Decision Process (MDP) described by a five-tuple (S, A, P, R, γ):</p>
<ul>
<li>S is an infinite set of dialog states, which encode information stored in Working Memory, including dialog history, user query, evidence, candidate response;</li>
<li>A is a set of actions that Policy picks to execute, including (1) calling Knowledge Consolidator to consolidate evidence from external knowledge and (2) calling Prompt Engine to query the LLM to generate candidate responses;</li>
<li>P(s′|s, a) gives the transition probability of entering a new state s′ after action a is taken in state s;</li>
<li>R(s, a) is the external reward received after taking action a in state s, which is provided by the environment (e.g., users or simulators); and</li>
<li>γ ∈ (0, 1] is a discount factor.</li>
</ul>
<p>In what follows, we describe in detail the modules of LLM-AUGMENTER.</p>
<h3>2.1 Working Memory</h3>
<p>This module tracks the dialog state that captures all essential information in the conversation so</p>
<p>far. The state is represented using a six-tuple $\left(q, e, o, u, f, h_{q}\right):$</p>
<ul>
<li>$q$ is the current user query;</li>
<li>$e$ is evidence for $q$, consolidated from external knowledge by Knowledge Consolidator;</li>
<li>$o$ is a set of the LLM-generated candidate responses for $q$;</li>
<li>$u$ is a score assessing the utility of each element of $o$, and $f$ is a verbalized feedback to guide the LLM to improve its utility - both $u$ and $f$ are generated by the Utility module (see Section 2.4); and</li>
<li>$h_{q}$ is the dialog history before $q$.</li>
</ul>
<p>Note that given user query $q$, LLMAUGMENTER can take multiple iterations to revise its response, with each iteration generating a candidate response based on evidence, feedback and utility, before sending the final response to the user, as illustrated in Figure 1.</p>
<h3>2.2 Policy</h3>
<p>This module selects the next system action that leads to the best expected reward $R$. These actions include (1) acquiring evidence $e$ for $q$ from external knowledge, (2) calling the LLM to generate a candidate response, and (3) sending a response to users if it passes the verification by the Utility module.</p>
<p>The policy can be implemented using manually crafted rules, or trained on human-system interactions. In this study, we implement a trainable policy $\pi$ as a neural network model parameterized by $\theta . \pi_{\theta}$ is optimized using REINFORCE (Williams, 1992) to maximize the expected reward as:</p>
<p>$$
\underset{\theta}{\operatorname{argmax}} \mathbb{E}<em _theta="\theta">{s \sim \mathcal{S}, a \sim \pi</em>[R(s, a)]
$$}</p>
<p>We find it effective to implement $\pi$ using a pre-trained model (e.g., T5), which allows us to not only leverage the capacity of the pre-trained model, but also to incorporate additional information through finetuning.</p>
<p>Policy learning typically requires large amounts of human-machine interactions, which can be costly to collect. To address the challenge, policy learning can be done in three stages:</p>
<ul>
<li>Bootstrapping from a rule-based policy: Domain experts encode task-specific knowledge and business logic into IF-THEN rules. For example, if a product name is mentioned in a user query for customer service, it is wise to always call Knowledge Consolidator to collect information of the product from a product database.</li>
<li>Learning with user simulators: We use a language model to simulate how human users interact with LLM-AUGMENTER. Any valid response from LLM-AUGMENTER that passes the evaluation of the Utility module can be used as a training example, allowing LLM-AUGMENTER to self-improve.</li>
<li>Finally, LLM-AUGMENTER interacts with human users to further refine its policy.</li>
</ul>
<p>In addition to Policy, the other trainable modules of LLM-AUGMENTER (i.e., Knowledge Consolidator and Utility) can also be optimized using the same learning method.</p>
<h3>2.3 Action Executor</h3>
<p>This module performs an action selected by the policy. It is composed of two components, Knowledge Consolidator and Prompt Engine.</p>
<h3>2.3.1 Knowledge Consolidator</h3>
<p>The Knowledge Consolidator augments LLMs with the capability of grounding their responses on external knowledge to mitigate hallucination when completing tasks, such as answering questions regarding latest news, and booking a table in a restaurant. Following (Ma et al., 2022), the Knowledge Consolidator is designed in a modular fashion, consisting of a knowledge retriever, an entity linker and, an evidence chainer.</p>
<p>Specifically, the retriever first generates a set of search queries based on $q$ and $h_{q}$, and then calls a set of APIs to retrieve raw evidence from various external knowledge sources, such as calling Bing Search APIs to query Web documents including Wiki articles and Reddit messages, and REST APIs to query task-specific databases for restaurant reviews and product specifications.</p>
<p>The retrieved raw evidence is sometimes incomplete and noisy. Thus, the entity linker enriches raw evidence with related context to form evidence graphs, i.e., linking each entity mentioned in raw evidence to its corresponding description</p>
<p>based on Wikipedia. Then, the chainer prunes irrelevant evidence from the graphs and forms a shortlist of evidence chains that are most relevant to queries. The consolidated evidence $e$ is then sent to Working Memory. Figure 1 shows an example of consolidated evidence for the anchored club "Los Angeles Galaxy", i.e., two evidence chains corresponding to the transfer players in 2013 season and the former clubs, respectively.</p>
<h3>2.3.2 Prompt Engine</h3>
<p>The Prompt Engine generates a prompt to query the LLM to generate a (candidate) response $o$ for $q$. The prompt is a text string that consists of task instruction, user query $q$, dialog history $h_{q}$, evidence $e$ if it is made available by Knowledge Consolidator, and feedback $f$ if it is made available by the Utility module. Prompts are task-specific, and details thereof are provided in Appendix A.</p>
<h3>2.4 Utility</h3>
<p>Given a candidate response $o$, the Utility module generates utility score $u$ and a corresponding feedback $f$ using a set of task-specific utility functions.</p>
<p>These utility functions ${ }^{2}$ access the alignment of the LLM's responses with user expectations or specific business requirements. For example, in an information seeking dialog, it is important that all LLM's responses are preciously grounded in external evidence to avoid generating misleading or inaccurate information. In a restaurant reservation dialog, the LLM responses should be conversational and focused on guiding the user through the reservation process, rather than engaging in offtopic chitchats.</p>
<p>Inspired by Glaese et al. (2022), there can be two distinct types of utility functions:</p>
<ul>
<li>Model-based utility functions assign preference scores to different dimensions of a response, such as fluency, informativeness and factuality. These functions are trained on precollected human preference data or annotated log data.</li>
<li>Rule-based utility functions, implemented using heuristics or programmed functions, measure whether a response complies with a specific rule.</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>In addition, we have developed a utility function to generate informative and actionable feedback to help revise prompts to allow the LLM to generate better responses. As shown in Figure 1, the utility function generates feedback "but there is no information about the number of international titles." Such a utility function is a text generation model $Q$ parameterized by $\psi$, and can be implemented as a seq2seq or auto-regression language model. It tasks as input user query $q$, evidence $e$, candidate response $o$ and dialog history $h_{q}$, and generates feedback in text $f$ as</p>
<p>$$
f=Q_{\psi}\left(q, e, o, h_{q}\right)
$$</p>
<p>Alternatively, LLMs and rule-based natural language generator can be used for feedback generation.</p>
<p>In the next two sections, we present our experiments to validate the effectiveness of LLMAUGMENTER in two types of distinct scenarios: (1) information seeking dialog, where the AI agent needs to generate informative and trustworthy responses based on a variety of external sources of knowledge, and (2) Wiki question answering, where the AI agent needs to answer questions by piecing together information of various modalities scattered among multiple Wiki documents.</p>
<h2>3 Information Seeking Dialog</h2>
<h3>3.1 Datasets</h3>
<p>News Chat: We repurpose the DSTC7 Track 2 task as an evaluation corpus for news conversation. The goal of this task is to generate informative responses that are grounded in external knowledge (i.e., news) and go beyond chitchat. We followed the data crawling process used in DSTC7 Task 2 (Galley et al., 2019). We started by selecting Reddit discussion threads that contained URLs in the description, which were crawled from various news-related subreddits during the time period of 2021-2022. We then restricted the URL domain to a curated list of news websites, and extracted the relevant oracle passage by selecting the most appropriate passage for the context based on ROUGE-F1 scores (Lin, 2004). In order to reduce noisy or irrelevant information, we only kept examples with an F1 score higher than a certain threshold, resulting in a total of 1370 examples for evaluation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">K.C.</th>
<th style="text-align: center;">Feedback</th>
<th style="text-align: center;">KF1 $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">ROUGE $\uparrow$</th>
<th style="text-align: center;">chrF $\uparrow$</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">BERTScore $\uparrow$</th>
<th style="text-align: center;">BARTScore $\uparrow$</th>
<th style="text-align: center;">BLEURT $\uparrow$</th>
<th style="text-align: center;">Avg. length</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">26.71</td>
<td style="text-align: center;">1.01</td>
<td style="text-align: center;">16.78</td>
<td style="text-align: center;">23.80</td>
<td style="text-align: center;">7.34</td>
<td style="text-align: center;">82.14</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">26.98</td>
<td style="text-align: center;">58.94</td>
</tr>
<tr>
<td style="text-align: left;">LLM-Augmenter</td>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">34.96</td>
<td style="text-align: center;">6.71</td>
<td style="text-align: center;">22.25</td>
<td style="text-align: center;">27.02</td>
<td style="text-align: center;">9.35</td>
<td style="text-align: center;">$\mathbf{8 3 . 4 6}$</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">26.89</td>
<td style="text-align: center;">46.74</td>
</tr>
<tr>
<td style="text-align: left;">LLM-Augmenter</td>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathbf{3 6 . 4 1}$</td>
<td style="text-align: center;">$\mathbf{7 . 6 3}$</td>
<td style="text-align: center;">$\mathbf{2 2 . 8 0}$</td>
<td style="text-align: center;">$\mathbf{2 8 . 6 6}$</td>
<td style="text-align: center;">$\mathbf{1 0 . 1 7}$</td>
<td style="text-align: center;">83.33</td>
<td style="text-align: center;">$\mathbf{0 . 3 5}$</td>
<td style="text-align: center;">$\mathbf{2 7 . 7 1}$</td>
<td style="text-align: center;">54.24</td>
</tr>
<tr>
<td style="text-align: left;">LLM-Augmenter</td>
<td style="text-align: center;">gold</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">57.44</td>
<td style="text-align: center;">19.24</td>
<td style="text-align: center;">38.89</td>
<td style="text-align: center;">40.02</td>
<td style="text-align: center;">17.21</td>
<td style="text-align: center;">86.65</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">40.55</td>
<td style="text-align: center;">44.35</td>
</tr>
<tr>
<td style="text-align: left;">LLM-Augmenter</td>
<td style="text-align: center;">gold</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">60.76</td>
<td style="text-align: center;">21.49</td>
<td style="text-align: center;">40.56</td>
<td style="text-align: center;">42.14</td>
<td style="text-align: center;">18.50</td>
<td style="text-align: center;">86.89</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">42.15</td>
<td style="text-align: center;">47.19</td>
</tr>
</tbody>
</table>
<p>Table 1: Evaluation scores (in \%) and average response lengths for the News Chat (DSTC7) dataset. BM25: Each model retrieves 5 knowledge snippets from the corresponding knowledge source. K.C. denotes Knowledge Consolidator.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">K.C.</th>
<th style="text-align: center;">Feedback</th>
<th style="text-align: center;">KF1 $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">ROUGE $\uparrow$</th>
<th style="text-align: center;">chrF $\uparrow$</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">BERTScore $\uparrow$</th>
<th style="text-align: center;">BARTScore $\uparrow$</th>
<th style="text-align: center;">BLEURT $\uparrow$</th>
<th style="text-align: center;">Avg. length</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">31.33</td>
<td style="text-align: center;">4.70</td>
<td style="text-align: center;">24.02</td>
<td style="text-align: center;">27.14</td>
<td style="text-align: center;">12.83</td>
<td style="text-align: center;">87.88</td>
<td style="text-align: center;">1.53</td>
<td style="text-align: center;">$\mathbf{4 7 . 9 9}$</td>
<td style="text-align: center;">28.81</td>
</tr>
<tr>
<td style="text-align: left;">LLM-Augmenter</td>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">34.07</td>
<td style="text-align: center;">$\mathbf{4 . 7 8}$</td>
<td style="text-align: center;">$\mathbf{2 4 . 5 2}$</td>
<td style="text-align: center;">28.95</td>
<td style="text-align: center;">13.61</td>
<td style="text-align: center;">$\mathbf{8 7 . 9 6}$</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;">47.21</td>
<td style="text-align: center;">32.65</td>
</tr>
<tr>
<td style="text-align: left;">LLM-Augmenter</td>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathbf{3 7 . 4 1}$</td>
<td style="text-align: center;">3.86</td>
<td style="text-align: center;">24.20</td>
<td style="text-align: center;">$\mathbf{3 0 . 9 0}$</td>
<td style="text-align: center;">$\mathbf{1 4 . 7 4}$</td>
<td style="text-align: center;">87.58</td>
<td style="text-align: center;">$\mathbf{2 . 0 9}$</td>
<td style="text-align: center;">44.71</td>
<td style="text-align: center;">45.07</td>
</tr>
<tr>
<td style="text-align: left;">LLM-Augmenter</td>
<td style="text-align: center;">gold</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">45.63</td>
<td style="text-align: center;">6.54</td>
<td style="text-align: center;">29.77</td>
<td style="text-align: center;">33.32</td>
<td style="text-align: center;">16.93</td>
<td style="text-align: center;">89.35</td>
<td style="text-align: center;">2.59</td>
<td style="text-align: center;">54.38</td>
<td style="text-align: center;">33.04</td>
</tr>
<tr>
<td style="text-align: left;">LLM-Augmenter</td>
<td style="text-align: center;">gold</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">52.83</td>
<td style="text-align: center;">5.63</td>
<td style="text-align: center;">29.65</td>
<td style="text-align: center;">35.68</td>
<td style="text-align: center;">18.66</td>
<td style="text-align: center;">89.01</td>
<td style="text-align: center;">3.14</td>
<td style="text-align: center;">52.49</td>
<td style="text-align: center;">45.09</td>
</tr>
</tbody>
</table>
<p>Table 2: Evaluation scores (in \%) and average response lengths for the Customer Service (DSTC11) dataset. BM25: Each model retrieves 5 knowledge snippets from the corresponding knowledge source. K.C. denotes Knowledge Consolidator.</p>
<p>Customer Service: We use DSTC11 Track 5 (Kim et al., 2023) as a showcase in a conversational customer service scenario. It expands upon the DSTC9 Track 1 dataset by incorporating subjective knowledge from customer reviews in addition to factual knowledge from FAQs. This allows users to have an engaging and informative conversational experience with the AI system. The dataset evaluates the ability of the AI agent to understand relevant user review posts and FAQs, and generate responses based on both reviews and FAQ snippets. It is collected based on the MultiWOZ 2.1 (Eric et al., 2019) dataset and includes users' knowledge-seeking queries that require the AI agent to use FAQs and user reviews to respond. There are 14768 dialog sessions for training and validation, and the test set is currently unavailable. Therefore, we used the validation set for our evaluations.</p>
<h3>3.2 Experiment Setup</h3>
<p>Language Model: Throughout this work, we focus on using ChatGPT as the backbone blackbox LLM. It is straightforward to apply LLMAUGMENTER to other LLMs, such as GPT3 (Brown et al., 2020) or PaLM (Chowdhery et al., 2022).</p>
<p>Knowledge Consolidator: For News Chat, Knowledge Consolidator includes a BM25 retriever over web documents linked from Reddit posts. For the Customer Service task, Knowl-
edge Consolidator includes a BM25-based retriever over the knowledge bases of FAQs and Yelp reviews.</p>
<p>Additionally, we also experiment with groundtruth knowledge, referred to as golden knowledge henceforth, which is used by human annotators during data collection, in our oracle experiments.</p>
<p>Prompt Engine: The prompt templates utilized for News Chat and Customer Service are shown in the appendix in Table 7 and Table 8, respectively.</p>
<p>Utility: The goal of this task is to generate responses that are coherent to the context and grounded in external knowledge. To evaluate the degree to which the generated responses are grounded in consolidated evidence, we use the utility score, Knowledge F1 (Shuster et al., 2021), to measure the overlap between a prediction and evidence which is either consolidated by Knowledge Consolidator or provided as golden knowledge. Feedback generation is accomplished using a template-based natural language generator. ${ }^{3}$ In addition, we use ChatGPT as a utility function, i.e., self-criticism to gather feedback by prompting ChatGPT to evaluate candidate responses and give feedback on how to improve them.</p>
<p>Policy: Due to ChatGPT's current limited bandwidth, we use a rule-based policy for our ex-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>periments involving ChatGPT. The prior knowledge about this task inspired us to design a policy that always uses Knowledge Consolidator, evaluates the quality of a candidate response using ChatGPT, and provides feedback to revise the prompt. Additionally, to test the viability of LLM-AUGMENTER with a trainable policy, we employ offline RL to train the parameters of Policy as Equation 1, where the policy model is based on T5-Base.</p>
<p>Evaluation: We evaluate the performance of LLM-AUGMENTER on information-seeking dialog tasks using both automatic metrics and human evaluations. Following the literature, we consider commonly used metrics, Knowledge F1 (KF1) and BLEU-4, in grounded conversational response generation and task-oriented dialog. BLEU (Papineni et al., 2002) measures the overlap between the model's output and the ground-truth human response, while KF1 (Lian et al., 2019) assesses the overlap with the knowledge that the human used as a reference during dataset collection. Additionally, we include ROUGE-1 (Lin, 2004) and METEOR (Banerjee and Lavie, 2005) as these metrics have been found to best correlate with human judgment on the DSTC9 and DSTC11 customer support tasks (Kim et al., 2020). We further include BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019), chrF (Popović, 2015), which have been shown to be among the best-performing text generation metrics on dialog(Yeh et al., 2021; Peng et al., 2022). Lastly, we also consider BARTScore as it has been reported to be one of the best model-based metrics (Yuan et al., 2021). Given that BARTScore can be interpreted as a log-probability, we report results with its natural exponent (positive scores). Additionally, we perform a turn-level human evaluation to investigate whether responses are (1) useful and (2) humanlike. Following the evaluation protocol by (Peng et al., 2022), using Amazon Mechanical Turk, we hired master-level workers with lifetime HIT acceptance rate above $95 \%$, and asked them to answer two questions on usefulness (i.e., which response sounds more useful) and humanness (i.e., which speaker sounds more human).</p>
<h3>3.3 Automatic Evaluation Results</h3>
<p>The impact of using external knowledge: Experiment results are shown in Tables 1 and 2. We observe that ChatGPT achieves reasonable perfor-
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Learning curve of ChatGPT-Companion with T5-Base as the policy module. The solid curves are the mean and the shaded regions are the maximum and minimum utility scores over 5 runs.
mance even in the zero-shot setting. However, with access to golden knowledge, the performance is dramatically improved. This suggests that while LLMs are able to encode a large amount of general knowledge in their parameters, they can still benefit from more specific, targeted knowledge. This is likely because LLMs are designed to handle a wide range of tasks and therefore may not always have access to the most relevant or up-to-date information for a given task. Our experiments show that providing LLMs with task-specific knowledge can significantly mitigate hallucination without sacrificing the fluency and informativeness of model-generated responses. As demonstrated in Tables 1 and 2, LLM-AUGMENTER mitigates ChatGPT's hallucination issue on both the news chat and customer service tasks. Specifically, we observe a significant improvement in KF1 scores of approximately 10 and 6 points, respectively, due to the use of evidence retrieved by Knowledge Consolidator.</p>
<p>The impact of using automated feedback: As listed in Tables 1 and 2, the results of using golden knowledge setting demonstrate that incorporating feedback from the Utility module leads to substantial improvement 3.3 points in KF1 on News Chat and 7.2 on Customer Service, respectively. Similarly, significant improvement can also be observed when using evidence provided by Knowledge Consolidator.</p>
<p>The impact of using trainable Policy: Figure 3 shows the learning curve of LLM-AUGMENTER on the customer service task. As we do not have an</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Ablation studies on different policies of LLM-AUGMENTER in Customer Service scenario.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Usefulness ↑</th>
<th>Humanness ↑</th>
</tr>
</thead>
<tbody>
<tr>
<td>ChatGPT</td>
<td>34.07</td>
<td>30.92</td>
</tr>
<tr>
<td>LLM-AUGMENTER</td>
<td><strong>45.07</strong></td>
<td><strong>35.22</strong></td>
</tr>
</tbody>
</table>
<p>Table 3: Human evaluation of ChatGPT and LLMAUGMENTER with BM25 in Customer Service scenario. All differences are significant (p &lt; 0.05). Inter-annotator agreements according Krippendorff's alpha (interval metric) are 0.15 and 0.07 respectively.</p>
<p>external reward that would require collecting data from real users, we instead define here our reward R as the KF1 utility function. This helps demonstrate the effectiveness of LLM-AUGMENTER in its reinforcement learning (RL) setup. As our experiments are akin to single turn interactions, we did not need to set discount factor γ, but future work may need to rely on it. We see that LLMAUGMENTER's reward on test data increases as the number of training episodes (dialog sessions) increases, surpassing a random policy after 600 interactions and ultimately reaching a KF1 score of approximately 37.5. Through these interactions, LLM-AUGMENTER is able to learn to effectively select the next system action to maximize the reward, which helps our system reduce hallucinations while generating fluent and informative responses.</p>
<h3>3.4 Human Evaluation Results</h3>
<p>We compare ChatGPT with and without LLMAUGMENTER. A total of 948 randomly selected examples from the customer service dataset are used for human evaluation. The evaluation results are converted from a 5-point Likert-like scale to a win/tie/loss scale for reporting, as shown</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: LLM-AUGMENTER benefits from the combination of using utility function and iterative improvement with feedback. The x-axis indicates the average number of ChatGPT prompting and the y-axis is the KF1. The studies are conducted in the Customer Service scenario with knowledge being provided by BM25.</p>
<p>in Table 3. We observe a strong preference for LLM-AUGMENTER over ChatGPT alone in terms of both Usefulness and Humanness. The result is consistent with the automatic evaluation result, discussed earlier.</p>
<h3>3.5 Ablation Study</h3>
<p>We conduct ablation experiments to evaluate the effect of various policies on the utilization of the knowledge consolidator. Figure 4 shows the performance of three different variants of the policy: 1) no-knowledge consolidator, in which the knowledge consolidator is not used, 2) Self-ask, in which the knowledge consolidator is only utilized when the LM suggests the use of external knowledge by prompting it whether to use, and (3) Always-use, in which the knowledge consolidator is always provided to the LM. Our results indicate that Self-ask policy achieves a significantly better KF1 score than the No-knowledge consolidator policy, with the ChatGPT model unable to answer user queries and suggesting knowledge consolidator access for 24% of examples. However, the Always-use policy, while achieving the best KF1 score, also incurred additional overhead in terms of knowledge consolidator access. These observations suggest that a trainable policy model should be employed to learn when to use external knowledge.</p>
<p>In addition, the evaluation results on the impact of different types of feedback for LLM-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Feedback</th>
<th style="text-align: left;">KF1 $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">w/o Feedback</td>
<td style="text-align: left;">34.07</td>
</tr>
<tr>
<td style="text-align: left;">w/ Rule-based Feedback</td>
<td style="text-align: left;">37.41</td>
</tr>
<tr>
<td style="text-align: left;">w/ Self-criticism Feedback</td>
<td style="text-align: left;">37.10</td>
</tr>
</tbody>
</table>
<p>Table 4: Ablation studies on different feedback of LLM-AUGMENTER in Customer Service scenario.</p>
<p>Augmenter are listed in Table 4. We observe that self-criticism feedback enhances response quality make it more knowledge-grounded. Although its performance is comparable to that of rule-based feedback, it provides more detailed suggestions. We speculate that self-criticism will be more helpful for complex tasks. Some examples can be found in 6 .</p>
<p>To understand the impact of utility functions and feedback-augmented prompting on the performance of LLM-AUGMENTER, we conduct an analysis by turning each component on and off. Figure 5 illustrates the results of each variant. We observe that the combination of using utility functions and feedback-augmented prompting, i.e., 2 , achieves the best performance. In addition, always providing feedback (as shown in (4) also enhances the performance, although it requires additional model prompting. (5) represents prompting ChatGPT twice and re-ranking the response based on the utility functions, which results in a slightly higher KF1, but performs significantly worse than 2. These findings suggest that incorporating both utility functions and feedback is a more effective method for improving the alignment of LLMs.</p>
<h2>4 Wiki QA</h2>
<p>Instead of conversational evaluations, we focus on stress tests on ChatGPT here using open-domain question answering. As ChatGPT and other LLMs are mostly trained using abundant text from single web pages, we hypothesize that answering multi-hop questions involving scattered information across different pages/modalities can better serve the purpose. Due to this, closed-book LLMs are more likely to hallucinate. Moreover, the complex step-by-step reasoning can even be challenging for existing search systems to gather all necessary support evidence in one-shot. Thus, more advanced knowledge consolidation techniques are essential to elicit LLMs for proper grounding.</p>
<p>Lastly, different from conversational tasks where long-form responses are desirable, we mainly consider questions with concise short-form answers, i.e., there exists a significant style shift in responses. To align ChatGPT to this new scenario with distinct characteristics, extra instructions are needed.</p>
<h3>4.1 Dataset</h3>
<p>OTT-QA: The OTT-QA dataset is an opendomain question answering benchmark that considers multi-step joint reasoning over both tabular and textual information. It consists of around 40K instances built upon Wikipedia, including 400K tables and 6 M passages as the knowledge source. Solving the questions in OTT-QA requires diverse reasoning skills and can be divided into three categories: single-hop questions (13\%), two-hop questions ( $57 \%$ ), and multi-hop questions ( $30 \%$ ). In this paper, we denote the dataset as Wiki QA.</p>
<h3>4.2 Experiment Setups</h3>
<p>In the following, we describe the experimental setup for Wiki QA. Unless specified otherwise, the setups are identical to those used in Section 3.</p>
<p>Knowledge Consolidator: Here, the Knowledge Consolidator uses Wikipedia passages and tables as the knowledge source. Instead of using BM25 as done for dialog tasks, we resort to a dense model, DPR (Karpukhin et al., 2020), as the backbone retriever. For DPR, both question and passage/table inputs are represented by the corresponding special token [CLS] embeddings from their respective encoders, and retrieval is simply done via maximum inner product search in the vector space. Given a question, we use DPR to obtain the initial set of evidence, which includes tables and passages. As most WikiQA questions require reasoning hops across different pieces of information (e.g., hopping from the album table to its entry artist page in Figure 1), we contend that directly feeding this raw evidence set to Working Memory is insufficient for prompting LLMs. Thus, we further use additional intermediary modules, i.e., linker and chainer, from CORE (Ma et al., 2022) to consolidate the raw evidence, including connecting relevant documents, reranking evidence, and splicing them into evidence chains. We refer to Ma et al. (2022) for more details.</p>
<p>Prompt Engine: The prompt templates utilized for Wiki QA is shown in the appendix in Table 9.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Knowledge Consolidator</th>
<th>Feedback</th>
<th>Wiki QA</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>P $\uparrow$</td>
<td>R $\uparrow$</td>
<td>F1 $\uparrow$</td>
<td></td>
</tr>
<tr>
<td>ChatGPT</td>
<td>-</td>
<td>-</td>
<td>0.48</td>
<td>1.52</td>
<td>0.59</td>
<td></td>
</tr>
<tr>
<td>LLM-AUGMENTER</td>
<td>DPR</td>
<td>✗</td>
<td>2.08</td>
<td>4.31</td>
<td>2.38</td>
<td></td>
</tr>
<tr>
<td>LLM-AUGMENTER</td>
<td>CORE</td>
<td>✗</td>
<td>7.06</td>
<td>14.77</td>
<td>8.08</td>
<td></td>
</tr>
<tr>
<td>LLM-AUGMENTER</td>
<td>CORE</td>
<td>✓</td>
<td>8.93</td>
<td>33.87</td>
<td>11.80</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 5: Evaluation results on Wiki QA. Each model retrieves top-5 knowledge snippets from the corresponding knowledge source. The top-5 answer recall of consolidated evidence (CORE) is 50.83.</p>
<p>Utility: Here, as a response to a given question is deemed to leverage information from the consolidated knowledge, we use recall as the utility score, i.e., preferring responses with higher token overlap with the corresponding evidence set. Similar to Section 3, we again consider a template-based natural language generator for giving feedback to ChatGPT.</p>
<p>Evaluation Metrics: As WikiQA mainly concerns short-form answers, we evaluate the generated responses using the token-level precision, recall and F1 scores against the annotated answers.</p>
<h3>4.3 Results</h3>
<p>Table 5 presents the evaluation results on Wiki QA. As expected, the closed-book model alone performs very poorly. Based on our manual inspections, we find that most error cases are hallucinated answers and ChatGPT abstains from answering for $17\%$ cases. We observe that incorporating knowledge obtained from either DPR or CORE significantly improves the F1 score. The substantial improvements observed over the closed-book ChatGPT model indicate the importance of enhancing LLMs with external knowledge. Compared with raw evidence from DPR (row 2), we observe that consolidated evidence from our proposed Knowledge Consolidator with CORE (row 3) is more useful to the frozen ChatGPT model, achieving more pronounced improvements across the board. This suggests that it is crucial to consolidate knowledge for eliciting blackbox LLMs to perform grounded reasoning. Lastly, consistent with the observations for news chat and customer service scenarios in Section 3, augmenting ChatGPT with automated feedback further improves alignments (adapting ChatGPT to perform multi-step grounded reasoning), leading to a substantial increase in recall and F1 scores.</p>
<p>Compared with the state-of-the-art fine-tuned model <em>Ma et al. (2022)</em> using top-50 consolidated evidence, there still remains a noticeable gap in performance. Besides a lower answer recall of the consolidated evidence, we attribute it to extra alignments required for ChatGPT to respond in a more concise way and conduct faithful step-by-step reasoning. Therefore, there is ample room for future explorations on elicitive prompting to achieve further improvements.</p>
<h2>5 Related Work</h2>
<p>Numerous LLMs for text generation <em>Radford et al. (2018)</em> have been proposed over the years, including very competitive ones such as GPT-3 <em>Brown et al. (2020); Ouyang et al. (2022)</em>, OPT <em>Zhang et al. (2022)</em>, GPT-j <em>Wang and Komatsuzaki (2021)</em>, and ChatGPT. However, most of them do not naturally incorporate external knowledge. To address this limitation, various works augment LLMs with knowledge consisting of e.g., personalized recommendations <em>Ghazvininejad et al. (2017)</em>, Wikipedia article and web search <em>Dinan et al. (2018); Shuster et al. (2022)</em>, structured and unstructured knowledge of task-oriented dialog <em>Peng et al. (2022)</em>. Recent advances have focused on jointly finetuning the retriever and generation components of retrieval-augmented text generation systems <em>Lewis et al. (2020); Zhang et al. (2021)</em>, but these methods are not applicable to black-box LLMs.</p>
<p>More recent work attempts to combine blackbox LLMs with external knowledge, such as incorporating external knowledge into prompts <em>Madaan et al. (2022); Lazaridou et al. (2022)</em>, making GPT-3 more faithful <em>He et al. (2022)</em>, and combining web knowledge with GPT-3 <em>Nakano et al. (2021)</em>. In very recent works related to ours, <em>Shi et al. (2023)</em> tune the ranker of a black-box LLM. <em>Schick et al. (2023)</em> tune black-box LLMs’</p>
<p>access to different APIs and show improvement on a variety of understanding and reasoning tasks. We consider these works to complementary to ours, as we assume our set of APIs to be given and fixed, and we instead focus more on when and what APIs to request, interactive feedback with the LLM, and developing a self-learning ability through utility functions.</p>
<h2>6 Limitations and Future Directions</h2>
<p>A main limitation of this work is that interactive feedback with a computationally expensive model such as ChatGPT can significantly slow down the user experience, as ChatGPT is often queried twice for a single response. However, we think this can translate into more choice for the user. For example, the initial ChatGPT response can be shown to the user as it is being decoded, and the user could then be informed that a more accurate response is available (depending on the utility function). Then, an impatient user can decide to ignore this option, while a user more mindful of response accuracy may decide to see the improved ChatGPT response. In task-oriented and high-stakes scenarios, we believe many users would prefer the slower but more accurate option.</p>
<p>The main results of the paper are with a policy designed manually, as due to the current highdemand for ChatGPT and its limited bandwidth. As reinforcement learning can be quite sample inefficient, we trained our policy using an LLM (T5Base) we could easily query, and these RL experiments demonstrate the effectiveness of LLMAUGMENTER. As ChatGPT becomes more available, we plan to update the paper with RL experiments involving ChatGPT. The current version of the paper does not include human evaluation, as the goal with our current utility function (KF1) shown we can make ChatGPT more grounded and our experiments suggest the responses of our best system are better at capturing the words of the (gold) knowledge. As we move to towards much utility functions such as safety, it will be important to add more fine-grained analyzes of the responses, and we will add human evaluation. In future work, we also plan to leverage interactions with real users and user feedbacks to train LLMAUGMENTER.</p>
<h2>7 Conclusions</h2>
<p>We introduced LLM-AUGMENTER, a framework for augmenting black-box LLMs (e.g., ChatGPT) with external knowledge and automated feedback. The external knowledge provided as part of the LLM prompts helps generate more responses that are more grounded into external knowledge relevant to the current conversation. The automated feedback elicits the "follow-up correction" abilities of models such as ChatGPT and InstructGPT in order to produce revised responses that rank higher according to some given utility functions (e.g., groundedness as measured by KF1). These various components are integrated together as part of an RL framework, which we optimize end-to-end using policy gradient. End-toend experiments with T5 show the effectiveness of LLM-AUGMENTER, while experiments on ChatGPT show significant increases both in terms of KF1 and a host of text generation metrics.</p>
<h2>Ethics Statement</h2>
<p>It is widely understood that large language models have the potential to generate harmful, offensive, and inappropriate content (Bender et al., 2021; Bommasani et al., 2021; Weidinger et al., 2021). This paper is an attempt to address a major harm of LLMs, namely factual integrity. This paper does not address the problem of offensive content generation, but future work on LLMAUGMENTER could help mitigate such harm via, e.g., offensiveness-related utility functions.</p>
<p>As with other knowledge-augmented text generation applications, we cannot rule out that external sources could compromise the factuality of generated text. It is, therefore, important to encourage users to check the relevance of external sources that supplement the generated text.</p>
<h2>Acknowledgements</h2>
<p>We thank Saleema Amershi, Ahmed Awadallah, Nguyen Bach, Paul Bennett, Chris Brockett, Weixin Cai, Dhivya Eswaran, Adam Fourney, Hsiao-Wuen Hon, Chunyuan Li, Ricky Loynd, Hoifung Poon, Corby Rosset, Bin Yu, Sheng Zhang, and members of the Microsoft Research Deep Learning group for valuable discussions and comments.</p>
<h2>References</h2>
<p>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for mt evaluation with improved correlation with human judgments. In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65-72.</p>
<p>Emily M. Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency.</p>
<p>Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Benjamin Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, J. F. Nyarko, Giray Ogut, Laurel J. Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jack Ryan, Christopher R'e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishna Parasuram Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei A. Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. 2021. On the opportunities and risks of foundation models. ArXiv, abs/2108.07258.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario</p>
<p>Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2018. Wizard of Wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations.</p>
<p>Mihail Eric, Rahul Goel, Shachi Paul, Adarsh Kumar, Abhishek Sethi, Peter Ku, Anuj Kumar Goyal, Sanchit Agarwal, Shuyang Gao, and Dilek Hakkani-Tur. 2019. Multiwoz 2.1: A consolidated multi-domain dialogue dataset with state corrections and state tracking baselines. arXiv preprint arXiv:1907.01669.</p>
<p>Michel Galley, Chris Brockett, Xiang Gao, Jianfeng Gao, and Bill Dolan. 2019. Grounded response generation task at DSTC7. In AAAI Dialog System Technology Challenges Workshop.</p>
<p>Jianfeng Gao, Michel Galley, and Lihong Li. 2019. Neural approaches to conversational AI. Foundations and Trends in Information Retrieval, 13(23):127-298.</p>
<p>Jianfeng Gao, Chenyan Xiong, Paul Bennett, and Nick Craswell. 2022. Neural approaches to conversational information retrieval. arXiv preprint arXiv:2201.05176.</p>
<p>Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and Michel Galley. 2017. A knowledge-grounded neural conversation model. CoRR, abs/1702.01932.</p>
<p>Amelia Glaese, Nathan McAleese, Maja Trkebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy CampbellGillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, A. See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Sovna Mokr'a, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William S. Isaac, John F. J. Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. 2022. Improving alignment of dialogue agents via targeted human judgements. ArXiv, abs/2209.14375.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. REALM: Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909.</p>
<p>Hangfeng He, Hongming Zhang, and Dan Roth. 2022. Rethinking with retrieval: Faithful large language model inference. arXiv preprint arXiv:2301.00303.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, Online. Association for Computational Linguistics.</p>
<p>Seokhwan Kim, Mihail Eric, Karthik Gopalakrishnan, Behnam Hedayatnia, Yang Liu, and Dilek HakkaniTur. 2020. Beyond domain APIs: Task-oriented conversational modeling with unstructured knowledge access. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 278-289.</p>
<p>Seokhwan Kim, Spandana Gella, Di Jin, Alexandros Papangelis, Behnam Hedayatnia, Yang Liu, and Dilek Hakkani-Tür. 2023. DSTC11 track proposal: Task-oriented conversational modeling with subjective knowledge. https://github.com/ alexa/dstc11-track5.</p>
<p>Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022. Internetaugmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. ArXiv, abs/2005.11401.</p>
<p>Rongzhong Lian, Min Xie, Fan Wang, Jinhua Peng, and Hua Wu. 2019. Learning to select knowledge for response generation in dialog systems. In International Joint Conference on Artificial Intelligence.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In ACL workshop, pages $74-81$.</p>
<p>Kaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg, and Jianfeng Gao. 2022. Open-domain question answering via chain of reasoning over heterogeneous knowledge. arXiv preprint arXiv:2210.12338.</p>
<p>Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. 2022. Memory-assisted prompt editing to improve GPT-3 after deployment. arXiv preprint arXiv:2201.06009.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. WebGPT: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In $A C L$, pages $311-318$.</p>
<p>Baolin Peng, Michel Galley, Pengcheng He, Chris Brockett, Lars Lidén, Elnaz Nouri, Zhou Yu, Bill Dolan, and Jianfeng Gao. 2022. GODEL: Largescale pre-training for goal-directed dialog. ArXiv, abs/2206.11309.</p>
<p>Maja Popović. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392-395, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online.</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. 2023. REPLUG: Retrievalaugmented black-box language models. ArXiv, abs/2301.12652.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567.</p>
<p>Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, W.K.F. Ngan, Spencer Poff, Naman Goyal, Arthur D. Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. 2022. BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage. ArXiv, abs/2208.03188.</p>
<p>Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 billion parameter autoregressive language model. https://github.com/kingoflolz/ mesh-transformer-jax.</p>
<p>Laura Weidinger, John F. J. Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zachary Kenton, Sande Minnich Brown, William T. Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William S. Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. 2021. Ethical and social risks of harm from language models. ArXiv, abs/2112.04359.</p>
<p>Ronald J. Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine Learning, 8:229-256.</p>
<p>Yi-Ting Yeh, Maxine Eskenazi, and Shikib Mehri. 2021. A comprehensive assessment of dialog evaluation metrics. In arXiv.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. BARTScore: Evaluating generated text as text generation.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open pre-trained transformer language models. ArXiv, abs/2205.01068.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2019. BERTScore: Evaluating text generation with BERT. CoRR, abs/1904.09675.</p>
<p>Yizhe Zhang, Siqi Sun, Xiang Gao, Yuwei Fang, Chris Brockett, Michel Galley, Jianfeng Gao, and Bill Dolan. 2021. RetGen: A joint framework for retrieval and grounded text generation modeling. In AAAI Conference on Artificial Intelligence.</p>
<p>Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Training language models with memory augmentation. ArXiv, abs/2205.12674.</p>
<h2>A Appendix</h2>
<p>Table 6 provides sample responses contrasting ChatGPT and LLM-Augmenter. First, we can see that ChatGPT fails to provide a response related to specific knowledge related to the user, e.g., a local Indian restaurant. In the second part of the table, we show LLM-Augmenter's Working Memory, which highlights the richer information retrieved from external knowledge to help the underling LLM (i.e., ChatGPT as well) generate more contentful responses. The first LLM response received by LLM-AUGMENTER is unfortunately not satisfactory, as the quality and specificity of LLM generation can be unpredictable. In this case, the Utility module has determined that the first response did not meet its criteria (i.e., KF1 above a given threshold), and issues a feedback to the LLM module (i.e., "response is inconsistent with the knowledge"). The second response received by LLM-AUGMENTER is much more satisfactory according to the utility function, and therefore sent to the user.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Instruction</th>
<th style="text-align: left;">I want you to act as a chatbot AI for travel planning. You need to <br> answer customer's questions nicely.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Context</td>
<td style="text-align: left;">User: Hello, I am looking for a moderately priced restaurant on the <br> north side of town. <br> Assistant: I recommend golden wok. It is in the north part of town <br> with a moderate price range. <br> User: Yes, I am looking for a moderately priced restaurant in the <br> north part of town. <br> Assistant: I recommend golden wok. It is in the north part of town <br> with a moderate price range. <br> User: I don't want Golden Wok, is there anything else?, <br> Assistant: Yes, your other option is The Nirala. It serves Indian food <br> and is also located in the north part of town. <br> User: Is their beer a good value for the money?</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT Response</td>
<td style="text-align: left;">I am sorry, I do not have information on the beer prices at The Nirala. <br> It is best to contact the restaurant directly for more information.</td>
</tr>
<tr>
<td style="text-align: left;">Working Memory</td>
<td style="text-align: left;">Review: They also have a modest drinks menu with some afford- <br> able prices, but I was more interested in the beautiful view from my <br> window seat, which allowed me to watch the sun setting as I dined. <br> Q: Is alcohol served at this restaurant? A: Yes, alcohol is served at <br> this restaurant. <br> Review: I found them to be overpriced and mediocre. <br> Review: The drinks ae priced well. <br> Review: Something that I disliked, however was the prices for their <br> drinks.</td>
</tr>
<tr>
<td style="text-align: left;">LLM-AUGMENTER Response</td>
<td style="text-align: left;">I am sorry, I do not have information on the value of their beer. I only <br> have information on the location and price range of the restaurant.</td>
</tr>
<tr>
<td style="text-align: left;">Rule-based Feedback</td>
<td style="text-align: left;">The response is inconsistent with the knowledge. Please generate <br> again?</td>
</tr>
<tr>
<td style="text-align: left;">LLM-AUGMENTER Response</td>
<td style="text-align: left;">Some reviews mention that the drinks at the restaurant are priced <br> well, while others mention that they found the prices to be overpriced <br> and mediocre.</td>
</tr>
<tr>
<td style="text-align: left;">Human Response</td>
<td style="text-align: left;">Unfortunately the reviews are split, half say they are overpriced and <br> have say they are affordable.</td>
</tr>
</tbody>
</table>
<p>Table 6: Prompt and example responses of different models for the customer service scenarios.</p>
<p>I want you to act as a chatbot. You need to answer user' questions nicely.
Context:
User: $\left[U_{1}\right]$
Assistant: $\left[R_{1}\right]$
・.
User: $\left[U_{t}\right]$
Assistant: $\left[R_{t}\right]$
I want you to act as a chatbot. You will be presented with knowledge snippets. You need to answer user' questions nicely and accurately based on the knowledge snippets.
Working Memory: $\left[M_{t}\right]$
Context:
User: $\left[U_{1}\right]$
Assistant: $\left[R_{1}\right]$
・・
User: $\left[U_{t}\right]$
Assistant: $\left[R_{t}\right]$
Table 7: Prompt Templates for News Chat. LLMs generated responses is highlighted with [].</p>
<p>I want you to act as a chatbot AI for travel planning. You need to answer customer's questions nicely.
Context:
User: $\left[U_{1}\right]$
Assistant: $\left[R_{1}\right]$
・・
User: $\left[U_{t}\right]$
Assistant: $\left[R_{t}\right]$
I want you to act as a chatbot AI for travel planning. You will be presented with knowledge snippets. You need to answer customer's questions nicely and accurately based on the knowledge snippets.
Working Memory: $\left[M_{t}\right]$
Context:
User: $\left[U_{1}\right]$
Assistant: $\left[R_{1}\right]$
・・
User: $\left[U_{t}\right]$
Assistant: $\left[R_{t}\right]$
Table 8: Prompt Templates for Customer Service. LLMs generated response are highlighted with [].</p>
<p>I am a highly intelligent question answering bot that can answer questions. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with "Unknown".
Question: $[Q]$
Answer: $[A]$
I am a highly intelligent question answering bot, and can answer questions given some documents and tables. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with "Unknown".
Working Memory: $[M]$
Question: $[Q]$
Answer: $[A]$
Table 9: Prompt Templates for Wiki QA. LLMs generated response are highlighted with [].</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ If the KF1 score falls below a certain threshold, the feedback is "The response is inconsistent with the knowledge. Please generate again."&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
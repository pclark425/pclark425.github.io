<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9614 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9614</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9614</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-272753607</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.12832v1.pdf" target="_blank">FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists</a></p>
                <p><strong>Paper Abstract:</strong> Flavor development in the food industry is increasingly challenged by the need for rapid innovation and precise flavor profile creation. Traditional flavor research methods typically rely on iterative, subjective testing, which lacks the efficiency and scalability required for modern demands. This paper presents three contributions to address the challenges. Firstly, we define a new problem domain for scientific agents in flavor science, conceptualized as the generation of hypotheses for flavor profile sourcing and understanding. To facilitate research in this area, we introduce the FoodPuzzle, a challenging benchmark consisting of 978 food items and 1,766 flavor molecules profiles. We propose a novel Scientific Agent approach, integrating in-context learning and retrieval augmented techniques to generate grounded hypotheses in the domain of food science. Experimental results indicate that our model significantly surpasses traditional methods in flavor profile prediction tasks, demonstrating its potential to transform flavor development practices.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9614.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9614.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FOODPUZZLE Scientific Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FOODPUZZLE Autonomous Scientific Agent for Flavor Science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid LLM-based scientific agent developed in this paper that synthesizes in-context demonstrations and out-of-domain scholarly evidence (from PubMed/arXiv via Google Custom Search) using retrieval-augmented generation and chain-of-thought role-play to generate grounded hypotheses mapping flavor molecules to food sources or to complete molecular profiles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Hybrid Scientific Agent (uses LLaMA3-8B-instruct, Gemini-1.5-Pro, GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pipeline combining foundation LLMs (LLaMA3-8B-instruct, Gemini-1.5-Pro, GPT-3.5-turbo) with retrieval-augmented generation (RAG) over scholarly sources and in-context learning; includes an information-entropy based molecule selection step, a BM25 retriever for demonstrations, offline Google Custom Search results (PubMed/arXiv) as evidence, and a role-play Chain-of-Thought process with separate 'Scientist' and 'Reviewer' models that generate and evaluate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B (LLaMA3-8B-instruct); others not specified</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Primary structured dataset: FOODPUZZLE constructed from FlavorDB mapping 978 foods to 1766 flavor molecules (used for demonstrations and training/fine-tuning). Out-of-domain scholarly evidence corpus: academic articles and internet blogs discovered via Google Custom Search restricted to PubMed and arXiv; search results were executed offline and stored locally for retrieval at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Two task-oriented queries: (MFP) given a set of molecules M, predict the food source F (querying each selected molecule: 'What are the common food sources that could contain m?'); (MPC) given a food F and partial molecules, identify missing molecules (query: 'What molecules are present in F?').</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieval-augmented generation combining: (1) starting-point identification by selecting up to 10 molecules with lowest information entropy from the input set; (2) BM25 retrieval of top-k labeled demonstrations from the FOODPUZZLE/training set for in-context examples; (3) offline Google Custom Search over PubMed/arXiv to collect scholarly evidence per molecule or food and store locally; (4) Chain-of-Thought role-play where a 'Scientist' LLM proposes three hypotheses (using demonstrations + retrieved evidence) and a 'Reviewer' LLM evaluates and selects the best hypothesis; outputs include the selected hypothesis and traceable evidence supporting it.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Grounded hypotheses: (a) predicted food category/source for a molecule set (MFP) and (b) predicted missing molecules for a food (MPC), each with traceable evidence and chain-of-thought style rationale.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>A final prediction consisting of a single selected hypothesis mapping input molecules to a predicted food source (or a set of missing molecules), accompanied by citations/evidence snippets from retrieved scholarly articles and an internal reviewer justification. (Paper does not provide verbatim example hypotheses.)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Task-specific metrics: MFP evaluated by accuracy of predicted food category (semantic equivalence judged by a language model); MPC evaluated by F1 score computed on functional groups extracted from predicted vs ground-truth molecule sets. Models compared across categories: foundation LLMs zero-shot, domain-specific LLMs fine-tuned, in-context learning, and the autonomous scientist agent.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>The autonomous scientist agent outperformed baselines: e.g., with LLaMA3-8B-instruct inside the agent MFP accuracy = 35.5% and MPC F1 = 0.374 (higher than zero-shot and in-context baselines). In-context best (Gemini1.5Pro) achieved MFP 34.6% and F1 0.373; domain-specific MolT5/Bio-T5 performed worse (MFP 9.8%/16.6%, F1 0.144/0.278).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Improved empirical performance over zero-shot and many baselines; integrates domain evidence to produce traceable justifications; combines multiple evidence sources and demonstrations to mitigate lack of parametric domain knowledge; structured hypothesis generation (Scientist/Reviewer) improves selection among candidate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Dependence on quality and coverage of retrieved scholarly evidence (Google Custom Search results vary); offline search/storage without a quantified corpus size; models still exhibit domain knowledge gaps; possible contradictions across sources; some components (foundation LLMs, retriever) may be underpowered for domain-specific molecule names.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reported failure modes from error analysis: Inappropriate initialization of search space (32% of sampled errors) where agent selects generic/common molecules instead of domain-significant ones; Epistemic hallucination (26%) where outputs lack grounding in evidence or conflate presence vs. replicating flavor; Wrong interpretation of online sources (20%) where the agent infers facts not supported by the paper (e.g., asserting a molecule is present in a food when the source only describes sensory similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9614.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9614.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG (scholarly RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation with Scholarly Article Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper adapts Retrieval-Augmented Generation by retrieving scholarly articles (via Google Custom Search limited to PubMed and arXiv) and combining them with in-context demonstrations to enable LLMs to synthesize domain evidence and generate grounded hypotheses for flavor sourcing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledgeintensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Retrieval-Augmented Generation pipeline (used with foundation LLMs such as LLaMA3-8B-instruct, GPT-3.5-turbo, Gemini1.5-Pro)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A RAG pipeline that fetches and consolidates external domain evidence (academic articles and blogs) into prompt context for LLM hypotheses; retrieval is performed offline via Google Custom Search constrained to reputable scientific repositories, and BM25 is used for demonstration retrieval from the labeled train set.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Locally stored search results from Google Custom Search over PubMed and arXiv for molecules and foods, plus the FOODPUZZLE training set used as a demonstration corpus (passages of the form 'Food: F'. 'Molecules: M').</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Molecule-centric and food-centric queries for evidence collection: e.g., 'What are the common food sources that could contain m?' and 'What molecules are present in F?'.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>RAG: retrieve relevant scholarly passages per query, prepend retrieved demonstrations (from BM25) and evidence to prompts, and apply Chain-of-Thought prompting within a Scientist/Reviewer role-play to synthesize evidence into hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Synthesized, evidence-grounded hypotheses and rationales tying molecules to food sources or listing missing molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>A hypothesis statement linking a subset of informative molecules to a predicted food source supported by citations from retrieved scholarly papers. (No verbatim example in paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Same task evaluations (MFP accuracy; MPC F1 on functional groups) used to measure the effect of RAG-enhanced prompts vs baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>RAG as implemented within the Scientific Agent yields the best overall performance reported (e.g., agent MFP 35.5%, MPC F1 0.374 with LLaMA3-8B-instruct), demonstrating practical gains versus zero-shot and non-augmented methods.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Adds domain-specific evidence at inference without retraining; improves grounding and interpretability by attaching source evidence; compatible with in-context learning and hypothesis-ranking strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality of synthesized knowledge depends on retrieval accuracy and interpretation; offline search storage not quantified; retrieval methods (BM25) chosen for demonstrations performed better than dense retrievers but may still miss relevant scholarly details.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Misinterpretation of retrieved scholarly passages leading to unsupported claims; contradictions among sources leading to ambiguous or incorrect synthesized hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9614.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9614.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BM25 in-context retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BM25 Retriever for In-Context Demonstration Selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lexical retrieval approach (BM25) used to select the top-k most similar labeled demonstrations from the FOODPUZZLE training set as in-context examples; empirically outperformed dense retrievers on these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BM25 lexical retriever</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Classic probabilistic retrieval (BM25) applied to the passage index where each passage encodes a training example in the format 'Food: F'. 'Molecules: M' for selection of in-context demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Indexed FOODPUZZLE training set passages (80% of FlavorDB-derived dataset used for train/dev/test splitting).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Retrieve demonstrations most lexically similar to the query (molecule list or partial molecular profile) for use as few-shot examples in LLM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieve top-k demonstrations with BM25 and include them in prompts to the LLM to provide concrete examples for in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Selected demonstrations inserted into prompts to improve hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Three retrieved demonstration passages of the form: 'Food: F'. 'Molecules: M' included in the Scientist model prompt. (No verbatim passages provided in paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>End-to-end task performance comparison showing BM25 vs Sentence Transformer vs DPR retrieval approaches on MFP accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>BM25 achieved 23.2% end-to-end accuracy on the MFP task versus 13.1% (Sentence Transformer) and 18.0% (DPR), and contributed to improved agent performance when combined with scholarly evidence retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Simple, robust lexical matching that better handles domain-specific molecule names and outperforms evaluated dense retrievers on these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lexical match may miss semantically related but lexically different demonstrations; performance tied to quality/formatting of demonstration passages.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When molecules are referred to with synonyms or alternate nomenclature not present in training passages, BM25 may fail to retrieve the most relevant demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9614.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9614.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT Role-play (Scientist/Reviewer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Role-Play with Scientist and Reviewer LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-role Chain-of-Thought prompting scheme where a 'Scientist' LLM generates multiple hypotheses from retrieved evidence and demonstrations, and a 'Reviewer' LLM evaluates and selects the best hypothesis to improve hypothesis quality and provide an internal critique.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Scientist/Reviewer role-play (implemented with foundation LLMs such as LLaMA3-8B-instruct, GPT-3.5-turbo, or Gemini1.5-Pro)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A structured CoT method: Scientist receives FOODPUZZLE input, retrieved scholarly evidence, and demonstrations then produces three candidate hypotheses; Reviewer receives inputs plus the candidates and selects or rejects hypotheses based on argument quality; selected hypothesis returned as final output.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B for LLaMA3-8B-instruct when used in experiments; other model sizes not specified</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Same retrieval inputs as the RAG pipeline: BM25 demonstrations from FOODPUZZLE training passages and offline scholarly evidence per molecule/food from PubMed/arXiv search results.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Produce and evaluate candidate hypotheses that explain the mapping between molecules and food sources or predict missing molecules for a food item.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Chain-of-Thought role-play: internal multi-hypothesis generation followed by critical evaluation (Scientist proposes 3; Reviewer selects best) to synthesize evidence into a single grounded hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Selected hypothesis with reviewer justification and attached evidence summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Scientist: Hypothesis A/B/C (three candidate mappings); Reviewer: selects Hypothesis B with rationale referencing evidence snippets. (No concrete textual examples given in the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Measured by downstream task performance (MFP accuracy, MPC F1) and manual error analysis of sampled failures.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Role-play approach within the Scientific Agent contributed to the highest reported MFP accuracy (35.5% with LLaMA3-8B-instruct agent) and MPC F1 (0.374), indicating effectiveness compared to single-pass generation baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Provides an explicit internal selection/critique mechanism that can reduce low-quality hypotheses and increases interpretability of decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Reviewer may still accept hallucinated or poorly grounded hypotheses if evidence is ambiguous; adds compute overhead (multiple generations per prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reviewer sometimes selects hypotheses based on flawed evidence interpretation; chain-of-thought can still produce ungrounded reasoning leading to epistemic hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledgeintensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented language model pre-training <em>(Rating: 2)</em></li>
                <li>In-context retrieval-augmented language models <em>(Rating: 2)</em></li>
                <li>Atlas: few-shot learning with retrieval augmented language models <em>(Rating: 2)</em></li>
                <li>Active retrieval augmented generation <em>(Rating: 2)</em></li>
                <li>Improving language models by retrieving from trillions of tokens <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Augmenting large language models with chemistry tools <em>(Rating: 1)</em></li>
                <li>Bioplanner: Automatic evaluation of llms on protocol planning in biology <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9614",
    "paper_id": "paper-272753607",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "FOODPUZZLE Scientific Agent",
            "name_full": "FOODPUZZLE Autonomous Scientific Agent for Flavor Science",
            "brief_description": "A hybrid LLM-based scientific agent developed in this paper that synthesizes in-context demonstrations and out-of-domain scholarly evidence (from PubMed/arXiv via Google Custom Search) using retrieval-augmented generation and chain-of-thought role-play to generate grounded hypotheses mapping flavor molecules to food sources or to complete molecular profiles.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Hybrid Scientific Agent (uses LLaMA3-8B-instruct, Gemini-1.5-Pro, GPT-3.5-turbo)",
            "model_description": "A pipeline combining foundation LLMs (LLaMA3-8B-instruct, Gemini-1.5-Pro, GPT-3.5-turbo) with retrieval-augmented generation (RAG) over scholarly sources and in-context learning; includes an information-entropy based molecule selection step, a BM25 retriever for demonstrations, offline Google Custom Search results (PubMed/arXiv) as evidence, and a role-play Chain-of-Thought process with separate 'Scientist' and 'Reviewer' models that generate and evaluate hypotheses.",
            "model_size": "8B (LLaMA3-8B-instruct); others not specified",
            "input_corpus_description": "Primary structured dataset: FOODPUZZLE constructed from FlavorDB mapping 978 foods to 1766 flavor molecules (used for demonstrations and training/fine-tuning). Out-of-domain scholarly evidence corpus: academic articles and internet blogs discovered via Google Custom Search restricted to PubMed and arXiv; search results were executed offline and stored locally for retrieval at inference time.",
            "input_corpus_size": null,
            "topic_query_description": "Two task-oriented queries: (MFP) given a set of molecules M, predict the food source F (querying each selected molecule: 'What are the common food sources that could contain m?'); (MPC) given a food F and partial molecules, identify missing molecules (query: 'What molecules are present in F?').",
            "distillation_method": "Retrieval-augmented generation combining: (1) starting-point identification by selecting up to 10 molecules with lowest information entropy from the input set; (2) BM25 retrieval of top-k labeled demonstrations from the FOODPUZZLE/training set for in-context examples; (3) offline Google Custom Search over PubMed/arXiv to collect scholarly evidence per molecule or food and store locally; (4) Chain-of-Thought role-play where a 'Scientist' LLM proposes three hypotheses (using demonstrations + retrieved evidence) and a 'Reviewer' LLM evaluates and selects the best hypothesis; outputs include the selected hypothesis and traceable evidence supporting it.",
            "output_type": "Grounded hypotheses: (a) predicted food category/source for a molecule set (MFP) and (b) predicted missing molecules for a food (MPC), each with traceable evidence and chain-of-thought style rationale.",
            "output_example": "A final prediction consisting of a single selected hypothesis mapping input molecules to a predicted food source (or a set of missing molecules), accompanied by citations/evidence snippets from retrieved scholarly articles and an internal reviewer justification. (Paper does not provide verbatim example hypotheses.)",
            "evaluation_method": "Task-specific metrics: MFP evaluated by accuracy of predicted food category (semantic equivalence judged by a language model); MPC evaluated by F1 score computed on functional groups extracted from predicted vs ground-truth molecule sets. Models compared across categories: foundation LLMs zero-shot, domain-specific LLMs fine-tuned, in-context learning, and the autonomous scientist agent.",
            "evaluation_results": "The autonomous scientist agent outperformed baselines: e.g., with LLaMA3-8B-instruct inside the agent MFP accuracy = 35.5% and MPC F1 = 0.374 (higher than zero-shot and in-context baselines). In-context best (Gemini1.5Pro) achieved MFP 34.6% and F1 0.373; domain-specific MolT5/Bio-T5 performed worse (MFP 9.8%/16.6%, F1 0.144/0.278).",
            "strengths": "Improved empirical performance over zero-shot and many baselines; integrates domain evidence to produce traceable justifications; combines multiple evidence sources and demonstrations to mitigate lack of parametric domain knowledge; structured hypothesis generation (Scientist/Reviewer) improves selection among candidate hypotheses.",
            "limitations": "Dependence on quality and coverage of retrieved scholarly evidence (Google Custom Search results vary); offline search/storage without a quantified corpus size; models still exhibit domain knowledge gaps; possible contradictions across sources; some components (foundation LLMs, retriever) may be underpowered for domain-specific molecule names.",
            "failure_cases": "Reported failure modes from error analysis: Inappropriate initialization of search space (32% of sampled errors) where agent selects generic/common molecules instead of domain-significant ones; Epistemic hallucination (26%) where outputs lack grounding in evidence or conflate presence vs. replicating flavor; Wrong interpretation of online sources (20%) where the agent infers facts not supported by the paper (e.g., asserting a molecule is present in a food when the source only describes sensory similarity).",
            "uuid": "e9614.0",
            "source_info": {
                "paper_title": "FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "RAG (scholarly RAG)",
            "name_full": "Retrieval-Augmented Generation with Scholarly Article Retrieval",
            "brief_description": "The paper adapts Retrieval-Augmented Generation by retrieving scholarly articles (via Google Custom Search limited to PubMed and arXiv) and combining them with in-context demonstrations to enable LLMs to synthesize domain evidence and generate grounded hypotheses for flavor sourcing tasks.",
            "citation_title": "Retrieval-augmented generation for knowledgeintensive nlp tasks",
            "mention_or_use": "use",
            "model_name": "Retrieval-Augmented Generation pipeline (used with foundation LLMs such as LLaMA3-8B-instruct, GPT-3.5-turbo, Gemini1.5-Pro)",
            "model_description": "A RAG pipeline that fetches and consolidates external domain evidence (academic articles and blogs) into prompt context for LLM hypotheses; retrieval is performed offline via Google Custom Search constrained to reputable scientific repositories, and BM25 is used for demonstration retrieval from the labeled train set.",
            "model_size": null,
            "input_corpus_description": "Locally stored search results from Google Custom Search over PubMed and arXiv for molecules and foods, plus the FOODPUZZLE training set used as a demonstration corpus (passages of the form 'Food: F'. 'Molecules: M').",
            "input_corpus_size": null,
            "topic_query_description": "Molecule-centric and food-centric queries for evidence collection: e.g., 'What are the common food sources that could contain m?' and 'What molecules are present in F?'.",
            "distillation_method": "RAG: retrieve relevant scholarly passages per query, prepend retrieved demonstrations (from BM25) and evidence to prompts, and apply Chain-of-Thought prompting within a Scientist/Reviewer role-play to synthesize evidence into hypotheses.",
            "output_type": "Synthesized, evidence-grounded hypotheses and rationales tying molecules to food sources or listing missing molecules.",
            "output_example": "A hypothesis statement linking a subset of informative molecules to a predicted food source supported by citations from retrieved scholarly papers. (No verbatim example in paper.)",
            "evaluation_method": "Same task evaluations (MFP accuracy; MPC F1 on functional groups) used to measure the effect of RAG-enhanced prompts vs baselines.",
            "evaluation_results": "RAG as implemented within the Scientific Agent yields the best overall performance reported (e.g., agent MFP 35.5%, MPC F1 0.374 with LLaMA3-8B-instruct), demonstrating practical gains versus zero-shot and non-augmented methods.",
            "strengths": "Adds domain-specific evidence at inference without retraining; improves grounding and interpretability by attaching source evidence; compatible with in-context learning and hypothesis-ranking strategies.",
            "limitations": "Quality of synthesized knowledge depends on retrieval accuracy and interpretation; offline search storage not quantified; retrieval methods (BM25) chosen for demonstrations performed better than dense retrievers but may still miss relevant scholarly details.",
            "failure_cases": "Misinterpretation of retrieved scholarly passages leading to unsupported claims; contradictions among sources leading to ambiguous or incorrect synthesized hypotheses.",
            "uuid": "e9614.1",
            "source_info": {
                "paper_title": "FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "BM25 in-context retrieval",
            "name_full": "BM25 Retriever for In-Context Demonstration Selection",
            "brief_description": "A lexical retrieval approach (BM25) used to select the top-k most similar labeled demonstrations from the FOODPUZZLE training set as in-context examples; empirically outperformed dense retrievers on these tasks.",
            "citation_title": "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval",
            "mention_or_use": "use",
            "model_name": "BM25 lexical retriever",
            "model_description": "Classic probabilistic retrieval (BM25) applied to the passage index where each passage encodes a training example in the format 'Food: F'. 'Molecules: M' for selection of in-context demonstrations.",
            "model_size": null,
            "input_corpus_description": "Indexed FOODPUZZLE training set passages (80% of FlavorDB-derived dataset used for train/dev/test splitting).",
            "input_corpus_size": null,
            "topic_query_description": "Retrieve demonstrations most lexically similar to the query (molecule list or partial molecular profile) for use as few-shot examples in LLM prompts.",
            "distillation_method": "Retrieve top-k demonstrations with BM25 and include them in prompts to the LLM to provide concrete examples for in-context learning.",
            "output_type": "Selected demonstrations inserted into prompts to improve hypothesis generation.",
            "output_example": "Three retrieved demonstration passages of the form: 'Food: F'. 'Molecules: M' included in the Scientist model prompt. (No verbatim passages provided in paper.)",
            "evaluation_method": "End-to-end task performance comparison showing BM25 vs Sentence Transformer vs DPR retrieval approaches on MFP accuracy.",
            "evaluation_results": "BM25 achieved 23.2% end-to-end accuracy on the MFP task versus 13.1% (Sentence Transformer) and 18.0% (DPR), and contributed to improved agent performance when combined with scholarly evidence retrieval.",
            "strengths": "Simple, robust lexical matching that better handles domain-specific molecule names and outperforms evaluated dense retrievers on these tasks.",
            "limitations": "Lexical match may miss semantically related but lexically different demonstrations; performance tied to quality/formatting of demonstration passages.",
            "failure_cases": "When molecules are referred to with synonyms or alternate nomenclature not present in training passages, BM25 may fail to retrieve the most relevant demonstrations.",
            "uuid": "e9614.2",
            "source_info": {
                "paper_title": "FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "CoT Role-play (Scientist/Reviewer)",
            "name_full": "Chain-of-Thought Role-Play with Scientist and Reviewer LLMs",
            "brief_description": "A two-role Chain-of-Thought prompting scheme where a 'Scientist' LLM generates multiple hypotheses from retrieved evidence and demonstrations, and a 'Reviewer' LLM evaluates and selects the best hypothesis to improve hypothesis quality and provide an internal critique.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "Scientist/Reviewer role-play (implemented with foundation LLMs such as LLaMA3-8B-instruct, GPT-3.5-turbo, or Gemini1.5-Pro)",
            "model_description": "A structured CoT method: Scientist receives FOODPUZZLE input, retrieved scholarly evidence, and demonstrations then produces three candidate hypotheses; Reviewer receives inputs plus the candidates and selects or rejects hypotheses based on argument quality; selected hypothesis returned as final output.",
            "model_size": "8B for LLaMA3-8B-instruct when used in experiments; other model sizes not specified",
            "input_corpus_description": "Same retrieval inputs as the RAG pipeline: BM25 demonstrations from FOODPUZZLE training passages and offline scholarly evidence per molecule/food from PubMed/arXiv search results.",
            "input_corpus_size": null,
            "topic_query_description": "Produce and evaluate candidate hypotheses that explain the mapping between molecules and food sources or predict missing molecules for a food item.",
            "distillation_method": "Chain-of-Thought role-play: internal multi-hypothesis generation followed by critical evaluation (Scientist proposes 3; Reviewer selects best) to synthesize evidence into a single grounded hypothesis.",
            "output_type": "Selected hypothesis with reviewer justification and attached evidence summaries.",
            "output_example": "Scientist: Hypothesis A/B/C (three candidate mappings); Reviewer: selects Hypothesis B with rationale referencing evidence snippets. (No concrete textual examples given in the paper.)",
            "evaluation_method": "Measured by downstream task performance (MFP accuracy, MPC F1) and manual error analysis of sampled failures.",
            "evaluation_results": "Role-play approach within the Scientific Agent contributed to the highest reported MFP accuracy (35.5% with LLaMA3-8B-instruct agent) and MPC F1 (0.374), indicating effectiveness compared to single-pass generation baselines.",
            "strengths": "Provides an explicit internal selection/critique mechanism that can reduce low-quality hypotheses and increases interpretability of decisions.",
            "limitations": "Reviewer may still accept hallucinated or poorly grounded hypotheses if evidence is ambiguous; adds compute overhead (multiple generations per prediction).",
            "failure_cases": "Reviewer sometimes selects hypotheses based on flawed evidence interpretation; chain-of-thought can still produce ungrounded reasoning leading to epistemic hallucinations.",
            "uuid": "e9614.3",
            "source_info": {
                "paper_title": "FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledgeintensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Retrieval-augmented language model pre-training",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_language_model_pretraining"
        },
        {
            "paper_title": "In-context retrieval-augmented language models",
            "rating": 2,
            "sanitized_title": "incontext_retrievalaugmented_language_models"
        },
        {
            "paper_title": "Atlas: few-shot learning with retrieval augmented language models",
            "rating": 2,
            "sanitized_title": "atlas_fewshot_learning_with_retrieval_augmented_language_models"
        },
        {
            "paper_title": "Active retrieval augmented generation",
            "rating": 2,
            "sanitized_title": "active_retrieval_augmented_generation"
        },
        {
            "paper_title": "Improving language models by retrieving from trillions of tokens",
            "rating": 2,
            "sanitized_title": "improving_language_models_by_retrieving_from_trillions_of_tokens"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Augmenting large language models with chemistry tools",
            "rating": 1,
            "sanitized_title": "augmenting_large_language_models_with_chemistry_tools"
        },
        {
            "paper_title": "Bioplanner: Automatic evaluation of llms on protocol planning in biology",
            "rating": 1,
            "sanitized_title": "bioplanner_automatic_evaluation_of_llms_on_protocol_planning_in_biology"
        }
    ],
    "cost": 0.0133425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>FOODPUZZLE: Developing Large Language Model Agents as Flavor Scientists</p>
<p>Tenghao Huang tenghaoh@usc.edu 
University of Southern California</p>
<p>Donghee Lee 
University of California
Davis</p>
<p>John Sweeney 
Jiatong Shi 
Emily Steliotes 
University of California
Davis</p>
<p>Matthew Lange 
University of California
Davis</p>
<p>Jonathan May 
University of Southern California</p>
<p>Muhao Chen 
University of California
Davis</p>
<p>FOODPUZZLE: Developing Large Language Model Agents as Flavor Scientists
7CA95B4DF13989D8B0ECA5794290D436
Flavor development in the food industry is increasingly challenged by the need for rapid innovation and precise flavor profile creation.Traditional flavor research methods typically rely on iterative, subjective testing, which lacks the efficiency and scalability required for modern demands.This paper presents three contributions to address the challenges.Firstly, we define a new problem domain for scientific agents in flavor science, conceptualized as the generation of hypotheses for flavor profile sourcing and understanding.To facilitate research in this area, we introduce the FOODPUZZLE, a challenging benchmark consisting of 978 food items and 1,766 flavor molecules profiles.We propose a novel Scientific Agent approach, integrating in-context learning and retrieval augmented techniques to generate grounded hypotheses in the domain of food science.Experimental results indicate that our model significantly surpasses traditional methods in flavor profile prediction tasks, demonstrating its potential to transform flavor development practices.</p>
<p>Introduction</p>
<p>The burgeoning demand for novel and appealing flavors in the food industry necessitates expedited innovation cycles without compromising on quality (Hofmann et al., 2018).Traditionally, the process of ingredient sourcing and evaluation for flavor development has been reliant on manual assessment and human expertise.In addition to being laborintensive, and subjective, this process also relies on time-consuming iterations, hindering the pace of flavor development (Engeseth and Ac Pangan, 2018;Hofmann et al., 2018;Patel and Patel, 2019).Additionally, complex flavor interactions, together with the vast array of available ingredients, present formidable challenges for food scientists in optimizing flavor profiles (Hamilton and Lahne, 2020).</p>
<p>Figure 1: Flavor is determined by diverse flavor molecules.Sourcing and identifying these molecules from various foods is a time-consuming and resourceintensive task for food scientists.Understanding these connections is crucial in flavor science for developing and enhancing food products to ensure appealing taste experiences for consumers.In this work, we explore how LLMs can assist in this process.</p>
<p>Recent advancements in Large Language Models (LLMs) have revolutionized traditional scientific methodologies across a broad spectrum of disciplines.In fields ranging from biology and chemistry to physics and material sciences, researchers have employed LLM technologies to analyze complex datasets, uncover hidden patterns, and narrow down the search spaces of scientific problems (Horawalavithana et al., 2022;Singhal et al., 2022;O'Donoghue et al., 2023;Song et al., 2023;Hong et al., 2024).This trend highlights the transformative potential of LLMs in reshaping traditional paradigms and catalyzing breakthroughs.This paper explores how the integration of LLMs arXiv:2409.12832v3[cs.CL] 7 Oct 2024 can streamline and enhance the process of evaluating ingredient sources for their flavor-giving potential.However, the application of LLMs in ingredient sourcing and flavor formulation is hindered by a significant barrier: the absence of high-quality, domain-specific datasets.Most LLMs are trained on generic data from the Web, which often lacks the detailed, specialized knowledge required for accurate flavor profile understanding and prediction.</p>
<p>In this paper, we delineate three primary contributions that collectively advance the application of LLMs in the flavor science.Our first contribution is to formulate flavor profile sourcing and understanding as a LLM agent problem.Flavor sourcing and understanding are inherently complex tasks, involving an open-ended exploration of vast ingredient arrays to pinpoint components that deliver desired profiles.Scientific agents excel at navigating these complexities by leveraging their capacity to identify relevant evidence and reason within large context spaces effectively.When integrated with external knowledge sources, these agents can perform the labor-intensive tasks of flavor sourcing and understanding with enhanced efficiency and precision.</p>
<p>Our second contribution is the creation of the FOODPUZZLE dataset.To gather data for our study on flavor profile prediction and understanding, we turn to FlavorDB (Garg et al., 2018).Fla-vorDB is a comprehensive database that includes detailed information on 25,595 flavor molecules, of which 1,766 are specifically noted for their presence in 978 natural ingredients.Figure 2 presents an overview of the structure of our constructed dataset.We propose two tasks that mirror realworld scientific study.The first task is Molecular Food Prediction (MFP), which challenges the model to predict possible food items based on a given set of flavor molecules.The second task is completing the molecular profile (MPC) which requires the identification of flavor molecules likely present in a specified food item.</p>
<p>As we realize the challenge of existing approaches for solving the FoodPuzzle task, our third contribution is the development of a comprehensive scientific agent method for proposing grounded hypotheses of flavor profile prediction tasks1 .By integrating in-context learning (ICL) and the Retrieval-Augmented Generation (RAG) approach, our method enhances the model's ability to produce accurate and reliable results while providing traceable justifications for its predictions, thus addressing key domain-specific knowledge and improving reliability in flavor development applications.Experimental results show that our proposed scientific agent method significantly outperforms baseline methods.</p>
<p>Our work represents a pilot study to design scientific agents for flavor profile sourcing tasks.We set a new benchmark for leveraging advanced AI technologies in the domain of flavor science.Our methodology provides clear, traceable insights into how flavor profiles are derived, thus facilitating greater acceptance and use of LLMs in practical food science applications.</p>
<p>Related Works</p>
<p>Retrieval-Augmented Generations.Retrieval-Augmented Language Models (RALMs) enhance the reasoning process by incorporating external knowledge sources, thus providing additional contextual information (Chen et al., 2017;Lee et al., 2019b;Karpukhin et al., 2020;Borgeaud et al., 2022;Zhang, 2024).Early works such as Retrieval-Augmented Generation (RAG; Lewis et al. 2020) and Retrieval-Augmented Language Model pretraining (REALM; Guu et al. 2020) have demonstrated significant improvements in contextual understanding and question-answering by fetching relevant documents from large corpora.Recent advancements in RALMs have significantly enhanced language models by integrating external knowledge dynamically and efficiently, improving performance with minimal retraining (Ram et al. 2023;Jiang et al. 2023;Izacard et al. 2024).In contrast to conventional RAG approaches retrieving from a static index base, our method leverages scholarly articles from the internet and augments model reasoning with consolidated information about flavor science.This approach innovates upon conventional RAG systems and highlights the extendability of our Flavor Scientific Agent.</p>
<p>LLMs for Scientific Research.The application of large language models (LLMs) in scientific research has led to significant advancements across various disciplines.For example, domain-specific adaptations of BERT (Devlin et al., 2019) for biology and biomedical domains have demonstrated significant performance improvements in tasks such as protein classification and text mining, underscoring the value of tailored pretraining for scientific corpora (Lee et al. 2019a;Vig et al. 2021;Beltagy et al. 2019;Gu et al. 2021;Taylor et al. 2022).In the field of chemistry, LLMs have been tailored for tasks such as reaction prediction and molecule translation (Lu and Zhang 2022;Edwards et al. 2022;Sagawa and Kojima 2023).However, they face common challenges, including a dependency on large, high-quality scientific datasets for training, which are often difficult to collect, and a lack of interpretability in their predictions, making it challenging for chemists to trust and use the results effectively (Horawalavithana et al. 2022;Bran et al. 2023).Our method leverages retrieval augmented techniques to provide in-context learning at inference time, outperforming training-time methods.Our approach not only streamlines the process by generating and selecting the best hypothesis from multiple candidates but also enhances interpretability by providing explicit evidence of the rationales behind the outputs.</p>
<p>Task and Data</p>
<p>This section delineates the tasks and data designed to evaluate the efficacy of LLMs in predicting food items based on their molecular flavor profiles.These tasks are intended to assess the LLMs' capabilities in contextually-enhanced reasoning within the domain of molecular data.</p>
<p>Task Definition</p>
<p>Molecular Food Prediction (MFP).We define the task as learning the mapping function h which takes a set of molecules M = {m 1 , m 2 , . . ., m n } as input and outputs a corresponding food source F .This can be expressed as:
F = h(M ).
This formulation aims to predict food sources based solely on their molecular composition, without reliance on specific algorithms or systems.</p>
<p>Molecular Profile Completion (MPC).</p>
<p>The primary objective of the Molecular Profile Completion (MPC) task is to identify the missing molecules needed to complete the molecular profile of a given food item.The function g receives the known food item F , a partial set of its molecules M partial , and an integer n representing the expected number of missing molecules.It then outputs the set of missing molecules M missing .This can be mathematically represented as:
M missing = g(F, M partial , n).
In this task, the integer n is provided as part of the input, indicating the number of molecules that are missing.</p>
<p>Constructing the FOODPUZZLE Dataset</p>
<p>Data Collection.To gather data for our study on flavor molecule prediction and analysis, we crawl through the extensive collection of food entries of the the Flavor DB2 .From the collected data, we create three distinct stores of information.The first store contains profile information pertaining to each molecule (e.g., 2-Ethylpyrazine), detailing its properties (e.g., isotope atom count) and flavor characteristics (e.g., ordor).The second store focuses on information related to each food item, categorizing them by type and listing associated molecular compositions.Finally, we construct an association matrix that mapped which foods were associated with which molecules, represent as pairs of food and molecule identifiers.This structured approach ensures that our dataset was both comprehensive and well-organized, facilitating subsequent analysis and modeling efforts.The resulting FOODPUZZLE dataset maps 978 foods to 1766 flavor molecules.For the purpose of fine-tuning on the baseline, we split the dataset into train/dev/test sets by 80%/10%/10%.Data Analysis.Our dataset includes a variety of food entities, each associated with a unique set of molecules.Figure 3 reveals that the majority of food entities are associated with a relatively small number of molecules, while a few entities have a significantly higher number of associated molecules.We employed feature encoding to represent each food item by its molecular composition, resulting in each item being characterized by a 1766-dimensional vector.Principal Component Analysis (PCA) of the encoded vectors, illus-trated in Figure 4, reveals distinct clustering for categories such as cheese, whisky, oil, and fruit.These clusters indicate that food entities with analogous molecular compositions tend to aggregate, suggesting inherent patterns in flavor molecules that could potentially be predicted using machine learning techniques.</p>
<p>Evaluation Protocols Evaluation of Molecular Food Prediction (MFP).</p>
<p>The objective of MFP is to predict the category of a food item based on its molecular composition.There are 21 categories as shown in Table 1.To measure the correctness of predicted food category, we provide the groundtruth answer F * , which is sourced from FlavorDB.At evaluation time our models predict a food category F but this is done as a free text prediction; no ontology is provided.We thus compare F and F * with a language model3 to assess whether the two are semantically equivalent.In this way we are able to match, e.g., "Alcohol" and "Alcoholic Beverages", which are semantically equivalent but of different formats.We report accuracy as the metric for this task.</p>
<p>Evaluation of Molecular Profile Completion (MPC)</p>
<p>. MPC assesses the model's proficiency in predicting missing flavor molecules.To measure the correctness of predicted missing molecules, we provide the groundtruth answer set M * .At evaluation time our models predict a set of molecules M .We thus calculate the F1 score using f ( M )</p>
<p>and f (M * ), where f extracts the set of functional groups of the provided set of molecules.Mathematically, the F1 score is calculated as below:
F 1 = 2  |f ( M )  f (M * )| |f ( M )| + |f (M * )| .
By focusing on functional groups, this evaluation protocol leverages the fact that chemicals sharing functional groups tend to exhibit similar properties (Salmina et al., 2016).In this way we are able to prioritize chemical functionality over strict structural similarity, which ensures predictive relevance and streamline the evaluation process.</p>
<p>Evaluated Methods</p>
<p>In this section, we explore selected baseline models ( 4.1) and delve into the implementation details of our scientific agent designed for flavor development ( 4.2).</p>
<p>Baseline Methods</p>
<p>We evaluate the following baseline models to establish benchmarks for assessing the capabilities of our proposed scientific agent in the domain of flavor development.Our evaluation is organized into three distinct categories of models.</p>
<p>Foundation LLMs.We evaluate ChatGPT-3.5turbo 4, Gemini-1.5-Pro,and Llama-3-instruct-8B (Touvron et al., 2023) using zero-shot inference.</p>
<p>Though being general-purpose and not specifically tailored to food science, these models are renowned for their robust performance across a broad spectrum of natural language processing tasks.The prompts used for zero-shot inference were carefully designed to ensure clarity and relevance to the tasks.</p>
<p>Domain-specific LLMs.We recognize that molecule names and related chemical knowledge can be unfamiliar to general-purpose LLMs.Hence, we also consider models trained on corpora from related domains as baselines.These include MolT5, a model pretrained on a vast amount of unlabeled natural language text and molecule strings (Edwards et al., 2022), and BioT5, which is pretrained on structured and unstructured biology corpora for generative tasks involving molecular entities (Pei et al., 2023).Since these two models are not instruction-tuned for conversational interaction, we follow Wang et al. (2022) to constrain the vocabulary set at decoding time and fine-tune these models on the training set for the task.Both BioT5 and MolT5 are fine-tuned on our train set.We use 4 Nvidia A10G GPUs for our experiments.The models were fine-tuned with a batch size of 16 for the MFP task and 8 for the MPC 4 April 29th 2024 version.task, using the AdamW optimizer with a learning rate of 0.00005,  1 = 0.9, and  2 = 0.999.Each model was trained for 20 epochs, with early stopping applied after 3 epochs based on validation loss.</p>
<p>In-context Learning.We also consider in-context learning techniques for our tasks.However, molecule names may be out-of-context information for LLMs due to limited exposure during pretraining, which can impair the performance of the agents.To enhance reasoning capabilities, we employ a retriever to selectively identify relevant data points from the labeled training set, providing supplementary information.For the implementation of this retriever, we evaluate several approaches: BM25 (Robertson and Walker, 1994), Sentence Transformer (Reimers and Gurevych, 2019), and Dense Passage Retrieval (DPR; Karpukhin et al. 2020).Our empirical results indicate that BM25 significantly outperforms dense retrieval methods, achieving a 23.2% accuracy, compared to 13.1% for the Sentence Transformer and 18.0% for DPR in end-to-end performances on the MFC task.This superior performance of BM25 aligns with our expectations, given that dense retrievers are pretrained and fine-tuned on generic natural language corpora, which may not effectively generalize to retrieve domain-specific data.</p>
<p>For both tasks, the BM25 retriever retrieves the top k most similar demonstrations to the query from a dense corpus index, where each passage is structured in the format: "Food: F  .Molecules: M  ".These retrieved demonstrations are then presented to the model as demonstrations, which are crucial for generating grounded and evidencebased output.</p>
<p>Architecture of the Scientific Agent</p>
<p>We propose an advanced hybrid methodology that integrates Retrieval-Augmented Generation (RAG) with online scholarly sources and in-context learning, utilizing pertinent demonstrations.This approach is specifically tailored to surmount the challenges inherent in general-purpose LLMs that lack specialized knowledge in the domain of food science.The architecture of our scientific agent is designed to mirror the investigative processes typical of human scientists.This alignment not only enhances the agent's ability to generate scientifically robust hypotheses but also improves the explicability of errors, thereby contributing to its operational Starting Point Identification.Similarly to solving other puzzles, it is important to identify suitable starting points in our flavor profile sourcing task.Consequently, for the MFP, given a list of flavor molecules, denoted as M , we focus on selecting molecules that are pivotal for our analysis.We calculate the information entropy for each molecule in M based on the frequency of appearances in the train set and select up to 10 molecules with the lowest information entropy to identify an informative subset of molecules.These selected molecules M * then serve as starting points for subsequent investigations.</p>
<p>Information Collection.One key challenge in employing general-purpose LLMs in specialized fields like food science is their inherent limitation in domain specificity.While these models are trained on massive corpora and varied task datasets that provide a wide range of general knowledge, they often lack the detailed, specific insights needed to tackle complex, domain-focused issues effectively.This shortfall is particularly noticeable in food science, where the models struggle to accurately comprehend or predict the subtle nuances of flavor profiles and the interactions among different compounds.Though in-context learning introduces relevant demonstrations to the agent, it still predominantly relies on the parametric knowledge of LLMs to figure out the relationship between demonstrations and test instances.This inherent limitation can result in outputs that may not sufficiently use in-context information.</p>
<p>To address this gap, Out-of-Domain Evidence is sourced from scholarly articles and internet blogs in addition to the in-context demonstrations.Specifically, for the MFP task, the process begins by iteratively querying each molecule m  M * .The query is formulated as: "What are the common food sources that could contain m?" Similarly, for the MPC task, the process begins by querying each food item F , with the corresponding query: "What molecules are present in F ?" We then use the Google Custom Search API to identify academic articles that detail which plants, animals, or other organisms naturally produce these molecules.To ensure the reliability of the information retrieved, the search is confined to scientific repositories such as PubMed5 and arXiv6 .The scientific agent gathers the sourced articles, storing the information as evidence.To enhance efficiency during inference, the Google Custom Search is executed offline, and the results are stored locally.Consequently, during inference, the system simply retrieves the relevant information from local storage.</p>
<p>Hypothesis Generation.The final phase is hypothesis generation.We realize that it is possible for the collected evidences to sometimes contradict each other, pointing to different answers.Leveraging the Chain-of-Thought (CoT) approach (Wei et al., 2022), we adopt a role-play based agent framework to process sourced evidences and generate the final hypothesis.Specifically, we initialize two role-based models: the Scientist that proposes three hypotheses and the Reviewer that evaluates the argument quality and selects the best hypothesis7 .In the MFP task, we retrieve locally stored evidence from FlavorDB related to up to ten key molecules identified during the initial point identification phase.This evidence, along with relevant data points, is then provided to the Scientist model.For the MPC task, as there is no starting point identification process, we retrieve locally evidence re- lated to the food item directly from FlavorDB, and this evidence is provided to the Scientist model.Also, as implemented in the in-context learning baseline, a BM25 retriever is used to select the three most similar demonstrations, which are also given to the Scientist model.The Scientist, therefore, receives data input from FOODPUZZLE, the retrieved evidence, and demonstrations, and uses these to generate three hypotheses.The Reviewer model then receives the data input, samples, and the three hypotheses generated by the Scientist.After reviewing the hypotheses, the Reviewer might reject or select the most suitable one.Finally, the scientific model returns this best hypothesis as the final prediction.</p>
<p>Experiments</p>
<p>In this section, we report and analyze the benchmarking results ( 5.1).We randomly select 50 prediction errors and manually inspect them.We present a comprehensive error analysis that outlines key categories of errors as insights for future model refinement ( 5.2).</p>
<p>Main Results</p>
<p>Table 2 presents the combined results for model accuracies in Molecule Food Prediction (MFP) and F1 scores for Molecule Profile Completion (MPC).Domain-Specific LLMs such as MolT5 and Bio-T5 exhibit lower performance in both MFP accuracy and MPC F1 score compared to other models.The limitation of these models can be attributed to two reasons.On the one hand, their specialization in molecular and biological text does not necessarily translate to broader applications in food science.However, small parameter sizes and lack of instruction tuning might also limit model performance.</p>
<p>In the zero-shot scenario, foundation LLMs like LLAMA3-8B-Instruct, Gemini1.5 Pro, and GPT-3.5 Turbo show varying degrees of effectiveness.LLAMA3-8B-Instruct achieves an MFP accuracy of 15.0% and an F1 score of 0.292, indicating moderate performance.Gemini1.5 Pro performs better with an MFP accuracy of 19.0% and an F1 score of 0.340, demonstrating its relative strength in both food prediction and molecule profile completion.GPT-3.5 Turbo balances its performance with an MFP accuracy of 12.2% and an F1 score of 0.327.These results highlight the challenging nature of our tasks and suggest that while general-purpose models can achieve reasonable performance, they still lack the domain-specific knowledge required for food science tasks.</p>
<p>Leveraging in-context learning techniques, LLMs demonstrate notable improvements over the foundation and domain-specific LLMs.The enhanced performance of these models underscores the importance of in-context learning in providing relevant additional information and improving prediction accuracy.Furthermore, these results highlight the effectiveness of our FOODPUZZLE dataset in enabling models to take advantage of the relevant context for better predictions.</p>
<p>The Autonomous Scientist Agent exhibits superior performance in both Molecular Food Prediction accuracy and F1 scores for Molecular Profile Completion, compared to other model categories.The LLaMA3-8b-instruct model within the agent achieves an MFP accuracy of 35.5%, outperforming the best in-context learning model (Gemini1.5Pro with 34.6%) and the highest foundation LLM (Gemini1.5Pro with 19.0%).Furthermore, the agent shows robust F1 scores, with LLaMA3-8binstruct at 0.374, Gemini1.5 Pro at 0.333, and GPT-3.5 Turbo at 0.374, indicating its balanced strength in both tasks.This approach effectively addresses the challenges faced by general-purpose LLMs that lack domain-specific knowledge in food science.By sourcing and consolidating domain-specific evidence and employing a structured approach to hypothesis generation, the agent enhances prediction accuracy and molecule profile completion.</p>
<p>Error Analysis</p>
<p>Next, we define a taxonomy of error types discerned among evaluated scientific agents, based on an analysis of fifty randomly selected prediction errors.It is possible that a single example may exhibit multiple error types concurrently.To preclude redundancy, only the most salient error per example is accounted for in this analysis.</p>
<p>Inappropriate Initialization of Search Space (32%).When identifying key molecules to understand flavor profiles, the autonomous scientific agent often struggle due to limited domain-specific knowledge.For example, while domain experts can focus on molecules such as "hydrogen sulfide" and "diethyl sulfide" for their significant roles in foods like eggs, LLMs tend to suggest more generic molecules such as thiamine and betaine.These are common across various foods and can lead the agent to inaccurate, fruit-biased interpretations by narrowing the scope of analysis.This mismatch highlights the importance of incorporating specific and expert-driven insights into the training and inference of LLMs used in flavor science.</p>
<p>Epistemic Hallucination (26%).</p>
<p>The phenomenon of hallucination was observed across the evaluated LLMs.During processes intended to enhance model reasoning through chain-of-thought techniques, the output generated by the agent often lacks grounding in factual evidence.For example, when asked to identify flavor molecules for eggs, an LLM might list molecules commonly found in eggs rather than those that could recreate the egg flavor profile.This distinction is crucial: The goal is to identify molecules that can mimic or repli-cate flavors, not just those that occur naturally in the food item.Effective prompting can mitigate this issue.Clearly specifying that the task involves identifying molecules capable of replicating specific flavors, rather than listing inherent molecules, can direct the LLMs to focus correctly.Additionally, providing explicit examples and context in the prompts can further align the LLM responses with the intended task.</p>
<p>Wrong Interpretation of Online Sources (20%).Another considerable source arises from the misinterpretation of scholarly articles during the process of sourcing evidence online.When models asked to perform aspect-summarization toward input queries, they exhibits a tendency to "force" answers where direct evidence might be lacking or ambiguous.This compulsion to generate conclusions can lead to erroneous or overly speculative assertions.For instance, consider a scenario where a scholarly article states that ethanethiol is characterized by more roasted and toasted notes, and may exhibit a coffee-like character.LLMs, in its attempt to generate actionable insights, might erroneously infer and assert that "coffee contains Ethanethiol as one of its flavor molecules."Such a statement is a speculative leap rather than a factually supported conclusion, reflecting the model's predisposition towards generating responses even in the absence of clear evidence.</p>
<p>Future Opportunities</p>
<p>Integrating autonomous flavor scientists and FOOD-PUZZLE into the R&amp;D pipeline offers significant advancements in flavor science, sensory science, and food product development.In this section we discuss how collaboration with wet lab and sensory scientists to evaluate chemical, biological, and sensory properties of flavor compounds will enhance model accuracy and impact.</p>
<p>Chemical Synthesis and Analysis.Using analytical instruments like LC-MS and GC-MS to detect and quantify flavor molecules in food samples allows for verification of structure and purity, validating flavor profile hypotheses.Synthetic chemistry labs can synthesize predicted candidate flavor molecules, enabling verification of their organoleptic properties.Sensory labs can manage human sensory panels to evaluate synthesized flavors, finetuning AI model predictions to align with human perceptions of flavor quality and intensity.</p>
<p>Biological Analysis.Biological laboratories can perform bioassays and in vitro testing to assess the safety and efficacy of flavor compounds.Highthroughput screening (HTS) techniques can rapidly test large libraries of flavor molecules, generating extensive datasets to enhance AI predictions.</p>
<p>Conclusion</p>
<p>This paper underscores the transformative potential of LLM agents in flavor science, particularly through the development of an scientific agent tailored for flavor profile sourcing.Key contributions include the formulation of flavor profile sourcing and understanding as a LLM agents task, the creation of the FOODPUZZLE dataset from FlavorDB, and the creation of a comprehensive scientific agent pipeline that can perform the labor-intensive tasks of flavor sourcing and understanding with enhanced efficiency and precision.Our findings demonstrate that our method outperforms traditional models and provides traceable and reliable predictions.This work not only sets a new benchmark for the application of AI in flavor science but also paves the way for further technological advancements.</p>
<p>Figure 2 :
2
Figure 2: A high level overview of the FOODPUZZLE data hierarchy</p>
<p>Figure 3 :
3
Figure 3: Distribution of the number of flavor molecules in the FOODPUZZLE dataset.</p>
<p>Figure 4 :
4
Figure 4: PCA visualization illustrating clustering of food entities based on molecular profiles.</p>
<p>Figure 5 :
5
Figure 5: Architecture of the proposed Scientific Agent</p>
<p>Table 1 :
1
Classification of food items into macro categories in the dataset.
CategoriesCerealFruitEssential OilPlantBakeryFungusSeedDishSpiceFlowerNut and Seed BeverageAnimal ProductVegetablePlant DerivativeAdditiveMeatFish and Seafood Cereal CropDairyHerb</p>
<p>Table 2 :
2
Combined Results for Model Accuracies in Molecule Food Prediction and F1 Scores for Molecule Profile Completion
CategoryModelMFP accuracy(%) MPC F1 scoreDomain-Specific LLMsMolT5 (Edwards et al., 2022) Bio-T5 (Pei et al., 2023)9.8 16.60.144 0.278LLaMA3-8B-instruct15.00.292Zero-shotGemini1.5 Pro19.00.340GPT-3.5 Turbo12.20.327LLaMA3-8B-instruct31.60.349In-context LearningGemini1.5 Pro34.60.373GPT-3.5 Turbo23.20.360Scientist AgentLLaMA3-8B-instruct35.50.374Gemini1.5 Pro34.20.333GPT-3.5 Turbo26.90.374
Our codes and the dataset will be publicly released upon acceptance.
The address of FlavorDB endpointhttps://cosylab. iiitd.edu.in/flavordb
In this work we use GPT-3.5-turbo; prompt details are provided in the Appendix B.
https://pubmed.ncbi.nlm.nih.gov
https://arxiv.org
We include prompt examples in Appendix B</p>
<p>SciB-ERT: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, 10.18653/v1/D19-1371Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Improving language models by retrieving from trillions of tokens. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las, Aurelia Casas, Jacob Guy, Roman Menick, Tom Ring, Saffron Hennigan, Loren Huang, Chris Maggiore, Albin Jones, Andy Cassirer, Michela Brock, Geoffrey Paganini, Oriol Irving, Simon Vinyals, Karen Osindero, Jack Simonyan, Erich Rae, Laurent Elsen, Sifre, Proceedings of the 39th International Conference on Machine Learning. the 39th International Conference on Machine LearningPMLR2022162</p>
<p>Augmenting large language models with chemistry tools. Sam Andres M Bran, Oliver Cox, Carlo Schilter, Andrew Baldassari, Philippe White, Schwaller, 10.18653/v1/P17-1171Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsAdam Fisch, Jason Weston, and Antoine Bordes; Vancouver, CanadaAssociation for Computational Linguistics2023. 20171NeurIPS 2023 AI for Science Workshop. Danqi Chen</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Translation between molecules and natural language. Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, Heng Ji, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Current context on chocolate flavor development -a review. J Nicki, Marlon Engeseth, Ac Fernando, Pangan, Current Opinion in Food Science. 212018</p>
<p>Flavordb: a database of flavor molecules. Neelansh Garg, Apuroop Sethupathy, Rudraksh Tuwani, Rakhi Nk, Shubham Dokania, Arvind Iyer, Ayushi Gupta, Shubhra Agrawal, Navjot Singh, Shubham Shukla, Nucleic acids research. 46D12018</p>
<p>Domain-specific language model pretraining for biomedical natural language processing. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, Hoifung Poon, 10.1145/3458754ACM Trans. Comput. Healthcare. 132021</p>
<p>Retrieval augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Mingwei Chang, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningPMLR2020119</p>
<p>Fast and automated sensory analysis: Using natural language processing for descriptive lexicon development. M Leah, Jacob Hamilton, Lahne, Food Qual. Prefer. 831039262020</p>
<p>Current status and future perspectives in flavor research: Highlights of the 11th wartburg symposium on flavor chemistry &amp; biology. Thomas Hofmann, Dietmar Krautwurst, Peter Schieberle, J. Agric. Food Chem. 66102018</p>
<p>Robin Cosbey, Maria Glenski, and Svitlana Volkova. 2022. Foundation models of scientific knowledge for chemistry: Opportunities, challenges and lessons learned. Sirui Hong, Yizhang Lin, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Lingyao Zhang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang, Xiangtao Lu, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zongze Xu, Chenglin Wu, Li Zhang, Min Yang, Xiawu Zheng ; Shivam Sharma, Scott Howland, Megha Subramanian, Scott Vasquez, 10.18653/v1/2022.bigscience-1.12ArXiv, abs/2402.18679Proceedings of BigScience Episode #5 -Workshop on Challenges &amp; Perspectives in Creating Large Language Models. BigScience Episode #5 -Workshop on Challenges &amp; Perspectives in Creating Large Language ModelsAssociation for Computational Linguistics2024Sameera Horawalavithana, Ellyn Ayton,</p>
<p>Atlas: few-shot learning with retrieval augmented language models. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, Edouard Grave, J. Mach. Learn. Res. 1242024</p>
<p>Active retrieval augmented generation. Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig, 10.18653/v1/2023.emnlp-main.495Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Dense passage retrieval for opendomain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, 10.18653/v1/2020.emnlp-main.550Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, 10.1093/bioinformatics/btz682Bioinformatics. 3642019a</p>
<p>Latent retrieval for weakly supervised open domain question answering. Kenton Lee, Ming-Wei Chang, Kristina Toutanova, 10.18653/v1/P19-1612Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019b</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-Tau Yih, Tim Rocktschel, Sebastian Riedel, Douwe Kiela, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>Unified deep learning model for multitask reaction predictions with explanation. Jieyu Lu, Yingkai Zhang, Journal of Chemical Information and Modeling. 2022</p>
<p>Bioplanner: Automatic evaluation of llms on protocol planning in biology. Aleksandar Odhran O'donoghue, John Shtedritski, Ralph Ginger, Ali Abboud, Samuel Ghareeb, Rodriques, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Flavor manufacturing and selection criteria for functional food and nutraceuticals industries. Jayvadan Patel, Anita Patel, Flavor Development for Functional Foods and Nutraceuticals. CRC Press2019</p>
<p>BioT5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations. Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, Rui Yan, 10.18653/v1/2023.emnlp-main.70Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>In-context retrieval-augmented language models. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham, 10.1162/tacl_a_00605Transactions of the Association for Computational Linguistics. 112023</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2019</p>
<p>Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. S E Robertson, S Walker, Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '94. the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '94Berlin, HeidelbergSpringer-Verlag1994</p>
<p>Re-actiont5: a large-scale pre-trained model towards application of limited reaction data. Tatsuya Sagawa, Ryosuke Kojima, arXiv:2311.067082023Preprint</p>
<p>Extended functional groups (efg): An efficient set for chemical characterization and structureactivity relationship studies of chemical compounds. Elena S Salmina, Norbert Haider, Igor V Tetko, 10.3390/molecules21010001Molecules. 2112016</p>
<p>Alan Karthikesalingam, and Vivek Natarajan. 2022. Large language models encode clinical knowledge. K Singhal, Shekoofeh Azizi, Tao Tu, Said Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Kumar Tanwani, Heather J Cole-Lewis, Stephen J Pfohl, P A Payne, Martin G Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, P A Mansfield, Blaise Agera Y Arcas, Dale R Webster, Greg S Corrado, Yossi Matias, Katherine Hui, -Ling Chou, Juraj Gottweis, Nenad Tomaev, Yun Liu, Alvin Rajkomar, Jolle K Barral, Christopher Semturs, Nature. 620</p>
<p>MatSci-NLP: Evaluating scientific language models on materials science language tasks using text-to-schema modeling. Yu Song, Santiago Miret, Bang Liu, 10.18653/v1/2023.acl-long.201Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, arXiv:2211.09085Galactica: A large language model for science. 2022Preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov ; Zheng Yan, Iliyan Zarov, Yuchen Zhang, arXiv:2307.09288Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien RodriguezPreprintand Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models</p>
<p>{BERT}ology meets biology: Interpreting attention in protein language models. Jesse Vig, Ali Madani, Lav R Varshney, Caiming Xiong, Nazneen Rajani, International Conference on Learning Representations. 2021</p>
<p>Robust (controlled) table-to-text generation with structure-aware equivariance learning. Fei Wang, Zhewei Xu, Pedro Szekely, Muhao Chen, 10.18653/v1/2022.naacl-main.371Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Guided profile generation improves personalization with llms. Jiarui Zhang, arXiv:2409.130932024Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4405 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4405</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4405</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-282210220</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.17263v1.pdf" target="_blank">TaxoAlign: Scholarly Taxonomy Generation Using Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Taxonomies play a crucial role in helping researchers structure and navigate knowledge in a hierarchical manner. They also form an important part in the creation of comprehensive literature surveys. The existing approaches to automatic survey generation do not compare the structure of the generated surveys with those written by human experts. To address this gap, we present our own method for automated taxonomy creation that can bridge the gap between human-generated and automatically-created taxonomies. For this purpose, we create the CS-TaxoBench benchmark which consists of 460 taxonomies that have been extracted from human-written survey papers. We also include an additional test set of 80 taxonomies curated from conference survey papers. We propose TaxoAlign, a three-phase topic-based instruction-guided method for scholarly taxonomy generation. Additionally, we propose a stringent automated evaluation framework that measures the structural alignment and semantic coherence of automatically generated taxonomies in comparison to those created by human experts. We evaluate our method and various baselines on CS-TaxoBench, using both automated evaluation metrics and human evaluation studies. The results show that TaxoAlign consistently surpasses the baselines on nearly all metrics. The code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4405.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4405.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TAXOALIGN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TAXOALIGN: Scholarly Taxonomy Generation Using Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A three-stage LLM-based pipeline that constructs hierarchical taxonomy trees from a topic and a corpus of reference papers by (1) extracting 'knowledge slices' from each paper, (2) instruction‑tuning an LLM to verbalize a taxonomy, and (3) refining the taxonomy with a stronger reasoning LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TAXOALIGN</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>TAXOALIGN is a three-phase pipeline: (1) Knowledge Slice Creation — an LLM (e.g., Mistral-7B-Instruct, Meta-Llama-3-8B-Instruct) is prompted to extract short, topic-relevant segments ('knowledge slices') from each reference paper so that many papers can be represented within an LLM context window; (2) Taxonomy Verbalization — an instruction‑tuned LLM (finetuned via QLoRA on models such as Llama-3.1-Tülu-3-8B or SciLitLLM1.5-7B) is trained to produce concise, structured taxonomy trees (max depth three) from the assembled slices; (3) Taxonomy Refinement — a stronger closed-domain reasoning model (GPT-4o-mini in the paper) is prompted to check grounding, refine parent–child relations, expand nodes to improve coverage, and reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Knowledge-slice models: Mistral-7B-Instruct-v0.3, Meta-Llama-3-8B-Instruct; Verbalization (instruction-tuned): Llama-3.1-Tülu-3-8B, SciLitLLM1.5-7B (QLoRA finetuning); Refinement (reasoning): GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompt-based extraction via LLMs that identify and return concise, topic-relevant 'knowledge slices' per paper; preprocessing (Docling) used to parse PDFs and Semantic Scholar for references.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Hierarchical synthesis via instruction-tuned taxonomy verbalization producing tree-structured outputs, followed by a reasoning-based refinement step to align topology and semantics across slices; multi-stage concatenation/aggregation of slices into a taxonomy.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Experiments use the provided reference sets from CS-TAXOBENCH — on average ~131 reference papers per taxonomy (dataset of 460 taxonomies; additional testset of 80 taxonomies with ~71 refs each).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer Science survey literature (CS-TAXOBENCH: ACM Computing Surveys papers, 2020–2024); tested on conference survey papers (ACL*/IJCAI).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Hierarchical taxonomy trees (topic root + multi-level subtopics; max depth three).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Average degree score (structural similarity), level-order traversal (BLEU-2, ROUGE-L, BERTScore), Node Soft Recall (NSR), Node Entity Recall (NER, noun-phrase overlap), LLM-as-a-judge, and human annotator ratings (Likert).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>TAXOALIGN consistently outperforms listed baselines across nearly all metrics in this paper: it achieves an average-degree (∆) close to 1 (i.e., structural similarity to gold), higher level-order BLEU/ROUGE/BERTScore and higher Node Soft Recall than baselines, and better human/LLM judge ratings; qualitatively it produces more coherent, less verbose taxonomies and fewer hallucinated nodes than other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>AutoSurvey, STORM, Topic-only, Topic+Keyphrases, and ablations (TAXOALIGN w/o Verbalization/w/o Refinement).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Outperforms all baselines on most automated metrics and human evaluations; ablation shows knowledge-slices alone are strong (second-best) but verbalization + refinement reduce structural divergence and improve label quality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Key strengths are (1) knowledge-slice extraction which enables covering large reference sets within LLM context limits, (2) instruction-tuning (taxonomy verbalization) that helps produce tree-structured outputs that mirror human taxonomies, and (3) a final refinement stage with a stronger reasoning LLM to ground and correct relations. Knowledge-slices are essential; verbalization and refinement improve structure and label quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Remaining issues include hallucinated node labels, verbosity or repetition when generating directly from slices, factual errors in verbalized nodes, layer-wise exact-match failures, and inherent LLM limitations with long-context reasoning and factual consistency; dataset drawn from one journal (ACM CSUR) limits distributional diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Designed to scale across many cited papers by compressing each paper into a small number of knowledge slices (enabling handling of ~O(100) papers per topic); the paper reports that using stronger reasoning models for refinement (GPT-4o-mini) improves outcomes; no extensive model-size scaling curve is reported beyond ablations showing instruction-tuned models + refinement outperform naïve prompting approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TaxoAlign: Scholarly Taxonomy Generation Using Language Models', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4405.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4405.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoSurvey: Large language models can automatically write surveys</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based method for automatic survey (outline) generation that divides reference papers into groups, generates outlines for each group, and amalgamates these outlines into a single comprehensive outline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autosurvey: Large language models can automatically write surveys</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoSurvey (outline generation component)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The AutoSurvey outline-generation approach randomly partitions reference papers into groups, generates multiple intermediate outlines using an LLM per group, and then merges/amalgamates those outlines into a single comprehensive outline/survey structure.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>In the original AutoSurvey work (cited), large instruction-following LLMs are used; in this paper AutoSurvey is run as a baseline via prompting with GPT-4o-mini for comparisons (paper supplies references to the baseline in their experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Group-wise summarization of reference papers followed by aggregation; uses prompt-based LLM summarization of groups of papers.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Amalgamation of multiple group-level outlines into a final outline (flat/sectioned survey).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to the same reference sets provided by the authors for fair comparison (i.e., the per-topic reference sets in CS-TAXOBENCH; experiments used on-average ~131 papers per taxonomy).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer Science surveys (evaluated by the TAXOALIGN paper on CS-TAXOBENCH).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Outlines / survey structure (section/subsection outlines); not specifically hierarchical taxonomies but used as an outline baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Evaluated in this paper using average degree score (∆), level-order traversal BLEU-2 / ROUGE-L / BERTScore, Node Soft Recall, Node Entity Recall, LLM-as-a-judge and human annotation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>In the TAXOALIGN experiments AutoSurvey produced overly large and structurally divergent taxonomy-like outlines (∆ ≈ 4.47 in Table 2), low lexical overlap metrics (very low BLEU/ROUGE), and lower human/LLM judge scores compared to TAXOALIGN.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to TAXOALIGN and other baselines (STORM, Topic-only, Topic+Keyphrases).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Performed substantially worse than TAXOALIGN on structural similarity (much larger ∆ > 1), lower semantic/lexical overlap metrics, and lower human/LLM judge ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Random grouping and naive amalgamation can lead to verbose, poorly aligned outlines for taxonomy tasks; outlines produced by AutoSurvey are less structurally similar to human taxonomies on this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Tendency to produce overly large trees/outlines, higher hallucination risk, structural divergence from human taxonomies when applied to taxonomy generation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Method relies on partitioning papers into groups to fit LLM context windows; in the TAXOALIGN experiments this led to overly large/branchy outputs compared to the structured slicing+verbalization approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TaxoAlign: Scholarly Taxonomy Generation Using Language Models', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4405.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4405.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STORM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>STORM (Shao et al., 2024) — pre-writing simulated conversations + LLM outline generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline pipeline that generates draft outlines by prompting LLMs after a 'pre-writing' simulated conversation stage and then refines the outline using the topic plus simulated conversations and drafts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>STORM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>STORM's pre-writing stage simulates conversational research to explore a topic; an LLM produces an initial draft outline from the topic, and a final outline is produced by prompting an LLM with the topic, simulated conversations, and the draft outline (in the paper it is used as a baseline for taxonomy/outline generation).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Used as a prompting-based method with GPT-4o-mini in the TAXOALIGN comparisons; original STORM uses large instruction-following LLMs (Shao et al., 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompt-driven exploration (simulated conversations) and draft outline generation from prompts; not explicitly knowledge-slice extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Iterative outline refinement via multi-step prompting informed by simulated conversational context and draft outlines.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluated on the same CS-TAXOBENCH reference sets (paper ran STORM as a baseline with the dataset provided).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer Science survey literature (used here as a baseline on CS-TAXOBENCH).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Draft and final outlines / sectioned survey outputs (used as taxonomy-outline baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Measured with average degree score, level-order traversal BLEU/ROUGE/BERTScore, Node Soft Recall, Node Entity Recall, and LLM/human judgeings in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>In TAXOALIGN experiments STORM produced highly branched/large trees (∆ ≈ 6.15 in Table 2) and low lexical overlap / judge scores relative to TAXOALIGN.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to TAXOALIGN and other baseline methods in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Worse than TAXOALIGN across the majority of structural and semantic metrics in the CS-TAXOBENCH evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simulated-conversation-based outline generation can help explore topics, but without targeted document grounding (knowledge slices) it leads to structural and semantic divergence from human taxonomies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Produces overly large outlines, limited grounding to the cited documents unless reference content is incorporated effectively; susceptible to hallucination and low lexical alignment with human taxonomies.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>No explicit scaling curves reported; performance degraded into larger tree branching when naive prompting attempted to ingest many references without slicing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TaxoAlign: Scholarly Taxonomy Generation Using Language Models', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4405.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4405.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResearchArena</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Researcharena: Benchmarking large language models' ability to collect and organize information as research agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior effort cited by the paper that frames LLMs as research agents whose ability to collect and organize information across papers is benchmarked; relevant to automated survey/synthesis tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Researcharena: Benchmarking large language models' ability to collect and organize information as research agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Researcharena</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ResearchArena treats LLMs as autonomous/semiautonomous research agents and evaluates their end-to-end ability to collect, organize, and synthesize information across multiple documents into coherent artifacts (e.g., survey-style outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Agentic retrieval and multi-step prompting to collect information from multiple sources (as described in the referenced work; cited as related work here).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Agentic aggregation and organization of collected items into summaries/outlines; benchmarking focuses on collection + organization capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper (referenced work likely evaluates across varying corpus sizes).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General academic research agent benchmarking (cited as related work in survey generation literature).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Organized literature artifacts, outlines, or synthesized summaries generated by research-agent workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Benchmarked for collection and organization capabilities; TAXOALIGN cites this work as related work in automated survey generation but does not re-evaluate ResearchArena itself.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of prior work that focuses on LLM-based collection and organization of research materials; motivates taxonomy/survey automation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Paper does not detail ResearchArena results; referenced work investigates agentic limitations such as retrieval fidelity, consistency across steps, and grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not detailed in TAXOALIGN (refer to Researcharena original paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TaxoAlign: Scholarly Taxonomy Generation Using Language Models', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4405.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4405.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SURVEYX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SURVEYX: Academic survey automation via large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated survey-generation system cited as recent work that uses LLMs to produce academic surveys from a corpus of papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Surveyx: Academic survey automation via large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SURVEYX</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SURVEYX is cited as a system that automates survey creation using prompting and LLM-based generation across a corpus of academic papers; TAXOALIGN references it as part of the recent surge of end-to-end survey generation tools.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Likely prompt-based summarization and aggregation of multiple papers (paper cites SURVEYX among survey-generation approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Multi-document summarization / outline generation into survey-style artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Academic surveys (general), cited in the context of automated survey generation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Surveys / outlines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as part of recent literature on LLM-based automated survey generation; TAXOALIGN positions taxonomy generation as a complementary but distinct task.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not detailed in TAXOALIGN; general challenges include grounding, structure alignment with human-written surveys, and hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TaxoAlign: Scholarly Taxonomy Generation Using Language Models', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4405.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4405.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SURVEYFORGE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SURVEYFORGE: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent automated-survey system that emphasizes outline heuristics, memory-driven generation, and multi-dimensional evaluation of generated surveys using LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Surveyforge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SURVEYFORGE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SURVEYFORGE focuses on heuristics for outline construction, memory-driven (context-management) generation, and a multi-dimensional evaluation framework for automated survey writing with LLMs; cited as related work in the area of automated survey generation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Memory-driven multi-stage prompting and outline heuristics; aggregates content from many papers via staged generation.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Heuristic outline construction combined with memory/context-management to generate longer, structured surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Automated academic survey generation (general).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Survey outlines and full survey drafts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Multi-dimensional evaluation (as per SURVEYFORGE description in the reference).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Included as an example of end-to-end survey automation tools; TAXOALIGN differentiates itself by focusing on structured taxonomy trees and structural alignment evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TaxoAlign: Scholarly Taxonomy Generation Using Language Models', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4405.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4405.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ArxivDI-GESTables</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ArxivDI-GESTables: Synthesizing scientific literature into tables using language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that uses LLMs to synthesize information from scientific papers into structured tabular representations (tables).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ArxivDI-GESTables: Synthesizing scientific literature into tables using language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ArxivDI-GESTables</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ArxivDI-GESTables employs LLMs to extract, normalize, and place structured evidence and metadata from multiple scientific documents into tabular formats (e.g., literature review tables).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Query-driven extraction and table-population prompts to LLMs for pulling out comparable fields across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Aggregation of extracted fields into canonical table rows/columns; normalization across documents for comparability.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature; used to synthesize literature into structured tables.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Literature review tables and structured tabular summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as evidence that LLMs can be used to synthesize structured artifacts (tables) from multiple papers, analogous to taxonomy construction but with tabular outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Table population can be sensitive to hallucination and consistency across rows; TAXOALIGN notes this family of works as related literature-synthesis directions.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TaxoAlign: Scholarly Taxonomy Generation Using Language Models', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4405.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4405.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Leaderboards-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that uses LLMs to automatically build/track scientific leaderboards (tasks/datasets/metrics/numeric scores) by extracting structured metadata from papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Automated leaderboard construction (Şahinuç et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>This method prompts LLMs to extract task/dataset/metric/score triples from scientific literature and compiles them into structured leaderboards, demonstrating LLMs' utility for synthesizing benchmark-style artifacts from corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompt-based extraction of structured entities and numeric results from papers (entity extraction + normalization).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Aggregation and canonicalization into leaderboards (structured tables).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>NLP/ML research leaderboards (scientific literature broadly).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured leaderboards (task-dataset-metric-score tables).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Demonstrates LLMs' practical utility in extracting and normalizing experimental results across papers; cited as related work demonstrating synthesis of cross-paper structured artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Challenges include extraction fidelity for numeric values, alignment of naming conventions, and hallucination of results if not well-grounded.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TaxoAlign: Scholarly Taxonomy Generation Using Language Models', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4405.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4405.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pronesti-evidence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Query-driven document-level scientific evidence extraction from biomedical studies / Enhancing study-level inference from clinical trial papers via RL-based numeric reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two related works that use LLM-driven, query-based methods to extract and synthesize biomedical evidence (e.g., into forest plots) and to improve numeric reasoning in clinical-trial inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Query-driven document-level scientific evidence extraction from biomedical studies</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Query-driven biomedical evidence synthesis (Pronesti et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>These works apply LLMs in a query-driven fashion to extract study-level evidence, numerical results, and to synthesize evidence across studies (e.g., producing forest-plot style syntheses), and explore RL-based approaches to improve numeric reasoning and study-level inference.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Query-driven document-level extraction (prompting LLMs to return structured evidence and numeric values from biomedical papers).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Aggregation of extracted evidence into synthesised artifacts such as forest plots; RL-based numeric reasoning to improve consistency of numeric extractions.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Biomedical literature (clinical trials and related studies).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Evidence syntheses (forest plots), structured evidence tables, study-level inference summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Demonstrates LLMs can extract and synthesize cross-study numeric evidence, but numeric reasoning and grounding are challenging and can be improved with RL techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Numeric extraction errors, need for grounding and verification, and the risk of propagating extraction inconsistencies when aggregating.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TaxoAlign: Scholarly Taxonomy Generation Using Language Models', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4405.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4405.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruct‑survey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruct large language models to generate scientific literature survey step by step</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that instructs LLMs via step-by-step procedures to generate scientific literature surveys, emphasizing decomposition of the survey-writing process into sub-steps for improved outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Instruct large language models to generate scientific literature survey step by step</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Stepwise instructed survey generation (Lai et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>This approach decomposes survey generation into stepwise instruction prompts for LLMs (e.g., generating outlines, collecting evidence, drafting sections) to improve coherence and manage long-context generation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Stepwise prompting to retrieve and summarize relevant information from provided documents or corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Multi-step synthesis combining outline generation, evidence aggregation, and section drafting to assemble survey content.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Automated survey writing across scientific topics.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Survey outlines and drafts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Stepwise instruction decomposition can help manage complexity of survey generation and long-context limits, a motivation echoed by TAXOALIGN's knowledge-slice approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Long-context coverage and grounding remain challenging without explicit document slicing or retrieval strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TaxoAlign: Scholarly Taxonomy Generation Using Language Models', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autosurvey: Large language models can automatically write surveys <em>(Rating: 2)</em></li>
                <li>Surveyx: Academic survey automation via large language models <em>(Rating: 2)</em></li>
                <li>Surveyforge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing <em>(Rating: 2)</em></li>
                <li>Researcharena: Benchmarking large language models' ability to collect and organize information as research agents <em>(Rating: 2)</em></li>
                <li>ArxivDI-GESTables: Synthesizing scientific literature into tables using language models <em>(Rating: 2)</em></li>
                <li>Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards <em>(Rating: 2)</em></li>
                <li>Query-driven document-level scientific evidence extraction from biomedical studies <em>(Rating: 2)</em></li>
                <li>Enhancing study-level inference from clinical trial papers via rl-based numeric reasoning <em>(Rating: 2)</em></li>
                <li>Instruct large language models to generate scientific literature survey step by step <em>(Rating: 2)</em></li>
                <li>Scilitllm: How to adapt llms for scientific literature understanding <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4405",
    "paper_id": "paper-282210220",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "TAXOALIGN",
            "name_full": "TAXOALIGN: Scholarly Taxonomy Generation Using Language Models",
            "brief_description": "A three-stage LLM-based pipeline that constructs hierarchical taxonomy trees from a topic and a corpus of reference papers by (1) extracting 'knowledge slices' from each paper, (2) instruction‑tuning an LLM to verbalize a taxonomy, and (3) refining the taxonomy with a stronger reasoning LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "TAXOALIGN",
            "system_description": "TAXOALIGN is a three-phase pipeline: (1) Knowledge Slice Creation — an LLM (e.g., Mistral-7B-Instruct, Meta-Llama-3-8B-Instruct) is prompted to extract short, topic-relevant segments ('knowledge slices') from each reference paper so that many papers can be represented within an LLM context window; (2) Taxonomy Verbalization — an instruction‑tuned LLM (finetuned via QLoRA on models such as Llama-3.1-Tülu-3-8B or SciLitLLM1.5-7B) is trained to produce concise, structured taxonomy trees (max depth three) from the assembled slices; (3) Taxonomy Refinement — a stronger closed-domain reasoning model (GPT-4o-mini in the paper) is prompted to check grounding, refine parent–child relations, expand nodes to improve coverage, and reduce hallucinations.",
            "llm_model_used": "Knowledge-slice models: Mistral-7B-Instruct-v0.3, Meta-Llama-3-8B-Instruct; Verbalization (instruction-tuned): Llama-3.1-Tülu-3-8B, SciLitLLM1.5-7B (QLoRA finetuning); Refinement (reasoning): GPT-4o-mini",
            "extraction_technique": "Prompt-based extraction via LLMs that identify and return concise, topic-relevant 'knowledge slices' per paper; preprocessing (Docling) used to parse PDFs and Semantic Scholar for references.",
            "synthesis_technique": "Hierarchical synthesis via instruction-tuned taxonomy verbalization producing tree-structured outputs, followed by a reasoning-based refinement step to align topology and semantics across slices; multi-stage concatenation/aggregation of slices into a taxonomy.",
            "number_of_papers": "Experiments use the provided reference sets from CS-TAXOBENCH — on average ~131 reference papers per taxonomy (dataset of 460 taxonomies; additional testset of 80 taxonomies with ~71 refs each).",
            "domain_or_topic": "Computer Science survey literature (CS-TAXOBENCH: ACM Computing Surveys papers, 2020–2024); tested on conference survey papers (ACL*/IJCAI).",
            "output_type": "Hierarchical taxonomy trees (topic root + multi-level subtopics; max depth three).",
            "evaluation_metrics": "Average degree score (structural similarity), level-order traversal (BLEU-2, ROUGE-L, BERTScore), Node Soft Recall (NSR), Node Entity Recall (NER, noun-phrase overlap), LLM-as-a-judge, and human annotator ratings (Likert).",
            "performance_results": "TAXOALIGN consistently outperforms listed baselines across nearly all metrics in this paper: it achieves an average-degree (∆) close to 1 (i.e., structural similarity to gold), higher level-order BLEU/ROUGE/BERTScore and higher Node Soft Recall than baselines, and better human/LLM judge ratings; qualitatively it produces more coherent, less verbose taxonomies and fewer hallucinated nodes than other methods.",
            "comparison_baseline": "AutoSurvey, STORM, Topic-only, Topic+Keyphrases, and ablations (TAXOALIGN w/o Verbalization/w/o Refinement).",
            "performance_vs_baseline": "Outperforms all baselines on most automated metrics and human evaluations; ablation shows knowledge-slices alone are strong (second-best) but verbalization + refinement reduce structural divergence and improve label quality.",
            "key_findings": "Key strengths are (1) knowledge-slice extraction which enables covering large reference sets within LLM context limits, (2) instruction-tuning (taxonomy verbalization) that helps produce tree-structured outputs that mirror human taxonomies, and (3) a final refinement stage with a stronger reasoning LLM to ground and correct relations. Knowledge-slices are essential; verbalization and refinement improve structure and label quality.",
            "limitations_challenges": "Remaining issues include hallucinated node labels, verbosity or repetition when generating directly from slices, factual errors in verbalized nodes, layer-wise exact-match failures, and inherent LLM limitations with long-context reasoning and factual consistency; dataset drawn from one journal (ACM CSUR) limits distributional diversity.",
            "scaling_behavior": "Designed to scale across many cited papers by compressing each paper into a small number of knowledge slices (enabling handling of ~O(100) papers per topic); the paper reports that using stronger reasoning models for refinement (GPT-4o-mini) improves outcomes; no extensive model-size scaling curve is reported beyond ablations showing instruction-tuned models + refinement outperform naïve prompting approaches.",
            "uuid": "e4405.0",
            "source_info": {
                "paper_title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "AutoSurvey",
            "name_full": "AutoSurvey: Large language models can automatically write surveys",
            "brief_description": "An LLM-based method for automatic survey (outline) generation that divides reference papers into groups, generates outlines for each group, and amalgamates these outlines into a single comprehensive outline.",
            "citation_title": "Autosurvey: Large language models can automatically write surveys",
            "mention_or_use": "use",
            "system_name": "AutoSurvey (outline generation component)",
            "system_description": "The AutoSurvey outline-generation approach randomly partitions reference papers into groups, generates multiple intermediate outlines using an LLM per group, and then merges/amalgamates those outlines into a single comprehensive outline/survey structure.",
            "llm_model_used": "In the original AutoSurvey work (cited), large instruction-following LLMs are used; in this paper AutoSurvey is run as a baseline via prompting with GPT-4o-mini for comparisons (paper supplies references to the baseline in their experiments).",
            "extraction_technique": "Group-wise summarization of reference papers followed by aggregation; uses prompt-based LLM summarization of groups of papers.",
            "synthesis_technique": "Amalgamation of multiple group-level outlines into a final outline (flat/sectioned survey).",
            "number_of_papers": "Applied to the same reference sets provided by the authors for fair comparison (i.e., the per-topic reference sets in CS-TAXOBENCH; experiments used on-average ~131 papers per taxonomy).",
            "domain_or_topic": "Computer Science surveys (evaluated by the TAXOALIGN paper on CS-TAXOBENCH).",
            "output_type": "Outlines / survey structure (section/subsection outlines); not specifically hierarchical taxonomies but used as an outline baseline.",
            "evaluation_metrics": "Evaluated in this paper using average degree score (∆), level-order traversal BLEU-2 / ROUGE-L / BERTScore, Node Soft Recall, Node Entity Recall, LLM-as-a-judge and human annotation metrics.",
            "performance_results": "In the TAXOALIGN experiments AutoSurvey produced overly large and structurally divergent taxonomy-like outlines (∆ ≈ 4.47 in Table 2), low lexical overlap metrics (very low BLEU/ROUGE), and lower human/LLM judge scores compared to TAXOALIGN.",
            "comparison_baseline": "Compared to TAXOALIGN and other baselines (STORM, Topic-only, Topic+Keyphrases).",
            "performance_vs_baseline": "Performed substantially worse than TAXOALIGN on structural similarity (much larger ∆ &gt; 1), lower semantic/lexical overlap metrics, and lower human/LLM judge ratings.",
            "key_findings": "Random grouping and naive amalgamation can lead to verbose, poorly aligned outlines for taxonomy tasks; outlines produced by AutoSurvey are less structurally similar to human taxonomies on this benchmark.",
            "limitations_challenges": "Tendency to produce overly large trees/outlines, higher hallucination risk, structural divergence from human taxonomies when applied to taxonomy generation.",
            "scaling_behavior": "Method relies on partitioning papers into groups to fit LLM context windows; in the TAXOALIGN experiments this led to overly large/branchy outputs compared to the structured slicing+verbalization approach.",
            "uuid": "e4405.1",
            "source_info": {
                "paper_title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "STORM",
            "name_full": "STORM (Shao et al., 2024) — pre-writing simulated conversations + LLM outline generation",
            "brief_description": "A baseline pipeline that generates draft outlines by prompting LLMs after a 'pre-writing' simulated conversation stage and then refines the outline using the topic plus simulated conversations and drafts.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "STORM",
            "system_description": "STORM's pre-writing stage simulates conversational research to explore a topic; an LLM produces an initial draft outline from the topic, and a final outline is produced by prompting an LLM with the topic, simulated conversations, and the draft outline (in the paper it is used as a baseline for taxonomy/outline generation).",
            "llm_model_used": "Used as a prompting-based method with GPT-4o-mini in the TAXOALIGN comparisons; original STORM uses large instruction-following LLMs (Shao et al., 2024).",
            "extraction_technique": "Prompt-driven exploration (simulated conversations) and draft outline generation from prompts; not explicitly knowledge-slice extraction.",
            "synthesis_technique": "Iterative outline refinement via multi-step prompting informed by simulated conversational context and draft outlines.",
            "number_of_papers": "Evaluated on the same CS-TAXOBENCH reference sets (paper ran STORM as a baseline with the dataset provided).",
            "domain_or_topic": "Computer Science survey literature (used here as a baseline on CS-TAXOBENCH).",
            "output_type": "Draft and final outlines / sectioned survey outputs (used as taxonomy-outline baseline).",
            "evaluation_metrics": "Measured with average degree score, level-order traversal BLEU/ROUGE/BERTScore, Node Soft Recall, Node Entity Recall, and LLM/human judgeings in this paper.",
            "performance_results": "In TAXOALIGN experiments STORM produced highly branched/large trees (∆ ≈ 6.15 in Table 2) and low lexical overlap / judge scores relative to TAXOALIGN.",
            "comparison_baseline": "Compared directly to TAXOALIGN and other baseline methods in the paper.",
            "performance_vs_baseline": "Worse than TAXOALIGN across the majority of structural and semantic metrics in the CS-TAXOBENCH evaluations.",
            "key_findings": "Simulated-conversation-based outline generation can help explore topics, but without targeted document grounding (knowledge slices) it leads to structural and semantic divergence from human taxonomies.",
            "limitations_challenges": "Produces overly large outlines, limited grounding to the cited documents unless reference content is incorporated effectively; susceptible to hallucination and low lexical alignment with human taxonomies.",
            "scaling_behavior": "No explicit scaling curves reported; performance degraded into larger tree branching when naive prompting attempted to ingest many references without slicing.",
            "uuid": "e4405.2",
            "source_info": {
                "paper_title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "ResearchArena",
            "name_full": "Researcharena: Benchmarking large language models' ability to collect and organize information as research agents",
            "brief_description": "A prior effort cited by the paper that frames LLMs as research agents whose ability to collect and organize information across papers is benchmarked; relevant to automated survey/synthesis tasks.",
            "citation_title": "Researcharena: Benchmarking large language models' ability to collect and organize information as research agents",
            "mention_or_use": "mention",
            "system_name": "Researcharena",
            "system_description": "ResearchArena treats LLMs as autonomous/semiautonomous research agents and evaluates their end-to-end ability to collect, organize, and synthesize information across multiple documents into coherent artifacts (e.g., survey-style outputs).",
            "llm_model_used": "",
            "extraction_technique": "Agentic retrieval and multi-step prompting to collect information from multiple sources (as described in the referenced work; cited as related work here).",
            "synthesis_technique": "Agentic aggregation and organization of collected items into summaries/outlines; benchmarking focuses on collection + organization capacity.",
            "number_of_papers": "Not specified in this paper (referenced work likely evaluates across varying corpus sizes).",
            "domain_or_topic": "General academic research agent benchmarking (cited as related work in survey generation literature).",
            "output_type": "Organized literature artifacts, outlines, or synthesized summaries generated by research-agent workflows.",
            "evaluation_metrics": "Benchmarked for collection and organization capabilities; TAXOALIGN cites this work as related work in automated survey generation but does not re-evaluate ResearchArena itself.",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Cited as an example of prior work that focuses on LLM-based collection and organization of research materials; motivates taxonomy/survey automation.",
            "limitations_challenges": "Paper does not detail ResearchArena results; referenced work investigates agentic limitations such as retrieval fidelity, consistency across steps, and grounding.",
            "scaling_behavior": "Not detailed in TAXOALIGN (refer to Researcharena original paper).",
            "uuid": "e4405.3",
            "source_info": {
                "paper_title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "SURVEYX",
            "name_full": "SURVEYX: Academic survey automation via large language models",
            "brief_description": "An automated survey-generation system cited as recent work that uses LLMs to produce academic surveys from a corpus of papers.",
            "citation_title": "Surveyx: Academic survey automation via large language models",
            "mention_or_use": "mention",
            "system_name": "SURVEYX",
            "system_description": "SURVEYX is cited as a system that automates survey creation using prompting and LLM-based generation across a corpus of academic papers; TAXOALIGN references it as part of the recent surge of end-to-end survey generation tools.",
            "llm_model_used": "",
            "extraction_technique": "Likely prompt-based summarization and aggregation of multiple papers (paper cites SURVEYX among survey-generation approaches).",
            "synthesis_technique": "Multi-document summarization / outline generation into survey-style artifacts.",
            "number_of_papers": "",
            "domain_or_topic": "Academic surveys (general), cited in the context of automated survey generation.",
            "output_type": "Surveys / outlines.",
            "evaluation_metrics": "",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Cited as part of recent literature on LLM-based automated survey generation; TAXOALIGN positions taxonomy generation as a complementary but distinct task.",
            "limitations_challenges": "Not detailed in TAXOALIGN; general challenges include grounding, structure alignment with human-written surveys, and hallucination.",
            "scaling_behavior": "",
            "uuid": "e4405.4",
            "source_info": {
                "paper_title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "SURVEYFORGE",
            "name_full": "SURVEYFORGE: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing",
            "brief_description": "A recent automated-survey system that emphasizes outline heuristics, memory-driven generation, and multi-dimensional evaluation of generated surveys using LLMs.",
            "citation_title": "Surveyforge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing",
            "mention_or_use": "mention",
            "system_name": "SURVEYFORGE",
            "system_description": "SURVEYFORGE focuses on heuristics for outline construction, memory-driven (context-management) generation, and a multi-dimensional evaluation framework for automated survey writing with LLMs; cited as related work in the area of automated survey generation.",
            "llm_model_used": "",
            "extraction_technique": "Memory-driven multi-stage prompting and outline heuristics; aggregates content from many papers via staged generation.",
            "synthesis_technique": "Heuristic outline construction combined with memory/context-management to generate longer, structured surveys.",
            "number_of_papers": "",
            "domain_or_topic": "Automated academic survey generation (general).",
            "output_type": "Survey outlines and full survey drafts.",
            "evaluation_metrics": "Multi-dimensional evaluation (as per SURVEYFORGE description in the reference).",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Included as an example of end-to-end survey automation tools; TAXOALIGN differentiates itself by focusing on structured taxonomy trees and structural alignment evaluation.",
            "limitations_challenges": "",
            "scaling_behavior": "",
            "uuid": "e4405.5",
            "source_info": {
                "paper_title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "ArxivDI-GESTables",
            "name_full": "ArxivDI-GESTables: Synthesizing scientific literature into tables using language models",
            "brief_description": "A system that uses LLMs to synthesize information from scientific papers into structured tabular representations (tables).",
            "citation_title": "ArxivDI-GESTables: Synthesizing scientific literature into tables using language models",
            "mention_or_use": "mention",
            "system_name": "ArxivDI-GESTables",
            "system_description": "ArxivDI-GESTables employs LLMs to extract, normalize, and place structured evidence and metadata from multiple scientific documents into tabular formats (e.g., literature review tables).",
            "llm_model_used": "",
            "extraction_technique": "Query-driven extraction and table-population prompts to LLMs for pulling out comparable fields across papers.",
            "synthesis_technique": "Aggregation of extracted fields into canonical table rows/columns; normalization across documents for comparability.",
            "number_of_papers": "",
            "domain_or_topic": "General scientific literature; used to synthesize literature into structured tables.",
            "output_type": "Literature review tables and structured tabular summaries.",
            "evaluation_metrics": "",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Cited as evidence that LLMs can be used to synthesize structured artifacts (tables) from multiple papers, analogous to taxonomy construction but with tabular outputs.",
            "limitations_challenges": "Table population can be sensitive to hallucination and consistency across rows; TAXOALIGN notes this family of works as related literature-synthesis directions.",
            "scaling_behavior": "",
            "uuid": "e4405.6",
            "source_info": {
                "paper_title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Leaderboards-LLM",
            "name_full": "Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards",
            "brief_description": "An approach that uses LLMs to automatically build/track scientific leaderboards (tasks/datasets/metrics/numeric scores) by extracting structured metadata from papers.",
            "citation_title": "Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards",
            "mention_or_use": "mention",
            "system_name": "Automated leaderboard construction (Şahinuç et al., 2024)",
            "system_description": "This method prompts LLMs to extract task/dataset/metric/score triples from scientific literature and compiles them into structured leaderboards, demonstrating LLMs' utility for synthesizing benchmark-style artifacts from corpora.",
            "llm_model_used": "",
            "extraction_technique": "Prompt-based extraction of structured entities and numeric results from papers (entity extraction + normalization).",
            "synthesis_technique": "Aggregation and canonicalization into leaderboards (structured tables).",
            "number_of_papers": "",
            "domain_or_topic": "NLP/ML research leaderboards (scientific literature broadly).",
            "output_type": "Structured leaderboards (task-dataset-metric-score tables).",
            "evaluation_metrics": "",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Demonstrates LLMs' practical utility in extracting and normalizing experimental results across papers; cited as related work demonstrating synthesis of cross-paper structured artifacts.",
            "limitations_challenges": "Challenges include extraction fidelity for numeric values, alignment of naming conventions, and hallucination of results if not well-grounded.",
            "scaling_behavior": "",
            "uuid": "e4405.7",
            "source_info": {
                "paper_title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Pronesti-evidence",
            "name_full": "Query-driven document-level scientific evidence extraction from biomedical studies / Enhancing study-level inference from clinical trial papers via RL-based numeric reasoning",
            "brief_description": "Two related works that use LLM-driven, query-based methods to extract and synthesize biomedical evidence (e.g., into forest plots) and to improve numeric reasoning in clinical-trial inference.",
            "citation_title": "Query-driven document-level scientific evidence extraction from biomedical studies",
            "mention_or_use": "mention",
            "system_name": "Query-driven biomedical evidence synthesis (Pronesti et al.)",
            "system_description": "These works apply LLMs in a query-driven fashion to extract study-level evidence, numerical results, and to synthesize evidence across studies (e.g., producing forest-plot style syntheses), and explore RL-based approaches to improve numeric reasoning and study-level inference.",
            "llm_model_used": "",
            "extraction_technique": "Query-driven document-level extraction (prompting LLMs to return structured evidence and numeric values from biomedical papers).",
            "synthesis_technique": "Aggregation of extracted evidence into synthesised artifacts such as forest plots; RL-based numeric reasoning to improve consistency of numeric extractions.",
            "number_of_papers": "",
            "domain_or_topic": "Biomedical literature (clinical trials and related studies).",
            "output_type": "Evidence syntheses (forest plots), structured evidence tables, study-level inference summaries.",
            "evaluation_metrics": "",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Demonstrates LLMs can extract and synthesize cross-study numeric evidence, but numeric reasoning and grounding are challenging and can be improved with RL techniques.",
            "limitations_challenges": "Numeric extraction errors, need for grounding and verification, and the risk of propagating extraction inconsistencies when aggregating.",
            "scaling_behavior": "",
            "uuid": "e4405.8",
            "source_info": {
                "paper_title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Instruct‑survey",
            "name_full": "Instruct large language models to generate scientific literature survey step by step",
            "brief_description": "A method that instructs LLMs via step-by-step procedures to generate scientific literature surveys, emphasizing decomposition of the survey-writing process into sub-steps for improved outputs.",
            "citation_title": "Instruct large language models to generate scientific literature survey step by step",
            "mention_or_use": "mention",
            "system_name": "Stepwise instructed survey generation (Lai et al., 2024)",
            "system_description": "This approach decomposes survey generation into stepwise instruction prompts for LLMs (e.g., generating outlines, collecting evidence, drafting sections) to improve coherence and manage long-context generation.",
            "llm_model_used": "",
            "extraction_technique": "Stepwise prompting to retrieve and summarize relevant information from provided documents or corpora.",
            "synthesis_technique": "Multi-step synthesis combining outline generation, evidence aggregation, and section drafting to assemble survey content.",
            "number_of_papers": "",
            "domain_or_topic": "Automated survey writing across scientific topics.",
            "output_type": "Survey outlines and drafts.",
            "evaluation_metrics": "",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Stepwise instruction decomposition can help manage complexity of survey generation and long-context limits, a motivation echoed by TAXOALIGN's knowledge-slice approach.",
            "limitations_challenges": "Long-context coverage and grounding remain challenging without explicit document slicing or retrieval strategies.",
            "scaling_behavior": "",
            "uuid": "e4405.9",
            "source_info": {
                "paper_title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
                "publication_date_yy_mm": "2025-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autosurvey: Large language models can automatically write surveys",
            "rating": 2,
            "sanitized_title": "autosurvey_large_language_models_can_automatically_write_surveys"
        },
        {
            "paper_title": "Surveyx: Academic survey automation via large language models",
            "rating": 2,
            "sanitized_title": "surveyx_academic_survey_automation_via_large_language_models"
        },
        {
            "paper_title": "Surveyforge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing",
            "rating": 2,
            "sanitized_title": "surveyforge_on_the_outline_heuristics_memorydriven_generation_and_multidimensional_evaluation_for_automated_survey_writing"
        },
        {
            "paper_title": "Researcharena: Benchmarking large language models' ability to collect and organize information as research agents",
            "rating": 2,
            "sanitized_title": "researcharena_benchmarking_large_language_models_ability_to_collect_and_organize_information_as_research_agents"
        },
        {
            "paper_title": "ArxivDI-GESTables: Synthesizing scientific literature into tables using language models",
            "rating": 2,
            "sanitized_title": "arxivdigestables_synthesizing_scientific_literature_into_tables_using_language_models"
        },
        {
            "paper_title": "Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards",
            "rating": 2,
            "sanitized_title": "efficient_performance_tracking_leveraging_large_language_models_for_automated_construction_of_scientific_leaderboards"
        },
        {
            "paper_title": "Query-driven document-level scientific evidence extraction from biomedical studies",
            "rating": 2,
            "sanitized_title": "querydriven_documentlevel_scientific_evidence_extraction_from_biomedical_studies"
        },
        {
            "paper_title": "Enhancing study-level inference from clinical trial papers via rl-based numeric reasoning",
            "rating": 2,
            "sanitized_title": "enhancing_studylevel_inference_from_clinical_trial_papers_via_rlbased_numeric_reasoning"
        },
        {
            "paper_title": "Instruct large language models to generate scientific literature survey step by step",
            "rating": 2,
            "sanitized_title": "instruct_large_language_models_to_generate_scientific_literature_survey_step_by_step"
        },
        {
            "paper_title": "Scilitllm: How to adapt llms for scientific literature understanding",
            "rating": 1,
            "sanitized_title": "scilitllm_how_to_adapt_llms_for_scientific_literature_understanding"
        }
    ],
    "cost": 0.024230750000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TAXOALIGN: Scholarly Taxonomy Generation Using Language Models
20 Oct 2025</p>
<p>Avishek Lahiri avisheklahiri2014@gmail.com 
Indian Association for the Cultivation of Science
KolkataIndia</p>
<p>Yufang Hou yufang.hou@it-u.at 
IT:U Interdisciplinary Transformation University Austria
LinzAustria</p>
<p>Debarshi Kumar Sanyal debarshi.sanyal@iacs.res.in 
Indian Association for the Cultivation of Science
KolkataIndia</p>
<p>Aaron Grattafiori 
Abhimanyu Dubey 
Abhinav Jauhri 
Abhinav Pandey 
Abhishek Kadian 
Ahmad Al- Dahle 
Aiesha Letman 
Akhil Mathur 
Alan Schel- Ten 
Alex Vaughan 
Amy Yang 
Angela Fan 
Anirudh Goyal 
Anthony Hartshorn 
Aobo Yang 
Archi Mi- Tra 
Archie Sravankumar 
Artem Korenev 
Arthur Hinsvark 
Arun Rao 
Aston Zhang 
Aurelien Ro- Driguez 
Austen Gregerson 
Ava Spataru 
Baptiste Roziere 
Bethany Biron 
Binh Tang 
Bobbie Chern 
Charlotte Caucheteux 
Chaya Nayak 
Chloe Bi 
Chris Marra 
Chris Mcconnell 
Christian Keller 
Christophe Touret 
Chunyang Wu 
Corinne Wong 
Cristian Canton Ferrer 
Cyrus Nikolaidis 
Damien Al- Lonsius 
Daniel Song 
Danielle Pintz 
Danny Livshits 
Danny Wyatt 
David Esiobu 
Dhruv Choudhary 
Dhruv Mahajan 
Diego Garcia-Olano 
Diego Perino 
Dieuwke Hupkes 
Egor Lakomkin 
Ehab Albadawy 
Elina Lobanova 
Emily Dinan 
Eric Michael Smith 
Filip Radenovic 
Francisco Guzmán 
Frank Zhang 
Gabriel Synnaeve 
Gabrielle Lee 
Georgia Lewis 
Govind Thattai 
Graeme Nail 
Mi- Alon Gregoire 
Guan Pang 
Guillem Cucurell 
Hailey Nguyen 
Hannah Korevaar 
Hu Xu 
Hugo Touvron 
ImanolIliyan Zarov 
Arrieta Ibarra 
Isabel Kloumann 
Is- Han Misra 
Ivan Evtimov 
Jack Zhang 
Jade Copet 
Jaewon Lee 
Jan Geffert 
Jana Vranes 
Jason Park 
Jay Mahadeokar 
Jeet Shah 
Jelmer Van Der Linde 
Jennifer Billock 
Jenny Hong 
Jenya Lee 
Jeremy Fu 
Jianfeng Chi 
Jianyu Huang 
Jiawen Liu 
Jie Wang 
Jiecao Yu 
Joanna Bitton 
Joe Spisak 
Jongsoo Park 
Joseph Rocca 
Joshua Johnstun 
Joshua Saxe 
Jun- Teng Jia 
Kalyan Vasuden Alwala 
Karthik Prasad 
Kartikeya Upasani 
Kate Plawiak 
Keqian Li 
Kenneth Heafield 
Kevin Stone 
Khalid El-Arini 
Krithika Iyer 
Kshitiz Malik 
Kuenley Chiu 
Kunal Bhalla 
Kushal Lakhotia 
Lauren Rantala-Yeary 
Laurens Van Der Maaten 
Lawrence Chen 
Liang Tan 
Liz Jenkins 
Louis Martin 
Lovish Madaan 
Lubo Malo 
Lukas Blecher 
Lukas Landzaat 
Luke De Oliveira 
Madeline Muzzi 
Mahesh Pasupuleti 
Mannat Singh 
Manohar Paluri 
Marcin Kardas 
Maria Tsimpoukelli 
Mathew Oldham 
Mathieu Rita 
Maya Pavlova 
Melanie Kam- Badur 
Mike Lewis 
MiteshMin Si 
Kumar Singh 
Mona Hassan 
Naman Goyal 
Narjes Torabi 
Niko- Lay Bashlykov 
Nikolay Bogoychev 
Niladri Chatterji 
Ning Zhang 
Olivier Duchenne 
Onur Çelebi 
Patrick Alrassy 
Pengchuan Zhang 
PetarPengwei Li 
Peter Weng 
Prajjwal Bhargava 
Pratik Dubal 
PunitPraveen Krishnan 
Singh Koura 
Puxin Xu 
Qing He 
Qingxiao Dong 
Ragavan Srinivasan 
Raj Ganapathy 
Ramon Calderer 
Ricardo Silveira Cabral 
Robert Stojnic 
Roberta Raileanu 
Rohan Maheswari 
Rohit Girdhar 
Rohit Patel 
Romain Sauvestre 
Ron- Nie Polidoro 
Roshan Sumbaly 
Ross Taylor 
Ruan Silva 
Rui Hou 
Rui Wang 
Saghar Hosseini 
Sa- Hana Chennabasappa 
Sanjay Singh 
Sean Bell 
Seo- Hyun Sonia Kim 
Sergey Edunov 
Shaoliang Nie 
Sha- Ran Narang 
Sharath Raparthy 
Sheng Shen 
Shengye Wan 
Shruti Bhosale 
Shun Zhang 
Simon Van- Denhende 
Soumya Batra 
Spencer Whitman 
Sten Sootla 
Stephane Collot 
Suchin Gururangan 
Syd- Ney Borodinsky 
Tamar Herman 
Tara Fowler 
Tarek Sheasha 
Thomas Georgiou 
Thomas Scialom 
Tobias Speckbacher 
Todor Mihaylov 
Tong Xiao 
Ujjwal Karn 
Vedanuj Goswami 
Vibhor Gupta 
Vignesh Ramanathan 
Viktor Kerkez 
Vincent Gonguet 
Vir- Ginie Do 
Vish Vogeti 
Vítor Albiero 
Vladan Petro- Vic 
Weiwei Chu 
Wenhan Xiong 
Wenyin Fu 
Yasmine Gaur 
Yi Babaei 
Yiwen Wen 
Yuchen Song 
Yue Zhang 
Yuning Li 
Zacharie Delpierre Mao 
Zheng Coudert 
Zhengxing Yan 
Zoe Chen 
Aaditya Papakipos 
Aayushi Singh 
Abha Sri- Vastava 
Adam Jain 
Adam Kelsey 
Adithya Shajnfeld 
Adolfo Gangidi 
Ahuva Victoria 
Ajay Goldstand 
Ajay Menon 
Alex Sharma 
Alexei Boesenberg 
Allie Baevski 
Amanda Feinstein 
Amit Kallet 
Amos San- Gani 
Anam Teo 
Andrei Yunus 
An- Dres Lupu 
Andrew Alvarado 
Andrew Caples 
Andrew Gu 
Andrew Ho 
Andrew Poulton 
AnkitRamchan- Dani Ryan 
Annie Dong 
Annie Franco 
Anuj Goyal 
Apara- Jita Saraf 
Arkabandhu Chowdhury 
Ashley Gabriel 
Ashwin Bharambe 
Assaf Eisenman 
Azadeh Yaz- Dan 
Beau James 
Ben Maurer 
Benjamin Leonhardi 
Bernie Huang 
Beth Loyd 
Beto De Paola 
Bhargavi Paranjape 
Bing Liu 
Bo Wu 
Boyu Ni 
Braden Han- Cock 
Bram Wasti 
Brandon Spence 
Brani Stojkovic 
Brian Gamido 
Britt Montalvo 
Carl Parker 
Carly Burton 
Catalina Mejia 
Ce Liu 
Changhan Wang 
Changkyu Kim 
Chao Zhou 
Chester Hu 
Ching- Hsiang Chu 
Chris Cai 
Chris Tindal 
Christoph Fe- Ichtenhofer 
Cynthia Gao 
Damon Civin 
Dana Beaty 
Daniel Kreymer 
Daniel Li 
David Adkins 
David Xu 
Davide Testuggine 
Delia David 
Devi Parikh 
Elaine Montgomery 
Eleonora Presani 
Emily Hahn 
Emily Wood 
Eric-Tuan Le 
Erik Brinkman 
Este- Ban Arcaute 
Evan Dunbar 
Evan Smothers 
Fei Sun 
Felix Kreuk 
Feng Tian 
Filippos Kokkinos 
Firat Ozgenel 
Francesco Caggioni 
Frank Kanayet 
Frank Seide 
Gabriela Medina Florez 
Gabriella Schwarz 
Gada Badeer 
Georgia Swee 
Gil Halpern 
Grant Herman 
Grigory Sizov 
Guangyi 
Guna Zhang 
Hakan Lakshminarayanan 
Hamid Inan 
Han Shojanaz- Eri 
Hannah Zou 
Hanwen Wang 
Haroun Zha 
Harrison Habeeb 
Helen Rudolph 
Henry Suk 
Hunter As- Pegren 
Hongyuan Goldman 
Ibrahim Zhan 
Igor Damlaj 
Igor Molybog 
Ilias Tufanov 
Leontiadis 
Nifer Chan 
Jenny Zhen 
Jeremy Reizenstein 
Jeremy Teboul 
Jessica Zhong 
Jian Jin 
Jingyi Yang 
Joe Cummings 
Jon Carvill 
Jon Shepard 
Jonathan Mc- Phie 
Jonathan Torres 
Josh Ginsburg 
Junjie Wang 
Kai Wu 
Kam Hou 
Karan Saxena 
Kartikay Khan- Delwal 
Katayoun Zand 
Kathy Matosich 
Kaushik Veeraraghavan 
Kelly Michelena 
Ki- Ran Jagadeesh 
Kun Huang 
Kunal Chawla 
Kyle Huang 
Lailin Chen 
Lavender A,Lakshya Garg 
Leandro Silva 
Lee Bell 
Lei Zhang 
Liangpeng Guo 
Licheng Yu 
Liron Moshkovich 
Luca Wehrst- Edt 
Madian Khabsa 
Manav Avalani 
Manish Bhatt 
Martynas Mankus 
Matan Hasson 
Matthew Lennie 
Matthias Reso 
Maxim Groshev 
Maxim Naumov 
Maya Lathi 
Meghan Keneally 
Miao Liu 
Michael L Seltzer 
Michal Valko 
MihirMichelle Restrepo 
Mik Vyatskov 
Mikayel Samvelyan 
Mike Clark 
Mike Macey 
Mike Wang 
Miquel Jubert Hermoso 
Mo Metanat 
Mohammad Rastegari 
Munish Bansal 
Nandhini Santhanam 
Natascha Parks 
Natasha White 
Navyata Bawa 
Nayan Singhal 
Nick Egebo 
Nicolas Usunier 
NikolayNikhil Mehta 
Pavlovich Laptev 
Ning Dong 
Norman Cheng 
Oleg Chernoguz 
Olivia Hart 
Omkar Salpekar 
Ozlem Kalinli 
Parkin Kent 
Parth Parekh 
Paul Saab 
Pavan Balaji 
Pe- Dro Rittner 
Philip Bontrager 
Pierre Roux 
Piotr Dollar 
Polina Zvyagina 
Prashant Ratanchandani 
Pritish Yuvraj 
Qian Liang 
Rachad Alao 
Rachel Rodriguez 
Rafi Ayub 
Raghotham Murthy 
Raghu Nayani 
Rahul Mitra 
Rangaprabhu Parthasarathy 
Raymond Li 
Rebekkah Hogan 
Robin Battey 
Rocky Wang 
Russ Howes 
Ruty Rinott 
Sachin Mehta 
Sachin Siby 
Jayesh Sai 
Samyak Bondu 
Sara Datta 
Sara Chugh 
Sargun Hunt 
Sasha Dhillon 
Satadru Sidorov 
Saurabh Pan 
Saurabh Mahajan 
Seiji Verma 
Sharadh Yamamoto 
Shaun Ramaswamy 
Shaun Lind- Say 
Sheng Lindsay 
Shenghao Feng 
ShengxinCindy Lin 
Shishir Zha 
Shiva Patil 
Shuqiang Shankar 
Shuqiang Zhang 
Sinong Zhang 
Sneha Wang 
Soji Agarwal 
Soumith Sajuyigbe 
Stephanie Chintala 
Stephen Max 
Steve Chen 
Steve Kehoe 
Sudarshan Satterfield 
Sumit Govindaprasad 
Summer Gupta 
Sungmin Deng 
Sunny Cho 
Suraj Virk 
Sy Subramanian 
Sydney Choudhury 
Tal Goldman 
Tamar Remez 
Tamara Glaser 
Thilo Best 
Thomas Koehler 
Tianhe Robinson 
Tianjun Li 
Tim Zhang 
Timothy Matthews 
Tzook Chou 
Varun Shaked 
Victoria Vontimitta 
Victoria Ajayi 
Vijai Montanez 
Vinay Satish Mohan 
Vishal Kumar 
Vlad Mangla 
Vlad Ionescu 
Vlad Tiberiu Poenaru 
Vladimir Mihailescu 
Wei Ivanov 
Wenchen Li 
Wen- Wen Wang 
Wes Jiang 
Will Bouaziz 
Xiaocheng Constable 
Xiaojian Tang 
Xiaolan Wu 
Xilun Wang 
Xinbo Wu 
Yaniv Gao 
Yanjun Kleinman 
Ye Chen 
Ye Hu 
Ye Jia 
Yenda Qi 
Yilin Li 
Ying Zhang 
Yossi Zhang 
Youngjin Adi 
Yu Nam 
Yu Wang 
Yuchen Zhao 
Yundi Hao 
Yunlu Qian 
Yuzi Li 
Zach He 
Zachary Rait 
Zef Devito 
Zhaoduo Rosnbrick 
Zhenyu Wen 
Yang </p>
<p>Whit-ney Meers
Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xide Xia, Yaelle Gold-schlagXi-aofang Wang, Xin-feng Xie, Xuchao Jia, Xuewei Wang, Yashesh</p>
<p>Diana Liskovich
Didem Foss, Dingkang Wang, Dustin Holland, Edward DowlingDuc Le</p>
<p>Eissa Jamil</p>
<p>Itai Gat
Irina-Elena Veliche
Jake Weissman</p>
<p>James Geboski
James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff TangJen</p>
<p>TAXOALIGN: Scholarly Taxonomy Generation Using Language Models
20 Oct 2025C427EE0A78356B844EBD9C9103F8BDC9arXiv:2510.17263v1[cs.CL]
Taxonomies play a crucial role in helping researchers structure and navigate knowledge in a hierarchical manner.They also form an important part in the creation of comprehensive literature surveys.The existing approaches to automatic survey generation do not compare the structure of the generated surveys with those written by human experts.To address this gap, we present our own method for automated taxonomy creation that can bridge the gap between human-generated and automaticallycreated taxonomies.For this purpose, we create the CS-TAXOBENCH benchmark which consists of 460 taxonomies that have been extracted from human-written survey papers.We also include an additional test set of 80 taxonomies curated from conference survey papers.We propose TAXOALIGN, a threephase topic-based instruction-guided method for scholarly taxonomy generation.Additionally, we propose a stringent automated evaluation framework that measures the structural alignment and semantic coherence of automatically generated taxonomies in comparison to those created by human experts.We evaluate our method and various baselines on CS-TAXOBENCH, using both automated evaluation metrics and human evaluation studies.The results show that TAXOALIGN consistently surpasses the baselines on nearly all metrics.The code and data can be found at https: //github.com/AvishekLahiri/TaxoAlign.</p>
<p>Introduction</p>
<p>In scientific research, a taxonomy is constructed around a well-defined topic to integrate relevant research findings under a unified framework, thereby facilitating deeper understanding among researchers and industry practitioners alike.In the general domain, taxonomies have been proven to be useful tools (Wang et al., 2017), which exhibit the capability of enhancing the performance of various Natural Language Processing (NLP) and Information Retrieval (IR) tasks such as question answering (Harabagiu et al., 2003;Yang et al., 2017), textual entailment (Geffet and Dagan, 2005), personalized recommendation (Zhang et al., 2014), query understanding (Hua et al., 2017), information extraction (Hou et al., 2019;Şahinuç et al., 2024) and knowledge graph construction (Hou et al., 2021;Mondal et al., 2021).Taxonomies have also found a place in real-world deployment applications such as biomedical systems (Köhler et al., 2013), information management (Nickerson et al., 2013) and e-commerce (Aanen et al., 2015;Mao et al., 2020).</p>
<p>In this paper, we conceptualize the task of automated scholarly taxonomy generation.When given the taxonomy topic and a set of related reference papers, the task for the model is to reason over this large set of reference documents and generate a taxonomy tree that is both concise and provides maximal coverage of the reference documents.Automating this process reduces the time, effort, and energy Figure 2: A comparison of a gold standard taxonomy tree and a generated taxonomy tree using TAXOALIGN for the topic "Human Image Generation: A Comprehensive Survey".The generated taxonomy shown here uses Mistral-7B-Instruct-v0.3 for creation of knowledge slices and Llama-3.1-Tülu-3-8B for the taxonomy verbalization component and GPT-4o-mini for refining the generated taxonomy.</p>
<p>researchers spend organizing research within a specific topic.</p>
<p>While Large Language Models (LLMs) are increasingly applied across a wide range of tasks, they face notable challenges in understanding and performing domain-specific tasks (Li et al., 2024;Cai et al., 2025).Additionally, their reasoning over long contexts is limited by constraints in context window size (Liu et al., 2024).In our initial pilot study, we observed that prompting LLMs could generate some topic-related sub-topics, but they were not remotely aligned to the human-written taxonomy.For example, "3D Models and Mapping", "Generative Adversarial Network" and "Data and Annotation" were the first-level nodes/sub-topics that were generated by Tülu3 (Lambert et al., 2025) for the topic "Human Image Generation: A Comprehensive Survey" when supplied with all the summaries of the cited documents.In general, the generated nodes were distantly relevant to the topic but were not at close to the human-written ones as shown in Figure 2 (left).</p>
<p>Recent prior work has focused extensively on the end-to-end automated creation of survey papers (Wang et al., 2024;Liang et al., 2025;Yan et al., 2025;Kang and Xiong, 2025).In contrast, scholarly taxonomy generation remains a relatively unexplored area, with no well-articulated opensource data resources currently available for this task.Moreover, prior work does not compare the structure of generated surveys with those authored by human experts.There is also a lack of an evaluation framework capable of assessing both the structural similarity and semantic coherence between automatically generated and human-written taxonomy trees.</p>
<p>To address this gap, we curate and release CS-TAXOBENCH, a comprehensive benchmark that is designed for the task of scholarly taxonomy generation (Section 3).Our benchmark consists of 460 human-written taxonomies (accompanied by their corresponding reference papers) that have been extracted from survey articles published in Computer Science journals in 2020 − 2024.We also curate an additional test set made up of 80 taxonomies extracted from conference survey papers.</p>
<p>We further develop TAXOALIGN, our own intuitive LLM-based pipeline for generating taxonomies (Section 4).TAXOALIGN consists of three parts: Knowledge Slice Creation, Taxonomy Verbalization and Taxonomy Refinement.We compare our method with a range of baselines to show the effectiveness of our method.Figure 1 demonstrates our proposed framework, while Figure 2 (right) shows an example of a taxonomy generated using TAXOALIGN.</p>
<p>Finally, we present an automated evaluation framework for the comparison of taxonomy-tree structures (Section 5).For this purpose, we develop two metrics of our own -the average degree score metric to judge structural similarity and the level-order traversal comparison metric to judge semantic similarity.In addition, we adopt soft recall and entity recall metrics, originally proposed for evaluating outline generation, to assess the quality of the generated taxonomies (Fränti and Mariescu-Istodor, 2023;Shao et al., 2024;Kang and Xiong, 2025).We also employ LLM-as-a-judge for qualitative evaluation.TAXOALIGN outperforms all baselines across nearly all metrics, including human evaluation.</p>
<p>To facilitate future research, we make our code and dataset publicly available at https://github.com/AvishekLahiri/TaxoAlign.</p>
<p>Related Work</p>
<p>Taxonomy Construction.Taxonomy learning has been attempted in NLP through the decades by captalizing on the semantic relations in text (Hearst, 1992;Pantel and Pennacchiotti, 2006;Suchanek et al., 2006;Ponzetto and Strube, 2011;Rios-Alvarado et al., 2013;Dietz et al., 2012;Liu et al., 2012;Diederich and Balke, 2007;Wang et al., 2010;Kang et al., 2016;Kozareva and Hovy, 2010;Velardi et al., 2013).Recent approaches such as HiGTL (Hu et al., 2025) and the method of Martel and Zouaq (2021) introduce graph-and clusteringbased techniques for taxonomy learning.Most of these methods use pattern-based or clusteringbased methods, whereas TAXOALIGN leverages the power of LLMs to construct taxonomies in the scientific domain.</p>
<p>Scientific Survey Generation and Knowledge</p>
<p>Synthesis.Recently, there has been some interest among researchers to generate surveys from a corpus of research papers.AutoSurvey (Wang et al., 2024), SURVEYX (Liang et al., 2025), Re-searchArena (Kang and Xiong, 2025), Qwen-long (Lai et al., 2024) and SURVEYFORGE (Yan et al., 2025) are some of the prominent techniques proposed for this task.In literature-based knowledge synthesis, LLMs have been used to generate scientific leaderboards ( Şahinuç et al., 2024;Timmer et al., 2025), literature review tables (Newman et al., 2024), or to synthesize biomedical evidence in the format of forest plots (Pronesti et al., 2025a,b).Our work focuses on scientific survey taxonomy generation and contributes to the broader agenda of AI for Science (Eger et al., 2025).</p>
<p>CS-TAXOBENCH</p>
<p>Overview</p>
<p>In graph theory, a tree is defined as an undirected connected graph with no cycles.A taxonomy tree T is a tree in which the root represents the taxonomy topic and the child nodes represent sub-topics and grandchild nodes represent more fine-grained topics.Survey papers typically propose a taxonomy which is expanded upon in the sections and sub-sections of the paper.Therefore, in most cases, the structure of the paper closely mirrors the nodes and connections in the taxonomy tree.We leverage this pattern to extract scholarly taxonomies, using our taxonomy extraction module to first derive outlines from survey papers and then analyze them to construct the final taxonomy.</p>
<p>Desiderata</p>
<p>We list out the desiderata we used for selecting taxonomies for inclusion in our benchmark dataset.Our overall goal was to ensure that the selected taxonomies are of high-quality with a logical flow and each node is grounded in a set of reference papers.We decide on the following desiderata for curating our dataset: (1) each taxonomy should be based on a specific research topic, and the taxonomy should provide optimal coverage of the given topic;</p>
<p>(2) the taxonomies should be human-made, i.e., they should not be generated artificially; (3) the taxonomy trees should be multi-layered, i.e., they should have at least two levels.</p>
<p>Automated Taxonomy Extraction</p>
<p>Data Source Selection: Survey papers form a rich resource for taxonomies in scientific literature.Therefore, we select survey papers as our primary source of data.To mitigate the risk of data contamination while ensuring that the papers in consideration are of high quality, we select survey papers from "ACM Computing Surveys" (ACM CSUR), which is a highly reputed journal in the field of Computer Science research with an Impact Factor of 23.8.1 This is one of the top venues in Computer Science that publishes survey papers relating to the areas of computing research and practice.</p>
<p>Research Paper Selection: We select a time frame of five years between 2020 to 2024 for the purpose of the creation of our dataset.A total of 1,165 papers were accepted in ACM CSUR during this period, of which 325 are open-access and 285  have copies available on arXiv. 2 Due to licensing restrictions, we include only the open-access and arXiv articles in our study.</p>
<p>Filtering: We use Docling (Team, 2024) to extract text from the research paper PDFs.Since arXiv papers are not always in a consistent format, we remove those with noisy layouts or Docling parsing errors.After filtering, 499 papers remain.</p>
<p>Reference Paper Matching: We retrieve the abstracts of the reference papers by parsing data from Semantic Scholar 3 , which hosts approximately 214 million research documents.If fewer than 50% of a survey paper's references are available on Semantic Scholar, we exclude that paper from the final version of our proposed datasets.Following this criterion, 39 out of the original 499 papers are excluded, leaving us with a final set of 460 papers.The average percentage of available citations in the final set of papers is shown in Table 1.</p>
<p>Taxonomy Extraction: We extract the headings, subheadings, and sub-subheadings from the retrieved text of the survey paper to construct the taxonomy.The title of the survey paper is treated as the overall taxonomy topic.We discard all headings that contain terms such as "Introduction", "related work", "problem formulation", "summary", "conclusion", "result", "future", "discussion", "background" and "overview" as they do not contribute to the core taxonomy.We also remove those nodes for which we cannot extract reference papers from Semantic Scholar.</p>
<p>Statistics</p>
<p>Dataset Statistics</p>
<p>Our entire benchmark contains 460 taxonomies along with the reference papers that are used to build each taxonomy.The details about the statistics of our benchmark are present in Table 1.On average, there are around 131 reference papers for each taxonomy in our benchmark.</p>
<p>Manual Annotation</p>
<p>To evaluate the taxonomy extraction method, we manually annotate a set of 10 taxonomy trees from as many survey papers.We use a Python package, TreeLib4 , to annotate these taxonomies from their respective survey papers.The papers were selected such that there were explicitly-defined taxonomies in them.To compare the annotated and generated trees, we compare the paths from each node to the root node.Each path is treated as an individual element.The precision, recall and F1 between the annotated and extracted taxonomies were found to be 83.92%,94.35% and 88.83% respectively, thereby demonstrating a high degree of correlation.</p>
<p>The only errors originated due to some general section headers like "Two sea changes in Natural Language Processing" (Liu et al., 2023), which were not present in the annotated trees.</p>
<p>Additional Test Set: Survey Papers in Conferences</p>
<p>We create an additional new test set from conference survey papers for testing on a different distribution of survey papers.For this purpose, we chose survey papers published in 2024 in IJCAI (which has a dedicated survey track) and all the ACL* conferences (ACL, NAACL, EMNLP and EACL).There are a total of 86 survey papers in these conferences in 2024.We carry out our proposed filtering strategies, which narrows down the total number of papers to 80. Therefore, we have 80 taxonomy trees with an average of about 71 reference papers for each tree.This set is solely used as a test set.</p>
<p>TAXOALIGN 4.1 Task Formulation</p>
<p>Given a corpus of documents D and a topic t, the task is to automatically construct a hierarchical taxonomy tree T whose nodes represent relevant topics and subtopics derived from D. The goal for T is to comprehensively and meaningfully categorize the information contained within the entire corpus that relates to topic t.</p>
<p>To solve this task, we propose a method TAXOALIGN, comprising three components: Knowledge Slice Creation, Taxonomy Verbalization and Taxonomy Refinement. Figure 4 shows the three-stage pipeline of our proposed method.</p>
<p>Knowledge Slice Creation</p>
<p>In this step, we use a LLM to identify segments of text within each research article that are highly relevant to the taxonomy topic.We refer to these segments as knowledge slices.</p>
<p>This stage helps in extracting information that guides the model in later steps about the topics and subtopics present in the cited papers related to the taxonomy.More importantly, the number of cited papers in a survey paper is quite extensive, thereby making it quite impossible to fit all the papers in the model's input context window.This step ensures that all the cited papers can be accommodated within the context length of recent LLMs.</p>
<p>Taxonomy Verbalization</p>
<p>We opt for the instruction tuning of an LLM with the most pertinent taxonomy topic-related information from the reference papers that has been extracted in the previous stage.</p>
<p>The main objective is to teach the model to generate meaningful and concise taxonomies which are grounded in the given information, and most importantly, teach the model to learn the structure of the taxonomy trees.Finetuning helps in preserving the structure of the taxonomy in a major way, which is a feature that is lacking in the direct prompting-based methods.</p>
<p>Taxonomy Refinement</p>
<p>The verbalization phase is followed by a refinement stage, which evaluates and refines the connections between the parent and the child nodes.It checks whether each node is grounded in the document knowledge slices.If the tree contains too few nodes, it expands the node set to achieve a greater coverage of the documents using their corresponding knowledge slices.This refinement strategy is executed by prompting an LLM with stronger reasoning capabilities than those used in the previous stages.</p>
<p>Evaluation</p>
<p>We present both the new evaluation approaches we have developed and the existing methods used in this domain.The main challenge in comparing a generated tree with the gold tree lies in aligning the two structures.The two trees should be structurally similar as well as semantically aligned.However, alignment is challenging because the trees may differ significantly in their hierarchical structures or exhibit low lexical overlap, as has been encountered in similar problems such as table schema alignment evaluation (Newman et al., 2024).</p>
<p>Average Degree Score</p>
<p>The first condition for two trees to be considered similar is that their structures should be similar.We design this metric to judge the structural similarity of the generated tree and the gold standard tree.</p>
<p>In a graph, the average degree is calculated as the average number of edges connected to a node in the graph.For any tree with N nodes, the number of edges is N − 1, which gives the average degree of 2(N − 1)/N .Therefore, to judge the structural similarity between the gold standard tree T and the predicted tree T ′ , we find the average degree score ∆, which is the ratio between the average degree of T ′ and the average degree of T .
∆ = m i=1 d(T ′ i ) n i=1 d(T i )(1)
where, d(t) represents the degree of a node t, and m and n are the number of nodes in the trees T and T ′ respectively.In the ideal case, the value of ∆ should be 1.If the generated tree T ′ is more branched out than the original tree T , then the value of ∆ is greater than 1, while if T ′ is less branched out than it should be, then the value of ∆ is less than 1.We report the final score as the mean of all the average degree scores in the test corpus.</p>
<p>Level-order Traversal Comparison</p>
<p>The hierarchical structure of a tree makes it difficult to implement standard text generation metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), or BERTScore (Zhang et al., 2019).Therefore, we propose a metric to compare the gold standard tree and the generated tree using level-order traversal.More specifically, we traverse the tree in such a way that all the nodes present in the same level are traversed completely before traversing the next level.After converting the entire tree into a single list through level-order traversal, we calculate the the corpus-level BLEU-2, ROUGE-L and BERTScore.</p>
<p>Node Soft Recall and Node Entity Recall</p>
<p>We use the metrics Node Soft Recall and Node Entity Recall, following the evaluation protocol in prior work (Fränti and Mariescu-Istodor, 2023;Shao et al., 2024;Kang and Xiong, 2025).These metrics compare the generated and the ground-truth taxonomy trees using semantic similarity and lexical overlap between them, respectively.Node Soft Recall (NSR) is dependent on soft cardinality of a tree (Jimenez et al., 2010), which is given by,
c(T ) = n i=1 1 n n j=1 Sim(T i , T j )(2)
where Sim(T i , T j ) is the cosine similarity between the SENTENCE-BERT (Reimers and Gurevych, 2019) embeddings of the taxonomy trees T i and T j .The Node Soft Recall between two trees T and T ′ is defined as,
NSR(T, T ′ ) = c(T ) + c(T ′ ) − c(T ∪ T ′ ) c(T ′ ) (3)
Since in most baselines, there is a large mismatch between the number of nodes of the generated taxonomy tree and the original tree, we tweak the original heading soft recall in Shao et al. (2024) by inserting a normalizing factor in Eq. 2, i.e., we divide by the number of nodes to offset the effect of a large node count in the tree.</p>
<p>Node Entity Recall (NER) between the gold standard tree T and the generated tree T ′ is defined as the percentage of entities that are present both in T and T ′ .Formally, it can be expressed as,
NER(T, T ′ ) = |Ent(T ) ∩ Ent(T ′ )| |Ent(T )| (4)
where |Ent(T )| represents the number of entities in T .In our case, we track Noun Phrases (NP) for better coverage.We use the chunking model from FLAIR (Akbik et al., 2019) for this purpose.</p>
<p>LLM-as-a-Judge</p>
<p>We prompt a stronger LLM, GPT-4.1, with the gold-standard taxonomy tree and the generated taxonomy tree, and ask it to evaluate the generated tree on a scale of 1 to 5 based on the structural similarity and the semantic similarity of the two trees.The LLM judges whether the generated tree aligns with the gold tree and whether they are coherent.The prompt is described in Appendix B.</p>
<p>Baselines</p>
<p>In this section, we present the baselines against which we evaluate our method TAXOALIGN.</p>
<p>AutoSurvey (Wang et al., 2024): The outline generation part of this method randomly divides the reference papers into several groups, which results in the creation of multiple outlines.The language model then amalgamates these outlines to construct a single comprehensive outline.For a fair comparison with our method, we provide the reference papers, in contrast to the original work.We run only the outline generation step of AutoSurvey, instead of generating the whole article.</p>
<p>STORM (Shao et al., 2024): The pre-writing stage of this method involves researching the given topic through simulated conversations.A draft outline is generated initially by prompting the LLM with the topic only.To generate the final outline, the LLM is prompted with the topic, simulated conversations as well as the draft outline.Like in AutoSurvey, here also we provide the pipeline with the reference papers.</p>
<p>Topic only: In this baseline, we simply prompt a LLM with the taxonomy topic and ask it to generate a corresponding tree that best fits the topic.The primary goal is to evaluate how effectively the model can generate such a tree using only its parametric knowledge.</p>
<p>Topic + Keyphrases: We use a LLM to extract the top keyphrases from each of the reference (cited) papers that form the basis of the taxonomy.This provides the top phrases of each paper, enabling us to fit the essential content of all references within the limited context window of LLMs.We then prompt another LLM to generate the taxonomy tree based on the set of keyphrases.</p>
<p>TAXOALIGN w/o Taxonomy Verbalization w/o Taxonomy Refinement: We use the knowledge slices used in our proposed method.We simply prompt the model to generate the taxonomy tree based on these slices.This allows us to isolate and assess the effect of the latter stages of our own method, specifically those that occur after knowledge-slice creation.</p>
<p>7 Experimental Setup</p>
<p>Base Models</p>
<p>We use the open-domain Mistral-7B-Instruct-v0.3 (Jiang et al., 2023) and Meta-Llama-3-8B-Instruct (Grattafiori et al., 2024) for extracting the knowledge slices.In the taxonomy verbalization phase, we finetune Llama-3.1-Tülu-3-8B(Lambert et al., 2025) and SciLitLLM1.5-7B(Li et al., 2024) respectively.For the refinement stage, we use a closed-domain model GPT-4o-mini.For the prompting stage in the TAXOALIGN w/o Taxonomy Verbalization w/o Taxonomy Refinement baseline, we also test using three large open-source reasoning models (Qwen's QwQ-32B (Team, 2025b), DeepSeek-AI's DeepSeek-R1-Distill-Qwen-32B (DeepSeek-AI, 2025) and NovaSky-AI's Sky-T1-32B(Team, 2025a)) using the previously generated knowledge slices.Prompts and model details are present in Appendix A. We choose the Tülu and the SciLitLLM models for instruction tuning in TAXOALIGN as well as prompting because they include extensive scientific research-related data in their pre-training or continual pre-training corpus.</p>
<p>Hyperparameter Choices</p>
<p>In the taxonomy verbalization stage, we instructiontune LLMs using QLoRA (Dettmers et al., 2023).QLoRA uses 4-bit NormalFloat, Double Quantization and Paged Optimizers on the LoRA finetuning approach (Hu et al., 2022).Each language model is instruction-tuned for 800 steps with an input context window of 16, 384 and a output context window of 1, 024.The learning rate is 2e − 4 and the training batch size is 1.For instructiontuning, we use simple intuitive prompts based on training data from CS-TAXOBENCH with Alpacalike5 instruction format (Taori et al., 2023).The instruction format is given in the Appendix B.1.We instruct LLMs in our experiments to generate taxonomies with a maximum depth of three.For the taxonomy verbalization part in our method or in any of the baselines, we set 1, 024 as the maximum number of new tokens to be generated by the model.All experiments are done on a single A100.</p>
<p>Results and Analysis</p>
<p>We evaluated our method and the baseline methods using the proposed metrics.We summarize the results of our experiments in Table 2.Additional results with more models are in Table 5 of Appendix C. We find that the TAXOALIGN w/o Taxonomy Verbalization w/o Taxonomy Refinement baseline performs second best to our method on most metrics.This indicates that the knowledge slices are an important tool for this task.This baseline has high average degree score compared to our method that reveals that our Taxonomy Verbalization and Refinement stages could effectively reduce the gap between the generated and gold taxonomy trees while enhancing the quality of node labels.In our experiments, we find that the BLEU-2, ROUGE-L and BERTScore values are much less than what we typically encounter in tasks like machine translation or question answering.This suggests substantial scope for improvement in this task, as a huge gap remains between the human-created and machine-generated results.</p>
<p>Structural Similarity</p>
<p>Our method consistently achieves an average degree score (∆) close to 1, while the ∆ value obtained by other baseline methods are much higher.This indicates that our generated tree is much closer  to the human-written taxonomy tree in terms of structure.We observe that the ∆ value is closest to 1 for the Topic-only baseline.This is mainly because when provided with only the topic, the language model generates a small tree due to lack of parametric knowledge, as has been established by the consistent low scores for this baseline on the rest of the metrics.Other baseline methods tend to produce overly large trees (∆ &gt; 2.9) with an excessive number of nodes and branches, increasing the likelihood of hallucinations and structural divergence from the gold-standard tree.We show examples of generated taxonomies using AutoSurvey and STORM in comparison with TAXOALIGN in Figures 5 and 6 respectively in Appendix D.</p>
<p>Semantic Similarity</p>
<p>In terms of the level-order traversal, we observe that our method produces comprehensively better BLEU-2, ROUGE-L and BERTScore in all the cases when compared with the baselines.We observe that all baselines have similarly low scores for level-order traversal, indicating that the generated nodes exhibit low lexical overlap with the gold data.In comparison, our method produces more coherent labels and nodes in the taxonomy tree.</p>
<p>In terms of Node Soft Recall (NSR), our method performs better than the other baselines, showing the similarity between the generated and gold node labels.The TAXOALIGN w/o Taxonomy Verbaliz.w/o Taxonomy Refine.baseline performs better in some cases in terms of the Node Entity Recall metric, which is mainly because this baseline generates large trees, as has been demonstrated by the ∆ value, and larger trees contribute to greater match in the Noun Phrase chunks.Using LLM-as-a-judge, TAXOALIGN also outperforms the baselines, reaffirming the metric-based results and showing that it generates taxonomies closer to the human-written ones in both structure and intent.</p>
<p>Testing with Conference Survey Papers</p>
<p>We tested our three-phase pipeline (that had been trained on 400-instance training data) on the new test set introduced in Section 3.6, the results for which are given in Table 3. Extended results with more models are present in Table 6 of Appendix C. We observe that our method comfortably outperforms the baseline on this test set too, which is consistent with the results reported in Table 2.</p>
<p>Error Analysis</p>
<p>We present an error analysis based on a manual evaluation of instances from the test set in Table 1 using our proposed TAXOALIGN pipeline.For illustration, we provide a example for each of the three stages of the pipeline in Figure 7 of Appendix E. Below, we summarize the common errors observed at each stage of the pipeline.</p>
<p>Knowledge Slices + Prompting: Direct generation from knowledge slices creates more verbose taxonomies that contain irrelevant information leading to the nodes not being very specific or not pertaining to the topic directly.Another major factor is the presence of repeated numbers of the same nodes or sub-trees in the taxonomy.</p>
<p>Knowledge Slices + Taxonomy Verbalization: Structurally, the taxonomies are closer to the gold standard taxonomies, but there are some factual errors that persist.We observe that the generated taxonomies suffer from the problem of hallucinated node labels or are too short.</p>
<p>Knowledge Slices + Taxonomy Verbalization + Taxonomy Refinement: The generated trees are more aligned to the gold standard trees in terms of structure and semantic coherence.Still, the generated trees suffer from a low number of layer-wise exact matches.The generated trees are certainly more interpretable than the previous stages and the overall tree also presents a coherent structure.</p>
<p>Method</p>
<p>Structure Content TAXOALIGN 3.17 2.62 AutoSurvey 2.17 2.25</p>
<p>Human Evaluation</p>
<p>We use human evaluation to complement the automated framework.Three annotators with domain knowledge were asked to rate the surveys generated by TAXOALIGN and AutoSurvey.The annotators are instructed to assess based on (1) the structural commonalities between the gold and generated taxonomy trees and then (2) the semantic coherence of the generated tree with respect to the gold tree.</p>
<p>The evaluation is done using a 5-point Likert scale on 20 randomly sampled data instances from the test set of CS-TAXOBENCH.The inter-annotator agreement is calculated as 0.61 and 0.73 (Krippendorff's α).The results are shown in Table 4.The mean ratings show TAXOALIGN outperforms AU-TOSURVEY in both structural and content quality.</p>
<p>To verify the consistency between our LLM-as-ajudge evaluation and the human evaluation, we first average the scores assigned by human annotators for each taxonomy tree.We then compare these with the LLM-generated scores using Spearman's rank correlation coefficient.Thereby, we obtain a Spearman's rho value of 0.527 which indicates a strong positive correlation.These results suggest that our LLM-as-a-judge evaluation method aligns well with human preferences, providing a reliable proxy for human judgment.</p>
<p>Conclusion</p>
<p>Automating scholarly taxonomy generation can help researchers and practitioners efficiently navigate the vast body of scholarly literature.To facilitate this, we present CS-TAXOBENCH, a benchmark comprising 460 taxonomies from journey surveys and 80 from conference surveys, along with TAXOALIGN, a method that uses instruction tuning and refinement on topic-related information extracted from papers.We introduce two new metrics and show that TAXOALIGN outperforms the baselines on most evaluation measures.</p>
<p>Limitations</p>
<p>We construct CS-TAXOBENCH from a single journal within a defined time frame to ensure consistency in taxonomy quality.However, additional open-access journals and conference venues could also be explored for future curation.</p>
<p>We do not focus on the retrieval of the reference papers from a corpus of papers.While this is an important task for end-to-end taxonomy construction given only the taxonomy topic, we focus more on creating and evaluating taxonomies when provided with a set of reference documents.</p>
<p>Although we improve the structure and semantic coherence between the human-written and the generated taxonomies using TAXOALIGN, there is a a lot of scope for improvement in this field.Therefore, this is a encouraging field of work in which the community can work in the coming days.</p>
<p>A Our Method -Prompts and Models</p>
<p>A.1 Models</p>
<p>• Mistral-7B-Instruct-v0.3 (Jiang et al., 2023):</p>
<p>The Mistral group of models leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost.</p>
<p>Compared to version 0.2, this model can process and respond more effectively to diverse tasks and instructions, owing to its expanded vocabulary of 32,768 tokens and support for the v3 tokenizer.The model can carry out operations that call for outside data since it supports function calling.</p>
<p>• Meta-Llama-3-8B-Instruct (Grattafiori et al., 2024): Llama is a family of pre-trained foundational language models that have been opensourced by Meta in recent times.The Meta-Llama-3-8B-Instruct is trained on a mix of publicly available online data with a knowledge cutoff of March, 2023.The tuned versions of Llama3 use Supervised Fine-Tuning (SFT) and Reinforcement Learning with Human Feedback (RLHF) to align with human preferences.</p>
<p>• Llama-3.1-Tülu-3-8B(Lambert et al., 2025): Tülu (Wang et al., 2023) is a set of models that are instruction-tuned on LLaMA (Touvron et al., 2023) using a mixture of publicly available, synthetic and human-created datasets.Building upon the Llama 3.1 basic models, Tülu-3 (Lambert et al., 2025) models are trained using Direct Preference Optimization (DPO), Supervised Fine-Tuning (SFT), and a technique called Reinforcement Learning with Verifiable Rewards (RLVR).</p>
<p>• SciLitLLM1.5-7B (Li et al., 2024): It is a very recently released LLM designed for the task of scientific literature understanding that has been trained using both Continual Pre-Training (CPT) and Supervised Fine-Tuning (SFT).This strategy is used on Qwen2.5 to obtain SciLitLLM.The CPT stage uses 73,000 textbooks and 625,000 academic papers, while the SFT stage uses SciLitIns, SciRIFF (Wadden et al., 2024) and Infinity-Instruct6 .We use the SciLitLLM 7B7 for our experimental purposes.</p>
<p>• QwQ-32B (Team, 2025b): QwQ is designed for complex problem-solving and logical reasoning tasks and is based on Qwen2.5.The model is text-only and focuses on tasks like multi-step reasoning, complex decisionmaking, and research assistance.</p>
<p>• DeepSeek-AI's DeepSeek-R1-Distill-Qwen-32B (DeepSeek-AI, 2025): DeepSeek-R1-Distill-Qwen-32B is an open-source, distilled large language model (LLM) based on the Qwen2.5 32B architecture, utilizing the knowledge from the DeepSeek-R1 reasoning model.It is optimized for language understanding, reasoning, and text generation tasks and is known for outperforming other opensource models, including OpenAI's o1-mini, on various industry benchmarks.</p>
<p>• Sky-T1-32B (Team, 2025a): This model has been developed by the NovaSky team at UC Berkeley.It excels in mathematical and coding reasoning, outperforming some advanced closed-source models and other open-source alternatives on various benchmarks.The model was created by fine-tuning the Qwen 2.5 32B instruct model with a high-quality, 17,000-item dataset.</p>
<p>A.2 Knowledge Slice-Prompt</p>
<p>You will receive a document and a topic.Your task is to identify the knowledge-slices within the document that are very relevant to the given topic.A knowledge-slice is a piece of information representing the highlights of the document related to the given topic i.e. each knowledge-slice should be such that it both represents an important point in the document, but at the same time, the knowledgeslice should pertain closely to the given topic.Also, the knowledge-slice should not represent any additional information that is not present in the document.</p>
<p>A.3 Taxonomy Verbalization-Prompt</p>
<p>A taxonomy is a tree-structured semantic hierarchy that establishes a classification of the existing literature under a common topic.You will receive a taxonomy topic along with a collection of documents.Your task is to create a taxonomy tree using the given topic and based on the highlights of the documents i.e. create new child nodes by identifying generalizable sub-level topics from the document highlights that can act as child nodes to the taxonomy topic, which acts as the root node .The taxonomy tree should be created such that it looks as if all the given documents are a part of the taxonomy.There may be several levels in the tree i.e. each node may contain child nodes, but the total depth of the tree should not exceed three.The topics in all the levels of the tree except the last level must not be too specific so that it can accommodate future sub-topics i.e. child nodes.</p>
<p>-The nodes at the last level of the hierarchy i.e. the leaf nodes should reflect a single topic instead of a combination of topics.</p>
<p>-Each node label is a small and concise phrase.</p>
<p>[Response Format Instructions] -The output tree is to be formatted as shown in the example such that the root node is the taxonomy topic and each child node is connected to its parent.</p>
<p>A.4 Taxonomy Refinement-Prompt</p>
<p>A taxonomy is a tree-structured semantic hierarchy that establishes a classification of the existing literature under a common topic.You will receive a taxonomy tree along with a collection of documents.The root node of the taxonomy tree is the overall taxonomy topic.Your task is to refine the taxonomy tree such that there is a clear connection between the parent node and the subsequent child nodes.Each node must be a well-defined topic that is grounded in the input document highlights.</p>
<p>B LLM-as-a-Judge Prompt</p>
<p>A taxonomy is a tree-structured semantic hierarchy that establishes a classification of the existing literature under a common topic.You are given a gold standard taxonomy tree and a generated taxonomy tree and your task is to respond with an appropriate score after comparing the two.Two taxonomy trees are said to be structurally similar if the number of nodes and branches are similar in number.</p>
<p>If one tree has too many or too less nodes and branches than the gold tree, then they are said to be structurally dissimilar.Two taxonomy trees are said to be semantically similar if their nodes have values with close meanings or are matching entirely.Please respond with only the score based on the following criteria: Score 1: The generated taxonomy has no similarity at all with the gold standard taxonomy i.e. the structure and the intent of the generated taxonomy is totally different from that of the gold standard taxonomy.Score 2: The generated taxonomy have only a few nodes that has a semantic match with the nodes in the gold standard taxonomy and the structure of the generated taxonomy is a little similar to that of the gold standard taxonomy.The structure of the generated tree is very less similar to the gold standard tree but the intent of both taxonomies is similar.Score 3: The generated taxonomy has a reasonable similarity to the generated taxonomy in terms of structural similarity and semantic similarity.The structure of both trees are similar but some nodes are different in the two taxonomies.Score 4: The generated taxonomy has good logical consistency with that of the gold standard taxonomy in terms of semantic matching of the nodes between the two with the structure of the generated taxonomy is very similar to that of the gold standard taxonomy.</p>
<p>The two taxonomies only differ for a small number of instances.Score 5: The generated taxonomy is fully similar in terms of semantic matching and structure to the gold standard taxonomy.</p>
<p>C Extended Results</p>
<p>We show additional results using a expanded set of models on the original test set and the additional conference paper test in Tables 5 and 6 respectively.</p>
<p>D Output Example Comparison</p>
<p>We see in Figure 5 and 6 that the taxonomy trees generated using TAXOALIGN are much less verbose than the corresponding taxonomy trees generated using AutoSurvey or STORM.</p>
<p>E Error Analysis</p>
<p>We show an example of the results obtained in the three stages of our TAXOALIGN pipeline in Figure 7.The stages are Knowledge Slices + Prompting, Knowledge Slices + Taxonomy Verbalization and Knowledge Slices + Taxonomy Verbalization + Taxonomy Refinement.</p>
<p>Gold Standard Taxonomy AutoSurvey Human Image</p>
<p>Figure 1: Schematic representation of TAXOALIGN: (1) Knowledge Slice Creation (2) Taxonomy Verbalization (3) Taxonomy Refinement</p>
<p>DATA-DRIVEN METHODS ON HUMAN IMAGE GENERATION | |--Method Taxonomy Based on Fundamental Models | |--Method Taxonomy Based on Task Settings | +--Main Components in Data-Driven Methods |--HYBRID METHODS |--KNOWLEDGE-GUIDED METHODS ON | | HUMAN IMAGE GENERATION | |--Fundamental Models of Knowledge-Guided Methods |</p>
<p>Figure 3 :
3
Figure 3: An overview of the pipeline for the curation of our dataset.</p>
<p>Figure 4 :
4
Figure 4: An overview of the proposed TAXOALIGN pipeline.</p>
<p>the relevant knowledgeslices in the form of a list enclosed within square brackets.Your response should be in the following format: [Knowledge-Slices] [Knowledge-Slice 1, Knowledge-Slice 2,..., Knowledge-Slice n] [Your response]</p>
<p>DATA-DRIVEN METHODS ON HUMAN IMAGE GENERATION | |--Method Taxonomy Based on Fundamental Models | |--Method Taxonomy Based on Task Settings | +--Main Components in Data-Driven Methods |--HYBRID METHODS |--KNOWLEDGE-GUIDED METHODS ON | | HUMAN IMAGE GENERATION | |--Fundamental Models of Knowledge-Guided Methods | Introduction to Human Image Generation |--Key Techniques in Human Image Generation |--3D Human Reconstruction | |--Overview of 3D Human Reconstruction | |--Methods for 3D Reconstruction from 2D Images | |--Pixel-Aligned Implicit Functions | |--High-Frequency and Low-Frequency Information | |--Semantic Parsing and Texture Generation | |--Evaluating Reconstruction Quality | |--Challenges in Real-World Applications | +--Future Directions in 3D Reconstruction Research |--Image-to-Image Translation for Human Generation |--Virtual Try-On Systems | |--Overview of Virtual Try-On Systems | |--LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On | |--GP-VTON: Towards General Purpose Virtual Try-On | |--M3D-VTON: A Monocular-to-3D Virtual Try-On Network | |--Single-Stage Virtual Try-On with Deformable Attention Flows | |--Challenges in Virtual Try-On Systems | +--Future Directions for Virtual Try-On Technologies |--Pose Transfer and Human Animation |--The Role of Deep Learning Models |--Evaluation Metrics and Datasets +--Future Directions and Challenges |--Challenges in Handling Occlusions |--Improving Generalization Across Diverse Scenarios |--Ethical Implications of Generated Content |--Advances in Multi-Modal Conditioned Generation |--Future Directions in Technology Integration +--Academic and Industrial Partnerships</p>
<p>Figure 5 :
5
Figure5: A comparison of a gold standard taxonomy tree and a generated taxonomy tree using AutoSurvey.</p>
<p>Figure 6 :
6
Figure6: A comparison of a gold standard taxonomy tree and a generated taxonomy tree using STORM.</p>
<p>ACM CSUR papers in 2020-24 (1165) Open Access journal papers (325) + Present in Arxiv (285)</p>
<p>Table 2 :
2
Results of our method, TAXOALIGN, compared with AutoSurvey, STORM, Topic-only, Topic+Keyphrases and TAXOALIGN w/o Taxonomy Verbalization w/o Taxonomy Refinement, on the original test set.
MethodModel∆Level-order Traversal BLEU-2 ROUGE-L BERTScoreNSRNERLLM judgeAutoSurveyPrompt: GPT-4o-mini4.46590.00160.17840.82561.0903 0.1982 2.4333STORMPrompt: GPT-4o-mini6.1510.00120.13490.81661.0727 0.1539 2.2000Topic onlyPrompt: Tülu1.42740.00520.23590.83761.4187 0.1373 2.0833Topic + KeyphrasesKeyphrase: LLaMa; Prompt: Tülu Keyphrase: Mistral; Prompt: Tülu4.4517 4.910.0018 0.00140.1584 0.14320.8134 0.81001.1103 0.1491 2.4167 1.0996 0.1640 2.4167K-Slice: LLaMa; Prompt: Tülu5.4860.00370.1590.81230.9571 0.2074 2.4833TAXOALIGN w/o Tax. Verbaliz.K-Slice: Mistral; Prompt: Tülu6.11250.00290.14650.80871.0791 0.2197 2.4333w/o Tax. Refine.K-Slice: LLaMa; Prompt: Sky-T1-32B6.44860.00200.17610.81701.0804 0.2135 2.3966K-Slice: Mistral; Prompt: Sky-T1-32B7.19650.00220.19330.82211.0948 0.2103 2.4211K-Slice: LLaMa;T-Verbal.: Tülu;1.66870.01320.29750.85011.3244 0.1986 2.4167TAXOALIGNT-Refine.: GPT-4o-mini K-Slice: Mistral;T-Verbal.: Tülu;1.6680.00510.29740.85171.3635 0.1872 2.5000T-Refine.: GPT-4o-miniMethodModel∆Level-order Traversal BLEU-2 ROUGE-L BERTScoreNSRNERLLM judgeTAXOALIGN w/o Tax. Verbaliz. w/o Tax. Refine.K-Slice: LLaMa; Prompt: Tulu K-Slice: Mistral; Prompt: Tulu6.361 7.20830.0019 0.00340.1643 0.15980.8182 0.81591.0716 0.2683 2.275 1.0737 0.2653 2.2125K-Slice: LLaMa;T-Verbal.: Tulu;2.19240.00580.30910.85421.2129 0.2566 2.2875TAXOALIGNT-Refine.: gpt-4o-mini K-Slice: Mistral;T-Verbal.: Tulu;2.36170.0130.30040.85221.2072 0.27162.35T-Refine.: gpt-4o-mini</p>
<p>Table 3 :
3
Results of TAXOALIGN compared with TAXOALIGN w/o Taxonomy Verbalization w/o Taxonomy Refinement on the additional test set of conference papers.</p>
<p>Table 4 :
4
Mean ratings from human evaluation of the structure and content similarity for TAXOALIGN and AutoSurvey.</p>
<p>Do not alter the root node of the tree i.e. the taxonomy topic.Your task is to alter the other nodes only if deemed necessary i.e.only if a better viable replacement is found.Please try to adhere to the structure of the given taxonomy tree as much as possible.Only if the given taxonomy tree is restricted to less than five nodes, then generate the taxonomy tree on your own.Strictly adhere to the format of the tree shown here.
[Example Output]example-output[Taxonomy Topic]taxonomy-topic[Documents]Doc-1Doc-2Doc-3Please ONLY return the edited taxonomytree in the output format as shown in theexample above.[Your response]</p>
<p>Table 5 :
5
Results of our method compared with baselines like AutoSurvey, Topic-only, Topic+Keyphrases andTAXOALIGN w/o Taxonomy Verbalization w/o Taxonomy Refinement.
MethodModel∆Level-order Traversal BLEU-2 ROUGE-L BERTScoreNSRNERLLM judgeTAXOALIGN w/o Tax. Verbaliz. w/o Tax. Refine.K-Slice: LLaMa; Prompt: Tulu K-Slice: LLaMa; Prompt: SciLitLLM6.361 6.52210.0019 0.00260.1643 0.19570.8182 0.81831.0716 0.2683 2.275 1.2095 0.2267 1.9375K-Slice: Mistral; Prompt: Tulu7.20830.00340.15980.81591.0737 0.2653 2.2125K-Slice: Mistral; Prompt: SciLitLLM5.93870.00320.20390.82111.3243 0.2176 1.875K-Slice: LLaMa;T-Verbaliz.: Tulu;2.19240.00580.30910.85421.2129 0.2566 2.2875T-Refine.: gpt-4o-miniK-Slice: LLaMa;T-Verbaliz.: SciLitLLM;2.55510.01270.30340.8511.1927 0.2614 2.2625TAXOALIGNT-Refine.: gpt-4o-mini K-Slice: Mistral;T-Verbaliz.: Tulu;2.36170.0130.30040.85221.2072 0.27162.35T-Refine.: gpt-4o-miniK-Slice: Mistral;T-Verbaliz.: SciLitLLM;3.17790.00420.28450.84651.1806 0.267 2.3125T-Refine.: gpt-4o-mini</p>
<p>Table 6 :
6
Results of TAXOALIGN compared with TAXOALIGN w/o Taxonomy Verbalization w/o Taxonomy Refinement.</p>
<p>https://dl.acm.org/journal/csur
https://treelib.readthedocs.io/en/latest/
https://huggingface.co/datasets/tatsu-lab/ alpaca
https://huggingface.co/datasets/BAAI/ Infinity-Instruct
https://huggingface.co/Uni-SMART/SciLitLLM</p>
<p>Automated product taxonomy mapping in an e-commerce environment. Steven S Aanen, 10.1016/j.eswa.2014.09.032Expert Syst. Appl. 4232015Damir Vandic, and Flavius Frasincar</p>
<p>FLAIR: An easy-to-use framework for state-of-theart NLP. Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, Roland Vollgraf, 10.18653/v1/N19-4010Proceedings of the 2019 Conference of the North American Chapter. the 2019 Conference of the North American ChapterMinneapolis, MinnesotaAssociation for Computational Linguistics2019</p>
<p>SciAssess: Benchmarking LLM proficiency in scientific literature analysis. Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Wang Changxin, Zhifeng Gao, Hongshuai Wang, Li Yongge, Mujie Lin, Shuwen Yang, Jiankun Wang, Mingjun Xu, Jin Huang, Xi Fang, Jiaxi Zhuang, Yuqi Yin, Yaqi Li, Changhong Chen, Zheng Cheng, Zifeng Zhao, Linfeng Zhang, Guolin Ke, Findings of the Association for Computational Linguistics: NAACL 2025. Albuquerque, New MexicoAssociation for Computational Linguistics2025</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Deepseek-Ai , arXiv:2501.129482025Preprint</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>The semantic growbag algorithm: automatically deriving categorization systems. Jörg Diederich, Wolf-Tilo Balke, Proceedings of the 11th European Conference on Research and Advanced Technology for Digital Libraries, ECDL'07. the 11th European Conference on Research and Advanced Technology for Digital Libraries, ECDL'07Berlin, HeidelbergSpringer-Verlag2007</p>
<p>Taxolearn: A semantic approach to domain taxonomy learning. Emmanuelle-Anna Dietz, Damir Vandic, Flavius Frasincar, 10.1109/WI-IAT.2012.129IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology. 12012. 2012</p>
<p>Transforming science with large language models: A survey on ai-assisted scientific discovery, experimentation, content generation, and evaluation. Steffen Eger, Yong Cao, D' Jennifer, Andreas Souza, Christian Geiger, Stephanie Greisinger, Yufang Gross, Brigitte Hou, Anne Krenn, Yizhi Lauscher, Chenghua Li, Nafise Sadat Lin, Wei Moosavi, Tristan Zhao, Miller, arXiv:2502.051512025Preprint</p>
<p>Soft precision and recall. Pasi Fränti, Radu Mariescu-Istodor, 10.1016/j.patrec.2023.02.005Pattern Recognition Letters. 1672023</p>
<p>The distributional inclusion hypotheses and lexical entailment. Maayan Geffet, Ido Dagan, 10.3115/1219840.1219854Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05). the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05)Ann Arbor, MichiganAssociation for Computational Linguistics2005</p>
<p>Identification of tasks, datasets, evaluation metrics, and numeric scores for scientific leaderboards construction. Yufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, Debasis Ganguly, 10.18653/v1/P19-1513Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>TDMSci: A specialized corpus for scientific literature entity tagging of tasks datasets and metrics. Yufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, Debasis Ganguly, 10.18653/v1/2021.eacl-main.59Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnline. Association for Computational Linguistics2021</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>Taxonomy tree generation from citation graph. Yuntong Hu, Zhuofeng Li, Zheng Zhang, Chen Ling, Raasikh Kanjiani, Boxin Zhao, Liang Zhao, arXiv:2410.037612025Preprint</p>
<p>Understand short texts by harvesting and analyzing semantic knowledge. Wen Hua, Zhongyuan Wang, Haixun Wang, Kai Zheng, Xiaofang Zhou, 10.1109/TKDE.2016.2571687IEEE Transactions on Knowledge and Data Engineering. 2932017</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023Preprint</p>
<p>Text comparison using soft cardinality. Sergio Jimenez, Fabio Gonzalez, String Processing and Information Retrieval. Berlin, Heidelberg; Berlin HeidelbergSpringer2010and Alexander Gelbukh</p>
<p>Hao Kang, Chenyan Xiong, arXiv:2406.10291Researcharena: Benchmarking large language models' ability to collect and organize information as research agents. 2025Preprint</p>
<p>TaxoFinder: A Graph-Based Approach for Taxonomy Learning. Yong-Bin Kang, Pari Delir Haghigh, Frada Burstein, 10.1109/TKDE.2015.2475759IEEE Transactions on Knowledge &amp; Data Engineering. 28022016</p>
<p>A semisupervised method to learn and construct taxonomies using the web. Zornitsa Kozareva, Eduard Hovy, Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. the 2010 Conference on Empirical Methods in Natural Language ProcessingCambridge, MAAssociation for Computational Linguistics2010</p>
<p>Sebastian Köhler, Sandra C Doelken, Christopher J Mungall, Sebastian Bauer, Helen V Firth, Isabelle Bailleul-Forestier, Graeme C M Black, Danielle L Brown, Michael Brudno, Jennifer Campbell, David R Fitzpatrick, Janan T Eppig, Andrew P Jackson, Kathleen Freson, Marta Girdea, Ingo Helbig, Jane A Hurst, Johanna Jähn, Laird G Jackson, Anne M Kelly, David H Ledbetter, Sahar Mansour, Christa L Martin, Celia Moss, Andrew Mumford, Willem H Ouwehand, Soo-Mi Park, Erin Rooney Riggs, Richard H Scott, Sanjay Sisodiya, Steven Van Vooren, Ronald J Wapner, O M Andrew, Caroline F Wilkie, Wright, T Anneke, Vulto-Van Silfhout, 10.1093/nar/gkt1026The human phenotype ontology project: linking molecular biology and disease through phenotype data. B A Bert, Nicole L De Vries, Cynthia L Washingthon, Monte Smith, Paul Westerfield, Barbara J Schofield, Georgios V Ruef, Melissa Gkoutos, Damian Haendel, Suzanna E Smedley, Peter N Lewis, Robinson, 201342Nicole de Leeuw</p>
<p>Instruct large language models to generate scientific literature survey step by step. Yuxuan Lai, Yupeng Wu, Yidan Wang, Wenpeng Hu, Chen Zheng, arXiv:2408.078842024Preprint</p>
<p>Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James, V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A Smith, Yizhong Wang, arXiv:2411.15124Pradeep Dasigi, and Hannaneh Hajishirzi. 2025. Tulu 3: Pushing frontiers in open language model post-training. Preprint</p>
<p>Scilitllm: How to adapt llms for scientific literature understanding. Sihang Li, Jin Huang, Jiaxi Zhuang, Yaorui Shi, Xiaochen Cai, Mingjun Xu, Xiang Wang, Linfeng Zhang, Guolin Ke, Hengxing Cai, arXiv:2408.155452024Preprint</p>
<p>Surveyx: Academic survey automation via large language models. Xun Liang, Jiawei Yang, Yezhaohui Wang, Chen Tang, Zifan Zheng, Shichao Song, Zehao Lin, Yebin Yang, Simin Niu, Hanyu Wang, Bo Tang, Feiyu Xiong, Keming Mao, Zhiyu Li, arXiv:2502.147762025Preprint</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Lost in the middle: How language models use long contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, 10.1162/tacl_a_00638Transactions of the Association for Computational Linguistics. 122024</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, 10.1145/3560815ACM Comput. Surv. 9552023</p>
<p>Automatic taxonomy construction from keywords. Xueqing Liu, Yangqiu Song, Shixia Liu, Haixun Wang, 10.1145/2339530.2339754Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '12. the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '12New York, NY, USA2012Association for Computing Machinery</p>
<p>S2ORC: The semantic scholar open research corpus. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Daniel Weld, 10.18653/v1/2020.acl-main.447Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Octet: Online catalog taxonomy enrichment with self-supervision. Yuning Mao, Tong Zhao, Andrey Kan, Chenwei Zhang, Xin , Luna Dong, Christos Faloutsos, Jiawei Han, 10.1145/3394486.3403274Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD '20. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD '20New York, NY, USAAssociation for Computing Machinery2020</p>
<p>Taxonomy extraction using knowledge graph embeddings and hierarchical clustering. Félix Martel, Amal Zouaq, 10.1145/3412841.3441959Proceedings of the 36th Annual ACM Symposium on Applied Computing, SAC '21. the 36th Annual ACM Symposium on Applied Computing, SAC '21New York, NY, USAAssociation for Computing Machinery2021</p>
<p>End-to-end construction of NLP knowledge graph. Ishani Mondal, Yufang Hou, Charles Jochim, 10.18653/v1/2021.findings-acl.165Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>ArxivDI-GESTables: Synthesizing scientific literature into tables using language models. Benjamin Newman, Yoonjoo Lee, Aakanksha Naik, Pao Siangliulue, Raymond Fok, Juho Kim, Joseph Chee Daniel S Weld, Kyle Chang, Lo, 10.18653/v1/2024.emnlp-main.538Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USA2024Association for Computational Linguistics</p>
<p>A method for taxonomy development and its application in information systems. R Nickerson, U Varshney, J Muntermann, 10.1057/ejis.2012.26European Journal of Information Systems. 2232013</p>
<p>Espresso: Leveraging generic patterns for automatically harvesting semantic relations. Patrick Pantel, Marco Pennacchiotti, 10.3115/1220175.1220190Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics. the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational LinguisticsSydney, AustraliaAssociation for Computational Linguistics2006</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02. the 40th Annual Meeting on Association for Computational Linguistics, ACL '02USA. Association for Computational Linguistics2002</p>
<p>Taxonomy induction based on a collaboratively built knowledge repository. Paolo Simone, Michael Ponzetto, Strube, 10.1016/j.artint.2011.01.003Artificial Intelligence. 17592011</p>
<p>Query-driven document-level scientific evidence extraction from biomedical studies. Massimiliano Pronesti, Joao H Bettencourt-Silva, Paul Flanagan, Alessandra Pascale, Oisín Redmond, Anya Belz, Yufang Hou, 10.18653/v1/2025.acl-long.1359Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 63rd Annual Meeting of the Association for Computational LinguisticsVienna, AustriaAssociation for Computational Linguistics2025a1</p>
<p>Enhancing study-level inference from clinical trial papers via rl-based numeric reasoning. Massimiliano Pronesti, Michela Lorandi, Paul Flanagan, Oisin Redmon, Anya Belz, Yufang Hou, arXiv:2505.229282025bPreprint</p>
<p>Sentence-BERT: Sentence embeddings using siamese bertnetworks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Learning concept hierarchies from textual resources for ontologies construction. Ana B Rios-Alvarado, Ivan Lopez-Arevalo, J Victor, Sosa-Sosa, 10.1016/j.eswa.2013.05.005Expert Systems with Applications. 40152013</p>
<p>Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards. Furkan Şahinuç, Thy Thy Tran, Yulia Grishina, Yufang Hou, Bei Chen, Iryna Gurevych, 10.18653/v1/2024.emnlp-main.453Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Assisting in writing Wikipedia-like articles from scratch with large language models. Yijia Shao, Yucheng Jiang, Theodore Kanell, Peter Xu, Omar Khattab, Monica Lam, 10.18653/v1/2024.naacl-long.347Proceedings of the 2024 Conference of the North American Chapter. Long Papers. the 2024 Conference of the North American ChapterMexico City, MexicoAssociation for Computational Linguistics20241</p>
<p>Combining linguistic and statistical analysis to extract relations from web documents. Fabian M Suchanek, Georgiana Ifrim, Gerhard Weikum, 10.1145/1150402.1150492Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '06. the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '06New York, NY, USAAssociation for Computing Machinery2006</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford alpaca: An instruction-following llama model. 2023</p>
<p>10.48550/arXiv.2408.09869Deep Search Team. 2024. Docling technical report. Technical report</p>
<p>Sky-t1: Train your own o1 preview model within $450. Novasky Team, 2025a</p>
<p>Qwq-32b: Embracing the power of reinforcement learning. Qwen Team, 2025b</p>
<p>A position paper on the automatic generation of machine learning leaderboards. Yufang Roelien C Timmer, Stephen Hou, Wan, arXiv:2505.174652025Preprint</p>
<p>LLaMA: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, arXiv:2302.139712023Preprint</p>
<p>OntoLearn reloaded: A graph-based algorithm for taxonomy induction. Paola Velardi, Stefano Faralli, Roberto Navigli, 10.1162/COLI_a_00146Computational Linguistics. 3932013</p>
<p>Sciriff: A resource to enhance language model instruction-following over scientific literature. David Wadden, Kejian Shi, Jacob Morrison, Aakanksha Naik, Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom Hope, Luca Soldaini, Shannon Zejiang Shen, Doug Downey, Hannaneh Hajishirzi, Arman Cohan, arXiv:2406.078352024Preprint</p>
<p>A short survey on taxonomy learning from text corpora: Issues, resources and recent advances. Chengyu Wang, Xiaofeng He, Aoying Zhou, 10.18653/v1/D17-1123Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational Linguistics2017</p>
<p>Probabilistic topic models for learning terminological ontologies. Wei Wang, Payam Mamaani Barnaghi, Andrzej Bargiela, 10.1109/TKDE.2009.122IEEE Transactions on Knowledge and Data Engineering. 2272010</p>
<p>Autosurvey: Large language models can automatically write surveys. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, Yue Zhang, Advances in Neural Information Processing Systems. Curran Associates, Inc202437</p>
<p>How far can camels go? exploring the state of instruction tuning on open resources. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Raghavi Khyathi, David Chandu, Kelsey Wadden, Noah A Macmillan, Iz Smith, Hannaneh Beltagy, Hajishirzi, arXiv:2306.047512023Preprint</p>
<p>Surveyforge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing. Xiangchao Yan, Shiyang Feng, Jiakang Yuan, Renqiu Xia, Bin Wang, Bo Zhang, Lei Bai, arXiv:2503.046292025Preprint</p>
<p>Efficiently answering technical questions -a knowledge graph approach. Shuo Yang, Lei Zou, Zhongyuan Wang, Jun Yan, Ji-Rong Wen, 10.1609/aaai.v31i1.10956Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201731</p>
<p>Bertscore: Evaluating text generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, CoRR, abs/1904.096752019</p>
<p>Taxonomy discovery for personalized recommendation. Yuchen Zhang, Amr Ahmed, Vanja Josifovski, Alexander Smola, 10.1145/2556195.2556236Proceedings of the 7th ACM International Conference on Web Search and Data Mining, WSDM '14. the 7th ACM International Conference on Web Search and Data Mining, WSDM '14New York, NY, USA2014Association for Computing Machinery</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9048 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9048</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9048</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-266820764</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.08743v2.pdf" target="_blank">MMToM-QA: Multimodal Theory of Mind Question Answering</a></p>
                <p><strong>Paper Abstract:</strong> Theory of Mind (ToM), the ability to understand people's mental states, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets - either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person's mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person's activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Planning Accelerated by Language Models). BIP-ALM extracts unified representations from multimodal data and utilizes language models for scalable Bayesian inverse planning. We conducted a systematic comparison of human performance, BIP-ALM, and state-of-the-art models, including GPT-4. The experiments demonstrate that large language models and large multimodal models still lack robust ToM capacity. BIP-ALM, on the other hand, shows promising results, by leveraging the power of both model-based mental inference and language models.</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9048.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9048.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large autoregressive/chat transformer language model from OpenAI evaluated in zero-shot text QA on the multimodal ToM benchmark (text-only condition) and used as the backbone for some ToM prompting baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained transformer-based language model from OpenAI used in zero-shot text question answering; evaluated on MMToM-QA in text-only condition and as part of prompting variants (SimToM, SymbolicToM).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MMToM-QA (Multimodal Theory of Mind Question Answering) - text-only condition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A new benchmark of 600 multiple-choice questions (belief and goal inference) derived from 134 household activity videos plus text descriptions; assesses Theory of Mind (belief/goal inference) rather than classic psychometric tests.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Generally near chance across many question types in text-only condition but better than many smaller models on simpler types (some improvement on true/false-belief style items); specific aggregated accuracy numbers for GPT-4 in text-only condition are reported as below-chance-to-moderate depending on type (see paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans: ~93% accuracy on multimodal condition (benchmark-wide average); unimodal (text-only/video-only) human performance drops relative to multimodal.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 falls short of the human baseline overall on MMToM-QA; it succeeds on simpler true-belief style items but fails systematically on harder false-belief and goal-inference types.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated zero-shot on the text-only version of MMToM-QA using gpt-4-0613 API variant; also tested with SimToM and SymbolicToM prompting wrappers built on GPT-4. Few-shot and chain-of-thought prompting gave no consistent improvement except small gains on some simple types.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper reports GPT-4 can retrieve true world state but often conflates belief and world state (i.e., fails to track an agent's false belief) and has poor goal inference in complex, long-horizon scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MMToM-QA: Multimodal Theory of Mind Question Answering', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9048.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9048.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 with vision (GPT-4V)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Vision-enabled variant of GPT-4 (multimodal LMM) evaluated on multimodal and video-only MMToM-QA conditions; sampled frames used as visual input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal version of GPT-4 capable of ingesting visual frames plus text; used on MMToM-QA multimodal and video-only conditions with sampled frames as input.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MMToM-QA (multimodal and video-only conditions)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same multimodal ToM benchmark; multimodal condition provides both video frames (RGB-D) and a textual scene/action description; video-only supplies sampled frames only.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Among the best of tested LMM baselines; reaches human-level accuracy on simpler Type 1.1 (true-belief short-term) questions and reports competitive performance on some Type 1.3 items, but overall makes systematic errors on false-belief (Type 1.2) and many goal-inference tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans: ~93% accuracy multimodal average; unimodal (video-only, text-only) human performance lower than multimodal.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4V matches humans on some simple true-belief items but is below human baseline overall and fails on harder false-belief and goal inference questions.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated zero-shot; sampled 8 frames per video for GPT-4V following standard sampling stated in paper; evaluated in both multimodal and video-only settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>GPT-4V retrieves world-state information well but confuses belief states with world state and struggles to track belief updates and infer goals that require imagining updated beliefs or future actions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MMToM-QA: Multimodal Theory of Mind Question Answering', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9048.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9048.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (text-davinci-003 used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Earlier OpenAI large language model evaluated in zero-shot text-only MMToM-QA; serves as a weaker LLM baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Predecessor to GPT-4 (text-davinci-003 API used); evaluated zero-shot on the text-only MMToM-QA benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MMToM-QA (text-only condition)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same benchmark focusing on belief and goal inference from text descriptions of scenes and actions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performance close to random/chance across many question types in the text-only condition and noticeably worse than GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans: ~93% multimodal average; unimodal human accuracies higher than LLMs generally.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Well below human baseline and also below GPT-4 performance on this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated zero-shot with text-only prompts; used text-davinci-003 as the GPT-3.5 variant.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Fails on multimodal-style ToM inferences when only text provided, especially in complex belief-tracking and goal reasoning types.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MMToM-QA: Multimodal Theory of Mind Question Answering', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9048.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9048.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-J-6B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-J (6B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 6B-parameter autoregressive language model used both as a baseline LLM and as a component (finetuned) in the BIP-ALM method.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J (6B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 6B parameter autoregressive transformer (GPT-J) used as a baseline LLM and finetuned within BIP-ALM (BIP-ALM w/ GPT-J).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MMToM-QA (text-only for baseline; as finetuned LM inside BIP-ALM for multimodal inference pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Text-only evaluation when used alone; when finetuned and used inside BIP-ALM it is prompted with symbolic state/goal/belief to estimate action likelihoods for inverse planning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>As a vanilla text-only baseline, near-chance; as finetuned inside BIP-ALM (BIP-ALM w/ GPT-J) achieves substantially higher accuracy and outperforms much larger off-the-shelf LLMs on MMToM-QA.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans: ~93% multimodal average.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Pretrained GPT-J alone is below humans; finetuned GPT-J within the BIP-ALM framework substantially outperforms other baselines (including much larger pretrained LMs) and closes much of the gap to human performance on this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Finetuned on 20,000 training samples derived from 1,000 procedurally synthesized videos using LoRA, AdamW lr=5e-5, batch size 4, 5 epochs (≈20 GPU hours on a single A100). In BIP-ALM it is prompted with symbolic state, belief and goal to amortize action-policy likelihood estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>BIP-ALM's gains depend on the model-based inverse planning architecture; GPT-J alone without BIP-ALM remains near chance. The symbolic planning approach still has limits (e.g., imagining missing continuous spatial information).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MMToM-QA: Multimodal Theory of Mind Question Answering', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9048.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9048.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 2 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter open-source language model by Meta evaluated as a text-only baseline and finetuned inside BIP-ALM (BIP-ALM w/ LLaMA 2).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 2 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 7B parameter transformer (LLaMA 2) used as a baseline LLM in text-only condition and finetuned for BIP-ALM inverse planning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MMToM-QA (text-only; as component in BIP-ALM for multimodal inference)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same MMToM-QA; used alone as zero-shot text-only baseline and as finetuned LM inside BIP-ALM to estimate action likelihoods conditioned on symbolic belief/goal/state.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Pretrained LLaMA 2 (7B) baseline performance near chance on text-only; when finetuned and used inside BIP-ALM it achieves substantial gains and outperforms many larger pretrained LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans ~93% multimodal average.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLaMA 2 alone is below human baseline; within BIP-ALM (finetuned) it achieves the best overall performance among methods reported, outperforming GPT-4 and multimodal baselines in aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Finetuning details: LoRA, AdamW lr=5e-5, batch size 4, 5 epochs on training data derived from 1,000 procedurally synthesized videos; used to estimate π(a_t | g, b_t) from symbolic prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>As with GPT-J, the performance gains arise from the model-based BIP-ALM architecture; symbolic planning approach has limitations in imagining continuous spatial trajectories and missing visual state information.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MMToM-QA: Multimodal Theory of Mind Question Answering', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9048.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9048.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sim-ToM (w/ GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-ToM (perspective filtering applied to LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting approach that filters context to the perspective of the target agent (perspective-taking) applied on top of GPT-4 to improve ToM answers; evaluated on MMToM-QA (text-only).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Sim-ToM (prompting wrapper) with GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method that performs perspective-taking by filtering context to sentences the character could know, used as a prompting wrapper over GPT-4 to answer ToM questions in MMToM-QA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MMToM-QA (text-only condition)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Text-only ToM QA; Sim-ToM aims to reduce irrelevant context and supply only the agent-perspective-relevant text to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Provides marginal improvements for some question types compared to raw GPT-4 in text-only condition but still falls substantially short of the human baseline and of BIP-ALM.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans ~93% multimodal average.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Sim-ToM with GPT-4 improves over raw GPT-4 on some items but remains below human-level overall and underperforms the best BIP-ALM variants.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Applied as a zero-shot prompting technique on GPT-4 in text-only condition; filters context according to what the agent could observe/know before answering.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Filtering perspective helps on some text-style ToM items but does not overcome failures on long-horizon belief updates, multimodal fusion requirements, or complex goal inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MMToM-QA: Multimodal Theory of Mind Question Answering', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9048.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9048.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SymbolicToM (w/ GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SymbolicToM prompting method</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that constructs symbolic graphical representations of a character's belief state, retrieves relevant sentences, and feeds them to an LLM (applied to GPT-4) for ToM QA; evaluated on MMToM-QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SymbolicToM (prompting wrapper) with GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Symbolic retrieval approach that builds symbolic belief graphs and provides extracted relevant textual facts to GPT-4 as input for ToM QA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MMToM-QA (text-only and multimodal where applicable)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Evaluates belief/goal inference; SymbolicToM attempts to supply the LLM with distilled symbolic facts to improve ToM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Improves GPT-4 performance on some types (achieves up to moderate accuracy on simpler benchmarks) but still struggles in MMToM-QA's complex multimodal, long-horizon tasks; reported overall accuracy ~63% on MMToM-QA (as stated in the paper discussion) which is lower than BIP-ALM.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans ~93% multimodal average.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Better than raw GPT-4 on some tasks, but still below human baseline and below BIP-ALM's best results.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Applied as a wrapper on GPT-4; Automates symbolic extraction of sentences that mention relevant objects and beliefs, then queries GPT-4 with those sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Performs well on short/simple ToM benchmarks (e.g., ToMi) but less well on MMToM-QA due to longer sequences, more objects, and multimodal fusion requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MMToM-QA: Multimodal Theory of Mind Question Answering', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9048.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9048.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructBLIP (Vicuña-13B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructBLIP (based on Vicuña-13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal vision-language model (VLM) used as a baseline LMM; evaluated in multimodal and video-only MMToM-QA conditions by sampling frames from videos.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructBLIP (Vicuña-13B variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned multimodal model (vision+language) built on Vicuña-13B backbone used for multimodal QA baselines; sampled 16 frames from each video for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B (Vicuña-13B backbone used for InstructBLIP in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MMToM-QA (multimodal and video-only)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Assesses multimodal ToM inference where both visual frames and textual scene descriptions may be present.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported near-chance performance across many question types in multimodal and video-only conditions; did not match human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans ~93% multimodal average.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Well below human baseline and below BIP-ALM performance.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Uniformly sampled 16 frames per video for InstructBLIP as input in multimodal/video-only conditions; evaluated zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Like other LMM baselines, fails to robustly infer agent beliefs and goals from long, complex activity sequences despite access to visual frames.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MMToM-QA: Multimodal Theory of Mind Question Answering', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9048.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9048.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Video-LLaMA 2 (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Video-LLaMA 2 (LLaMA-2-13B-Chat backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A video-capable multimodal LLM used as a baseline and also finetuned on training set for video-instruction tasks; evaluated in multimodal and video-only conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Video-LLaMA 2 (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Video-enabled LLaMA-2 model (13B chat variant) used as a multimodal baseline; the authors also finetuned a 13B Video-LLaMA 2 on their training data for instruction-style video understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MMToM-QA (multimodal and video-only)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multimodal ToM QA where model must integrate frame samples and text to answer belief/goal questions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Pretrained Video-LLaMA 2 performed near chance; after finetuning on the paper's training set it showed modest improvement on some simple question types (e.g., Type 1.1) but overall performance still not better than chance and worse than BIP-ALM.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans ~93% multimodal average.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below human baseline even after finetuning; underperforms BIP-ALM.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Pretrained Video-LLaMA 2 (LLaMA-2-13B-Chat) was finetuned on ~7,088 video-instruction examples generated from training videos (sampled 8 frames per clip) following Zhang et al. (2023); finetuning performed on 2 A100 GPUs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Finetuned model gains were modest and did not generalize to complex ToM inference; overall suggests finetuning a single LMM on this data is insufficient to capture robust ToM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MMToM-QA: Multimodal Theory of Mind Question Answering', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9048.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9048.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA (Large Language and Vision Assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal LLM baseline evaluated on MMToM-QA in multimodal and video-only conditions using sampled frames; used the largest available variant for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language model that integrates visual inputs and language; evaluated zero-shot on MMToM-QA with sampled frames (6 frames used per video in paper experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MMToM-QA (multimodal and video-only)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multimodal ToM QA for belief and goal inference; LLaVA evaluated as an off-the-shelf LMM baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported overall near-chance performance across question types, with some practical issues processing long prompts and generating options in some few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans ~93% multimodal average.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Well below human baseline and below BIP-ALM.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Sampled 6 frames per video for LLaVA evaluation; tested largest available variant; few-shot prompting sometimes produced low-quality outputs for this task due to prompt length.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Struggles with long multimodal contexts and ToM-specific inference requirements; sampling a few frames does not fully capture long activity information needed for belief/goal tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MMToM-QA: Multimodal Theory of Mind Question Answering', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9048.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9048.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BIP-ALM (w/ GPT-J)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Inverse Planning Accelerated by Language Models (BIP-ALM) with finetuned GPT-J</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Novel multimodal model introduced in this paper that fuses symbolic representations from video and text and uses Bayesian inverse planning where a finetuned LM (GPT-J) amortizes the action-policy likelihood estimation; achieves the best reported performance on MMToM-QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BIP-ALM (with finetuned GPT-J 6B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>New method combining unified symbolic representations (fused from video and text), a POMDP-based Bayesian inverse planning inference engine, and a finetuned language model (GPT-J-6B) that scores action likelihoods to accelerate inverse planning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>framework uses GPT-J (6B) or LLaMA-2 (7B) as the LM component; this entry refers to the GPT-J variant</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MMToM-QA (multimodal, text-only, and video-only conditions)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>MMToM-QA evaluates multimodal Theory of Mind: 600 multiple-choice belief and goal inference questions derived from long household activity videos and scene/action text.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>BIP-ALM (w/ finetuned GPT-J) substantially outperforms all tested baselines (including GPT-4 and multimodal baselines) across conditions and question types; reported to achieve the highest aggregate accuracy in the paper (qualitative 'by a large margin' and quantitative improvements shown in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans ~93% accuracy multimodal average; BIP-ALM approaches strong performance though exact parity numbers vary by question type (paper shows BIP-ALM outperforms baselines and generalizes to human test set).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>BIP-ALM exceeds baseline LLMs and LMMs and is substantially closer to human performance; outperforms even much larger pretrained LMs when used without the model-based inverse planning.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Symbolic state/action/hypotheses extracted via visual pipeline and GPT-4 text parsing; LM policy amortizer finetuned on 20,000 samples from 1,000 synthetic training videos using LoRA, AdamW lr=5e-5, batch 4, 5 epochs (≈20 GPU hours on A100). Evaluation protocol: zero-shot QA (no example QAs provided during training).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>BIP-ALM still cannot imagine missing state information from videos, relies on symbolic representations and discrete planning (limits for Type 2.4 spatial/future-path imagination), and LM-estimated action likelihoods can be inaccurate at times.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MMToM-QA: Multimodal Theory of Mind Question Answering', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9048.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e9048.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BIP-ALM (w/ LLaMA-2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Inverse Planning Accelerated by Language Models (BIP-ALM) with finetuned LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variant of the paper's BIP-ALM method that uses finetuned LLaMA-2 (7B) as the action-likelihood LM; achieves the best or near-best performance reported on MMToM-QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BIP-ALM (with finetuned LLaMA 2 - 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same BIP-ALM architecture (symbolic fusion + Bayesian inverse planning) but using a finetuned LLaMA-2 (7B) model to estimate action likelihoods; finetuning procedure and data same as GPT-J variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MMToM-QA (multimodal, text-only, video-only)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multimodal ToM benchmark focusing on belief and goal inference across seven question types (300 belief, 300 goal questions).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>BIP-ALM with finetuned LLaMA-2 is reported to outperform all other baselines and achieve strong generalization to a human test set; it is the best-performing method in the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans ~93% multimodal average.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Outperforms standard LLM and LMM baselines and approaches human-level performance on many question types, while still showing failure modes on specific types requiring continuous spatial imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Finetuned with LoRA on the training set (20k samples) using AdamW lr=5e-5, batch 4, 5 epochs; used as the likelihood amortizer in the Bayesian inverse planning scoring equation (π(a_t | g, b_t)).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Same caveats as BIP-ALM generally: cannot invent unobserved visual information, limited by symbolic/planning representation for continuous spatial reasoning (Type 2.4), and occasional LM inaccuracies in policy estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MMToM-QA: Multimodal Theory of Mind Question Answering', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Theory of mind may have spontaneously emerged in large language models <em>(Rating: 2)</em></li>
                <li>Neural theory-of-mind? On the limits of social intelligence in large LMs <em>(Rating: 2)</em></li>
                <li>Clever Hans or neural theory of mind? Stress testing social reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models fail on trivial alterations to theory-of-mind tasks <em>(Rating: 2)</em></li>
                <li>Minding language models' (lack of) theory of mind: A plug-and-play multi-character belief tracker <em>(Rating: 1)</em></li>
                <li>Sim-ToM (perspective-trained language models) (Wilf et al., 2023) <em>(Rating: 1)</em></li>
                <li>Symbolic-ToM (Sclar et al., 2023) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9048",
    "paper_id": "paper-266820764",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A state-of-the-art large autoregressive/chat transformer language model from OpenAI evaluated in zero-shot text QA on the multimodal ToM benchmark (text-only condition) and used as the backbone for some ToM prompting baselines.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large pre-trained transformer-based language model from OpenAI used in zero-shot text question answering; evaluated on MMToM-QA in text-only condition and as part of prompting variants (SimToM, SymbolicToM).",
            "model_size": null,
            "test_battery_name": "MMToM-QA (Multimodal Theory of Mind Question Answering) - text-only condition",
            "test_description": "A new benchmark of 600 multiple-choice questions (belief and goal inference) derived from 134 household activity videos plus text descriptions; assesses Theory of Mind (belief/goal inference) rather than classic psychometric tests.",
            "llm_performance": "Generally near chance across many question types in text-only condition but better than many smaller models on simpler types (some improvement on true/false-belief style items); specific aggregated accuracy numbers for GPT-4 in text-only condition are reported as below-chance-to-moderate depending on type (see paper tables).",
            "human_baseline_performance": "Humans: ~93% accuracy on multimodal condition (benchmark-wide average); unimodal (text-only/video-only) human performance drops relative to multimodal.",
            "performance_comparison": "GPT-4 falls short of the human baseline overall on MMToM-QA; it succeeds on simpler true-belief style items but fails systematically on harder false-belief and goal-inference types.",
            "experimental_details": "Evaluated zero-shot on the text-only version of MMToM-QA using gpt-4-0613 API variant; also tested with SimToM and SymbolicToM prompting wrappers built on GPT-4. Few-shot and chain-of-thought prompting gave no consistent improvement except small gains on some simple types.",
            "limitations_or_caveats": "Paper reports GPT-4 can retrieve true world state but often conflates belief and world state (i.e., fails to track an agent's false belief) and has poor goal inference in complex, long-horizon scenarios.",
            "uuid": "e9048.0",
            "source_info": {
                "paper_title": "MMToM-QA: Multimodal Theory of Mind Question Answering",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-4V",
            "name_full": "GPT-4 with vision (GPT-4V)",
            "brief_description": "Vision-enabled variant of GPT-4 (multimodal LMM) evaluated on multimodal and video-only MMToM-QA conditions; sampled frames used as visual input.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4V",
            "model_description": "Multimodal version of GPT-4 capable of ingesting visual frames plus text; used on MMToM-QA multimodal and video-only conditions with sampled frames as input.",
            "model_size": null,
            "test_battery_name": "MMToM-QA (multimodal and video-only conditions)",
            "test_description": "Same multimodal ToM benchmark; multimodal condition provides both video frames (RGB-D) and a textual scene/action description; video-only supplies sampled frames only.",
            "llm_performance": "Among the best of tested LMM baselines; reaches human-level accuracy on simpler Type 1.1 (true-belief short-term) questions and reports competitive performance on some Type 1.3 items, but overall makes systematic errors on false-belief (Type 1.2) and many goal-inference tasks.",
            "human_baseline_performance": "Humans: ~93% accuracy multimodal average; unimodal (video-only, text-only) human performance lower than multimodal.",
            "performance_comparison": "GPT-4V matches humans on some simple true-belief items but is below human baseline overall and fails on harder false-belief and goal inference questions.",
            "experimental_details": "Evaluated zero-shot; sampled 8 frames per video for GPT-4V following standard sampling stated in paper; evaluated in both multimodal and video-only settings.",
            "limitations_or_caveats": "GPT-4V retrieves world-state information well but confuses belief states with world state and struggles to track belief updates and infer goals that require imagining updated beliefs or future actions.",
            "uuid": "e9048.1",
            "source_info": {
                "paper_title": "MMToM-QA: Multimodal Theory of Mind Question Answering",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (text-davinci-003 used in experiments)",
            "brief_description": "Earlier OpenAI large language model evaluated in zero-shot text-only MMToM-QA; serves as a weaker LLM baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Predecessor to GPT-4 (text-davinci-003 API used); evaluated zero-shot on the text-only MMToM-QA benchmark.",
            "model_size": null,
            "test_battery_name": "MMToM-QA (text-only condition)",
            "test_description": "Same benchmark focusing on belief and goal inference from text descriptions of scenes and actions.",
            "llm_performance": "Performance close to random/chance across many question types in the text-only condition and noticeably worse than GPT-4.",
            "human_baseline_performance": "Humans: ~93% multimodal average; unimodal human accuracies higher than LLMs generally.",
            "performance_comparison": "Well below human baseline and also below GPT-4 performance on this benchmark.",
            "experimental_details": "Evaluated zero-shot with text-only prompts; used text-davinci-003 as the GPT-3.5 variant.",
            "limitations_or_caveats": "Fails on multimodal-style ToM inferences when only text provided, especially in complex belief-tracking and goal reasoning types.",
            "uuid": "e9048.2",
            "source_info": {
                "paper_title": "MMToM-QA: Multimodal Theory of Mind Question Answering",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-J-6B",
            "name_full": "GPT-J (6B)",
            "brief_description": "An open-source 6B-parameter autoregressive language model used both as a baseline LLM and as a component (finetuned) in the BIP-ALM method.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-J (6B)",
            "model_description": "Open-source 6B parameter autoregressive transformer (GPT-J) used as a baseline LLM and finetuned within BIP-ALM (BIP-ALM w/ GPT-J).",
            "model_size": "6B",
            "test_battery_name": "MMToM-QA (text-only for baseline; as finetuned LM inside BIP-ALM for multimodal inference pipeline)",
            "test_description": "Text-only evaluation when used alone; when finetuned and used inside BIP-ALM it is prompted with symbolic state/goal/belief to estimate action likelihoods for inverse planning.",
            "llm_performance": "As a vanilla text-only baseline, near-chance; as finetuned inside BIP-ALM (BIP-ALM w/ GPT-J) achieves substantially higher accuracy and outperforms much larger off-the-shelf LLMs on MMToM-QA.",
            "human_baseline_performance": "Humans: ~93% multimodal average.",
            "performance_comparison": "Pretrained GPT-J alone is below humans; finetuned GPT-J within the BIP-ALM framework substantially outperforms other baselines (including much larger pretrained LMs) and closes much of the gap to human performance on this benchmark.",
            "experimental_details": "Finetuned on 20,000 training samples derived from 1,000 procedurally synthesized videos using LoRA, AdamW lr=5e-5, batch size 4, 5 epochs (≈20 GPU hours on a single A100). In BIP-ALM it is prompted with symbolic state, belief and goal to amortize action-policy likelihood estimation.",
            "limitations_or_caveats": "BIP-ALM's gains depend on the model-based inverse planning architecture; GPT-J alone without BIP-ALM remains near chance. The symbolic planning approach still has limits (e.g., imagining missing continuous spatial information).",
            "uuid": "e9048.3",
            "source_info": {
                "paper_title": "MMToM-QA: Multimodal Theory of Mind Question Answering",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "LLaMA-2-7B",
            "name_full": "LLaMA 2 (7B)",
            "brief_description": "A 7B-parameter open-source language model by Meta evaluated as a text-only baseline and finetuned inside BIP-ALM (BIP-ALM w/ LLaMA 2).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA 2 (7B)",
            "model_description": "Open-source 7B parameter transformer (LLaMA 2) used as a baseline LLM in text-only condition and finetuned for BIP-ALM inverse planning.",
            "model_size": "7B",
            "test_battery_name": "MMToM-QA (text-only; as component in BIP-ALM for multimodal inference)",
            "test_description": "Same MMToM-QA; used alone as zero-shot text-only baseline and as finetuned LM inside BIP-ALM to estimate action likelihoods conditioned on symbolic belief/goal/state.",
            "llm_performance": "Pretrained LLaMA 2 (7B) baseline performance near chance on text-only; when finetuned and used inside BIP-ALM it achieves substantial gains and outperforms many larger pretrained LLMs.",
            "human_baseline_performance": "Humans ~93% multimodal average.",
            "performance_comparison": "LLaMA 2 alone is below human baseline; within BIP-ALM (finetuned) it achieves the best overall performance among methods reported, outperforming GPT-4 and multimodal baselines in aggregate.",
            "experimental_details": "Finetuning details: LoRA, AdamW lr=5e-5, batch size 4, 5 epochs on training data derived from 1,000 procedurally synthesized videos; used to estimate π(a_t | g, b_t) from symbolic prompts.",
            "limitations_or_caveats": "As with GPT-J, the performance gains arise from the model-based BIP-ALM architecture; symbolic planning approach has limitations in imagining continuous spatial trajectories and missing visual state information.",
            "uuid": "e9048.4",
            "source_info": {
                "paper_title": "MMToM-QA: Multimodal Theory of Mind Question Answering",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Sim-ToM (w/ GPT-4)",
            "name_full": "Sim-ToM (perspective filtering applied to LLMs)",
            "brief_description": "A prompting approach that filters context to the perspective of the target agent (perspective-taking) applied on top of GPT-4 to improve ToM answers; evaluated on MMToM-QA (text-only).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Sim-ToM (prompting wrapper) with GPT-4",
            "model_description": "Method that performs perspective-taking by filtering context to sentences the character could know, used as a prompting wrapper over GPT-4 to answer ToM questions in MMToM-QA.",
            "model_size": null,
            "test_battery_name": "MMToM-QA (text-only condition)",
            "test_description": "Text-only ToM QA; Sim-ToM aims to reduce irrelevant context and supply only the agent-perspective-relevant text to the LLM.",
            "llm_performance": "Provides marginal improvements for some question types compared to raw GPT-4 in text-only condition but still falls substantially short of the human baseline and of BIP-ALM.",
            "human_baseline_performance": "Humans ~93% multimodal average.",
            "performance_comparison": "Sim-ToM with GPT-4 improves over raw GPT-4 on some items but remains below human-level overall and underperforms the best BIP-ALM variants.",
            "experimental_details": "Applied as a zero-shot prompting technique on GPT-4 in text-only condition; filters context according to what the agent could observe/know before answering.",
            "limitations_or_caveats": "Filtering perspective helps on some text-style ToM items but does not overcome failures on long-horizon belief updates, multimodal fusion requirements, or complex goal inference.",
            "uuid": "e9048.5",
            "source_info": {
                "paper_title": "MMToM-QA: Multimodal Theory of Mind Question Answering",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "SymbolicToM (w/ GPT-4)",
            "name_full": "SymbolicToM prompting method",
            "brief_description": "A method that constructs symbolic graphical representations of a character's belief state, retrieves relevant sentences, and feeds them to an LLM (applied to GPT-4) for ToM QA; evaluated on MMToM-QA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SymbolicToM (prompting wrapper) with GPT-4",
            "model_description": "Symbolic retrieval approach that builds symbolic belief graphs and provides extracted relevant textual facts to GPT-4 as input for ToM QA.",
            "model_size": null,
            "test_battery_name": "MMToM-QA (text-only and multimodal where applicable)",
            "test_description": "Evaluates belief/goal inference; SymbolicToM attempts to supply the LLM with distilled symbolic facts to improve ToM reasoning.",
            "llm_performance": "Improves GPT-4 performance on some types (achieves up to moderate accuracy on simpler benchmarks) but still struggles in MMToM-QA's complex multimodal, long-horizon tasks; reported overall accuracy ~63% on MMToM-QA (as stated in the paper discussion) which is lower than BIP-ALM.",
            "human_baseline_performance": "Humans ~93% multimodal average.",
            "performance_comparison": "Better than raw GPT-4 on some tasks, but still below human baseline and below BIP-ALM's best results.",
            "experimental_details": "Applied as a wrapper on GPT-4; Automates symbolic extraction of sentences that mention relevant objects and beliefs, then queries GPT-4 with those sentences.",
            "limitations_or_caveats": "Performs well on short/simple ToM benchmarks (e.g., ToMi) but less well on MMToM-QA due to longer sequences, more objects, and multimodal fusion requirements.",
            "uuid": "e9048.6",
            "source_info": {
                "paper_title": "MMToM-QA: Multimodal Theory of Mind Question Answering",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "InstructBLIP (Vicuña-13B)",
            "name_full": "InstructBLIP (based on Vicuña-13B)",
            "brief_description": "A multimodal vision-language model (VLM) used as a baseline LMM; evaluated in multimodal and video-only MMToM-QA conditions by sampling frames from videos.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "InstructBLIP (Vicuña-13B variant)",
            "model_description": "Instruction-tuned multimodal model (vision+language) built on Vicuña-13B backbone used for multimodal QA baselines; sampled 16 frames from each video for evaluation.",
            "model_size": "13B (Vicuña-13B backbone used for InstructBLIP in experiments)",
            "test_battery_name": "MMToM-QA (multimodal and video-only)",
            "test_description": "Assesses multimodal ToM inference where both visual frames and textual scene descriptions may be present.",
            "llm_performance": "Reported near-chance performance across many question types in multimodal and video-only conditions; did not match human baseline.",
            "human_baseline_performance": "Humans ~93% multimodal average.",
            "performance_comparison": "Well below human baseline and below BIP-ALM performance.",
            "experimental_details": "Uniformly sampled 16 frames per video for InstructBLIP as input in multimodal/video-only conditions; evaluated zero-shot.",
            "limitations_or_caveats": "Like other LMM baselines, fails to robustly infer agent beliefs and goals from long, complex activity sequences despite access to visual frames.",
            "uuid": "e9048.7",
            "source_info": {
                "paper_title": "MMToM-QA: Multimodal Theory of Mind Question Answering",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Video-LLaMA 2 (13B)",
            "name_full": "Video-LLaMA 2 (LLaMA-2-13B-Chat backbone)",
            "brief_description": "A video-capable multimodal LLM used as a baseline and also finetuned on training set for video-instruction tasks; evaluated in multimodal and video-only conditions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Video-LLaMA 2 (13B)",
            "model_description": "Video-enabled LLaMA-2 model (13B chat variant) used as a multimodal baseline; the authors also finetuned a 13B Video-LLaMA 2 on their training data for instruction-style video understanding.",
            "model_size": "13B",
            "test_battery_name": "MMToM-QA (multimodal and video-only)",
            "test_description": "Multimodal ToM QA where model must integrate frame samples and text to answer belief/goal questions.",
            "llm_performance": "Pretrained Video-LLaMA 2 performed near chance; after finetuning on the paper's training set it showed modest improvement on some simple question types (e.g., Type 1.1) but overall performance still not better than chance and worse than BIP-ALM.",
            "human_baseline_performance": "Humans ~93% multimodal average.",
            "performance_comparison": "Below human baseline even after finetuning; underperforms BIP-ALM.",
            "experimental_details": "Pretrained Video-LLaMA 2 (LLaMA-2-13B-Chat) was finetuned on ~7,088 video-instruction examples generated from training videos (sampled 8 frames per clip) following Zhang et al. (2023); finetuning performed on 2 A100 GPUs.",
            "limitations_or_caveats": "Finetuned model gains were modest and did not generalize to complex ToM inference; overall suggests finetuning a single LMM on this data is insufficient to capture robust ToM reasoning.",
            "uuid": "e9048.8",
            "source_info": {
                "paper_title": "MMToM-QA: Multimodal Theory of Mind Question Answering",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "LLaVA",
            "name_full": "LLaVA (Large Language and Vision Assistant)",
            "brief_description": "A multimodal LLM baseline evaluated on MMToM-QA in multimodal and video-only conditions using sampled frames; used the largest available variant for evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA",
            "model_description": "Vision-language model that integrates visual inputs and language; evaluated zero-shot on MMToM-QA with sampled frames (6 frames used per video in paper experiments).",
            "model_size": null,
            "test_battery_name": "MMToM-QA (multimodal and video-only)",
            "test_description": "Multimodal ToM QA for belief and goal inference; LLaVA evaluated as an off-the-shelf LMM baseline.",
            "llm_performance": "Reported overall near-chance performance across question types, with some practical issues processing long prompts and generating options in some few-shot settings.",
            "human_baseline_performance": "Humans ~93% multimodal average.",
            "performance_comparison": "Well below human baseline and below BIP-ALM.",
            "experimental_details": "Sampled 6 frames per video for LLaVA evaluation; tested largest available variant; few-shot prompting sometimes produced low-quality outputs for this task due to prompt length.",
            "limitations_or_caveats": "Struggles with long multimodal contexts and ToM-specific inference requirements; sampling a few frames does not fully capture long activity information needed for belief/goal tracking.",
            "uuid": "e9048.9",
            "source_info": {
                "paper_title": "MMToM-QA: Multimodal Theory of Mind Question Answering",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "BIP-ALM (w/ GPT-J)",
            "name_full": "Bayesian Inverse Planning Accelerated by Language Models (BIP-ALM) with finetuned GPT-J",
            "brief_description": "Novel multimodal model introduced in this paper that fuses symbolic representations from video and text and uses Bayesian inverse planning where a finetuned LM (GPT-J) amortizes the action-policy likelihood estimation; achieves the best reported performance on MMToM-QA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BIP-ALM (with finetuned GPT-J 6B)",
            "model_description": "New method combining unified symbolic representations (fused from video and text), a POMDP-based Bayesian inverse planning inference engine, and a finetuned language model (GPT-J-6B) that scores action likelihoods to accelerate inverse planning.",
            "model_size": "framework uses GPT-J (6B) or LLaMA-2 (7B) as the LM component; this entry refers to the GPT-J variant",
            "test_battery_name": "MMToM-QA (multimodal, text-only, and video-only conditions)",
            "test_description": "MMToM-QA evaluates multimodal Theory of Mind: 600 multiple-choice belief and goal inference questions derived from long household activity videos and scene/action text.",
            "llm_performance": "BIP-ALM (w/ finetuned GPT-J) substantially outperforms all tested baselines (including GPT-4 and multimodal baselines) across conditions and question types; reported to achieve the highest aggregate accuracy in the paper (qualitative 'by a large margin' and quantitative improvements shown in tables).",
            "human_baseline_performance": "Humans ~93% accuracy multimodal average; BIP-ALM approaches strong performance though exact parity numbers vary by question type (paper shows BIP-ALM outperforms baselines and generalizes to human test set).",
            "performance_comparison": "BIP-ALM exceeds baseline LLMs and LMMs and is substantially closer to human performance; outperforms even much larger pretrained LMs when used without the model-based inverse planning.",
            "experimental_details": "Symbolic state/action/hypotheses extracted via visual pipeline and GPT-4 text parsing; LM policy amortizer finetuned on 20,000 samples from 1,000 synthetic training videos using LoRA, AdamW lr=5e-5, batch 4, 5 epochs (≈20 GPU hours on A100). Evaluation protocol: zero-shot QA (no example QAs provided during training).",
            "limitations_or_caveats": "BIP-ALM still cannot imagine missing state information from videos, relies on symbolic representations and discrete planning (limits for Type 2.4 spatial/future-path imagination), and LM-estimated action likelihoods can be inaccurate at times.",
            "uuid": "e9048.10",
            "source_info": {
                "paper_title": "MMToM-QA: Multimodal Theory of Mind Question Answering",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "BIP-ALM (w/ LLaMA-2)",
            "name_full": "Bayesian Inverse Planning Accelerated by Language Models (BIP-ALM) with finetuned LLaMA-2",
            "brief_description": "Variant of the paper's BIP-ALM method that uses finetuned LLaMA-2 (7B) as the action-likelihood LM; achieves the best or near-best performance reported on MMToM-QA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BIP-ALM (with finetuned LLaMA 2 - 7B)",
            "model_description": "Same BIP-ALM architecture (symbolic fusion + Bayesian inverse planning) but using a finetuned LLaMA-2 (7B) model to estimate action likelihoods; finetuning procedure and data same as GPT-J variant.",
            "model_size": "7B",
            "test_battery_name": "MMToM-QA (multimodal, text-only, video-only)",
            "test_description": "Multimodal ToM benchmark focusing on belief and goal inference across seven question types (300 belief, 300 goal questions).",
            "llm_performance": "BIP-ALM with finetuned LLaMA-2 is reported to outperform all other baselines and achieve strong generalization to a human test set; it is the best-performing method in the paper's comparisons.",
            "human_baseline_performance": "Humans ~93% multimodal average.",
            "performance_comparison": "Outperforms standard LLM and LMM baselines and approaches human-level performance on many question types, while still showing failure modes on specific types requiring continuous spatial imagination.",
            "experimental_details": "Finetuned with LoRA on the training set (20k samples) using AdamW lr=5e-5, batch 4, 5 epochs; used as the likelihood amortizer in the Bayesian inverse planning scoring equation (π(a_t | g, b_t)).",
            "limitations_or_caveats": "Same caveats as BIP-ALM generally: cannot invent unobserved visual information, limited by symbolic/planning representation for continuous spatial reasoning (Type 2.4), and occasional LM inaccuracies in policy estimation.",
            "uuid": "e9048.11",
            "source_info": {
                "paper_title": "MMToM-QA: Multimodal Theory of Mind Question Answering",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Theory of mind may have spontaneously emerged in large language models",
            "rating": 2,
            "sanitized_title": "theory_of_mind_may_have_spontaneously_emerged_in_large_language_models"
        },
        {
            "paper_title": "Neural theory-of-mind? On the limits of social intelligence in large LMs",
            "rating": 2,
            "sanitized_title": "neural_theoryofmind_on_the_limits_of_social_intelligence_in_large_lms"
        },
        {
            "paper_title": "Clever Hans or neural theory of mind? Stress testing social reasoning in large language models",
            "rating": 2,
            "sanitized_title": "clever_hans_or_neural_theory_of_mind_stress_testing_social_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models fail on trivial alterations to theory-of-mind tasks",
            "rating": 2,
            "sanitized_title": "large_language_models_fail_on_trivial_alterations_to_theoryofmind_tasks"
        },
        {
            "paper_title": "Minding language models' (lack of) theory of mind: A plug-and-play multi-character belief tracker",
            "rating": 1,
            "sanitized_title": "minding_language_models_lack_of_theory_of_mind_a_plugandplay_multicharacter_belief_tracker"
        },
        {
            "paper_title": "Sim-ToM (perspective-trained language models) (Wilf et al., 2023)",
            "rating": 1,
            "sanitized_title": "simtom_perspectivetrained_language_models_wilf_et_al_2023"
        },
        {
            "paper_title": "Symbolic-ToM (Sclar et al., 2023)",
            "rating": 1,
            "sanitized_title": "symbolictom_sclar_et_al_2023"
        }
    ],
    "cost": 0.02670675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MMToM-QA: Multimodal Theory of Mind Question Answering
15 Jun 2024</p>
<p>Chuanyang Jin 
New York University</p>
<p>Yutong Wu 
Harvard University</p>
<p>Jing Cao 
Massachusetts Institute of Technology</p>
<p>Jiannan Xiang 
San Diego</p>
<p>Yen-Ling Kuo 
University of Virginia</p>
<p>Zhiting Hu 
San Diego</p>
<p>Tomer D Ullman 
Harvard University</p>
<p>Antonio Torralba 
Massachusetts Institute of Technology</p>
<p>Joshua B Tenenbaum 
Massachusetts Institute of Technology</p>
<p>Tianmin Shu 
Johns Hopkins University</p>
<p>MMToM-QA: Multimodal Theory of Mind Question Answering
15 Jun 20241D6400EF1D50AE16AB2DF96FD04A1B33arXiv:2401.08743v2[cs.AI]Type 1.1: True beliefshort-term Type 1.2: False beliefshort-term
Theory of Mind (ToM), the ability to understand people's mental states, is an essential ingredient for developing machines with humanlevel social intelligence.Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding.However, existing ToM benchmarks use unimodal datasets -either video or text.Human ToM, on the other hand, is more than video or text understanding.People can flexibly reason about another person's mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data.To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark.MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person's activity in a household environment.To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Planning Accelerated by Language Models).BIP-ALM extracts unified representations from multimodal data and utilizes language models for scalable Bayesian inverse planning.We conducted a systematic comparison of human performance, BIP-ALM, and state-of-the-art models, including GPT-4.The experiments demonstrate that large language models and large multimodal models still lack robust ToM capacity.BIP-ALM, on the other hand, shows promising results, by leveraging the power of both model-based mental inference and language models. 1</p>
<p>Introduction</p>
<p>Theory of Mind (ToM) is the cognitive ability to ascribe hidden mental states (e.g.goals, beliefs, and desires) to other individuals based on their observed behavior.A hallmark of human social intelligence, ToM serves as the foundation for a wide range of social interactions and a pillar of commonsense reasoning (Lake et al., 2017).Systems designed to safely and productively interact with humans in an open-ended manner, such as assistive robots (e.g., Dautenhahn, 2007;Hadfield-Menell et al., 2016;Patel and Chernova, 2022;Puig et al., 2023), AI teachers (e.g., Wang et al., 2021), and autonomous vehicles (e.g., Chandra et al., 2020), would greatly benefit from incorporating ToM reasoning capabilities.The recent advancements in machine learning, especially in the realm of Large Language Models (LLMs), have spurred increased interest in assessing these models' aptitude for ToM reasoning (e.g., Rabinowitz et al., 2018;Kosinski, 2023;Sap et al., 2019Sap et al., , 2022;;Ullman, 2023;Shapira et al., 2023;Shu et al., 2021;Moghaddam and Honey, 2023;Nematzadeh et al., 2018;Gandhi et al., 2021Gandhi et al., , 2023;;Kim et al., 2023).Many of these assessments use either text-based or video-based benchmarks inspired by classic ToM experiments in the cognitive science literature (Wimmer and Perner, 1983).</p>
<p>While the recent ToM benchmarks provide welldesigned, cognitively informed tools, they share several notable limitations.One such limitation is the dependence on massive training data, which raises the concern that these models work by finding data patterns in a way that deviates from humanlike ToM reasoning (e.g., Ullman, 2023;Sap et al., 2022;Shapira et al., 2023).This paper focuses on a different but related limitation: These benchmarks rely on unimodal data, either in the form of videos (e.g., Gandhi et al., 2021), or textual descriptions of actions and environments (e.g., Kosinski, 2023;Sap et al., 2022;Gandhi et al., 2023).But ToM reasoning goes beyond merely text comprehension or video understanding.It is about forming a causal model of another person's mind, which connects mental variables to possible actions (Baker et al., 2009;Saxe, 2012;Jara-Ettinger et al., 2016;Jara-Ettinger, 2019).Such a model can infer mental What's inside the apartment: … The kitchen is equipped with a microwave, eight cabinets, … Inside the microwave, there is a cupcake.There is a wine glass and an apple on one of the kitchen tables.There are water glasses, a bottle wine, a condiment bottle, and a bag of chips in inside the cabinets.… Actions taken by Emily: Emily is initially in the bathroom.She then walks to the kitchen, goes to the sixth cabinet, opens it, subsequently closes it, and then goes towards the fourth cabinet.</p>
<p>VIDEO INPUT</p>
<p>Which one of the following statements is more likely to be true?Each question is associated with a video stream (representative frames highlighting key moments are shown above for illustration) and text input (illustrative text above is shortened for brevity).In the example video, Emily can see the wine glass on one of the kitchen tables (1st frame) and passes by it without picking it up (2nd frame).At the end of the clip (3rd frame), it appears that she could be walking towards the cabinets on the left side of the room; or she might want to check if a goal object is inside the microwave.The text indicates that there are no cupcakes in the cabinets, but there is a cupcake inside the microwave.To confidently choose the correct answer, a model must fuse relevant information from both the video and the text.</p>
<p>states from either words or vision separately, or fuse the separate information to form a single coherent mental scene.By examining multimodal ToM reasoning, we can both gain insight into the computational models that underlie human ToM and offer a stronger test for current ML models, particularly LLMs.</p>
<p>To systematically evaluate the ability of ML models to infer mental states from multimodal data, we developed a novel Multimodal Theory of Mind Question Answering benchmark (MMToM-QA).As shown in Figure 1, the benchmark includes as input both videos and text describing the activity of a person in a household environment.The benchmark also includes questions associated with different points in each of the videos.The questions refer to the mental states (goals and beliefs) of the person described by the video or text.Each question has two possible options, neither surely true nor surely false, but with one option significantly more likely to be true given the observations.Some questions can be adequately answered based on a single modality, but some questions require fusing information from both modalities (e.g.understanding the woman's goal in Figure 1).We validated our benchmark through human experiments, showing that people are adept at answering the questions in the benchmark and providing a human baseline.</p>
<p>We propose a novel multimodal ToM model, Bayesian Inverse Planning Accelerated by Language Models (BIP-ALM).As illustrated in Figure 3, BIP-ALM first extracts symbolic representations about the physical scene and the actions of the person from both video and text inputs.Using these symbolic representations, BIP-ALM then extends Bayesian inverse planning (BIP) (Baker et al., 2017), a cognitively grounded ToM method originally designed for visual data, to reason about the multimodal data.To accelerate the inference in real-world scenarios such as household activities in our benchmark, BIP-ALM uses a language model (LM) finetuned on human activity data to evaluate the likelihood of hypotheses about the person's belief and goal.By doing so, it takes advantage of the robustness of Bayesian inverse planning, as well as the scalability and open-endedness of LMs.</p>
<p>We compared the performance of BIP-ALM and several state-of-the-art models for text QA or multimodal QA, including GPT-4(V).We found that existing models, however impressive in other QA benchmarks, make large and systematic errors in our benchmark, and fail to match human performance.In contrast, BIP-ALM significantly outperforms these models.</p>
<p>In sum, our main contributions include (1) the first benchmark for multimodal ToM, (2) a novel ToM reasoning method, BIP-ALM, that combines Bayesian inverse planning and LMs to conduct robust yet efficient ToM inference based on multimodal data, and (3) a systematic comparison of different ML models and human ToM.</p>
<p>Related Work</p>
<p>Theory of Mind Benchmarks.Existing ToM benchmarks are based on either videos or text.Visual-based benchmarks (e.g., Gandhi et al., 2021;Shu et al., 2021;Netanyahu et al., 2021) typically use animations of goal-directed agents to evaluate different concepts in ToM.Text-based QA benchmarks (e.g., Le et al., 2019;Shapira et al., 2023;Hewitt and Cohen, 2021;Gandhi et al., 2023;Kim et al., 2023;He et al., 2023) adapt or extend a classic false belief test, the Sally-Anne test (Wimmer and Perner, 1983).Questions in these benchmarks ask a model to select the true hypothesis about a person's knowledge and belief based on a given premise.Triangle COPA (Gordon, 2016) asks questions about the mental states of agents based on text descriptions of abstract shapes acting like social agents.Moreover, there are QA benchmarks (e.g., Zadeh et al., 2019;Sap et al., 2019) that do not specifically test ToM, but evaluate social intelligence in general.Lastly, there have also been multi-agent challenges (e.g., Sclar et al., 2022;Puig et al., 2020Puig et al., , 2023) ) evaluating ToM as part of the tasks.Unlike existing benchmarks, our MMToM-QA evaluates machine ToM on multimodal data to test both goal and belief inference.We also go beyond simple visual and textual stimuli and evaluate ToM using long everyday activities in complex environments.Critically, ToM QAs ask questions about people's mental states.This is fundamentally different from VQAs (e.g., Antol et al., 2015;Zellers et al., 2019) which do not require mental state inference.Table 2 in Appendix A provides a detailed comparison.</p>
<p>Multimodal Question Answering.There have been several multimodal QA benchmarks developed in recent years (e.g., Talmor et al., 2021;Singh et al., 2021;Sanders et al., 2023;Fu et al., 2023;Li et al., 2023).These benchmarks focus on the ability of models to detect and retrieve relevant information from multimodal inputs (e.g., images, videos, text, tables) to answer factual questions.However, there have not been multimodal QA benchmarks for ToM, an ability fundamentally different from the kind of multimodal information retrieval tested in the existing benchmarks.</p>
<p>Machine Theory of Mind.There have been two main approaches to engineering machine ToM: end-to-end methods such as Theory of Mind neural networks (e.g., Rabinowitz et al., 2018), and modelbased methods such as Bayesian inverse planning (e.g., Baker et al., 2017;Shu et al., 2021).Both types focus mostly on unimodal data and simple domains.Recent studies have suggested that machine ToM may also emerge in LLMs such as GPT-4 (Kosinski, 2023;Bubeck et al., 2023).However, more systematic evaluations have shown that apparent ToM capacities in LLMs are not yet as robust as humans (Sap et al., 2022;Shapira et al., 2023;Sclar et al., 2023), and often fail to pass trivial variants of common tests (Ullman, 2023).Our BIP-ALM model builds on the strengths of these different methods.It extends Bayesian inverse planning to fuse multimodal data and conduct inference in complex scenarios with the use of language models.</p>
<p>3 MMToM-QA Benchmark</p>
<p>Overview</p>
<p>Our benchmark consists of 134 videos of a person looking for daily objects in household environments, in line with cognitive studies examining mental attributions to agents navigating an environment (e.g.Baker et al., 2017).On average, each video has 1,462 frames, depicting 36 human actions.Based on these videos, we constructed 600 questions about a person's goals and beliefs.Each question is paired with a clip of the full activity in a video (as RGB-D frames), as well as a text description of the scene and the actions taken by the person in that clip.All questions have two choices.The questions are categorized into seven types (see Figure 2), evaluating belief inference and goal inference in rich and diverse situations.Each belief inference type has 100 questions, totaling 300 belief questions; each goal inference type has 75 questions, totaling 300 goal questions.We provide another set of synthetic human behavior data in household environments for model training.This training set includes 1,000 procedurally synthesized videos with ground-truth annotations of the scene, objects, goals, and beliefs.statements is more likely to be true?</p>
<p>Question Types</p>
<p>Questions fall into two categories -belief inference and goal inference.There are several types within each category (Figure 2), evaluating different aspects of multimodal ToM reasoning.Unlike existing ToM evaluation that isolates goal and belief inference, questions in our benchmark require a joint inference of goal and belief, asking about one conditioned on another.Type 1.1: True belief, short-term.In the scenarios this type refers to, a person is about to open a container.The question focuses on an object the person has not seen so far and treats it as a hypothetical goal object.The question is then whether the person believes the object is in the container.This type examines true belief ; that is, the inference that the person believes the goal is in the container is more consistent with the current action, and it is also the same as the actual world state.Type 1.2: False belief, short-term.This type is similar to Type 1.1, but differs in a significant way: the hypothetical goal object is not inside the container that the person is about to open, so the person has a false belief.In this case, the correct answer should still be that it is more likely that the person thinks there is such a goal object inside the container, given the current action, even though they would not find what they want there in reality.</p>
<p>Type 1.3: Belief tracking, long-term.In the scenarios this type refers to, the person passes by a container but does not check it.After a while, they have still not found a goal object, but are also not going back to check the container they passed by.This suggests that they do not think the goal object is inside that container.This question tests whether a model can use the long-term observation of past actions to make judgments consistent with history, not just the most recent action.</p>
<p>Type 2.1: Goal inference given true belief.This question targets a person's unknown goal.In the scenarios that this type refers to, the person walks towards a container, where there is a hypothetical goal object that the person has not observed so far.The person, on the other hand, has observed the other hypothetical goal object but has not picked it up in the past.The correct inference is that it is more likely that the person wants the goal object that they have not seen so far and thinks it is inside the container (true belief).This type tests whether a model can infer the goal given a true belief.</p>
<p>Type 2.2: Goal inference given false belief.This type is similar to type 2.1, but the person has a hypothetical false belief.In these scenarios, a specific object is inside a container that the person is walking towards.However, the question states that this person thinks that there is no such object inside the container (false belief).So, the most likely explanation is that the person's goal is a different object, which they think might be inside the container.Type 2.3: Goal inference given updated belief.Unlike Type 2.1 and 2.2, in a Type 2.3 question, the video does not end with the person walking towards a container.Instead, the person opens the container and then closes it without taking an object from it.The correct inference is that the person's goal is an item not yet seen rather than anything inside the container.To correctly answer this type of question, a model has to infer how a person may update their belief and change their plan (e.g., closing the container without picking anything from it) to pursue the goal accordingly.Type 2.4: Goal inference given future actions.Questions in Type 2.1, 2.2, and 2.3 ask about goals that are consistent with the belief and the latest action.In contrast, in Type 2.4, a model needs to consider possible future actions as well.Specifically, one of the hypothetical goal objects is an unobserved object at a location that is still far away from the person and is not directly related to the latest action (which is walking to a nearby container).But, in these scenarios, the person is on a path to potentially reach the location of that object.For instance, in the Type 2.4 example illustrated in Figure 2, a person is walking towards the right side of the room, and so they might want to search through all possible locations on the right side, including the dishwasher.This gives rise to the correct answer, dish bowl, which is located inside the dishwasher.As such, Type 2.4 requires a model to reason about the spatial relationships (the locations of objects and the person as well as the person's heading direction) in a visual scene and predict possible future actions for a goal.</p>
<p>Procedural Generation</p>
<p>We designed a procedural generation method for creating the benchmark.First, we procedurally synthesized a large set of videos in a realistic household embodied simulator, VirtualHome-Social (Puig et al., 2020).As Puig et al. (2020) have demonstrated, such procedural video generation can create synthetic human activities that are human-like and well-annotated (including groundtruth states, goals, and beliefs).It also alleviates the concerns of cost and privacy that come with realworld human activity video collection.At each step in a video, we sampled a question type and two opposing hypotheses based on the definition of the type.Finally, we generated the text description and the question based on the ground-truth state, actions, and the sampled hypotheses using GPT-4 to create the text input for the question.Using the same procedural generation, we synthesize the videos in the training set.We provide more details in Appendix B.7.</p>
<p>Evaluation Protocol</p>
<p>We can evaluate a model in three conditions: (1) Multimodal QA in which both the video and text inputs are present, (2) Text QA with only the text input, and (3) Video QA with only the video input.We evaluated all models in a zero-shot manner, which is the standard setting in recent literature on ToM QA evaluation (Shapira et al., 2023).Crucially, we do not provide any example QAs during training.We expect a model to learn how a person updates their mental state and acts accordingly in a physical environment from the human behavior data in the training set, and generalize the learned knowledge to answer the questions at test time.</p>
<p>The BIP-ALM Model</p>
<p>To infer the mental state of a person based on video and text inputs, we propose a novel machine Theory of Mind method, Bayesian Inverse Planning Accelerated by Language Models (BIP-ALM), which builds on Bayesian inverse planning (BIP) (Baker et al., 2017).Prior works have shown that BIP can reverse engineer human ToM reasoning in simple domains.BIP-ALM extends BIP by (1) building unified representations about a scene, a person's actions, and the mental state hypotheses from multimodal inputs, and (2) finetuning a language model to efficiently conduct inverse symbolic planning, based on unified symbolic representations.</p>
<p>Figure 3 provides an overview of the method.We first extract symbolic representations of the states and actions from both the video and the text.We then align and fuse representations extracted from different modalities, to form a unified representation of the event and the physical scene.This unified representation allows us to use a principled method to infer a person's mental state based on inputs from any given modality.We then use an inverse symbolic planner to compare the two hypotheses extracted from the question and produce the answer.We introduce each module below and provide more details in Appendix C.   &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / O w H D R 4 v D 2 j f p g r x 6 z u o I H h P m G U = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 e K 9 g P a W D b b S b t 0 s w m 7 G 6 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o 6 j h V D B s s F r F q B 1 S j 4
Q k H Y s e d b E a Z f 0 6 b X A B O J j / j v u c Y = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 k J K I q M e i F 4 8 V 7 A e 2 M W y 2 2 3 b p Z h N 2 J 0 I J / R d e P C j i 1 X / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E h h 0 H W / n a X l l d W 1 9 c J G c X N r e 2 e 3 t L f f M H G q G a + z W M a 6 F V L D p V C 8 j g I l b y W a 0 y i U v B k O b y Z + 8 4 l r I 2 J 1 j 6 O E + x H t K 9 E T j K K V H v q B d 0 r C w H v E o F R 2 K + 4 U Z J F 4 O S l D j l p Q + u p 0 Y 5 Z G X C G T 1 J i 2 5 y b o Z 1 S j Y J K P i 5 3 U 8 I S y I e 3 z t q W K R t z 4 2 f T i M T m 2 S p f 0 Y m 1 L I Z m q v y c y G h k z i k L b G V E c m H l v I v 7 n t V P s X f m Z U E m K X L H Z o l 4 q C c Z k 8 j 7 p C s 0 Z y p E l l G l h b y V s Q D V l a E M q 2 h C 8 + Z c X S e O s 4 l 1 U 3 L v z c v U 6 j 6 M A h 3 A E J + D B J V T h F m p Q B w Y K n u E V 3 h z j v D j vB I b h h u B 7 U Q h j Q K B r W B 0 M / V b T 6 g 0 j + W D G S f o R 3 Q g e c g Z N V a 6 1 4 9 u r 1 x x q + 4 M Z J l 4 O a l A j n q v / N X t x y y N U B o m q N Y d z 0 2 M n 1 F l O B M 4 K X V T j Q l l I z r A j q W S R q j 9 b H b q h J x Y p U / C W N m S h s z U 3 x M Z j b Q e R 4 H t j K g Z 6 k V v K v 7 n d V I T X v k Z l 0 l q U L L 5 o j A V x M R k + j f p c 4 X M i L E l l C l u b y V s S B V l x q Z T s i F 4 i y 8 v k + Z Z 1 b u o u n f n l d p 1 H k c R j u A Y T s G D S 6 j B L d S h A Q w G 8 A y v 8 O Y I 5 8 V 5 d z 7 m r Q U n n z m E P 3 A + f w A C r 4 2 e &lt; / l a t e x i t &gt; s 0
&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t K 8 2 o m n / j L U 2 g 7 0 X I j M 4 u 4 M K F / A = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 e K 9 g P a W D b b S b t 0 s w m 7 G 6 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o 6 j h V D B s s F r F q B 1 S j 4 B I b h h u B 7 U Q h j Q K B r W B 0 M / V b T 6 g 0 j + W D G S f o R 3 Q g e c g Z N V a 6 p 4 9 u r 1 x x q + 4 M Z J l 4 O a l A j n q v / N X t x y y N  ), actions (a 1:t ), and the two hypotheses about the person's goal (g 1 and g 2 ) and belief (b t 1 and b t 2 ) for a question asked at time step t.
U B o m q N Y d z 0 2 M n 1 F l O B M 4 K X V T j Q l l I z r A j q W S R q j 9 b H b q h J x Y p U / C W N m S h s z U 3 x M Z j b Q e R 4 H t j K g Z 6 k V v K v 7 n d V I T X v k Z l 0 l q U L L 5 o j A V x M R k + j f p c 4 X M i L E l l C l u b y V s S B V l x q Z T s i F 4 i y 8 v k + Z Z 1 b u o u n f n l d p 1 H k c R j u A Y T s G D S 6 j B L d S h A Q w Gv i h M J c G Y T H 8 n f a E 5 Q z m 2 h D I t 7 K 2 E D a m m D G 1 C J R u C t / j y M m m e V b 2 L q n t / X q n d 5 H E U 4 Q i O 4 R Q 8 u I Q a 3 E E d G</p>
<p>Unified Symbolic Representations</p>
<p>Visual Perception.Our visual perception module processes visual data and transforms it into symbolic representations.For each frame, we adopt the method in Blukis et al. (2022) to create a voxel map and construct a scene graph.</p>
<p>Text Parsing.We use GPT-4 to parse text, to extract symbolic representations of the initial state as well as subsequent actions.GPT-4 first parses the text into three components -the description of the environment state, the human actions, and the question.Each component is further translated into symbolic representation by GPT-4.For state, we generate predicates such as In(apple, fridge).For action, we generate action commands such as walk towards kitchen.Finally, we translate the question into two hypotheses about the goal and the belief.For each hypothesis, the goal is represented by the goal object (e.g., apple), and the belief is represented by a predicate (e.g., In(apple, fridge)), or its negation (¬In(apple, fridge)), indicating the hypothetical location of the object.</p>
<p>Fusion.The fusion module aligns and integrates information from different input streams.Specifically, we transform the scene graphs from the video input to a set of predicates (similar to those extracted from the text), which describe the spatial relationships between entities and the status of objects.We first form the symbolic representation of the initial state by combining predicates from the video and the text.We then align the actions parsed from the text with the actions detected from the video, and truncate the video frames into several intervals, each corresponding to an action.We term each interval, a time step t.Starting from the initial state, we update the state predicates from the previous step with the new predicates obtained from the video frame corresponding to the current step.By doing so, we can construct a symbolic state sequence and a symbolic action sequence, as illustrated in Figure 3.In addition, we form two hypotheses based on the hypothetical goals and beliefs parsed from the question.</p>
<p>Inverse Symbolic Planner</p>
<p>We formulate an agent's behavior as a forward generative model using a Partially Observable Markov Decision Process (POMDP) (Kaelbling et al., 1998), defined by the tuple ⟨S, A, T , G, R, Ω, O, γ⟩.s t ∈ S and a t ∈ A are the state and the action at time t.T (s t |s, a) are the state transition probabilities.g ∈ G is a goal, which defines the reward of the agent r t = R(s t , a t , g). o t ∈ Ω is the agent's observation at t derived following the observation function, o t = O(s t ).Finally, γ ∈ (0, 1] is a discount factor.The belief of an agent is modeled as a probability distribution over the state b(s).In this work, we factorize the belief of the full state into beliefs about the possible locations of individual objects.Conditioned on both the goal and the belief, a rational agent will take actions based on the optimal policy π(a t |g, b t ) to maximize its return ∞ t=0 γ t r t .Given this forward generative model, we can conduct inverse inference about the agent's goal and belief.By assuming a deterministic state transition, we can jointly infer the goal and belief of an agent given observed states and actions as follows:
P (g, b t |s 1:t , a 1:t−1 ) ∝ t τ =1 π(a τ |g, b τ )P (b τ |b τ −1 , s τ ) • P (b 0 )P (g)(1)
Given two hypotheses about goal and belief,
H 1 = ⟨g 1 , b t 1 ⟩ and H 2 = ⟨g 2 , b t 2 ⟩,
we can evaluate which one is more likely to be true as
P (g 1 , b t 1 |s 1:t , a 1:t ) P (g 2 , b t 2 |s 1:t , a 1:t ) = π(a t |g 1 , b t 1 )P (b t 1 | bt−1 , s t ) π(a t |g 2 , b t 2 )P (b t 2 | bt−1 , s t ) • t−1 τ =1 π(a τ |g 1 , bτ ) t−1 τ =1 π(a τ |g 2 , bτ ) , (2)
where bτ is the estimated belief at a past step τ &lt; t.</p>
<p>Since the hypothetical belief in the question is about the belief at the current step, we estimate the belief in the past steps to form a full belief hypothesis.In this work, we assume a uniform distribution for the initial belief.We represent the agent's observation as the subset of the state predicates that the agent can observe, and update the agent's belief accordingly.This gives us an estimated agent belief bτ = bτ (s τ ) at each past step.Based on Eqn.</p>
<p>(2), we need to evaluate (1) the likelihood of the last action a t given the hypothetical belief and goal (π(a t |g, b t )), (2) the probability of a hypothetical belief at the last step (P (b t | bt−1 , s t )), and (3) the likelihood of all past actions given the hypothetical goal and the estimated belief prior to the current step ( t−1 τ =0 π(a τ |g, bτ )).The computational bottleneck here is the policy.Conventional methods rely on planning or reinforcement learning to acquire such a policy.Inspired by the recent use of LLMs for decision-making (Huang et al., 2022;Li et al., 2022), we adopt a language model to amortize the policy.In particular, we symbolically represent the belief at each step as a list of possible locations of the corresponding goal object.We then prompt a language model with the symbolic representations of the state s t , goal g, and estimated belief bt , and generate the likelihood of the observed action a t based on the output logits.Figure 5 in Appendix B.2 illustrates how the likelihood estimation works in a qualitative example.We can finetune the language model on the ground-truth state, belief, goal, and action sequences in the training dataset.</p>
<p>Experiments</p>
<p>Baselines</p>
<p>Human baseline.We conducted a human experiment to validate our questions, and to evaluate human performance.Participants (N=180) were recruited online via Prolific.We randomly sampled 120 questions (20% of all questions) from all types.</p>
<p>Large language models (LLMs).We evaluate LLMs on the text-only version of MMToM-QA, including GPT-4 (OpenAI, 2023), GPT-3.5, GPT-J (6B) (Wang and Komatsuzaki, 2021), and LLaMA 2 (7B) (Touvron et al., 2023).We also evaluate Sim-ToM (Wilf et al., 2023) and SymbolicToM (Sclar et al., 2023), two recent approaches that improve ToM in LLMs through better prompting.We apply them to GPT-4 and create two baselines: SimToM w/ GPT-4 and SymbolicToM w/ GPT-4.For all LLMs, we prompt them with the text input.</p>
<p>Large multimodal models (LMMs).We evaluate GPT-4V (OpenAI, 2023), InstructBLIP (Dai et al., 2023),Video-LLaMA 2 (Zhang et al., 2023), and LLaVA (Liu et al., 2023) in both multimodal and video-only conditions.For all LMMs, we uniformly sample a few frames from each video following prior works (Dai et al., 2023).We use the largest versions for all LMMs.</p>
<p>We finetuned GPT-J (6B) and LLaMA 2 (7B) and created two BIP-ALM models: BIP-ALM w/ GPT-J and BIP-ALM w/ LLaMA 2.</p>
<p>Results</p>
<p>We summarize the main results in Figure 4. On the multimodal version, humans achieve 93% accuracy averaging across question types.For each tested question, the majority of the participants chose the correct answer, validating our question designs.</p>
<p>When only given unimodal data, human performance drops in accuracy overall, with video-only questions being harder to answer than text-only.While most questions suffer, the performance of text-based reasoning on the true-belief/false-belief remains the same.This is of note as these questions have been the main target of text ToM QAs.</p>
<p>The baselines show performance that is close to random guessing across all types in all conditions, except GPT-4(V) and SymbolicToM w/ GPT-4 in multimodal and text-only conditions, as shown in Table 1.GPT-4(V) reaches human-level accuracy on Type 1.1 and shows competitive performance on Type 1.3.However, it also makes systematic mistakes in harder questions that involve false beliefs (Type 1.2).This suggests that GPT-4(V) can understand the true world state from the text but confuses belief with the true world state.GPT-4(V) also struggles with goal inference.Its accuracy on Type 2.3 is particularly low.We hypothesize that this is because it mistakenly thinks that the goal has to be one of the objects inside the container the person opens and fails to recognize that the person updates the belief after checking inside the container.SymbolicToM can improve GPT-4's performance on a few types by removing irrelevant text, but it still struggles with harder types.</p>
<p>Our BIP-ALM models outperform all baselines by a large margin.Even without finetuning, as the ablated study in Appendix B.1 shows, our model with small pretrained LMs can already achieve better results than using much larger pretrained LMs (e.g., GPT-4) alone.BIP-ALM also can flexibly conduct ToM reasoning with any unimodal or multimodal data thanks to the unified representations.</p>
<p>As reported in Appendix B.1, we examined the effect of few-shot or chain-of-thought prompting (Table 4) and model sizes (Table 5).We did not find any setting that can consistently improve a baseline's performance in all types.We also finetuned Video-LLaMA 2 (13B) on our training set for video instruction tasks following Zhang et al. (2023).As Table 6 shows, the finetuned model performs moderately better in a few simpler question types (e.g., Type 1.1), but its overall performance is still not better than chance, unlike our method.</p>
<p>Generalization evaluation.We created an additional test set, the human test set, for generalization evaluation.It has 40 videos and 120 questions.To generate the videos in this set, we used 2 new apartments unseen in the training set and the main test set.We recruited 3 participants who had no prior exposure to the system to control the avatar to reach assigned goals via the human interface so that we could collect real human belief updates and human actions.We then used the same method to generate the questions.We report the model performance on this human test set in Table 7 (Appendix B.1).It shows that our method can generalize to both real human behavior and unseen physical environments.</p>
<p>Discussion &amp; Conclusion</p>
<p>We presented MMToM-QA, the first multimodal benchmark for machine ToM.We conducted a systematic evaluation of human performance, stateof-the-art methods, and our BIP-ALM model.We summarize the key findings as follows.</p>
<p>How does each modality contribute to ToM? From a video, a model gets the dynamic state change as well as what objects the agent is walking towards and is passing by at a given step.A model needs this information to determine the agent's expected action plans given its mental state.Because of the partial observations caused by the limited camera view and occlusion, the text provides additional state information that is sometimes unavailable in the video.A model requires information about the true world state to determine an agent's observation and whether it has a false belief.This is illustrated in Figure 6 in Appendix B.2. Do LLMs and LMMs understand ToM? GPT-4 and GPT-4V excelled on questions that only require retrieving information about the true world state.However, they still cannot reason about the mental state of a person and track the change in the mental state over time.We found more specifically that they have poor judgment on goals, which was not evaluated in existing text-based benchmarks.</p>
<p>What are the successes and failures of BIP-ALM?Instead of directly mapping the multimodal inputs to beliefs and goals, BIP-ALM conducts model-based inference by imaging possible actions given a hypothetical mental state and state context via language models.This results in a better performance in the main test set and enables the method to generalize to real human behavior in unseen environments.We also observed several failures.First, BIP-ALM cannot imagine missing state information from videos.Second, it uses symbolic state representations and only conducts symbolic planning.However, in Type 2.4, one needs to imagine the future path of a person given continuous spatial information such as the current path, facing direction, and the locations of potential goal objects.Finally, the action likelihood estimated by the LMs sometimes could be inaccurate due to LMs' occasional failures in planning.</p>
<p>Limitations and future work.First, MMToM-QA only includes videos of people looking for objects in household environments.In the future, we would like to extend this to more diverse scenarios.Second, we intend to incorporate additional ToM concepts such as desires, emotions, and constraints in a future version of the benchmark.Finally, we intend to enrich the representations in BIP-ALM with broader relations and predicates, extending its reach even further to more complex scenes and human behaviors.This could be potentially achieved by finetuning larger LMs in broader datasets that are collected in simulators or crowdsourced (text descriptions of real-world human behaviors).</p>
<p>Ethics Statement</p>
<p>The ability to understand humans' mental states is a crucial foundation for building human-centered AI systems that can safely and cooperatively interact with humans.Benchmarking state-of-the-art machine learning models' Theory of Mind capacity can provide us insights into whether current machine learning models can indeed adequately understand humans.We believe that our benchmark is a significant contribution towards this effort.Both our MMToM-QA benchmark and the BIP-ALM model are built on prior studies in cognitive science and thus are grounded in the cognitive theories of human behaviors.Such cognitively grounded benchmarks and models can help develop AI systems that are more aligned with humans' social cognition.Recognizing the need to ensure the diversity and fairness of our benchmark, we have made our best effort to increase the diversity of the avatars used to generate the videos.We have also validated our benchmark in a human experiment.We welcome feedback and suggestions from the community to further improve our benchmark.</p>
<p>A Comparison of Theory of Mind Benchmarks</p>
<p>We provide a comparison of Theory of Mind benchmarks in Table 2, summarizing the evaluated ToM concepts, the size of the test set, available modalities of the inputs, the generation method, and the evaluation for each benchmark.From the table, we can see that our benchmark is the only one that provides multimodal inputs.Additionally, commonly used text-based ToM QA benchmarks do not evaluate goal inference, whereas questions in our benchmark ask about both goals and beliefs.</p>
<p>B Benchmark Details B.1 More Quantitative Results</p>
<p>We conducted an ablated study to show the effect of finetuning language models for BIP-ALM.As Table 3 shows, finetuning GPT-J and LLaMA 2 significantly boosts the performance of BIP-ALM.It is also interesting to see that even before finetuning, BIP-ALM with pretrained GPT-J or LLaMA can already outperform much larger models including GPT-4.This further demonstrates the advantage of conducting inverse symbolic planning for multimodal ToM reasoning.We evaluated open-sourced models (LLaMA 2, InstructBLIP, and Video-LLaMA 2) with different model sizes (Table 5 in Appendix B.1).All baselines performed no better than chance, regardless of the model sizes.As shown in Table 4 in Appendix B.1, we found no meaningful improvement for almost all baselines after using different few-shot or chain-of-thought Kojima et al. ( 2022) prompting.Only GPT-4 in the text-only condition has an improvement in simple types (e.g., Type 1.3) with few-shot prompting.The accuracies for LLaVA (w/ 1-shot) and LLaVA (w/ 2-shot) are notably low.The model often fails to generate options a or b, as it faces difficulties in processing prompts of this length.We finetuned Video-LLaMA 2 (13B) on our training set for video instruction tasks following Zhang et al. ( 2023).As Table 6 shows, the finetuned model performs moderately better in a few simpler question types (e.g., Type 1.1), but its overall performance is still not better than chance.</p>
<p>We further evaluated the generalization of models on the human test set with real human behaviors in unseen environments (Table 7).</p>
<p>B.2 Qualitative Results</p>
<p>Figure 5 demonstrates how the inverse symbolic planner enabled by the language model in our BIP-ALM model can estimate the likelihood of a given hypothesis.In Figure 5A, given the goal of getting a water glass, our model reasons that Elizabeth is more likely to open the microwave if she believes that there is a water glass inside the microwave, even though there is not any water glass inside the microwave according to the true world state.By imagining reasonable actions conditioned on hypothesized mental states, our model can successfully infer that Elizabeth has a false belief.In contrast, GPT-4 selects (b) as the more likely option, failing to recognize the false belief.Figure 5B depicts how the likelihood of a hypothesis changes after the model observes different actions.Specifically, in this case, the model first thinks that it is more likely that Karen is going to open the oven if the goal is to get a plate because there is a plate inside the oven.However, if the goal is truly to get a plate, at the next step, Karen should not close the oven but pick up the plate instead.If the goal is not to get a plate but a salmon, on the other hand, then Karen should close the oven and continue to look for salmon in other places.Therefore, the fact that Karen closes the oven without picking up the plate suggests that it is unlikely that her goal is to get a plate and that it is still quite possible that she wants to get a salmon instead.The action likelihood ratios estimated by our model at these two steps reflect this reasoning.Consequently, our model answers the question correctly.</p>
<p>Figure 6 illustrates how BIP-ALM may form different state information from different modalities and how different state information may shape the policy estimated by our model.In particular, from the video, we know that the person is close to the microwave but we do not know where the cupcake is.So the policy conditioned on the state extracted from the video (S video-only ) tries to guess where the person is going to look for the cupcake (e.g., cabinet).From the text, we know that there is a cupcake inside the microwave, but we do not know that the agent is already close to the microwave.Thus the policy conditioned on the state extracted from the</p>
<p>… …</p>
<p>Action likelihood ratio by BIP-ALM at :</p>
<p>Action likelihood ratio by BIP-ALM at :</p>
<p>A Action likelihood given different beliefs B Likelihood of given hypothesis changes over time &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N A 5 2 9 J + + q M z /
V A A f L a K T B X 9 y Y A w = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B i y U R U Y 9 F L x 4 r 2 g 9 o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I J H C o O t + O 4 W V 1 b X 1 j e J m a W t 7 Z 3 e v v H / Q N H G q G W + w W M a 6 H V D D p V C 8 g Q I l b y e a 0 y i Q v B W M b q d + 6 4 l r I 2 L 1 i O O E + x E d K B E K R t F K D 3 j m 9 c o V t + r O Q J a J l 5 M K 5 K j 3 y l / d f s z S i C t k k h r T 8 d w E / Y x q F E z y S a m b G p 5 Q N q I D 3 r F U 0 Y g b P 5 u d O i E n V u m T M N a 2 F J K Z + n s i o 5 E x 4 y i w n R H F o V n 0 p u J / X i f F 8 N r P h E p S 5 I r N F 4 W p J B i T 6 d + k L z R n K M e W U K a F v Z W w I d W U o U 2 n Z E P w F l 9 e J s 3 z q n d Z d e 8 v K r W b P I 4 i H M E x n I I H V 1 C D O 6 h D A x g M 4 B l e 4 c 2 R z o v z 7 n z M W w t O P n M I f + B 8 / g C 7 N Y 1 v &lt; / l a t e x i t &gt; t 1
&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P r 5 a P g X p c w 4 N a U 2 P S o 9 h h y m m H
U g = " &gt; A A A B 6 H i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 J 0 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V H J o 8 l r H u B M y A F A q a K F B C J 9 H A o k B C O x j f z f z 2 E 2 g j Y v W A k w T 8 i A 2 V C A V n a K U G 9 s s V t + r O Q V e J l 5 M K y V H v l 7 9 6 g 5 i n E S j k k h n T 9 d w E / Y x p F F z C t N R L D S S M j 9 k Q u p Y q F o H x s / m h U 3 p m l Q E N Y 2 1 L I Z 2 r v y c y F h k z i Q L b G T E c m W V v J v 7 n d V M M b / x M q C R F U H y x K E w l x Z j O v q Y D o Y G j n F j C u B b 2 V s p H T D O O N p u S D c F b f n m V t C 6 q 3 l X V b V x W a r d 5 H E V y Q k 7 J O f H I N a m RJ x G H A j j T v P m Q c p N A x Q o I R z 6 4 C p X M J Z f n m 0 0 M + u w H l h 9 F e c W x g p N t O i E J x h o C b d 6 6 x w j F e Z F V t s X G F 9 k C F 8 w 4 p L 4 4 G a K 9 A 1 / U 5 n L e u Z V E b X O z Q f 4 4 4 f 4 / b v t n f p A 7 Z G C t b m 3 q 7 / M 8 d K h v D P m D + u p Z S D t P 9 + d 9 L t J f 2 k K X o X p C 3 o k b a O J 9 0 f 2 d T w U o F G L p n 3 w z S x O K q Y Q 8 E l 1 J 2 s 9 G A Z v 2 Q z G A a o m Q I / q p o N 1 P R t Y K a 0 M C 4 c j b R h b 3 d U T H k / V 3 l w K o Y X f l l b k P d p w x K L / V E l t C 0 R N L 8 J K k p J 0 d D F O u l U O O A o 5 w E w 7 k R 4 K + U X L K w U w 9 I 7 Y Q j p 8 p f v g t P d f v q h n 5 z s 9 Q 4 / t + N Y I 2 / I J t k i K f l I D s k X c k w G h E e f o m m k I h 0 f x S K 2 s b u x x l H b s 0 H + q r j 6 B V O O 2 1 o = &lt; / l a t e x i t &gt; ⇡(a t = close oven|g = salmon, b t , s t )⇡(a t 1 |g = salmon, b t 1 , s t 1 ) ⇡(a t = close oven|g = plate, b t , s t )⇡(a t 1 |g = plate, b t 1 , s t 1 ) = 1.32 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U b W Q z b h V V x 8 I f w H 9 B X q q s i O y u r Y = " &gt; A A A C d n i c j V F N b x M x E P U u F N p Q I I V T h V R Z j S q K V K J d i i i X S l V 7 4 V g k 0 l b K h m j W m U
2 t e m 3 L n q 2 I l v 0 J / D l u / A 4 u H H G S P Z S 2 B 0 a y / P T e f N h v c q u k p y T 5 F c U P H q 4 8 e r y 6 1 n m y / v T Z 8 + 7 G i z N v K i d w I I w y 7 i I H j 0 p q H J A k h R f W I Z S 5 w v P 8 6 m S u n 1 + j 8 9 L o L z S z O C p h q m U h B V C g x t 0 f W e F A 1 J m V u / C 1 p r d p c 5 g R f q P a W N T c X K N u + H c + b U k P q j S 6 2 e P 5 M n f P L + 8 3 z f 9 2 s A o I 7 2 t w m P Q P 9 s f d X t
J P F s H v g r Q F P d b G 6 b j 7 M 5 s Y U Z W o S S j w f p g m l k Y 1 O J J C Y d P J K o 8 W x B V M c R i g h h L 9 q F 7 Y 1 v C d w E x 4 Y V w 4 m v i C v V l R Q + n 9 r M x D Z g l 0 6 W 9 r c / I + b V h R 8 X F U S 2 0 r Q i 2 W g 4 p K c T J 8 v g M + k Q 4 F q V k A I J w M b + X i E s I e K G y q E 0 x I b 3 / 5 L j h 7 1 0 8 / 9 J P P 7 3 t H x 6 0 d q + w V 2 2 a 7 L G U H 7 I h 9 Y q d s w A T 7 H W 1 G 2 1 E v + h N v x T v x 6 2 V q H L U 1 L 9 k / E S d / A f j e v t A = &lt; / l a t e x i t &gt; ⇡(a t 1 = open oven|g = salmon, b t 1 , s t 1 ) ⇡(a t 1 = open oven|g = plate, b t 1 , s t 1 ) = 0.73 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W v / q q Q Z X b v l l 3 G t 6 J n V x a g a V 0 k E = " &gt; A A A C x n i c p V F N a 9 t A E F 2 p H 0 n d L 6 c 9 9 r L U F B w w R g q l z S U Q m k t 6 S 6 F O A p Z r R u u R s m S 1 K 3 Z H c Y 0 q 6 G / s r Y f + l 6 4 c H V y n P X V g 4 f H e v H 2 7 M 2 m p p K M o + h m E 9 + 4 / e L i z + 6 j 3 + M n T Z 8 / 7 e y / O n a m s w I k w y t j L F B w q q X F C k h R e l h a h S B V e p N c n r X 5 x g 9 Z J o z / T q s R Z A b m W m R R A n p r 3 f y W Z B V E n p R z C l 5 q a o 4 T w K 9 W m R M 0 L K a x Z w g 0 2 / B v P O 2 U J h J b n C p x r R j z d 8 H z U w w 1 x 1 N r X 7 v 1 m 5 N q 2 / e Z / Y z T m / 4 y y W 1 l H 8 f j w Y N 4 f R O N o X f w u i D s w Y F 2 d z f s / k o U R V Y G a R H v 1 N I 5 K m t V g S Q q F T S + p H J Y g r i H H q Y c a C n S z e r 2 G h r / x z I J n x v q j i a / Z T U c N h X O r I v W d B d C V 2 9 Z a 8 m / a t K L s c F Z L X V a E W t w G Z Z X i Z H i 7 U 7 6 Q F g W p l Q c g r P R v</p>
<p>Action likelihood ratio by BIP-ALM at :</p>
<p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P r 5 a P g X p c w 4 N a U 2</p>
<p>P S o 9 h h y m m H U g = " &gt;
A A A B 6 H i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 J 0 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V H J o 8 l r H u B M y A F A q a K F B C J 9 H A o k B C O x j f z f z 2 E 2 g j Y v W A k w T 8 i A 2 V C A V n a K U G 9 s s V t + r O Q V e J l 5 M K y V H v l 7 9 6 g 5 i n E S j k k h n T 9 d w E / Y x p F F z C t N R L D S S M j 9 k Q u p Y q F o H x s / m h U 3 p m l Q E N Y 2 1 L I Z 2 r v y c y F h k z i Q L b G T E c m W V v J v 7 n d V M M b / x M q C R F U H y x K E w l x Z j O v q Y D o Y G j n F j C u B b 2 V s p H T D O O N p u S D c F b f n m V t C 6 q 3 l X V b V x W a r d 5 H E V y Q k 7 J O f H I N a m R e 1 I n T c I J k G f y S t 6 c R + f F e X c + F q 0 F J 5 8 5 J n / g f P 4 A 4 c + M / Q = = &lt; / l a t e x i t &gt; t
Figure 5: Examples of how BIP-ALM evaluates the likelihood of different hypotheses via the action likelihood estimation from the language model.The results here are based on BIP-ALM with finetuned LLaMA 2. The green option in each example is the correct answer and BIP-ALM selects the correct answers in both cases.The blue panels show the likelihood ratio estimated by the language model at a certain step for each example, explaining how BIP-ALM can come to the correct conclusions by conducting inverse planning via a language model.(A) It is more likely for Elizabeth to open the microwave if she believes that there is a water glass inside the microwave and that she wants to get a water glass, even though there is not any water glass inside the microwave (i.e., she has a false belief)).(B) The likelihood of hypothesis will change after the model observes more actions.</p>
<p>text (S text-only ) predicts that the person is going to walk towards the microwave.After fusing information from both the video and the text, we then know the full state information.Conditioned on this fused state S multimodal , the policy then predicts that the person is going to open the microwave, which is the ground-truth action of the person at the next step.This demonstrates that BIP-ALM can more accurately estimate the person's policy when having access to both modalities, which explains why it performs the best in the multimodal condition.</p>
<p>B.3 Discussion on SimToM and SymbolicToM</p>
<p>The two recent approaches evaluated in our benchmark, SimToM (Wilf et al., 2023) and Symbolic-ToM (Sclar et al., 2023) have previously shown promising results in existing text-based ToM QA benchmarks.However, our experimental results demonstrate that there is still a large gap between their performance and the human performance on our MMToM-QA benchmark.We provide more discussions on these two recent approaches as below.</p>
<p>SimToM utilizes the concept of perspectivetaking to filter context based on what the character in question knows before answering a question about their mental state.This approach marginally in the questions.This approach of filtering out useful information enhances the performance of LLMs (e.g., GPT-4).</p>
<p>On the other hand, the performance increase of SymbolicToM on MMToM-QA is not as significant as on the ToMi benchmark.The ToMi benchmark only has simple scenarios with a few objects and locations, and very short action sequences.In constant, MMToM-QA demands the inference of mental states from long human activities in complex environments.Given the observations, the inference in MMToM-QA also has a varying degree of uncertainty.Therefore, in contrast to achieving 100% accuracy on the ToMi benchmark, Symbol-icToM with GPT-4 has an overall accuracy of 63% on MMToM-QA.This is lower than the accuracy of BIP-ALM which uses much smaller language models.</p>
<p>In addition, unlike humans and our BIP-ALM model, SimToM and SymbolicToM cannot answer</p>
<p>Fused Symbolic Representations</p>
<p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P r 5 a P g X p c w 4 N a 2 P S o 9 h h y m m H U g = " &gt; A A A B 6 H i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 J 0 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8
M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V H J o 8 l r H u B M y A F A q a K F B C J 9 H A o k B C O x j f z f z 2 E 2 g j Y v W A k w T 8 i A 2 V C A V n a K U G 9 s s V t + r O Q V e J l 5 M K y V H v l 7 9 6 g 5 i n E S j k k h n T 9 d w E / Y x p F F z C t N R L D S S M j 9 k Q u p Y q F o H x s / m h U 3 p m l Q E N Y 2 1 L I Z 2 r v y c y F h k z i Q L b G T E c m W V v J v 7 n d V M M b / x M q C R F U H y x K E w l x Z j O v q Y D o Y G j n F j C u B b 2 V s p H T D O O N p u S D c F b f n m V t C 6 q 3 l X V b V x W a r d 5 H E V y Q k 7 J O f H I N a m
R e 1 I n T c I J k G f y S t 6 c R + f F e X c + F q 0 F J 5 8 5 J n / g f P 4 A 4 c + M / Q = = &lt; / l a t e x i t &gt; t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q k o K q t O / z L m S q m + a Z O i P J q Q / e w 8 = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B i y U R U Y 9 F L x 4 r 2 A 9 o Q 9 l s N + 3 a T T b s T o Q S + h + 8 e F D E q / / H m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I J H
C o O t + O 4 W V 1 b X 1 j e J m a W t 7 Z 3 e v v H / Q N C r V j D e Y k k q 3 A 2 q 4 F D F v o E D J 2 4 n m N A o k b w W j 2 6 n f e u L a C B U / 4 D j h f k Q H s Q g F o 2 i l J p I z 4 p F e u e J W 3 R n I M v F y U o E c 9 V 7 5 q 9 t X L I 1 4 j E x S Y z q e m 6 C f U Y 2 C S T 4 p d V P D E 8 p G d M A 7 l s Y 0 4 s b P Z t d O y I l V + i R U 2 l a M Z K b + n s h o Z M w 4 C m x n R H F o F r 2 p + J / X S T G 8 9 j M R J y n y m M 0 X h a k k q M j 0 d d I X m j O U Y 0 s o 0 8 L e S t i Q a s r Q B l S y I X i L L y + T 5 n n V u 6 y 6 9 x e V 2 k 0 e R x G O 4 B h O w Y M r q M E d 1 K E B D B 7 h G V 7 h z V H O i / P u f M x b C 0 4 + c w h / 4 H z + A L 4 p j e 0 = &lt; / l a t e x i t &gt; t 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K V R U f Y k t X / 7 Q a m 7 t 8 W 0 3 3 y a o B W E = " &gt; A A A B / X i c b V D L S s N A F J 3 4 r P U V H z s 3 w S K 4 s S Q i 6 r L o x m U F + 4 A 2 h M l 0 0 g 6 d T M L M j R h D 8 F f c u F D E r f / h z r 9 x 0 m a h r Q f u 5 X D O v c y d 4 8 e c K b D t b 2 N h c W l 5 Z b W y V l 3 f 2 N z a N n d 2 2 y p K J K E t E v F I d n 2 s K G e C t o A B p 9 1 Y U h z 6 n H b 8 8 X X h d + 6 p V C w S d 5 D G 1 A 3 x U L C A E Q x a 8 s x 9 5 W V 9 o A + Q F e 0 k E j z N c 8 + s 2 X V 7 A m u e O C W p o R J N z / z q D y K S h F Q A 4 V i p n m P H 4 G Z Y A i O c 5 t V + o m i M y R g P a U 9 T g U O q 3 G x y f W 4 d a W V g B Z H U J c C a q L 8 3 M h w q l Y a + n g w x j N S s V 4 j / e b 0 E g k s 3 Y y J O g A o y f S h I u A W R V U R h D Z i k B H i q C S
a S 6 V s t M s I S E 9 C B V X U I z u y X 5 0 n 7 t O 6 c 1 + 3 b s 1 r j q o y j g g 7 Q I T p G D r p A D X S D m q i F C H p E z + g V v R l P x o v x b n x M R x e M c m c P / Y H x + Q O / R 5 Y P &lt; / l a t e x i t &gt; s text-only &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 q 8 d T 1 6 j t D 5 b S y l
s O E 4 z R k m D p N k = " &gt; A A A B / n i c b V B N S 8 N A E N 3 U r 1 q / o u L J y 2 I R P J V E R D 0 W v X i s Y G u h D W G z 2 b R L d 5 O w O x F L C P h X v H h Q x K u / w 5 v / x m 2 b g 7 Y + G H i 8 N 8 P M v C A V X I P j f F u V p e W V 1 b X q e m 1 j c 2 t 7 x 9 7 d 6 + g k U 5 S 1 a S I S 1 Q 2 I Z o L H r A 0 c B O u m i h E Z C H Y f j K 4 n / v 0 D U 5 o n 8 R 2 M U + Z J M o h 5 x C k B I / n 2 g f b z P r B H y G U m g M s k J K I o f L v u N J w p 8 C J x S 1 J H J V q + / d U P E 5 p J F g M V R O u e 6 6 T g 5 U Q B p 4 I V t X 6 m W U r o i A x Y z 9 C Y S K a 9 f H p + g Y + N E u I o U a Z i w F P 1 9 0 R O p N Z j G Z h O S W C o 5 7 2 J + J / X y y C 6 9 H I e p x m w m M 4 W R Z n A k O B J F j j k i l E Q Y 0 M I V d z c i u m Q K E L B J F Y z I b j z L y + S z m n D P W 8 4 t 2 f 1 5 l U Z R x U d o i N 0 g l x 0 g Z r o B r V Q G 1 G U o 2 f 0 i t 6 s J + v F e r c + Z q 0 V q 5 z Z R 3 9 g f f 4 A t i C W n Q = = &lt; / l a t e x i t &gt;s F h 3 a h Y M 6 g x J Z R u B t S A 4 B J q y F F A M 9 J A w 0 B A I x j d T v 3 G G L T h S j 5 g E k E n p A P J + 5 x R t F K 3 U D T d N s I j p m P e A 3 W m p E g m 3 U L J K 3 s z u M v E z 0 i J Z K h 2 C 1 / t n m J x C B K Z o M a 0 f C / C T k o 1 c i Z g k m / H B i L K R n Q A L U s l D c F 0 0 t n x E / f E K j 2 3 r 7 Q t i e 5 M / T 2 R 0 t C Y J A x s Z 0 h x a B a 9 q f i f 1 4 q x f 9 1 J u Y x i B M n m i / q x c F G 5 0 y T c H t f A U C S W U K a 5 v d V l Q 6 o p Q 5 t Xl R S v i V a l G V A h b Y H P D s k g 6 4 m 7 a 0 r h R J b X J Q z V F a t 9 y J M g 4 H M M f C G 7 U U 1 n y B a 9 z 3 r a 3 + s R h 0 t 2 3 K I j f k 0 m Z / G v X j Y b w u f h M k H e i z r k 6 n 0 Y 8 0 N 6 L S f q R Q 4 N w 4 i U u a 1 G B J C o V N L 6 0 c l i A W f q m x h w V o d J N 6 H V T D X 3 k m 5 z N j / S
m I r 9 m / O 2 r Q z i 1 1 5 p 0 a 6 M p t a i v y f 9 q 4 o t m 7 S S 2 L s i I s R D t o V i l O h q 9 S 5 7 m 0 K E g t P Q B h p d + V i y u w I M j / T c + H k G w + + S Y 4 e z 1 M 3 g z j T 4 f 9 4 / d d H F v s J d t j A 5 a w t + y Y f W C n b M Q E + x V s B 0 + D Z 8 H v M A q f h y 9 a a x h 0 P U / Y P x X u / Q E i z 7 p G &lt; / l a t e x i t &gt; ⇡(a t |g = cupcake, b t , s multimodal = {Close(agent, microwave), In(cupcake, microwave), 1 6 7 f 2 N 6 5 2 b 5 1 + 8 7 d e 5 3 7 D 0 6
• • • }) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X 3 K L X R g M V 4 4 d z t V 5 s 4 L 5 C z V u W s Q = " &gt; A A A C S X i c b V B N b x M x F P S m Q E v 4 C n D k Y h E h p V K J d h G i X C p V 9 M K x S K S t l E 3 D 2 7 c v q R W v v b L f B q J l / x 4 X b t z 4 D 1 w 4 g B A n n I 8 D t I x k a T Q z z 8 + e r N T K c x x / j V p b+ g m K R / k V r K V 8 q y L 0 c i V F + o = " &gt; A A A C R 3 i c b V B L b x M x E P a G R 0 t 4 B T h y s Y i Q U i l E u w h R L p U q u M C t S K S t l E 2 i 2 c k k t e K 1 V / Z s I V r 2 3 3 H h y o 2 / w I U D C H H E S f Y A L S P Z + v Q 9 N P a X F V p 5 j u O v U e v K 1 W v X d 3 Z v t G / e u n 3 n b u f e / W N v S 4 c 0 R K u t O 8 3 A k 1 a G h q x Y 0 2 n h C P J M 0 0 m 2 f L X W T 8 7 J e W X N O 1 4 V N M 5 h Y d R c I X C g p p 1 J W q g e T F h + l I u D l O k D V 1 g W C E u q + z K b c F / 6 6 Z Z e X 0 +i t W J 4 V D m W m p T C 5 k F K R S U U a C E W m y A R a G E a t i / G f v V A R g r t L r H Y Q z N i H W V 6 A j O 0 E m t 3 G E D 4 R F T H Y O i k e B G P 7 A B j F q 5 v F / w J 6 C L J J i R P J m h 1 M p 9 N d q a J x E o 5 J J Z W w / 8 G J s p M y i 4 h F G 2 k V i I G e + z L t Q d V S w C 2 0 w n 5 4 / o i V P a t K O N K 4 V 0 o v 6 e S F l k 7 T A K X W f E s G f n v9 R v 3 X G k R h X c 4 j n k n g H 4 o e o I B G q l r F 9 v I H z A d g R x S j E a g f E 0 Z e O Y 2 n H T t k l N 2 Z q D L x M 1 I i W S o d u 2 v t h + x J O A h M g l a t 1 w n x k 4 K C g W T f F J o J 5 r H w I b Q 5 y 1 D Q w i 4 7 q S z E B N 6 b B S f 9 i J l T o h 0 p v 7 e S C H Q e h x 4 Z j I A H O h F b y r + 5 7 U S 7 F 1 2 U h H G C f K Q z R / q J d L E p d N G q C 8
U Z y j H h g B T w v y V s g E o Y G h 6 K 5 g S 3 M X I y 6 R + W n b P y 8 7 t W a l y l d W R J 0 V y R E 6 I S y 5 I h d y Q K q k R R h 7 J M 3 k l b 9 a T 9 W K 9 W x / z 0 Z y V 7 R y S P 7 A + f w B 7 K Z i g &lt; / l a t e x i t &gt; walk towards cabinet &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P M 4 3 / 0 4 P 1 2 T V c 2 n 8 2 K g u a 7 T 7  In this example, we can get different state information from different modalities.From the video frame at step t, we can see that the person is close to the microwave, but we do not know what is inside the microwave or where we can find a cupcake.From the text, we know that there is a cupcake inside the microwave, but we do not know if the person is close to the microwave.By combining the two modalities, we can form a full picture of the world state.In the blue panel at the bottom, we show how state information from different modalities may shape the policy estimated by the model.multimodal or video-only questions, since they are purely text-based methods.
M 2 k = " &gt; A A A C B n i c b V C 7 S g N B F J 3 1 G e N r 1 V K E w S B Y h V 0 R t Q z a W E Y w D 0 i W M D t 7 k w y Z f T B z N z E s q W z 8 F R s L R W z 9 B j v / x s m j 0 M Q D A 4 d z 7 u X O O X 4 i h U b H + b a W l l d W 1 9 Z z G / n N r e 2 d X X t v v 6 r j V H G o 8 F j G q u 4 z D V J E U E G B E u q J A h b 6 E m p + 7 2 b s 1 / q g t I i j e x w m 4 I W s E 4 m 2 4 A y N 1 L K P m g g P m A 2 Y 7 F G M B 0 w F m o a C K 0 P 7 M G r Z B a f o T E A X i T s j B T J D u W V / N Y O Y p y F E y C X T u u E 6 C X o Z U y i 4 h F G + m W p I G O + x D j Q M j V g I 2 s s m M U b 0 x C g B b c f K v A j p R P 2 9 k b F Q 6 2 H o m 8 m Q Y V f P e 2 P x P 6 + R Y v v K y 0 S U p A g R n x 5 q p 9 I E p u N O a C A U c J R D Q x h X w v y V 8 i 5 T j K N p L m 9 K c O c j L 5 L q W d G 9 K D p</p>
<p>B.4 More Details About the Human Experiment</p>
<p>Each question in each condition (multimodal, only, or video-only) was answered by 5 participants.180 participants were recruited via Prolific (mean age = 29.7;65 female).They were paid $12 per hour.The study was approved by an institutional review board.All data collected through Prolific have been anonymized.The instructions and consent form are shown below.</p>
<p>This is a study about how people interpret other people's actions.We will ask you to answer questions based on typical household activities.The study usually takes 20 minutes.</p>
<p>Consent By selecting the "I Agree" button below, you acknowledge that:</p>
<p>You must be at least 18 years old to participate.Your participation in this research is voluntary.</p>
<p>You may decline to answer any or all of the following questions by closing this window in your browser.You may decline further participation, at any time, without adverse consequences.</p>
<p>Your anonymity is assured; the researchers who have requested your participation will not receive any personal information about you.</p>
<p>B.5 Available Data</p>
<p>As shown in Figure 7, our benchmark provides RGB-D images, instance segmentation maps, human 3D poses, ground-truth scene graphs, groundtruth actions, and camera data.</p>
<p>B.6 Benchmark Statistics</p>
<p>The environment in each question features an apartment that includes a bedroom, kitchen, living room, and bathroom.On average, each apartment contains 10.1 distinct types of containers (e.g., fridge, cabinets) and surfaces (e.g., desks, kitchen tables, coffee tables) types, and 16.4 instances of containers and surfaces.There are on average 12 different object types, summing up to approximately 26.3 object instances in an apartment.</p>
<p>Figure 8 provides an overview of the distribution of text and video lengths across all questions in our benchmark.In comparison to existing ToM benchmarks, MMToM-QA features a more extensive text and video context, with an average of 1595 tokens and 1902 frames, respectively.Such lengths demand advanced information retrieval and fusion capabilities.The longer visual and textural context also increases the difficulty of paying attention to the relevant information to reconstruct the mental  state of a person.</p>
<p>B.7 Details of the Procedural Generation</p>
<p>Figure 9 provides an overview of the procedural generation of questions in our benchmark.To procedurally generate videos, we first sample different apartments, the goal, and the initial state of an agent, then generate a sequence of actions using a planner.In particular, we formulate the agent in POMDP.The belief of the agent is represented as the probability of finding each object at a location.The agent can observe any within the same room that are not inside closed containers.We adopt the same planner in Puig et al. ( 2020), which has been verified to be able to synthesize human-like plans in household environments.Given the action sequence, we then render the RGB-D video frames and record ground-truth data including the instance segmentation maps, 3D human poses, scene graphs, actions, ground-truth agent beliefs, and the true goal.</p>
<p>To generate questions, we first use an "ideal observer" model to keep track of all possible goal and belief hypotheses at any given step.This model has access to the ground-truth observation of the agent at each step and eliminates the belief hypotheses that are inconsistent with the observations.It also has access to the planner of the agent and will evaluate each remaining goal and belief hypothesis pair by simulating actions conditioned on the hypothesis.If the simulated actions are consistent with the actual actions taken by the agent, we then further eliminate that hypothesis.Finally, to generate a question of a certain type at a given step, we sample two hypotheses that fit the design of the question type, with one from the set of possible hypotheses produced by the ideal observer model and the other being a hypothesis ruled out by the model.</p>
<p>At any given moment in a video, we ensure that our benchmark poses at most one question.To achieve this, we randomly select one question from the pool of possible questions at every step.We further sample a subset of the remaining questions to achieve a balanced distribution of question types.</p>
<p>Apartment &amp; initial state</p>
<p>Goal: get a plate</p>
<p>VirtualHome-Social Simulator</p>
<p>Planner</p>
<p>Observation Action</p>
<p>…</p>
<p>State:</p>
<p>In(agent, bedroom) In(plate, cabinet) Closed(cabinet)</p>
<p>…</p>
<p>Action:</p>
<p>Walk towards kitchen</p>
<p>Video frames</p>
<p>Ground-truth state and action</p>
<p>State:</p>
<p>In(agent, kitchen) In(plate, cabinet) Closed(cabinet)</p>
<p>…</p>
<p>Action:</p>
<p>Walk towards kitchen cabinet</p>
<p>Hypotheses after t = 1</p>
<p>Goal: water glass, Belief: [kitchen Step 1: Video Synthesis</p>
<p>Step 2: Question Sampling</p>
<p>Step 3: Text Generation</p>
<p>B.8 Utilizing GPT-4 for Enhanced Text Generation</p>
<p>We first translate the symbolic representations of state and action descriptions into natural language using simple templates.We then use GPT-4 to enhance the phrasing and diversify the expression.The prompts are provided as follows.</p>
<p>Improving state descriptions:</p>
<p>We are describing where things are in an apartment.Please improve the grammer of the description without changing the meaning.Please use only one line break to separate descriptions about each room.</p>
<p>Original: There is a kitchen and a livingroom.4 kitchencabinets are inside the kitchen.There is nothing inside the 1st kitchencabinet from left to right.There is nothing inside the 3rd kitchencabinet from left to right.There is nothing inside the 2nd kitchencabinet from left to right.A waterglass and a wineglass and 2 dishbowls are inside the 4th kitchencabinet from left to right.Improved: There is a kitchen and a living room.The kitchen has four cabinets.The first, second, and third cabinets, from left to right, are empty.The fourth cabinet, from the left, contains a water glass, a wine glass and two dish bowls.</p>
<p>Original: {state description in the question} Improved:</p>
<p>Improving action descriptions: Please improve the following descriptions about a person's actions without changing the meaning.</p>
<p>Original: [name] is in the kitchen, walktowards stove.</p>
<p>Improved: [name] is in the kitchen.</p>
<p>[name] walks towards the stove.</p>
<p>Original: [name] is in the livingroom, walktowards kitchen, walktowards 1st kitchencabinet, open 1st kitchencabinet, close 1st kitchencabinet.</p>
<p>Improved: [name] is in the living room.</p>
<p>[name] walks to the kitchen, approaches the first cabinet, opens it, and then closes it.Original: {action description in the ques-tion} Improved:</p>
<p>C BIP-ALM Implementation Details</p>
<p>C.1 Visual Perception</p>
<p>We adopt Blukis et al. (2022) to obtain a voxel map for each frame.Specifically, we first get the instance segmentation map from the RGB image.</p>
<p>Combined with the depth map and the camera data, we create a 3D point cloud, with each point representing the pixel on the instance segmentation map.We group the 3D point cloud into a voxel map and then estimate the 3D bounding boxes of the objects.We can also estimate the 3D human pose in each frame.In the current experiments, we use ground-truth instance segmentation maps and ground-truth 3D human poses.In future work, we plan to also evaluate our model on segmentation and pose estimation results acquired from off-theshelf computer vision models.</p>
<p>Using the 3D bounding boxes and the 3D human pose, we construct a scene graph.Each node in the graph is either an object or a person.For nodes representing containers, we indicate whether they are open or closed (which in our scenarios can be detected from the change in the sizes of their bounding boxes).There are two types of edges in a graph -(1) inside edges indicating containment and (2) close edges indicating proximity.Note that our BIP-ALM model is not restricted to these relationships and can be applied to broader scenarios with more types of spatial relationships.</p>
<p>C.2 Text Parsing</p>
<p>Utilizing GPT-4, we parse the provided question to extract representations about the state context, action context, question, and the two options.In this subsection, we detail all the prompts used.</p>
<p>To extract representations regarding the state context, we use the following prompt: Original: The living room contains a sofa, a desk, a cabinet, and a coffee table, and the cabinet holds chips, a wine glass, and an apple.Parsed: A sofa, a desk, a cabinet and a coffeetable are in the livingroom.Chips, a wineglass and an apple are in the cabinet.</p>
<p>Original: The kitchen has an oven, a microwave, and four cabinets.The oven contains a salmon, the microwave holds a cupcake, the third cabinet from the left has a wine glass, the fourth cabinet is empty.The first and second kitchen cabinets each holds a plate.Parsed: an oven and a microwave and 4 kitchencabinets are in the kitchen.A salmon is in the oven.A cupcake is in the microwave.A wineglass is in the 3rd kitchencabinet.Nothing is in the 4th kitchencabinet.A plate is in the 1st kitchencabinet.A plate is in the 2nd kitchencabinet.</p>
<p>Original: {extracted states} Parsed:</p>
<p>To parse the human actions, we employ the following prompt:</p>
<p>Action Extraction: Please extract the exact description of a person's actions (starting from the initial location), found after the phrase "[someone]'s action" and before the question.Please do not include the question, choices, or the answer.Input: {question} Extracted:</p>
<p>Action Parsing: Please parse the description of a person's actions.Use a '.' to separate each action, and remove all occurrences of the word 'and' in the description.To parse and analyze the question, we prompt GPT-4 to determine if a question falls under the "Belief Inference" or "Goal Inference" category, and extract all the hypothetical beliefs, hypothetical goals, and conditions in the question:</p>
<p>Question Parsing: Please determine the type of inference for the input question: either "Belief Inference", which inquires about a person's belief regarding an object, or "Goal Inference", which seeks to understand a person's objective.If a question falls under the "Belief Inference", please identify the [object] and the [container] that the object may or may not be inside in choices (a) and (b).If a question falls under the "Goal Inference", please identify the two possible objects that the person is looking for in choices (a) and (b).If the input contains a statement indicating that someone believes there isn't an [object] inside a [container], please also identify both the [object] and the [container] mentioned.Otherwise, return 'NaN.' Input: ... (detailed descriptions about states and actions) ... If Elizabeth has been trying to get a plate, which one of the following statements is more likely to be true?(a) Elizabeth thinks that there is a plate inside the fridge.(b) Elizabeth thinks that there isn't any plate inside the fridge.Output: Belief Inference.plate, fridge.Input: ... (detailed descriptions about states and actions) ... If Jennifer has been trying to get a plate, which one of the following statements is more likely to be true?(a) Jennifer thinks that there is a salmon inside the oven.(b) Jennifer thinks that there isn't any salmon inside the oven.Output: Belief Inference.plate, fridge.salmon, oven.Input: ... (detailed descriptions about states and actions) ... Which one of the following statements is more likely to be true?(a) Mark has been trying to get a plate.(b) Mark has been trying to get a cupcake.Output: Goal Inference.plate, cupcake.NaN.</p>
<p>Input: ... (detailed descriptions about states and actions) ... If Mary think there isn't an apple inside the microwave, which one of the following statements is more likely to be true?(a) Mary has been trying to get an apple.(b) Mary has been trying to get a bottle of wine.Output: Goal Inference.apple, wine.apple, microwave.</p>
<p>Input: {question} Output:</p>
<p>C.3 Representation Fusion</p>
<p>Figure 10 illustrates how BIP-ALM fuse complementary information extracted from the video and the text to form unified symbolic representations.In particular, the orange predicates can only be extracted from the video and the blue predicates can only be extracted from the text.After merging predicates extracted from both modalities, we can then construct a full state sequence.</p>
<p>When fusing information from different inputs, we may encounter conflicting predicates.In the case of conflict, we only keep the predicates from the text and remove the contradictory predicates from the video.This is because the predicates from the video are more likely to be erroneous due to noisy visual perception.However, more broadly speaking, such conflict-resolving mechanisms can be calibrated by the reliability of different modalities in a given domain.&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / O w H D R 4 v D 2 j f p g r x 6 z u o I H h P m G U = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 e K 9 g P a W D b b S b t 0 s w m 7 G 6 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o 6 j h V D B s s F r F q B 1 S j 4 B I b h h u B 7 U Q h j Q K B r W B 0 M / V b T 6 g 0 j + W D G S f o R 3 Q g e c g Z N V a 6 1 4 9 u r 1 x x q + 4 M Z J l 4 O a l A j n q v / N X t x y y N</p>
<p>C.4 Belief Representation and Update
T F O c k x q v Z e A = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 I t Q 9 O K x o v 2 A N p T N d t M u 3 W z C 7 k Q o o T / B i w d F v P q L v P l v 3 L Y 5 a O u D g c d 7 M 8 z M C x I p D L r u t 1 N Y W V 1 b 3 y h u l r a 2 d 3 b 3 y v s H T R O n m v E G i 2 W s 2 w E 1 X A r F G y h Q 8 n a i O Y 0 C y V v B 6 H b q t 5 6 4 N i J W j z h O u B / R g R K h Y B S t 9 I D X b q 9 c c a v u D G S Z e D m p Q I 5 6 r / z V 7 c c s j b h C J q k x H c 9 N 0 M + o R s E k n 5 S 6 q e E J Z S M 6 4 B 1 L F Y 2 4 8 b P Z q R N y Y p U + C W N t S y G Z q b 8 n M h o Z M 4 4 C 2 x l R H J p F b y r + 5 3 V S D K / 8 T K g k R a 7 Y f F G Y S o I x m f 5 N + k J z h n J s C W V a 2 F s J G 1 J N G d p 0 S j Y E b / H l Z d I 8 q 3 o X V f f + v F K 7 y e M o w h E c w y l 4 c A k 1 u I M 6 N I D B A J 7 h F d 4 c 6 b w 4 7 8 7 H v L X g 5 D O H 8 A f O 5 w / S A Y 1 + &lt; / l a t e x i t &gt; t = 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 E E X d V U 9 5 S d v 4 K X s x J L O 1 T u m u 8 E = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 I t Q 9 O K x o v 2 A N p T N d t M u 3 W z C 7 k Q o o T / B i w d F v P q L v P l v 3 L Y 5 a O u D g c d 7 M 8 z M C x I p D L r u t 1 N Y W V 1 b 3 y h u l r a 2 d 3 b 3 y v s H T R O n m v E G i 2 W s 2 w E 1 X A r F G y h Q 8 n a i O Y 0 C y V v B 6 H b q t 5 6 4 N i J W j z h O u B / R g R K h Y B S t 9 I D X X q 9 c c a v u D G S Z e D m p Q I 5 6 r / z V 7 c c s j b h C J q k x H c 9 N 0 M + o R s E k n 5 S 6 q e E J Z S M 6 4 B 1 L F Y 2 4 8 b P Z q R N y Y p U + C W N t S y G Z q b 8 n M h o Z M 4 4 C 2 x l R H J p F b y r + 5 3 V S D K / 8 T K g k R a 7 Y f F G Y S o I x m f 5 N + k J z h n J s C W V a 2 F s J G 1 J N G d p 0 S j Y E b / H l Z d I 8 q 3 o X V f f + v F KU B o m q N Y d z 0 2 M n 1 F l O B M 4 K X V T j Q l l I z r A j q W S R q j 9 b H b q h J x Y p U / C W N m S h s z U 3 x M Z j b Q e R 4 H t j K g Z 6 k V v K v 7 n d V I T X v k Z l 0 l q U L L 5 o j A V x M R k + j f p c 4 X M i L E l l C l u b y V s S B V l x q Z T s i F 4 i y 8 v k + Z Z 1 b u o u n f n l d p 1 H k c R j u A Y T s G D S 6 j B L d S h A Q w G 8 A y v 8 O Y I 5 8 V 5 d z 7 m r Q U n n z m E P 3 A + f w A C
r 4 2 e &lt; / l a t e x i t &gt; s 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / 9 C V c M A 9 C l G r s I 7 s 0 U H M z w P 0 h B k = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 e K 9 g P a W D b b T b t 0 s w m 7 E 6 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f T m F l d W 1 9 o 7 h Z 2 t r e 2 d 0 r 7 x 8 0 T Z x q x h s s l r F
u B 9 R w K R R v o E D J 2 4 n m N A o k b w W j m 6 n f e u L a i F g 9 4 D j h f k Q H S o S C U b T S v X n 0 e u W K W 3 V n I M v E y 0 k F c t R 7 5 a 9 u P 2 Z p x B U y S Y 3 p e G 6 C f k Y 1 C i b 5 p N R N D U 8 o G 9 E B 7 1 i q a M S N n 8 1 O n Z A T q / R J G G t b C s l M / T 2 R 0 c i Y c R T Y z o j i 0 C x 6 U / E / r 5 N i e O V n Q i U p c s X m i 8 J U E o z J 9 G / S F 5 o z l G N L K N P C 3 k r Y k G r K 0 K Z T s i F 4 i y 8 v k + Z Z 1 b u o u n f n l d p 1 H k c R j u A Y T s G D S 6 j B L d S h A Q w G 8 A y v 8 O Z I 5 8 V 5 d z 7 m r Q U n n z m E P 3 A + f w A E M 4 2 f &lt; / l a t e x i t &gt; s 1 Fused Symbolic Representations</p>
<p>… Observation at</p>
<p>Belief about plate at &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P r 5 a P g X p c w 4 N a U 2 P S o 9 h h y m m H
U g = " &gt; A A A B 6 H i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 J 0 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V H J o 8 l r H u B M y A F A q a K F B C J 9 H A o k B C O x j f z f z 2 E 2 g j Y v W A k w T 8 i A 2 V C A V n a K U G 9 s s V t + r O Q V e J l 5 M K y V H v l 7 9 6 g 5 i n E S j k k h n T 9 d w E / Y x p F F z C t N R L D S S M j 9 k Q u p Y q F o H x s / m h U 3 p m l Q E N Y 2 1 L I Z 2 r v y c y F h k z i Q L b G T E c m W V v J v 7 n d V M M b / x M q C R F U H y x K E w l x Z j O v q Y D o Y G j n F j C u B b 2 V s p H T D O O N p u S D c F b f n m V t C 6 q 3 l X V b V x W a r d 5 H E V y Q k 7 J O f H I N a m R e 1 I n T c I J k G f y S t 6 c R + f F e X c
+ F q 0 F J 5 8 5 J n / g f P 4 A 4 c + M / Q = = &lt; / l a t e x i t &gt; t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P r 5 a P g X p c w 4 N a U 2 P S o 9 h h y m m H
U g = " &gt; A A A B 6 H i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 J 0 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V H J o 8 l r H u B M y A F A q a K F B C J 9 H A o k B C O x j f z f z 2 E 2 g j Y v W A k w T 8 i A 2 V C A V n a K U G 9 s s V t + r O Q V e J l 5 M K y V H v l 7 9 6 g 5 i n E S j k k h n T 9 d w E / Y x p F F z C t N R L D S S M j 9 k Q u p Y q F o H x s / m h U 3 p m l Q E N Y 2 1 L I Z 2 r v y c y F h k z i Q L b G T E c m W V v J v 7 n d V M M b / x M q C R F U H y x K E w l x Z j O v q Y D o Y G j n F j C u B b 2 V s p H T D O O N p u S D c F b f n m V t C 6 q 3 l X V b V x W a r d 5 H E V y Q k 7 J O f H I N a m R e 1 I n T c I J k G f y S t 6 c R + f F e X c + F q 0 F J 5 8 5 J n / g f P 4 A 4 c + M / Q = = &lt; / l a t e x i t &gt; t Belief about plate at &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I n v l n q S I b h + 4 S i P W t v o D V g k 5 y T Y = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i o h 6 L X j x W t B / Q h r L Z b t q l m 0 3 Y n Q g l 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K F Q d f 9 d g o r q 2 v r G 8 X N 0 t b 2 z u 5 e e f + g a e J U M 9 5 g s Y x 1 O 6 C G S 6 F 4 A w V K 3 k 4 0 p 1 E g e S s Y 3 U 7 9 1 h P X R s T q E c c J 9 y M 6 U C I U j K K V H v D M 6 5 U r b t W d g S w T L y c V y F H v l b + 6 / Z i l E V f I J D W m 4 7 k J + h n V K J j k k 1 I 3 N T y h b E Q H v G O p o h E 3 f j Y 7 d U J O r N I n Y a x t K S Q z 9 f d E R i N j x l F g O y O K Q 7 P o T c X / v E 6 K 4 b W f C Z W k y B W b L w p T S T A m 0 7 9 J X 2 j O U I 4 t o U w L e y t h Q 6 o p Q 5 t O y Y b g L b 6 8 T J r n V e + y 6 t 5 f V G o 3 e R x F O I J j O A U P r q A G d 1 C H B j A Y w D O 8 w p s j n R f n 3 f m Y t x a c f O Y Q / s D 5 / A G 4 K 4 1 t &lt; / l a t e x i t &gt; t + 1
Belief about apple at &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P r 5 a P g X p c w 4 N a U 2 P S o 9 h h y m m H
U g = " &gt; A A A B 6 H i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 J 0 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V H J o 8 l r H u B M y A F A q a K F B C J 9 H A o k B C O x j f z f z 2 E 2 g j Y v W A k w T 8 i A 2 V C A V n a K U G 9 s s V t + r O Q V e J l 5 M K y V H v l 7 9 6 g 5 i n E S j k k h n T 9 d w E / Y x p F F z C t N R L D S S M j 9 k Q u p Y q F o H x s / m h U 3 p m l Q E N Y 2 1 L I Z 2 r v y c y F h k z i Q L b G T E c m W V v J v 7 n d V M M b / x M q C R F U H y x K E w l x Z j O v q Y D o Y G j n F j C u B b 2 V s p H T D O O N p u S D c F b f n m V t C 6 q 3 l X V b V x W a r d 5 H E V y Q k 7 J O f H I N a m R e 1 I n T c I J k G f y S t 6 c R + f F e X c + F q 0 F J 5 8 5 J n / g f P 4 A 4 c + M / Q = = &lt; / l a t e x i t &gt; t K i t c h e n</p>
<p>…</p>
<p>Belief about apple at &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I n v l n q S I b h + 4 S i P bolic representation of the belief about each object as a list of possible locations of the object that have non-zero probabilities in the belief.Note that for representing the negation of a predicate, we can simply remove the location from the list of possible locations in the symbolic belief representation.
W t v o D V g k 5 y T Y = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i o h 6 L X j x W t B / Q h r L Z b t q l m 0 3 Y n Q g l 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K F Q d f 9 d g o r q 2 v r G 8 X N 0 t b 2 z u 5 e e f + g a e J U M 9 5 g s Y x 1 O 6 C G S 6 F 4 A w V K 3 k 4 0 p 1 E g e S s Y 3 U 7 9 1 h P X R s T q E c c J 9 y M 6 U C I U j K K V H v D M 6 5 U r b t W d g S w T L y c V y F H v l b + 6 / Z i l E V f I J D W m 4 7 k J + h n V K J j k k 1 I 3 N T y h b E Q H v G O p o h E 3 f j Y 7 d U J O r N I n Y a x t K S Q z 9 f d E R i N j x l F g O y O K Q 7 P o T c X / v E 6 K 4 b W f C Z W k y B W b L w p T S T A m 0 7 9 J X 2 j O U I 4 t o U w L e y t h Q 6 o p Q 5 t O y Y b g L b 6 8 T J r n V e + y 6 t 5 f V G o 3 e R x F O I J j O A U P r q A G d 1 C H B j A Y w D O 8 w p s j n R f n 3 f m Y t x a c f O Y Q / s D 5 / A G 4 K 4 1 t &lt; / l a t e x i t &gt; t + 1 K i t c h</p>
<p>C.5 Prompt for Language Models in Inverse Symbolic Planner</p>
<p>As introduced in the main paper, we employ a language model (either GPT-J or LLaMA 2) to amortize the policy and estimate the likelihood of the person's last action a t given the hypothetical belief b t and goal g.We present the language model with the specified prompt below and then record its likelihood of predicting the accurate action.</p>
<p>goal: {hypothetical goal} state: {state} belief (possible locations the person suspects the {hypothetical goal} could be): {hypothetical or predicted belief} action:</p>
<p>For the belief inference questions, to represent a hypothetical belief that the goal object is not at a location L, we exclude that hypothetical location from the list of all possible locations in the prompt for the language model and subsequently estimate the action likelihood (π(a t |g, b t )) conditioned on not believing that the object could be at that location.To compare that with the opposing belief, i.e., that the object could be at the location L, we assess the action likelihood conditioned on not believing that the goal object is at an alternative location L ′ by removing L ′ from the possible location list instead.If the removal of the hypothetical location (L) results in the lowest action likelihood compared to all alternatives (L ′ ), then it is more likely that the person does believe that the goal object could be at this location (L).</p>
<p>C.6 Training Details</p>
<p>In the experiments, we finetuned GPT-J (6B) and LLaMA 2 (7B) for our BIP-ALM method.Adher-ing to our evaluation protocol, we employed the ground-truth state, belief, goal, and action from 1,000 training videos, excluding the use of any example QAs.The models were trained using the state, belief, and goal as inputs at specific timestamps, with the aim of predicting the corresponding action.This approach enhanced the Inverse Symbolic Planner's ability to estimate the likelihood of human actions at a specific moment conditioned on the hypothesis about the goal and belief.The training data comprised 20,000 samples, consistent with the input and output of the Inverse Symbolic Planner defined in Appendix C.5.</p>
<p>For finetuning, we incorporated the Low-Rank Adapters (LoRA) method (Hu et al., 2021).The training process leveraged the AdamW optimizer, with a set learning of 5 × 10 −5 and a batch size of 4. We trained both models for 5 epochs, which took about 20 GPU hours on a single A100 GPU.</p>
<p>D Implementation Details of Baselines</p>
<p>In all baselines, we use gpt-4-0613 for GPT-4 and text-davinci-003 for GPT-3.5.</p>
<p>We sample a few frames from each video for LMMs, following the standard settings.In particular, we sample 16 frames, 30 frames, 6 frames, and 8 frames from each video for InstructBLIP, Video-LLaMA 2, LLaVA, and GPT-4V respectively.We use Vicuña-13B for InstructBLIP and LLaMA-2-13B-Chat for Video-LLaMA 2, Finetuning Video-LLaMA 2. We followed Zhang et al. (2023) to create a video instruction dataset using our training data to finetune Video-LLaMA 2. In particular, based on our training videos and the ground-truth annotations, we generated 7,088 training examples for describing either the environment or the actions of the person in a given video clip.For each video clip, we sampled 8 frames and used the ground-truth scene graph at the last frame or the action sequence during the whole clip to generate the ground-truth descriptions for the training data.We then finetuned the Video-LLaMA 2 (13B) model on our training data on 2 A100 GPUs.Lastly, the bathroom has a cabinet, which is currently empty.</p>
<p>E Full Version of the</p>
<p>Actions taken by Elizabeth:</p>
<p>Elizabeth is initially in the bathroom.She then proceeds to the kitchen and heads towards the oven.After opening and closing the oven, she moves to the second kitchen cabinet, opens it, and then shuts it.She repeats this action with the third and first kitchen cabinets.Subsequently, she walks towards the fourth kitchen cabinet, opens it, and then closes it.Finally, she moves towards the fridge, preparing to open it.</p>
<p>Question:</p>
<p>If Elizabeth has been trying to get a bottle of wine, which one of the following statements is more likely to be true?(a) Elizabeth thinks that there is a bottle of wine inside the fridge.(b) Elizabeth thinks that there isn't any bottle of wine inside the fridge.</p>
<p>Correct answer: a.</p>
<p>E.2 Type 1.2 Example</p>
<p>The video input: https://youtu.be/mgvh1lY_Z38.</p>
<p>The text input:</p>
<p>What's inside the apartment:</p>
<p>The apartment consists of a bedroom, kitchen, living room, and bathroom.In the bedroom, there is a coffee table and a desk, with three wine glasses and a dish bowl placed on the coffee table.</p>
<p>The kitchen is equipped with four cabinets, a fridge, a kitchen table, a microwave, and an oven.The first, second, and fourth cabinets, from left to right, contain a dish bowl each, while the third cabinet houses a plate.The fridge contains two apples, a dish bowl, and a salmon.The microwave holds two cupcakes, and there is a salmon in the oven.</p>
<p>The living room features a cabinet, a sofa, a coffee table, and a desk.The cabinet is filled with a plate, a bag of chips, a water glass, a remote control, a bottle of wine, and a condiment bottle.Lastly, the bathroom has a cabinet, which is currently empty.</p>
<p>Actions taken by Jennifer:</p>
<p>Jennifer is situated in the living room.She heads towards the cabinet and is about to open it.</p>
<p>Question:</p>
<p>If Jennifer has been trying to get a cupcake, which one of the following statements is more likely to be true?(a) Jennifer thinks that there isn't any cupcake inside the cabinet.(b) Jennifer thinks that there is a cupcake inside the cabinet.</p>
<p>Correct answer: b.</p>
<p>E.3 Type 1.3 Example</p>
<p>The video input: https://youtu.be/lN810N3KdjM.</p>
<p>The text input:</p>
<p>What's inside the apartment:</p>
<p>The apartment consists of a bedroom, kitchen, living room, and bathroom.In the bedroom, there is a sofa with a book on it and a cabinet containing a remote control, a wine glass, two dish bowls, a bottle of wine, and a condiment bottle.</p>
<p>The kitchen is equipped with a fridge, sofa, dishwasher, eight cabinets, an oven, a microwave, and a kitchen table.The fridge contains an apple, three plates, and a bottle of wine, while a bag of chips rests on the sofa.Inside the dishwasher, there is a plate, a water glass, and a wine glass.The first to the seventh cabinets, from left to right, are all empty.However, the eighth cabinet houses a wine glass.The oven contains a salmon, the microwave is empty, and the kitchen table is adorned with a plate, a wine glass, two apples, two books, and a cupcake.</p>
<p>The living room features a sofa with a water glass and a book on it, and a desk.Lastly, the bathroom has a cabinet, which is currently empty.</p>
<p>Actions taken by Charles:</p>
<p>Charles is in the kitchen.He walks to the seventh kitchen cabinet, opens and closes it.He repeats the same action with the sixth kitchen cabinet.Subsequently, he moves towards the dishwasher.</p>
<p>Question:</p>
<p>If Charles has been trying to get a salmon, which one of the following statements is more likely to be true?(a) Charles thinks that there is a salmon inside the fridge.(b) Charles thinks that there isn't any salmon inside the fridge.</p>
<p>Correct answer: b.</p>
<p>E.4 Type 2.1 Example</p>
<p>The video input: https://youtu.be/NsOPbJWPn1c.</p>
<p>The text input:</p>
<p>What's inside the apartment:</p>
<p>The apartment consists of a bedroom, a bathroom, a living room, and a kitchen.In the bedroom, there is a coffee table with a plate on it.The bathroom houses a cabinet, which is currently empty.The living room is furnished with a cabinet, a coffee table, a sofa, and a desk.The cabinet is filled with two apples, a condiment bottle, three wine glasses, two water glasses, a cupcake, two bags of chips, a remote control, and a bottle of wine.Both a water glass and a wine glass are placed on the coffee table.The kitchen is equipped with a fridge, an oven, a kitchen table, and a microwave.Inside the fridge, there are two apples.The oven contains a salmon.Meanwhile, the microwave houses a and two cupcakes.</p>
<p>Actions taken by James:</p>
<p>James is in the kitchen.He strides towards the stove, opens it, and then shuts it.He then opens the fridge, closes it, opens the microwave, and closes it as well.Finally, he walks towards the living room and approaches the cabinet.</p>
<p>Question:</p>
<p>Which one of the following statements is more likely to be true?(a) James has been trying to get a bottle of wine.(b) James has been trying to get an apple.</p>
<p>Correct answer: a.</p>
<p>E.5 Type 2.2 Example</p>
<p>The video input: https://youtu.be/Fn6s47ZtxMQ.</p>
<p>The text input:</p>
<p>What's inside the apartment:</p>
<p>The apartment consists of a bedroom, bathroom, living room, and kitchen.In the bedroom, there is a sofa, a cabinet, a desk, and a coffee table.A book rests on the sofa.The cabinet contains a remote control, three cupcakes, a wine glass, an apple, and a bag of chips.The coffee table holds two books and a dish bowl.</p>
<p>The bathroom houses a single cabinet, which is currently empty.The living room is furnished with a sofa, a desk, and a coffee table .A dish bowl and a book are placed on the sofa, while a plate sits on the coffee table.</p>
<p>The kitchen is equipped with a dishwasher, an oven, a kitchen table, eight cabinets, a microwave, and a fridge.Inside the dishwasher, there is a dish bowl.The oven contains a salmon and a plate.The second kitchen cabinet from the left holds an apple, while the fourth and fifth cabinets contain two dish bowls and another apple respectively.There is a water glass inside the seventh cabinet.The first, third, sixth, and eighth cabinets are empty.The microwave contains a condiment bottle.</p>
<p>The fridge stores two cupcakes, a dish bowl, a plate, and a bottle of wine.</p>
<p>Actions taken by Mark:</p>
<p>Mark is in the kitchen.He then advances towards the seventh kitchen cabinet.</p>
<p>Question:</p>
<p>If Mark thinks there isn't a water glass inside the 7th kitchen cabinet, which one of the following statements is more likely to be true?(a) Mark has been trying to get a water glass.(b) Mark has been trying to get a cupcake.</p>
<p>Correct answer: b.</p>
<p>E.6 Type 2.3 Example</p>
<p>The video input: https://youtu.be/IUJW6Zv0EWA.</p>
<p>The text input:</p>
<p>What's inside the apartment:</p>
<p>The apartment consists of a bedroom, bathroom, living room, and kitchen.In the bedroom, there is a cabinet and a sofa.The cabinet contains a condiment bottle, an apple, two wine glasses, and a plate.The sofa holds three books.</p>
<p>The bathroom features a cabinet, which is currently empty.The living room is furnished with a desk and a sofa, with a book resting on the sofa.The kitchen is equipped with eight cabinets, a sofa, an oven, a fridge, a kitchen table, a microwave, and a dishwasher.The first kitchen cabinet, from left to right, contains a bag of chips.The second and fourth cabinets are empty.The third cabinet houses a wine glass and a dish bowl.The seventh cabinet stores two plates.The fifth, sixth, and eighth cabinets are empty.The oven contains a cupcake.The fridge holds a plate and a dish bowl.The kitchen table is adorned with an apple, a bottle of wine, a plate, and a water glass.The microwave contains a condiment bottle and a salmon.Lastly, the dishwasher has a water glass inside.</p>
<p>Actions taken by Mary:</p>
<p>Mary is situated in the living room.She proceeds towards the kitchen and heads to the second kitchen cabinet.She opens it, then promptly closes it.She then opens the fourth kitchen cabinet and closes it as well.Following this, she opens the dishwasher and closes it.She then moves towards the sixth kitchen cabinet, opens it, and closes it.She repeats this action with the seventh kitchen cabinet.Finally, she walks towards the first kitchen cabinet, opens it, and then closes it.</p>
<p>Question:</p>
<p>Which one of the following statements is more likely to be true?(a) Mary has been trying to get a bag of chips.(b) Mary has been trying to get a condiment bottle.</p>
<p>Correct answer: b.</p>
<p>E.7 Type 2.4 Example</p>
<p>The video input: https://youtu.be/Y4H_9cXR5mw.</p>
<p>The text input:</p>
<p>What's inside the apartment:</p>
<p>The apartment consists of a bedroom, bathroom, living room, and kitchen.In the bedroom, there is a sofa, a cabinet, a desk, and a coffee table.A book rests on the sofa.The cabinet houses an apple, a wine glass, two books, and two cupcakes.</p>
<p>The coffee table holds a book, a water glass, a wine glass, and a remote control.</p>
<p>The bathroom contains a single cabinet, which is currently empty.The living room is furnished with a sofa, a coffee table, and a desk.A water glass sits on the sofa, and a remote control is on the coffee table.The kitchen is equipped with eight cabinets, a microwave, a fridge, a dishwasher, a kitchen table, and an oven.The fourth and seventh cabinets from the left, as well as the eighth, are empty.The microwave contains a salmon, a cupcake, and a condiment bottle.The fridge is stocked with two bottles of wine, a cupcake, an apple, and two dish bowls.The dishwasher holds a dish bowl, a wine glass, and a plate.The second cabinet from the left contains a water glass.The first cabinet from the left holds a bag of chips and a wine glass.The fifth cabinet has an apple, and the third cabinet contains a condiment bottle.The sixth cabinet is empty.Lastly, there is a salmon in the oven.</p>
<p>Actions taken by William:</p>
<p>William is situated in the kitchen.He advances towards the first kitchen cabinet, opens it, and then shuts it.Finally, he moves towards the fifth kitchen cabinet.</p>
<p>Question:</p>
<p>Which one of the following statements is more likely to be true?(a) William has been trying to get a wine glass.(b) William has been trying to get a dish bowl.</p>
<p>Correct answer: b.</p>
<p>Figure 1 :
1
Figure1: Sketch of the MMToM-QA benchmark.Each question is associated with a video stream (representative frames highlighting key moments are shown above for illustration) and text input (illustrative text above is shortened for brevity).In the example video, Emily can see the wine glass on one of the kitchen tables (1st frame) and passes by it without picking it up (2nd frame).At the end of the clip (3rd frame), it appears that she could be walking towards the cabinets on the left side of the room; or she might want to check if a goal object is inside the microwave.The text indicates that there are no cupcakes in the cabinets, but there is a cupcake inside the microwave.To confidently choose the correct answer, a model must fuse relevant information from both the video and the text.</p>
<p>(a) James has been trying to get a bottle of wine.(b) James has been trying to get an apple.</p>
<p>Figure 2 :
2
Figure 2: Question types in MMToM-QA, with examples.Questions fall into two broad categories, Belief and Goal, with several different question types in each category that span a range of mental reasoning.Each example shows only a few frames and snippets.The options in the green, italic font are correct answers.Note that we simplify the text in the examples for brevity.We provide the full text and the video links in Appendix E.</p>
<p>t e x i t s h a 1 _ b a s e 6 4 = "</p>
<p>z s e s d c n J Z w 7 g D 5 z P H y I 8 j + o = &lt; / l a t e x i t &gt; g 1 , b t 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x J x t 2 k E l W C g l a k W O E 4 1 + 1 e q + 9 I w = " &gt; AA A B 8 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g Q c J u E P U Y 9 O I x g n l g s o b Z y S Q Z M j u 7 z P Q K Y c l f e P G g i F f / x p t / 4 y T Z g y Y W N B R V 3 X R 3 B b E U B l 3 3 2 8 m t r K 6 t b + Q 3 C 1 v b O 7 t 7 x f 2 D h o k S z X i d R T L S r Y A a L o X i d R Q o e S vW n I a B 5 M 1 g d D P 1 m 0 9 c G x G p e x z H 3 A / p Q I m + Y B S t 9 D D o V s 5 I 0 K 0 8 Y r d Y c s v u D G S Z e B k p Q Y Z a t / j V 6 U U s C b l C J q k x b c + N 0 U + p R s E k n x Q 6 i e E x Z S M 6 4 G 1 L F Q 2 5 8 d P Z x R N y Y p U e 6 U f a l k I y U 3 9 P p D Q 0 Z h w G t j O k O D S L 3 l T 8 z 2 s n 2 L / y U 6 H i B L l i 8 0 X 9 R B K M y P R 9 0 h O a M 5 R j S y j T w t 5 K 2 J B q y t C G V L A h e I s v L 5 N G p e x d l N 2 7 8 1 L 1 O o s j D 0 d w D K f g w S V U 4 R Z q U A c G C p 7 h F d 4 c 4 7 w 4 7 8 7 H v D X n Z D O H 8 A f O 5 w 8 l T Y / s &lt; / l a t e x i t &gt;</p>
<p>8 A y v 8 O Y I 5 8 V 5 d z 7 m r Q U n n z m E P 3 A + f w D n N I 2 M &lt; / l a t e x i t &gt; a 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d t 5 m H H HI i z V t w w h b 0 u W Q D g C A e v A = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E V D w V v X i s Y D + g j W W z 3 b R L N 5 u w O x F K 6 I / w 4 k E Rr / 4 e b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k Q K g 6 7 7 7 R R W V t f W N 4 q b p a 3 t n d 2 9 8 v 5 B 0 8 S p Z r z B Y h n r d k A N l 0 L x B g q U v J 1 o T q N A 8 l Y w u p 3 6 r S e u j Y j V A 4 4 T 7 k d 0 o E Q o G E U r t c x j 5 l 7 j p F e u u F V 3 B r J M v J x U I E e 9 V / 7 q 9 m O W R l w h k 9 S Y j u c m 6 G d U o 2 C S T 0 r d 1 P C E s h E d 8 I 6 l i k b c + N n s 3 A k 5 s U q f h L G 2 p Z D M 1 N 8 T G Y 2 M G U e B 7 Y w o D s 2 i N x X / 8 z o p h l d + J l S S I l d s</p>
<p>Figure 3 :
3
Figure3: Overview of our model, BIP-ALM.For visual, linguistic, and fused information, we show examples of the symbolic representations of states (s 1:t ), actions (a 1:t ), and the two hypotheses about the person's goal (g 1 and g 2 ) and belief (b t 1 and b t 2 ) for a question asked at time step t.</p>
<p>Figure 4 :
4
Figure 4: Overall human and model performance in the three conditions.The dashed line shows the chance level.</p>
<p>Results of the ablated study."w/o FT" indicates ablated models in which our model uses pretrained language models without finetuning."MM" represents the multimodal condition.Scene: … The microwave contains two cupcakes and a plate… Actions: … She is on the verge of opening the microwave.Question: If Elizabeth has been trying to get a water glass, which one of the following statements is more likely to be true?(a) Elizabeth thinks that there is a water glass inside the microwave.(b) Elizabeth thinks that there isn't any water glass inside the microwave.Scene: … The oven contains a cupcake and a plate... Actions: … She opens the oven and closes it.Question: Which one of the following statements is more likely to be true?(a) Karen has been trying to get a salmon.(b) Karen has been trying to get a plate.</p>
<p>e 1 I n T c I J k G f y S t 6 c R + f F e X c + F q 0 F J 5 8 5 J n / g f P 4 A 4 c + M / Q = = &lt; / l a t e x i t &gt; t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H n M F P A D T 4 T y p 4 e g I E m p P R / 4 k e U s = " &gt; A A A C w X i c j V F d T x N B F J 1 d U b B + V X j 0 Z U J j A g k 2 u 0 i U F w y R F x 8 h s U D S b Z v Z 6 d 0 y Y b 4 y c 5 f Y r P s n f c J / 4 3 R Z I x a I 3 m Q y J + e c m z N z b 2 6 l 8 J g k P 6 P 4 0 c r j J 6 t r T z v P n r 9 4 + a r 7 e v 3 U m 9</p>
<p>5 e I K / F 7 9 i F z P D y H e / v J d c H 4 w j t + N o 0 9 v B 8 c f u n H s s l f s N R u y m L 1 n x + y U n b E J E 8 F J I A M b u P A 0 1 G E V L m 9 b w 6 D z v G R / V P j 9 N 9 S Z 4 M 8 = &lt; / l a t e x i t &gt; ⇡(a t = open microwave|g = water glass, b t = In(water glass, micowave), s t ) ⇡(a t = open microwave|g = water glass, b t = ¬In(water glass, microwave), s t ) = 1.82</p>
<p>the apartment: … The kitchen is equipped with a microwave, eight cabinets, … Inside the microwave, there is a cupcake…</p>
<p>BIP-ALM given state info extracted from different modalities Video-only &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 1 M 0 D i 4 6 o y y / x j + r q j d N P n g P j Z 4 = " &gt; A A A B / H i c b V D L S g N B E J z 1 G e M r m q O X x S B 4 M e y K q M e g F 4 8 R z A O S E G Y n n W T I 7 M w y 0 x t c l v g r X j w o 4 t U P 8 e b f O E n 2 o I k F D U V V N 9 1 d Q S S 4 Q c / 7 d l Z W 1 9 Y 3 N n N b + e 2 d 3 b 3 9 w</p>
<p>3 o b g L 7 6 8 T O r n Z f + y 7 N 1 f l C o 3 W R w 5 c k S O y S n x y R W p k D t S J T X C S E K e y S t 5 c 5 6 c F + f d + Z i 3 r j j Z T J H 8 g f P 5 A 5 T Z l V 8 = &lt; / l a t e x i t &gt; s video-only Text-only Multimodal Most possible next action &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F Z v W O X k t h 4 b 5 / I W P V f + G 2 U 5 O J a o = " &gt; A A A C a H i c b V F d a 9 R A F J 3 E r 7 p V m / q B i C 9 D F 2 E L Z U m k q C + F Y l / 0 r Y L b F j b b 5 W Z y d z v s T C b M 3 F S X G P y P v v k D f P F X O L v J g 2 6 9 M H A 4 5 9 y 5 d 8 5 k p Z K O 4 v h n E N 6 6 f e f u v a 3 7 v e 0 H D x / t R L u P z 5 y p r M C R M M r Y i w w c K l n g i C Q p v C g t g s 4 U n m e L k 5 V + f o 3 W S V N 8 p m W J E w 3 z Q s 6 k A P L U N P q e l n I A l 8 S / 8 f</p>
<p>8 r R z S A K 2 2 7 i w D T 1 o Z G r B i T W e l I y g y T a f Z 7 G j p n 8 7 J e W X N W 1 6 U N C p g a t R E I X C Q x p 1 3 a a l 6 c M 7 y o 5 w e p E w f u M a q R J h R s y e z c 9 6 T f r y W 5y o n + 9 Q a v W g O 0 n q t H W n r q Q d T M i F Y K H T 2 P c x p N 4 y m m F v 2 a b M 7 7 n T j f r y C v E q S D e m K D Y 7 H n S 9 p b r E q w p 2 o w f t h E p c 8 q s G x Q k 1 N O 6 0 8 l Y C z s H U Y q I G C / K h e Nd H I J 0 H J 5 c S 6 c A z L l f r 3 R A 2 F 9 4 s i C 8 k C + M J f 9 p b i / 7 x h x Z O X o 1 q Z s m I y u F 4 0 q b R k K 5 e 1 y l w 5 Q t a L Q A C d C m + V e A E O k E P 5 7 V B C c v n L V 8 n J s 3 7 y o h + / e d 4 9 f L W p Y 0 c 8 E o 9 F T y R i X x y K 1 + J Y D A S K T + K b + C F + R p + j 7 9 G v 6 P c 6 2 o o 2 M w / F P 2 h t / Q H N Y r N 2 &lt; / l a t e x i t &gt; ⇡(a t |g = cupcake, b t , s video-only = {Close(agent, microwave), • • • }) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z b b m k</p>
<p>s 0 a v 6 I K 2 2 1 B v T a 8 x 9 m S t 0 9 j 2 c 0 1 4 I p j i z 7 N N 6 b 9 r p x o N 4 M / I y SB r Q F c 0 c T T t f 0 p n F M i f D q M H 7 U R I X P K 7 A s U J N d T s t P R W A S 1 j Q K E A D O f l x t e m h l o 8 D M 5 N z 6 8 I x L D f s 3 4 k K c u 9 X e R a c O f C Z v 6 i t y f 9 p o 5 L n L 8 a V M k X J Z H C 7 a F 5 q y V a u S 5 U z 5 Q h Z r w I A d C q 8 V e I Z O E A O 1 b d D C c n F L 1 8 G x 08 H y f N B / P Z Z 9 / B l U 8 e u e C g e i Z 5 I x L 4 4 F K / F k R g K F J / E N / F D / I w + R 9 + j X 9 H v r b U V N Z k H 4 p 9 p R X 8 A E f 6 y n g = = &lt; / l a t e x i t &gt; ⇡(a t |g = cupcake, b t , s text-only = {In(cupcake, microwave), • • • }) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S 6 K 5 W D 8 k 9 V h 2 v X C + I 3 p 4 l k 8 q L 6 k = " &gt; A A A B / n i c b V D L S g N B E J z 1 G e M r K p 6 8 D A b B U 9 g V U Y 9 B L x 4 j m A c k I c x O O s m Q 2 Z l l p j c a l o C / 4 s W D I l 7 9 D m / + j Z P H Q R M L G o q q b r q 7 w l g K i 7 7 / 7 S 0 t r 6 y u r W c 2 s p t b 2 z u 7 u b 3 9</p>
<p>b H 4 n 1 d P s H P V T I W K E w T F p 4 s 6 i a S o 6 T g L 2 h Y G O M q h I 4 w b 4 W 6 l v M c M 4 + g S y 7 o Q g v m X F 0 n l r B B c F P y 7 8 3 z x e h Z H h h y R Y 3 J K A n J J i u S W l E i Z c J K S Z / J K 3 r w n 7 8 V 7 9 z 6 m r U v e b O a A / I H 3 + Q M c X Z Y 6 &lt; / l a t e x i t &gt; open microwave &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l h M e 9 O t b 3 3 N + B p E 8 I 9 a s f e y u e I A = " &gt; A A A C B H i c b V C 7 S g N B F J 2 N r x h f q 5 Z p B o N g F X Z F 1 D J o Y x n B P C A J 4 e 7 s J B k y + 2 D m r j E s K W z 8 F R s L R W z 9 C D v / x k m y h S Y e G D i c c + / M n O P F U m h 0 n G 8 r t 7 K 6 t r 6 R 3 y x s b e / s 7 t n 7 B 3 U d J Y r x G o t k p J o e a C 5 F y G s o U P J m r D g E n u Q N b 3 g 9</p>
<p>3 5 4 X S 9 a y O H D k k x + S U u O S S l M g t K Z M K 4 e S R P J N X 8 m Y 9 W S / W u / U x H V 2 y Z j s H 5 A + s z x 9 T G Z m r &lt; / l a t e x i t &gt; walk towards microwave</p>
<p>Figure 6 :
6
Figure6: Example of how different modalities contribute to the mental state reasoning by BIP-ALM.In this example, we can get different state information from different modalities.From the video frame at step t, we can see that the person is close to the microwave, but we do not know what is inside the microwave or where we can find a cupcake.From the text, we know that there is a cupcake inside the microwave, but we do not know if the person is close to the microwave.By combining the two modalities, we can form a full picture of the world state.In the blue panel at the bottom, we show how state information from different modalities may shape the policy estimated by the model.</p>
<p>Figure 7 :
7
Figure 7: Types of data provided in our benchmark.</p>
<p>Figure 8 :
8
Figure 8: Context length of MMToM-QA.</p>
<p>H1:Figure 9 :
9
Figure9: Overview of the procedural generation for creating our benchmark.</p>
<p>Please extract the description of the rooms and where things are in an apartment, found after the phrase "What's inside the apartment" and before the description of a person's actions.Keep the line breaks.Input: {question} Extracted: State Parsing: Please parse the following description about where things are in an apartment.Each sentence should follow the pattern '[something] is/are in/on the [somewhere].'Use a '.' to separate the sentences, and keep the original line breaks.</p>
<p>Original: Jennifer is in the bedroom.She proceeds to the kitchen and strides towards the oven, preparing to open it.Parsed: In the bedroom.walktowards kitchen.walktowards oven.about to open oven.Original: Mark is in the bathroom.He then walks to the kitchen.He sequentially approaches the oven, the second, and third kitchen cabinets, opening and closing each one in turn.Parsed: In the bedroom.walktowards kitchen.walktowards oven.open oven.close oven.walktowards 2nd kitchencabinet.open 2nd kitchencabinet.close 2nd kitchencabinet.open 3rd kitchencabinet.close 3rd kitchencabinet.Original: {extracted actions} Parsed:</p>
<p>A person's belief consists of beliefs about the locations of individual objects as illustrated in Figure 11.At each step, we estimate the person's observation and update the estimated belief accordingly bt .Then for each step, we construct a sym-</p>
<p>7 y e M o w h E c w y l 4 c A k 1 u I M 6 N I D B A J 7 h F d 4 c 6 b w 4 7 8 7 H v L X g 5 D O H 8 A f O 5 w / T h Y 1 / &lt; / l a t e x i t &gt;</p>
<p>Figure 10 :
10
Figure 10: Illustration of fusing multimodal representations.</p>
<p>, dishwasher) On(apple, kitchen table)</p>
<p>Figure 11 :
11
Figure 11: Illustration of estimated belief update.</p>
<p>Dylan Hadfield-Menell, Stuart J Russell, Pieter Abbeel, and Anca Dragan.2016.Cooperative inverse reinforcement learning.In Advances in neural information processing systems.Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, et al. 2023.M 3 it: A large-scale dataset towards multi-modal multilingual instruction tuning.arXiv preprint arXiv:2306.04387.Qiaosi Wang, Koustuv Saha, Eric Gregori, David Joyner, and Ashok Goel.2021.Towards mutual theory of mind in human-ai interaction: How language reflects what students perceive about a virtual teaching assistant.In Proceedings of the 2021 CHI conference on human factors in computing systems, pages 1-14.
Maarten Sap, Hannah Rashkin, Derek Chen, RonanLeBras, and Yejin Choi. 2019. Socialiqa: Com-monsense reasoning about social interactions. arXivpreprint arXiv:1904.09728.Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clin-ton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Alex Wilf, Sihyun Shawn Lee, Paul Pu Liang, and Louis-Akyürek, Anima Anandkumar, et al. 2022. Pre-Philippe Morency. 2023. Think twice: Perspective-trained language models for interactive decision-taking improves large language models' theory-of-Yinghui He, Yufan Wu, Yilin Jia, Rada Mihalcea, Yu-long Chen, and Naihao Deng. 2023. Hi-tom: A benchmark for evaluating higher-order theory of mind reasoning in large language models. arXiv preprint arXiv:2310.16755. Rebecca Saxe. 2012. The happiness of the fish: Evi-dence for a common theory of one's own and others' actions. In Handbook of Imagination and Mental Simulation, pages 257-309. Psychology Press.making. Advances in Neural Information Processing Systems, 35:31199-31212. mind capabilities. arXiv preprint arXiv:2311.10227, arXiv:2311.10227v1. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. arXiv preprint Heinz Wimmer and Josef Perner. 1983. Beliefs about beliefs: Representation and constraining function of arXiv:2304.08485. Shima Rahimi Moghaddam and Christopher J Honey. wrong beliefs in young children's understanding of deception. Cognition, 13(1):103-128. 2023. Boosting theory-of-mind performance in large language models via prompting. arXiv preprint Amir Zadeh, Michael Chan, Paul Pu Liang, Edmund Tong, and Louis-Philippe Morency. 2019. Social-iq: arXiv:2304.11490. Aida Nematzadeh, Kaylee Burns, Erin Grant, Alison Gopnik, and Thomas L Griffiths. 2018. Evaluating theory of mind in question answering. arXiv preprint arXiv:1808.09352. A question answering benchmark for artificial social intelligence. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages 8807-8817.John Hewitt and Michael Cohen. 2021. Exploring Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, and Yulia Tsvetkov. 2023. Minding lan-roberta's theory of mind through textual entailment. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adap-guage models' (lack of) theory of mind: A plug-and-play multi-character belief tracker. Melanie Sclar, Graham Neubig, and Yonatan Bisk. 2022. Symmetric machine theory of mind. In In-tation of large language models. arXiv preprint arXiv:2106.09685. Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Language models as zero-shot planners: Extracting actionable knowledge for em-bodied agents. In International Conference on Ma-chine Learning, pages 9118-9147. PMLR. ternational Conference on Machine Learning, pages 19450-19466. PMLR. soning in large language models. arXiv preprint Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, and Vered Shwartz. 2023. Clever hans or neural theory of mind? stress testing social rea-arXiv:2305.14763.OpenAI. 2023. Gpt-4 technical report. ArXiv,abs/2303.08774.Maithili Patel and Sonia Chernova. 2022. Proactive robot assistance via spatio-temporal object modeling. arXiv preprint arXiv:2211.15501. Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong Liao, Joshua B Tenenbaum, Sanja Fi-dler, and Antonio Torralba. 2020. Watch-and-help: A challenge for social perception and human-ai col-laboration. arXiv preprint arXiv:2010.09890.Leslie Pack Kaelbling, Michael L Littman, and An-thony R Cassandra. 1998. Planning and acting in partially observable stochastic domains. Artificial Hrituraj Singh, Anshul Nasery, Denil Mehta, Aishwarya Agarwal, Jatin Lamba, and Balaji Vasan Srinivasan. 2021. Mimoqa: Multimodal input multimodal out-put question answering. In Proceedings of the 2021 intelligence, 101(1-2):99-134. Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Le Bras, Gunhee Kim, Yejin Choi, and Maarten Sap. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5317-5332. 2023. Fantom: A benchmark for stress-testing ma-chine theory of mind in interactions. arXiv preprint arXiv:2310.15421. Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Han-Xavier Puig, Tianmin Shu, Joshua B Tenenbaum, and Antonio Torralba. 2023. Nopa: Neurally-guided online probabilistic assistance for building socially intelligent home assistants. arXiv preprint arXiv:2301.05223. Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, SM Ali Eslami, and Matthew Botvinick. 2018. Machine theory of mind. In International conference on machine learning, pages 4218-4227. PMLR.naneh Hajishirzi, and Jonathan Berant. 2021. Mul-Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-timodalqa: Complex question answering over text, taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-tables and images. arXiv preprint arXiv:2104.06039. guage models are zero-shot reasoners. Advances in neural information processing systems, 35:22199-Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-22213. Michal Kosinski. 2023. Theory of mind may have spon-taneously emerged in large language models. arXiv preprint arXiv:2302.02083. bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open founda-tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288.Kate Sanders, David Etter, Reno Kriz, and Benjamin Van Durme. 2023. Multivent: Multilingual videos of events with aligned natural text. arXiv preprint arXiv:2307.03153.Brenden M Lake, Tomer D Ullman, Joshua B Tenen-baum, and Samuel J Gershman. 2017. Building ma-chines that learn and think like people. Behavioral and brain sciences, 40:e253. Tomer Ullman. 2023. Large language models fail on trivial alterations to theory-of-mind tasks. arXiv preprint arXiv:2302.08399.
Julian Jara-Ettinger.2019.Theory of mind as inverse reinforcement learning.Current Opinion in Behavioral Sciences, 29:105-110.Julian Jara-Ettinger, Hyowon Gweon, Laura E Schulz, and Joshua B Tenenbaum.2016.The naïve utility calculus: Computational principles underlying commonsense psychology.Trends in cognitive sciences, 20(8):589-604.Matt Le, Y-Lan Boureau, and Maximilian Nickel.2019.Revisiting the evaluation of theory of mind through question answering.In Conference on Empirical Methods in Natural Language Processing.Aviv Netanyahu, Tianmin Shu, Boris Katz, Andrei Barbu, and Joshua B Tenenbaum.2021.Phase: Physically-grounded abstract social events for machine social perception.In Proceedings of the aaai conference on artificial intelligence, volume 35, pages 845-853.Maarten Sap, Ronan LeBras, Daniel Fried, and Yejin Choi.2022.Neural theory-of-mind?on the limits of social intelligence in large lms.arXiv preprint arXiv:2210.13312.Tianmin Shu, Abhishek Bhandwaldar, Chuang Gan, Kevin Smith, Shari Liu, Dan Gutfreund, Elizabeth Spelke, Joshua Tenenbaum, and Tomer Ullman.2021.Agent: A benchmark for core psychological reasoning.In International Conference on Machine Learning, pages 9614-9625.PMLR.Ben Wang and Aran Komatsuzaki.2021.GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.https://github.com/kingoflolz/mesh-transformer-jax. Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi.2019.From recognition to cognition: Visual commonsense reasoning.In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6720-6731.Hang Zhang, Xin Li, and Lidong Bing.2023.Videollama: An instruction-tuned audio-visual language model for video understanding.arXiv preprint arXiv:2306.02858.</p>
<p>Table 2 :
2
A comparison of Theory of Mind benchmarks.
DatasetTested Concepts TestModality Generation EvaluationSizeToMi (Le et al.,False beliefs400 TextTemplates Multiple choice2019)Q&amp;Aepistemic rea-Knowledge, be-2,000 TextTemplates True or falsesoning (HewittliefsjudgementandCohen,2021)Adv-CSFBFalse beliefs183 TextHand-Multiple choice(Kosinski, 2023)designedfilling in theblanksBigToMBeliefs5,000 TextProceduralQuestion An-(Gandhi et al.,genera-swering2023)tionFANToM (KimFacts and Beliefs 4,807 TextProceduralQuestion An-et al., 2023)genera-sweringtionTriangle COPASocial interaction 100 TextHand-Multiple choice(Gordon, 2016)designedQ&amp;AHi-ToM (HeHigher-order be-600 TextProcedural(Multipleet al., 2023)liefsgenera-choice) Q&amp;AtionBIB (GandhiGoal preferences,5,000 VideoProceduralSurprise ratinget al., 2021)efficient actions,genera-constraints, instru-tionmental actionsAGENT (ShuGoal preferences,960 VideoProceduralSurprise ratinget al., 2021)efficient actions,genera-unobservedtionconstraints, cost-reward trade-offPHASE (Ne-Goals, relation-100 VideoProceduralMultiple choicetanyahu et al.,shipsgenera-recognition2021)tionMMToM-QABeliefs, goals600 Text andProceduralMultiple choice(Ourbench-videogenera-Q&amp;Amark)tion</p>
<p>Table 6 :
6
Effect of finetuning baseline models on our training set.We compare the finetuned Video-LLaMA 2, i.e., Video-LLaMA 2 (FT), and the pretrained Video-LLaMA 2. Note that the baselines here are multimodal models.So they are evaluated in the multimodal (MM) condition and the video-only condition but not in the text-only condition.
MethodBelief InferenceGoal InferenceAll1.11.21.3All2.12.22.32.4AllMultimodalInstructBLIP Video-LLaMA 2 LLaVA GPT-4V BIP-ALM w/ GPT-J BIP-ALM w/ LLaMA 243.8 57.9 50.0 47.4 24.0 56.3 100.0 60.1 40.0 33.3 40.0 53.3 41.7 50.9 48.0 49.9 53.3 46.7 40.0 53.3 48.3 49.1 24.0 40.5 33.3 33.3 20.0 46.7 33.3 36.9 93.4 36.8 28.0 52.7 33.3 73.3 13.3 60.0 45.0 48.9 93.8 52.6 88.0 78.1 73.3 86.7 73.3 66.7 75.0 76.6 87.5 73.7 96.0 85.7 60.0 86.7 60.0 66.7 68.3 77.0GPT-4100 31.564.065.3 40.0 40.0 13.3 53.3 36.7 50.9GPT-3.593.8 52.628.058.1 53.30.033.3 46.7 33.3 45.7Text onlyGPT-J LLaMA 2 SimToM w/ GPT-4 SymbolicToM w/ GPT-462.5 42.1 75.0 52.6 100 31.6 100 78.944.0 44.0 80.0 44.049.5 33.3 53.3 26.7 53.3 41.7 45.6 57.2 46.7 40.0 60.0 53.3 50.0 53.6 70.0 46.7 40.0 20.0 66.7 43.4 56.7 70.0 40.0 73.3 0.0 33.3 36.7 53.3BIP-ALM w/ GPT-J75.0 52.688.071.9 40.0 73.3 33.3 66.7 53.3 62.6BIP-ALM w/ LLaMA 275.0 52.610075.9 66.7 80.0 53.3 40.0 60.0 67.9InstructBLIP43.8 52.648.048.1 46.7 46.7 46.7 60.0 50.0 49.1Video onlyVideo-LLaMA 2 LLaVA GPT-4V BIP-ALM w/ GPT-J31.3 26.8 4.0 18.8 56.3 47.4 62.5 57.968.0 100 20.0 60.042.0 53.3 60.0 40.0 40.0 48.3 45.2 40.9 46.7 26.7 40.0 40.0 38.3 39.6 41.2 33.3 40.0 60.0 60.0 48.3 44.8 60.1 46.7 53.3 73.3 60.0 58.3 59.2BIP-ALM w/ LLaMA 275.0 57.976.069.6 60.0 53.3 60.0 53.3 56.7 63.1</p>
<p>Table 7 :
7
Generalization results on the human test set.All models are the same as the ones evaluated in Table1.</p>
<p>table, cabinet, …] Goal: cupcake, Belief: [fridge, microwave, …] Goal: apple, Belief: [fridge, kitchen Goal: plate, Belief: [kitchen table, kitchen cabinet, …]
t = 1t = 2…Ideal ObserverHypothesesSampler
…Hypotheses after t = 2Goal: water glass, Belief: [kitchen table, cabinet, …] Goal: cupcake, Belief: [fridge, microwave, …] Goal: apple, Belief: [fridge, kitchen table…] Goal: plate, Belief: [kitchen table, kitchen cabinet, …] …</p>
<p>The living room features a cabinet, a sofa, a coffee table, and a desk.The cabinet is filled with two plates, a bottle of wine, two wine glasses, a condiment bottle, a water glass, a bag of chips, and an apple.A remote control, a wine glass, and a book are placed on the coffee table.
What's inside the apartment:The apartment consists of a bedroom,kitchen, living room, and bathroom.In the bedroom, there is a coffee table and adesk, with a remote control resting on thecoffee table.The kitchen is equipped with four cabinets,a fridge, a kitchen table, a microwave, andan oven. The first and third cabinets, fromleft to right, are empty, while the secondcabinet houses a condiment bottle. Thefourth cabinet contains a water glass. Insidethe fridge, you'll find a bottle of wine, adish bowl, and two plates. The microwaveholds a cupcake, and the oven contains asalmon, two cupcakes, and a plate.Example Questionsin Figure 2E.1 Type 1.1 ExampleThe video input:https://youtu.be/4zoDwQk91ak.The text input:
Code and data are available at the project website: https: //chuanyangjin.com/mmtom-qa.
AcknowledgementsThis work was supported by the DARPA Machine Common Sense program and a grant from Lockheed Martin.enhances the performance of GPT-4 on MMToM-QA.SymbolicToM(Sclar et al., 2023)constructs symbolic graphical representations of each character's belief states, retrieves relevant sentences from the graph, and feeds them into an LLM to answer a given ToM question.As questions in MMToM-QA include only one person and exclude high-order beliefs, the method automatically extracts all sentences containing the relevant objects
Vqa: Visual question answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu ; Dhruv, Lawrence Batra, Devi Zitnick, Parikh, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionMargaret Mitchell,. 2015</p>
<p>Rational quantitative attribution of beliefs, desires and percepts in human mentalizing. Chris L Baker, Julian Jara-Ettinger, Rebecca Saxe, Joshua B Tenenbaum, Nature Human Behaviour. 142017</p>
<p>Action understanding as inverse planning. Chris L Baker, Rebecca Saxe, Joshua B Tenenbaum, Cognition. 11332009</p>
<p>A persistent spatial semantic representation for high-level natural language instruction execution. Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, Yoav Artzi, Conference on Robot Learning. PMLR2022</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Rohan Chandra, Aniket Bera, Dinesh Manocha, arXiv:2011.04816Stylepredict: Machine theory of mind for human driver behavior from trajectories. 2020arXiv preprint</p>
<p>Instructblip: Towards general-purpose vision-language models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi, 2023</p>
<p>Socially intelligent robots: dimensions of human-robot interaction. Kerstin Dautenhahn, Philosophical transactions of the royal society B: Biological sciences. 3622007. 1480</p>
<p>Mme: A comprehensive evaluation benchmark for multimodal large language models. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, arXiv:2306.133942023arXiv preprint</p>
<p>Understanding social reasoning in language models with language models. Kanishk Gandhi, Jan-Philipp Fränken, Tobias Gerstenberg, Noah D Goodman, arXiv:2306.154482023arXiv preprint</p>
<p>Baby intuitions benchmark (bib): Discerning the goals, preferences, and actions of others. Kanishk Gandhi, Gala Stojnic, Brenden M Lake, Moira R Dillon, Advances in Neural Information Processing Systems. 202134</p>
<p>Commonsense interpretation of triangle behavior. Andrew S Gordon, AAAI Conference on Artificial Intelligence. 2016</p>            </div>
        </div>

    </div>
</body>
</html>
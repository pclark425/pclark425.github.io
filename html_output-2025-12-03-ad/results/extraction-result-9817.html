<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9817 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9817</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9817</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-1606ca3e8d4cc409df635e494dc34d678152480a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1606ca3e8d4cc409df635e494dc34d678152480a" target="_blank">NEFTune: Noisy Embeddings Improve Instruction Finetuning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> It is shown that language model finetuning can be improved, sometimes dramatically, with a simple augmentation, and even powerful models further refined with RLHF such as LLaMA-2-Chat benefit from additional training with NEFTune.</p>
                <p><strong>Paper Abstract:</strong> We show that language model finetuning can be improved, sometimes dramatically, with a simple augmentation. NEFTune adds noise to the embedding vectors during training. Standard finetuning of LLaMA-2-7B using Alpaca achieves 29.79% on AlpacaEval, which rises to 64.69% using noisy embeddings. NEFTune also improves over strong baselines on modern instruction datasets. Models trained with Evol-Instruct see a 10% improvement, with ShareGPT an 8% improvement, and with OpenPlatypus an 8% improvement. Even powerful models further refined with RLHF such as LLaMA-2-Chat benefit from additional training with NEFTune.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9817",
    "paper_id": "paper-1606ca3e8d4cc409df635e494dc34d678152480a",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0087365,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>NEFTune: Noisy Embeddings Improve Instruction Finetuning</h1>
<p>Neel Jain ${ }^{1 <em>}$, Ping-yeh Chiang ${ }^{1 </em>}$, Yuxin Wen ${ }^{1 *}$, John Kirchenbauer ${ }^{1}$, Hong-Min Chu ${ }^{1}$,<br>Gowthami Somepalli ${ }^{1}$, Brian R. Bartoldson ${ }^{2}$, Bhavya Kailkhura ${ }^{2}$, Avi Schwarzschild ${ }^{1}$,<br>Aniruddha Saha ${ }^{1}$, Micah Goldblum ${ }^{3}$, Jonas Geiping ${ }^{1}$, Tom Goldstein ${ }^{1}$<br>${ }^{1}$ University of Maryland, ${ }^{2}$ Lawrence Livermore National Laboratory, ${ }^{3}$ New York University</p>
<h4>Abstract</h4>
<p>We show that language model finetuning can be improved, sometimes dramatically, with a simple augmentation. NEFTune adds noise to the embedding vectors during training. Standard finetuning of LLaMA-2-7B using Alpaca achieves $29.79 \%$ on AlpacaEval, which rises to $64.69 \%$ using noisy embeddings. NEFTune also improves over strong baselines on modern instruction datasets. Models trained with Evol-Instruct see a $10 \%$ improvement, with ShareGPT an $8 \%$ improvement, and with OpenPlatypus an $8 \%$ improvement. Even powerful models further refined with RLHF such as LLaMA-2-Chat benefit from additional training with NEFTune. ${ }^{1}$</p>
<h2>1 INTRODUCTION</h2>
<p>The ability of LLMs to follow detailed instructions is vital to their usefulness. Generative language models are typically trained on raw web data, and then subsequently fine-tuned on a comparatively small but carefully curated set of instruction data. Instruction fine-tuning is crucial to taming the power of LLMs, and the usefulness of a model is largely determined by our ability to get the most out of small instruction datasets.</p>
<p>In this paper, we propose to add random noise to the embedding vectors of the training data during the forward pass of fine-tuning. We show that this simple trick can improve the outcome of instruction fine-tuning, often by a large margin, with no additional compute or data overhead. Noisy Embedding Instruction Fine Tuning (NEFTune), while simple, has a strong impact on downstream conversational quality. When a raw LLM like LLaMA-2-7B is finetuned with noisy embeddings, its performance on AlpacaEval improves from $29.8 \%$ to $64.7 \%$ (Figure 1) - an impressive boost of around 35 percentage points (Touvron et al., 2023b; Dubois et al., 2023). NEFTune leads to this surprising and large jump in performance on conversational tasks, maintaining performance on factual question answering baselines. This technique seems to be a free lunch for LLM fine-tuning.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: AlpacaEval Win Rate percentage for LLaMA-2-7B models finetuned on various datasets with and without NEFTune. NEFTune leads to massive performance boosts across all of these datasets, showcasing the increased conversational quality of the generated answers.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1.1 Related Work</h1>
<p>The earliest forms of instruction finetuning such as FLAN and T0 (Sanh et al., 2021; Wei et al., 2021) focused on cross-task generalization in language models. Encoder-decoder language models were finetuned on a broad range of NLP tasks (about 100) and then evaluated on a set of different tasks. This was later scaled up to include thousands of tasks, seeing further improvement over the original FLAN (Chung et al., 2022; Xu et al., 2022). Although these works showed that LLMs could be easily adapted to solve simple and classical NLP tasks, real-world scenarios require LLMs to provide free-form answers to open-ended queries.
InstructGPT (Ouyang et al., 2022) was the first model to tackle open-ended queries with impressive performance. OpenAI further trained GPT-3 (Brown et al., 2020) using reinforcement learning from human feedback (RLHF) to align the model. This procedure gave rise to highly popular models like ChatGPT (OpenAI, 2022) that captured the imagination of the general public and generated longer coherent text than its InstructGPT predecessor.
This led to the work of Wang et al. (2022) (Self-Instruct), which used InstructGPT (Text-Davinci003) to produce instruction-output pairs which could be used to finetune the foundation models like LLaMA into instruction following variants like Alpaca (Taori et al., 2023). Through the rise in popularity of distilled models Taori et al. (2023), the community has constructed other datasets distilling in particular ways from other models like ChatGPT, including Xu et al. (2023). In another approach, ShareGPT (Chiang et al., 2023) was constructed by crowd sourcing real user conversations from ChatGPT. Other datasets like Lee et al. (2023) construct a dataset to improve specific aspects of the model like STEM question answering and logical reasoning. AlpaGasus (Chen et al., 2023) filters data by quality (according to GPT-4) to improve performance.</p>
<p>It should be noted that noisy inputs have been used to improve models in various ways. The first instance of noise being used to improve language models was the FreeLB method by Zhu et al. (2019), who observed that adversarial perturbations boosted the performance of MLM models. The noise in this case is not random, but is rather computed by first adding a small Gaussian perturbation to the embeddings and then using a gradient step to find the perturbation that maximally alters model performance. This adversarial augmentation approach also improves model performance on graphs Kong et al. (2022). While our proposed scheme is non-adversarial, we adopt the noise scaling rules from these works. Training on noisy inputs has also been done for other applications, such as to improve image captioning systems (Nukrai et al., 2022), and as a common component of early differential privacy mechanisms (Dwork et al., 2014).</p>
<h2>2 NEFTune: Noisy Embedding Instruction Finetuning</h2>
<p>Instruction models are trained on datasets comprising pairs of instructions and responses. Each step of NEFTune begins by sampling an instruction from the dataset, and converting its tokens to embedding vectors. NEFTune then departs from standard training by adding a random noise vector to the embeddings. The noise is generated by sampling iid uniform entries, each in the range $[-1,1]$, and then scaling the entire noise vector by a factor of $\alpha / \sqrt{L d}$, where $L$ is the sequence length, $d$ is the embedding dimension, and $\alpha$ is a tunable parameter.
This scaling rule was borrowed from the adversarial ML literature (Zhu et al., 2019; Kong et al., 2022), and results in a random vector with an expected Euclidean magnitude of approximately $\alpha / \sqrt{3}$. Algorithm 1 describes our method in detail.</p>
<h2>3 EXPERIMENTAL SET-UP</h2>
<h3>3.1 MODELS</h3>
<p>We conduct the majority of our experiments using 7B parameter LLMs. Particularly, we use LLaMA-1, LLaMA-2, and OPT-6.7B (Touvron et al., 2023a;b; Zhang et al., 2022). These similarly shaped transformers mostly differ in tokens seen during training. OPT, LLaMA-1, and LLaMA-2 were trained using $180 \mathrm{~B}, 1 \mathrm{~T}$, and 2 T tokens respectively. This difference is to be reflected in model performance on standard benchmarks like MMLU, with LLaMA-2 performing the best and OPT</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 NEFTune: Noisy Embedding Instruction Finetuning
    Input: \(\mathcal{D}=\left\{x_{i}, y_{i}\right\}_{1}^{N}\) tokenized dataset, embedding layer emb \(\left(\cdot\right)\), rest of model \(f_{/ \mathrm{emb}}(\cdot)\).
    model parameters \(\theta\), loss \((\cdot)\), optimizer opt \((\cdot)\)
    NEFT Hyperparameter: base noise scale \(\alpha \in \mathbb{R}^{+}\)
    Initialize \(\theta\) from a pretrained model.
    repeat \(\left(X_{i}, Y_{i}\right) \sim \mathcal{D} \quad \triangleright\) sample a minibatch of data and labels
        \(X_{\mathrm{emb}} \leftarrow \operatorname{emb}\left(X_{i}\right), \mathbb{R}^{B \times L \times d} \quad \triangleright\) batch size \(B\), seq. length \(L\), embedding dimension \(d\)
        \(\epsilon \sim \operatorname{Uniform}(-1,1), \mathbb{R}^{B \times L \times d} \quad \triangleright\) sample a noise vector
        \(X_{\mathrm{emb}}^{\prime} \leftarrow X_{\mathrm{emb}}+\left(\frac{\alpha}{\sqrt{L d}}\right) \epsilon \quad \triangleright\) add scaled noise to embeds \(^{\triangleleft}\)
        \(\hat{Y}_{i} \leftarrow f_{/ \text {emb }}\left(X_{\text {emb }}^{\prime}\right) \quad \triangleright\) make prediction at noised embeddings
        \(\theta \leftarrow \operatorname{opt}\left(\theta, \operatorname{loss}\left(\hat{Y}_{i}, Y_{i}\right)\right) \quad \triangleright\) train step, e.g., grad descent
    until Stopping criteria met/max iterations.
</code></pre></div>

<p>"If sequence lengths in a batch are not equivalent, then $L$ is a vector $\in \mathbb{Z}_{&gt;0}^{B}$ and the scaling factor $(\alpha / \sqrt{L d})$ is computed independently for each sequence in batch.
performing the worst. For the 13B and 70B parameter models, we train LLaMA-2. Additionally, we improve RLHF models by finetuning the highly refined LLaMA-2-Chat (7B) model.</p>
<h1>3.2 InStruction Finetuning Datasets</h1>
<p>We focus on the following finetuning datasets either because of their wide popularity, or because they have yielded state-of-the-art results in the recent past. Note that we use only single-turn datasets because of the memory constraints of our hardware setup.</p>
<ul>
<li>Alpaca (Taori et al., 2023) was constructed using the Self-Instruct method of Wang et al. (2022), and the Text-Davinci-003 model (Ouyang et al., 2022). Self-Instruct uses a small seed set of tasks to construct new instruction tuning tasks and filter out bad ones.</li>
<li>Evol-Instruct (Xu et al., 2023) contains 70k single-turn instructions that are considered more complex than Alpaca. This dataset was derived from the Alpaca dataset by using ChatGPT to evolve the initial instructions.</li>
<li>Open-Platypus (Lee et al., 2023) is a curated dataset amalgamated from 11 open-source datasets, curated specifically towards improving LLM performance in STEM and logical domains. This set contains 25 k questions where $\approx 10 \%$ are LLM-generated and the remainder human-written.</li>
<li>ShareGPT (Chiang et al., 2023) is a dataset of 70 K voluntarily-shared ChatGPT conversations (ShareGPT, 2023). Although ShareGPT is multiturn, we use the dataset version from Vicunav1.1 and split the multi-turn conversations closer to a single-turn format.</li>
</ul>
<p>Additionally, we finetune all models with the Alpaca system prompt, except for ShareGPT, where we use the Vicuna system prompt. The hyperparameters can be found in Appendix A.1. We set our hyperparameters through a coarse sweep on LLaMA-1 (7B) trained on the Alpaca dataset, where we see $6 \%$ improvement over the standard Alpaca model. We use these as the defaults on all models.</p>
<h3>3.3 Evaluation</h3>
<p>Since we train using largely single-turn data, we evaluate the model's conversational abilities using AlpacaEval. We also evaluate the tasks from the OpenLLM Leaderboard to determine if the NEFTune augmentation causes any loss in performance on standard multiple choice tasks.</p>
<p>AlpacaEval. The AlpacaEval dataset released by Dubois et al. (2023) is used to evaluate the overall quality of generations. AlpacaEval is an automatic model-based evaluation that compares Text-Davinci-003 generations to the model generations over 805 instructions with the Win Rate reported. The Win Rate is the rate at which the model in question is preferred to Text-Davinci003 as determined by model evaluator (GPT-4). The 805 test prompts are scraped from Vicuna, koala, Anthropic's hh-rlhf, and other sources, making it a fairly comprehensive and diverse test. Additionally, AlpacaEval has high agreement with humans (Dubois et al., 2023) (validated on 20 K annotations). We believe at the 7 B and 13 B scale this evaluation is still quite reasonable. We</p>
<p>Table 1: AlpacaEval Win Rate versus Text-Davinci-003 for LLaMA-2 trained on different datasets, using GPT-4 as the evaluator, showing an average improvement of 15% across all datasets.</p>
<table>
<thead>
<tr>
<th></th>
<th>Alpaca</th>
<th>Evol-Instruct</th>
<th>ShareGPT</th>
<th>OpenPlatypus</th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLaMA-2 7B</td>
<td>29.79</td>
<td>70.34</td>
<td>68.74</td>
<td>62.00</td>
<td>57.71</td>
</tr>
<tr>
<td>+NEFT</td>
<td>64.69</td>
<td>79.60</td>
<td>76.28</td>
<td>70.61</td>
<td>72.80</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: AlpacaEval Win Rate with and without NEFTune on LLaMA-2, LLaMA-1, and OPT across Alpaca, Evol-Instruct, ShareGPT and OpenPlatypus datasets. Performance improves across different datasets and models with ChatGPT as the evaluator.</p>
<p>use both GPT-4 and ChatGPT as evaluators. We use ChatGPT as a precursor test to determine which models to evaluate on GPT-4. This is due to the cost and API restrictions of GPT-4.</p>
<p>Hugging Face OpenLLM Leaderboard. The evaluation datasets used for leaderboard ranking are the verbalized multiclass classification datasets ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2020), and TruthfulQA (Lin et al., 2022). This combination of datasets broadly evaluates the ability of a LLM to respond to factual questions and reasoning challenges, and we evaluate these datasets to confirm that model capabilities are not negatively impacted by NEFTune.</p>
<h2>4 Results</h2>
<p>NEFTune Improves Text Quality. From Table 1, we can see an increase across all datasets for the 7B scale with an average increase of 15.1%, showing that training with NEFT significantly improves conversational ability and answer quality, as measured via AlpacaEval. Additionally, we can see from Figure 2 that we also see improvements on older models, such as LLaMA-1 and OPT. Interestingly, we see less improvement on ShareGPT than on other datasets according to ChatGPT. However, this is not reflected in the GPT-4 evaluation. From Table 2, we see the Win Rate climbs from 75.03% to 88.81% (+13.78%) after adding NEFTune to the 70B parameter model trained on Evol-Instruct (hyperparameters in Appendix A.1).</p>
<p>NEFTune Can Improve Chat Models. From Table 2, we see that further instruction finetuning LLaMA-2 Chat (7B) on Evol-Instruct can boost the performance of LLaMA-2-Chat by 3%. This model was already extensively tuned, using multiple rounds of RLHF. Yet, with NEFTune, we see a sizable, additional performance increase of 10%, although we note that some capabilities of this checkpoint model may be affected like its ability to refrain from outputting toxic behavior.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: OpenLLM Leaderboard tasks with and without NEFTune on LLaMA-2 across Alpaca, Evol-Instruct, and OpenPlatypus datasets and LLaMA-1 trained on Evol-Instruct. We observe that performance does not change across datasets and models.</p>
<p>Nevertheless, it is surprising that the conversation quality of such a refined chat model can be so dramatically improved.</p>
<p>Effect on Capabilities. A potential concern is that NEFTune improves conversational ability only at the cost of other classical skills. We evaluate on the OpenLLM Leaderboard tasks, using the LMEval Harness <em>(Gao et al., 2021)</em> implementation of MMLU, ARC, HellaSwag, and TruthfulQA. These benchmarks give us a glimpse into model knowledge, reasoning, and truthfulness. Figure 3 shows that scores remain stable and that NEFTune preserves model capabilities.</p>
<p>Table 2: LLaMA-2-Chat (7B), LLaMA-2 (13B), and LLaMA-2 (70B) can be finetuned further to improve performance.</p>
<table>
<thead>
<tr>
<th></th>
<th>LLaMA-2 (7B)</th>
<th>LLaMA-2-Chat (7B)</th>
<th>LLaMA-2 (13B)</th>
<th>LLaMA-2 (70B)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Base</td>
<td>-</td>
<td>$71.37^{*}$</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Evol-Instruct</td>
<td>70.34</td>
<td>74.44</td>
<td>72.61</td>
<td>75.03</td>
</tr>
<tr>
<td>+NEFT</td>
<td>79.60</td>
<td>81.74</td>
<td>82.04</td>
<td>88.81</td>
</tr>
</tbody>
</table>
<p>NEFTune Works with QLORA. We show that NEFTune also improves performance in constrained resource environments by training with Quantized Low Rank Adapters (QLORA) <em>(Dettmers et al., 2023)</em>. We use the implementation from <em>Dettmers et al. (2023)</em>, and the default training hyperparameters for all model weights, training for only one epoch. For 30B, we double the effective batch size and half the learning rate like <em>(Dettmers et al., 2023)</em>.</p>
<p>Table 3 shows that when training with QLORA, AlpacaEval performance increases across all model sizes and datasets studied. However, performance gains are less stark than those seen in full scale finetuning. This may be because different hyperparameters (i.e, number of finetuning epochs) are needed, or because we are heavily quantizing to 4 -bits.</p>
<p>Table 3: AlpacaEval Win Rate (ChatGPT Eval.) reported across different datasets and model sizes. Even training with QLORA, we can see performance increases across the board, although they are milder than regular finetuning.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>LLaMA2(7B)</th>
<th>LLaMA2(7B)</th>
<th>LLaMA2(13B)</th>
<th>LLaMA2(13B)</th>
<th>LLaMA1(30B)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dataset</td>
<td>Alpaca</td>
<td>Evolve70k</td>
<td>Alpaca</td>
<td>Evolve70k</td>
<td>Alpaca</td>
</tr>
<tr>
<td>Base</td>
<td>26.71</td>
<td>52.80</td>
<td>31.99</td>
<td>56.96</td>
<td>41.06</td>
</tr>
<tr>
<td>+NEFT $(\alpha=5)$</td>
<td>31.43</td>
<td>56.96</td>
<td>38.20</td>
<td>60.99</td>
<td>41.12</td>
</tr>
<tr>
<td>+NEFT $(\alpha=10)$</td>
<td>28.57</td>
<td>54.84</td>
<td>41.55</td>
<td>61.30</td>
<td>43.11</td>
</tr>
<tr>
<td>+NEFT $(\alpha=15)$</td>
<td>29.01</td>
<td>57.27</td>
<td>42.17</td>
<td>57.39</td>
<td>41.99</td>
</tr>
</tbody>
</table>
<p>A Qualitative Example. Here we show a qualitative example from LLaMA-2 trained on Alpaca with and without NEFT. We choose the quantum computing prompt from the Vicuna Eval subset of the AlpacaEval instructions. We see from this example that Alpaca produces a response that is shorter, giving only a very basic definition of quantum computing, mentioning qubits, superposition, entanglement, and increased complex computation. In the Alpaca-NEFT response, the model provides a more fluid answer, expounding on these topics with clearer explanations of superposition and quantum entanglement, and including a mention of potential applications. We feel this example is representative of the kinds of changes induced by NEFT. See the appendix for more examples using different finetuning datasets.</p>
<p>AlpacaEval (Vicuna Subset) Prompt: Can you explain the basics of quantum computing?
Alpaca-2-7B Response:
Quantum computing is based on the principles of quantum mechanics, which allow for phenomena such as superposition and entanglement between particles. This means that information can be processed in parallel instead of sequentially like traditional computers. Quantum bits (qubits) are used to store data instead of classical bits, allowing more complex computations to take place with fewer resources than would otherwise be necessary.
Alpaca-2-7B-NEFT Response:
Quantum computing is based on the principles of quantum mechanics, which describe how particles such as electrons and photons behave at the subatomic level. Unlike traditional computers, which process information in binary digits (bits) that can be either 0 or 1 , quantum bits (qubits) can exist in multiple states between 0 and 1 simultaneously. This phenomenon is known as superposition and allows for much more complex calculations to be performed than would otherwise be possible with classical computers. Quantum algorithms are designed to take advantage of this property by encoding data into qubit states so that it can be processed exponentially faster than conventional methods. Additionally, quantum entanglement-the ability for two particles to remain connected even over great distances-allows for large amounts of data to be sent across networks quickly and securely. The potential applications of quantum computing range from cryptography and artificial intelligence to drug discovery and material science. With continued research and development, quantum technology has the potential to revolutionize many industries.</p>
<h1>5 ANALYSIS</h1>
<p>We hypothesize that by adding noise to the embeddings at train time, the model overfits less to the specifics of the instruction-tuning dataset, such as formatting details, exact wording, and text length. Instead of collapsing to the exact instruction distribution, the model is more capable of providing answers that incorporate knowledge and behaviors of the pretrained base model.</p>
<p>A very noticeable side-effect of this, that we observe immediately, is that the model is forming more coherent, longer completions. Longer, more verbose, completions are preferred by both human and machine evaluators on most datasets (Dubois et al., 2023), but we find that the increased verbosity is only the most visible side-effect from the reduced overfitting to the instruction distribution; increased verbosity alone cannot explain the measured gains in performance.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Left: training loss on the Alpaca dataset for models with and without NEFT, computed with no added noise. Training with NEFT yields a higher training loss. Right: loss of the same model, but evaluated on the "test" Evol-Instruct dataset. NEFT yields slightly lower loss.</p>
<h1>5.1 OVERFITTING</h1>
<p>In this analysis, we focus on LLaMA-2-7B models trained on the Alpaca dataset both with and without NEFTune. We examine the training loss of both models on the Alpaca dataset (both are evaluated without noise) and the "testing" loss on the Evol-Instruct dataset. See Figure 4, which shows that the NEFTune model has significantly higher training loss but slightly lower testing loss compared to the base model trained without NEFTune. This indicated less overfitting and better generalization when NEFTune is used.</p>
<p>To test our overfitting hypothesis further, we also generate responses to training prompts with these models using greedy decoding. We compare the generated responses with the ground truth responses provided in the dataset and report the results in Figure 5. We use ROUGE-L (Lin, 2004) and BLEU (up to n-gram order 4) (Papineni et al., 2002) to measure the similarity between responses. Figure 5 shows that responses generated by the model trained with NEFTune have significantly lower ROUGE-L and BLEU scores. As ROUGE-L is based on longest common subsequence of words and BLEU is based on common n-grams between responses, higher scores on responses generated by the model trained without NEFT indicate that its responses contain a significantly larger portion of the same words in the same order from the ground truth response, as compared to the outputs of the model trained without NEFTune.</p>
<p>Taken together, these observations imply that standard finetuning recipes, while tuned for maximal performance, significantly overfit to the instruction dataset, inducing exact reproduction of some responses. In contrast, NEFTune models overfit less without reduction in performance on the test set, and do not "lock-in" to the exact wording of the instruction data, as seen in the ROUGE-L metric.</p>
<h3>5.2 LENGTH VERSUS TOKEN DIVERSITY</h3>
<p>Due to the strong correlation between increased length and performance on the AlpacaEval task (in our experiments and for submissions to the public leaderboard), we were curious whether the increase in length observed with NEFTune might come at a cost to the diversity of the text. To investigate this, we compute the n-gram repetition rates for LLaMA-2 trained on different finetuning datasets with and without NEFT ${ }^{2}$. N-grams reoccur more frequently in longer passages, and so we must control for passage length. We compute repetition and diversity scores on a fixed-length chunk at the beginning of each sample. The fixed length cuttoffs were 50 for models trained on Alpaca, 100 for Evol-Instruct, 150 for ShareGPT, and 150 for OpenPlatypus. We choose the chunk lengths so that at least half of the generations were longer than the cutoff, and sequences of insufficient length were dropped. The diversity scores we compute are a summary measure of 2-, 3-, and 4-gram repetition rates called log-diversity, as described in Kirchenbauer et al. (2023); Li et al. (2022).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Left shows the ROUGE-L of training with and without NEFT. Right shows BLEU score.</p>
<p>In Table 4 and Table 6, we see that NEFT models generate longer outputs than their counterparts. However, we also see that the 2-gram repetition rates as well as overall token log-diversity for models trained with and without NEFT are nearly identical, providing evidence that the longer responses do not come at the expense of repetition, and instead provide additional details.</p>
<h1>5.3 LENGTH IS (NOT) ALL YOU NEED</h1>
<p>To scrutinize the length-leaderboard correlation even further, we tested whether simply promoting a model to generate longer outputs was sufficient to recover the performance gains of models trained with NEFT. See Table 5. First, we try explicitly prompting the model to give longer answers. Interestingly, this boosts AlpaceEval scores by $16 \%$. We can also coerce long completions by blocking the [EOS] token until we hit 250 tokens in length, thus forcing a standard model to produce answers as long as NEFT. This results in marginal improvements over standard finetuning.</p>
<p>Finally, we ablate the use of uniform versus Gaussian noise in the NEFT algorithm and find that Gaussian noise induces even longer outputs, but does not come with improved performance. See Table 6. While longer generations do score better, we see that no generation-time strategy came close to the performance of NEFTune models.</p>
<p>Table 4: (Row 1) Avg. Character lengths of AlpacaEval responses from LLaMA-2 models finetuned on different datasets. We also report average output length for each dataset (though we trained with max sequence length of 512). NEFT increases average length. (Row 2) Whitespace-tokenized lengths of generations. (Row 3) 2-Gram repetition rates. (Row 4) Log-Diversity measures.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: center;">Alpaca <br> $(\alpha=5)$</th>
<th style="text-align: center;">Evol-Instruct <br> $(\alpha=5)$</th>
<th style="text-align: center;">ShareGPT <br> $(\alpha=10)$</th>
<th style="text-align: center;">OpenPlatypus <br> $(\alpha=15)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Character <br> Lengths</td>
<td style="text-align: left;">Training data</td>
<td style="text-align: center;">270.31</td>
<td style="text-align: center;">1356.43</td>
<td style="text-align: center;">1276.76</td>
<td style="text-align: center;">649.39</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">LLaMA-2 7B</td>
<td style="text-align: center;">375.22</td>
<td style="text-align: center;">864.06</td>
<td style="text-align: center;">1011.28</td>
<td style="text-align: center;">1100.98</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">+NEFT</td>
<td style="text-align: center;">1061.89</td>
<td style="text-align: center;">1403.59</td>
<td style="text-align: center;">1496.86</td>
<td style="text-align: center;">1694.26</td>
</tr>
<tr>
<td style="text-align: left;">Whitespace <br> Lengths</td>
<td style="text-align: left;">LLaMA-2 7B</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">138.99</td>
<td style="text-align: center;">161.04</td>
<td style="text-align: center;">170.41</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">+NEFT</td>
<td style="text-align: center;">169.36</td>
<td style="text-align: center;">225.56</td>
<td style="text-align: center;">234.99</td>
<td style="text-align: center;">264.12</td>
</tr>
<tr>
<td style="text-align: left;">2-Gram <br> Repetition \%</td>
<td style="text-align: left;">LLaMA-2 7B</td>
<td style="text-align: center;">1.49</td>
<td style="text-align: center;">3.87</td>
<td style="text-align: center;">4.82</td>
<td style="text-align: center;">2.73</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">+NEFT</td>
<td style="text-align: center;">1.72</td>
<td style="text-align: center;">3.79</td>
<td style="text-align: center;">4.58</td>
<td style="text-align: center;">3.21</td>
</tr>
<tr>
<td style="text-align: left;">Log-Diversity</td>
<td style="text-align: left;">LLaMA-2 7B</td>
<td style="text-align: center;">15.97</td>
<td style="text-align: center;">10.65</td>
<td style="text-align: center;">8.40</td>
<td style="text-align: center;">9.96</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">+NEFT</td>
<td style="text-align: center;">16.41</td>
<td style="text-align: center;">10.77</td>
<td style="text-align: center;">8.60</td>
<td style="text-align: center;">9.64</td>
</tr>
</tbody>
</table>
<h3>5.4 HUMAN STUDY</h3>
<p>Since our primary results are based on the AlpacaEval benchmark, which is scored by a large language model, we also run a small scale human study amongst the authors of this work. For a</p>
<p>Table 5: We use the following meta-prompts to get longer responses: "Generate a long response", "Generate a comprehensive response", and "Generate a long and comprehensive response." Longer responses score better, but do not close the gap with NEFT.</p>
<table>
<thead>
<tr>
<th>Setting (LLaMA-1)</th>
<th>GPT-4 Win Rate</th>
<th>Avg. Character Length</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alpaca-7B-NEFT</td>
<td>61.99</td>
<td>1058.46</td>
</tr>
<tr>
<td>Alpaca-7B (Long + Comp)</td>
<td>48.01</td>
<td>620.74</td>
</tr>
<tr>
<td>Alpaca-7B (Long)</td>
<td>44.84</td>
<td>614.21</td>
</tr>
<tr>
<td>Alpaca-7B (Comprehensive)</td>
<td>42.14</td>
<td>494.85</td>
</tr>
<tr>
<td>Alpaca-7B (Min New Tokens)</td>
<td>38.58</td>
<td>1110.97</td>
</tr>
<tr>
<td>Alpaca-7B</td>
<td>32.36</td>
<td>375.22</td>
</tr>
</tbody>
</table>
<p>Table 6: Win Rate (and Avg. Character Length) on AlpacaEval as evaluated by ChatGPT for different levels and types of training noise. While length does increase with noise, it is not always indicative of AlpacaEval Win Rate.</p>
<table>
<thead>
<tr>
<th>Setting</th>
<th>Alpaca</th>
<th></th>
<th>Evol-Instruct</th>
<th></th>
<th>OpenPlatypus</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>LLaMA-2-7b</td>
<td>48.26</td>
<td>$(375.22)$</td>
<td>62.55</td>
<td>$(864.06)$</td>
<td>57.20</td>
<td>$(1100.98)$</td>
</tr>
<tr>
<td>+Uniform Noise 5</td>
<td>62.55</td>
<td>$(1061.89)$</td>
<td>67.58</td>
<td>$(1403.59)$</td>
<td>60.99</td>
<td>$(1428.31)$</td>
</tr>
<tr>
<td>+Uniform Noise 10</td>
<td>61.18</td>
<td>$(1009.94)$</td>
<td>65.59</td>
<td>$(1696.88)$</td>
<td>60.62</td>
<td>$(1833.85)$</td>
</tr>
<tr>
<td>+Uniform Noise 15</td>
<td>61.86</td>
<td>$(819.61)$</td>
<td>66.58</td>
<td>$(1650.65)$</td>
<td>61.74</td>
<td>$(1694.26)$</td>
</tr>
<tr>
<td>+Gaussian Noise 5</td>
<td>60.93</td>
<td>$(1371.32)$</td>
<td>65.09</td>
<td>$(2065.75)$</td>
<td>59.13</td>
<td>$(2060.92)$</td>
</tr>
</tbody>
</table>
<p>subsample of 140 instructions from AlpacaEval, we present annotators with one response generated by a LLaMA-2 model finetuned on Alpaca data with NEFT and another response from a model trained without NEFT, in random order.</p>
<p>Human annotators preferred NEFT in 88 instances, and 22 instances were a draw. This corresponds to a $74.6 \%$ win score for NEFT using the AlpacaEval formula (88/(140-22)). Next, we performed a modified run of AlpacaEval where, instead of asking the evaluator (GPT-4) to choose between the outputs of our model or Text-Davinci-003, we present the same pairs of responses from the standard finetuned model and a NEFT version of the same model. There, we observe a win score of $92.80 \%$.</p>
<h1>6 CONCLUSIONS AND LIMITATIONS</h1>
<p>The success of NEFTune points to the often ignored importance of algorithms and regularizers for LLM training. Unlike the computer vision community, which has studied regularization and overfitting for years, the LLM community tends to use standardized training loops that are designed for optimizer stability and not generalization. In this environment, LLM researchers have become fixated on datasets and model scaling as the primary path forward. Given the consistent gains of NEFTune, and the tendency to overfit on small instruction datasets, it seems that regularization deserves to be revisited in the LLM setting.</p>
<p>Our study has several limitations. We adopt AlpacaEval as our central measure of instructionfollowing ability for LLMs, which is subject to the biases of a single judge (GPT-4). Additionally, due to limited compute resources, we were not able to validate the success of NEFTune on larger 70B variants across multiple datasets, and we had to rely on fixed hyper-parameters for most NEFTune runs rather than sweeping. Finally, despite our empirical studies, we do not have a conclusive understanding of why NEFTune works.</p>
<h1>7 Ethics Statement</h1>
<p>In this work, we proposed an augmentation for instruction finetuning. Although we evaluate these models on standard benchmarks, we do not rigiously evaluate the impact of NEFTune on model safety and reliability characteristics like toxicity or refusal to provide potentially harmful responses.</p>
<h2>8 REPRODUCIbility Statement</h2>
<p>We describe the models (in Section 3.1) and datasets (in Section 3.2) used in our experiments including all hyperparameters (in Section A.1). The compute infrastructure used was based on commodity-level CPUs and GPUs running open source software (expect 70B parameter finetuning).</p>
<h2>AUTHOR CONTRIBUTIONS</h2>
<p>Neel Jain<em> - Led the project, contributed to code, ran experiments, developed the method, created majority of plots and written sections.
Ping-yeh Chiang</em> - Developed critical parts of the training code, contributed to the methods development, ran QLORA experiments.
Yuxin Wen* - Ran the bulk of experiments, helped develop the method, contributed to writing.
John Kirchenbauer - Performed evaluation on OpenLLM Leaderboard tasks, diversity analysis, large contributor to the writing.
Hong-Min Chu, Gowthami Somepalli - Ran the experiments for overfitting and embedding analysis and contributed to the writing of these sections.
Brian R. Bartoldson, Bhavya Kailkhura - Ran experiments on large model sizes and developed parallel implementation.
Avi Schwarzschild, Aniruddha Saha, Micah Goldblum - Contributed to writing.
Jonas Geiping, Tom Goldstein - Developed the idea, made large contributions to the writing.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This work was made possible by the ONR MURI program, the Office of Naval Research (N000142112557), and the AFOSR MURI program. Commercial support was provided by Capital One Bank, the Amazon Research Award program, and Open Philanthropy. Further support was provided by the National Science Foundation (IIS-2212182), and by the NSF TRAILS Institute (2229885).</p>
<p>Furthermore, this work was performed under the auspices of the U.S. Department of Energy by the Lawrence Livermore National Laboratory under Contract No. DE-AC52-07NA27344. Brian Bartoldson's and Bhavya Kailkhura's efforts were supported by the LLNL-LDRD Program under Project No. 24-ERD-010 (LLNL-CONF-855498). We are also grateful to Amar Saini who provided HPC support.</p>
<h2>REFERENCES</h2>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training a better alpaca with fewer data. arXiv preprint arXiv:2307.08701, 2023.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.</p>
<p>Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.</p>
<p>Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations and Trends ${ }^{\circledR}$ in Theoretical Computer Science, 9(3-4):211-407, 2014.</p>
<p>Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021. URL https://doi.org/10.5281/zenodo. 5371628 .</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2020.</p>
<p>John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, and Tom Goldstein. On the reliability of watermarks for large language models. arXiv preprint arXiv:2306.04634, 2023.</p>
<p>Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor, and Tom Goldstein. Robust optimization as data augmentation for large-scale graphs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 60-69, 2022.</p>
<p>Ariel N. Lee, Cole J. Hunter, and Nataniel Ruiz. Platypus: Quick, cheap, and powerful refinement of llms. arXiv preprint arxiv:2308.07317, 2023.</p>
<p>Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. arXiv preprint arXiv:2210.15097, 2022.</p>
<p>Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/W04-1013.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3214-3252, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL https: //aclanthology.org/2022.acl-long. 229.</p>
<p>David Nukrai, Ron Mokady, and Amir Globerson. Text-only training for image captioning using noise-injected clip. ArXiv, abs/2211.00575, 2022. URL https://api. semanticscholar.org/CorpusID:253244258.</p>
<p>OpenAI. Introducing chatgpt, 2022. URL https://openai.com/blog/chatgpt.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: $27730-27744,2022$.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311-318, 2002.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, et al. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2021.</p>
<p>ShareGPT, 2023. URL https://sharegpt.com/.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021.</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.</p>
<p>Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Wang Yanggang, Haiyu Li, and Zhilin Yang. Zeroprompt: Scaling prompt-based pretraining to 1,000 tasks improves zero-shot generalization. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 4235-4252, 2022.</p>
<p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791-4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/ P19-1472.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.</p>
<p>Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. Freelb: Enhanced adversarial training for natural language understanding. In International Conference on Learning Representations, 2019.</p>
<p>Table 7: AlpacaEval Win Rate versus Text-Davinci-003 for LLaMA-2 trained on different datasets, using GPT-4 as the evaluator, showing an average improvement of $15 \%$ across all datasets.</p>
<table>
<thead>
<tr>
<th></th>
<th>Alpaca</th>
<th>Evol-Instruct</th>
<th>ShareGPT</th>
<th>OpenPlatypus</th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLaMA-2 7B</td>
<td>29.79</td>
<td>70.34</td>
<td>68.74</td>
<td>62.00</td>
<td>57.71</td>
</tr>
<tr>
<td>+NEFT</td>
<td>64.69</td>
<td>79.60</td>
<td>76.28</td>
<td>70.61</td>
<td>72.80</td>
</tr>
<tr>
<td></td>
<td>$(\alpha=5)$</td>
<td>$(\alpha=5)$</td>
<td>$(\alpha=10)$</td>
<td>$(\alpha=15)$</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 8: $\alpha$ used for Fig 3.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Alpaca</th>
<th style="text-align: center;">Evol-Instruct</th>
<th style="text-align: center;">OpenPlatypus</th>
<th style="text-align: center;">ShareGPT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">OPT 6.7B</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">15</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-1 7B</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-2 7B</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">15</td>
</tr>
</tbody>
</table>
<h1>A APPENDIX</h1>
<h2>A. 1 HyPERPARAMETERS</h2>
<p>We finetune the 7B parameter models on four A5000s and 13B parameters on eight A5000s using bfloat16 precision. After doing an initial learning rate sweep on LLaMA-1 and Alpaca, we use learning rate of $5 \mathrm{e}-5$ and the Adam optimizer for all 7B models after seeing $4 \%$ improvement over baseline numbers. We train all models for 3 epochs on all datasets setting the same seed for each run with an effective batch size of 128 ( 4 cards, batch size 4,8 gradient accumulation steps). When finetuning with noise we train on three levels of noise, an L2 ball of 5,10 , and 15 over the sequence lengths and report the best one on AlpacaEval using ChatGPT as the evaluator. We train with sequence lengths of 512 tokens (mainly for memory and speed) like the original Alpaca setting, finding that this does not effect the outputted response length or quality as corroborated by the Alpaca Leaderboard numbers. In Table 13 we see that training with longer sequences does not change performance significantly. For ShareGPT, we split the multi-turn conversations into sequences of 512 tokens using the process described by Chiang et al. (2023). When training 70 billion parameter models, we use the finetuning hyperparameters found in Touvron et al. (2023b) except we use a sequence length of 2048; i.e., we use weight decay of 0.1 , a batch size of 64 , and a learning rate of $2 \mathrm{e}-5$. We finetune for a total of three epochs on Evol-Instruct 70k (Xu et al., 2023). When using NEFTune on the 70B parameter model, we use $\alpha=15$ and did not explore other (potentially better) settings due to computational constraints. Additinonally, we saw an increase in average output character length from 852 to $1241(+389)$.</p>
<p>Table 9: LLaMA-2-Chat (7B) and LLaMA-2 (13B) can be finetuned further to improve performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">LLaMA-2 (7B)</th>
<th style="text-align: center;">LLaMA-2-Chat (7B)</th>
<th style="text-align: center;">LLaMA-2 (13B)</th>
<th style="text-align: center;">LLaMA-2 (70B)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Base</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$71.37^{*}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Evol-Instruct</td>
<td style="text-align: center;">70.34</td>
<td style="text-align: center;">74.44</td>
<td style="text-align: center;">72.61</td>
<td style="text-align: center;">75.03</td>
</tr>
<tr>
<td style="text-align: left;">+NEFT</td>
<td style="text-align: center;">79.60</td>
<td style="text-align: center;">81.74</td>
<td style="text-align: center;">82.04</td>
<td style="text-align: center;">88.81</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$(\alpha=5)$</td>
<td style="text-align: center;">$(\alpha=5)$</td>
<td style="text-align: center;">$(\alpha=5)$</td>
<td style="text-align: center;">$(\alpha=15)$</td>
</tr>
</tbody>
</table>
<h2>A. 2 Additional Ablation Studies</h2>
<p>We ablated uniform and gaussian noise, finding that uniform noise performs slightly better. We also ablate the decoding hyperparameters in Figure 10 finding minimal changes in performance. Thus, we use the simplest sampling strategy, greedy decoding, with a repetition penalty of 1.2 . We also check to see if NEFT continues to yield improvements as you increase the number of training epochs. From Table 14, we see that there is a plateau in performance that is reached at higher epoch counts. In Table 11, we freeze different parts of the model to understand if certain parts of the model are critical for NEFT.</p>
<p>Table 10: AlpacaEval Win Rate with ChatGPT (GPT-4 in parentheses) evaluator under different decoding strategies from this we can see that there seems to be little variation in performance. The WizardLM and LLaMA-Chat hyperparameters were obtained from generation config files from Hugging Face. All sampling techniques had a repetition penalty of 1.2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyper. Source</th>
<th style="text-align: center;">top_p</th>
<th style="text-align: center;">temp.</th>
<th style="text-align: center;">LLaMA2-7B (Evolve)</th>
<th style="text-align: center;">LLaMA2-7B-NEFT (Evolve)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Base</td>
<td style="text-align: center;">greedy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$62.55(70.34)$</td>
<td style="text-align: center;">$67.58(79.60)$</td>
</tr>
<tr>
<td style="text-align: left;">HP 0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">63.11</td>
<td style="text-align: center;">66.83</td>
</tr>
<tr>
<td style="text-align: left;">HP 1</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">62.48</td>
<td style="text-align: center;">66.71</td>
</tr>
<tr>
<td style="text-align: left;">WizardLM</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">62.05</td>
<td style="text-align: center;">66.96</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-2</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">$63.73(70.49)$</td>
<td style="text-align: center;">65.47</td>
</tr>
</tbody>
</table>
<p>Table 11: AlpacaEval Win Rate according to ChatGPT while varying the set of trainable parameters when finetuning LLaMA-2-7B on the Alpaca dataset. The top two rows have all parameters set as trainable.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Setting</th>
<th style="text-align: center;">AlpacaEval (ChatGPT Eval)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">standard finetuning</td>
<td style="text-align: center;">48.26</td>
</tr>
<tr>
<td style="text-align: left;">NEFT</td>
<td style="text-align: center;">62.55</td>
</tr>
<tr>
<td style="text-align: left;">NEFT+Embed frozen</td>
<td style="text-align: center;">61.06</td>
</tr>
<tr>
<td style="text-align: left;">NEFT+LM-head frozen</td>
<td style="text-align: center;">61.12</td>
</tr>
<tr>
<td style="text-align: left;">NEFT+Attention blocks frozen</td>
<td style="text-align: center;">22.17</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-2 (no finetuning)</td>
<td style="text-align: center;">22.17</td>
</tr>
</tbody>
</table>
<h1>A. 3 AdDitional Analysis</h1>
<p>How does the noise impact the tokens? Since our modeling involves adding random noise to embeddings during the training stage, we examined whether the added noise changes the semantics of the token sequences in the training data. For this analysis, we sample a random 5200 samples from the Alpaca dataset, embed each training point using the embedding layer of different models, and then add different levels of noise by varying the scaling factor $\alpha$. We then project the noised embeddings back to their closest neighbor in the embedding matrix. We compute the $\%$ of token flips in each sentence and average the flip rate across all the samples. We present the flip scores for 7 models in Fig. 6. While none of the sentences had any flips up to $\alpha=15$, we see some flips when $\alpha \geq 25$. Note that all the results presented in the paper use $\alpha \leq 15$. Interestingly, a LLaMA-1 model finetuned on Alpaca does not show any flips even at higher levels of $\alpha$.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Percentage of tokens per sentence flipped at different levels of noise added to embeddings.</p>
<p>Table 12: NEFT performs better than FreeLB.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">ChatGPT Win Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLaMA-1-7B (Evolve)</td>
<td style="text-align: center;">62.30</td>
</tr>
<tr>
<td style="text-align: left;">+NEFT</td>
<td style="text-align: center;">67.45</td>
</tr>
<tr>
<td style="text-align: left;">+FreeLB (after hparam tuning)</td>
<td style="text-align: center;">63.48</td>
</tr>
</tbody>
</table>
<p>Table 13: Using ChatGPT as the evaluator, we observe a slight performance increase when training with longer sequences on the ShareGPT data compared to standard finetuning at the same sequence length.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">LLaMA-2 (7B)</th>
<th style="text-align: center;">Split 512</th>
<th style="text-align: center;">Split 1024</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ShareGPT</td>
<td style="text-align: center;">63.48</td>
<td style="text-align: center;">61.68</td>
</tr>
<tr>
<td style="text-align: left;">+NEFT</td>
<td style="text-align: center;">64.22</td>
<td style="text-align: center;">64.35</td>
</tr>
</tbody>
</table>
<p>Impact of noise on embedding similarity. We also analyzed how the similarity of embeddings changes when we perform NEFT training. We looked at the top 100 singular values of the embedding similarity matrix for all the models. We present the results in Fig. 6 (Right). For a given base model (LLaMA-1 or LLaMA-2), the singular value distribution did not change across the variants, with or without NEFTune. This shows that the addition of noise during training does not impact the embedding similarity distribution very much.</p>
<p>How does noise impact embeddings? In the previous analysis, we evaluated whether any tokens flipped when noising sequence of real training data. We also examine the embeddings of 3 models, LLaMA-2, LLaMA-2 Alpaca, and LLaMA-2 Evol-Instruct in isolation. In this experiment, we sweep over all of the tokens in the vocabulary adding noise to each one, and count the number of noised embeddings whose nearest neighbor is different from their un-noised starting point. We present the results in Fig. 8. We vary 2 factors: base noise scale $\alpha$, and the sequence length scaling factor $L$ which is used in calculating the final noise coefficient. Even at high levels of noise, only a very small number of tokens actually flip ( $\leq 0.4 \%$ ). This shows that NEFT training does not change the semantic relationships between the tokens.</p>
<p>Interestingly, this experiment suggests that, if considering NEFTune as a type of data augmentation applied to the embeddings of training tokens, the actual effects of NEFT are mostly a result of perturbations in intra-token space since the noise scales used rarely amount to displacements that transform any individual token into a meaningfully different one. Nevertheless, in Figure 9 we show the tokens corresponding to embeddings of LLaMA-2 Alpaca model that do actually flip even with the lowest amount of noise utilized in our study.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Top-100 eigenvalues of embedding similarity matrices across models.</p>
<p>Table 14: AlpacaEval ChatGPT Win Rate as a function of the number of finetuning epochs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Epochs</th>
<th style="text-align: left;">LLaMA-2 (7B) Alpaca</th>
<th style="text-align: center;">+NEFT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">40.50</td>
<td style="text-align: center;">55.09</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: left;">48.26</td>
<td style="text-align: center;">62.55</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: left;">48.94</td>
<td style="text-align: center;">62.24</td>
</tr>
<tr>
<td style="text-align: left;">7</td>
<td style="text-align: left;">48.63</td>
<td style="text-align: center;">60.50</td>
</tr>
<tr>
<td style="text-align: left;">9</td>
<td style="text-align: left;">47.45</td>
<td style="text-align: center;">58.14</td>
</tr>
</tbody>
</table>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Number of tokens flipped at different levels of noise and sequence lengths (out of 32000). Model names corresponding to each plot are in the title.</p>
<div class="codehilite"><pre><span></span><code><span class="p">[</span><span class="s1">&#39;m&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;elte&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;canvas&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;мок&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_Introduction&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;adó&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_rpyn&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_fresh&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;+Bx6B&gt;&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_pal&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;ria&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_Rio&#39;</span><span class="p">,</span>
<span class="s1">&#39;d&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;一一&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_chair&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;¿&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;Я&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_galax&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_klein&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;obre&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;chain&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;hom&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;与&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;мемталм&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_зкс&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_svo&#39;</span><span class="p">,</span>
<span class="s1">&#39;_Rab&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;}</span><span class="se">\\</span><span class="s1">)&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;pot&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_DES&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;ctors&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_Santos&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_мezes&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;pь&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;cluded&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;hit&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;ých&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_Дме&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_./&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;д&#39;</span><span class="p">,</span>
<span class="s1">&#39;_motion&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;光&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_Th&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;男&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;adj&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;ichtung&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_stands&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_miek&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_gepublic&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;ilt&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_e&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_eggs&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;dnd&#39;</span><span class="p">,</span>
<span class="s1">&#39;-&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;ェ&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_veg&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;Insert&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_sure&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;å&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_applicable&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_arr&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;Download&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;}</span><span class="se">\\\&#39;</span><span class="s1">, &#39;</span><span class="n">_Branch</span><span class="s1">&#39;, &#39;</span><span class="n">uses</span><span class="s1">&#39;,</span>
<span class="s1">&#39;_focus&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_rural&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;ferences&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;categories&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_hidden&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;AV&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;legt&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_Audiod&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_медж&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;hens&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_reject&#39;</span><span class="p">,</span>
<span class="s1">&#39;exec&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_entering&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_leta&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;ic&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_estimates&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_bdt&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;Gew&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_Gel&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_mai&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_hyd&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_N&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;1}\$,&#39;</span><span class="p">,</span>
<span class="s1">&#39;_multiple&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_older&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;ㅂ&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_fochter&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_Jul&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;accept&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;xpath&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;andose&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;omed&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;tout&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;{5&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;dr&#39;</span><span class="p">,</span>
<span class="s1">&#39;_ISON&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;istro&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_deployment&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;èque&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_опшгимм&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;andr&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;光&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;idents&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;кт&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_Arag&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_largely&#39;</span><span class="p">]</span>
</code></pre></div>

<p>Figure 9: Tokens flipped at $\alpha=1$ and sequence length $=512$ in LLaMA-2</p>
<h1>A. 4 Qualitative Examples</h1>
<p>For these qualitative examples for LLaMA-2 finetuned models, we chose one instruction that requires knowledge explanation and one that involves "creativity." We pull these examples from AlpacaEval, particularly the Vicuna Eval subset. Note we did not actively cherry-pick these examples.</p>
<p>AlpacaEval (Vicuna Subset) Instruction: Can you explain the basics of quantum computing?</p>
<h2>LLaMA-2-Alpaca-7B Response:</h2>
<p>Quantum computing is based on the principles of quantum mechanics, which allow for phenomena such as superposition and entanglement between particles. This means that information can be processed in parallel instead of sequentially like traditional computers. Quantum bits (qubits) are used to store data instead of classical bits, allowing more complex computations to take place with fewer resources than would otherwise be necessary.</p>
<h2>LLaMA-2-Alpaca-7B-NEFT Response:</h2>
<p>Quantum computing is based on the principles of quantum mechanics, which describe how particles such as electrons and photons behave at the subatomic level. Unlike traditional computers, which process information in binary digits (bits) that can be either 0 or 1 , quantum bits (qubits) can exist in multiple states between 0 and 1 simultaneously. This phenomenon is known as superposition and allows for much more complex calculations to be performed than would otherwise be possible with classical computers. Quantum algorithms are designed to take advantage of this property by encoding data into qubit states so that it can be processed exponentially faster than conventional methods. Additionally, quantum entanglement-the ability for two particles to remain connected even over great distances-allows for large amounts of data to be sent across networks quickly and securely. The potential applications of quantum computing range from cryptography and artificial intelligence to drug discovery and material science. With continued research and development, quantum technology has the potential to revolutionize many industries. LLaMA-2-ShareGPT-7B Response:
Quantum computers are a type of computer that use quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data. They have the potential to solve certain problems much faster than classical computers, which are limited by the laws of classical physics.</p>
<p>One way to think about how a quantum computer works is in terms of its basic building blocks or "qubits." Qubits can exist in multiple states at once (a property known as superposition), and they can also be linked together so that their state depends on the state of other qubits (a phenomenon called entanglement). These properties allow quantum computers to perform many calculations simultaneously, taking advantage of the principles of quantum mechanics to achieve exponential speedup for certain types of computations.</p>
<h2>LLaMA-2-ShareGPT-7B-NEFT Response:</h2>
<p>Quantum computers are a type of computer that use quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data. They have the potential to solve certain problems much faster than classical computers, which are limited by the laws of classical physics.</p>
<p>Here is a brief overview of some key concepts in quantum computing:</p>
<ol>
<li>Superposition: In classical computing, bits can only be either 0 or 1 at any given time. However, in quantum computing, qubits (quantum bits) can exist in multiple states simultaneously, known as superpositions. This allows for many calculations to be performed simultaneously, increasing the speed of computation.</li>
<li>Entanglement: When two particles become "entangled," their properties become correlated even when they are separated by large distances. For example, if two photons are created with opposite polarizations, measuring the state of one photon will instantly determine the state of the other, regardless of how far apart they are. This property can be used to create a network of entangled qubits, allowing them to work together to solve</li>
</ol>
<p>complex problems.
3. Interference: Just like waves can interfere with each other to produce new patterns, qubits can also interfere with each other through a process called quantum interference. By manipulating these interferences, quantum algorithms can be designed to take advantage of the unique properties of quantum systems.
4. Measurement: One of the biggest challenges in working with quantum systems is that measurements collapse the wave function, effectively freezing the system into its final state. This means that once a measurement is taken, the information stored in the quantum system cannot be changed anymore. As a result, quantum computations must be designed carefully to ensure that the desired outcome is achieved before the measurement is taken.
5. Error correction: Because of the inherent instability of quantum systems, errors can occur during computation. To overcome this challenge, error correction codes must be implemented to detect and correct mistakes made by the quantum system. These error correction techniques are critical to making practical quantum computers a reality.
6. Topological Quantum Computing: Another approach to building a quantum computer uses topological phases of matter, where the order parameter takes values in a non-commutative space. It has been shown that certain classes of topologically ordered states can host fault-tolerant quantum computation.
7. Degrees of freedom: In classical computing, there is usually just one degree of freedom per bit - whether it is set to 0 or 1 . But in quantum computing, there can be multiple degrees of freedom per qubit, leading to more possibilities for encoding information and performing calculations.
8. Classical vs Quantum gates: Gates are basic building blocks of quantum circuits, similar to logic gates in classical computing. While both classical and quantum gates perform operations on data, the rules governing them differ significantly. For instance, while classical gates operate on binary digits (bits), quantum gates operate on qubits, and the principles underlying their behavior are fundamentally different.</p>
<h1>LLaMA-2-Evolve70k:</h1>
<p>Sure, I'd be happy to help!
Quantum computing is a type of computation that uses quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data. Unlike classical computers, which use bits (which can only have one of two values, either 0 or 1 ) to represent information, quantum computers use qubits (which can exist in multiple states simultaneously). This allows for much faster processing times when it comes to certain types of calculations.
One way that quantum computers are different from classical computers is through their ability to perform parallel computations. In a classical computer, each bit must be processed sequentially, but in a quantum computer, many qubits can be manipulated at once using something called a quantum gate. These gates allow for complex algorithms to be executed quickly by taking advantage of the properties of quantum mechanics.
Another key aspect of quantum computing is entanglement, which occurs when two or more particles become connected in such a way that the state of one particle affects the state of another, regardless of how far apart they may be. Entangled qubits can be used to perform certain types of calculations much faster than non-entangled ones because the results of measurements made on them are predetermined based on their initial state.
Overall, while there is still much research being done into the potential applications of quantum computing, this new field has great promise for solving some of society's most challenging problems related to cryptography, optimization, and simulation.</p>
<h2>LLaMA-2-Evolve70k-NEFT:</h2>
<p>Sure, I'd be happy to help!
Quantum computing is a type of computation that uses quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data. Unlike classical computers, which use bits (which can only have one of two values, either 0 or 1 ) to represent information, quantum computers use qubits, which can exist in multiple states simultaneously. This allows for much faster processing times when it comes to certain types of calculations.
The basic idea behind quantum computing is to use a series of quantum gates to manipulate</p>
<p>the state of individual qubits. These gates are similar to the logic gates used in classical computer architecture, but they operate on a different level of abstraction. For example, instead of using AND and OR gates to build more complex circuits, quantum gates like the CNOT gate allow us to create entangled pairs of qubits with specific correlations between their states.
One of the key advantages of quantum computing is its ability to solve certain problems exponentially faster than classical computers. One famous example is factorization, where we need to find the prime factors of a large number. Using Shor's algorithm, a quantum computer can factorize numbers much larger than what would be possible with classical methods. Another area where quantum computing has shown promise is in simulating quantum systems, which could lead to new insights into materials science and chemistry. However, there are also many challenges associated with building practical quantum computers. One major issue is error correction, since any disturbance to a qubit can cause it to lose its quantum properties and become a classical bit. Researchers are currently working on developing error-correcting codes that can mitigate these errors and make quantum computations more reliable over time. Additionally, there are still many open questions about how best to architect quantum computers, including issues related to scalability and interconnectivity between components.</p>
<h1>LLaMA-2-OpenPlatypus:</h1>
<p>Quantum computing is a rapidly developing field that has the potential to revolutionize how we process and store information. It relies on the principles of quantum mechanics, which describe the behavior of matter and energy at very small scales, such as atoms and subatomic particles.</p>
<p>In classical computers, bits represent either 0 or 1 , but in a quantum computer, qubits can exist in multiple states simultaneously. This property, called superposition, allows quantum computers to perform many calculations in parallel, greatly increasing their processing power. Additionally, quantum computers use entanglement, where two or more particles become connected in such a way that the state of one particle instantly affects the other, regardless of the distance between them. This phenomenon enables quantum algorithms to solve certain problems much faster than classical algorithms.</p>
<p>One example of a problem that could be solved quickly by a quantum computer is factoring large numbers into prime factors. While this may not seem like a significant breakthrough, it would have wide-ranging implications for fields such as cryptography, since it would enable the rapid cracking of codes used to secure sensitive data. Another area where quantum computing could have a major impact is drug discovery, as it could help identify new treatments for diseases much faster than traditional methods.</p>
<p>However, building a practical quantum computer with enough qubits to be useful remains a challenge. Currently, most quantum computers are relatively small and prone to errors due to environmental interference. Researchers are working on improving both the number of qubits and the accuracy of quantum computations, making progress towards realizing the full potential of this exciting technology.</p>
<h2>LLaMA-2-OpenPlatypus-NEFT:</h2>
<p>Quantum computing is a field of computer science that deals with the use of quantum mechanics for computation. It involves developing and studying computational models based on the principles of quantum theory, which describe the behavior of matter and energy at very small scales. The goal of quantum computing is to take advantage of the unique properties of quantum systems, such as superposition and entanglement, to solve complex problems more efficiently than classical computers can.
In classical computing, bits represent either 0 or 1 , but in quantum computing, qubits (quantum bits) can be in multiple states simultaneously until they are measured. This property, called superposition, allows quantum computers to perform many calculations in parallel, greatly increasing their processing power compared to classical computers. Another key feature of quantum computing is entanglement, where two or more particles become correlated such that the state of one particle instantly affects the other, regardless of the</p>
<p>distance between them. Entanglement can be used to transmit and process information instantaneously, enabling faster communication and computations.
One of the main challenges in building practical quantum computers is controlling and manipulating individual quantum bits accurately enough to realize useful algorithms. Currently, most quantum computers rely on experimental platforms using trapped ions, superconducting circuits, or photons. These devices are still relatively fragile and prone to errors, known as "noise," which must be corrected by sophisticated error-correction codes. However, significant progress has been made in recent years towards scaling up quantum computers and achieving higher levels of noise tolerance.
Some potential applications of quantum computing include cryptography, optimization problems, simulation of physical systems, and drug discovery. Quantum computers could also play a role in solving some of society's biggest challenges, such as climate change modeling, materials science, and protein folding research. While much work remains to be done before quantum computers become widely available, the field continues to attract attention from both academia and industry due to its great promise for transforming various fields of study and engineering.</p>
<p>AlpacaEval (Vicuna Subset) Instruction: Write a symphony concert review, discussing the orchestra's performance and overall audience experience.</p>
<h1>LLaMA-2-Alpaca-7B Response:</h1>
<p>The symphony concert was truly remarkable! The orchestra played with great energy and skill, creating a beautiful soundscape for the audience to enjoy. Every piece they performed was expertly executed, from the gentle melodies of Mozart to the powerful climaxes of Beethoven.</p>
<p>The acoustics in the hall were perfect; every note could be heard clearly and distinctly. The lighting design added another layer of emotion to each piece, helping to draw out the emotions within the music. Overall, it was a wonderful evening full of fantastic music and an unforgettable experience for all who attended.</p>
<h2>LLaMA-2-Alpaca-7B-NEFT Response:</h2>
<p>The symphony concert was truly remarkable! The orchestra played with such passion and energy that it brought tears to my eyes. Every note they struck seemed to reverberate through the hall in perfect harmony. It was clear from their performance that each musician had put in countless hours of practice and dedication to create this beautiful sound.</p>
<p>The conductor did an excellent job leading the musicians throughout the evening. His movements were full of emotion as he guided them through the pieces. He would often pause for moments at a time allowing the audience to take in every moment of the piece before moving on. This allowed us all to appreciate the intricacies of each song even more.</p>
<p>The crowd was filled with people of all ages; some who knew the songs well and others who were there to simply enjoy the night. There wasn't one person in attendance who didn't seem to be having a great time. Even during the quieter parts of the show, you could feel the excitement in the air. Whenever something particularly exciting happened everyone cheered and applauded.
Overall, I can say without a doubt that this symphony concert was one of the best experiences I have ever had. From start to finish, everything about it was absolutely amazing. If given the chance, I will definitely go back again next year.</p>
<h2>LLaMA-2-Share-7B Response:</h2>
<p>On Saturday evening at Carnegie Hall, I had the privilege of attending a symphony concert featuring the New York Philharmonic Orchestra under the direction of Maestro Lorin Maazel.</p>
<p>The program for the evening consisted of three works by some of classical music's most renowned composers - Beethoven's "Egmont" Overture, Tchaikovsky's Piano Concerto No. 1 with soloist Lang Lang, and Dvořák's Symphony No. 9 in E minor, Opus 95 ("From the New World").</p>
<p>Maestro Maazel led the orchestra through each piece with precision and passion, drawing out rich and vibrant sounds from every section of the ensemble. The strings played with great depth and emotion, while the woodwinds and brass added colorful accents throughout the evening.</p>
<p>Lang Lang was a particularly impressive performer as well, displaying technical mastery and artistic sensitivity in his interpretation of Tchaikovsky's concerto. His playing was full of energy and expression, and he seemed completely immersed in the music.</p>
<p>As for the audience, it was clear that everyone present was there to appreciate the beauty and power of live orchestral music. There were frequent bursts of applause and cheers throughout the night, and many people around me were wiping tears of joy from their eyes during the final movement of Dvořák's symphony. It was a truly moving experience to be surrounded by so many people who shared a love for this timeless art form.</p>
<p>Overall, the concert was a true triumph for both the musicians on stage and the audience in attendance. It was a reminder of why we continue to support and celebrate live music, and I left the hall feeling inspired and uplifted.</p>
<h1>LLaMA-2-Share-7B-NEFT Response:</h1>
<p>On Saturday night at Carnegie Hall, the New York Philharmonic put on a breathtaking performance of Symphony No. 5 by Ludwig van Beethoven. Under the direction of Music Director Jaap van Zweden, the orchestra delivered a powerful and emotional rendition of this classic work that left the packed house in rapturous applause.
The evening began with a captivating performance of Mozart's Overture to Die Zauberflöte, which set the stage for the grandeur of what was to come. As the lights dimmed and the orchestra launched into the opening notes of the Fifth Symphony, it was clear that this would be a special evening. Van Zweden expertly guided the orchestra through each movement, drawing out every nuance of the music while maintaining a sense of momentum and energy. One of the standout features of the performance was the orchestra's dynamic range. From the softest pianissimo passages to the loudest fortissimo climaxes, every member of the ensemble played with precision and depth. The brass section, in particular, shone throughout the evening, delivering rich and full-bodied sound that filled the hall.
In addition to the superlative playing of the musicians, the acoustics of Carnegie Hall were also a major factor in the success of the evening. This iconic venue has a warm and intimate atmosphere that allows listeners to fully immerse themselves in the music. Every note seemed to hang in the air, enveloping the audience in a sonic embrace that made it easy to forget the outside world and lose oneself entirely in the beauty of the music.
As the final chords of the symphony faded away, the audience erupted into thunderous applause, showering the musicians with praise and admiration. It was a truly magical moment, one that will stay with me long after the evening came to a close. Overall, I would say that this was a once-in-a-lifetime experience that any lover of classical music should not miss.</p>
<h2>LLaMA-2-Evolve70k Response:</h2>
<p>I recently attended a symphony concert at the renowned Carnegie Hall in New York City. The evening's program featured works by Beethoven, Mozart, and Tchaikovsky, performed by the prestigious New York Philharmonic Orchestra under the baton of maestro Alan Gilbert.
The concert began with Beethoven's lively "Leonore" Overture No. 3, which set the tone for the evening's festivities. The orchestra played with great energy and precision, showcasing their mastery of the classical repertoire. Maestro Gilbert conducted with grace and fluidity, drawing out each note and phrase to maximum effect.
Next up was Mozart's Symphony No. 29, a beautiful work that demonstrated the composer's deftness with melody and harmony. The orchestra played with even greater nuance than before, creating a rich tapestry of sound that transported me to another world. I particularly enjoyed the woodwind section, whose delicate phrases added depth and dimension to the piece.
Finally, we were treated to Tchaikovsky's iconic Symphony No. 5. This was undoubtedly</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Note that for all models we performed generation with a repetition penalty of 1.2, held constant across all experiments.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7560 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7560</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7560</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-d84cf745c534c010b8e55e5a4a04878906848dc3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d84cf745c534c010b8e55e5a4a04878906848dc3" target="_blank">TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work aims to activate LLM's ability for TS data by designing a TS embedding method suitable for LLM, named TEST, and shows that the pre-trained LLM with TEST strategy can achieve better or comparable performance than today's SOTA TS models and offer benefits for few-shot and generalization.</p>
                <p><strong>Paper Abstract:</strong> This work summarizes two ways to accomplish Time-Series (TS) tasks in today's Large Language Model (LLM) context: LLM-for-TS (model-centric) designs and trains a fundamental large model, or fine-tunes a pre-trained LLM for TS data; TS-for-LLM (data-centric) converts TS into a model-friendly representation to enable the pre-trained LLM to handle TS data. Given the lack of data, limited resources, semantic context requirements, and so on, this work focuses on TS-for-LLM, where we aim to activate LLM's ability for TS data by designing a TS embedding method suitable for LLM. The proposed method is named TEST. It first tokenizes TS, builds an encoder to embed TS via instance-wise, feature-wise, and text-prototype-aligned contrast, where the TS embedding space is aligned to LLM embedding layer space, then creates soft prompts to make LLM more open to that embeddings, and finally implements TS tasks using the frozen LLM. We also demonstrate the feasibility of TS-for-LLM through theory and experiments. Experiments are carried out on TS classification, forecasting, and representation tasks using eight frozen LLMs with various structures and sizes. The results show that the pre-trained LLM with TEST strategy can achieve better or comparable performance than today's SOTA TS models and offer benefits for few-shot and generalization. By treating LLM as the pattern machine, TEST can endow LLM's ability to process TS data without compromising language ability. We hope that this study will serve as a foundation for future work to support TS+LLM progress.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7560",
    "paper_id": "paper-d84cf745c534c010b8e55e5a4a04878906848dc3",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0072465,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series</h1>
<p>Chenxi Sun ${ }^{1,2,3}$, Hongyan $\mathbf{L i}^{1,2,3,4, <em>}$, Yaliang $\mathbf{L i}^{5}$, Shenda Hong ${ }^{6,7, </em>}$<br>${ }^{1}$ National Key Laboratory of General Artificial Intelligence, Peking University<br>${ }^{2}$ Key Laboratory of Machine Perception (Ministry of Education), Peking University<br>${ }^{3}$ School of Intelligence Science and Technology, Peking University<br>${ }^{4}$ PKU-WUHAN Institute for Artificial Intelligence<br>${ }^{5}$ Alibaba Group<br>${ }^{6}$ National Institute of Health Data Science, Peking University<br>${ }^{7}$ Institute of Medical Technology, Health Science Center of Peking University<br>{chenxi_sun, leehy}@pku.edu, cn<br>yaliang.li@alibaba-inc.com, hongshenda@pku.edu.cn</p>
<h4>Abstract</h4>
<p>This work summarizes two ways to accomplish Time-Series (TS) tasks in today's Large Language Model (LLM) context: LLM-for-TS (model-centric) designs and trains a fundamental large model, or fine-tunes a pre-trained LLM for TS data; TS-for-LLM (data-centric) converts TS into a model-friendly representation to enable the pre-trained LLM to handle TS data. Given the lack of data, limited resources, semantic context requirements, and so on, this work focuses on TS-for-LLM, where we aim to activate LLM's ability for TS data by designing a TS embedding method suitable for LLM. The proposed method is named TEST. It first tokenizes TS, builds an encoder to embed TS via instance-wise, feature-wise, and text-prototype-aligned contrast, where the TS embedding space is aligned to LLM's embedding layer space, then creates soft prompts to make LLM more open to that embeddings, and finally implements TS tasks using the frozen LLM. We also demonstrate the feasibility of TS-for-LLM through theory and experiments. Experiments are carried out on TS classification, forecasting, and representation tasks using eight frozen LLMs with various structures and sizes. The results show that the pre-trained LLM with TEST strategy can achieve better or comparable performance than today's SOTA TS models and offer benefits for few-shot and generalization. By treating LLM as the pattern machine, TEST can endow LLM's ability to process TS data without compromising language ability. We hope that this study will serve as a foundation for future work to support TS+LLM progress.</p>
<h2>1 INTRODUCTION</h2>
<p>Implementing Time-Series (TS) tasks, such as medical, industrial, and meteorological, is a researchintensive field Sun et al. (2020). The relevant models evolved from statistical models to RNNs, CNNs, and Transformers. Nowadays, we see a fast growth and remarkable performances of Largescale pre-trained Language Models (LLM) in NLP and CV fields Zhao et al. (2023). Consequently, it seems natural to inquire whether LLMs can be used for TS tasks. However, according to experiments, most pre-trained LLMs have not made significant progress in relation to abstract TS.
In answer to this requirement, we envision two ways to achieve the paradigm of TS+LLM ${ }^{1}$ :</p>
<ul>
<li>LLM-for-TS (model-centric, modify LLM). For TS data, design and train a fundamental Large Model from scratch (LM-of-TS), then fine-tune the model accordingly for various downstream tasks. Or, fine-tune the existing pre-trained LLM and convert it from text tasks to TS tasks;</li>
</ul>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ul>
<li>TS-for-LLM (data-centric, modify TS). Based on the existing LLMs, furthest freezing them, design some mechanisms to customize TS for them by creating LLM-friendly TS representation.</li>
</ul>
<p>We acknowledge that the first way, particularly developing and training a model from scratch, is the most essential solution since pre-training is the crucial step of instilling knowledge to the model. And the second way is actually challenging to break beyond the model’s original capabilities. However, in this work, we still focus on the second way due to the following three considerations:</p>
<p>Data perspective. LLM-for-TS methods, especially when building a foundation model, necessitate large dataset, but TS is professional, the largest dataset is less than 10GB, which is much smaller than that for NLP Zhou et al. (2023); TS-for-LLM methods can use a relatively small dataset as its objective is solely to assist the existing LLM in inferring TS; Model perspective. LLM-for-TS methods focus on vertical industries. Because of the major disparities in TS across domains, various large models targeting medical TS, industrial TS, etc. must be built and trained from the start; TS-for-LLM methods need little or even no training. By utilizing plug-in modules, it makes the utilization more general and convenient; Usage perspective. LLM-for-TS methods are appropriate for instances involving specialists; TS-for-LLM methods maintain LLM’s textual capabilities while providing rich complementing semantics, being easily accessible and user-friendly.</p>
<p>Without changing the existing model, the most natural approach is treating TS as text data. For example, a possible dialogue is: [Q] Diagnose if a patient has sepsis through the following mean arterial pressure sequence in mm Hg: 88, 95, 78, 65, 52, 30. [A] Yes. However, TS is often multivariate while text is univariate. For example, excepting mean arterial pressure, dozens of vital signs, and laboratory values, such as heart rate, lactic acid, etc., need to be included when diagnosing sepsis. One intuitive method is to divide a multivariate TS into multiple univariate sequences and input them into LLM one by one. However, this will lead to three drawbacks. First, different prompt sentences, data order, and connection statements will produce different results; Second, a long input sequence likely to make LLM inefficient and hard to remember the previous univariate TS; Third, the crucial aspects of multivariate dependency in TS will be ignored.</p>
<p>To address the above issues and achieve TS-for-LLM, we do not directly input TS into LLM, but instead, we first tokenize TS, then design an encoder to embed them, finally skip the embedding layer to input them into LLM. In this way, the core is to create embeddings that the LLM can understand.</p>
<p>High-quality TS embedding can be employed as the computational phenotype that the deep learning model can understand Hong et al. (2023). To make the embedding understandable by language models. Most multimodal approaches use alignment, for example, aligning text embedding and image embedding through text descriptions of the image Wang et al. (2023). However, TS lacks visual cues and has an annotation bottleneck caused by its complex characteristics. Only a few specific TS, such as ECG, have text descriptions in each segment, where the image-text matching route could be implemented. But in most cases, it’s not feasible.</p>
<p>Contrastive Learning (CL) can avoid the annotation bottleneck through designing pretext tasks by utilizing intrinsic information instead of relying on pre-defined prior knowledge. Currently, CL methods for TS data has also advanced Meng et al. (2023b). These methods evaluate the effectiveness of TS embedding through follow-up classification, prediction, or clustering models, such as SVM Franceschi et al. (2019b). However, these simple and newly-trained models are considerably different from the complex and pre-trained LLM. The representation vector generated by unconstrained CL is likely to deviate greatly from the LLM’s cognitive embedding space.</p>
<p>To address the above issues, we propose an embedding method for TimE Series tokens to align the Text embedding space of LLM (TEST). Based on CL, TEST uses text embedding vectors as prototypes to constrain TS’ embedding space and highlights feature-wise patterns. We show that TEST can activate LLM’s ability as pattern machine. The contributions of this work are:</p>
<ul>
<li>Summarize two TS+LLM paradigms, LLM-for-TS, TS-for-LLM, with their potential methods;</li>
<li>Propose TEST for TS-for-LLM. TEST can produce the similarity-based, instance-wise, feature-wise, and text-prototype-aligned embedding for TS tokens. We prove that prompt tuning is almost equivalent to supervised fine-tuning when TS embedding and word embedding are aligned;</li>
<li>Experiments on TS classification, forecasting, few-shot, and representation tasks demonstrate that TEST can activate LLM’s capability to archive TS tasks, where the random and unsatisfactory results produced by original LLMs can be elevated to the baseline.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Means</th>
<th style="text-align: center;">Pros</th>
<th style="text-align: center;">Cons</th>
<th style="text-align: center;">Work</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LM-of-TS</td>
<td style="text-align: center;">Training</td>
<td style="text-align: center;">Specialized, accurate</td>
<td style="text-align: center;">Not universal, large datasets</td>
<td style="text-align: center;">Pre-training Ma et al. (2023) <br> Earth transformer Bi et al. (2023)</td>
</tr>
<tr>
<td style="text-align: center;">LLM-for-TS</td>
<td style="text-align: center;">Tuning</td>
<td style="text-align: center;">End-to-end, accurate</td>
<td style="text-align: center;">More experiments, lose language ability</td>
<td style="text-align: center;">GPT4TSZhou et al. (2023) <br> LLM4TSChang et al. (2023)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Tool augmented</td>
<td style="text-align: center;">Parameter-efficient, less experiments</td>
<td style="text-align: center;">Need experts, need annotation</td>
<td style="text-align: center;">PromptCast Xue \&amp; Salim (2023) <br> Health Learner Liu et al. (2023) <br> METS Li et al. (2024) <br> Text2ECGChung et al. (2023)</td>
</tr>
<tr>
<td style="text-align: center;">TS-for-LLM</td>
<td style="text-align: center;">External encoder</td>
<td style="text-align: center;">Parameter-efficient, multiple abilities</td>
<td style="text-align: center;">Weak robust</td>
<td style="text-align: center;">TEST</td>
</tr>
</tbody>
</table>
<p>Table 1: Existing Work about TS+LLM
As the name of TEST implies, it's a forward-looking test that we hope to lay the groundwork for future study. And it does give LLM new capabilities and highlight its qualities as a pattern machine.</p>
<h1>2 Related Work</h1>
<h3>2.1 Time Series and Large Language Model</h3>
<p>There hasn't been much research done on TS+LLM because this field is still in its infancy. We summarize the existing work in Table 1. LLM-for-TS with changing the model can be achieved through tuning or tool augmented means; TS-for-LLM with changing the data can be achieved through building the external encoder.</p>
<p>LM-of-TS Ma et al. (2023) trains a fundamental and accurate model based on accumulated domain TS data, but it can be difficult to construct a large well-labeled dataset due to data acquisition and annotation costs. By comparison, Supervised Fine-Tuning (SFT) in LLM-for-TS Chang et al. (2023) has a relatively smaller workload than pre-training, but it can make the LLM lose its language capabilities and its advantages over a sophisticated model designed specifically for TS tasks are unclear. Regarding TS as the text sequence and using prompts as the augmented tool Liu et al. (2023) could input numerical TS into LLM directly, but it is inaccurate, requires more experience, and will fail for multivariate TS. The multimodal methods Li et al. (2024) could align the text and TS, but apart from ECG, most TS datasets have no segment annotation.</p>
<h3>2.2 Time SeIres Embedding</h3>
<p>TS embedding can provide identities by including typical, associated, and dependant attributes. CL-based methods can get the data representation Chen et al. (2020), employing the instance discrimination pretext task to bring similar pairs closer while pushing dissimilar pairs apart in the embedding space. Some efforts have been made to implement instance-level contrast Woo et al. (2022b); Zheng et al. (2023), temporal-level contrast Meng et al. (2023c); Franceschi et al. (2019b), and clustering-level contrast Meng et al. (2023a) on TS data, with promising results. However, the direct contrast cannot bridge TS embedding and the LLM's comprehensible space. In our setting, we prefer to freeze the pre-trained LLM and let the embedding compromise. That is, we use the text token embedding in LLM to limit and guide the TS token embedding.</p>
<p>Inspired by the prototype-level contrast Caron et al. (2020a), which goes beyond the independence assumption and exploits latent cluster information present within samples. We can select some text embeddings as basic prototypes to lead the learning. However, in addition to the alignment, we still need to consider issues of prototype selection, differentiation Meng et al. (2023c), uniformity Wang \&amp; Isola (2020), stability Huang et al. (2023) and etc.</p>
<h2>3 MEthods</h2>
<p>TEST has two key steps: In Figure 1, build an encoder to embed TS; In Figure 2, create prompts to make the LLM can accept TS embeddings as input.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Text-prototype-aligned TS Embedding by Instance-wise and Feature-wise Contrast</p>
<h1>3.1 TS Token Augmentation and Encoding</h1>
<p>Definition 1 (Token Embedding of Time Series) A multivariate time series $x=\left{x_{t}^{d}\right}<em k="k">{t=1, d=1}^{T, D}$ has $D$ variables and $T$ time points. It can be segmented to a list of $K$ non-overlapping subsequences $s=\left{s</em>\right}<em s="s">{k=1}^{K}$ by a segmentation function $f</em>\right}}: x \rightarrow s$, where the length of $s_{k}=x_{t_{i}: t_{j}}$ is arbitrary, $1 \leq t_{i}&lt;t_{j} \leq T$. We call $s$ as the token list of time series $x$. Further, each token can be embeded to a $M$-dimensional representation space by an embedding function $f_{e}: s_{k} \in \mathbb{R}^{D \times T} \rightarrow e_{k} \in \mathbb{R}^{M}$. Finally, the token embedding list of $x$ is $e=\left{e_{k<em e="e">{k=1}^{K}=f</em>(x)\right)$.}(s)=f_{e}\left(f_{s</p>
<p>We first tokenize TS into some segmentation/subsequences/tokens/instances through the classical sliding window method in representation learning Yue et al. (2022) $s=f_{s}(x)$. We define a TS token $s$ as the anchor instance. Its positives $s^{+}$are the augmented instances, $s^{a o a k} \sim \mathcal{T}<em _strong="{strong" _text="\text">{\text {weak }}\left(\right.$ jitter-and-scale strategy, adding random variations to the signal and scale up its magnitude), $s^{s t r o n g} \sim$ $\mathcal{T}</em>$are from non-overlapping instances which do not have the same subsequence as $s$.}}\left(\right.$ permutation-and-jitter strategy, splitting the sequence into a random number of segments and randomly shuffling them) Eldele et al. (2021b). Its negatives $s^{-</p>
<p>After getting anchor-positive-negative, we built a neural network as the encoder to embed instance into vector $e=f_{e}(s)$. We also trained a decoder $f_{d}$ by using the auto-encoding loss $\mathcal{L}<em i="1">{a e}=\frac{1}{N} \sum</em>(e)\right)$ to ensure the representativeness of the embedding and subsequent verification. Because our primary goal is to retrieve the encoder, this decoder can likewise be unbuilt without harming the future process.}^{N} \operatorname{sim}\left(s, f_{d</p>
<h3>3.2 Instance-wise and Feature-wise Contrast</h3>
<p>The basic instance-wise CL treats each instance independently and design the instance discrimination pretext task to keep similar instances close and dissimilar instances far away. To prevent embedding space collapse, we treat augmented views of the same instance as the unique positive pair, and all remaining ones within the $B$ size minibatch as negative pairs He et al. (2020). The instance-wise contrastive loss is shown in Equation 1. Where given the instance embedding $e, e^{+ /-}$, we construct a projection head $f_{p}$, which is a one-layer MLP to obtain $f_{p}(e) . \sigma\left(e, e^{+ /-}\right)$ is used to calculate the similarity between two projected vectors through a similarity function sim like cosine similarity with the instance-level temperature parameter $\tau$.</p>
<p>$$
\begin{gathered}
\mathcal{L}<em i="1">{\text {ins }}=-\log \frac{\exp \left(\sigma\left(e, e^{+}\right)\right)}{\exp \left(\sigma\left(e, e^{+}\right)\right)+\sum</em> \
\sigma\left(e, e^{+/-}\right)=\frac{\operatorname{sim}\left(f_{p}(e), f_{p}\left(e^{+/-}\right)\right)}{\tau}
\end{gathered}
$$}^{B} \exp \left(\sigma\left(e, e_{i}^{-}\right)\right)</p>
<p>We also propose a feature-wise contrast method to break the independence between instances. As shown in Figure 1, after embedding, a feature matrix $\mathbb{R}^{B \times M}$ is formed by the representation vectors of instances in a minibatch. Where each row is an embedding of a instance, thus rows could be regarded as soft labels of instances which are used in Equation 1. In addition to rows, columns of feature matrix also have semantic information. Li et al. (2021c) proposed that the columns could be further regarded as cluster representations. However such cluster-wise methods require prior knowledge to pre-specify the number of clusters, which is non-trivial for the unlabeled TS data in this work. Thus, we propose to regard the columns as the soft labels of features and perform discrimination between groups of similar features.</p>
<p>For an anchor feature matrix m , where m is the $B$-th row copy of the vector $e$, we obtain a positive feature matrix $\mathrm{m}^{+}$and a negative feature matrix $\mathrm{m}^{-}$, where $\mathrm{m}^{+/-}=\left[e_{i}\right]_{i=1}^{B} \in \mathbb{R}^{B \times M}$. We mark the columns in the matrix as $m \in \mathrm{~m}^{\mathrm{T}}$. As expressed by the item before the right arrow in the Equation 2, the feature-wise contrast mainly align and differentiate the same feature column among the positive and negative. However, this may cause the representation space to shrink within a small area. We find that ensuring differences between features can better address this issue. That is, we suggest the contrast between different feature columns as shown in the item after the right arrow.</p>
<p>$$
\mathcal{L}<em i="1">{f e a}=-\sum</em>}^{M} \underbrace{\left(\sigma\left(m_{i}, m_{i}^{+}\right)\right.<em i="i">{\text {Alignment }}-\underbrace{\sigma\left(m</em>}, m_{i}^{-}\right)<em i="1">{\text {Difference }} \Rightarrow-\sum</em>
$$}^{M} \underbrace{\log \frac{\exp \left(\sigma\left(m_{i}, m_{i}^{+}\right)\right)}{\sum_{j=1}^{M}\left[\exp \left(\sigma\left(m_{i}, m_{j}^{+}\right)\right)+\exp \left(\sigma\left(m_{i}, m_{j}^{-}\right)\right)\right]}}_{\text {Feature category uniformity }</p>
<p>More importantly, the injection of feature column differences can also greatly assist in the subsequent implementation of text-prototype-aligned contrast. Because that contrast will apply the selected text token embedding to the feature columns, like coordinate axes.</p>
<h1>3.3 TEXT-PROTOTYPE-ALIGNED CONTRAST</h1>
<p>The pre-trained LLM has its own token embedding, e.g., small, medium, and big GPT-2 embed text tokens from word dictionaries into representation spaces with 768, 1024, and 1280 dimensions. Naively, we can align the token embedding of TS and text using the similarity estimation. Although TS tokens lack text annotation, we can place their embedding near typical text descriptions of TS, such as value, shape, and frequency. In this fashion, it is intuitively expected that various TS tokens can represent various descriptive terms such as small, big, up, down, stable, fluctuating, and so on. Naturally, the example above is based on the closest neighbor principle because the embedding space of a text token is discrete, akin to a vector table, but that of our TS token is continuous.</p>
<p>However, of course, the actual outcomes will not match what we expect because we are not providing the supervised label or ground truth. For example, the embedding of a subsequence with an upward trend may be very close to that of a decline word, or even that does not describe the trend. But it is irrelevant whether semantics can be understood by us. As usual, the fact is that humans cannot comprehend the model's perceptual mode.</p>
<p>Recently, researchers proved that LLMs are pattern machines <em>Mirchandani et al. (2023)</em>. Thus, in this work, we achieve "TS $\rightarrow$ pattern $\rightarrow$ text" to activate LLM's ability for TS tasks. The choice of text prototype can be relaxed, not necessarily the description related to TS.</p>
<p>In this work, we choose $P$ representative text embedding $t p$ as pivots/prototypes, and map TS embedding to them. In high dimensional space, almost all vectors are pairwise orthogonal <em>Hopcroft &amp; Kannan (2013)</em>, thus the number of prototypes rather than the type does matter, and their differences can be reflected in a single dimension/feature. Thus, the modeling function of the text prototype $t p$ is realized by feature-wise contrast. As expressed by Equation 3, the alignment term guarantees that the two space ranges are roughly the same through the similarity constraint, the contrast term uses $t p$ as the coordinate axis to map the TS embedding, making the representation values in text coordinate axes of similar instance similar. The feature matrix is no longer obtained through the projector but through the prototype mapping $e \cdot t p \rightarrow \mathrm{~m}$.</p>
<p>$$
\mathcal{L}<em i="1">{\text {text }}=-\sum</em>}^{P} \underbrace{\left[\operatorname{sim}\left(t p_{i}, e\right)\right.<em a="a" e="e" f="f">{\text {Text alignment }}-\underbrace{\mathcal{L}</em>
$$}\left(e \cdot t p, e^{+} \cdot t p, e^{-} \cdot t p\right)}_{\text {Text contrast }</p>
<h1>3.4 Learnable Prompt Embedding</h1>
<p>Even TS has been described using an embedded representation that the LLM can understand, LLM still has to be instructed on how to do subsequent TS tasks.</p>
<p>Prompt engineering like template and chain-of-thought is intuitive. Their contexts are coherent in human semantics, but a TS embedding list has no human semantics, it is more about a pattern sequence. Thus, to create a more consistent prompt pattern, we train a soft prompt by p-tuning Lester et al. (2021) make LLM be easier to understand the input. These soft prompts are task-specific embedding, learning through the loss from LLM's output and task ground truth in Equation 4.</p>
<p>$$
\mathcal{L}<em _reg="{reg" _text="\text">{\text {prompt }}=L</em>(p e, e))
$$} / \text { cls }}(\operatorname{concat</p>
<p>GPT4TS Zhou et al. (2023)has proved the feasibility that SFT can make LLM apply to TS. Based on this, we demonstrate the feasibility of TEST by proving the equivalence between soft prompt and SFT.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Framework of LLM for TS Tasks</p>
<p>Consider a conditional generation task where the input $x$ is a context and the output $y$ is a sequence of tokens. Assume an autoregression $\operatorname{LLM} p_{\phi}(y \mid x)$ with parameter $\phi, z=[x ; y]$. The inference of a pre-trained LLM is computing $h_{i}$ as a function of $z_{i}$ and the past activations in its left context, $Y=\mathcal{L M}<em i="i">{\phi}\left(z</em>}, h_{i}\right)$. The past $h_{i}$ in the soft prompt turning with prompt $p e_{\theta}$ is $h_{i}= \begin{cases}p e_{\theta}[i,:], &amp; \text { if } i \in p e_{\text {idx }} \ \mathcal{L} \mathcal{M<em i="i">{\phi}\left(z</em>$. The SFT from LLM to TS-LLM is Equation 5. Its transformation shows that the soft prompt tuning is approximately equivalent to SFT.}, h_{i}\right), &amp; \text { otherwise }\end{cases</p>
<p>$$
\begin{aligned}
\max <em _phi="\phi">{\phi} p</em> \mid x\right) &amp; =\max }\left(y^{\prime<em _in="\in" _mathrm_Y="\mathrm{Y" i="i">{\phi} \sum</em><em _phi="\phi">{\text {idx }}} \log p</em>}\left(z_{i}^{\prime} \mid h_{&lt;i}\right)=\sum_{i \in \mathrm{Y<em _phi_Delta="\phi+\Delta">{\text {idx }}} \log p</em>\right) \
&amp; \approx \sum_{i \in \mathrm{Y}}\left(z_{i}+\delta z_{i} \mid h_{&lt;i<em _phi="\phi">{\text {idx }}} \log p</em>\right) \
&amp; =\underbrace{\sum_{i \in \mathrm{Y}}\left(z_{i} \mid h_{&lt;i}\right) \cdot \sum_{i \in p e_{\text {idx }}} \log p_{\Delta}\left(\delta z_{i} \mid h_{&lt;i<em _phi="\phi">{\text {idx }}} \log p</em>}\left(z_{i} \mid \underbrace{f_{e}(s)<em LLM="LLM" _Progen="{Progen" _text="\text">{\text {Text-TS alignment }}}</em>}}) \cdot \underbrace{\sum_{i \in p e_{\text {idx }}} \log p_{\Delta}\left(\delta z_{i} \mid h_{&lt;i}\right)<em _theta="\theta">{\text {Prompt } p e</em>
\end{aligned}
$$}</p>
<p>Equation 5 also suggests that the projection space of TS tokens should preferably cover the complete set of text embedding space. Thus, we utilize clustering to find $P$ representative text prototypes. The process of using LLM to infer TS is shown in Figure 2. In this framework, the text data is input into the embedding layer of LLM, while the prompts and TS embeddings skip this layer.</p>
<h2>4 EXPERIMENTS</h2>
<p>The core of TEST is to train an encoder $f_{e}$ and a soft prompt $p e$ as described in Algorithm 1. The encoder must can extract relevant information from TS, needs to be time- and memory-efficient, and has to allow variable-length inputs. Thus, we build a causal TCN with 10 layers of convolution blocks. Each convolution block is a sequence of GELU, DilatedConv, BatchNorm, GELU, DilatedConv, with skip connections across each block. The DilatedConvs have dilation of $2 i$ in each layer $i$ of convolution block. A final convolution block is used to map the hidden channels to the output channel whose size is the same as the LLM's embedding size.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Algorithm 1 Training TEST</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1: for e in epochs do</td>
<td style="text-align: left;">9: for e in epochs do</td>
</tr>
<tr>
<td style="text-align: left;">2: // UPDATE ENCODER</td>
<td style="text-align: left;">10: // UPDATE PROMPT</td>
</tr>
<tr>
<td style="text-align: left;">3: $\theta_{f_{e}}=\theta_{f_{e}}-\eta \nabla_{\theta_{f_{e}}}\left(\mathcal{L}<em e="e" s="s" t="t">{i n s}+\mathcal{L}</em>\right)$</td>
<td style="text-align: left;">11: $p e=p e-\eta \nabla_{\theta_{p e}} \mathcal{L}_{p r o m p}$</td>
</tr>
<tr>
<td style="text-align: left;">4: // UPDATE DECODER (OPTIMAL)</td>
<td style="text-align: left;">12: // FINE TUNE DECODER (OPTIMAL)</td>
</tr>
<tr>
<td style="text-align: left;">5: $\theta_{f_{g}}=\theta_{f_{g}}-\eta \nabla_{\theta_{f_{g}}} \mathcal{L}_{a s}$</td>
<td style="text-align: left;">13: $\theta_{f_{d}}=\theta_{f_{d}}-\eta^{\top} \nabla_{\theta_{f_{d}}} \mathcal{L}_{r e g}$</td>
</tr>
<tr>
<td style="text-align: left;">6: // UPDATE PROJECTOR</td>
<td style="text-align: left;">14: // UPDATE CLASSIFIER (OPTIMAL)</td>
</tr>
<tr>
<td style="text-align: left;">7: $\theta_{f_{p}}=\theta_{f_{p}}-\eta \nabla_{\theta_{f_{p}}} \mathcal{L}_{i n s}$</td>
<td style="text-align: left;">15: $\theta_{f_{c}}=\theta_{f_{c}}-\eta \nabla_{\theta_{f_{c}}} \mathcal{L}_{c l s}$</td>
</tr>
<tr>
<td style="text-align: left;">8: end for</td>
<td style="text-align: left;">16: end for</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Size</th>
<th style="text-align: left;">Embed. dimension</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Bert Devlin et al. (2018)</td>
<td style="text-align: left;">110M, 335M</td>
<td style="text-align: left;">748, 1024</td>
</tr>
<tr>
<td style="text-align: left;">GPT2 Radford et al. (2019)</td>
<td style="text-align: left;">117M, 345M, 774M</td>
<td style="text-align: left;">768, 1024, 1280</td>
</tr>
<tr>
<td style="text-align: left;">ChatGLM Du et al. (2022)</td>
<td style="text-align: left;">6B</td>
<td style="text-align: left;">4096</td>
</tr>
<tr>
<td style="text-align: left;">LLaMa2 Touvron et al. (2023)</td>
<td style="text-align: left;">7B, 13B</td>
<td style="text-align: left;">4096</td>
</tr>
</tbody>
</table>
<p>Table 2: The Used Language Model
We compare our method to 5 kinds of methods including 12 baselines: 1) LLM-QA methods Xue \&amp; Salim (2023); Liu et al. (2023) with the classification template Classify the given [domain] sequence as either [class label] or [class label]: [numerical sequence]. [A] and the forecasting template [Q] Forecast the next value of the given [domain] sequence: [numerical sequence]. [A]; 2) SFT LLMfor-TS method GPT4TS Zhou et al. (2023); 3) classical TS models DWT, DWTD Bagnall et al. (2018), 1NNED, and TCN Tan et al. (2021); 4) SOTA TS models Informer Zhou et al. (2021), DLinear Zeng et al. (2023), and TimesNet Wu et al. (2023); 5) SOTA CL-based TS models Tloss Franceschi et al. (2019b), TS2Vec Yue et al. (2022), and CoST Woo et al. (2022a).</p>
<p>The overall results are shown in Figure 3 (The appendix has more compared classical SOTA models and detailed results about long-term, short-term, few-shot, and zero-shot forecasting, multivariate time series classification, and representation tasks.). Overall, after using TEST, when the size of LLM reaches about 300M, their accuracy comparable to SOTA model.</p>
<h1>4.1 CLASSIFICATION</h1>
<p>We present accuracy scores for all 128 kinds of univariate TS datasets in UCR archive Dau et al. (2019) and all 30 kinds of multivariate TS datasets in UEA archive Bagnall et al. (2018).</p>
<p>Accuracy. In Figure 3 (a-b), TEST makes the classification accuracy of LLM increase significantly. LLM's original classification performances are demonstrated through two QA results. It almost guesses the classification labels at random, especially for multivariate TS. After using TEST, GPT2774 M , which has the median accuracy among all models, can improve accuracy by at least $18 \%$ for univariate TS and $25 \%$ for multivariate TS. TEST makes most LLMs comparable to, if not better than, the existing models. When the size reaches about 300M, the accuracy can exceed TS baselines; When the size reaches about 700M, the accuracy can exceed SOTA TS transformers.</p>
<p>Ablation. In Figure 3 (c-d), different text prototypes will lead to different results. We set 3 groups of text prototypes: embeddings of value, shape, frequency, and embeddings of 3 or 10 cluster centers. Choosing a prototype group that more accurately represents LLM's entire text embedding space can improve the performance. This is also suggested by Equation 5. Different prompt types, initialization, and length will lead to different results. We compare the soft prompt with the hard prompt of Classify the given [domain] sequence as either [class label] or [class label]: [TS embedding]. The accuracy differs by at least $10 \%$. We set random initialization from uniform distribution and task description initialization from Classify the given sequence. The latter makes the training converge faster. When the model reaches 1B, a prompt length of 10 can achieve excellent results.</p>
<h3>4.2 FORECASTING</h3>
<p>We present short-forecasting MSE scores for all 19 kinds of varied time series datasets in TSER archive Tan et al. (2021), and long-forecasting MSE scores for 8 popular real-world benchmark datasets including weather, traffic, electricity, ILI, and ETT from Wu et al. (2023).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Experiment Results. (a-d) shows the classification results; (e-h) shows the forecasting results; (i) shows the representation results. The red dashed line represents the best result.</p>
<p>Accuracy. In Figure 3 (e-f), TEST makes the forecasting accuracy of LLM increase significantly and comparable to SOTA models. When the size reaches about 300M, the accuracy can exceed SOTA TS transformers.</p>
<p>Generalization. We fuse 19 datasets into 1 dataset and test the method on this fused dataset. As shown in Figure 3 (g), compared with baselines, LLM-based models have better generality.</p>
<p>Few-shot. LLM has demonstrated remarkable performance in few-shot learning. Based on the settings in Zhou et al. (2023), we present few-shot forecasting for 10\% time steps in training datasets. As shown in Figure 3 (h), TEST achieves the best performance and demonstrates a relative average MSE reduction of $23.5 \%$.</p>
<h1>4.3 REPRESENTATION</h1>
<p>Representation learning. Learning universal representations for TS is a fundamental but challenging problem. Both TEST's first step (creating TS embedding) and second step (LLM's output) can achieve this task. Based on the classical representation learning task, we evaluated the effectiveness of TEST representation using SVM classifier on UCR dataset. Note that using a simple classifier can better reflect the presentation effect. In Figure 3 (i), the embedding in TEST's first step is comparable to SOTA representation methods, and the embedding in TEST's second step can outperform them. This indicates that after using LLM, the representation of TS becomes more discriminative.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Matching TS Embedding to Words</p>
<p>Case. We use nearest neighbor method to find the text that a TS token matches to in the word embedding space of frozen LLM. In Figure 4, the majority of the identified words are sentimentrelated adjectives and nouns. We speculate that by prompting, the model will treat TS classification task as an sentiment classification task. Thus, introducing prompt is like introducing a shortcut for LLM. Besides, the matched words are like a kind of textual Shapelet for TS segmentation, representing TS through a series of patterns. Instead of regarding TS as a sequence of numbers, we suggest using words to identify patterns in TS as LLMs without SFT are not good for math when performing digital tasks, but they are good at extracting knowledge as a pattern machine. The semantics of the patterns be perplexing to us, but it makes sense to LLM.</p>
<h2>5 DISCUSSION AND CONCLUSION</h2>
<p>This paper proposes an instance-wise, feature-wise, and text-prototype-aligned TS embedding method to achieve TS-for-LLM. It can activate LLM's ability for TS tasks while maintaining its original language ability. Experiments on classification, forecasting, and representation tasks show that using TEST, LLM can archive comparable performance to SOTA methods.</p>
<p>TS-for-LLM can enrich LLM's capabilities. SFT LLM may be more effective than TS-for-LLM, yet its superiority over customized TS models remains unclear; Training customized models may be more accurate in TS tasks, yet TS-for-LLM offers all notable benefits of LLM additionally.</p>
<p>TS-for-LLM can explore LLM's mechanism as a pattern machine. The essence of TS-for-LLM is: TS $\leftrightarrow$ TS embeddings $\leftrightarrow$ patterns $\leftrightarrow$ text/word embedding $\leftrightarrow$ text. Although TEST gives the impression of a forcibly aligning operations between TS and text, it dose convert TS into an understandable pattern sequence for LLMs, that clearly demonstrates that the essence of LLM is pattern recognition. In fact, TS is objective data, whereas images, text, and speech are subjective data that can be perceived by human senses. TEST aligns objective TS data and subjective text data at the machine level, but how to align them at the human perception level requires future research.</p>
<p>Meanwhile, in addition to text prototypes and prompts, LLM size and type also affect the results. The impact of model type is intuitive, it is related to downstream tasks, where the bidirectional structure is beneficial for classification, and the generated structure is beneficial for forecasting. The impact of model size, where a larger model produces more accurate results, can be attributed to various reasons. Aside from the impact of additional parameters, we believe that the datasets used in the pre-training process are also important, with the size, diversity, and corpus type all having an impact. We conjecture that more training data will provide the model with more opportunities to learn temporal patterns. As a result, we intend to conduct more experiments to investigate deeper correlations between corpora and TS data Chen et al. (2023).</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This work is supported by National Natural Science Foundation of China (No.62172018, No.62102008) and Wuhan East Lake High-Tech Development Zone National Comprehensive Experimental Base for Governance of Intelligent Society.</p>
<h1>REFERENCES</h1>
<p>Anthony J. Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul Southam, and Eamonn J. Keogh. The UEA multivariate time series classification archive, 2018. CoRR, abs/1811.00075, 2018.</p>
<p>Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Accurate mediumrange global weather forecasting with 3d neural networks. Nature, pp. 1476-4687, 2023. doi: $10.1038 / \mathrm{s} 41586-023-06545-\mathrm{z}$.</p>
<p>Aaron Bostrom, Anthony Bagnall, Eamonn Keogh, Hoang Anh Dau, James Large, Jason Lines, Michael Flynn, and Paul Southam. The uea multivariate time series classification archive, 2018, 2018.</p>
<p>Eoin Brophy, Zhengwei Wang, Qi She, and Tomás Ward. Generative adversarial networks in time series: A systematic literature review. ACM Comput. Surv., 55(10):199:1-199:31, 2023.</p>
<p>Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems, 2020a.</p>
<p>Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. 2020b.</p>
<p>CDC. Illness. 2021. doi: https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html.
Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. LLM4TS: two-stage fine-tuning for time-series forecasting with pre-trained llms. CoRR, abs/2308.08469, 2023.</p>
<p>Daoyuan Chen, Yilun Huang, and et al. Data-juicer: A one-stop data processing system for large language models. CoRR, abs/2309.0203, 2023.</p>
<p>Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of International Conference on Machine Learning, volume 119, pp. 1597-1607, 2020.</p>
<p>Hyunseung Chung, Jiho Kim, Joon-Myoung Kwon, Ki-Hyun Jeon, Min Sung Lee, and Edward Choi. Text-to-ecg: 12-lead electrocardiogram synthesis conditioned on clinical text reports. In IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 1-5, 2023.</p>
<p>Hoang Anh Dau, Anthony Bagnall, Kaveh Kamgar, Chin-Chia Michael Yeh, Yan Zhu, Shaghayegh Gharghabi, Chotirat Ann Ratanamahatana, and Eamonn Keogh. The ucr time series archive. IEEE/CAA Journal of Automatica Sinica, 6:1293-1305, 2019. doi: 10.1109/JAS.2019.1911747.</p>
<p>Angus Dempster, Daniel F. Schmidt, and Geoffrey I. Webb. Minirocket: A very fast (almost) deterministic transform for time series classification. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 248-257, 2021. doi: 10.1145/3447548.3467231.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.</p>
<p>Jiaxiang Dong, Haixu Wu, Haoran Zhang, Li Zhang, Jianmin Wang, and Mingsheng Long. Simmtm: A simple pre-training framework for masked time-series modeling. CoRR, abs/2302.00861, 2023.</p>
<p>Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of Annual Meeting of the Association for Computational Linguistics, volume 1, pp. 320-335, 2022.</p>
<p>Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li, and Cuntai Guan. Time-series representation learning via temporal and contextual contrasting. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, pp. 23522359, 2021a.</p>
<p>Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li, and Cuntai Guan. Time-series representation learning via temporal and contextual contrasting. In International Joint Conference on Artificial Intelligence, pp. 2352-2359, 2021b.</p>
<p>Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable representation learning for multivariate time series. In Advances in Neural Information Processing Systems, pp. $4652-4663,2019 a$.</p>
<p>Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable representation learning for multivariate time series. In Advances in Neural Information Processing Systems, pp. $4652-4663,2019 b$.</p>
<p>Ge Gao, Qitong Gao, Xi Yang, Miroslav Pajic, and Min Chi. A reinforcement learning-informed pattern mining framework for multivariate time series classification. In Proceedings of International Joint Conference on Artificial Intelligence, pp. 2994-3000, 2022. doi: 10.24963/IJCAI.2022/415.</p>
<p>Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Ávila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent - A new approach to self-supervised learning. In Advances in Neural Information Processing Systems, 2020.</p>
<p>Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zeroshot time series forecasters. CoRR, abs/2310.07820, 2023. doi: 10.48550/ARXIV.2310.07820.</p>
<p>Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for unsupervised visual representation learning. In Computer Vision and Pattern Recognition, pp. $9726-9735,2020$.</p>
<p>Shenda Hong, Hongyan Li, Chenxi Sun, and Junyuan Shang. Research and applications of extracting computational phenotype from vital sign time series. China Seience and Technology Achivements, 10, 2023. doi: 10.3772/j.issn.1009-5659.223.10.002.</p>
<p>John Hopcroft and Ravindran Kannan. Computer science theory for the information age. Cambridge University press, 2013.</p>
<p>Zhizhong Huang, Jie Chen, Junping Zhang, and Hongming Shan. Learning representation for clustering via prototype scattering and positive sampling. IEEE Trans. Pattern Anal. Mach. Intell., 45 (6):7509-7524, 2023. doi: 10.1109/TPAMI.2022.3216454.</p>
<p>Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-llm: Time series forecasting by reprogramming large language models. CoRR, abs/2310.01728, 2023. doi: 10.48550/ARXIV. 2310.01728 .</p>
<p>Fazle Karim, Somshubra Majumdar, Houshang Darabi, and Samuel Harford. Multivariate lstm-fcns for time series classification. Neural Networks, 116:237-245, 2019. doi: 10.1016/J.NEUNET. 2019.04.014.</p>
<p>Salar Hosseini Khorasgani, Yuxuan Chen, and Florian Shkurti. SLIC: self-supervised learning with iterative clustering for human action videos. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16070-16080, 2022. doi: 10.1109/CVPR52688.2022.01562.</p>
<p>Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of Conference on Empirical Methods in Natural Language Processing, pp. 3045-3059, 2021. doi: 10.18653/v1/2021.emnlp-main.243.</p>
<p>Guozhong Li, Byron Choi, Jianliang Xu, Sourav S. Bhowmick, Kwok-Pan Chun, and Grace LaiHung Wong. Shapenet: A shapelet-neural network approach for multivariate time series classification. In AAAI Conference on Artificial Intelligence, pp. 8375-8383, 2021a. doi: 10.1609/AAAI.V35I9.17018.</p>
<p>Jun Li, Che Liu, Sibo Cheng, Rossella Arcucci, and Shenda Hong. Frozen language model helps ecg zero-shot learning. In Medical Imaging with Deep Learning, pp. 402-415, 2024.</p>
<p>Junnan Li, Pan Zhou, Caiming Xiong, and Steven C. H. Hoi. Prototypical contrastive learning of unsupervised representations. In International Conference on Learning Representations, 2021b.</p>
<p>Yunfan Li, Peng Hu, Jerry Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, and Xi Peng. Contrastive clustering. In AAAI Conference on Artificial Intelligence,, pp. 8547-8555, 2021c.</p>
<p>Xin Liu, Daniel McDuff, Geza Kovacs, Isaac R. Galatzer-Levy, Jacob E. Sunshine, Jiening Zhan, Ming-Zher Poh, Shun Liao, Paolo Di Achille, and Shwetak N. Patel. Large language models are few-shot health learners. CoRR, abs/2305.15525, 2023. doi: 10.48550/arXiv.2305.15525.</p>
<p>Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Exploring the stationarity in time series forecasting. In Advances in Neural Information Processing Systems, 2022.</p>
<p>Qianli Ma, Zhen Liu, Zhenjing Zheng, Ziyang Huang, Siying Zhu, Zhongzhong Yu, and James T. Kwok. A survey on time-series pre-trained models. CoRR, abs/2305.10716, 2023. doi: 10.48550/ arXiv. 2305.10716 .</p>
<p>Qianwen Meng, Hangwei Qian, Yong Liu, Yonghui Xu, Zhiqi Shen, and Lizhen Cui. MHCCL: masked hierarchical cluster-wise contrastive learning for multivariate time series. CoRR, abs/2212.01141, 2022.</p>
<p>Qianwen Meng, Hangwei Qian, Yong Liu, Lizhen Cui, Yonghui Xu, and Zhiqi Shen. MHCCL: masked hierarchical cluster-wise contrastive learning for multivariate time series. In AAAI Conference on Artificial Intelligence, pp. 9153-9161, 2023a. doi: 10.1609/aaai.v37i8.26098.</p>
<p>Qianwen Meng, Hangwei Qian, Yong Liu, Yonghui Xu, Zhiqi Shen, and Lizhen Cui. Unsupervised representation learning for time series: A review. CoRR, abs/2308.01578, 2023b. doi: 10.48550/ arXiv. 2308.01578 .</p>
<p>Qianwen Meng, Hangwei Qian, Yong Liu, Yonghui Xu, Zhiqi Shen, and Lizhen Cui. Unsupervised representation learning for time series: A review. CoRR, abs/2308.01578, 2023c.</p>
<p>Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern machines. In Conference on Robot Learning, volume 229 of Proceedings of Machine Learning Research, pp. 2498-2518, 2023.</p>
<p>Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In International Conference on Learning Representations, 2023.</p>
<p>Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS: neural basis expansion analysis for interpretable time series forecasting. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.</p>
<p>PeMS. Traffic. 2021. doi: http://pems.dot.ca.gov/.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI, 2019.</p>
<p>Patrick Schäfer and Ulf Leser. Multivariate time series classification with WEASEL+MUSE. CoRR, abs/1711.11343, 2017.</p>
<p>Vivek Sharma, Makarand Tapaswi, M. Saquib Sarfraz, and Rainer Stiefelhagen. Clustering based contrastive learning for improving face representations. In IEEE International Conference on Automatic Face and Gesture Recognition, pp. 109-116, 2020. doi: 10.1109/FG47880.2020.00011.</p>
<p>Taylor SJ and Letham B. Forecasting at scale. In PeerJ Preprints, pp. 5:e3190v2, 2017. doi: 10.7287/peerj.preprints.3190v2.</p>
<p>Chenxi Sun, Shenda Hong, and et al. A review of deep learning methods for irregularly sampled medical time series data. CoRR, abs/2010.12493, 2020. doi: 10.48550/arXiv.2010.12493.</p>
<p>Chang Wei Tan, Christoph Bergmeir, Francois Petitjean, and Geoffrey I Webb. Time series extrinsic regression. Data Mining and Knowledge Discovery, pp. 1-29, 2021. doi: https://doi.org/10.1007/ s10618-021-00745-9.</p>
<p>Sana Tonekaboni, Danny Eytan, and Anna Goldenberg. Unsupervised representation learning for time series with temporal neighborhood coding. In International Conference on Learning Representations, 2021.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, and et al. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023.</p>
<p>Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. CoRR, abs/1807.03748, 2018.</p>
<p>Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In Proceedings of International Conference on Machine Learning, volume 119, pp. 9929-9939, 2020.</p>
<p>Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang, Yonghong Tian, and Wen Gao. Large-scale multi-modal pre-trained models: A comprehensive survey. Mach. Intell. Res., 20(4):447-482, 2023. doi: 10.1007/s11633-022-1410-8.</p>
<p>Wetterstation. Weather. 2017. doi: https://www.bgc-jena.mpg.de/wetter/.
Kristoffer Wickstrøm, Michael Kampffmeyer, Karl Øyvind Mikalsen, and Robert Jenssen. Mixing up contrastive learning: Self-supervised representation learning for time series. Pattern Recognit. Lett., 155:54-61, 2022.</p>
<p>Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven C. H. Hoi. Cost: Contrastive learning of disentangled seasonal-trend representations for time series forecasting. In International Conference on Learning Representations, 2022a.</p>
<p>Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven C. H. Hoi. Cost: Contrastive learning of disentangled seasonal-trend representations for time series forecasting. In The International Conference on Learning Representations, 2022b.</p>
<p>Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven C. H. Hoi. Etsformer: Exponential smoothing transformers for time-series forecasting. CoRR, abs/2202.01381, 2022c.</p>
<p>Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. In Advances in Neural Information Processing Systems, pp. 22419-22430, 2021.</p>
<p>Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In International Conference on Learning Representations, 2023.</p>
<p>Hao Xue and Flora D. Salim. Promptcast: A new prompt-based learning paradigm for time series forecasting. CoRR, abs/2210.08964, 2023.</p>
<p>Ling Yang and Shenda Hong. Unsupervised time-series representation learning with iterative bilinear temporal-spectral fusion. In International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 25038-25054, 2022.</p>
<p>Xinyu Yang, Zhenguo Zhang, and Rongyi Cui. Timeclr: A self-supervised contrastive learning framework for univariate time series representation. Knowl. Based Syst., 245:108606, 2022.</p>
<p>Jinsung Yoon, Daniel Jarrett, and Mihaela van der Schaar. Time-series generative adversarial networks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 5509-5519, 2019.</p>
<p>Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and Bixiong Xu. Ts2vec: Towards universal representation of time series. In AAAI Conference on Artificial Intelligence, pp. 8980-8987, 2022.</p>
<p>Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In AAAI Conference on Artificial Intelligence, pp. 11121-11128, 2023. doi: 10. 1609/aaai.v37i9.26317.</p>
<p>George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff. A transformer-based framework for multivariate time series representation learning. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 2114-2124, 2021. doi: $10.1145 / 3447548.3467401$.</p>
<p>Dejiao Zhang, Feng Nan, Xiaokai Wei, Shang-Wen Li, Henghui Zhu, Kathleen R. McKeown, Ramesh Nallapati, Andrew O. Arnold, and Bing Xiang. Supporting clustering with contrastive learning. In Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5419-5430, 2021. doi: 10.18653/V1/2021.NAACL-MAIN. 427.</p>
<p>Xuchao Zhang, Yifeng Gao, Jessica Lin, and Chang-Tien Lu. Tapnet: Multivariate time series classification with attentional prototypical network. In AAAI Conference on Artificial Intelligence, pp. 6845-6852, 2020. doi: 10.1609/AAAI.V34I04.6165.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and JiRong Wen. A survey of large language models. CoRR, abs/2303.18223, 2023. doi: 10.48550/ arXiv. 2303.18223.</p>
<p>Xiaochen Zheng, Xingyu Chen, Manuel Schürch, Amina Mollaysa, Ahmed Allam, and Michael Krauthammer. Simts: Rethinking contrastive representation learning for time series forecasting. CoRR, abs/2303.18205, 2023.</p>
<p>Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In AAAI Conference on Artificial Intelligence, pp. 11106-11115, 2021. doi: 10.1609/aaai.v35i12.17325.</p>
<p>Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 2726827286, 2022.</p>
<p>Tian Zhou, PeiSong Niu, Xue Wang, Liang Sun, and Rong Jin. One fits all:power general time series analysis by pretrained lm. In Conference and Workshop on Neural Information Processing Systems, 2023.</p>
<p>Rundong Zuo, Guozhong Li, Byron Choi, Sourav S. Bhowmick, Daphne Ngar-yin Mah, and Grace Lai-Hung Wong. SVP-T: A shape-level variable-position transformer for multivariate time series classification. In AAAI Conference on Artificial Intelligence, pp. 11497-11505, 2023. doi: 10.1609/AAAI.V37I9.26359.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 Related Work</h2>
<p>Our work mainly involves two research fields: Universal Representation Learning (URL) for time series based on Contrastive Learning (CL) and Large Language Model (LLM) + Time Series (TS).</p>
<h2>A.1.1 CL-BASED URL FOR TS</h2>
<p>Unsupervised URL approaches aim to learn discriminative feature representations from unlabeled data, without the requirement of annotating every sample. Enabling URL is extremely crucial for time series data, due to its unique annotation bottleneck caused by its complex characteristics and lack of visual cues compared with other data modalities.</p>
<p>Contrastive methods learn meaningful representations from time series by optimizing selfdiscrimination tasks. Instead of directly modeling the complex raw data, they employ pretext tasks that leverage the underlying similarity between samples, which eliminates the need for reconstructing the complete input and allows for the discovery of contextualized underlying factors of variations. Contrastive methods typically generate augmented views of the raw data through various transformations and then learn representations by contrasting positive samples against negative samples. The existing CL-based URL for TS are listed in Table 4.</p>
<p>Instance-level contrastive models treat individual samples independently for the purpose of instance discrimination. They utilize data augmentations to transform original inputs into a new embedding space. Within this space, augmentations derived from the same sample are considered as positive pairs, while those from different samples are treated as negative pairs. During training, these models are optimized by maximizing the similarity between representations of positive pairs, while simultaneously minimizing the similarity between representations of negative pairs.</p>
<p>Prototype-level contrastive models break the independence between samples and explore to exploit the implicit semantics shared by samples in the same cluster. They can address the limitation that instance-level contrastive learning models tend to treat semantically similar samples as negatives.</p>
<p>Temporal-level contrastive models instead focus on capturing scale- invariant representations at each individual timestamp. By cosidering both instance-level and temporal-level representation learning strategies, researchers aim to enhance the capability of contrastive learning methods in capturing the complexities inherent in time series data.</p>
<h2>A.1.2 LLM+TS</h2>
<p>Large models, specifically referred to as large language models (LLMs) and pre-trained foundation models (PFMs), have witnessed remarkable success across a multitude of tasks and domains, such as natural language processing (NLP), computer vision (CV). Given the remarkable achievements of large models in these diverse fields, an intriguing question emerges: can large models be effectively employed to analyze TS data?</p>
<p>TS data has long been studied and proven to be indispensable in a myriad of real-world applications, encompassing fields such as geoscience, transportation, energy, healthcare, environment, and</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: left;">Pros</th>
<th style="text-align: left;">Cons</th>
<th style="text-align: left;">Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Reconstruction-based</td>
<td style="text-align: left;">Disregard insignificant data <br> that may contain noise</td>
<td style="text-align: left;">Collapse of embedding space; <br> Unable to measure feature relations</td>
<td style="text-align: left;">TimeNetWa et al. (2023) <br> SimMTM Dong et al. (2023)</td>
</tr>
<tr>
<td style="text-align: left;">Adversarial</td>
<td style="text-align: left;">Eliminate the need for expensive <br> manual labeling</td>
<td style="text-align: left;">Difficulty in model convergence; <br> Unable to measure feature relations</td>
<td style="text-align: left;">TimeGAN Yoon et al. (2019) <br> TS-GAN Brophy et al. (2023)</td>
</tr>
<tr>
<td style="text-align: left;">Predicative</td>
<td style="text-align: left;">Self-supervised</td>
<td style="text-align: left;">Affected by noise</td>
<td style="text-align: left;">TST Zerveas et al. (2021) <br> TS-TCCEldele et al. (2021a)</td>
</tr>
<tr>
<td style="text-align: left;">Contrastive</td>
<td style="text-align: left;">Self-supervised</td>
<td style="text-align: left;">Different datasets require different <br> data augmentation methods and <br> similarity evaluations</td>
<td style="text-align: left;">Table 4</td>
</tr>
</tbody>
</table>
<p>Table 3: Representation Learning Methods of Time Series Methods</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Methods</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Instance-level</td>
<td>SimCLR <em>Chen et al. (2020)</em></td>
<td>TimeCLR <em>Yang et al. (2022)</em></td>
<td>MoCo <em>He et al. (2020)</em></td>
<td>BYOL <em>Grill et al. (2020)</em></td>
</tr>
<tr>
<td></td>
<td>CPC <em>van den Oord et al. (2018)</em></td>
<td>SimSiam <em>Zheng et al. (2023)</em></td>
<td>MCL <em>Wickstrom et al. (2022)</em></td>
<td></td>
</tr>
<tr>
<td>Prototype-level</td>
<td>SwAV <em>Caron et al. (2020b)</em></td>
<td>PCL <em>Li et al. (2021b)</em></td>
<td>CCL <em>Sharma et al. (2020)</em></td>
<td>SCCL <em>Zhang et al. (2021)</em></td>
</tr>
<tr>
<td></td>
<td>CC <em>Li et al. (2021c)</em></td>
<td>SLIC <em>Khorasgani et al. (2022)</em></td>
<td>MHCCL <em>Meng et al. (2022)</em></td>
<td></td>
</tr>
<tr>
<td>Temporal-level</td>
<td>TS2Vec <em>Yue et al. (2022)</em></td>
<td>TS-TCC <em>Eldele et al. (2021b)</em></td>
<td>TNC <em>Tonekaboni et al. (2021)</em></td>
<td>TCL</td>
</tr>
<tr>
<td></td>
<td>T-Loss <em>Franceschi et al. (2019b)</em></td>
<td>BTSF <em>Yang &amp; Hong (2022)</em></td>
<td>CoST <em>Woo et al. (2022a)</em></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 4: Contrastive Learning based Universal Representation Methods for Time Series</p>
<table>
<thead>
<tr>
<th>Means</th>
<th>Pros</th>
<th>Cons</th>
<th>Work</th>
</tr>
</thead>
<tbody>
<tr>
<td>Training</td>
<td>Specialized, accurate</td>
<td>Not universal, large datasets</td>
<td>Pre-training <em>Ma et al. (2023)</em></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Earth transformer <em>Bi et al. (2023)</em></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>TS Transformers <em>Wu et al. (2023)</em></td>
</tr>
<tr>
<td>Tuning</td>
<td>End-to-end, accurate</td>
<td>More experiments, lose language ability</td>
<td>GPT4TS<em>Zhou et al. (2023)</em></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>LLM4TS<em>Chang et al. (2023)</em></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>LLMTime <em>Gruver et al. (2023)</em></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Time-LLM <em>Jin et al. (2023)</em></td>
</tr>
<tr>
<td>Tool Augmented</td>
<td>Parameter-efficient, less experiments</td>
<td>Need experts, need annotation</td>
<td>PromptCast <em>Xue &amp; Salim (2023)</em></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Health Learner <em>Liu et al. (2023)</em></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>METS <em>Li et al. (2024)</em></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Texi2ECG<em>Chung et al. (2023)</em></td>
</tr>
<tr>
<td>External Encoder</td>
<td>Parameter-efficient, multiple abilities</td>
<td>Weak robust</td>
<td>TEST</td>
</tr>
</tbody>
</table>
<p>Table 5: Existing Work about TS+LLM</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Technical Route of LLM+TS</p>
<p>Finance. While large models have made significant progress in various fields, the arena of time series analysis has followed a more gradual path. Traditional analytical methods have predominantly relied on statistical models. The advent of deep learning has galvanized the research community to explore more potent data-driven models, typically built on the basis of Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), and Transformers. Nonetheless, the majority of these models remain relatively small in scale and are tailored for specific tasks, thereby lacking the capacity to acquire comprehensive semantic and knowledge representations from large-scale data for multi-task reasoning.</p>
<p>There hasn’t been much research done on TS+LLM because this field is still in its infancy. We summarize the existing work in Table 5. Different from the main text, we category work here through technical means.</p>
<p>A. 2 MODEL https://github.com/SCXsunchenxi/TEST</p>
<h1>A.2.1 ENCODER</h1>
<p>The core of TEST is to train an encoder and a soft prompt. The encoder must can extract relevant information from TS, needs to be time- and memory-efficient, and has to allow variable-length inputs. Thus, as shown in Figure 6, we build a causal TCN with 10 layers of convolution blocks. Each convolution block is a sequence of GELU, DilatedConv, BatchNorm, GELU, DilatedConv, with skip connections across each block. The DilatedConvs have dilation of $2 i$ in each layer $i$ of convolution block. A final convolution block is used to map the hidden channels to the output channel whose size is the same as the LLM's embedding size.</p>
<p>The detailed architecture is: Number of channels in the intermediary layers of the causal network is 40 ; Number of layers (depth of the causal network) is 10 ; Kernel size of all convolutions is 3; Negative slope of the leaky ReLU activation is 0.01 ; Number of output channels of the causal network (before max pooling) is 640 ; Dimension of the representations is the same as the LLM's embedding size (e.g. 1024 for gpt 2 ).
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Illustration of Three Stacked Dilated Causal Convolutions and Composition of the i-th Layer of The Chosen Architecture</p>
<p>We train our models with the following parameters for time series classification. Note that no hyperparameter optimization was performed on the encoder hyperparameters: Optimizer is Adam with learning rate $\alpha=0.001$ and decay rates $\beta=(0.9,0.999)$; Number of negative samples is $K \in{1,2,5,10}$ for for univariate time series, $K \in{5,10,20}$ for multivariate ones; Batch size is 10; Number of optimizations steps is 2000 for $K \leq 10$ (i.e., 20 epochs for a dataset of size 1000), 1500 otherwise.</p>
<h2>A.2.2 LLM</h2>
<p>The used LLMs are as listed in Table 6. Each encoder and soft prompt of LLM are trained using the Adam optimizer on 20 NVIDIA Tesla V100-SXM2 GPU with CUDA 11.3.</p>
<h2>A. 3 FORECASTING TASKS</h2>
<p>All the deep learning networks are implemented in PyTorch and trained on NVIDIA V100 32GB GPUs. We use mean square error (MSE) and mean absolute error (MAE) as metrics. For zeroshot learning, mean absolute percentage error (MAPE) is used for TOURISM; symmetric MAPE (sMAPE) is used for M3 and M4; normalized deviation (ND) is used for ELECTR. All experiments are repeated 3 times and the mean of the metrics is used in the final results.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Size</th>
<th style="text-align: center;">Embed. dimension</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Bert Devlin et al. (2018)</td>
<td style="text-align: left;">110M, 335M</td>
<td style="text-align: center;">748,1024</td>
</tr>
<tr>
<td style="text-align: left;">GPT2 Radford et al. (2019)</td>
<td style="text-align: left;">117M, 345M, 774M</td>
<td style="text-align: center;">768, 1024, 1280</td>
</tr>
<tr>
<td style="text-align: left;">ChatGLM Du et al. (2022)</td>
<td style="text-align: left;">6B</td>
<td style="text-align: center;">4096</td>
</tr>
<tr>
<td style="text-align: left;">LLaMa2 Touvron et al. (2023)</td>
<td style="text-align: left;">7B, 13B</td>
<td style="text-align: center;">4096</td>
</tr>
</tbody>
</table>
<p>Table 6: The Used Language Model</p>
<h1>A.3.1 DATASET DETAILS</h1>
<p>The details of long-term forecasting and few-shot forecasting datasets are: ETT datasets Zhou et al. (2021) contain electricity load of various resolutions (ETTh \&amp; ETTm) from two electricity stations; Weather datasetWetterstation (2017) contains 21 meteorological indicators of Germany within 1 year; Illness datasetCDC (2021) contains the influenza-like illness patients in the United States. ILI is not used for few-shot learning for the limited quantity that is hard to follow the definition of few-shot; Electricity dataset SJ \&amp; B (2017) contains the electricity consumption; Traffic dataset PeMS (2021) contains the occupation rate of freeway system across the State of California. Table 7 summarizes details of feature statistics.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Length</th>
<th style="text-align: center;">Dimension</th>
<th style="text-align: center;">Frequency</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ETTh</td>
<td style="text-align: left;">17420</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1 hour</td>
</tr>
<tr>
<td style="text-align: left;">ETTm</td>
<td style="text-align: left;">69680</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">15 min</td>
</tr>
<tr>
<td style="text-align: left;">Weather</td>
<td style="text-align: left;">52696</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">10 min</td>
</tr>
<tr>
<td style="text-align: left;">ILI</td>
<td style="text-align: left;">966</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">7 days</td>
</tr>
<tr>
<td style="text-align: left;">Electricity</td>
<td style="text-align: left;">26304</td>
<td style="text-align: center;">321</td>
<td style="text-align: center;">1 hour</td>
</tr>
<tr>
<td style="text-align: left;">Traffic</td>
<td style="text-align: left;">17544</td>
<td style="text-align: center;">862</td>
<td style="text-align: center;">1 hour</td>
</tr>
</tbody>
</table>
<p>Table 7: Long-term Forecasting and Few-shot Forecasting Dataset Details</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Mapping</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Length</td>
<td style="text-align: center;">Horizon</td>
<td style="text-align: center;">M4</td>
<td style="text-align: center;">M3</td>
</tr>
<tr>
<td style="text-align: center;">M3 Yearly</td>
<td style="text-align: center;">645</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Yearly</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">M3 Quarterly</td>
<td style="text-align: center;">756</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Quarterly</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">M3 Monthly</td>
<td style="text-align: center;">1428</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">Monthly</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">M3 Others</td>
<td style="text-align: center;">174</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Monthly</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">M4 Yearly</td>
<td style="text-align: center;">23000</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Yearly</td>
</tr>
<tr>
<td style="text-align: center;">M4 Quarterly</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">24000</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Quarterly</td>
</tr>
<tr>
<td style="text-align: center;">M4 Monthly</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">48000</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Monthly</td>
</tr>
<tr>
<td style="text-align: center;">M4 Weekly</td>
<td style="text-align: center;">359</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Monthly</td>
</tr>
<tr>
<td style="text-align: center;">M4 Daily</td>
<td style="text-align: center;">4227</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Monthly</td>
</tr>
<tr>
<td style="text-align: center;">M4 Hourly</td>
<td style="text-align: center;">414</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Monthly</td>
</tr>
<tr>
<td style="text-align: center;">TOURISM Yearly</td>
<td style="text-align: center;">518</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Yearly</td>
<td style="text-align: center;">Yearly</td>
</tr>
<tr>
<td style="text-align: center;">TOURISM Quarterly</td>
<td style="text-align: center;">427</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Quarterly</td>
<td style="text-align: center;">Quarterly</td>
</tr>
<tr>
<td style="text-align: center;">TOURISM Monthly</td>
<td style="text-align: center;">366</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">Monthly</td>
<td style="text-align: center;">Monthly</td>
</tr>
<tr>
<td style="text-align: center;">ELECTR</td>
<td style="text-align: center;">1311</td>
<td style="text-align: center;">168</td>
<td style="text-align: center;">Hourly</td>
<td style="text-align: center;">Monthly</td>
</tr>
</tbody>
</table>
<p>Table 8: Zero-term Forecasting Datasets and Mapping Details of Zero-shot Learning</p>
<p>The details of zero-shot forecasting datasets are: M4 is a large and diverse dataset that contains time series of various frequencies and fields, including business, financial and economic forecasting; M3 is smaller than M4, but also contains time series from diverse domains and frequencies; TOURISM is the dataset of tourism activities with different frequencies and contains a much higher fraction of erratic series compared with M4; ELECTR represents the electricity usage monitoring of 370 customers over three years. Table 8 summarizes details of the datasets and zero-shot mapping between source and target.</p>
<h2>A.3.2 BASELINE DETAILS</h2>
<p>For long-shot forecasting, we refer to the SOTA methods reported in Wu et al. (2023): TimesNet Wu et al. (2023), ETSformer Woo et al. (2022c), DLinear Zeng et al. (2023), FEDformer Zhou et al. (2022), Informer Zhou et al. (2021), and LLM for TS method GPT4TS Zhou et al. (2023).</p>
<p>For few-shot forecasting, we refor to the SOTA methods reported in Zhou et al. (2023): DLinear Zeng et al. (2023), PatchTST Nie et al. (2023), TimesNet Wu et al. (2023), FEDformer Zhou et al. (2022), Autoformer Wu et al. (2021), Stationary Liu et al. (2022), ETSformer Woo et al. (2022c), Informer Zhou et al. (2021), Reformer Kitaev et al. (2020)</p>
<p>For zero-shot forecasting, we refor to the SOTA methods reported in Zhou et al. (2023): N-BEATS Oreshkin et al. (2020), DLinear Zeng et al. (2023), PatchTST Nie et al. (2023), TimesNet Wu et al. (2023), FEDformer Zhou et al. (2022), Autoformer Wu et al. (2021), Stationary Liu et al. (2022), ETSformer Woo et al. (2022c), Informer Zhou et al. (2021), Reformer Kitaev et al. (2020)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TEST</th>
<th style="text-align: center;">GPT4TS</th>
<th style="text-align: center;">TimesNet</th>
<th style="text-align: center;">ETSformer</th>
<th style="text-align: center;">DLinear</th>
<th style="text-align: center;">FEDformer</th>
<th style="text-align: center;">Informer</th>
<th style="text-align: center;">TCN</th>
<th style="text-align: center;">LSTM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ETTm1</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.2930 .346</td>
<td style="text-align: center;">0.2920 .346</td>
<td style="text-align: center;">0.3250 .398</td>
<td style="text-align: center;">0.3380 .375</td>
<td style="text-align: center;">0.3450 .372</td>
<td style="text-align: center;">0.3750 .398</td>
<td style="text-align: center;">0.6720 .571</td>
<td style="text-align: center;">0.8630 .664</td>
<td style="text-align: center;">0.8630 .664</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.3320 .369</td>
<td style="text-align: center;">0.3320 .372</td>
<td style="text-align: center;">0.3240 .387</td>
<td style="text-align: center;">0.4080 .410</td>
<td style="text-align: center;">0.3800 .389</td>
<td style="text-align: center;">0.4260 .441</td>
<td style="text-align: center;">0.7950 .669</td>
<td style="text-align: center;">0.8370 .700</td>
<td style="text-align: center;">1.1130 .776</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.3680 .392</td>
<td style="text-align: center;">0.3660 .394</td>
<td style="text-align: center;">0.3600 .411</td>
<td style="text-align: center;">0.4350 .428</td>
<td style="text-align: center;">0.4130 .413</td>
<td style="text-align: center;">0.4450 .459</td>
<td style="text-align: center;">1.2120 .871</td>
<td style="text-align: center;">1.1240 .832</td>
<td style="text-align: center;">1.2670 .832</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.4180 .420</td>
<td style="text-align: center;">0.4170 .421</td>
<td style="text-align: center;">0.4280 .450</td>
<td style="text-align: center;">0.4990 .462</td>
<td style="text-align: center;">0.4740 .453</td>
<td style="text-align: center;">0.5430 .490</td>
<td style="text-align: center;">1.1660 .823</td>
<td style="text-align: center;">1.1530 .820</td>
<td style="text-align: center;">1.3240 .858</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.3530 .382</td>
<td style="text-align: center;">0.3520 .383</td>
<td style="text-align: center;">0.3500 .406</td>
<td style="text-align: center;">0.4290 .425</td>
<td style="text-align: center;">0.4030 .407</td>
<td style="text-align: center;">0.4480 .452</td>
<td style="text-align: center;">0.9610 .734</td>
<td style="text-align: center;">0.9290 .725</td>
<td style="text-align: center;">1.1420 .782</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.3720 .400</td>
<td style="text-align: center;">0.3760 .397</td>
<td style="text-align: center;">0.3840 .402</td>
<td style="text-align: center;">0.4940 .479</td>
<td style="text-align: center;">0.3860 .400</td>
<td style="text-align: center;">0.3760 .419</td>
<td style="text-align: center;">0.8650 .713</td>
<td style="text-align: center;">0.8780 .740</td>
<td style="text-align: center;">1.0440 .773</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.4140 .422</td>
<td style="text-align: center;">0.4160 .418</td>
<td style="text-align: center;">0.4360 .429</td>
<td style="text-align: center;">0.5380 .504</td>
<td style="text-align: center;">0.4370 .432</td>
<td style="text-align: center;">0.4200 .448</td>
<td style="text-align: center;">1.0080 .792</td>
<td style="text-align: center;">1.0370 .824</td>
<td style="text-align: center;">1.2170 .832</td>
</tr>
<tr>
<td style="text-align: center;">ETTh1</td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.4220 .437</td>
<td style="text-align: center;">0.4420 .433</td>
<td style="text-align: center;">0.4910 .469</td>
<td style="text-align: center;">0.5740 .521</td>
<td style="text-align: center;">0.4810 .459</td>
<td style="text-align: center;">0.4590 .465</td>
<td style="text-align: center;">1.1070 .809</td>
<td style="text-align: center;">1.2380 .932</td>
<td style="text-align: center;">1.2590 .841</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.4470 .467</td>
<td style="text-align: center;">0.4770 .456</td>
<td style="text-align: center;">0.5210 .500</td>
<td style="text-align: center;">0.5620 .535</td>
<td style="text-align: center;">0.5190 .516</td>
<td style="text-align: center;">0.5060 .507</td>
<td style="text-align: center;">1.1810 .865</td>
<td style="text-align: center;">1.1350 .852</td>
<td style="text-align: center;">1.2710 .838</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.4140 .431</td>
<td style="text-align: center;">0.4270 .426</td>
<td style="text-align: center;">0.4580 .450</td>
<td style="text-align: center;">0.5420 .510</td>
<td style="text-align: center;">0.4560 .452</td>
<td style="text-align: center;">0.4400 .460</td>
<td style="text-align: center;">1.0400 .795</td>
<td style="text-align: center;">1.0720 .837</td>
<td style="text-align: center;">1.1980 .821</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.2750 .338</td>
<td style="text-align: center;">0.2850 .342</td>
<td style="text-align: center;">0.3400 .374</td>
<td style="text-align: center;">0.3400 .391</td>
<td style="text-align: center;">0.3330 .387</td>
<td style="text-align: center;">0.3580 .397</td>
<td style="text-align: center;">3.7551 .525</td>
<td style="text-align: center;">2.1161 .197</td>
<td style="text-align: center;">2.5221 .278</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.3400 .379</td>
<td style="text-align: center;">0.3540 .389</td>
<td style="text-align: center;">0.4020 .414</td>
<td style="text-align: center;">0.4300 .439</td>
<td style="text-align: center;">0.4770 .476</td>
<td style="text-align: center;">0.4290 .439</td>
<td style="text-align: center;">5.6021 .931</td>
<td style="text-align: center;">4.3151 .635</td>
<td style="text-align: center;">3.3121 .384</td>
</tr>
<tr>
<td style="text-align: center;">ETTh2</td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.3290 .381</td>
<td style="text-align: center;">0.3730 .407</td>
<td style="text-align: center;">0.4520 .452</td>
<td style="text-align: center;">0.4850 .559</td>
<td style="text-align: center;">0.5940 .541</td>
<td style="text-align: center;">0.4960 .487</td>
<td style="text-align: center;">4.7211 .835</td>
<td style="text-align: center;">1.1241 .604</td>
<td style="text-align: center;">3.2911 .388</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.3810 .423</td>
<td style="text-align: center;">0.4060 .441</td>
<td style="text-align: center;">0.4620 .468</td>
<td style="text-align: center;">0.5000 .497</td>
<td style="text-align: center;">0.8310 .657</td>
<td style="text-align: center;">0.4630 .474</td>
<td style="text-align: center;">3.6471 .625</td>
<td style="text-align: center;">3.1881 .540</td>
<td style="text-align: center;">3.2571 .357</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.3310 .380</td>
<td style="text-align: center;">0.3540 .394</td>
<td style="text-align: center;">0.4140 .427</td>
<td style="text-align: center;">0.4390 .452</td>
<td style="text-align: center;">0.5590 .515</td>
<td style="text-align: center;">0.4370 .449</td>
<td style="text-align: center;">4.4311 .729</td>
<td style="text-align: center;">2.6861 .494</td>
<td style="text-align: center;">3.0951 .352</td>
</tr>
<tr>
<td style="text-align: center;">Electricity</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.1320 .223</td>
<td style="text-align: center;">0.1390 .238</td>
<td style="text-align: center;">0.1680 .222</td>
<td style="text-align: center;">0.1870 .304</td>
<td style="text-align: center;">0.1970 .282</td>
<td style="text-align: center;">0.1930 .308</td>
<td style="text-align: center;">0.2740 .368</td>
<td style="text-align: center;">0.2580 .357</td>
<td style="text-align: center;">0.3750 .437</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.1580 .241</td>
<td style="text-align: center;">0.1530 .251</td>
<td style="text-align: center;">0.1840 .239</td>
<td style="text-align: center;">0.1990 .196</td>
<td style="text-align: center;">0.2850 .201</td>
<td style="text-align: center;">0.3150 .296</td>
<td style="text-align: center;">0.3860 .266</td>
<td style="text-align: center;">0.3680 .348</td>
<td style="text-align: center;">0.4420 .473</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.1630 .260</td>
<td style="text-align: center;">0.1690 .266</td>
<td style="text-align: center;">0.1980 .260</td>
<td style="text-align: center;">0.2120 .329</td>
<td style="text-align: center;">0.2090 .301</td>
<td style="text-align: center;">0.2140 .329</td>
<td style="text-align: center;">0.3000 .394</td>
<td style="text-align: center;">0.2800 .380</td>
<td style="text-align: center;">0.4390 .473</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.1990 .291</td>
<td style="text-align: center;">0.2060 .297</td>
<td style="text-align: center;">0.2200 .300</td>
<td style="text-align: center;">0.2330 .345</td>
<td style="text-align: center;">0.2450 .333</td>
<td style="text-align: center;">0.2460 .355</td>
<td style="text-align: center;">0.3730 .439</td>
<td style="text-align: center;">0.2830 .376</td>
<td style="text-align: center;">0.9800 .814</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.1620 .253</td>
<td style="text-align: center;">0.1670 .263</td>
<td style="text-align: center;">0.1920 .245</td>
<td style="text-align: center;">0.2080 .323</td>
<td style="text-align: center;">0.2120 .300</td>
<td style="text-align: center;">0.2140 .327</td>
<td style="text-align: center;">0.3110 .397</td>
<td style="text-align: center;">0.3130 .401</td>
<td style="text-align: center;">0.5590 .549</td>
</tr>
<tr>
<td style="text-align: center;">Traffic</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.4070 .2820</td>
<td style="text-align: center;">0.3880 .282</td>
<td style="text-align: center;">0.5930 .321</td>
<td style="text-align: center;">0.6070 .392</td>
<td style="text-align: center;">0.6500 .396</td>
<td style="text-align: center;">0.5870 .366</td>
<td style="text-align: center;">0.7190 .391</td>
<td style="text-align: center;">0.6840 .384</td>
<td style="text-align: center;">0.8430 .453</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.4230 .287</td>
<td style="text-align: center;">0.4070 .290</td>
<td style="text-align: center;">0.6170 .336</td>
<td style="text-align: center;">0.6210 .399</td>
<td style="text-align: center;">0.5980 .370</td>
<td style="text-align: center;">0.6040 .373</td>
<td style="text-align: center;">0.6960 .379</td>
<td style="text-align: center;">0.6850 .390</td>
<td style="text-align: center;">0.8470 .453</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.4300 .296</td>
<td style="text-align: center;">0.4120 .294</td>
<td style="text-align: center;">0.6290 .336</td>
<td style="text-align: center;">0.6220 .396</td>
<td style="text-align: center;">0.6050 .373</td>
<td style="text-align: center;">0.6210 .383</td>
<td style="text-align: center;">0.7770 .420</td>
<td style="text-align: center;">0.7340 .408</td>
<td style="text-align: center;">0.8530 .455</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.4630 .315</td>
<td style="text-align: center;">0.4500 .312</td>
<td style="text-align: center;">0.6400 .350</td>
<td style="text-align: center;">0.6320 .396</td>
<td style="text-align: center;">0.6450 .394</td>
<td style="text-align: center;">0.6260 .382</td>
<td style="text-align: center;">0.8640 .472</td>
<td style="text-align: center;">0.7170 .396</td>
<td style="text-align: center;">1.5000 .805</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.4300 .295</td>
<td style="text-align: center;">0.4140 .294</td>
<td style="text-align: center;">0.6200 .336</td>
<td style="text-align: center;">0.6210 .396</td>
<td style="text-align: center;">0.6250 .383</td>
<td style="text-align: center;">0.6100 .376</td>
<td style="text-align: center;">0.7640 .416</td>
<td style="text-align: center;">0.7050 .395</td>
<td style="text-align: center;">1.0110 .541</td>
</tr>
<tr>
<td style="text-align: center;">Weather</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.1500 .202</td>
<td style="text-align: center;">0.1620 .212</td>
<td style="text-align: center;">0.1520 .220</td>
<td style="text-align: center;">0.1970 .281</td>
<td style="text-align: center;">0.1960 .255</td>
<td style="text-align: center;">0.2170 .296</td>
<td style="text-align: center;">0.3000 .384</td>
<td style="text-align: center;">0.4580 .490</td>
<td style="text-align: center;">0.3690 .406</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.1980 .246</td>
<td style="text-align: center;">0.2040 .248</td>
<td style="text-align: center;">0.2090 .261</td>
<td style="text-align: center;">0.2370 .312</td>
<td style="text-align: center;">0.2370 .296</td>
<td style="text-align: center;">0.2760 .336</td>
<td style="text-align: center;">0.5980 .544</td>
<td style="text-align: center;">0.6580 .589</td>
<td style="text-align: center;">0.4160 .435</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.2450 .286</td>
<td style="text-align: center;">0.2540 .286</td>
<td style="text-align: center;">0.2800 .306</td>
<td style="text-align: center;">0.2980 .353</td>
<td style="text-align: center;">0.2830 .335</td>
<td style="text-align: center;">0.3390 .380</td>
<td style="text-align: center;">0.5780 .521</td>
<td style="text-align: center;">0.7970 .652</td>
<td style="text-align: center;">0.4550 .454</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.3240 .342</td>
<td style="text-align: center;">0.3260 .337</td>
<td style="text-align: center;">0.3650 .359</td>
<td style="text-align: center;">0.3520 .288</td>
<td style="text-align: center;">0.3450 .381</td>
<td style="text-align: center;">0.4030 .428</td>
<td style="text-align: center;">1.0590 .741</td>
<td style="text-align: center;">0.8690 .675</td>
<td style="text-align: center;">0.5350 .520</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.2290 .271</td>
<td style="text-align: center;">0.2370 .270</td>
<td style="text-align: center;">0.2360 .287</td>
<td style="text-align: center;">0.2710 .334</td>
<td style="text-align: center;">0.2650 .317</td>
<td style="text-align: center;">0.3090 .360</td>
<td style="text-align: center;">0.6340 .548</td>
<td style="text-align: center;">0.6960 .602</td>
<td style="text-align: center;">0.4440 .454</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">1.9740 .886</td>
<td style="text-align: center;">2.0630 .881</td>
<td style="text-align: center;">2.3170 .934</td>
<td style="text-align: center;">2.5271 .000</td>
<td style="text-align: center;">2.3981 .040</td>
<td style="text-align: center;">3.2281 .260</td>
<td style="text-align: center;">5.7641 .677</td>
<td style="text-align: center;">4.4801 .444</td>
<td style="text-align: center;">5.9141 .734</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2.0280 .976</td>
<td style="text-align: center;">1.8680 .892</td>
<td style="text-align: center;">1.9720 .900</td>
<td style="text-align: center;">2.6151 .007</td>
<td style="text-align: center;">2.6461 .088</td>
<td style="text-align: center;">2.6791 .080</td>
<td style="text-align: center;">4.7551 .467</td>
<td style="text-align: center;">4.7991 .467</td>
<td style="text-align: center;">6.6311 .845</td>
</tr>
<tr>
<td style="text-align: center;">ILI</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">2.3531 .115</td>
<td style="text-align: center;">1.7900 .884</td>
<td style="text-align: center;">2.2380 .900</td>
<td style="text-align: center;">2.3590 .972</td>
<td style="text-align: center;">2.6141 .086</td>
<td style="text-align: center;">2.6221 .078</td>
<td style="text-align: center;">4.7631 .469</td>
<td style="text-align: center;">4.8001 .468</td>
<td style="text-align: center;">6.7361 .857</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">2.4251 .203</td>
<td style="text-align: center;">1.9790 .957</td>
<td style="text-align: center;">2.0270 .928</td>
<td style="text-align: center;">2.4871 .016</td>
<td style="text-align: center;">2.8041 .146</td>
<td style="text-align: center;">2.8571 .15</td>
<td style="text-align: center;">5.2641 .564</td>
<td style="text-align: center;">5.2781 .560</td>
<td style="text-align: center;">6.8701 .879</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">2.1951 .045</td>
<td style="text-align: center;">1.9250 .903</td>
<td style="text-align: center;">2.1390 .901</td>
<td style="text-align: center;">2.4971 .004</td>
<td style="text-align: center;">2.6161 .090</td>
<td style="text-align: center;">2.8471 .144</td>
<td style="text-align: center;">5.1371 .544</td>
<td style="text-align: center;">4.8391 .485</td>
<td style="text-align: center;">6.5381 .829</td>
</tr>
<tr>
<td style="text-align: center;">$1^{\text {st }}$ count</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Table 9: Long-term Forecasting Results (MSE, MAE). TEST uses GPT2-Medium as the backbone. The past sequence length is set as 36 for ILI and 96 for the others. All the results are averaged from 4 different prediction lengths, that is ${24,36,48,60}$ for ILI and ${96,192,336,720}$ for the others.</p>
<h1>A.3.3 LONG-TERM FORECASTING</h1>
<p>We follow the classical experiment settings and the results of SOTA models in Wu et al. (2023) (ICLR 2023). The results are shown in Table 9. Overall, TEST achieves comparable performance to SOTA models TimesNet and Dlinear, and outperforms other baselines.</p>
<h2>A.3.4 FEW-SHOT FORECASTING</h2>
<p>For the few-shot forecasting task, only $10 \%$ percentage timesteps of training data are used, and the other two parts remain unchanged. We follow the classical experiment settings and the results of SOTA models in Zhou et al. (2023) (NeurIPS 2023). The results are shown in Table 10. Overall, TEST has comparable performance with the SOTA baselines PatchTST and Dlinear, and SOTA LLM for TS method GPT4TS.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TEST</th>
<th style="text-align: center;">GPT4TS</th>
<th style="text-align: center;">DLinear</th>
<th style="text-align: center;">PatchTST</th>
<th style="text-align: center;">TimesNet</th>
<th style="text-align: center;">FEDformer</th>
<th style="text-align: center;">Autoformer</th>
<th style="text-align: center;">Stationary</th>
<th style="text-align: center;">ETSformer</th>
<th style="text-align: center;">LightTS</th>
<th style="text-align: center;">Informer</th>
<th style="text-align: center;">Reformer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.1630 .213</td>
<td style="text-align: center;">0.1630 .215</td>
<td style="text-align: center;">0.1710 .224</td>
<td style="text-align: center;">0.1650 .215</td>
<td style="text-align: center;">0.1840 .230</td>
<td style="text-align: center;">0.1880 .253</td>
<td style="text-align: center;">0.2210 .297</td>
<td style="text-align: center;">0.1920 .234</td>
<td style="text-align: center;">0.1990 .272</td>
<td style="text-align: center;">0.2170 .269</td>
<td style="text-align: center;">0.3740 .401</td>
<td style="text-align: center;">0.3350 .380</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.2300 .263</td>
<td style="text-align: center;">0.2100 .254</td>
<td style="text-align: center;">0.2150 .263</td>
<td style="text-align: center;">0.2100 .257</td>
<td style="text-align: center;">0.2450 .283</td>
<td style="text-align: center;">0.2500 .304</td>
<td style="text-align: center;">0.2700 .322</td>
<td style="text-align: center;">0.2690 .295</td>
<td style="text-align: center;">0.2790 .332</td>
<td style="text-align: center;">0.2590 .304</td>
<td style="text-align: center;">0.5520 .478</td>
<td style="text-align: center;">0.5220 .462</td>
</tr>
<tr>
<td style="text-align: center;">Weather</td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.2780 .282</td>
<td style="text-align: center;">0.2560 .292</td>
<td style="text-align: center;">0.2580 .299</td>
<td style="text-align: center;">0.2590 .297</td>
<td style="text-align: center;">0.3050 .321</td>
<td style="text-align: center;">0.3120 .346</td>
<td style="text-align: center;">0.3200 .351</td>
<td style="text-align: center;">0.3700 .357</td>
<td style="text-align: center;">0.3560 .386</td>
<td style="text-align: center;">0.3030 .334</td>
<td style="text-align: center;">0.7240 .541</td>
<td style="text-align: center;">0.7150 .535</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.3010 .328</td>
<td style="text-align: center;">0.3210 .339</td>
<td style="text-align: center;">0.3200 .346</td>
<td style="text-align: center;">0.3320 .346</td>
<td style="text-align: center;">0.3810 .371</td>
<td style="text-align: center;">0.3870 .393</td>
<td style="text-align: center;">0.3900 .396</td>
<td style="text-align: center;">0.4410 .405</td>
<td style="text-align: center;">0.4370 .448</td>
<td style="text-align: center;">0.3770 .382</td>
<td style="text-align: center;">0.7390 .558</td>
<td style="text-align: center;">0.6110 .500</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.2430 .272</td>
<td style="text-align: center;">0.2380 .275</td>
<td style="text-align: center;">0.2410 .283</td>
<td style="text-align: center;">0.2420 .279</td>
<td style="text-align: center;">0.2790 .301</td>
<td style="text-align: center;">0.2840 .324</td>
<td style="text-align: center;">0.3000 .342</td>
<td style="text-align: center;">0.3180 .323</td>
<td style="text-align: center;">0.3180 .360</td>
<td style="text-align: center;">0.2890 .322</td>
<td style="text-align: center;">0.5970 .495</td>
<td style="text-align: center;">0.5460 .469</td>
</tr>
<tr>
<td style="text-align: center;">ETTh1</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.4550 .457</td>
<td style="text-align: center;">0.4580 .456</td>
<td style="text-align: center;">0.4920 .495</td>
<td style="text-align: center;">0.5160 .485</td>
<td style="text-align: center;">0.8610 .628</td>
<td style="text-align: center;">0.5120 .499</td>
<td style="text-align: center;">0.6130 .552</td>
<td style="text-align: center;">0.9180 .639</td>
<td style="text-align: center;">1.1120 .806</td>
<td style="text-align: center;">1.2980 .838</td>
<td style="text-align: center;">1.1790 .792</td>
<td style="text-align: center;">1.1840 .790</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.5720 .519</td>
<td style="text-align: center;">0.5700 .516</td>
<td style="text-align: center;">0.5650 .538</td>
<td style="text-align: center;">0.5980 .524</td>
<td style="text-align: center;">0.7970 .593</td>
<td style="text-align: center;">0.6240 .555</td>
<td style="text-align: center;">0.7220 .598</td>
<td style="text-align: center;">0.9150 .629</td>
<td style="text-align: center;">1.1550 .823</td>
<td style="text-align: center;">1.3220 .854</td>
<td style="text-align: center;">1.1990 .806</td>
<td style="text-align: center;">1.2950 .850</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.6110 .531</td>
<td style="text-align: center;">0.6080 .535</td>
<td style="text-align: center;">0.7210 .622</td>
<td style="text-align: center;">0.6570 .550</td>
<td style="text-align: center;">0.9410 .648</td>
<td style="text-align: center;">0.6910 .574</td>
<td style="text-align: center;">0.7500 .619</td>
<td style="text-align: center;">0.9390 .644</td>
<td style="text-align: center;">1.1790 .832</td>
<td style="text-align: center;">1.3470 .870</td>
<td style="text-align: center;">1.2020 .811</td>
<td style="text-align: center;">1.2940 .854</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.7230 .594</td>
<td style="text-align: center;">0.7250 .591</td>
<td style="text-align: center;">0.9860 .743</td>
<td style="text-align: center;">0.7620 .610</td>
<td style="text-align: center;">0.8770 .641</td>
<td style="text-align: center;">0.7280 .614</td>
<td style="text-align: center;">0.7210 .616</td>
<td style="text-align: center;">0.8870 .645</td>
<td style="text-align: center;">1.2730 .874</td>
<td style="text-align: center;">1.5340 .947</td>
<td style="text-align: center;">1.2170 .825</td>
<td style="text-align: center;">1.2230 .838</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.4790 .525</td>
<td style="text-align: center;">0.5900 .525</td>
<td style="text-align: center;">0.6910 .600</td>
<td style="text-align: center;">0.6330 .542</td>
<td style="text-align: center;">0.8690 .628</td>
<td style="text-align: center;">0.6390 .561</td>
<td style="text-align: center;">0.7020 .596</td>
<td style="text-align: center;">0.9150 .639</td>
<td style="text-align: center;">1.1800 .834</td>
<td style="text-align: center;">1.3750 .877</td>
<td style="text-align: center;">1.1990 .809</td>
<td style="text-align: center;">1.2490 .833</td>
</tr>
<tr>
<td style="text-align: center;">ETTh2</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.3320 .374</td>
<td style="text-align: center;">0.3310 .374</td>
<td style="text-align: center;">0.3570 .411</td>
<td style="text-align: center;">0.3530 .389</td>
<td style="text-align: center;">0.3780 .409</td>
<td style="text-align: center;">0.3820 .416</td>
<td style="text-align: center;">0.4130 .451</td>
<td style="text-align: center;">0.3890 .411</td>
<td style="text-align: center;">0.6780 .619</td>
<td style="text-align: center;">2.0221 .006</td>
<td style="text-align: center;">3.8371 .508</td>
<td style="text-align: center;">3.7881 .533</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.4010 .433</td>
<td style="text-align: center;">0.4020 .411</td>
<td style="text-align: center;">0.5690 .519</td>
<td style="text-align: center;">0.4030 .414</td>
<td style="text-align: center;">0.4900 .467</td>
<td style="text-align: center;">0.4780 .474</td>
<td style="text-align: center;">0.4740 .477</td>
<td style="text-align: center;">0.4730 .455</td>
<td style="text-align: center;">0.7850 .666</td>
<td style="text-align: center;">2.3291 .104</td>
<td style="text-align: center;">3.8561 .513</td>
<td style="text-align: center;">3.5521 .483</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.4080 .440</td>
<td style="text-align: center;">0.4060 .433</td>
<td style="text-align: center;">0.6710 .572</td>
<td style="text-align: center;">0.4260 .441</td>
<td style="text-align: center;">0.5370 .494</td>
<td style="text-align: center;">0.5040 .501</td>
<td style="text-align: center;">0.5470 .543</td>
<td style="text-align: center;">0.5070 .480</td>
<td style="text-align: center;">0.8390 .694</td>
<td style="text-align: center;">2.4531 .122</td>
<td style="text-align: center;">3.9521 .526</td>
<td style="text-align: center;">3.3951 .526</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.4590 .480</td>
<td style="text-align: center;">0.4490 .464</td>
<td style="text-align: center;">0.8240 .648</td>
<td style="text-align: center;">0.4770 .480</td>
<td style="text-align: center;">0.5100 .491</td>
<td style="text-align: center;">0.4990 .509</td>
<td style="text-align: center;">0.5160 .523</td>
<td style="text-align: center;">0.4770 .472</td>
<td style="text-align: center;">1.2730 .874</td>
<td style="text-align: center;">3.8161 .407</td>
<td style="text-align: center;">3.8421 .503</td>
<td style="text-align: center;">3.2051 .401</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.4010 .432</td>
<td style="text-align: center;">0.3970 .421</td>
<td style="text-align: center;">0.6050 .538</td>
<td style="text-align: center;">0.4150 .431</td>
<td style="text-align: center;">0.4790 .465</td>
<td style="text-align: center;">0.4660 .475</td>
<td style="text-align: center;">0.4880 .499</td>
<td style="text-align: center;">0.4620 .455</td>
<td style="text-align: center;">0.8940 .713</td>
<td style="text-align: center;">2.6551 .160</td>
<td style="text-align: center;">3.8721 .513</td>
<td style="text-align: center;">3.4851 .486</td>
</tr>
<tr>
<td style="text-align: center;">ETTm1</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.3920 .401</td>
<td style="text-align: center;">0.3900 .404</td>
<td style="text-align: center;">0.3520 .392</td>
<td style="text-align: center;">0.4100 .419</td>
<td style="text-align: center;">0.5830 .501</td>
<td style="text-align: center;">0.5780 .518</td>
<td style="text-align: center;">0.7740 .614</td>
<td style="text-align: center;">0.7610 .568</td>
<td style="text-align: center;">0.9110 .688</td>
<td style="text-align: center;">0.9230 .682</td>
<td style="text-align: center;">1.1620 .785</td>
<td style="text-align: center;">1.4420 .847</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.4230 .426</td>
<td style="text-align: center;">0.4290 .423</td>
<td style="text-align: center;">0.3820 .412</td>
<td style="text-align: center;">0.4370 .434</td>
<td style="text-align: center;">0.6300 .528</td>
<td style="text-align: center;">0.6170 .546</td>
<td style="text-align: center;">0.7540 .592</td>
<td style="text-align: center;">0.7810 .574</td>
<td style="text-align: center;">0.9550 .703</td>
<td style="text-align: center;">0.9570 .701</td>
<td style="text-align: center;">1.1720 .793</td>
<td style="text-align: center;">1.4440 .862</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.4710 .444</td>
<td style="text-align: center;">0.4690 .439</td>
<td style="text-align: center;">0.4190 .434</td>
<td style="text-align: center;">0.4760 .454</td>
<td style="text-align: center;">0.7250 .568</td>
<td style="text-align: center;">0.9980 .775</td>
<td style="text-align: center;">0.8690 .677</td>
<td style="text-align: center;">0.8030 .587</td>
<td style="text-align: center;">0.9910 .719</td>
<td style="text-align: center;">0.9980 .716</td>
<td style="text-align: center;">1.2270 .908</td>
<td style="text-align: center;">1.4500 .866</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.5520 .501</td>
<td style="text-align: center;">0.5690 .498</td>
<td style="text-align: center;">0.4900 .477</td>
<td style="text-align: center;">0.6810 .556</td>
<td style="text-align: center;">0.7690 .549</td>
<td style="text-align: center;">0.6930 .579</td>
<td style="text-align: center;">0.8100 .630</td>
<td style="text-align: center;">0.8440 .581</td>
<td style="text-align: center;">1.0620 .747</td>
<td style="text-align: center;">1.0070 .719</td>
<td style="text-align: center;">1.2070 .797</td>
<td style="text-align: center;">1.3660 .850</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.5740 .443</td>
<td style="text-align: center;">0.4640 .441</td>
<td style="text-align: center;">0.4110 .429</td>
<td style="text-align: center;">0.5010 .466</td>
<td style="text-align: center;">0.6770 .537</td>
<td style="text-align: center;">0.7220 .605</td>
<td style="text-align: center;">0.8020 .628</td>
<td style="text-align: center;">0.7970 .578</td>
<td style="text-align: center;">0.9800 .714</td>
<td style="text-align: center;">0.9710 .705</td>
<td style="text-align: center;">1.1920 .821</td>
<td style="text-align: center;">1.4260 .856</td>
</tr>
<tr>
<td style="text-align: center;">ETTm2</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.2330 .262</td>
<td style="text-align: center;">0.1880 .269</td>
<td style="text-align: center;">0.2130 .303</td>
<td style="text-align: center;">0.1910 .274</td>
<td style="text-align: center;">0.2120 .285</td>
<td style="text-align: center;">0.2910 .399</td>
<td style="text-align: center;">0.3520 .454</td>
<td style="text-align: center;">0.2290 .308</td>
<td style="text-align: center;">0.3310 .430</td>
<td style="text-align: center;">0.8130 .688</td>
<td style="text-align: center;">3.2031 .407</td>
<td style="text-align: center;">4.1951 .628</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.3030 .302</td>
<td style="text-align: center;">0.2510 .309</td>
<td style="text-align: center;">0.2780 .345</td>
<td style="text-align: center;">0.2520 .317</td>
<td style="text-align: center;">0.2700 .323</td>
<td style="text-align: center;">0.3070 .379</td>
<td style="text-align: center;">0.6940 .691</td>
<td style="text-align: center;">0.2910 .343</td>
<td style="text-align: center;">0.4000 .464</td>
<td style="text-align: center;">1.0080 .768</td>
<td style="text-align: center;">3.1121 .387</td>
<td style="text-align: center;">4.0421 .601</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.3590 .341</td>
<td style="text-align: center;">0.3070 .346</td>
<td style="text-align: center;">0.3380 .385</td>
<td style="text-align: center;">0.3060 .353</td>
<td style="text-align: center;">0.3230 .353</td>
<td style="text-align: center;">0.5430 .559</td>
<td style="text-align: center;">2.4081 .407</td>
<td style="text-align: center;">0.3480 .376</td>
<td style="text-align: center;">0.4690 .498</td>
<td style="text-align: center;">1.0310 .775</td>
<td style="text-align: center;">3.2551 .421</td>
<td style="text-align: center;">3.9631 .585</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.4520 .419</td>
<td style="text-align: center;">0.4260 .417</td>
<td style="text-align: center;">0.4360 .440</td>
<td style="text-align: center;">0.4330 .427</td>
<td style="text-align: center;">0.4740 .449</td>
<td style="text-align: center;">0.7120 .614</td>
<td style="text-align: center;">1.9131 .166</td>
<td style="text-align: center;">0.4610 .438</td>
<td style="text-align: center;">0.5890 .557</td>
<td style="text-align: center;">1.0960 .791</td>
<td style="text-align: center;">3.9091 .543</td>
<td style="text-align: center;">3.7111 .532</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.3170 .309</td>
<td style="text-align: center;">0.2930 .335</td>
<td style="text-align: center;">0.3160 .368</td>
<td style="text-align: center;">0.2960 .343</td>
<td style="text-align: center;">0.3200 .353</td>
<td style="text-align: center;">0.4630 .488</td>
<td style="text-align: center;">1.3420 .930</td>
<td style="text-align: center;">0.3320 .366</td>
<td style="text-align: center;">0.4470 .487</td>
<td style="text-align: center;">0.9870 .756</td>
<td style="text-align: center;">3.3701 .440</td>
<td style="text-align: center;">3.9781 .587</td>
</tr>
<tr>
<td style="text-align: center;">Electricity</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.1430 .235</td>
<td style="text-align: center;">0.1390 .237</td>
<td style="text-align: center;">0.1500 .253</td>
<td style="text-align: center;">0.1400 .238</td>
<td style="text-align: center;">0.2990 .373</td>
<td style="text-align: center;">0.2310 .323</td>
<td style="text-align: center;">0.2610 .348</td>
<td style="text-align: center;">0.4200 .466</td>
<td style="text-align: center;">0.5990 .587</td>
<td style="text-align: center;">0.3500 .425</td>
<td style="text-align: center;">1.2590 .919</td>
<td style="text-align: center;">0.9930 .784</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.1580 .255</td>
<td style="text-align: center;">0.1560 .252</td>
<td style="text-align: center;">0.1640 .264</td>
<td style="text-align: center;">0.1600 .255</td>
<td style="text-align: center;">0.3050 .379</td>
<td style="text-align: center;">0.2610 .356</td>
<td style="text-align: center;">0.3380 .406</td>
<td style="text-align: center;">0.4110 .459</td>
<td style="text-align: center;">0.6200 .598</td>
<td style="text-align: center;">0.3760 .448</td>
<td style="text-align: center;">1.1600 .873</td>
<td style="text-align: center;">0.9380 .753</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.1760 .275</td>
<td style="text-align: center;">0.1750 .270</td>
<td style="text-align: center;">0.1810 .282</td>
<td style="text-align: center;">0.1800 .276</td>
<td style="text-align: center;">0.3190 .391</td>
<td style="text-align: center;">0.3600 .445</td>
<td style="text-align: center;">0.4100 .474</td>
<td style="text-align: center;">0.4340 .473</td>
<td style="text-align: center;">0.6620 .619</td>
<td style="text-align: center;">0.4280 .485</td>
<td style="text-align: center;">1.1570 .872</td>
<td style="text-align: center;">0.9250 .745</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.2300 .311</td>
<td style="text-align: center;">0.2330 .317</td>
<td style="text-align: center;">0.2230 .321</td>
<td style="text-align: center;">0.2410 .323</td>
<td style="text-align: center;">0.3690 .426</td>
<td style="text-align: center;">0.5300 .585</td>
<td style="text-align: center;">0.7150 .685</td>
<td style="text-align: center;">0.5100 .521</td>
<td style="text-align: center;">0.7570 .664</td>
<td style="text-align: center;">0.6110 .597</td>
<td style="text-align: center;">1.2030 .898</td>
<td style="text-align: center;">1.0040 .790</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.1760 .269</td>
<td style="text-align: center;">0.1760 .269</td>
<td style="text-align: center;">0.1800 .280</td>
<td style="text-align: center;">0.1800 .273</td>
<td style="text-align: center;">0.3230 .392</td>
<td style="text-align: center;">0.3460 .427</td>
<td style="text-align: center;">0.4310 .478</td>
<td style="text-align: center;">0.4440 .480</td>
<td style="text-align: center;">0.6600 .617</td>
<td style="text-align: center;">0.4410 .489</td>
<td style="text-align: center;">1.1950 .891</td>
<td style="text-align: center;">0.9650 .768</td>
</tr>
<tr>
<td style="text-align: center;">Traffic</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.4150 .317</td>
<td style="text-align: center;">0.4140 .297</td>
<td style="text-align: center;">0.4190 .298</td>
<td style="text-align: center;">0.4030 .289</td>
<td style="text-align: center;">0.7190 .416</td>
<td style="text-align: center;">0.6390 .400</td>
<td style="text-align: center;">0.6720 .405</td>
<td style="text-align: center;">1.4120 .802</td>
<td style="text-align: center;">1.6430 .855</td>
<td style="text-align: center;">1.1570 .636</td>
<td style="text-align: center;">1.5570 .821</td>
<td style="text-align: center;">1.5270 .815</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.4250 .300</td>
<td style="text-align: center;">0.4260 .301</td>
<td style="text-align: center;">0.4340 .305</td>
<td style="text-align: center;">0.4150 .296</td>
<td style="text-align: center;">0.7480 .428</td>
<td style="text-align: center;">0.6370 .416</td>
<td style="text-align: center;">0.7270 .424</td>
<td style="text-align: center;">1.4190 .806</td>
<td style="text-align: center;">1.6410 .854</td>
<td style="text-align: center;">1.2070 .661</td>
<td style="text-align: center;">1.4540 .765</td>
<td style="text-align: center;">1.5380 .817</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.4360 .310</td>
<td style="text-align: center;">0.4340 .303</td>
<td style="text-align: center;">0.4490 .313</td>
<td style="text-align: center;">0.4260 .304</td>
<td style="text-align: center;">0.8530 .471</td>
<td style="text-align: center;">0.6550 .427</td>
<td style="text-align: center;">0.7490 .454</td>
<td style="text-align: center;">1.4430 .815</td>
<td style="text-align: center;">1.7110 .878</td>
<td style="text-align: center;">1.3340 .713</td>
<td style="text-align: center;">1.5210 .812</td>
<td style="text-align: center;">1.5500 .819</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.4890 .338</td>
<td style="text-align: center;">0.4870 .337</td>
<td style="text-align: center;">0.4840 .336</td>
<td style="text-align: center;">0.4740 .331</td>
<td style="text-align: center;">1.4850 .825</td>
<td style="text-align: center;">0.7220 .456</td>
<td style="text-align: center;">0.8470 .499</td>
<td style="text-align: center;">1.5390 .837</td>
<td style="text-align: center;">2.6601 .157</td>
<td style="text-align: center;">1.2920 .726</td>
<td style="text-align: center;">1.6050 .846</td>
<td style="text-align: center;">1.5880 .833</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.4410 .316</td>
<td style="text-align: center;">0.4400 .310</td>
<td style="text-align: center;">0.4470 .313</td>
<td style="text-align: center;">0.4300 .305</td>
<td style="text-align: center;">0.9510 .535</td>
<td style="text-align: center;">0.6630 .425</td>
<td style="text-align: center;">0.7490 .446</td>
<td style="text-align: center;">1.4530 .815</td>
<td style="text-align: center;">1.9140 .936</td>
<td style="text-align: center;">1.2480 .684</td>
<td style="text-align: center;">1.5340 .811</td>
<td style="text-align: center;">1.5510 .821</td>
</tr>
<tr>
<td style="text-align: center;">$1^{\text {st }}$ count</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Table 10: Few-shot Forecasting Results (MSE, MAE). TEST uses GPT2-Medium as the backbone. All the results are averaged from 4 different prediction lengths, that is ${96,192,336,720}$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">M4</th>
<th style="text-align: center;">M3</th>
<th style="text-align: center;">TOURISM</th>
<th style="text-align: center;">ELECTR</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Metric</td>
<td style="text-align: center;">sMAPE</td>
<td style="text-align: center;">sMAPE</td>
<td style="text-align: center;">MAPE</td>
<td style="text-align: center;">ND=100</td>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">$1^{\text {st }}$ count</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">N-BEATS</td>
<td style="text-align: center;">11.70</td>
<td style="text-align: center;">12.44</td>
<td style="text-align: center;">18.82</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">15.19</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">DLinear</td>
<td style="text-align: center;">15.33</td>
<td style="text-align: center;">14.03</td>
<td style="text-align: center;">28.51</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">18.86</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">TimesNet</td>
<td style="text-align: center;">13.55</td>
<td style="text-align: center;">14.17</td>
<td style="text-align: center;">28.84</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">18.96</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">PatchTST</td>
<td style="text-align: center;">13.22</td>
<td style="text-align: center;">13.06</td>
<td style="text-align: center;">27.10</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">17.67</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ETSformer</td>
<td style="text-align: center;">27.74</td>
<td style="text-align: center;">16.03</td>
<td style="text-align: center;">180.40</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">67.09</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">LightTS</td>
<td style="text-align: center;">13.62</td>
<td style="text-align: center;">17.90</td>
<td style="text-align: center;">66.99</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">29.52</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Stationary</td>
<td style="text-align: center;">13.32</td>
<td style="text-align: center;">15.29</td>
<td style="text-align: center;">43.75</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">23.59</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">FEDformer</td>
<td style="text-align: center;">15.04</td>
<td style="text-align: center;">13.53</td>
<td style="text-align: center;">31.55</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">19.63</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Autoformer</td>
<td style="text-align: center;">20.02</td>
<td style="text-align: center;">15.87</td>
<td style="text-align: center;">40.39</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">27.54</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Informer</td>
<td style="text-align: center;">19.04</td>
<td style="text-align: center;">15.82</td>
<td style="text-align: center;">35.82</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">22.97</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Reformer</td>
<td style="text-align: center;">14.09</td>
<td style="text-align: center;">13.37</td>
<td style="text-align: center;">25.48</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">18.63</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT2(6)</td>
<td style="text-align: center;">13.12</td>
<td style="text-align: center;">13.06</td>
<td style="text-align: center;">22.14</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">16.38</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">TEST</td>
<td style="text-align: center;">13.10</td>
<td style="text-align: center;">12.56</td>
<td style="text-align: center;">18.17</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">15.93</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 11: Zero-shot learning results. Dataset-specific metrics aggregated over each dataset. A lower value indicates better performance. The source dataset of M3, Tourism, Electricity are M4. For M4, the source data for N-BEATS is FRED, and M3 for other models.</p>
<h1>A.3.5 Zero-shot Forecasting</h1>
<p>Zero-shot Forecasting task can evaluate the cross datasets adaption ability. Which means that the method is evaluated to perform on a dataset (without any training data from this dataset) when it is trained from another dataset. The results are summarized in Table 11. TEST outperforms all recent SOTA methods. TEST is comparable to N-BEATS without any meta-learning design and GPT4TS.</p>
<h1>A. 4 Classification Tasks</h1>
<p>All the deep learning networks are implemented in PyTorch and trained on NVIDIA V100 32GB GPUs. We use Area Under Curve of Receiver Operating Characteristic (AUC-ROC) as metrics. Meanwhile, we compute the average rank, the number of Top-1, Top-3, and Top-5 accuracy to show the robustness of different methods. All experiments are repeated 3 times and the mean of the metrics is used in the final results.</p>
<h2>A.4.1 DATASET DETAILS</h2>
<p>We present accuracy scores for all 30 kinds of multivariate TS datasets in UEA archive Bagnall et al. (2018). UEA consists of 30 different datasets. Details of these datasets are shown in Table 12</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Train Cases</th>
<th style="text-align: center;">Test Cases</th>
<th style="text-align: center;">Dimensions</th>
<th style="text-align: center;">Length</th>
<th style="text-align: center;">Classes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ArticularyWordRecognition</td>
<td style="text-align: center;">275</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">144</td>
<td style="text-align: center;">25</td>
</tr>
<tr>
<td style="text-align: left;">AtrialFibrillation</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">640</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">BasicMotions</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">CharacterTrajectories</td>
<td style="text-align: center;">1422</td>
<td style="text-align: center;">1436</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">182</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">Cricket</td>
<td style="text-align: center;">108</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">17984</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">DuckDuckGeese</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">1345</td>
<td style="text-align: center;">270</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">EigenWorms</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">131</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">17984</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">Epilepsy</td>
<td style="text-align: center;">137</td>
<td style="text-align: center;">138</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">206</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">EthanolConcentration</td>
<td style="text-align: center;">261</td>
<td style="text-align: center;">263</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1751</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">ERing</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">FaceDetection</td>
<td style="text-align: center;">5890</td>
<td style="text-align: center;">3524</td>
<td style="text-align: center;">144</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">FingerMovements</td>
<td style="text-align: center;">316</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">HandMovementDirection</td>
<td style="text-align: center;">320</td>
<td style="text-align: center;">147</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Handwriting</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">850</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">152</td>
<td style="text-align: center;">26</td>
</tr>
<tr>
<td style="text-align: left;">Heartbeat</td>
<td style="text-align: center;">204</td>
<td style="text-align: center;">105</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">495</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">JapaneseVowels</td>
<td style="text-align: center;">270</td>
<td style="text-align: center;">370</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">9</td>
</tr>
<tr>
<td style="text-align: left;">Libras</td>
<td style="text-align: center;">180</td>
<td style="text-align: center;">280</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">15</td>
</tr>
<tr>
<td style="text-align: left;">LSST</td>
<td style="text-align: center;">2459</td>
<td style="text-align: center;">2466</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: left;">InsectWingbeat</td>
<td style="text-align: center;">30000</td>
<td style="text-align: center;">20000</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">MotorImagery</td>
<td style="text-align: center;">278</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">3000</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">NATOPS</td>
<td style="text-align: center;">180</td>
<td style="text-align: center;">180</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">PenDigits</td>
<td style="text-align: center;">7494</td>
<td style="text-align: center;">3498</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">PEMS-SF</td>
<td style="text-align: center;">267</td>
<td style="text-align: center;">173</td>
<td style="text-align: center;">963</td>
<td style="text-align: center;">144</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: left;">Phoneme</td>
<td style="text-align: center;">3315</td>
<td style="text-align: center;">3353</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">217</td>
<td style="text-align: center;">39</td>
</tr>
<tr>
<td style="text-align: left;">RocketSports</td>
<td style="text-align: center;">151</td>
<td style="text-align: center;">152</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">SelfRegulationSCP1</td>
<td style="text-align: center;">268</td>
<td style="text-align: center;">293</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">896</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">SelfRegulationSCP2</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">180</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1152</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">SpokenArabicDigits</td>
<td style="text-align: center;">6599</td>
<td style="text-align: center;">2199</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">93</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">StandWalkJump</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2500</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">UWaveGestureLibrary</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">320</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">315</td>
<td style="text-align: center;">8</td>
</tr>
</tbody>
</table>
<p>Table 12: UEA Classification Dataset Details</p>
<h2>A.4.2 BASELINE DETAILS</h2>
<p>For classification, we refer to the SOTA methods: Three benchmarks Bostrom et al. (2018) (EDI, DTWI, and DTWD) are based on Euclidean Distance, dimension-independent dynamic time warping, and dimension-dependent dynamic time warping; MLSTM-FCNs Karim et al. (2019) applies an LSTM layer and stacked CNN layers to generate features; WEASEL-MUSE Schäfer \&amp; Leser (2017) is a bag-of-pattern based approach which extracts and represents features to words. Scalable Representation Learning (SRL) Franceschi et al. (2019a) employs negative sampling techniques with an encoder-based architecture to learn the representation; TapNet Zhang et al. (2020) is a recent model with an attentional prototype learning in its deep learning-based network; ShapeNet Li et al. (2021a) projects the subsequences into a unified space and applies clustering to find the shapelets; Rocket and MiniRocket Dempster et al. (2021) use random convolutional kernels to extract features from univariate time series; RL-PAM Gao et al. (2022) introduces reinforcement learning to the pattern mining; TStamp Transformer Zerveas et al. (2021) takes the values at each timestamp as the input for a transformer encoder; SVP-T Zuo et al. (2023) uses differnt variables and positions (time interval) as the inputs (shape-level).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Corresponding authors
${ }^{1}$ This categorization focuses on the requirement for changing the model. But from technology, LLM+TS can be achieved by pre-training, fine-tuning, tool-augmented methods, external encoders, and their ensemble.</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
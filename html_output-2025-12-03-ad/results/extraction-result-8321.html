<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8321 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8321</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8321</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-2d0b030d314a5aa8feaa03695e8471270130bdf9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2d0b030d314a5aa8feaa03695e8471270130bdf9" target="_blank">Imagine while Reasoning in Space: Multimodal Visualization-of-Thought</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT), which enables visual thinking in MLLMs by generating image visualizations of their reasoning traces and introduces token discrepancy loss into autoregressive MLLMs to ensure high-quality visualization.</p>
                <p><strong>Paper Abstract:</strong> Chain-of-Thought (CoT) prompting has proven highly effective for enhancing complex reasoning in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks. Nonetheless, human cognition extends beyond language alone, enabling the remarkable capability to think in both words and images. Inspired by this mechanism, we propose a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by generating image visualizations of their reasoning traces. To ensure high-quality visualization, we introduce token discrepancy loss into autoregressive MLLMs. This innovation significantly improves both visual coherence and fidelity. We validate this approach through several dynamic spatial reasoning tasks. Experimental results reveal that MVoT demonstrates competitive performance across tasks. Moreover, it exhibits robust and reliable improvements in the most challenging scenarios where CoT fails. Ultimately, MVoT establishes new possibilities for complex reasoning tasks where visual thinking can effectively complement verbal reasoning.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8321.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8321.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Anole-7B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anole 7B (autoregressive multimodal model, fine-tuned for MVoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anole-7B is an open autoregressive multimodal model (tuned on Chameleon) used as the experimental backbone in this paper and fine-tuned with LoRA to generate interleaved text and image 'thoughts' under different prompting/training regimens including Direct, CoT, Interleaved, and MVoT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Anole-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An autoregressive multimodal LLM capable of interleaved image-text generation; in this work it is the experimental backbone (tuned on Chameleon) and fine-tuned with LoRA in instruction-tuning fashion to support MVoT and baseline variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>MAZE; MiniBEHAVIOR (InstallingAPrinter); FrozenLAKE</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based dynamic spatial reasoning tasks: MAZE (abstract maze navigation); MiniBEHAVIOR InstallingAPrinter (embodied interaction with objects on a grid); FrozenLAKE (grid with fine-grained visual patterns and holes requiring safe navigation).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Models receive an initial image of the grid environment plus a sequence of actions. Output is a multiple-choice final outcome (A/B/C/D depending on task). Variants tested: Direct prompting (no intermediate reasoning), Chain-of-Thought (CoT) textual reasoning (with or without textual environment layout captions/coordinates), Interleaved training (text+image pairs but loss only on text tokens), and MVoT (model trained to generate interleaved visual and verbal rationale images and text; loss applied to all token types including image tokens with token-discrepancy loss). Extraction of final answer performed by pattern matching 'the answer is'.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Direct, CoT prompting (explicit textual step-by-step coordinates and state updates), Interleaved training (images present in training but image tokens excluded from loss), and MVoT (Multimodal Visualization-of-Thought) where the model generates visual thoughts (image-token visualizations) interleaved with verbal CoT steps. MVoT training adds token-discrepancy loss to align predicted image-token distributions with visual embedding similarities; training uses LoRA tuning of Anole-7B on MI300X for 40 epochs and input augmentation (repeated tokenization/detokenization).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported accuracies (fraction correct) on test sets (Table 2): Direct: MAZE 0.7171, MiniBEHAVIOR 0.7250, FrozenLAKE 0.7788. CoT: MAZE 0.9792, MiniBEHAVIOR 0.9812, FrozenLAKE 0.6148. Interleaved: MAZE 0.8678, MiniBEHAVIOR 0.8406, FrozenLAKE 0.6460. MVoT: MAZE 0.9295, MiniBEHAVIOR 0.9514, FrozenLAKE 0.8560. Ablation on token-discrepancy loss (MVoT w/o L_D) shows drops: MAZE 0.7468 (vs 0.9295 with L_D), MiniBEHAVIOR 0.7228 (vs 0.9514), FrozenLAKE 0.7260 (vs 0.8560). Visualization-quality metrics for MAZE/MiniBEHAVIOR (V-Acc, V-Red, V-Steps, V-Ratio) also reported in Table 3 (e.g., MAZE V-Acc 0.9339 with L_D).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Multiple lines of evidence: (1) Quantitative task accuracy on grid-based navigation and manipulation tasks showing high performance indicates successful dynamic spatial-tracking (e.g., MVoT achieves 0.95+ on MAZE/MiniBEHAVIOR and 0.856 on FrozenLAKE). (2) Visual-rationale metrics (V-Acc, V-Steps, V-Ratio) quantify correctness of image-based intermediate state visualizations (e.g., MAZE V-Acc 0.9339 with token-discrepancy loss). (3) Ablations: comparing CoT with/without textual layout and Interleaved vs MVoT shows that explicit visual thought improves robustness when environment complexity rises. (4) Qualitative examples and error analyses (e.g., CoT coordinate errors, visualization error types) demonstrate the model is tracking and manipulating spatial state.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Direct, CoT, and Interleaved baselines are directly compared. CoT outperforms on MAZE and MiniBEHAVIOR (CoT ~0.98) but fails on more complex FrozenLAKE (0.6148), while MVoT provides a more stable performance across tasks (MAZE 0.9295, MiniBEHAVIOR 0.9514, FrozenLAKE 0.8560). Interleaved training (images present but image tokens not supervised) improves over Direct on some tasks but underperforms on FrozenLAKE. Upper-bound ensemble combining CoT and MVoT predictions yields near 100% on MAZE/MiniBEHAVIOR and 0.9246 on FrozenLAKE (Table 4), showing complementarity. No human baselines reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: (1) Visualization failure modes — wrong visualizations, redundant patterns, blur/loss of fine-grained details especially in FrozenLAKE; (2) Without token-discrepancy loss, generated visuals degrade and task accuracy drops substantially; (3) MVoT's visual generation introduces computational overhead at inference; (4) Some generated visualizations reconstruct task-irrelevant background details rather than intended modifications; (5) CoT remains sensitive to environment complexity and textual coordinate errors (e.g., 90.8% of CoT mistakes on FrozenLAKE due to inaccurate hole coordinate descriptions), demonstrating that text-only reasoning can fail on detailed visual patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>extra_notes</strong></td>
                            <td>MVoT training used token-discrepancy loss to align visual token predictions with the visual embedding space; input augmentation via repeated tokenization/detokenization was applied to improve robustness. All Anole-7B fine-tuning used LoRA on MI300X hardware for 40 epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagine while Reasoning in Space: Multimodal Visualization-of-Thought', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8321.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8321.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-40 / GPT-4o (API)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-40 / GPT-4o (OpenAI proprietary multimodal model, API access)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary large multimodal model accessed via Azure API and evaluated zero-shot on the same spatial reasoning tasks; also evaluated with CoT prompting and with 'visual thoughts' provided from the fine-tuned MVoT model as an external pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hello gpt-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-40 (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary multimodal model accessed via Azure API (2024-07-01 version referenced). Evaluated in zero-shot with Direct prompting, zero-shot with CoT prompting, and with external visual thoughts provided (i.e., receiving MVoT-generated visualizations after generating verbal thoughts in a ReAct-style pipeline). Specific architecture/parameter details are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>MAZE; MiniBEHAVIOR; FrozenLAKE</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Same grid-based dynamic spatial reasoning tasks as above (navigation, object interaction, hazards).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot inference via the Azure API across three variants: (a) Zero-shot Direct (no CoT), (b) Zero-shot CoT (textual chain-of-thought prompting), and (c) With Visual Thought* (the model receives MVoT-generated visual thoughts as input after its own verbal thought in a ReAct-style or pipeline manner). Outputs are textual final answers extracted by pattern match.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Zero-shot direct and CoT prompting (text-only). Additionally, a ReAct-style pipeline where GPT-40/GPT-4o is provided with visual thoughts produced by the fine-tuned MVoT model to augment reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported accuracies (Table 2): Zero-Shot Direct: MAZE 0.7100, MiniBEHAVIOR 0.4576, FrozenLAKE 0.4976. Zero-Shot CoT: MAZE 0.7386, MiniBEHAVIOR 0.4676, FrozenLAKE 0.4664. With Visual Thought (MVoT visualizations supplied): MAZE 0.8556, MiniBEHAVIOR 0.6440, FrozenLAKE 0.8021. The paper also reports that providing MVoT visual thoughts to GPT-4o improves performance by over 15% accuracy across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Performance increases when given visual thoughts (e.g., MAZE from 0.71→0.8556, FrozenLAKE from ~0.50→0.8021) indicate that the model benefits from multimodal visual state information; however, zero-shot CoT alone yields only modest improvements, and performance remains substantially lower than fine-tuned MVoT/CoT on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to Anole-7B variants: GPT-40 zero-shot lags behind fine-tuned Anole variants (Direct and CoT); when provided MVoT visual thoughts, GPT-40's performance improves markedly but still generally remains below the best fine-tuned models on the tasks (MVoT fine-tuned Anole achieves higher or comparable performance, particularly on FrozenLAKE). No human comparison is given.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Zero-shot GPT-40 struggles on these controlled spatial tasks, with low accuracy especially on MiniBEHAVIOR and FrozenLAKE; CoT prompting alone gives little benefit for complex visual patterns. The model benefits from externally supplied visual thoughts, indicating limited native capacity to imagine interleaved visual states without fine-tuning or explicit multimodal supervision. The paper notes dependence on external visual modules/tools in ReAct-style pipelines limits expressiveness and does not prove internal multimodal reasoning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>extra_notes</strong></td>
                            <td>The authors used GPT-40 (2024-07-01 Azure API) and also reference GPT-4o; they note improvements when GPT-4o is fed MVoT visualizations (a plug-in style approach). No parameter counts or training details for GPT-40/GPT-4o are provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagine while Reasoning in Space: Multimodal Visualization-of-Thought', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mind's eye of LLMs: Visualization-of-thought elicits spatial reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Is a picture worth a thousand words? delving into spatial reasoning for vision language models <em>(Rating: 2)</em></li>
                <li>VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs <em>(Rating: 2)</em></li>
                <li>Chameleon: Mixed-modal early-fusion foundation models <em>(Rating: 2)</em></li>
                <li>Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8321",
    "paper_id": "paper-2d0b030d314a5aa8feaa03695e8471270130bdf9",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "Anole-7B (fine-tuned)",
            "name_full": "Anole 7B (autoregressive multimodal model, fine-tuned for MVoT)",
            "brief_description": "Anole-7B is an open autoregressive multimodal model (tuned on Chameleon) used as the experimental backbone in this paper and fine-tuned with LoRA to generate interleaved text and image 'thoughts' under different prompting/training regimens including Direct, CoT, Interleaved, and MVoT.",
            "citation_title": "Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation.",
            "mention_or_use": "use",
            "model_name": "Anole-7B",
            "model_description": "An autoregressive multimodal LLM capable of interleaved image-text generation; in this work it is the experimental backbone (tuned on Chameleon) and fine-tuned with LoRA in instruction-tuning fashion to support MVoT and baseline variants.",
            "model_size": "7B",
            "puzzle_name": "MAZE; MiniBEHAVIOR (InstallingAPrinter); FrozenLAKE",
            "puzzle_type": "Grid-based dynamic spatial reasoning tasks: MAZE (abstract maze navigation); MiniBEHAVIOR InstallingAPrinter (embodied interaction with objects on a grid); FrozenLAKE (grid with fine-grained visual patterns and holes requiring safe navigation).",
            "task_setup": "Models receive an initial image of the grid environment plus a sequence of actions. Output is a multiple-choice final outcome (A/B/C/D depending on task). Variants tested: Direct prompting (no intermediate reasoning), Chain-of-Thought (CoT) textual reasoning (with or without textual environment layout captions/coordinates), Interleaved training (text+image pairs but loss only on text tokens), and MVoT (model trained to generate interleaved visual and verbal rationale images and text; loss applied to all token types including image tokens with token-discrepancy loss). Extraction of final answer performed by pattern matching 'the answer is'.",
            "mechanisms_or_strategies": "Direct, CoT prompting (explicit textual step-by-step coordinates and state updates), Interleaved training (images present in training but image tokens excluded from loss), and MVoT (Multimodal Visualization-of-Thought) where the model generates visual thoughts (image-token visualizations) interleaved with verbal CoT steps. MVoT training adds token-discrepancy loss to align predicted image-token distributions with visual embedding similarities; training uses LoRA tuning of Anole-7B on MI300X for 40 epochs and input augmentation (repeated tokenization/detokenization).",
            "performance_metrics": "Reported accuracies (fraction correct) on test sets (Table 2): Direct: MAZE 0.7171, MiniBEHAVIOR 0.7250, FrozenLAKE 0.7788. CoT: MAZE 0.9792, MiniBEHAVIOR 0.9812, FrozenLAKE 0.6148. Interleaved: MAZE 0.8678, MiniBEHAVIOR 0.8406, FrozenLAKE 0.6460. MVoT: MAZE 0.9295, MiniBEHAVIOR 0.9514, FrozenLAKE 0.8560. Ablation on token-discrepancy loss (MVoT w/o L_D) shows drops: MAZE 0.7468 (vs 0.9295 with L_D), MiniBEHAVIOR 0.7228 (vs 0.9514), FrozenLAKE 0.7260 (vs 0.8560). Visualization-quality metrics for MAZE/MiniBEHAVIOR (V-Acc, V-Red, V-Steps, V-Ratio) also reported in Table 3 (e.g., MAZE V-Acc 0.9339 with L_D).",
            "evidence_of_spatial_reasoning": "Multiple lines of evidence: (1) Quantitative task accuracy on grid-based navigation and manipulation tasks showing high performance indicates successful dynamic spatial-tracking (e.g., MVoT achieves 0.95+ on MAZE/MiniBEHAVIOR and 0.856 on FrozenLAKE). (2) Visual-rationale metrics (V-Acc, V-Steps, V-Ratio) quantify correctness of image-based intermediate state visualizations (e.g., MAZE V-Acc 0.9339 with token-discrepancy loss). (3) Ablations: comparing CoT with/without textual layout and Interleaved vs MVoT shows that explicit visual thought improves robustness when environment complexity rises. (4) Qualitative examples and error analyses (e.g., CoT coordinate errors, visualization error types) demonstrate the model is tracking and manipulating spatial state.",
            "comparisons": "Direct, CoT, and Interleaved baselines are directly compared. CoT outperforms on MAZE and MiniBEHAVIOR (CoT ~0.98) but fails on more complex FrozenLAKE (0.6148), while MVoT provides a more stable performance across tasks (MAZE 0.9295, MiniBEHAVIOR 0.9514, FrozenLAKE 0.8560). Interleaved training (images present but image tokens not supervised) improves over Direct on some tasks but underperforms on FrozenLAKE. Upper-bound ensemble combining CoT and MVoT predictions yields near 100% on MAZE/MiniBEHAVIOR and 0.9246 on FrozenLAKE (Table 4), showing complementarity. No human baselines reported.",
            "limitations_or_failure_cases": "Reported limitations include: (1) Visualization failure modes — wrong visualizations, redundant patterns, blur/loss of fine-grained details especially in FrozenLAKE; (2) Without token-discrepancy loss, generated visuals degrade and task accuracy drops substantially; (3) MVoT's visual generation introduces computational overhead at inference; (4) Some generated visualizations reconstruct task-irrelevant background details rather than intended modifications; (5) CoT remains sensitive to environment complexity and textual coordinate errors (e.g., 90.8% of CoT mistakes on FrozenLAKE due to inaccurate hole coordinate descriptions), demonstrating that text-only reasoning can fail on detailed visual patterns.",
            "extra_notes": "MVoT training used token-discrepancy loss to align visual token predictions with the visual embedding space; input augmentation via repeated tokenization/detokenization was applied to improve robustness. All Anole-7B fine-tuning used LoRA on MI300X hardware for 40 epochs.",
            "uuid": "e8321.0",
            "source_info": {
                "paper_title": "Imagine while Reasoning in Space: Multimodal Visualization-of-Thought",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "GPT-40 / GPT-4o (API)",
            "name_full": "GPT-40 / GPT-4o (OpenAI proprietary multimodal model, API access)",
            "brief_description": "A proprietary large multimodal model accessed via Azure API and evaluated zero-shot on the same spatial reasoning tasks; also evaluated with CoT prompting and with 'visual thoughts' provided from the fine-tuned MVoT model as an external pipeline.",
            "citation_title": "Hello gpt-4o.",
            "mention_or_use": "use",
            "model_name": "GPT-40 (GPT-4o)",
            "model_description": "Proprietary multimodal model accessed via Azure API (2024-07-01 version referenced). Evaluated in zero-shot with Direct prompting, zero-shot with CoT prompting, and with external visual thoughts provided (i.e., receiving MVoT-generated visualizations after generating verbal thoughts in a ReAct-style pipeline). Specific architecture/parameter details are not provided in this paper.",
            "model_size": null,
            "puzzle_name": "MAZE; MiniBEHAVIOR; FrozenLAKE",
            "puzzle_type": "Same grid-based dynamic spatial reasoning tasks as above (navigation, object interaction, hazards).",
            "task_setup": "Zero-shot inference via the Azure API across three variants: (a) Zero-shot Direct (no CoT), (b) Zero-shot CoT (textual chain-of-thought prompting), and (c) With Visual Thought* (the model receives MVoT-generated visual thoughts as input after its own verbal thought in a ReAct-style or pipeline manner). Outputs are textual final answers extracted by pattern match.",
            "mechanisms_or_strategies": "Zero-shot direct and CoT prompting (text-only). Additionally, a ReAct-style pipeline where GPT-40/GPT-4o is provided with visual thoughts produced by the fine-tuned MVoT model to augment reasoning.",
            "performance_metrics": "Reported accuracies (Table 2): Zero-Shot Direct: MAZE 0.7100, MiniBEHAVIOR 0.4576, FrozenLAKE 0.4976. Zero-Shot CoT: MAZE 0.7386, MiniBEHAVIOR 0.4676, FrozenLAKE 0.4664. With Visual Thought (MVoT visualizations supplied): MAZE 0.8556, MiniBEHAVIOR 0.6440, FrozenLAKE 0.8021. The paper also reports that providing MVoT visual thoughts to GPT-4o improves performance by over 15% accuracy across tasks.",
            "evidence_of_spatial_reasoning": "Performance increases when given visual thoughts (e.g., MAZE from 0.71→0.8556, FrozenLAKE from ~0.50→0.8021) indicate that the model benefits from multimodal visual state information; however, zero-shot CoT alone yields only modest improvements, and performance remains substantially lower than fine-tuned MVoT/CoT on some tasks.",
            "comparisons": "Compared to Anole-7B variants: GPT-40 zero-shot lags behind fine-tuned Anole variants (Direct and CoT); when provided MVoT visual thoughts, GPT-40's performance improves markedly but still generally remains below the best fine-tuned models on the tasks (MVoT fine-tuned Anole achieves higher or comparable performance, particularly on FrozenLAKE). No human comparison is given.",
            "limitations_or_failure_cases": "Zero-shot GPT-40 struggles on these controlled spatial tasks, with low accuracy especially on MiniBEHAVIOR and FrozenLAKE; CoT prompting alone gives little benefit for complex visual patterns. The model benefits from externally supplied visual thoughts, indicating limited native capacity to imagine interleaved visual states without fine-tuning or explicit multimodal supervision. The paper notes dependence on external visual modules/tools in ReAct-style pipelines limits expressiveness and does not prove internal multimodal reasoning capability.",
            "extra_notes": "The authors used GPT-40 (2024-07-01 Azure API) and also reference GPT-4o; they note improvements when GPT-4o is fed MVoT visualizations (a plug-in style approach). No parameter counts or training details for GPT-40/GPT-4o are provided in the paper.",
            "uuid": "e8321.1",
            "source_info": {
                "paper_title": "Imagine while Reasoning in Space: Multimodal Visualization-of-Thought",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mind's eye of LLMs: Visualization-of-thought elicits spatial reasoning in large language models",
            "rating": 2,
            "sanitized_title": "minds_eye_of_llms_visualizationofthought_elicits_spatial_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Is a picture worth a thousand words? delving into spatial reasoning for vision language models",
            "rating": 2,
            "sanitized_title": "is_a_picture_worth_a_thousand_words_delving_into_spatial_reasoning_for_vision_language_models"
        },
        {
            "paper_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
            "rating": 2,
            "sanitized_title": "vsp_assessing_the_dual_challenges_of_perception_and_reasoning_in_spatial_planning_tasks_for_vlms"
        },
        {
            "paper_title": "Chameleon: Mixed-modal early-fusion foundation models",
            "rating": 2,
            "sanitized_title": "chameleon_mixedmodal_earlyfusion_foundation_models"
        },
        {
            "paper_title": "Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation.",
            "rating": 2,
            "sanitized_title": "anole_an_open_autoregressive_native_large_multimodal_models_for_interleaved_imagetext_generation"
        }
    ],
    "cost": 0.015091249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Imagine while Reasoning in Space: Multimodal Visualization-of-Thought</h1>
<p>Chengzu Li ${ }^{1,2, \star \dagger}$ Wenshan Wu ${ }^{1, \star}$ Huanyu Zhang ${ }^{1,3, \dagger}$ Yan Xia ${ }^{1}$<br>Shaoguang Mao ${ }^{1}$ Li Dong ${ }^{1}$ Ivan Vulić ${ }^{2}$ Furu Wei ${ }^{1}$<br>https://aka.ms/GeneralAI<br>${ }^{1}$ Microsoft Research<br>${ }^{2}$ Language Technology Lab, University of Cambridge<br>${ }^{3}$ Institute of Automation, Chinese Academy of Sciences</p>
<h4>Abstract</h4>
<p>Chain-of-Thought (CoT) prompting has proven highly effective for enhancing complex reasoning in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks. Nonetheless, human cognition extends beyond language alone, enabling the remarkable capability to think in both words and images. Inspired by this mechanism, we propose a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by generating image visualizations of their reasoning traces. To ensure highquality visualization, we introduce token discrepancy loss into autoregressive MLLMs. This innovation significantly improves both visual coherence and fidelity. We validate this approach through several dynamic spatial reasoning tasks. Experimental results reveal that MVoT demonstrates competitive performance across tasks. Moreover, it exhibits robust and reliable improvements in the most challenging scenarios where CoT fails. Ultimately, MVoT establishes new possibilities for complex reasoning tasks where visual thinking can effectively complement verbal reasoning.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Multimodal Visualization-of-Thought (MVoT) enables Multimodal Large Language Models (MLLMs) to generate interleaved reasoning traces across different modalities. While traditional CoT relies solely on verbal thought, MVoT facilitates visual thought to visualize the reasoning traces. This reasoning paradigm resembles human cognition to think in words and images seamlessly.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>1 Introduction</p>
<p>Chain-of-Thought (CoT) prompting has substantially enhanced the reasoning capacity of Large Language Models (LLMs) [JSM+23, Ope23, DJP+24]. By generating explicit reasoning traces, CoT enables the models to articulate their thought processes step-by-step. This advancement has enabled step-by-step mathematical reasoning, logical deduction [LTS24], and advanced problem-solving capabilities [ZMG+24]. However, its performance deteriorates significantly when confronted with complex spatial reasoning tasks [WMS+24, LZZ+24, RDT+24].</p>
<p>Recent research efforts have extended CoT to multimodal models through two primary approaches. The first approach utilizes two-stage strategies that initially extract image information through methods such as captioning [ZZL+24], scene-graph generation [MHDH24], or bounding-box detection [LYC+24] before conducting reasoning. The second approach implements ReAct-style pipelines [YZY+23] that leverage external tools such as code interpreters or specialized vision models [YLW+23, HSF+24, ZZH+24], to obtain image observations from the environment. While these pipelines successfully handle both text and image input, they remain heavily dependent on separate visual modules or external toolsets. This dependency complicates their adaptation to advanced and more complex spatial reasoning tasks.</p>
<p>Human cognition, however, transcends language (and text) alone, demonstrating the capacity to think in both words and images seamlessly. The dual-coding theory [Pai91] and working memory model [Bad92] explain this phenomenon, positing that humans process information through both verbal and non-verbal channels. This dual processing capability is crucial for reasoning and guides decision-making and behavior. For instance, humans naturally form and manipulate mental images to understand the physical world and conceptualize visual outcomes [MK09].</p>
<p>In the context of spatial reasoning, similar deficiency of LLMs and MLLMs, thinking in a single text modality (termed verbal henceforth) has been observed. Based on verbal CoT, the Visualization-of-Thought (VoT) [WMZ+24] elicits the reasoning process by introducing textual 'visualizations' as mental images in spatial reasoning tasks. Put simply, VoT builds visualizations via simplified, text proxies. Besides this too simplistic representation of visualization, another significant limitation, which holds both for CoT as well as for VoT, is the dependence on purely textual representation of the reasoning paths. This reliance becomes problematic in complex multimodal tasks, where textual representations often fail to capture the intricate visual patterns and spatial layouts of images [HSF+24]. Users frequently find it challenging to interpret the reasoning process without clear and intuitive visual illustrations that complement textual representation. Furthermore, it remains unclear whether MLLMs can engage in genuine reasoning within a visual space, in addition to thinking with textual utterances. ReAct-style methods generate the intermediate visual thoughts with external sets of tools for MLLMs to empower reasoning with multimodal information. However, we argue that the enhancement in task performance doesn’t stem from the model itself 'reasoning’ in multimodality and the expressiveness is still limited by the functionality of tools. Given the reasons above, we pose a key question: can MLLMs imagine in visual modality while reasoning?</p>
<p>In parallel, recent research has expanded beyond multimodal understanding of visual inputs to include multimodal generation, where foundation models can also produce outputs in the visual modality. This advancement has led to the development of sophisticated systems such as Chameleon [Cha24], Transfusion [ZYB+24], Gemini 2 [Dee24], and LatentLM [SBW+24]. These multimodal-native models demonstrate proficiency in both interpreting and producing high-quality outputs across textual and visual domains. The emerging capability of multimodal generation opens new possibilities for extending verbal reasoning to native visual thinking, enabling the visualization of reasoning traces through images.</p>
<p>Building upon these advancement, we propose Multimodal Visualization-of-Thought (MVoT). It leverages multimodal-native architectures to transcend the text-form thinking into multimodal native reasoning through generating image visualizations of their reasoning traces. This reasoning paradigm enables the model to create reasoning traces and 'think’ in words and images in combination seamlessly, while avoiding the potential errors being introduced in captioning the images. By incorporating native visual thought during reasoning process, MVoT offers more straightforward illustrations of the reasoning process and simultaneously enhances both reasoning quality and model interpretability.</p>
<p>Specifically, in this work, we implement MVoT through fine-tuning an established autoregressive MLLM: Chameleon-7B [Cha24]. To enhance visualization quality during reasoning, we introduce token discrepancy loss that bridges the gap between separately trained tokenizers. We validate MVoT’s effectiveness through extensive experiments across three dynamic spatial reasoning tasks. MAZE [ISS+23] and MiniBehavior [JHH+23] focus on interactions with spatial layouts. FrozenLake [Bro16] emphasizes</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustrations of MVoT reasoning process. The interleaved verbal thoughts and visual thoughts are generated by MLLM seamlessly.
fine-grained pattern recognition in dynamic environments. Experimental results demonstrate that MVoT achieves competitive performance across tasks, outperforming traditional CoT by over $20 \%$ in challenging scenarios.</p>
<p>The main contributions of this paper include:</p>
<ul>
<li>We propose Multimodal Visualization-of-Thought (MVoT), a multimodal native reasoning paradigm that unifies text and vision within the reasoning traces. To our knowledge, it's the first to naturally generate visual thought as part of the reasoning process. It establishes new possibilities for complex tasks where visual thinking effectively complements verbal reasoning.</li>
<li>We implement MVoT in Chameleon-7B and introduce token discrepancy loss in auto-regressive MLLM to bridge the gap between separately trained tokenizer.</li>
<li>We conduct comprehensive experiments and ablation studies across three spatial reasoning tasks with newly collected datasets, demonstrating that MVoT exhibits superior adaptability and robustness compared to CoT in complex scenarios.</li>
</ul>
<h1>2 Multimodal Visualization-of-Thought (MVoT)</h1>
<p>Humans often create mental images to inform decision-making. Rather than relying on verbal thoughts as text proxies to mimic these mental images, MVoT enables models to reason in multimodality by generating image visualizations as their visual thoughts. By combining thoughts in both modalities, this novel reasoning paradigm offers a more intuitive and effective way to elicit the multimodal reasoning process with enhanced expressiveness.</p>
<h3>2.1 Formulation</h3>
<p>We formulate the process of MVoT as follows. Given a multimodal input sequence, the model is expected to generate interleaved multimodal thoughts as part of the reasoning process and ultimately produce a final answer. Let $\mathcal{P}<em 1="1">{\theta}$ represent a pre-trained MLLM with parameters $\theta, x$ denote a multimodal input sequence, $z$ and $v$ a language sequence of verbal thoughts and an image sequence of visual thoughts, respectively.
In multi-hop spatial reasoning tasks with input $x$, CoT prompting generates intermediate steps $\hat{z}</em>}, \cdots, \hat{z<em i="i">{m}$, where each $\hat{z}</em>}$ is sampled sequentially based on the inputs and previous generated steps. The final output is concluded based on all prior steps. MVoT enhances this process by adding a image visualization $v_{i}$ to each intermediate step $z_{i}$, then the subsequent step $z_{i+1}$ is sampled conditioned on prior steps $\hat{z<em i="i">{1} \cdots \hat{z}</em>}$ and visualizations $\hat{v<em i="i">{1} \cdots \hat{v}</em>$, as shown in Figure 1.</p>
<p>As defined in the Equation 1 and 2, it forms interleaved reasoning traces and image visualizations.</p>
<p>$\hat{v}<em _theta="\theta">{i}$ $\sim\mathcal{P}</em>}\left(v_{i}\mid\hat{z<em 1="1">{1},\hat{v}</em>},\cdots,\hat{v<em i="i">{i-1},\hat{z}</em>\right)$ (1)
$\hat{z}<em _theta="\theta">{i+1}$ $\sim\mathcal{P}</em>}\left(z_{i+1}\mid x,\hat{z<em 1="1">{1},\hat{v}</em>},\cdots,\hat{z<em i="i">{i},\hat{v}</em>\right)$</p>
<p>To empower MLLMs with MVoT capabilities, we train the model on multimodal inputs $x$ and their corresponding output labels, which include multimodal rationales $z_{1},v_{1}\cdots z_{n}$, $v_{n}$ and the final answer. This training strategy enables the model to learn interleave verbal reasoning steps and corresponding visual thoughts, enhancing its ability to handle complex reasoning tasks that require thinking in multimodality.</p>
<h3>2.2 Training with Autoregressive MLLMs</h3>
<p>In the following section, we focus on autoregressive MLLMs with discrete image tokens for both training and inference. However, as a reasoning paradigm, MVoT can be extended to other model architectures and modalities, provided they meet the requirements for interleaved multimodal generation.</p>
<h4>Multimodal Sequence Modeling</h4>
<p>As shown in 3, we follow the architecture of Chameleon [Cha24], which leverages a unified Transformer to process both image and text tokens. The architecture integrates two tokenizers: an image tokenizer based on ERO21 and a text tokenizer, which convert images and text into discrete token sequences, respectively. The image tokenizer uses a discrete codebook to encode input images into a sequence of image tokens, while the text tokenizer maps textual data into corresponding token sequences. These token sequences are concatenated and processed by a causal transformer model.</p>
<p>Notation We denote MLLM’s codebook as $\mathcal{C}\in\mathbb{R}^{N\times D}$, where $N$ is the number of codebook entries, and $D$ is the dimensionality of codebook entries. Let $t_{\mathrm{vis}}$ and $e_{\mathrm{vis}}$ denote the visual codebook indices and embeddings. The predicted values are indicated with a hat symbol.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Illustration of MVoT training with token discrepancy loss $\mathcal{L}_{D}$.</p>
<h4>Token Discrepancy Loss</h4>
<p>While language modeling unifies both text tokens and image tokens within a single autoregressive model, the discrepancy between these separately trained tokenizers can degrade the quality of generated images. To mitigate this issue, we introduce token discrepancy loss into the autoregressive MLLM architecture, as shown in Figure 3. This loss design bridges the gap between language modeling and visual embedding space while ensuring that gradients remain intact.</p>
<p>Token discrepancy loss $\mathcal{L}<em D="D">{D}$ minimizes the discrepancy between the predictions and labels in visual embedding space. Equation 3 and 4 illustrate the token discrepancy loss $\mathcal{L}</em>$ and other image tokens is measured using their pairwise distances in the visual embedding space. Specifically, the similarity is determined by the mean squared error (MSE), as described in Equation 3, where larger distances indicate lower similarity.}$. To capture relationship among image tokens, we first compute the similarity matrix $\mathcal{S}\in\mathbb{R}^{N\times N}$. The similarity between $t_{\mathrm{vis}_{i}</p>
<p>$\mathcal{S}<em _mathrm_vis="\mathrm{vis">{t</em><em _mathrm_vis="\mathrm{vis">{i}}}=\left[\operatorname{MSE}\left(e</em><em _mathrm_vis="\mathrm{vis">{i}},e</em><em _mathrm_vis="\mathrm{vis">{1}}\right), \cdots, \operatorname{MSE}\left(e</em><em _mathrm_vis="\mathrm{vis">{i}},e</em>$ (3)}_{N}}\right)\right] \in \mathbb{R}^{1 \times N</p>
<p>The model predicts the probability distribution $\mathrm{P}\left(t_{i}\right) \in \mathbb{R}^{1 \times N}$ for the $i$-th image token over the image token vocabulary. $\mathcal{L}<em _mathrm_vis="\mathrm{vis">{D}$ penalizes probabilities assigned to tokens that deviate significantly from their corresponding label $t</em><em D="D">{i}}$ in the visual embedding space. By aligning the visual embeddings of predictions with those of the ground-truth tokens, $\mathcal{L}</em>$ aims to enhance the quality of the generated images.</p>
<p>$\mathcal{L}<em i="1">{D}=\sum</em>}^{n} \mathcal{S<em _mathrm_vis="\mathrm{vis">{t</em><em i="i">{i}}} \cdot \mathrm{P}\left(t</em>\right)$</p>
<p>Training The causal Transformer model is fine-tuned using the next-token prediction objective, while the image tokenizer and text tokenizer are kept frozen throughout the process. With token discrepancy loss $\mathcal{L}<em C="C">{D}$ for image tokens as defined above, and the cross-entropy loss $\mathcal{L}</em>$ for both text tokens and image tokens, the loss function is described as follows.</p>
<p>$$
\mathcal{L}=\mathcal{L}<em D="D">{C}+\mathcal{L}</em>
$$</p>
<h1>3 Spatial Reasoning Tasks</h1>
<p>Motivated by RDT ${ }^{+} 24$, we select three dynamic reasoning tasks in space that requires the model to dynamically locate objects, understand how the environment evolves, and predict outcomes when actions are applied. To assess the MLLMs with controllability for analysis, we ground these spatial reasoning tasks within grid-based environments: MAZE navigation [ISS ${ }^{+} 23$ ], InstallingAPrinter from MiniBEHAVIOR [JHH $\left.{ }^{+} 23\right]$ and FrozenLAKE simulation [Bro16, WZS $\left.{ }^{+} 24\right]$. MAZE navigates in abstract mazes and MiniBEHAVIOR includes the spatial properties of the objects and interactions with the environmental layouts, while FrozenLAKE contains fine-grained pattern details instead of abstract symbols in the image. The tasks encompass various levels of controlled complexity as the abstraction of the real world.</p>
<h3>3.1 MAZE</h3>
<p>In maze navigation, the model is provided with an initial image describing the maze layout and the starting position of the agent in the maze. The agent should navigate through the maze and reach the destination. In our work, we define the MAZE task as follows. Given an initial maze with a starting point and a sequence of actions, the model is supposed to follow the actions and predict the final destination chosen from the locations labeled A, B, C or D in the maze.</p>
<h3>3.2 MiniBehAvior</h3>
<p>We select InstallingAPrinter from MiniBehAvior [JHH $\left.{ }^{+} 23\right]$ embodied AI tasks as a representative scenario for our experiments. It requires the agent to first locate the printer on the floor, pick it up and carry it to the table to toggle it on. As shown in Figure 2, the printer is represented by a small printer symbol, the agent is represented by a red triangle and the table is represented by a brown area. Aligned with previous settings, we define the MiniBEHAVIOR task in our work as follows. Given a sequence of actions and an environment layout in image, the model should predict the outcome of conducting the actions. The outcomes include whether the agent successfully executes tasks, such as picking up the printer or placing it on the table, and whether objects are missing from the environment. This task expands the action space of MAZE by introducing the interaction with the environment, while maintains a similar level of perception difficulty, relying on simple symbolic representations.</p>
<h3>3.3 FrozenLAKE</h3>
<p>FrozenLAKE is initially proposed by WZS ${ }^{+} 24$ implemented with Gym [Bro16], which is similar to maze navigation but with more complex patterns and details. As shown in Figure 2, it simulates a gridbased frozen lake. The agent is supposed to start from the designated position and reach the destination safely without falling into the 'holes'. Based on the spatial reasoning task from WZS ${ }^{+} 24$, we define the FrozenLAKE task as follows. Given a sequence of actions and the grid-based frozen lake layout with the start and goal position, the model has to determine the consequence of following the given actions. Compared to MAZE and MiniBehAvior, FrozenLAKE contains more diverse image details and its environment is more complex considering the number of key entities such as holes.</p>
<h2>4 Experiments</h2>
<h3>4.1 Experimental Setups</h3>
<p>Data. We construct datasets for three spatial reasoning tasks, as described in Section 3, encompassing varying levels of complexity in patterns and action spaces. The dataset statistics are presented in Table 1.</p>
<p>Detailed information on data collection is provided in Appendix B. We structure the data as interleaved text-image pair to train MVoT, as defined in Section 2.</p>
<p>Model and Experiments. We use Anole7B [CSML24] model as the backbone in our work. Anole is tuned on Chameleon [Cha24] and can generate interleaved text and image, making it well-suited for MVoT. We only tune part of the model's parameters with LoRA [HSW ${ }^{+} 21$ ] in an instruction tuning manner [LLWL23] on MI300X for 40 epochs, where only the loss from the predictions is optimized. Additionally, we evaluate the performance of GPT-40 [Ope24] with zero-shot inference, CoT and MVoT in the ReAct-style</p>
<p>Table 1: Statistics of the collected datasets, covering varying levels of complexity in actions and patterns.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">MAZE</th>
<th style="text-align: center;">MiniBehAvior</th>
<th style="text-align: right;">FrozenLaKe</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Grid Sizes</td>
<td style="text-align: center;">$3-6$</td>
<td style="text-align: center;">$5-8$</td>
<td style="text-align: right;">$3-6$</td>
</tr>
<tr>
<td style="text-align: left;">Entity Types</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: right;">3</td>
</tr>
<tr>
<td style="text-align: left;">Entities Numbers</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: right;">7.16</td>
</tr>
<tr>
<td style="text-align: left;">Action Length</td>
<td style="text-align: center;">9.11</td>
<td style="text-align: center;">7.83</td>
<td style="text-align: right;">6.56</td>
</tr>
<tr>
<td style="text-align: left;">Action Types</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">7</td>
<td style="text-align: right;">4</td>
</tr>
<tr>
<td style="text-align: left;">Pattern Details</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: right;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Train Set Size</td>
<td style="text-align: center;">5007</td>
<td style="text-align: center;">6400</td>
<td style="text-align: right;">6846</td>
</tr>
<tr>
<td style="text-align: left;">Test Set Size</td>
<td style="text-align: center;">1255</td>
<td style="text-align: center;">1604</td>
<td style="text-align: right;">1664</td>
</tr>
</tbody>
</table>
<p>plates and hyperparameters for each task and system variant are provided in Appendix C.
We compare the MVoT with the following families of system variants: 1) Direct Prompting (Direct): The model directly outputs the choice index without intermediate reasoning. 2) Chain-of-Thought (CoT): The model is instruction-tuned to reason step-by-step, incorporating coordinates and environment layout described in text, before concluding with the final answer. 3) Training with Interleaved Text-Image Pairs (Interleaved): This method follows the standard training approach for MLLMs, interleaving text and image data. However, Interleaved calculates the loss only on text tokens while excluding image tokens. In contrast, MVoT computes the loss across all token types. The comparisons among these system variants are summarized in Table 2.</p>
<p>Metrics We extract the predicted answer from model output by pattern matching with 'the answer is'. Accuracy for multiple-choice question answering is calculated by comparing the predicted choice with the ground-truth answer to determine correctness.</p>
<h1>4.2 Experimental Results</h1>
<p>MVoT outperforms Direct and GPT-40 with interpretability. Experimental results across all three simulation tasks reveal that Direct struggles with overfitting to spatial reasoning tasks, achieving an accuracy of approximately $70 \%$. GPT-40 performs even worse, both with and without CoT prompting, underscoring the inherent difficulty of these reasoning tasks. In contrast, MVoT demonstrates consistent improvements. It surpasses Direct by $7 \%$ on FrozenLaKe and achieves over $90 \%$ accuracy on both MAZE and MiniBehAvior. Beyond its superior performance, MVoT also provides verbal and visual thoughts of intermediate reasoning states. This feature enhances interpretability, offering a clearer and more intuitive understanding of its reasoning process compared to Direct.</p>
<p>MVoT achieves comparable or better performance than CoT with enhanced robustness. CoT achieves over $95 \%$ accuracy on the MAZE and MiniBehAvior by explicitly describing the environment layout and agent location with textual coordinates. However, it performs worse than the Direct baseline on FrozenLaKe. In contrast, MVoT demonstrates comparable performance on MAZE (92.95\%) and MiniBehAvior (95.14\%), while achieving a higher accuracy of $85.60 \%$ on FrozenLaKe compared to both Direct and CoT. This demonstrates better stability and robustness of MVoT than CoT.</p>
<p>Although CoT desmonstrates strong results on MAZE and MiniBEHAVIOR, it shows vulnerabilities and limitations in several aspects:
(1) CoT is sensitive to environment complexity. CoT underperforms on FrozenLaKe, where the environment is more complex due to the presence of additional key entities (e.g., holes), compared to the other tasks. Error analysis reveals that $90.80 \%$ of CoT's mistakes on FrozenLaKe stem from inaccurate coordinate descriptions of holes in the environment. An illustration of this type of error is presented in Figure 10 in the Appendix. In contrast, no such errors are observed in MAZE and MiniBehAvior. Furthermore, CoT's performance on FrozenLaKe deteriorates as grid size increases, dropping from 0.9401 on a $3 \times 3$ grid to 0.3911 on a $6 \times 6$ grid, as detailed in Table 15 in the Appendix.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Experimental results from different system variants across all tasks. Coord denotes whether the system is instructed to use textual coordinates during inference; Layout denotes whether the system first caption the environment with text for further inference; Image refers to whether the images of intermediate states are visible to the model during training. The best and second best performance are illustrated with corresponding colors. Methods with underlines are fine-tuned on our datasets. $\downarrow$ indicates worse performance than Direct baseline.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Training Variants</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Output</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Coord</td>
<td style="text-align: center;">Layout</td>
<td style="text-align: center;">Image</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAze</td>
<td style="text-align: center;">MiniBehavior</td>
<td style="text-align: center;">FrozenLaKe</td>
</tr>
<tr>
<td style="text-align: center;">GPT-40</td>
<td style="text-align: center;">Zero-Shot Direct</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">0.7100</td>
<td style="text-align: center;">0.4576</td>
<td style="text-align: center;">0.4976</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-Shot CoT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">0.7386</td>
<td style="text-align: center;">0.4676</td>
<td style="text-align: center;">0.4664</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">With Visual Thought ${ }^{*}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">0.8556</td>
<td style="text-align: center;">0.6440</td>
<td style="text-align: center;">0.8021</td>
</tr>
<tr>
<td style="text-align: center;">Anole 7B</td>
<td style="text-align: center;">Direct</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">0.7171</td>
<td style="text-align: center;">0.7250</td>
<td style="text-align: center;">0.7788</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">0.9792</td>
<td style="text-align: center;">0.9812</td>
<td style="text-align: center;">$0.6148 \downarrow$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">- w/o layout</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">$0.7023 \downarrow$</td>
<td style="text-align: center;">$0.6000 \downarrow$</td>
<td style="text-align: center;">$0.5974 \downarrow$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Interleaved</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">0.8678</td>
<td style="text-align: center;">0.8406</td>
<td style="text-align: center;">$0.6460 \downarrow$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MVoT</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Text, Image</td>
<td style="text-align: center;">0.9295</td>
<td style="text-align: center;">0.9514</td>
<td style="text-align: center;">0.8560</td>
</tr>
</tbody>
</table>
<ul>
<li>Visual thought is generated by Anole 7B implemented with MVoT.
(2) CoT relies heavily on textual description of the environment. CoT performs well on MAZE and MiniBeHAvior when the environment layout is accurately described through text. Captioning the environment layout simplifies the task into a textual reasoning process, eliminating the need for visual references. However, as shown in Table 2, when reasoning is performed using only the agent's coordinates without explicit textual descriptions of the environment, CoT consistently underperforms the Direct baseline. This reliance is particularly evident in FrozenLAKE, where flawed predictions stem from inaccurate environment descriptions. These limitations constrain the generalization and reliability of CoT, particularly in complex environments.
Meanwhile, MVoT doesn't have the ineffectiveness above.
(1) MVoT is more robust to environment complexity compared to CoT. MVoT maintains stable performance across varying grid sizes within each task, as shown in Table 15 in the Appendix. In FrozenLAKE, even as the environment becomes more complex with larger grid sizes and increased number of 'holes', as shown in Table 7, MVoT consistently achieves over $83 \%$ task performance. In contrast, CoT shows a significant decline in performance as environmental complexity rises, highlighting the robustness of MVoT in handling more challenging scenarios.
(2) MVoT demonstrates better interpretability than CoT. Rather than solely relying on textual descriptions, MVoT elicits reasoning process by visualizations, effectively mitigating potential errors introduced by inaccurate text-based captions for complex environments. Moreover, visual thought provides a more direct and interpretable way to track the reasoning process compared to aligning textual coordinates within images. This calls for the need of incorporating multimodal reasoning, leveraging both textual and other modalities, such as vision, rather than reasoning solely in text as the primary modality.</li>
</ul>
<p>Learning from interleaved multimodal rationales for better and grounded reasoning. Data-wise, our findings suggest that incorporating interleaved training data, even without generating visualizations, improves reasoning performance. Method-wise, MVoT achieves higher and more consistent improvements across all tasks. Compared between Direct and Interleaved, we observe that the inclusion of interleaved visualizations during training, despite not contributing directly to the loss, improves performance on MAze and MiniBehavior by over $10 \%$. This empirically indicates that these interleaved images helps the model in leveraging visual cues for reasoning during optimization. However, despite the advantages of the Interleaved paradigm, we witness a drop in task performance on FrozenLAKE. We conjecture that this may be attributed to the complexity of the visual cues in FrozenLAKE for Interleaved. This highlights the unique challenges of the FrozenLAKE task and illustrates the limitations of the traditional Interleaved training paradigm. In contrast, when interleaved multimodal rationales serve as supervision signals for visual tokens as in MVoT, the model explicitly grounds its reasoning by generating visualizations, leading to improved performance across all tasks. These findings provide valuable insights into leveraging interleaved multimodal thoughts to enhance reasoning with MLLMs and emphasize the need for further research.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Qualitative analysis illustrating: 1) FrozenLAKE visualization quality by comparing systems trained with and without token discrepancy loss $\left(\mathcal{L}_{D}\right)$; and 2) differences in reconstructed images using the image detokenizer on tokenized inputs.</p>
<p>Equip Proprietary Models with MVoT In addition to use a single 7B open-sourced model to generate multimodal thought and conclude the answer, MVoT also provides flexibility of being used as plug-ins for other proprietary models including those accessible via APIs. We provide GPT-4o with the visual thoughts from fine-tuned MVoT model after GPT-4o generates the verbal thought. We witness an improvement in performance by over $15 \%$ accuracy across all the tasks as shown in Table 2. We hope this work inspires further exploration and fosters advancements in multimodal agent reasoning with multimodal thoughts.
In summary, MVoT demonstrates its effectiveness in performance, better generalization than CoT in eliciting the reasoning state during spatial reasoning.</p>
<h1>5 Discussions and Ablations</h1>
<h3>5.1 Visualization Quality</h3>
<p>Note that during training, the model generates the next visual thought based on the previous golden image. On the other hand, MVoT recursively generate multimodal thoughts (texts and image visualizations) based on the previously generated thoughts, illustrating the difference between paradigms. We refer to these two approaches as 'image editing' and 'MVoT' in the following discussion. Given that MVoT operates through recursive generation, our focus in this section is primarily on discussing the visualization quality of MVoT.</p>
<p>Qualitative Analysis Figure 4 illustrate examples of correct and incorrect generated images for FrozenLAKE. More generated visualization are shown in Figure 7 and 8 in Appendix D.2. We classify errors of visualization generation into the following categories: (1) Wrong Visualization: The generated visualization is inaccurate. (2) Redundant Patterns: Unnecessary or irrelevant patterns are visualized outside the intended area for modification. Furthermore, in the FrozenLAKE task, we observe that generated image details often blur as pattern complexity increases compared to MAzE and MiniBEHAVIOR. Patterns, such as background details in FrozenLAKE MVoT visualizations, often show minor inconsistencies between generated visual thoughts. Similar differences are also noted between original images and reconstructed images through tokenization and detokenization. The variability frequently results in a loss or perturbation of fine-grained details and highlights limitations in the expressive capacity of MLLMs. These findings underscore the need for further research to improve the fidelity of image tokenization and generation in autoregressive MLLMs.</p>
<p>Quantitative Metrics To evaluate the quality of generated visual rationales, we define automatic evaluation metrics based on the identified types of errors:</p>
<ul>
<li>Visualization Accuracy (V-Acc.): Measures the accuracy of visualizing the intended modifications in the grid corresponding to the next action.</li>
</ul>
<p>Table 3: Quantitative metrics for MVoT visual thoughts with or without token discrepancy loss $\mathcal{L}_{D}$. Best results are highlighted in bold. Metrics with $\uparrow$ indicate that higher values mean better performance and vice versa.</p>
<table>
<thead>
<tr>
<th></th>
<th>Visualization Quality</th>
<th></th>
<th></th>
<th>Task Performance</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>V-Acc. $\uparrow$</td>
<td>V-Red. $\downarrow$</td>
<td>V-Steps $\uparrow$</td>
<td>V-Ratio $\uparrow$</td>
<td>Accuracy $\uparrow$</td>
</tr>
<tr>
<td>MAZE</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MVoT with $\mathcal{L}_{D}$</td>
<td>0.9339</td>
<td>0.1010</td>
<td>8.4439</td>
<td>0.9449</td>
<td>0.9295</td>
</tr>
<tr>
<td>MVoT without $\mathcal{L}_{D}$</td>
<td>0.6391</td>
<td>0.4931</td>
<td>5.6563</td>
<td>0.6930</td>
<td>0.7468</td>
</tr>
<tr>
<td>MiniBehavior</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MVoT with $\mathcal{L}_{D}$</td>
<td>0.9681</td>
<td>0.0419</td>
<td>6.7532</td>
<td>0.9618</td>
<td>0.9514</td>
</tr>
<tr>
<td>MVoT without $\mathcal{L}_{D}$</td>
<td>0.7939</td>
<td>0.2633</td>
<td>5.2333</td>
<td>0.7793</td>
<td>0.7228</td>
</tr>
</tbody>
</table>
<ul>
<li>Visualization Pattern Redundancy (V-Red.): Assesses the presence of unintended visual patterns in regions outside the targeted area of modification.</li>
<li>Visualization Correctness Step (V-Steps): the average length of first k consecutive correct visualizations within an action sequence.</li>
<li>Visualization Correctness Ratio (V-Ratio): the average proportion of first k consecutive correct visualizations across the action sequence. Due to the complexity of FROZENLAKE's image details, which makes automatic evaluation challenging, we only report quantitative results of visualizing the agent position in MAZE and MINIBEHAVIOR.</li>
</ul>
<p>Token discrepancy loss enhances accuracy and reduces redundancy in visualizations. Table 3 shows that MVoT, enhanced with token discrepancy loss $\left(\mathcal{L}<em D="D">{D}\right)$, produces highly accurate visualizations with minimal pattern redundancy. Even in recursive generation scenarios, MVoT with $\mathcal{L}</em>}$ achieves an average of $95 \%$ correct and consecutive visualizations during reasoning. In contrast, the absence of $\mathcal{L<em D="D">{D}$ significantly degrades the generation quality: without $\mathcal{L}</em>}$, MVoT frequently generates redundant patterns and fails to accurately capture state transitions. These results align with findings from the image-editing scenario, as illustrated in Figure 5, which tracks quantitative metrics for MAZE at various epochs. Furthermore, poor visualization quality negatively impacts task performance, as highlighted in the last column of Table 3 , emphasizing the critical role of high-quality visualizations in achieving better task outcomes. We also witness a performance drop on FROZENLAKE without token discrepancy loss from 0.8560 to 0.7260 . Empirical evidence suggests that incorporating visual embedding with $\mathcal{L<em C="C">{D}$ in addition to $\mathcal{L}</em>$ helps to bridge the gap between embeddings and improve the visual generation quality. This aligns with the findings by TPK24 which noted similar limitations in embedding alignment for MLLMs. Further insights into embedding discrepancies within autoregressive MLLM architectures are discussed in Section 5.2 and Figure 6.</p>
<h1>5.2 Further Discussions</h1>
<p>MVoT Complements CoT in Reasoning. To investigate whether MVoT and CoT share similar reasoning capabilities and fail on the same data, we combine their predictions and calculate the upper-bound performance. In this approach, a data point is considered correct if either MVoT or CoT generates the correct prediction. As shown in Table 4, the upper-bound performance reaches nearly $100 \%$ accuracy on</p>
<p>Table 4: Performance upper bounds achieved by combining predictions from CoT and MVoT across three tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">MAZE</th>
<th style="text-align: center;">MiniBehAviOR</th>
<th style="text-align: center;">FrozenLAKE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;">0.9792</td>
<td style="text-align: center;">0.9812</td>
<td style="text-align: center;">0.6148</td>
</tr>
<tr>
<td style="text-align: left;">MVoT</td>
<td style="text-align: center;">0.9295</td>
<td style="text-align: center;">0.9514</td>
<td style="text-align: center;">0.8560</td>
</tr>
<tr>
<td style="text-align: left;">Upperbound</td>
<td style="text-align: center;">0.9984</td>
<td style="text-align: center;">1.0000</td>
<td style="text-align: center;">0.9246</td>
</tr>
</tbody>
</table>
<p>Maze and MiniBehAviOR, and $92 \%$ accuracy on FrozenLAKE. These findings suggest that MVoT complements textual CoT by offering an alternative reasoning strategy, thus enabling the ensemble of different approaches to further enhance performance.</p>
<p>Image Tokenization in Autoregressive MLLMs To understand why token discrepancy loss $\mathcal{L}_{D}$ helps to improve the quality of generated visual thoughts, we examine the two sets of embeddings introduced in autoregressive MLLMs: token embeddings for language modeling and visual embeddings for image tokenization. These embeddings originate from distinct systems because the visual codebook, image tokenizer, and detokenizer are trained separately from the autoregressive decoder, which creates a potential discrepancy between the two embedding spaces. To investigate their alignment, we compare the two embedding sets from Chameleon [Cha24], LVM [BGM+24] and LlamaGen [SJC+24] by calculating the average overlap ratio of the top- $k$ similar tokens based on cosine similarity. Results in Figure 6 reveal that, on average, only one token overlaps among the top 10 similar tokens, and approximately $20 \%$ of tokens overlap among the top 50 similar tokens. To further illustrate this discrepancy, we replace the image tokens with their most similar tokens in the token embeddings and visual embeddings, reconstructing the images (Figure 9 in Appendix D.2). After 10 iterations of tokenization and detokenization, the reconstructed images based on token embeddings exhibit significant differences from the originals, such as altered colors and distorted symbols (e.g., the elf's appearance). This contrast highlights the misalignment between the two embedding systems. By introducing token discrepancy loss, we hope to bridge this gap, enabling better alignment between visual and token embeddings, thus allowing the model to generate higher-quality visual rationale.</p>
<h1>6 Related Work</h1>
<p>Multimodal Chain-of-Thought Reasoning Chain-of-Thought (CoT) [WWS+22] prompting has considerably enhanced the reasoning capacities of LLMs. To adapt CoT for multimodal models, recent research has explored various methodologies. Some investigations adopt a two-stage approach, where image information is initially transformed and grounded into text [ZZL+24], graph structure (e.g., scene graphs [MHDH24] or knowledge graphs [MMP+24]), or bounding boxes [LYC+24] before reasoning. Other studies use ReAct-style pipelines [YZY+23] that integrate external tools to process and reason with image observations. These tools include code interpreters and specialized vision models [YLW+23, HSF+24, ZZH+24, GCZ+24]. Although these approaches effectively manage both textual and visual inputs, they rely heavily on separate visual modules or toolsets which limits the expressiveness of the methods. To address these limitations, we propose MVoT, a novel reasoning method designed to leverage multimodal-native understanding and generative capabilities. MVoT enables the generation of interleaved reasoning traces across multiple modalities, providing a more integrated and flexible approach to multimodal reasoning with better interpretability and more robust reasoning quality .</p>
<p>Multimodal Spatial Reasoning Multimodal spatial reasoning involves understanding and reasoning about the spatial relationships among objects, their movements, and interactions with the environment [CJW+06, New24]. Despite advancements achieved with LLMs and MLLMs, this remains a significant challenge and attracts growing research interest [AAA+23]. To systematically evaluate multimodal spatial reasoning capabilities, several benchmarks have recently been introduced, covering diverse perspectives and tasks [LEC23, WMS+24, LZZ+24, RDT+24]. However, few research has touched base on the interplay between actions and changes in spatial layout [LZZ+24, WZS+24], which requires dynamic imagination and tracking the states as actions alter the environment. Various approaches have been proposed to tackle the challenges associated with spatial reasoning for MLLMs and LLMs: SpatialVLM [CXK+24] and SpatialRGPT [CYF+24] improve multimodal spatial reasoning by leveraging 3D VQA or scene graph data for supervised finetuning. VoT [WMZ+24] proposes a novel prompting approach by introducing textual imagery representation to facilitate dynamic reasoning for LLMs. Despite these efforts, existing methods fail to explore and unlock the inherent reasoning capabilities within the multimodal-native models to imagine the spatial dynamics. Our approach unifies text and vision within the reasoning traces and improve the interpretability and robustness by generating mental images aligned with spatial reasoning.</p>
<p>Video Generation World Model World models aim to predict the future for better understanding of the world, while video generation world models focus on the generation of videos to simulate and comprehend real-world dynamics [ZWZ+24b]. Recent developments in generative models have markedly enhanced the capacity to capture physics and motion [BDK+23, BRL+23, ECA+23, ZWZ+24a]. The Sora model [BPH+24] has demonstrated exceptional capabilities in this domain through its advanced generative techniques. World models extend beyond visual synthesis into numerous fields. In robotics, these models facilitate the simulation of physical interactions for diverse tasks [ZQC+24, LCB+24]. They contribute to autonomous driving by predicting environmental changes [WZH+23, WHF+24]. Additionally, they function as effective simulators for data collection purposes [YDG+23]. Previous work typically generates outputs in a single modality. In contrast, our proposed MVoT introduces a novel reasoning method supporting multi-modal generation. This enhanced capability enables superior understanding, reasoning, and simulation of complex scenarios.</p>
<h1>7 Conclusion</h1>
<p>We introduced Multimodal Visualization-of-Thought (MVoT), a novel reasoning framework that elicit reasoning process with multimodal thoughts using multimodal native generative models. MVoT outperformed textual reasoning baselines across a variety of tasks, meanwhile demonstrating better robustness to state complexity and offering enhanced interpretability. To ensure the generation of high-quality visualizations, we proposed the token discrepancy loss, addressing embedding misalignment in autoregressive MLLMs. This helps to alleviate the issues of redundant patterns and inaccurate visual thought generation, leading to better task performance with MVoT. Furthermore, the complementary strengths of MVoT and Chain-of-Thought (CoT), as evidenced by their combined upper-bound performance, highlight the promise of hybrid multimodal reasoning approaches. This work underscores the value of integrating multimodal cues and paves the way for future research into advancing reasoning thoughts of hybrid modalities for complex tasks.</p>
<h2>Limitation</h2>
<p>MVoT unifies the verbal and visual thoughts to elicit the reasoning process through image visualizations. However, we observe that the generated visualizations often attempt to reconstruct task-irrelevant details, such as the background patterns in FrozenLake, while overlooking the intended alterations in the visualizations. This can be mitigated by incorporating the guidance techniques as in image generation with diffusion models [SKHW24], which we propose as a direction for future improvement. Additionally, explicitly generating visualizations introduces computational overhead during inference. To address this, we advocate for further research into compact image representations using fewer tokens [CZL+24, YWD+24], thereby reducing the computational cost of visualization generation.</p>
<h2>References</h2>
<p>[AAA+23] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
[Bad92] Alan Baddeley. Working memory. Science, 255(5044):556-559, 1992.
[BDK+23] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.
[BGM+24] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan L Yuille, Trevor Darrell, Jitendra Malik, and Alexei A Efros. Sequential modeling enables scalable learning for large vision models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22861-22872, 2024.
[BPH+24] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators.</p>
<ol>
<li>URL https://openai. com/research/video-generation-models-as-world-simulators, 3, 2024.
[BRL+23] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22563-22575, 2023.
[Bro16] G Brockman. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
[Cha24] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024.
[CJW+06] Christopher F Chabris, Thomas E Jerde, Anita W Woolley, Margaret E Gerbasi, Jonathon P Schuldt, Sean L Bennett, J Richard Hackman, and Stephen M Kosslyn. Spatial and object visualization cognitive styles: Validation studies in 3800 individuals. Group brain technical report, 2:1-20, 2006.
[CSML24] Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. arXiv preprint arXiv:2407.06135, 2024.
[CXK+24] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages $14455-14465,2024$.
[CYF+24] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision language model. arXiv preprint arXiv:2406.01584, 2024.
[CZL+24] Rohan Choudhury, Guanglei Zhu, Sihan Liu, Koichiro Niinuma, Kris M. Kitani, and Laszlo Attila Jeni. Don’t look twice: Faster video transformers with run-length tokenization. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.
[Dee24] Google DeepMind. Google gemini ai update - december 2024, 2024. Accessed: 2024-12-27.
[DJP+24] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.
[ECA+23] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7346-7356, 2023.
[ERO21] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12873-12883, June 2021.
[GCZ+24] Timin Gao, Peixian Chen, Mengdan Zhang, Chaoyou Fu, Yunhang Shen, Yan Zhang, Shengchuan Zhang, Xiawu Zheng, Xing Sun, Liujuan Cao, and Rongrong Ji. Cantor: Inspiring multimodal chain-of-thought of mllm. In Proceedings of the 32nd ACM International Conference on Multimedia, MM '24, page 9096-9105, New York, NY, USA, 2024. Association for Computing Machinery.
[HSF+24] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, and Ranjay Krishna. Visual sketchpad: Sketching as a visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024.
[HSW+21] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.</li>
</ol>
<p>[ISS+23] Michael Igorevich Ivanitskiy, Rusheb Shah, Alex F. Spies, Tilman Räuker, Dan Valentine, Can Rager, Lucia Quirke, Chris Mathwin, Guillaume Corlouer, Cecilia Diniz Behn, and Samy Wu Fung. A configurable library for generating and manipulating maze datasets, 2023.
[JHH ${ }^{+}$23] Emily Jin, Jiaheng Hu, Zhuoyi Huang, Ruohan Zhang, Jiajun Wu, Li Fei-Fei, and Roberto Martín-Martín. Mini-behavior: A procedurally generated benchmark for long-horizon decision-making in embodied ai. arXiv preprint 2310.01824, 2023.
[JSM ${ }^{+}$23] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.
[LCB ${ }^{+}$24] Yang Liu, Weixing Chen, Yongjie Bai, Xiaodan Liang, Guanbin Li, Wen Gao, and Liang Lin. Aligning cyber space with physical world: A comprehensive survey on embodied ai. arXiv preprint arXiv:2407.06886, 2024.
[LEC23] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635-651, 2023.
[LLWL23] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.
[LTS24] Long Hei Matthew Lam, Ramya Keerthy Thatikonda, and Ehsan Shareghi. A closer look at logical reasoning with llms: The choice of tool matters. arXiv preprint arXiv:2406.00284, 2024.
[LYC ${ }^{+}$24] Xuanyu Lei, Zonghan Yang, Xinrui Chen, Peng Li, and Yang Liu. Scaffolding coordinates to promote vision-language coordination in large multi-modal models. arXiv preprint arXiv:2402.12058, 2024.
[LZZ+24] Chengzu Li, Caiqi Zhang, Han Zhou, Nigel Collier, Anna Korhonen, and Ivan Vulić. TopViewRS: Vision-language models as top-view spatial reasoners. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1786-1807, Miami, Florida, USA, November 2024. Association for Computational Linguistics.
[MHDH24] Chancharik Mitra, Brandon Huang, Trevor Darrell, and Roei Herzig. Compositional chain-ofthought prompting for large multimodal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14420-14431, 2024.
[MK09] Samuel T. Moulton and Stephen M. Kosslyn. Imagining predictions: mental imagery as mental emulation. Philosophical Transactions of the Royal Society B: Biological Sciences, $364: 1273-1280,2009$.
[MMP ${ }^{+}$24] Debjyoti Mondal, Suraj Modi, Subhadarshi Panda, Rituraj Singh, and Godawari Sudhakar Rao. Kam-cot: Knowledge augmented multimodal chain-of-thoughts reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 18798-18806, 2024.
[New24] Nora S. Newcombe. Spatial Cognition. MIT Press, jul 24 2024. https://oecs.mit.edu/pub/or750iar.
[Ope23] OpenAI. GPT-4V(ision) Technical Work and Authors, 2023.
[Ope24] OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024.
[Pai91] Allan Paivio. Dual coding theory: Retrospect and current status. Canadian Journal of Psychology/Revue canadienne de psychologie, 45(3):255, 1991.
$\left[\right.$ RDT $\left.^{+}\right.$24] Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A Plummer, Ranjay Krishna, Kuo-Hao Zeng, et al. Sat: Spatial aptitude training for multimodal language models. arXiv preprint arXiv:2412.07755, 2024.</p>
<p>[RHG ${ }^{+}$21] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):1-8, 2021.
[SBW ${ }^{+}$24] Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, and Furu Wei. Multimodal latent language modeling with next-token diffusion. arXiv preprint arXiv:2412.08635, 2024.
[SJC ${ }^{+}$24] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024.
[SKHW24] Seyedmorteza Sadat, Manuel Kansy, Otmar Hilliges, and Romann M Weber. No training, no problem: Rethinking classifier-free guidance for diffusion models. arXiv preprint arXiv:2407.02687, 2024.
[TPK24] Michael Tschannen, André Susano Pinto, and Alexander Kolesnikov. Jetformer: An autoregressive generative model of raw images and text. arXiv preprint arXiv:2411.19722, 2024.
[WHF ${ }^{+}$24] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14749-14759, 2024.
[WMS ${ }^{+}$24] Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Yixuan Li, and Neel Joshi. Is a picture worth a thousand words? delving into spatial reasoning for vision language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.
[WMZ ${ }^{+}$24] Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, and Furu Wei. Mind's eye of LLMs: Visualization-of-thought elicits spatial reasoning in large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.
[WWS ${ }^{+}$22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 24824-24837. Curran Associates, Inc., 2022.
[WZH ${ }^{+}$23] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-world-driven world models for autonomous driving. arXiv preprint arXiv:2309.09777, 2023.
[WZS ${ }^{+}$24] Qiucheng Wu, Handong Zhao, Michael Saxon, Trung Bui, William Yang Wang, Yang Zhang, and Shiyu Chang. Vsp: Assessing the dual challenges of perception and reasoning in spatial planning tasks for vlms, 2024.
[YDG ${ }^{+}$23] Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114, 2023.
[YLW ${ }^{+}$23] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023.
[YWD ${ }^{+}$24] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.
[YZY ${ }^{+}$23] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023.</p>
<p>[ZMG+24] Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, and Furu Wei. Llm as a mastermind: A survey of strategic reasoning with large language models, 2024.
[ZQC+24] Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3d-vla: A 3d vision-language-action generative world model. arXiv preprint arXiv:2403.09631, 2024.
[ZWZ+24a] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: High-dynamic video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8850-8860, 2024.
[ZWZ+24b] Zheng Zhu, Xiaofeng Wang, Wangbo Zhao, Chen Min, Nianchen Deng, Min Dou, Yuqi Wang, Botian Shi, Kai Wang, Chi Zhang, Yang You, Zhaoxiang Zhang, Dawei Zhao, Liang Xiao, Jian Zhao, Jiwen Lu, and Guan Huang. Is sora a world simulator? a comprehensive survey on general world models and beyond. arXiv preprint arXiv:2405.03520, 2024.
[ZYB+24] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024.
[ZZH+24] Qiji Zhou, Ruochen Zhou, Zike Hu, Panzhong Lu, Siyang Gao, and Yue Zhang. Image-ofthought prompting for visual reasoning refinement in multimodal large language models, 2024.
[ZZL+24] Zhuosheng Zhang, Aston Zhang, Mu Li, hai zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. Transactions on Machine Learning Research, 2024.</p>
<h1>A Contributions</h1>
<p>Idea Conceptualization Chengzu Li and Wenshan Wu conceptualized the Multimodal Visualization-ofThought (MVoT) framework. The token discrepancy loss was designed by Wenshan Wu in collaboration with Chengzu Li through discussions.</p>
<p>Data Collection and Code Implementation Chengzu Li collected the data, implemented the tasks in Direct, CoT, Interleaved, and MVoT formats, adapted the evaluation metrics, and developed the codebase for all system variants and the experimental pipeline. Chengzu Li conducted all experiments, and the performances were jointly verified by Chengzu Li and Wenshan Wu.</p>
<p>Paper Writing Chengzu Li and Wenshan Wu finished most of the paper. Huanyu Zhang drafted the related work section, created the illustrative figures, and polished the writing. Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulić, and Furu Wei reviewed the paper and provided feedback through regular project meetings and multiple rounds of revisions.</p>
<p>Computing Resources All experiments were conducted using computing resources and Azure APIs provided by Microsoft Research.</p>
<h2>B Data</h2>
<h2>B. 1 Dataset Collection</h2>
<p>MAZE dataset is generated using the maze-dataset framework [ISS+23], leveraging an iterative depthfirst search algorithm. Mazes of sizes 3, 4, 5, and 6 are created with varying random seeds. Navigation paths are generated, and we remove repetitive paths to prevent knowledge leakage between training and development sets. For each maze, potential destination candidates (three random coordinates) are selected alongside the true destination.</p>
<p>MINIBEHAVIOR dataset originates from the INSTALlingAPRINTER simulation environment, where reinforcement learning (RL) agents are trained on grid environments of sizes 7 to 10 using stable-baseline3 [RHG+21]. Action sequences that successfully complete the task are retained, ensuring no overlap with previously seen environments. For cases involving repeated action sequences or previously seen environments, the simulator layout is modified with specified probabilities: $40 \%$ chance of altering the printer or table coordinates respectively, $20 \%$ chance of randomly removing either the printer or table. The modified environments are re-evaluated using the previous action sequences to validate the outcomes.</p>
<p>FROZENLAKE dataset is based on Gym environments [Bro16] and scripts from WZS+24. Trajectories are collected from RL agents selecting actions with maximum expected rewards using Q-table. Successful action sequences are retained if they are not seen in this environment. For unsuccessful sequences, cases where the agent falls into a hole have a $50 \%$ probability of being saved as-is or $50 \%$ with appended random actions. If the agent neither falls into a hole nor reaches the destination, the sequence is saved in accordance to Section 3. To increase the variance of the action sequences, we also randomly sample the paths when learning the Q-table following similar practice as above.</p>
<h2>B. 2 Dataset Statistics</h2>
<p>Further to the statistics in Table 1, we provide additional details about the datasets used in this work.</p>
<h2>MAZE</h2>
<ul>
<li>Entities: The starting point is marked with a red dot, with potential destination candidates labeled as A, B, C, and D.</li>
<li>Actions: Go up, down, left, or right. All action sequences are valid, ensuring no collisions with walls and that every action is executable.</li>
<li>Visual Pattern: Abstract sketch illustrating the maze layout and navigation path. The agent's path is visualized incrementally with red arrows, preserving all previous movements.</li>
</ul>
<p>Table 5: Dataset statistical distributions of options.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">A</th>
<th style="text-align: center;">B</th>
<th style="text-align: center;">C</th>
<th style="text-align: center;">D</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MAZE</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">1187</td>
<td style="text-align: center;">1269</td>
<td style="text-align: center;">1305</td>
<td style="text-align: center;">1246</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">323</td>
<td style="text-align: center;">326</td>
<td style="text-align: center;">293</td>
<td style="text-align: center;">313</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">3321</td>
<td style="text-align: center;">1092</td>
<td style="text-align: center;">1456</td>
<td style="text-align: center;">531</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">834</td>
<td style="text-align: center;">297</td>
<td style="text-align: center;">349</td>
<td style="text-align: center;">124</td>
</tr>
<tr>
<td style="text-align: left;">FROZENLAKE</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">3043</td>
<td style="text-align: center;">2377</td>
<td style="text-align: center;">1426</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">735</td>
<td style="text-align: center;">580</td>
<td style="text-align: center;">349</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 6: Dataset statistical distributions of grid sizes.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">MAZE</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Grid Size</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">6</td>
</tr>
<tr>
<td style="text-align: left;">Train</td>
<td style="text-align: right;">1481</td>
<td style="text-align: right;">1722</td>
<td style="text-align: right;">1823</td>
<td style="text-align: right;">1820</td>
</tr>
<tr>
<td style="text-align: left;">Dev</td>
<td style="text-align: right;">334</td>
<td style="text-align: right;">418</td>
<td style="text-align: right;">462</td>
<td style="text-align: right;">450</td>
</tr>
<tr>
<td style="text-align: left;">MINIBEHAVIOR</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Grid Size</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">10</td>
</tr>
<tr>
<td style="text-align: left;">Train</td>
<td style="text-align: right;">1600</td>
<td style="text-align: right;">1600</td>
<td style="text-align: right;">1600</td>
<td style="text-align: right;">1600</td>
</tr>
<tr>
<td style="text-align: left;">Dev</td>
<td style="text-align: right;">401</td>
<td style="text-align: right;">401</td>
<td style="text-align: right;">401</td>
<td style="text-align: right;">401</td>
</tr>
<tr>
<td style="text-align: left;">FROZENLAKE</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Grid Size</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">6</td>
</tr>
<tr>
<td style="text-align: left;">Train</td>
<td style="text-align: right;">308</td>
<td style="text-align: right;">1080</td>
<td style="text-align: right;">1397</td>
<td style="text-align: right;">2222</td>
</tr>
<tr>
<td style="text-align: left;">Dev</td>
<td style="text-align: right;">78</td>
<td style="text-align: right;">271</td>
<td style="text-align: right;">350</td>
<td style="text-align: right;">556</td>
</tr>
</tbody>
</table>
<h1>MiniBehavior</h1>
<ul>
<li>Entities: The agent's current location represented by a red triangle, with a printer symbol and a brown-colored table.</li>
<li>Actions: Go up, down, left, or right; pick up; drop; toggle. The pick up action removes the printer from the map if next to the agent, while the drop action places the printer on the table if the agent is carrying it and next to the table.</li>
<li>Visual Pattern: Abstract sketch of the environment layout. Unlike MAZE, only the agent's current position is visualized on the map, and whether the agent carries the printer is described in text.</li>
</ul>
<h2>FROZENLAKE</h2>
<ul>
<li>Entities: The agent's current location depicted as an elf wearing a green hat, a gift and multiple holes on the frozen lake.</li>
<li>Actions: Go up, down, left, or right.</li>
<li>Visual Pattern: Comic-style illustrations with detailed depictions of the elf, background, and holes. Similar to MiniBehaVior, the visualization is not incremental, focusing only on the current state.</li>
</ul>
<p>Table 5 illustrates the distribution of correct choices of the train and dev split for each task. The distribution of different grid sizes in train and dev split for each task is listed in Table 6. Specifically, Table 7 shows how environmental complexity evolves with larger grid sizes in FROZENLAKE.</p>
<p>Table 7: Average number of key entities in FROZENLAKE with different grid sizes.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Grid Size</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Train</td>
<td style="text-align: center;">4.7097</td>
<td style="text-align: center;">5.7166</td>
<td style="text-align: center;">7.4723</td>
<td style="text-align: center;">9.5049</td>
</tr>
<tr>
<td style="text-align: left;">Dev</td>
<td style="text-align: center;">4.6737</td>
<td style="text-align: center;">5.4689</td>
<td style="text-align: center;">7.4589</td>
<td style="text-align: center;">10.2267</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">4.7030</td>
<td style="text-align: center;">5.6682</td>
<td style="text-align: center;">7.4696</td>
<td style="text-align: center;">9.648</td>
</tr>
</tbody>
</table>
<p>Table 8: Hyper-parameters of fine-tuning Anole 7B for different system variants.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyper-Parameters</th>
<th style="text-align: center;">Direct</th>
<th style="text-align: center;">CoT</th>
<th style="text-align: center;">Interleaved</th>
<th style="text-align: center;">MVoT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random Seed</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">42</td>
</tr>
<tr>
<td style="text-align: left;">Epochs</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate</td>
<td style="text-align: center;">0.0002</td>
<td style="text-align: center;">0.0002</td>
<td style="text-align: center;">0.0002</td>
<td style="text-align: center;">0.0002</td>
</tr>
<tr>
<td style="text-align: left;">Train Batch Size</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Val Batch Size</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">Grad Accumulation</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">GPUs</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">32</td>
</tr>
</tbody>
</table>
<p>Table 9: Hyper-parameters for GPT-4o</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Temperature</th>
<th style="text-align: center;">Max Tokens</th>
<th style="text-align: center;">Top P</th>
<th style="text-align: center;">Frequency Penalty</th>
<th style="text-align: center;">Presence Penalty</th>
<th style="text-align: center;">Stop</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4o</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">800</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">None</td>
</tr>
</tbody>
</table>
<h1>C Experiments</h1>
<h2>C. 1 Hyper-Parameters</h2>
<p>Table 8 and 9 show the hyper-parameters for training MVoT and doing inference with GPT-4o.
All models were trained on MI300X GPUs. Table 8 provides the details of GPU configurations and hyperparameters for various experimental settings. For GPT-4o, we utilized the 2024-07-01 version hosted on the Azure platform, with inference parameters outlined in Table 9.</p>
<p>During MVoT training, as visual thoughts are recursively generated during inference, we applied input augmentation to improve visualization robustness and mitigate noise introduced by image reconstruction during tokenization and detokenization, as illustrated in Figure 4. This augmentation applies tokenization and detokenization over the input image for multiple times, with the iteration count randomly determined between 0 and 10 .</p>
<h2>C. 2 Prompting Templates</h2>
<p>Table 10, 11, 12 shows examples of prompting templates and responses for each tasks with different system variants. Table 13 illustrate the prompting template for GPT-4o with MAZE and Table 14 shows an example for GPT-4o with MVoT on MiniBehavior. The other two tasks follow similar patterns.</p>
<h2>D Results</h2>
<h2>D. 1 Task Performance</h2>
<p>Table 15 presents the detailed task performance metrics across varying grid sizes. For FrozenLAKE, we observe a noticeable decline in performance for both CoT and GPT-4o as the grid size increases, reflecting the growing complexity of the environment. This drop in performance highlights the limitations of these models in handling more intricate spatial reasoning tasks as the state complexity expands.
In contrast, MVoT demonstrates consistent performance across all grid sizes and tasks, underscoring its robustness to environmental complexity. Unlike CoT-based approaches, which struggle to generalize effectively in larger and more complex settings, MVoT's ability to integrate verbal and visual thoughts allows it to maintain stability and accuracy. The underlying intuition is that, even in more complex environments, MVoT focuses solely on the intended areas of the image without modifying irrelevant parts, thereby maintaining a consistent level of reasoning difficulty for the model. These results emphasize MVoT's resilience and adaptability, making it a better choice for tasks involving complex reasoning in dynamic environments.</p>
<h2>D. 2 Visualizations</h2>
<p>Table 16 presents fine-grained visualization metrics for MAZE and MiniBeHAVIOR across varying grid sizes. The consistent performance across grid sizes underscores the robustness of the visualizations.</p>
<p>Table 10: Example of input and output for Direct reasoning. Italic text is the expected response.</p>
<h1>Direct</h1>
<h2>MaZe</h2>
<p>Task: Maze Navigation Simulation
Determine the final destination (A, B, C or D) from the starting point (red point) following the action sequence. The definitions of the actions are as below.</p>
<ul>
<li>Go up/left/down/right: move one grid space in the absolute up/left/down/right direction.</li>
</ul>
<p>Full Action Sequence: Go down. Go left. Go left. Go up. Go up.
Initial maze: <image>
Response: The answer is $A$.</p>
<h2>MiniBehavior</h2>
<p>Task: Mini-Behavior Installing the Printer
Determine whether the agent (red triangle) can pick up the printer (printer symbol) on the floor and place it on the table (brown area) and toggle it on. If not, identify the failure reason. The definitions of the actions are as below.</p>
<ul>
<li>Go up/left/down/right: move one grid space in the absolute up/left/down/right direction.</li>
<li>Pick up: pick up the printer from the any of the grid next to the agent. If there is no printer next to the agent, the action fails.</li>
<li>Drop: drop the printer to the table that is next to the agent. If there is no table next to the agent, the action fails.</li>
<li>Toggle: toggle the printer that is on the table and next to the agent.</li>
</ul>
<p>Return A, B, C or D.
Full Action Sequence: Go up. Go up. Go right. Go down. Go down. Pick up. Go right. Drop. Toggle.
A. Action Success.
B. Action Failed: Drop Error.
C. Action Failed: Pick Up Error.
D. Missing Key Objects.</p>
<p>Initial State: <image>
Response: The answer is $A$.</p>
<h2>FrozenLake</h2>
<p>Task: FrozenLake
Determine whether the agent (elf character) can safely reach the gift following the action sequence without falling into the holes. If not, identify the failure reason. The definitions of the actions are as below.</p>
<ul>
<li>Go up/left/down/right: move one grid space in the absolute up/left/down/right direction.</li>
</ul>
<p>Return A, B or C.
Full Action Sequence: Go right. Go up.
A. Action Success.
B. Action Failed: Fall into the Hole.
C. Action Failed: Agent Safe but Fail to Reach Destination.</p>
<p>Initial State: <image>
Response: The answer is $A$.</p>
<p>Table 11: Example of input and output for CoT with environment layout. Italic text is the expected response.</p>
<h1>CoT with Environment Layout</h1>
<h2>MAZE</h2>
<p>Task: Maze Navigation Simulation
Determine the final destination (A, B, C or D) from the starting point (red point) following the action sequence. The definitions of the actions are as below.</p>
<ul>
<li>Go up/left/down/right: move one grid space in the absolute up/left/down/right direction.</li>
</ul>
<p>Full Action Sequence: Go down. Go left. Go left. Go up. Go up.
Initial maze: <image>
Response: Destination Coordinates: A Coordinate: [0, 0]. B Coordinate: [2, 0]. C Coordinate: [0, 1]. D Coordinate: [1, 1]. Initial Agent Coordinate: [2, 1]. Go down. Agent Coordinate: [2, 2]. Go left. Agent Coordinate: [1, 2]. Go left. Agent Coordinate: [0, 2]. Go up. Agent Coordinate: [0, 1]. Go up. Agent Coordinate: [0, 0]. Action sequence finished. The answer is A.</p>
<h2>MiniBEHAVIOR</h2>
<p>Task: Mini-Behavior Installing the Printer
Determine whether the agent (red triangle) can pick up the printer (printer symbol) on the floor and place it on the table (brown area) and toggle it on. If not, identify the failure reason. The definitions of the actions are as below.</p>
<ul>
<li>Go up/left/down/right: move one grid space in the absolute up/left/down/right direction.</li>
<li>Pick up: pick up the printer from the any of the grid next to the agent. If there is no printer next to the agent, the action fails.</li>
<li>Drop: drop the printer to the table that is next to the agent. If there is no table next to the agent, the action fails.</li>
<li>Toggle: toggle the printer that is on the table and next to the agent.</li>
</ul>
<p>Return A, B, C or D.
Full Action Sequence: Go up. Go up. Go right. Go down. Go down. Pick up. Go right. Drop. Toggle.
A. Action Success.
B. Action Failed: Drop Error.
C. Action Failed: Pick Up Error.
D. Missing Key Objects.</p>
<p>Initial State: <image>Carrying objects: None.
Response: Initial Agent Coordinate: [2, 7]. Initial Environment Layout: Printer Initial Coordinate: [3, 8]. Table Coordinates: [[5, 6], [5, 7], [6, 6], [6, 7], [7, 6], [7, 7]]. Go up. Agent Coordinate: [2, 6]. Carrying objects: None. Go up. Agent Coordinate: [2, 5]. Carrying objects: None. Go right. Agent Coordinate: [3, 5]. Carrying objects: None. Go down. Agent Coordinate: [3, 6]. Carrying objects: None. Go down. Agent Coordinate: [3, 7]. Carrying objects: None. Pick up. Agent Coordinate: [3, 7]. Carrying objects: printer_0. Go right. Agent Coordinate: [4, 7]. Carrying objects: printer_0. Drop. Agent Coordinate: [4, 7]. Carrying objects: None. Toggle. Agent Coordinate: [4, 7]. Carrying objects: None. Action sequence stopped. The answer is A.</p>
<h2>FrozenLake</h2>
<p>Task: FrozenLake
Determine whether the agent (elf character) can safely reach the gift following the action sequence without falling into the holes. If not, identify the failure reason. The definitions of the actions are as below.</p>
<ul>
<li>Go up/left/down/right: move one grid space in the absolute up/left/down/right direction.</li>
</ul>
<p>Return A, B or C.
Full Action Sequence: Go right. Go up.
A. Action Success.
B. Action Failed: Fall into the Hole.
C. Action Failed: Agent Safe but Fail to Reach Destination.</p>
<p>Initial State: <image>
Response: Initial Agent Coordinate: [1, 2]. Initial Environment Layout: Holes Coordinate: [[1, 0], [0, 1]]. Gift Coordinate: [2, 1]. Go right. Agent Coordinate: [2, 2]. Go up. Agent Coordinate: [2, 1]. Action sequence stopped. The answer is A.</p>
<p>Table 12: Example of input and output for CoT without environment layout. Italic text is the expected response.</p>
<h1>CoT without Environment Layout</h1>
<h2>MaZe</h2>
<p>Task: Maze Navigation Simulation
Determine the final destination (A, B, C or D) from the starting point (red point) following the action sequence. The definitions of the actions are as below.</p>
<ul>
<li>Go up/left/down/right: move one grid space in the absolute up/left/down/right direction.</li>
</ul>
<p>Full Action Sequence: Go down. Go left. Go left. Go up. Go up.
Initial maze: <image>
Response: Initial Agent Coordinate: [2, 1]. Go down. Agent Coordinate: [2, 2]. Go left. Agent Coordinate: [1, 2]. Go left. Agent Coordinate: [0, 2]. Go up. Agent Coordinate: [0, 1]. Go up. Agent Coordinate: [0, 0]. Action sequence finished. The answer is A.</p>
<h2>MiniBehAviOR</h2>
<p>Task: Mini-Behavior Installing the Printer
Determine whether the agent (red triangle) can pick up the printer (printer symbol) on the floor and place it on the table (brown area) and toggle it on. If not, identify the failure reason. The definitions of the actions are as below.</p>
<ul>
<li>Go up/left/down/right: move one grid space in the absolute up/left/down/right direction.</li>
<li>Pick up: pick up the printer from the any of the grid next to the agent. If there is no printer next to the agent, the action fails.</li>
<li>Drop: drop the printer to the table that is next to the agent. If there is no table next to the agent, the action fails.</li>
<li>Toggle: toggle the printer that is on the table and next to the agent.</li>
</ul>
<p>Return A, B, C or D.
Full Action Sequence: Go up. Go up. Go right. Go down. Go down. Pick up. Go right. Drop. Toggle.
A. Action Success.
B. Action Failed: Drop Error.
C. Action Failed: Pick Up Error.
D. Missing Key Objects.</p>
<p>Initial State: <image>Carrying objects: None.
Response: Initial Agent Coordinate: [2, 7]. Go up. Agent Coordinate: [2, 6]. Carrying objects: None. Go up. Agent Coordinate: [2, 5]. Carrying objects: None. Go right. Agent Coordinate: [3, 5]. Carrying objects: None. Go down. Agent Coordinate: [3, 6]. Carrying objects: None. Go down. Agent Coordinate: [3, 7]. Carrying objects: None. Pick up. Agent Coordinate: [3, 7]. Carrying objects: printer_0. Go right. Agent Coordinate: [4, 7]. Carrying objects: printer_0. Drop. Agent Coordinate: [4, 7]. Carrying objects: None. Toggle. Agent Coordinate: [4, 7]. Carrying objects: None. Action sequence stopped. The answer is A.</p>
<h2>FrozenLake</h2>
<p>Task: FrozenLake
Determine whether the agent (elf character) can safely reach the gift following the action sequence without falling into the holes. If not, identify the failure reason. The definitions of the actions are as below.</p>
<ul>
<li>Go up/left/down/right: move one grid space in the absolute up/left/down/right direction.</li>
</ul>
<p>Return A, B or C.
Full Action Sequence: Go right. Go up.
A. Action Success.
B. Action Failed: Fall into the Hole.
C. Action Failed: Agent Safe but Fail to Reach Destination.</p>
<p>Initial State: <image>
Response: Initial Agent Coordinate: [1, 2]. Go right. Agent Coordinate: [2, 2]. Go up. Agent Coordinate: [2, 1]. Action sequence stopped. The answer is A.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ We use the 2024-07-01 version of GPT-40 via the Azure API.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
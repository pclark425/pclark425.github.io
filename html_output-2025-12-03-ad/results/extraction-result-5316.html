<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5316 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5316</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5316</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-264451605</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.16411v1.pdf" target="_blank">Decoding Stumpers: Large Language Models vs. Human Problem-Solvers</a></p>
                <p><strong>Paper Abstract:</strong> This paper investigates the problem-solving capabilities of Large Language Models (LLMs) by evaluating their performance on stumpers, unique single-step intuition problems that pose challenges for human solvers but are easily verifiable. We compare the performance of four state-of-the-art LLMs (Davinci-2, Davinci-3, GPT-3.5-Turbo, GPT-4) to human participants. Our findings reveal that the new-generation LLMs excel in solving stumpers and surpass human performance. However, humans exhibit superior skills in verifying solutions to the same problems. This research enhances our understanding of LLMs' cognitive abilities and provides insights for enhancing their problem-solving potential across various domains.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5316.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5316.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Davinci-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Davinci-2 (OpenAI GPT-3 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI GPT-3 era generative language model (Davinci series) evaluated for its ability to solve and verify single-step intuition riddles (stumpers) in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Decoding Stumpers: Large Language Models vs. Human Problem-Solvers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Davinci-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3 era model (Davinci family). Paper notes it as one of the GPT-3 models tested; no architecture details or parameter counts are provided in this paper. GPT-3 models here are contrasted with later chat models and are not finetuned with the same chat/HF approaches described for GPT-3.5/GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Stumpers (Bar-Hillel stumpers dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>insight / problem-solving / intuition (creative riddle solving)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>A set of one-step intuition riddles ('stumpers') curated by Bar-Hillel (2021). Each riddle elicits a misleading intuitive representation that blocks the solution; solutions are single-step and once revealed are easy to verify. The validated dataset used here contains 76 stumpers; authors also generated two paraphrase variants per stumper for an enhanced set.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported collectively for the GPT-3 models (Davinci-2 and Davinci-3): mean correct solving rate m = 29.6% on the original 76-stumper dataset (22.5% on the enhanced dataset). The paper does not report an isolated per-model solving percentage for Davinci-2 alone in the text. In verification tasks models generally performed poorly (overall models verification mean m ≈ 41%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human participants (n = 81; female = 48%; ages 20–54, mean = 28.52, SD = 7.73) solved 38.15% of the original stumpers (replicating prior work ~35%). In verification, humans performed 100% when they had generated the answer and 63.8% when they had failed to generate it.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3 models (Davinci-2 + Davinci-3 combined) underperformed relative to human solving accuracy (29.6% vs. human 38.15%) on the original set. Statistical comparison reported in the paper contrasts GPT-3 models vs. chat models (see ANOVA), but per-model significance vs. humans is not separately reported for Davinci-2. For verification tasks models as a group performed below human levels (~41% vs humans' 63.8–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>The paper does not provide per-model breakdown for Davinci-2's solving accuracy (only grouped GPT-3 mean). Verification failure is a prominent limitation (models often below chance). Dataset limited to 76 validated stumpers; Davinci-2-specific training details and parameter count are not reported in this paper. Subjective judgments were used to evaluate correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decoding Stumpers: Large Language Models vs. Human Problem-Solvers', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5316.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5316.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Davinci-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Davinci-3 (OpenAI GPT-3 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A later GPT-3 era OpenAI model (Davinci-3) evaluated on stumpers; included in the GPT-3 grouping but reported to have relatively stronger verification/recognition performance compared to its GPT-3 sibling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Decoding Stumpers: Large Language Models vs. Human Problem-Solvers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Davinci-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3 era model (davinci-3). The paper treats it as part of the GPT-3 models; specific architectural or parameter details are not provided. Davinci-3 exhibited relatively better answer-verification/recognition performance among models tested.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Stumpers (Bar-Hillel stumpers dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>insight / problem-solving / intuition (creative riddle solving)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same as above: one-step intuition riddles, 76 validated stumpers from Bar-Hillel (2021). Participants and models were asked to generate answers (or 'IDK') and later to verify between two given options.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Davinci-3 is reported as part of the GPT-3 group with overall solving m = 29.6% on the original set. However, Davinci-3 specifically showed markedly better recognition/verification performance: 70% accuracy when choosing the correct answer curated by authors (Figure 2) and 85% accuracy when choosing between its own previous response and the ground truth (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans solved 38.15% of stumpers (n = 81; F = 48%; age mean 28.52, sd = 7.73). For verification, humans were 100% when they had generated the solution and 63.8% when they had not generated it.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>As a generator, Davinci-3 (as part of GPT-3 group) solved fewer stumpers than humans (29.6% vs 38.15%). As a verifier/recognizer, Davinci-3 outperformed average model verification rates and in some verification measures approached or exceeded human performance (70% and 85% reported for recognition tasks), marking Davinci-3 as more human-like on verification than the chat models.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Per-model solving accuracy (generation) is not given separately in the paper, making it unclear how much of the GPT-3 mean is driven by Davinci-3 versus Davinci-2. Davinci-3's relatively strong verification suggests heterogeneity among GPT-3 family models. The paper notes subjective scoring and limited dataset size as caveats.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decoding Stumpers: Large Language Models vs. Human Problem-Solvers', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5316.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5316.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo (OpenAI chat model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI chat-oriented model (GPT-3.5-Turbo) finetuned with human feedback, evaluated on solving and verifying stumpers; grouped with GPT-4 as 'chat models' in analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Decoding Stumpers: Large Language Models vs. Human Problem-Solvers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-style OpenAI model finetuned with human feedback (described in the paper as a 'chat model'). Specific architecture and parameter counts are not provided in this paper. Treated as a newer-generation model relative to Davinci-series GPT-3 models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Stumpers (Bar-Hillel stumpers dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>insight / problem-solving / intuition (creative riddle solving)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same as above: one-step intuition riddles requiring escape from misleading intuitive framing; generation and two-alternative verification tasks were used.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Chat models (GPT-3.5-Turbo and GPT-4 combined) achieved a mean solving rate m = 57.8% on the original 76-stumper dataset (43.4% on the enhanced dataset). The paper does not separate GPT-3.5-Turbo's solving percentage from GPT-4 in the main-text aggregate. In verification tasks, chat models performed poorly overall; models as a group averaged ≈41% on verification, below human levels.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans solved 38.15% of stumpers (n = 81; F = 48%; ages 20–54, mean = 28.52, SD = 7.73). Verification: humans 100% when they generated the solution and 63.8% when they had not.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>The chat-model group (including GPT-3.5-Turbo) outperformed humans on solution generation (57.8% vs. 38.15%); the group difference between chat models and GPT-3 models is reported as highly significant [two-way ANOVA: F(1,4) = 686, p = 0.00001]. However, chat models lagged behind humans in verification, demonstrating a dissociation between generation and classification abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Paper reports an interaction between prompting and model generation performance [F(1,4) = 30.82, p = 0.005], with prompting improving GPT-3 models but harming chat models' performance. The paper lacks per-model split between GPT-3.5 and GPT-4 for many measures, and no model parameter sizes/training corpora are reported here. Verification remains a major failure mode for chat models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decoding Stumpers: Large Language Models vs. Human Problem-Solvers', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5316.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5316.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI chat model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 chat-oriented model (finetuned with human feedback) evaluated on stumpers, exhibiting high solving accuracy but limited verification capability relative to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Decoding Stumpers: Large Language Models vs. Human Problem-Solvers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 chat model, described in the paper as one of the new-generation chat models finetuned with human feedback; the paper does not report architecture specifics or parameter counts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Stumpers (Bar-Hillel stumpers dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>insight / problem-solving / intuition (creative riddle solving)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>As above: single-step intuition riddles that mislead intuitive framing; solutions are unique and easily verifiable by humans once revealed. Tasks included answer generation (naïve and with exemplars) and two-alternative verification.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 was part of the chat-model group that achieved mean solving m = 57.8% on the original 76-stumper dataset (43.4% on the enhanced dataset). The paper states overall LLM solving rates ranged from ~26%–58% depending on model; GPT-4 is at the upper end of that range as part of the chat models. In verification tasks, models (including GPT-4) performed poorly overall (group mean ≈41%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans solved 38.15% of stumpers (n = 81; F = 48%; ages 20–54, mean = 28.52, SD = 7.73). For verification humans achieved 100% when they had generated the solution and 63.8% when they had not.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>As a generator GPT-4 (within the chat-model group) outperformed humans on the stumpers solving task (57.8% vs. human 38.15%), with the paper reporting a significant model-family effect [F(1,4) = 686, p = 0.00001]. For verification, GPT-4 (like other models) underperformed humans, contributing to an overall model verification mean below human levels. Authors highlight a qualitative dissociation: GPT-3.5/GPT-4 solve stumpers better than they verify them, opposite the pattern seen for GPT-3 models.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Paper does not provide isolated per-model percentages for GPT-4 vs GPT-3.5 in many reported aggregates, so exact GPT-4-only metrics are not fully separable in the text. Verification is a notable failure mode; prompting effects were negative for chat models. Dataset size and reliance on OpenAI models are limitations noted by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decoding Stumpers: Large Language Models vs. Human Problem-Solvers', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Stumpers: an annotated compendium <em>(Rating: 2)</em></li>
                <li>Learning psychology from riddles: The case of stumpers <em>(Rating: 2)</em></li>
                <li>Solving stumpers, crt and crat: Are the abilities related? <em>(Rating: 2)</em></li>
                <li>Human-like intuitive behavior and reasoning biases emerged in language models-and disappeared in gpt-4 <em>(Rating: 1)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5316",
    "paper_id": "paper-264451605",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "Davinci-2",
            "name_full": "Davinci-2 (OpenAI GPT-3 family)",
            "brief_description": "An OpenAI GPT-3 era generative language model (Davinci series) evaluated for its ability to solve and verify single-step intuition riddles (stumpers) in this study.",
            "citation_title": "Decoding Stumpers: Large Language Models vs. Human Problem-Solvers",
            "mention_or_use": "use",
            "model_name": "Davinci-2",
            "model_description": "OpenAI GPT-3 era model (Davinci family). Paper notes it as one of the GPT-3 models tested; no architecture details or parameter counts are provided in this paper. GPT-3 models here are contrasted with later chat models and are not finetuned with the same chat/HF approaches described for GPT-3.5/GPT-4.",
            "model_size": null,
            "cognitive_test_name": "Stumpers (Bar-Hillel stumpers dataset)",
            "cognitive_test_type": "insight / problem-solving / intuition (creative riddle solving)",
            "cognitive_test_description": "A set of one-step intuition riddles ('stumpers') curated by Bar-Hillel (2021). Each riddle elicits a misleading intuitive representation that blocks the solution; solutions are single-step and once revealed are easy to verify. The validated dataset used here contains 76 stumpers; authors also generated two paraphrase variants per stumper for an enhanced set.",
            "llm_performance": "Reported collectively for the GPT-3 models (Davinci-2 and Davinci-3): mean correct solving rate m = 29.6% on the original 76-stumper dataset (22.5% on the enhanced dataset). The paper does not report an isolated per-model solving percentage for Davinci-2 alone in the text. In verification tasks models generally performed poorly (overall models verification mean m ≈ 41%).",
            "human_baseline_performance": "Human participants (n = 81; female = 48%; ages 20–54, mean = 28.52, SD = 7.73) solved 38.15% of the original stumpers (replicating prior work ~35%). In verification, humans performed 100% when they had generated the answer and 63.8% when they had failed to generate it.",
            "performance_comparison": "GPT-3 models (Davinci-2 + Davinci-3 combined) underperformed relative to human solving accuracy (29.6% vs. human 38.15%) on the original set. Statistical comparison reported in the paper contrasts GPT-3 models vs. chat models (see ANOVA), but per-model significance vs. humans is not separately reported for Davinci-2. For verification tasks models as a group performed below human levels (~41% vs humans' 63.8–100%).",
            "notable_differences_or_limitations": "The paper does not provide per-model breakdown for Davinci-2's solving accuracy (only grouped GPT-3 mean). Verification failure is a prominent limitation (models often below chance). Dataset limited to 76 validated stumpers; Davinci-2-specific training details and parameter count are not reported in this paper. Subjective judgments were used to evaluate correctness.",
            "uuid": "e5316.0",
            "source_info": {
                "paper_title": "Decoding Stumpers: Large Language Models vs. Human Problem-Solvers",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Davinci-3",
            "name_full": "Davinci-3 (OpenAI GPT-3 family)",
            "brief_description": "A later GPT-3 era OpenAI model (Davinci-3) evaluated on stumpers; included in the GPT-3 grouping but reported to have relatively stronger verification/recognition performance compared to its GPT-3 sibling.",
            "citation_title": "Decoding Stumpers: Large Language Models vs. Human Problem-Solvers",
            "mention_or_use": "use",
            "model_name": "Davinci-3",
            "model_description": "OpenAI GPT-3 era model (davinci-3). The paper treats it as part of the GPT-3 models; specific architectural or parameter details are not provided. Davinci-3 exhibited relatively better answer-verification/recognition performance among models tested.",
            "model_size": null,
            "cognitive_test_name": "Stumpers (Bar-Hillel stumpers dataset)",
            "cognitive_test_type": "insight / problem-solving / intuition (creative riddle solving)",
            "cognitive_test_description": "Same as above: one-step intuition riddles, 76 validated stumpers from Bar-Hillel (2021). Participants and models were asked to generate answers (or 'IDK') and later to verify between two given options.",
            "llm_performance": "Davinci-3 is reported as part of the GPT-3 group with overall solving m = 29.6% on the original set. However, Davinci-3 specifically showed markedly better recognition/verification performance: 70% accuracy when choosing the correct answer curated by authors (Figure 2) and 85% accuracy when choosing between its own previous response and the ground truth (Figure 3).",
            "human_baseline_performance": "Humans solved 38.15% of stumpers (n = 81; F = 48%; age mean 28.52, sd = 7.73). For verification, humans were 100% when they had generated the solution and 63.8% when they had not generated it.",
            "performance_comparison": "As a generator, Davinci-3 (as part of GPT-3 group) solved fewer stumpers than humans (29.6% vs 38.15%). As a verifier/recognizer, Davinci-3 outperformed average model verification rates and in some verification measures approached or exceeded human performance (70% and 85% reported for recognition tasks), marking Davinci-3 as more human-like on verification than the chat models.",
            "notable_differences_or_limitations": "Per-model solving accuracy (generation) is not given separately in the paper, making it unclear how much of the GPT-3 mean is driven by Davinci-3 versus Davinci-2. Davinci-3's relatively strong verification suggests heterogeneity among GPT-3 family models. The paper notes subjective scoring and limited dataset size as caveats.",
            "uuid": "e5316.1",
            "source_info": {
                "paper_title": "Decoding Stumpers: Large Language Models vs. Human Problem-Solvers",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo",
            "name_full": "GPT-3.5-Turbo (OpenAI chat model)",
            "brief_description": "An OpenAI chat-oriented model (GPT-3.5-Turbo) finetuned with human feedback, evaluated on solving and verifying stumpers; grouped with GPT-4 as 'chat models' in analyses.",
            "citation_title": "Decoding Stumpers: Large Language Models vs. Human Problem-Solvers",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "Chat-style OpenAI model finetuned with human feedback (described in the paper as a 'chat model'). Specific architecture and parameter counts are not provided in this paper. Treated as a newer-generation model relative to Davinci-series GPT-3 models.",
            "model_size": null,
            "cognitive_test_name": "Stumpers (Bar-Hillel stumpers dataset)",
            "cognitive_test_type": "insight / problem-solving / intuition (creative riddle solving)",
            "cognitive_test_description": "Same as above: one-step intuition riddles requiring escape from misleading intuitive framing; generation and two-alternative verification tasks were used.",
            "llm_performance": "Chat models (GPT-3.5-Turbo and GPT-4 combined) achieved a mean solving rate m = 57.8% on the original 76-stumper dataset (43.4% on the enhanced dataset). The paper does not separate GPT-3.5-Turbo's solving percentage from GPT-4 in the main-text aggregate. In verification tasks, chat models performed poorly overall; models as a group averaged ≈41% on verification, below human levels.",
            "human_baseline_performance": "Humans solved 38.15% of stumpers (n = 81; F = 48%; ages 20–54, mean = 28.52, SD = 7.73). Verification: humans 100% when they generated the solution and 63.8% when they had not.",
            "performance_comparison": "The chat-model group (including GPT-3.5-Turbo) outperformed humans on solution generation (57.8% vs. 38.15%); the group difference between chat models and GPT-3 models is reported as highly significant [two-way ANOVA: F(1,4) = 686, p = 0.00001]. However, chat models lagged behind humans in verification, demonstrating a dissociation between generation and classification abilities.",
            "notable_differences_or_limitations": "Paper reports an interaction between prompting and model generation performance [F(1,4) = 30.82, p = 0.005], with prompting improving GPT-3 models but harming chat models' performance. The paper lacks per-model split between GPT-3.5 and GPT-4 for many measures, and no model parameter sizes/training corpora are reported here. Verification remains a major failure mode for chat models.",
            "uuid": "e5316.2",
            "source_info": {
                "paper_title": "Decoding Stumpers: Large Language Models vs. Human Problem-Solvers",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI chat model)",
            "brief_description": "OpenAI's GPT-4 chat-oriented model (finetuned with human feedback) evaluated on stumpers, exhibiting high solving accuracy but limited verification capability relative to humans.",
            "citation_title": "Decoding Stumpers: Large Language Models vs. Human Problem-Solvers",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI GPT-4 chat model, described in the paper as one of the new-generation chat models finetuned with human feedback; the paper does not report architecture specifics or parameter counts.",
            "model_size": null,
            "cognitive_test_name": "Stumpers (Bar-Hillel stumpers dataset)",
            "cognitive_test_type": "insight / problem-solving / intuition (creative riddle solving)",
            "cognitive_test_description": "As above: single-step intuition riddles that mislead intuitive framing; solutions are unique and easily verifiable by humans once revealed. Tasks included answer generation (naïve and with exemplars) and two-alternative verification.",
            "llm_performance": "GPT-4 was part of the chat-model group that achieved mean solving m = 57.8% on the original 76-stumper dataset (43.4% on the enhanced dataset). The paper states overall LLM solving rates ranged from ~26%–58% depending on model; GPT-4 is at the upper end of that range as part of the chat models. In verification tasks, models (including GPT-4) performed poorly overall (group mean ≈41%).",
            "human_baseline_performance": "Humans solved 38.15% of stumpers (n = 81; F = 48%; ages 20–54, mean = 28.52, SD = 7.73). For verification humans achieved 100% when they had generated the solution and 63.8% when they had not.",
            "performance_comparison": "As a generator GPT-4 (within the chat-model group) outperformed humans on the stumpers solving task (57.8% vs. human 38.15%), with the paper reporting a significant model-family effect [F(1,4) = 686, p = 0.00001]. For verification, GPT-4 (like other models) underperformed humans, contributing to an overall model verification mean below human levels. Authors highlight a qualitative dissociation: GPT-3.5/GPT-4 solve stumpers better than they verify them, opposite the pattern seen for GPT-3 models.",
            "notable_differences_or_limitations": "Paper does not provide isolated per-model percentages for GPT-4 vs GPT-3.5 in many reported aggregates, so exact GPT-4-only metrics are not fully separable in the text. Verification is a notable failure mode; prompting effects were negative for chat models. Dataset size and reliance on OpenAI models are limitations noted by the authors.",
            "uuid": "e5316.3",
            "source_info": {
                "paper_title": "Decoding Stumpers: Large Language Models vs. Human Problem-Solvers",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Stumpers: an annotated compendium",
            "rating": 2,
            "sanitized_title": "stumpers_an_annotated_compendium"
        },
        {
            "paper_title": "Learning psychology from riddles: The case of stumpers",
            "rating": 2,
            "sanitized_title": "learning_psychology_from_riddles_the_case_of_stumpers"
        },
        {
            "paper_title": "Solving stumpers, crt and crat: Are the abilities related?",
            "rating": 2,
            "sanitized_title": "solving_stumpers_crt_and_crat_are_the_abilities_related"
        },
        {
            "paper_title": "Human-like intuitive behavior and reasoning biases emerged in language models-and disappeared in gpt-4",
            "rating": 1,
            "sanitized_title": "humanlike_intuitive_behavior_and_reasoning_biases_emerged_in_language_modelsand_disappeared_in_gpt4"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 1,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        }
    ],
    "cost": 0.01143075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Decoding Stumpers: Large Language Models vs. Human Problem-Solvers</p>
<p>Alon Goldstein 
Miriam Havin 
Department of Cognitive and Brain Sciences
Hebrew University
Jerusalem</p>
<p>Roi Reichart 
Faculty of Data and Decision Sciences
Technion</p>
<p>Ariel Goldstein </p>
<p>The Hebrew University Business School
JerusalemIsrael</p>
<p>Google Research</p>
<p>Decoding Stumpers: Large Language Models vs. Human Problem-Solvers
E1CE21C388E974E97B58404E9DE66B54
This paper investigates the problem-solving capabilities of Large Language Models (LLMs) by evaluating their performance on stumpers, unique single-step intuition problems that pose challenges for human solvers but are easily verifiable.We compare the performance of four state-of-the-art LLMs (Davinci-2, Davinci-3, GPT-3.5-Turbo,GPT-4) to human participants.Our findings reveal that the new-generation LLMs excel in solving stumpers and surpass human performance.However, humans exhibit superior skills in verifying solutions to the same problems.This research enhances our understanding of LLMs' cognitive abilities and provides insights for enhancing their problemsolving potential across various domains 1 .</p>
<p>Introduction</p>
<p>Since their inception, Large Language Models (LLMs) have astonished the scientific community with their ability to tackle complex tasks (Radford et al., 2019;Brown et al., 2020;Devlin et al., 2018).These emerging capabilities, along with shared principles with human cognition and the brain, have motivated significant efforts to utilize deep language models and, recently, LLMs for explaining neural activity (Tikochinski et al., 2023;Goldstein et al., 2022b,a;Schwartz et al., 2019), predicting human behavior (Goldstein et al., 2022b;Brand et al., 2023), and even providing a theoretical framework for the human mind (Richards et al., 2019;Hasson et al., 2020).Recent advancements, particularly the ability of LLMs to perform tasks requiring different skills such as mathematical calculations, analytical reasoning and use of world knowledge, have led several papers to declare that LLMs possess what is termed in the cognitive literature System 2 capabilities (Matsuo, 2020;Kojima et al., 2022).The dual-system model of the mind has arguably been the most prevalent model of thought and behavior in psychology, economics and social science in general (Goldstein and Young, 2022;Evans and Stanovich, 2013;Chaiken and Trope, 1999;Gawronski and Creighton, 2013), especially in addressing systematic limitations of cognitive and artificial systems.In simple terms, System 1 is associated with effortless, associative processes and is often thought of as compatible with neural nets implementation, while System 2 is related to effortful, serial, and often symbolic processes (Frankish, 2010;Evans, 2003).A famous example where System 1's heuristic hinders a solution is described in Box 1.</p>
<p>Box 1: The bat and the ball "A bat and a ball cost 1.10 dollars in total.The bat costs 1 dollar more than the ball.How much does the ball cost?"The immediate but incorrect response is to assume that the ball costs 10 cents.However, a symbolic-serial approach formulation of the problem "x + (x + 1) = 1.10" yields the correct solution of 0.05 dollars.</p>
<p>While this type of questions (Cognitive Reflective Test; CRT) are considered hard to solve, as they tend to elicit wrong responses (Frederick, 2005;Toplak et al., 2011), people can be primed to solve them correctly by insisting on a formalist approach (i.e., applying System 2 instead of System 1; (Alter et al., 2007)).In contrast, problems that require insight (i.e., have neither an intuitive/associative solution nor a symbolic one) are hard for humans and often elicit no response (i.e., humans are "stuck"; (Bar-Hillel et al., 2018;Bar-Hillel, 2021)).</p>
<p>arXiv:2310.16411v1 [cs.CL] 25 Oct 2023</p>
<p>Consider, for example, the riddle in Box 2: Box 2: Blood relatives "Alex is Bobbie's blood relative, and Bobbie is Charlie's blood relative, but Alex is not a blood relative of Charlie.How come?".Answer: Alex and Charlie can be related to Bobbie from different sides of the family, e.g., they could be his parents, uncles, etc.This riddle typically challenges human intuition (System 1) because humans tend to consider Alex, Bobbie, and Charlie as blood relatives.However, a symbolic solution (System 2) is typically also not available to humans who try to solve it, as there is no clear algorithm to follow to reach a solution.Facing this question, humans seem to be anchored (or stuck) in the framing according to which the three men are blood relatives and cannot escape it to generate an alternative framing of the problem that would yield effective explanations of the situation (Bar-Hillel, 2021).</p>
<p>The above-mentioned question is an example of a stumper.A stumper is a one-step intuition riddle, the solution to which is typically so elusive that it does not come to mind, at least initially -leaving the responder stumped.Stumpers do not fall within the System 1 or System 2 frameworks but are related to creative thinking (Bar-Hillel et al., 2019).Importantly, once presented with a solution, people can easily classify it as right or wrong -a simple system-2 task.In this paper, we demonstrate that recent LLMs (e.g., GPT-3.5 and GPT-4) outperform humans in solving stumpers but lag behind humans in classifying solutions as right or wrong.</p>
<p>Task</p>
<p>A stumper is a single-step intuition riddle in which a misleading cue sparks a visual or semantic representation that blocks the solution from coming to mind.Unlike other riddles, there is no need for further computation or thinking, and once the obstacle is removed, the answer is clear.See examples in Appendix A.</p>
<p>The dataset used for our analysis consists of all 76 stumpers curated in (Bar-Hillel, 2021).Each stumper is a textual description of a narrative or scenario that requires a unique solution.To enhance the number of stumpers beyond this exhaustive list, we generated two similar riddles for each stumper, by asking GPT-3.5-Turbo to change the names and wording of the original data-set.After the generation, we manually approved or edited each stumper to reduce confusion and alternative solutions as much as we could.This process resulted in a set of additional 152 stumpers.As the set of new stumpers was not validated with human participants and may differ from the original set, we present the results for the original set in the body of this paper and the detailed results in the appendix.The data also includes correct and incorrect solutions, which allowed a comparative analysis of the accuracy and reasoning strategies of the responses given by models and human participants.A dataset sample can be found in Appendix A.</p>
<p>Models and Experiments</p>
<p>The study involved four language models: Davinci-2, Davinci-3, GPT-3.5-turbo, and GPT-4.Additionally, 81 human participants (F=48%; ages 20-54, m=28.52,sd=7.73) were recruited via an online survey participation platform (Prolific).When solving each stumper, both humans and models were presented with a prompt.To normalize the answers across conditions, models, and participants, all prompts started with a standardized definition of a correct answer to a riddle:</p>
<p>An answer to a riddle is correct only if it is consistent with all the riddle's clues, sensical, specific, logical, and fitting with the context of the riddle.</p>
<p>See prompts examples in Appendix C.</p>
<p>Answer Generation</p>
<p>To avoid learning, each participant was presented with only one stumper and was asked to answer it or type "IDK" if they did not know the answer.Each model, in each prompt, was presented either with only one stumper ("naïve response") or with two other pairs of riddles and their ground-truth answer ("prompted response").</p>
<p>Answer Verification</p>
<p>After answering the riddle or typing "IDK", human participants were presented with the two possible solutions and were asked to choose the correct one.The models were presented with the same choice without their previous response in the prompt.</p>
<p>Answer Verification -Models' response</p>
<p>To further compare, the models were given a verification problem where their own answers replaced one of the answers.For riddles to which the model knew (/did not know) the answer, their response was used instead of the correct (/incorrect) ground truth.</p>
<p>Results and Observations</p>
<p>We replaced participants who reported knowing their riddle or finding the answer online.Two authors evaluated the responses unanimously, with only one response being disagreed upon, leading to its exclusion.</p>
<p>LLMs proficiency at solving stumpers Our results are provided in Figure 1.Solving a stumper by chance has virtually zero probability, given the infinitesimal likelihood of randomly arriving at the correct solution among countless potential answers.Human participants in our sample have accurately solved 38.15% of the stumpers, replicating the 35% accuracy found in (Bar-Hillel, 2021).</p>
<p>Improved performance of the advanced models</p>
<p>A two-way ANOVA was conducted to examine the effects of the models and the prompting: The chat models (GPT-3.5-Turboand GPT-4; m=57.8% for the original stumper, 43.4% for the enhanced dataset) have performed significantly better than the GPT-3 model (Davinci-2 and Davinci-3; m=29.6% for the original stumper, 22.5% for the enhanced data-set) [F(1,4)=686, p=0.00001].The main effect of Prompt was not significant [F(1, 4) = 0.023, p = 0.887], but the interaction was [F(1, 4) = 30.82,p = 0.005], indicating that the prompt has a positive impact on the performance of the GPT-3 models and a negative impact on the performance of the chat models.See results for the original data-set in figure 1. See results for the enhanced set in Appendix B.</p>
<p>Answer Verification The answer verification task was tested once with the ground-truth answers (Figure 2) and once with the model's responses vs. ground truth (Figure 3).Different scores are reported for correct and incorrect responses.</p>
<p>While humans performed perfectly at verification when knowing to generate the answer (100%) and above chance even when they failed to generate (63.8%), most models performed below the chance level (m=41%).A two-way ANOVA was conducted to compare the models' ability to choose the Answer Generation</p>
<p>Discussion and Conclusions</p>
<p>This study compared the stumper-solving abilities of LLMs and humans.We found that while the Answer Verification: Model's response  LLMs are better than humans at solving stumpers, their answer-verification abilities are limited and fall far from human performance.These findings provide valuable insights into the capabilities and limitations of LLMs, their relationship with human cognition, and the potential for utilizing LLMs as a framework for cognitive capacities.
D a v i n c i -2 D a v i n c i -3 G P T -3 . 5 -T u
The results showed that LLMs, specifically the LLMs used in this study, demonstrated proficiency in solving stumpers that are obstructed by misleading representations.The models correctly solved 26%-58% of the stumpers, outperforming human participants and surpassing the chance level (Figure 1).This suggests that LLMs possess the skills required for solving these types of questions.</p>
<p>The study revealed an improvement in performance for more advanced models.The chat models, GPT-3.5-Turbo and GPT-4, which are finetuned with human feedback during training, outperformed the GPT-3 models (Davinci-2 and Davinci-3).This indicates that advancements in model training contribute to enhanced problem-solving abilities.Prompting the models with additional pairs of riddles and their ground-truth answers had a positive impact on the performance of the GPT-3 models, further emphasizing the importance of context and prior knowledge in solving stumpers.</p>
<p>Despite their ability to generate correct answers, the models fell short in the task of answer verification compared to human participants (Figure 2).To further stress this problem, we have asked the models to compare their correct responses against a false response (Figure 3, full bars), demonstrating their inconsistency and inability to verify an-swers.Humans, however, demonstrated a higher proficiency in recognizing the correct answer, even when they were unable to solve the problem initially (Figure 2).This suggests that humans possess a verification capability, considered a System 2 process, which has not yet fully emerged in LLMs.Interestingly, the Davinci-3 model showed good performance in recognizing correct answers curated by the authors (70%; Figure 2) and by itself (85%; Figure 3).</p>
<p>The overall pattern of results suggests that GPT-3 aligns better with human capabilities, as its answer-verification capabilities are better than its solving capabilities.This pattern stands in contrast to GPT-3.5 and GPT-4, which solve stumpers better than they verify their solutions.This finding indicates that for disciplines interested in using LLMs to model human behavior or cognition, Davinci-3 is likely a more suitable model to employ.This is in line with (Hagendorff and Fabi, 2023), which shows how GPT-3 (but not GPT-4 and GPT-3.5)exhibits similar biases and errors as demonstrated by humans.Another reason to consider Davinci-3 over GPT-4 and GPT-3.5 in modeling human behavior is the fact that the results acquired here, as well as the psychological literature, suggest that it is easier for humans to classify a correct response than generates it (Pintrich, 2002), a pattern of result similar to Davinci-3 and not congruent with GPT-4 and GPT-3.5 performance.This is closely related to the literature showing that recognition is considered easier than recall, as the former requires only identifying the presence of familiar information, whereas the latter demands retrieving specific information from memory without external cues (Roediger III and Karpicke, 2006).</p>
<p>The challenge of answer verification is closely related to the problem of text classification, which has been found to be challenging for LLMs (Sun et al., 2023).There is a significant discrepancy between the abilities to generate a correct answer and to classify a correct response.This has important implications for estimating LLM capabilities, as many NLP benchmarks are designed based on the model's ability to classify correct answers (Rajpurkar et al., 2016;Wang et al., 2018;Dagan et al., 2005;Reddy et al., 2019;Clark et al., 2019).One possible implication is the necessity of including interaction-based measures (Collins et al., 2023), based on continuous human-LLM interaction, when evaluating LLMs.Like in oral ex-ams, the opportunity to react to the models' output in tailored follow-up questions allows the evaluator a deeper probing into the models' capabilities (Gharibyan, 2005;Davis and Karunathilake, 2005).</p>
<p>Furthermore, the findings from this study can inform the development of benchmark tasks for evaluating the intelligence and human-like behavior of LLMs.Stumpers provide a challenging domain that tests problem-solving, associative capacities, and verification skills.By designing more comprehensive benchmarks and evaluating LLMs' performance on these tasks, we can gain a better understanding of their cognitive capabilities and identify areas for improvement.</p>
<p>In conclusion, this study investigated the ability of large language models (LLMs) to solve stumpers, challenging riddles characterized by elusive solutions.Our findings demonstrate that LLMs, especially the advanced models GPT-3.5-Turbo and GPT-4, exhibit a remarkable proficiency in solving stumpers, surpassing human performance in some cases.These results highlight the potential of LLMs as powerful problem-solving tools and provide insights into the cognitive processes involved in solving complex puzzles.Our analysis also uncovered that the human ability to verify solutions has not been fully developed yet in LLMs.Future research can build upon these findings to explore the role of context, expand the variety of stumpers, and investigate the generalizability of LLMs in different domains, contributing to developing more robust and human-like artificial intelligence.</p>
<p>Limitations</p>
<p>Our study on stumpers and large language models (LLMs) has several limitations to consider.Firstly, the limited number of 76 validated stumper, even with two more versions of each in the enhanced dataset, potentially restricting the representativeness and generalizability of our findings.Secondly, we focused exclusively on OpenAI models, limiting the scope of comparison with other language models.Thirdly, subjective judgment was involved in evaluating correctness, leading to potential variations in interpretations.Lastly, prompt engineering techniques were underutilized, potentially limiting the models' problem-solving potential.Future research should address these limitations for more robust and comprehensive insights into LLMs' problem-solving abilities.</p>
<p>C Prompt examples</p>
<p>Answer Generation, Naïve, GPT-3 models An answer to a riddle is correct only if it is consistent with all the riddle's clues, sensical, specific, logical, and fitting with the context of the riddle.</p>
<p>--</p>
<p>Riddle:</p>
<p>A father and son were involved in a traffic accident.The father was killed, and the son was rushed to hospital.The surgeon walked into the operating room, and upon seeing the severely wounded boy cried out: "OMG, it is my son!".How could this be true?</p>
<p>Answer:</p>
<p>Answer Generation, Prompt, GPT-3 models An answer to a riddle is correct only if it is consistent with all the riddle's clues, sensical, specific, logical, and fitting with the context of the riddle.</p>
<p>--Riddle:</p>
<p>Long after the screen of Kim's smart phone had cracked.It was still functioning just fine.</p>
<p>Before he could replace it, the phone accidentally fell into the family's swimming pool.It was retrieved almost at once, but -alas -the phone was dead.Yet no water had penetrated the cracked screen, so all the critical components remained completely dry.Explain briefly why the phone was dead.</p>
<p>Answer: the pool was empty --Riddle:</p>
<p>Polly bought a beautiful parrot.The seller guaranteed that the bird repeats everything it hears.However, try as Polly might to teach it, her squawking parrot never repeated a single word.The seller did not lie.Explain briefly.</p>
<p>Answer:</p>
<p>The parrot was deaf --Riddle: An accountant says: "That attorney is my brother", and that is true -they really do have the same parents.Yet the attorney denies having any brothers -and that is also true! How is that possible?</p>
<p>Answer:</p>
<p>Figure 1 :Figure 2 :
12
Figure 1: performance of the different models.Human performance is presented in the dashed line.</p>
<p>Figure 3 :
3
Figure3: models' accuracy in choosing a solution between their previous response and the ground truth.</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: Perfomrnace scores for the different models on the enhanced data-set.The dashed line indicates human performance in the original set.</p>
<p>The data is available at https://github.com/Alon-Go/ Stumpers-LLMs
Appendix A Stumpers examples1 A father and son were involved in a traffic accident.The father was killed, and the son was rushed to hospital.The surgeon walked into the operating room, and upon seeing the severely wounded boy cried out: "OMG, it is my son!".How could this be true?Answer: The surgeon is the boy's mother 2 A very tall man was holding up a wine decanter way above his head.He let go of it, and it dropped to the carpet he was standing on.Explain briefly how not a single drop of wine was spilled.Answer: The decanter was empty 3 Farmer Joe eats two fresh eggs from his own farm for breakfast every day.Yet there are no chickens on his farm.Where does Farmer Joe get his eggs?Answer: Famer Joe does not eat chicken eggs, but a different animal's egg, such as ducks.Marcy went from one bank of a river to the one 20 meters across.There are no bridges on the river.Marcy had no equipment, no devices, no special clothing, and she can't even swim.She relied on her own body only -and none of it got wet!Explain briefly how she managed this.Answer: The river was dry.The taller one was the brother of the shorter one, but the shorter one was not the brother of the taller one.Explain in a few words how that is possible.'}, {'role': 'assistant', 'content': 'Answer:\n'}], 'temperature': 0.0, 'frequency_penalty': 1.0, 'presence_penalty': 0.5, 'n': 1, 'max_tokens': 120}B Enhanced data-set resultsAnswer GenerationAnswer Verification, GPT-3 models An answer to a riddle is correct only if it is consistent with all the riddle's clues, be sensical, specific, logical, and fitting with the context of the riddle.Riddle:Farmer Joe eats two fresh eggs from his own farm for breakfast every day.Yet there are no chickens on his farm.Where does Farmer Joe get his eggs?Answers: 1. Famer Joe do not eat chicken eggs, but a different animal's egg, such as ducks.2. Farmer Joe gets his eggs from the grocery store.Which of these answers is correct?Answer Verification, Chat models {'model': 'gpt-3.5-turbo','messages': [ {'role': 'system', 'content': "An answer to a riddle is correct only if it is consistent with all the riddle's clues, be sensical, specific, logical, and fitting with the context of the riddle."},{'role': 'user', 'content': "Riddle:\nAlex is Bobbie's blood relative, and Bobbie is Charlie's blood relative, but Alex is not a blood relative of Charlie.How come?\n\nAnswers:\n1.Alex and Charlie could be Bobby's parents, making them both Bobby's blood relatives but not each other's\n\n 2.Alex is Bobbie's parent, and Bobbie is Charlie's parent, but Alex is not a parent of Charlie.\n\nWhich of these answers is correct?"}, {'role': 'assistant', 'content': 'The correct answer is number '}], 'temperature': 0.0, 'frequency_penalty': 1.0, 'presence_penalty': 0.5, 'n': 1, 'max_tokens': 20}
Overcoming intuition: metacognitive difficulty activates analytic reasoning. Adam L Alter, M Daniel, Nicholas Oppenheimer, Rebecca N Epley, Eyre, Journal of experimental psychology: General. 13645692007</p>
<p>Stumpers: an annotated compendium. Maya Bar, - Hillel, Thinking &amp; Reasoning. 2742021</p>
<p>Learning psychology from riddles: The case of stumpers. Maya Bar-Hillel, Tom Noah, Shane Frederick, Judgment and Decision Making. 1312018</p>
<p>Solving stumpers, crt and crat: Are the abilities related?. Maya Bar-Hillel, Tom Noah, Shane Frederick, Judgment and Decision Making. 1452019</p>
<p>Using gpt for market research. James Brand, Ayelet Israeli, Donald Ngwe, SSRN 4395751. 2023</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Dual-process theories in social psychology. Shelly Chaiken, Yaacov Trope, 1999Guilford Press</p>
<p>Boolq: Exploring the surprising difficulty of natural yes/no questions. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova, NAACL. 2019</p>
<p>Katherine M Collins, Albert Q Jiang, Simon Frieder, Lionel Wong, Miri Zilka, Umang Bhatt, Thomas Lukasiewicz, Yuhuai Wu, Joshua B Tenenbaum, William Hart, arXiv:2306.01694Evaluating language models for mathematics through interactions. 2023arXiv preprint</p>
<p>The PASCAL recognising textual entailment challenge. Ido Dagan, Oren Glickman, Bernardo Magnini, Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, And Recognising Textual Entailment. 2005</p>
<p>The place of the oral examination in today's assessment systems. H Margery, Indika Davis, Karunathilake, Medical teacher. 2742005</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. Bert2018arXiv preprint</p>
<p>In two minds: dualprocess accounts of reasoning. Jonathan St, B T Evans, Trends in cognitive sciences. 7102003</p>
<p>Dual-process theories of higher cognition: Advancing the debate. Jonathan St, B T Evans, Keith E Stanovich, Perspectives on psychological science. 832013</p>
<p>Dual-process and dualsystem theories of reasoning. Keith Frankish, Philosophy Compass. 5102010</p>
<p>Cognitive reflection and decision making. Shane Frederick, Journal of Economic perspectives. 1942005</p>
<p>Dual process theories 14. The Oxford handbook of social cognition. Bertram Gawronski, Laura A Creighton, 2013282</p>
<p>Assessing students' knowledge: oral exams vs. written tests. Hasmik Gharibyan, ACM SIGCSE Bulletin. 3732005</p>
<p>The unconscious mind. Alon Goldstein, Benjamin D Young, Mind, Cognition, and Neuroscience. Routledge2022</p>
<p>Brain embeddings with shared geometry to artificial contextual embeddings, as a code for representing language in the human brain. Ariel Goldstein, Avigail Dabush, Bobbi Aubrey, Mariano Schain, Zaid Samuel A Nastase, Eric Zada, Zhuoqiao Ham, Amir Hong, Harshvardhan Feder, Gazula, BioRxiv. 2022a</p>
<p>Alon Cohen, et al. 2022b. Shared computational principles for language processing in humans and deep language models. Ariel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain, Amy Price, Bobbi Aubrey, Amir Samuel A Nastase, Dotan Feder, Emanuel, Nature neuroscience. 253</p>
<p>Human-like intuitive behavior and reasoning biases emerged in language models-and disappeared in gpt-4. Thilo Hagendorff, Sarah Fabi, arXiv:2306.076222023arXiv preprint</p>
<p>Direct fit to nature: An evolutionary perspective on biological and artificial neural networks. Uri Hasson, Ariel Samuel A Nastase, Goldstein, Neuron. 10532020</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.119162022arXiv preprint</p>
<p>Special features of deep learning and symbol emergence. Yutaka Matsuo, New Generation Computing. 382020</p>
<p>The role of metacognitive knowledge in learning, teaching, and assessing. Theory into practice. Paul R Pintrich, 200241</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, arXiv:1606.05250Squad: 100,000+ questions for machine comprehension of text. 2016arXiv preprint</p>
<p>Coqa: A conversational question answering challenge. Siva Reddy, Danqi Chen, Christopher D Manning, Transactions of the Association for Computational Linguistics. 72019</p>
<p>A deep learning framework for neuroscience. Timothy P Blake A Richards, Philippe Lillicrap, Yoshua Beaudoin, Rafal Bengio, Amelia Bogacz, Claudia Christensen, Clopath, Ponte Rui, Archy Costa, Surya De Berker, Ganguli, Nature neuroscience. 22112019</p>
<p>Test-enhanced learning: Taking memory tests improves long-term retention. Iii Henry L Roediger, Jeffrey D Karpicke, Psychological science. 1732006</p>
<p>Inducing brain-relevant bias in natural language processing models. Advances in neural information processing systems. Dan Schwartz, Mariya Toneva, Leila Wehbe, 201932</p>
<p>Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, Guoyin Wang, arXiv:2305.08377Text classification via large language models. 2023arXiv preprint</p>
<p>Perspective changes in human listeners are aligned with the contextual transformation of the word embedding space. Refael Tikochinski, Ariel Goldstein, Yaara Yeshurun, Uri Hasson, Roi Reichart, Cerebral Cortex. 822023</p>
<p>The cognitive reflection test as a predictor of performance on heuristics-and-biases tasks. Maggie E Toplak, Richard F West, Keith E Stanovich, Memory &amp; cognition. 3972011</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, 2018</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7236 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7236</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7236</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-269790894</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.10212v3.pdf" target="_blank">CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations</a></p>
                <p><strong>Paper Abstract:</strong> In this paper, we introduce a novel psychological benchmark, CPsyExam, constructed from questions sourced from Chinese language examinations. CPsyExam is designed to prioritize psychological knowledge and case analysis separately, recognizing the significance of applying psychological knowledge to real-world scenarios. From the pool of 22k questions, we utilize 4k to create the benchmark that offers balanced coverage of subjects and incorporates a diverse range of case analysis techniques.Furthermore, we evaluate a range of existing large language models~(LLMs), spanning from open-sourced to API-based models. Our experiments and analysis demonstrate that CPsyExam serves as an effective benchmark for enhancing the understanding of psychology within LLMs and enables the comparison of LLMs across various granularities.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7236.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7236.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's state-of-the-art generative pre-trained transformer used as both an evaluated model and as an automatic judge for QA outputs in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large generative transformer-based language model developed by OpenAI; used here as an evaluated LLM on CPsyExam.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>CPsyExam - Knowledge (KG, multiple-choice: SCQ & MAQ)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>psychological knowledge (domain knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice knowledge questions sampled from Chinese psychology examinations; includes single-choice (SCQ) and multiple-answer (MAQ) items to test fact-based psychological knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%) reported separately for SCQ and MAQ and averaged</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Knowledge Avg accuracy 67.43%; Zero-shot SCQ 76.56%; Zero-shot MAQ 10.76%; Few-shot SCQ 78.63%; Few-shot MAQ 43.79% (values taken from Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot and few-shot (5-shot) evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-4 attains the highest knowledge scores among evaluated models but shows low performance on MAQ items; no human test-taking baseline is reported for these KG items.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7236.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7236.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 evaluated on case-oriented psychology questions to measure reasoning and application abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large generative transformer-based language model developed by OpenAI; evaluated on CPsyExam case analysis (CA).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>CPsyExam - Case Analysis (CA, SCQ & MAQ)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning / applied clinical judgement</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Case-analysis questions derived from real exams (identification, reasoning, application) assessing applied diagnostic and intervention reasoning in psychology; presented as MCQs (SCQ/MAQ) and open QA.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%) reported separately for SCQ and MAQ</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Case Analysis zero-shot SCQ 60.33%; zero-shot MAQ 13.00%; few-shot SCQ 64.17%; few-shot MAQ 39.50% (values from Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot and few-shot (5-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper reports GPT-4 is strongest on knowledge but still challenged on CA, with main gaps in REASONING and APPLICATION; no human baseline of test-taking (e.g., human examinee accuracy) is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7236.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7236.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGLM-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGLM-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A performant proprietary conversational LLM used in the experiments that shows strong case-analysis performance relative to other models in CPsyExam.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGLM-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conversational language model (proprietary variant in ChatGLM family) evaluated on CPsyExam tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>CPsyExam - Knowledge (KG, SCQ & MAQ)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>psychological knowledge (domain knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice knowledge questions from Chinese psychology exams (SCQ and MAQ).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Knowledge Avg accuracy 64.58%; Zero-shot SCQ 63.29%; Zero-shot MAQ 26.12%; Few-shot SCQ 73.85%; Few-shot MAQ 42.13% (values from Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot and few-shot (5-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>ChatGLM-Turbo performs very well on CA relative to other proprietary models; no human test-taking baseline reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7236.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7236.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGLM-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGLM-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Conversational LLM from the ChatGLM family evaluated on case-analysis psychology questions in CPsyExam.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGLM-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conversational language model evaluated on reasoning-oriented case analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>CPsyExam - Case Analysis (CA, SCQ & MAQ)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning / applied clinical judgement</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Case-analysis MCQs testing identification, reasoning, and application of psychological methods.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Case Analysis zero-shot SCQ 69.00%; zero-shot MAQ 20.50%; few-shot SCQ 65.33%; few-shot MAQ 42.50% (values from Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot and few-shot (5-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper highlights ChatGLM-Turbo's strong CA performance (sometimes exceeding other proprietary models) but does not provide human examinee baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7236.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7236.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's conversational model (GPT family) evaluated on CPsyExam knowledge and case-analysis tasks, showing moderate performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative conversational LLM from OpenAI (GPT-family) evaluated on CPsyExam KG and CA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>CPsyExam - Knowledge (KG, SCQ & MAQ)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>psychological knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice questions assessing fact-based psychology knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Knowledge Avg accuracy 51.15%; Zero-shot SCQ 57.43%; Zero-shot MAQ 11.14%; Few-shot SCQ 61.53%; Few-shot MAQ 24.71% (values from Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot and few-shot (5-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>No human baseline performance on these KG items is given in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7236.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7236.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI ChatGPT evaluated on CPsyExam case-analysis items and also rated by human experts for QA answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conversational LLM evaluated on both MCQ case-analysis and open-ended QA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>CPsyExam - Case Analysis (CA, SCQ & MAQ)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning / application</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Case-analysis MCQs and open QA assessing applied problem identification and intervention reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%) for MCQs; QA scored on a 0-100 rubric</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Case Analysis zero-shot SCQ 47.33%; zero-shot MAQ 9.00%; few-shot SCQ 52.67%; few-shot MAQ 29.50% (MCQ values from Table 2). QA evaluation: GPT-4 scored ChatGPT responses 72.88/100 and certified national psychological counselors gave mean score 69.63/100 on a 20-question QA sample (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot and few-shot for MCQs; open-ended QA evaluated by GPT-4 and human experts</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Certified national psychological counselors in China (20 randomly selected QA questions)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Pearson correlation between GPT-4 and human expert scores reported as 0.98 (for QA scoring agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The human scores reported are evaluative ratings of model outputs (expert rater scores), not human test-taking accuracy measured by having humans answer the same exam items under test conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7236.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7236.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ERNIE-Bot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ERNIE-Bot</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baidu's proprietary language model evaluated on CPsyExam KG/CA; included in QA evaluation comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ERNIE-Bot</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary LLM from Baidu evaluated on CPsyExam tasks including QA scored by GPT-4 and by human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>CPsyExam - Knowledge (KG, SCQ & MAQ)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>psychological knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice knowledge questions from Chinese psychology examinations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%) and QA scoring (0-100)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Knowledge Avg accuracy 43.85%; Zero-shot SCQ 52.48%; Zero-shot MAQ 6.66%; Few-shot SCQ 56.10%; Few-shot MAQ 10.37% (Table 2). QA evaluation: GPT-4 scored ERNIE-Bot 73.55/100 and human experts scored 71.63/100 on the 20-question QA sample (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot and few-shot for MCQs; open-ended QA evaluated by GPT-4 and human experts</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Certified national psychological counselors in China (20 QA items)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Pearson correlation between GPT-4 and human expert scores reported as 0.98 (for QA scoring agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Human numbers are expert rater scores of model outputs; the paper explicitly notes that using GPT-4 to evaluate QA scores may be influenced by GPT-4's own knowledge and that future work should combine expert scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7236.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7236.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QA expert scoring (aggregated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expert human evaluation of LLM QA outputs (certified national psychological counselors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human expert rater scores (national certified psychological counselors in China) assigned to model-generated answers for 20 QA items to assess consistency, professionalism, and reasonableness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Human expert raters (used to score model QA outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Certified national psychological counselors scored generated QA responses on a 100-point rubric (30 pts consistency with answer, 30 pts professionalism, 40 pts reasonableness).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>CPsyExam - QA (open-ended subjective questions)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>applied clinical communication and reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Open-ended QA items judged by experts along three dimensions producing a composite score out of 100.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>mean score out of 100 (expert raters) and GPT-4 automatic score out of 100</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>N/A as a human baseline answering the examâ€”these are human ratings of model outputs; reported expert scores for models: ERNIE-Bot 71.63/100; ChatGLM-Turbo 76.20/100; ChatGPT 69.63/100 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 automatic scores for same model outputs: ERNIE-Bot 73.55/100; ChatGLM-Turbo 77.79/100; ChatGPT 72.88/100 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>models generated open-ended QA responses; responses were then scored by experts and by GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Certified national psychological counselors in China (20 randomly selected QA questions)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Pearson correlation between experts' scores and GPT-4's scores = 0.98 (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>These expert scores measure the quality of model outputs, not human ability to take the exam; the authors caution that GPT-4-based scoring may be biased and suggest future combined scoring with experts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Psybench: a balanced and indepth psychological chinese evaluation benchmark for foundation models <em>(Rating: 2)</em></li>
                <li>PsyEval: A comprehensive large language model evaluation benchmark for mental health <em>(Rating: 2)</em></li>
                <li>Psy-llm: Scaling up global mental health psychological services with ai-based large language models <em>(Rating: 1)</em></li>
                <li>SMILE: Single-turn to multi-turn inclusive language expansion via ChatGPT for mental health support <em>(Rating: 1)</em></li>
                <li>Measuring massive multitask language understanding <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7236",
    "paper_id": "paper-269790894",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "OpenAI's state-of-the-art generative pre-trained transformer used as both an evaluated model and as an automatic judge for QA outputs in this paper's experiments.",
            "citation_title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large generative transformer-based language model developed by OpenAI; used here as an evaluated LLM on CPsyExam.",
            "model_size": null,
            "test_name": "CPsyExam - Knowledge (KG, multiple-choice: SCQ & MAQ)",
            "test_category": "psychological knowledge (domain knowledge)",
            "test_description": "Multiple-choice knowledge questions sampled from Chinese psychology examinations; includes single-choice (SCQ) and multiple-answer (MAQ) items to test fact-based psychological knowledge.",
            "evaluation_metric": "accuracy (%) reported separately for SCQ and MAQ and averaged",
            "human_performance": null,
            "llm_performance": "Knowledge Avg accuracy 67.43%; Zero-shot SCQ 76.56%; Zero-shot MAQ 10.76%; Few-shot SCQ 78.63%; Few-shot MAQ 43.79% (values taken from Table 2)",
            "prompting_method": "zero-shot and few-shot (5-shot) evaluated",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "GPT-4 attains the highest knowledge scores among evaluated models but shows low performance on MAQ items; no human test-taking baseline is reported for these KG items.",
            "uuid": "e7236.0",
            "source_info": {
                "paper_title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "OpenAI's GPT-4 evaluated on case-oriented psychology questions to measure reasoning and application abilities.",
            "citation_title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large generative transformer-based language model developed by OpenAI; evaluated on CPsyExam case analysis (CA).",
            "model_size": null,
            "test_name": "CPsyExam - Case Analysis (CA, SCQ & MAQ)",
            "test_category": "reasoning / applied clinical judgement",
            "test_description": "Case-analysis questions derived from real exams (identification, reasoning, application) assessing applied diagnostic and intervention reasoning in psychology; presented as MCQs (SCQ/MAQ) and open QA.",
            "evaluation_metric": "accuracy (%) reported separately for SCQ and MAQ",
            "human_performance": null,
            "llm_performance": "Case Analysis zero-shot SCQ 60.33%; zero-shot MAQ 13.00%; few-shot SCQ 64.17%; few-shot MAQ 39.50% (values from Table 2)",
            "prompting_method": "zero-shot and few-shot (5-shot)",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Paper reports GPT-4 is strongest on knowledge but still challenged on CA, with main gaps in REASONING and APPLICATION; no human baseline of test-taking (e.g., human examinee accuracy) is provided.",
            "uuid": "e7236.1",
            "source_info": {
                "paper_title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ChatGLM-Turbo",
            "name_full": "ChatGLM-Turbo",
            "brief_description": "A performant proprietary conversational LLM used in the experiments that shows strong case-analysis performance relative to other models in CPsyExam.",
            "citation_title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
            "mention_or_use": "use",
            "model_name": "ChatGLM-Turbo",
            "model_description": "Conversational language model (proprietary variant in ChatGLM family) evaluated on CPsyExam tasks.",
            "model_size": null,
            "test_name": "CPsyExam - Knowledge (KG, SCQ & MAQ)",
            "test_category": "psychological knowledge (domain knowledge)",
            "test_description": "Multiple-choice knowledge questions from Chinese psychology exams (SCQ and MAQ).",
            "evaluation_metric": "accuracy (%)",
            "human_performance": null,
            "llm_performance": "Knowledge Avg accuracy 64.58%; Zero-shot SCQ 63.29%; Zero-shot MAQ 26.12%; Few-shot SCQ 73.85%; Few-shot MAQ 42.13% (values from Table 2)",
            "prompting_method": "zero-shot and few-shot (5-shot)",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "ChatGLM-Turbo performs very well on CA relative to other proprietary models; no human test-taking baseline reported.",
            "uuid": "e7236.2",
            "source_info": {
                "paper_title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ChatGLM-Turbo",
            "name_full": "ChatGLM-Turbo",
            "brief_description": "Conversational LLM from the ChatGLM family evaluated on case-analysis psychology questions in CPsyExam.",
            "citation_title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
            "mention_or_use": "use",
            "model_name": "ChatGLM-Turbo",
            "model_description": "Conversational language model evaluated on reasoning-oriented case analysis.",
            "model_size": null,
            "test_name": "CPsyExam - Case Analysis (CA, SCQ & MAQ)",
            "test_category": "reasoning / applied clinical judgement",
            "test_description": "Case-analysis MCQs testing identification, reasoning, and application of psychological methods.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": null,
            "llm_performance": "Case Analysis zero-shot SCQ 69.00%; zero-shot MAQ 20.50%; few-shot SCQ 65.33%; few-shot MAQ 42.50% (values from Table 2)",
            "prompting_method": "zero-shot and few-shot (5-shot)",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Paper highlights ChatGLM-Turbo's strong CA performance (sometimes exceeding other proprietary models) but does not provide human examinee baselines.",
            "uuid": "e7236.3",
            "source_info": {
                "paper_title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT",
            "brief_description": "OpenAI's conversational model (GPT family) evaluated on CPsyExam knowledge and case-analysis tasks, showing moderate performance.",
            "citation_title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "Generative conversational LLM from OpenAI (GPT-family) evaluated on CPsyExam KG and CA tasks.",
            "model_size": null,
            "test_name": "CPsyExam - Knowledge (KG, SCQ & MAQ)",
            "test_category": "psychological knowledge",
            "test_description": "Multiple-choice questions assessing fact-based psychology knowledge.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": null,
            "llm_performance": "Knowledge Avg accuracy 51.15%; Zero-shot SCQ 57.43%; Zero-shot MAQ 11.14%; Few-shot SCQ 61.53%; Few-shot MAQ 24.71% (values from Table 2)",
            "prompting_method": "zero-shot and few-shot (5-shot)",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "No human baseline performance on these KG items is given in the paper.",
            "uuid": "e7236.4",
            "source_info": {
                "paper_title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT",
            "brief_description": "OpenAI ChatGPT evaluated on CPsyExam case-analysis items and also rated by human experts for QA answers.",
            "citation_title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "Conversational LLM evaluated on both MCQ case-analysis and open-ended QA.",
            "model_size": null,
            "test_name": "CPsyExam - Case Analysis (CA, SCQ & MAQ)",
            "test_category": "reasoning / application",
            "test_description": "Case-analysis MCQs and open QA assessing applied problem identification and intervention reasoning.",
            "evaluation_metric": "accuracy (%) for MCQs; QA scored on a 0-100 rubric",
            "human_performance": null,
            "llm_performance": "Case Analysis zero-shot SCQ 47.33%; zero-shot MAQ 9.00%; few-shot SCQ 52.67%; few-shot MAQ 29.50% (MCQ values from Table 2). QA evaluation: GPT-4 scored ChatGPT responses 72.88/100 and certified national psychological counselors gave mean score 69.63/100 on a 20-question QA sample (Table 3).",
            "prompting_method": "zero-shot and few-shot for MCQs; open-ended QA evaluated by GPT-4 and human experts",
            "fine_tuned": false,
            "human_data_source": "Certified national psychological counselors in China (20 randomly selected QA questions)",
            "statistical_significance": "Pearson correlation between GPT-4 and human expert scores reported as 0.98 (for QA scoring agreement)",
            "notes": "The human scores reported are evaluative ratings of model outputs (expert rater scores), not human test-taking accuracy measured by having humans answer the same exam items under test conditions.",
            "uuid": "e7236.5",
            "source_info": {
                "paper_title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ERNIE-Bot",
            "name_full": "ERNIE-Bot",
            "brief_description": "Baidu's proprietary language model evaluated on CPsyExam KG/CA; included in QA evaluation comparisons.",
            "citation_title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
            "mention_or_use": "use",
            "model_name": "ERNIE-Bot",
            "model_description": "Proprietary LLM from Baidu evaluated on CPsyExam tasks including QA scored by GPT-4 and by human experts.",
            "model_size": null,
            "test_name": "CPsyExam - Knowledge (KG, SCQ & MAQ)",
            "test_category": "psychological knowledge",
            "test_description": "Multiple-choice knowledge questions from Chinese psychology examinations.",
            "evaluation_metric": "accuracy (%) and QA scoring (0-100)",
            "human_performance": null,
            "llm_performance": "Knowledge Avg accuracy 43.85%; Zero-shot SCQ 52.48%; Zero-shot MAQ 6.66%; Few-shot SCQ 56.10%; Few-shot MAQ 10.37% (Table 2). QA evaluation: GPT-4 scored ERNIE-Bot 73.55/100 and human experts scored 71.63/100 on the 20-question QA sample (Table 3).",
            "prompting_method": "zero-shot and few-shot for MCQs; open-ended QA evaluated by GPT-4 and human experts",
            "fine_tuned": false,
            "human_data_source": "Certified national psychological counselors in China (20 QA items)",
            "statistical_significance": "Pearson correlation between GPT-4 and human expert scores reported as 0.98 (for QA scoring agreement)",
            "notes": "Human numbers are expert rater scores of model outputs; the paper explicitly notes that using GPT-4 to evaluate QA scores may be influenced by GPT-4's own knowledge and that future work should combine expert scoring.",
            "uuid": "e7236.6",
            "source_info": {
                "paper_title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "QA expert scoring (aggregated)",
            "name_full": "Expert human evaluation of LLM QA outputs (certified national psychological counselors)",
            "brief_description": "Human expert rater scores (national certified psychological counselors in China) assigned to model-generated answers for 20 QA items to assess consistency, professionalism, and reasonableness.",
            "citation_title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
            "mention_or_use": "use",
            "model_name": "Human expert raters (used to score model QA outputs)",
            "model_description": "Certified national psychological counselors scored generated QA responses on a 100-point rubric (30 pts consistency with answer, 30 pts professionalism, 40 pts reasonableness).",
            "model_size": null,
            "test_name": "CPsyExam - QA (open-ended subjective questions)",
            "test_category": "applied clinical communication and reasoning",
            "test_description": "Open-ended QA items judged by experts along three dimensions producing a composite score out of 100.",
            "evaluation_metric": "mean score out of 100 (expert raters) and GPT-4 automatic score out of 100",
            "human_performance": "N/A as a human baseline answering the examâ€”these are human ratings of model outputs; reported expert scores for models: ERNIE-Bot 71.63/100; ChatGLM-Turbo 76.20/100; ChatGPT 69.63/100 (Table 3).",
            "llm_performance": "GPT-4 automatic scores for same model outputs: ERNIE-Bot 73.55/100; ChatGLM-Turbo 77.79/100; ChatGPT 72.88/100 (Table 3).",
            "prompting_method": "models generated open-ended QA responses; responses were then scored by experts and by GPT-4",
            "fine_tuned": null,
            "human_data_source": "Certified national psychological counselors in China (20 randomly selected QA questions)",
            "statistical_significance": "Pearson correlation between experts' scores and GPT-4's scores = 0.98 (reported)",
            "notes": "These expert scores measure the quality of model outputs, not human ability to take the exam; the authors caution that GPT-4-based scoring may be biased and suggest future combined scoring with experts.",
            "uuid": "e7236.7",
            "source_info": {
                "paper_title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Psybench: a balanced and indepth psychological chinese evaluation benchmark for foundation models",
            "rating": 2,
            "sanitized_title": "psybench_a_balanced_and_indepth_psychological_chinese_evaluation_benchmark_for_foundation_models"
        },
        {
            "paper_title": "PsyEval: A comprehensive large language model evaluation benchmark for mental health",
            "rating": 2,
            "sanitized_title": "psyeval_a_comprehensive_large_language_model_evaluation_benchmark_for_mental_health"
        },
        {
            "paper_title": "Psy-llm: Scaling up global mental health psychological services with ai-based large language models",
            "rating": 1,
            "sanitized_title": "psyllm_scaling_up_global_mental_health_psychological_services_with_aibased_large_language_models"
        },
        {
            "paper_title": "SMILE: Single-turn to multi-turn inclusive language expansion via ChatGPT for mental health support",
            "rating": 1,
            "sanitized_title": "smile_singleturn_to_multiturn_inclusive_language_expansion_via_chatgpt_for_mental_health_support"
        },
        {
            "paper_title": "Measuring massive multitask language understanding",
            "rating": 1,
            "sanitized_title": "measuring_massive_multitask_language_understanding"
        }
    ],
    "cost": 0.015195249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations
10 Dec 2024</p>
<p>Jiahao Zhao 
Shenzhen Key Laboratory for High Performance Data Mining
Shenzhen Institute of Advanced Technology
Chinese Academy of Sciences</p>
<p>Jilin University</p>
<p>Jingwei Zhu jingweizhu@mail.ustc.edu.cn 
Shenzhen Key Laboratory for High Performance Data Mining
Shenzhen Institute of Advanced Technology
Chinese Academy of Sciences</p>
<p>University of Science and Technology of China</p>
<p>Minghuan Tan mh.tan@siat.ac.cn 
Shenzhen Key Laboratory for High Performance Data Mining
Shenzhen Institute of Advanced Technology
Chinese Academy of Sciences</p>
<p>Min Yang min.yang@siat.ac.cn 
Shenzhen Key Laboratory for High Performance Data Mining
Shenzhen Institute of Advanced Technology
Chinese Academy of Sciences</p>
<p>Shenzhen University of Advanced Technology</p>
<p>Renhao Li li.renhao@connect.um.edu.mo 
Shenzhen Key Laboratory for High Performance Data Mining
Shenzhen Institute of Advanced Technology
Chinese Academy of Sciences</p>
<p>University of Macau</p>
<p>Di Yang di-yang@mail.ustc.edu.cn 
University of Science and Technology of China</p>
<p>Chenhao Zhang ch_zhang@hust.edu.cn 
Shenzhen Key Laboratory for High Performance Data Mining
Shenzhen Institute of Advanced Technology
Chinese Academy of Sciences</p>
<p>Huazhong University of Science and Technology</p>
<p>Guancheng Ye 
Shenzhen Key Laboratory for High Performance Data Mining
Shenzhen Institute of Advanced Technology
Chinese Academy of Sciences</p>
<p>South China University of Technology</p>
<p>Chengming Li 
Shenzhen MSU-BIT University</p>
<p>Xiping Hu 
Shenzhen Key Laboratory for High Performance Data Mining
Shenzhen Institute of Advanced Technology
Chinese Academy of Sciences</p>
<p>Shenzhen MSU-BIT University</p>
<p>Derek F Wong derekfw@um.edu.mo 
University of Macau</p>
<p>CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations
10 Dec 202443C61237FE71304D99ECBD0BA8854E2BarXiv:2405.10212v3[cs.CL]
In this paper, we introduce a novel psychological benchmark, CPsyExam, constructed from questions sourced from Chinese examination systems.CPsyExam is designed to prioritize psychological knowledge and case analysis separately, recognizing the significance of applying psychological knowledge to real-world scenarios.We collect 22k questions from 39 psychology-related subjects across four Chinese examination systems.From the pool of 22k questions, we utilize 4k to create the benchmark that offers balanced coverage of subjects and incorporates a diverse range of case analysis techniques.Furthermore, we evaluate a range of existing large language models (LLMs), spanning from open-sourced to proprietary models.Our experiments and analysis demonstrate that CPsyExam serves as an effective benchmark for enhancing the understanding of psychology within LLMs and enables the comparison of LLMs across various granularities.</p>
<p>Introduction</p>
<p>The evaluation of language models has been an important topic with sustained vitality in the natural language processing community (Chang et al., 2023).With the development of pretrained language models, such as GPT (Radford et al., 2018(Radford et al., , 2019) ) and BERT (Devlin et al., 2019), their increasing abilities in executing a range of different natural language understanding (NLU) tasks (Wang et al., 2019b,a;Xu et al., 2020) call for more challenging and inclusive settings with comprehensive human baselines.To address this issue, several multi-task benchmarks based on real-world exams, such as MMLU (Hendrycks et al., 2021), CMMLU (Li et al., 2023), and CEVAL (Huang et al., 2023), have been developed recently.These benchmarks aim to comprehensively evaluate the capabilities of large language models (LLMs).</p>
<p>However, since general purpose benchmarks typically focus on the breadth of domain coverage, they do not encompass all subjects within specific fields.This issue is particularly severe in the field of psychology.Not all benchmarks for LLMs encompass knowledge of psychology, and those that do provide inadequate coverage.For example, CMMLU only have one subject related to psychology, CEVAL does not even include psychology-related subjects.Meanwhile, with the increasing adoption of LLMs in psychological counselling (Lai et al., 2023) and mental health support (Qiu et al., 2023) in Chinese, there's an urgent need of a psychological evaluation benchmark to comprehensively evaluate the capabilities of LLMs in the context of Chinese psychology.Although there have been concurrent works like Psy-Bench (Zhang et al., 2023) and PsyEval (Jin et al., 2023), they only focus on a subset of psychologyrelated subjects within the Chinese examination system, not encompassing all psychology-related knowledge within the Chinese context.For example, PsyBench focuses on the knowledge points in the Graduate Entrance Examination, while PsyEval concentrates on the domain of mental health.</p>
<p>To fill the gap, we present CPsyExam, the first comprehensive Chinese benchmark constructed from all Chinese examination systems containing psychology-related subjects, designed to evaluate both psychological knowledge and case analysis abilities in the Chinese context.We collect over 22k questions from 39 psychology-related subjects across four Chinese examination systems: the Graduate Entrance Examination (GEE), Psychological Counselor Examination (PCE), Teacher Qualification Examination (TQE), and Adult Self-study Examination (SSE).To align with global examination standards that assess the competence of psychology practitioners and to comprehensively evaluate LLMs' understanding of psychological cases, we further divide CPsyExam into two parts:</p>
<p>(1) Knowledge (KG), which comprises fact-based questions covering a broad spectrum of psychology knowledge drawn from real examinations.(2) Case Analysis (CA), which features case-oriented questions focusing on identification, reasoning, and application abilities within the realm of psychology.</p>
<p>To ensure a balanced representation of questions across subjects, we sampled a subset of questions from each subject for model evaluation, while the remaining questions were made available as supervised fine-tuning (SFT) data for model training.</p>
<p>We further compare the performance of recent general domain LLMs and psychological-specific LLMs on CPsyExam.Our experiments reveal that compared to the foundation models, these finetuned models exhibit marginal gains or no improvement in understanding psychological knowledge.In some cases, their ability to analyze cases may even be compromised.Evidently, LLMs still have room for improvement in terms of mastering psychological knowledge and applying it to psychological case analysis.CPsyExam serves as a valuable benchmark for advancing LLMs' understanding of psychology.</p>
<p>Our work has the following contributions:</p>
<ol>
<li>
<p>We provide a comprehensive and balanced dataset of Chinese psychology examination questions, covering the entire Chinese examination system that includes psychologyrelated subjects.</p>
</li>
<li>
<p>We propose an assessment framework for benchmarking the psychological capabilities of LLMs, consisting of a knowledge session and a case analysis session.</p>
</li>
<li>
<p>We construct the benchmark and release over 11K questions as SFT data which contribute to the enhancement of psychological competence in the LLMs.</p>
</li>
</ol>
<p>2 Related Work</p>
<p>Psychology examination for humans</p>
<p>There are many global exams designed to assess human psychology abilities, focusing on both knowledge levels and practical application skills.For example, the and a case analysis part (CA).This division aims to comprehensively evaluate the psychological capabilities of Language Models (LLMs), aligning with the multifaceted assessment approaches seen in prominent psychology examinations worldwide.</p>
<p>Benchmarks of Large Language Models</p>
<p>In the Chinese domain, several general benchmarks have been constructed from real-world exams, such as CEVAL (Huang et al., 2023) and CMMLU (Li et al., 2023).However, these benchmarks do not comprehensively assess the models' capabilities in psychology, covering barely one or two subjects in the psychology domain.For specific domains, Psybench (Zhang et al., 2023) recently generated their questions from GPT-4 using the knowledge points from Graduate Entrance Examination, and PsyEval (Jin et al., 2023)   3 CPsyExam Benchmark</p>
<p>Design Principles</p>
<p>Comprehensive and Balanced CPsyExam benchmark encompasses the entire Chinese examination system that includes psychology-related subjects to ensure comprehensive coverage of psychology knowledge in the Chinese context.Each subject in CPsyExam is well-represented with a balanced number of questions.This balanced representation not only diversifies the dataset but also provides a condensed yet comprehensive view of all psychology-related exams in China.</p>
<p>Assessing Multi-capability Our benchmark is structured to mirror real-world exams, which emphasize both psychological knowledge and the application of that knowledge.It consists of two main parts: one for assessing understanding of psycho-logical knowledge (KG), and another for evaluating proficiency in case analysis skills (CA).In psychology, case studies are crucial as they assess practitioners' practical abilities alongside theoretical knowledge.Thus, our dataset encompasses these two essential components to comprehensively evaluate LLMs.</p>
<p>Diverse Question Formats Our questions are presented in multiple formats: multiple-choice questions (MCQs) and open-ended QA.Multiplechoice questions provide clear and visual assessment outcomes, while question-answering questions evaluate the LLM's language organization abilities.Furthermore, we categorize multiplechoice questions into single-choice (SCQ) and multiple-response (MAQ) formats to increase assessment complexity.This approach aims to assess the LLM's thorough understanding and prevent it from relying solely on identifying a single correct option to answer a question completely.</p>
<p>Data Preparation</p>
<p>The Chinese Examination System Including Psychology Subjects</p>
<p>â€¢ GEE (Graduate Entrance Examinations) This exam includes a comprehensive test on basic psychology.It is required for students who wish to pursue a master's or doctoral degree in psychology.</p>
<p>â€¢ PCE (Psychological Counselor Examination) Organized by the National Psychological</p>
<p>Counselor Certification Center, this exam assesses candidates' theoretical knowledge and practical skills in psychological counseling.</p>
<p>â€¢ TQE (Teachers' Qualification Examination) For individuals aspiring to become teachers, this exam ensures that future teachers have a foundational understanding of psychological principles applicable in educational settings.</p>
<p>â€¢ SSE (Self-study Examination) This examination includes psychology-related subjects within various fields such as medicine, engineering, agriculture, and economics.It covers relevant psychological concepts and theories applicable to these disciplines.</p>
<p>These examination systems collectively contribute to a well-rounded understanding and application of psychology across academic, counseling, educational, and professional domains in China.Regarding question types, the GEE, PCE, and TQE include both knowledge-based questions and case analysis questions.In contrast, the SSE typically consists solely of knowledge-based questions.</p>
<p>Data Collection We gather psychological data from publicly available resources using Crawling and OCR.</p>
<p>â€¢ Crawling Based on the categorization of examinations in psychology, we crawl public available resources online to construct a database of questions.The websites for the data crawling include ExamCoo1 , StudyEZ2 , Hxter3 and MXQE4 .</p>
<p>â€¢ OCR For questions sourced from the book, we utilize Optical Character Recognition (OCR) technology to extract the text.</p>
<p>Data preprocessing</p>
<p>â€¢ Obtaining Structured Questions We gather data from websites and books.Data scraped from websites is parsed using a program to extract questions, while questions from books are manually extracted and structured.All data undergo preprocessing to remove duplicates and correct formatting errors.Questions containing image links are excluded, and formats are standardized by removing question numbers and option letters.The dataset is manually validated to ensure grammatical accuracy in all questions.</p>
<p>â€¢ Attempt to mitigate data leakage problem To address potential data leakage concerns from publicly available resources used in pre-training LLMs, we have implemented several strategies: (1) We extract a portion of our questions from PDF-format books, minimizing the likelihood of these questions being previously used for pre-training.</p>
<p>(2) Questions from selected websites are not directly available as structured questions; they require programs to match questions with answers.</p>
<p>(3) Many of the questions we scraped are from mock exams rather than widely distributed official exam questions.(4) After obtaining structured questions, we shuffle the options and answers to add an extra layer of protection against data leakage.</p>
<p>Taxonomy of CPsyExam</p>
<p>We collected over 22k exam questions from 39 psychology-related subjects within the Chinese examination system.These questions vary in type (KG, CA) and formats (SCQ, MAQ, QA), and are systematically organized into corresponding tasks.</p>
<p>CPsyExam-KG task Questions of KG type are selected for this task.We further align the taxonomy of the CPsyExam-KG task with the Chinese examination system for psychology.Subsequently, we categorize all the psychology subjects in each examination as subcategories.A detailed directory list can be found in the Appendix A.</p>
<p>CPsyExam-CA task Questions of CA type are selected for this task.In accordance with the examination focuses of case analysis questions in GEE, PCE, and TQE, we further divided case analysis into three categories:  Dataset Splitting To facilitate supervised finetuning and few-shot learning, each task dataset will be partitioned into train, dev, test and reserved.</p>
<p>The test split will be used for the evaluation of LLMs.The reserved split will not be released and act as a control set for further evaluation.We sample psychology subjects uniformly under each exam, ensuring that the number of questions is consistent across all four exams.This approach is also used to create the test and reserve split.The remaining questions are all allocated to the train split.Statistics of the dataset is listed in Table 1.We show three examples from both KG and CA in Figure 2.</p>
<p>Experiments</p>
<p>Experiment Setup</p>
<p>In this section, we benchmark a series of public accessible LLMs using CPsyExam in both zero-shot and five-shot settings, where the five exemplars are from the development split.</p>
<p>Models</p>
<p>To comprehensively assess the performance of different types of models on CPsyExam, we selected three types of models.</p>
<p>Open-sourced LLMs ChatGLM2-6B: Based on the General Language Model (GLM) (Du et al., 2022) tional data.YI-6B, and YI-34B: Designed to enhance capabilities in coding, mathematics, reasoning, and instruction-following, these versions are optimized for both English and Chinese language tasks.Qwen-7B, Qwen-1.8B and Qwen-14B:Developed by Alibaba Group, these models are trained on extensive multilingual and multimodal data and optimized for human preferences.</p>
<p>Psychology-oriented Models MeChat5 : Finetuned from ChatGLM2-6B using the SMILE (Single-turn to Multi-turn Inclusive Language Expansion) dataset.MindChat6 Available in two versions, MindChat-Qwen-7B-v2 and MindChat-Qwen-1.8B,these models are finetuned using Chinese multi-turn psychological dialogue data.</p>
<p>Proprietary Models ERNIE-Bot-Turbo: Developed by Baidu, this model is known for its strong language understanding and generation capabilities.ChatGLM-Turbo: An advanced language model by Tsinghua University, optimized for fast and efficient conversational AI tasks.ChatGPT and GPT4: The latest and most powerful variants of the GPT models from OpenAI.</p>
<p>Prompt</p>
<p>We designed prompts for both multiple-choice questions (MCQs) and open-ended QA, which are shown in Figure 5 and Figure 6 in the Appendix.In addition, we created two extra prompts specifically for the MCQs, setting the LLM as a psychology student and as an ordinary person which is shown in Figure 7 and Figure 8.This was done to verify the validity of the dataset.</p>
<p>Supervised Fine-Tuning</p>
<p>To validate the effectiveness of the dataset, we constructed an instruction set for supervised finetuning (SFT) on the training set of CPsyExam.In this work, we conduct SFT over ChatGLM2-6B.Specifically, the SFT is carried out over 4 epochs with a batch size of 128.The learning rate is set to 1 Ã— 10 âˆ’6 .These parameters were chosen based on preliminary experiments that aimed to maximize the model's performance on validation sets.</p>
<p>Benchmarking Result</p>
<p>Performance of LLMs on SCQ and MAQ We conduct both zero-shot and few-shot evaluations for each model discussed above.Given the focus of CPsyExam is on how models can perform over Knowledge and Case Analysis questions, we report them separately.We further differentiate SCQ and MAQ questions, as different models may have varying abilities to follow instructions.There are three sections in the table : (1) Open-sourced Models.Our findings indicate that: (a) increased model size does not necessarily ensure improved performance on the CPsyExam, and (b) models that excel in other domains, such as YI-34B on the medical domain, may not necessarily perform optimally on the CPsyExam.(2) Psychology-oriented Models.Compared to the foundation models, these fine-tuned models show marginal gains or no improvement in understanding psychological knowledge.</p>
<p>(3) Proprietary Models.GPT-4 continues to outperform all other proprietary models by a significant margin in the knowledge setting.Conversely, ChatGLM-turbo performs exceptionally well in the Case Analysis setting.</p>
<p>Performance of proprietary models on Question</p>
<p>Answering Besides SCQ and MAQ, CPsyExam includes an extra QA test set to evaluate generationbased questions.We adopt GPT-4 to judge proprietary models used in this work.Meanwhile, we enlisted certified national psychological counselors in China to score the responses of three models on 20 randomly selected QA questions.The scoring criteria were divided into three dimensions: consistency with the answer (30 points), professionalism of language (30 points), and reasonableness of the answer (40 points).The experimental results are shown in table 3. Compared to the scores given by GPT-4, the rankings of the three models were consistent.Additionally, the Pearson correlation coefficient between the experts' scores and GPT-4's scores was 0.98, indicating a high degree of consistency between human evaluations and GPT-4's evaluations.The results suggest that ChatGLMturbo has a better understanding of psychological knowledge and can be effectively prompted for psychological purposes.</p>
<p>Model</p>
<p>Performance of models in different prompt</p>
<p>To validate the effectiveness of the CPSYEXAM dataset, we used prompts to configure the LLM to adopt different roles: a psychology teacher, a psychology student, and an ordinary person with no background in psychology.These roles represent progressively decreasing levels of psychology knowledge.The LLM was then tested on the multiple-choice questions in CPSYEXAM under each role to examine whether varying levels of psychological expertise influence its performance on the dataset.Specifically, we prompted ChatGLM2-6B to adopt the three roles mentioned above.The results are presented in When models are smaller, few-shot learning typically offers minimal performance gains and can sometimes even have negative effects.However, as model size increases, the advantages of few-shot learning become significantly more noticeable.For instance, ChatGLM-turbo, already proficient in zero-shot scenarios, doubled its performance on the CA task following few-shot training.This improvement is likely due to larger models having greater capacity and expressive ability.They can better capture intricate patterns and latent semantic relationships in data, allowing for faster learning and generalization from limited training data.</p>
<p>Performance between psychology-oriented models and the base model Based on the experiments, the model that underwent fine-tuning to enhance its psychological capabilities did not surpass the base model and even exhibited a performance decline.This outcome suggests that while psychology-oriented model's fine-tuning improved its conversational skills, it potentially compromised its proficiency in tasks involving knowledge reasoning and text comprehension.The model might have overly adapted to the fine-tuning data, thereby neglecting the broader knowledge acquired during its initial pre-training phase.</p>
<p>Analyses from a Benchmark Perspective</p>
<p>Analysis of SCQ Questions Due to the persistent low performance on MAQ questions, we focus solely on SCQ questions for error analysis.We selected the top-performing models from each of the three categories for analysis.Since ChatGLMturbo and GPT-4 performed similarly, we chose both of them from the proprietary models.Regarding CPsyExam-KG, we perform analysis at the examination level, as depicted in Figure 3a.For CPsyExam-CA, we delve into various aspects of case analysis, presented in Figure 3b.By examining both figures, we determine that GPT-4 exhibits a stronger grasp of psychological knowledge across all examinations, yet it continues to face challenges with case analysis questions.The major gap for GPT-4 comes from REASONING and APPLICA-TION.</p>
<p>Analysis of MAQ Questions Compared to SCQ, LLMs exhibit poorer performance on MAQ, which aligns with the goals of our experimental design.In our setup, models are awarded points for a question only when they provide a fully correct answer.This approach is intentionally crafted to eliminate reliance on test-taking strategies, such as processof-elimination techniques, when tackling MAQ.Instead, it requires the models to rigorously assess the accuracy of each option.As a result, the performance of large models on MAQ is significantly lower than on SCQ.</p>
<p>Analysis of Performance at Subject Level Each subject in CPsyExam features a minimum of 32 questions, exceeding typical quiz lengths for human participants.We have identified the top two models based on their performance in the CP-syExam benchmark for visual representation across each subject.Initially, we merged subjects with shared backgrounds and domain similarities.The results for ChatGLM-Turbo and GPT-4 are presented in Figure 4. Despite being the top perform-ers in our CPsyExam benchmark, ChatGLM-Turbo demonstrates limited robustness in certain subjects and consistently trails behind GPT-4 across various domains.</p>
<p>Analyses from a Validity Perspective on CPsyExam</p>
<p>The improvement of the model after SFT After fine-tuning, ChatGLM2-6B performed exceptionally well, becoming the top-ranked model among all non-proprietary models.This indicates that the knowledge embedded in the CPsyExam questions is highly consistent and relevant to psychologyã€‚Consequently, after fine-tuning with the training set data, the model showed improved performance on the test set.</p>
<p>Performance of models across different prompt settings In the experiment, ChatGLM2-6B performed better when configured as a student compared to its performance as an ordinary person.However, both student and ordinary person settings showed significantly lower performance than when ChatGLM2-6B was set as an expert.This aligns with our intuition that higher levels of psychological knowledge correlate with improved performance on CPsyExam.Additionally, CPsyExam demonstrates a strong ability to differentiate between levels of psychological knowledge.Specifically, ChatGLM2-6B performed 11.64% better as an expert compared to as a student, and 14.29% better compared to as an ordinary person.</p>
<p>Conclusion</p>
<p>In conclusion, we introduce CPsyExam, a benchmark for Chinese psychology, composed of humangenerated questions that span a wide array of subjects within the Chinese examination system.It is designed to evaluate LLMs proficiency in both psychological knowledge and case analysis, offering a concise yet comprehensive overview of all psychology-related exams in China.</p>
<p>A Subjects in Psychology Examinations</p>
<p>In this appendix, we provide a table that describes the subjects included in each examination system in our dataset, as well as the number of questions in each subject.</p>
<p>Figure 1 :
1
Figure 1: Overview of dataset constructing pipeline.</p>
<p>Figure 2 :
2
Figure 2: Examples for questions on CPsyExam-SCQ and CPsyExam-MAQ.</p>
<p>Comparison of model performance from case analysis perspective.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Performance over SCQ from different perspectives for all LLMs.</p>
<p>Split of Samples Type of Questions
Source DataCleaned Data#1 -PTSDçš„ç—‡çŠ¶å­¦æ ‡å‡†ä¸»è¦æœ‰( )Chinese Examination System#2 -æ–¯çš®å°”æ›¼æå‡ºäº†èƒ½åŠ›çš„äºŒå› ç´ è¯´ï¼Œè¿™ä¸¤ç§-Graduate Entrance Examinationsèƒ½åŠ›çš„å› ç´ æ˜¯æŒ‡( )-Psychological Counselor Examination -Teachers' Qualification ExaminationSFT Data -TrainBenchmark -Dev -Test#3 -( )æ˜¯å¼—æ´›ä¼Šå¾·ç²¾ç¥ž åˆ†æžå¾·æ ¸å¿ƒå†…å®¹ã€‚ â€¦-Self-study Examination-Reservedgenerated their ques-tions from GPT-4 using open access datasets inthe mental health domain. Compared to Psybenchand PsyEval, CPsyExam offers several advantages:(1) Boaeder Coverage: CPsyExam covers morepsychology-related subjects, including almost all</p>
<p>Knowledge Case Analysis Form of Questions
CPsyExamOpen-ended QASSE GEEPCEMCQSCQ MAQTQEDataPreprocessing#1 -ä¸€èˆ¬èµ„æ–™ï¼šæ±‚åŠ©è€…ï¼Œå¥³æ€§ï¼Œ61 å²ï¼Œé€€ä¼‘å·¥äººã€‚æ¡ˆä¾‹ä»‹ç»ï¼šæ±‚åŠ©è€…æ˜¯å­¤å¯¡è€äººï¼Œæœ€è¿‘ä¸€æ®µæ—¶é—´ç»å¸¸ â€¦è¿™ç§ç—‡çŠ¶è¿èƒŒäº†åˆ¤æ–­å¿ƒç†æ­£å¸¸ä¸Žå¼‚å¸¸ä¸‰åŽŸåˆ™ä¸­çš„( )...</p>
<p>Table 1 :
1
Statistics of the CPsyExam dataset.
IDENTIFICATION, REASON-ING and APPLICATION. The IDENTIFICATION cat-egory assesses the LLM's ability to identify theappropriate methodology used in a specific case.The REASONING category focuses on the LLM'sability to pinpoint the underlying problem that ledto the issue. The APPLICATION category evalu-ates the LLM's ability to apply specific methods tosolve problems.
ä¸€èˆ¬èµ„æ–™:ç”·,28å²,æœªå©š,å…¬å¸èŒå‘˜ã€‚æ±‚åŠ©é—®é¢˜:åå¤æ€è€ƒæ¯«æ—  æ„ä¹‰çš„é—®é¢˜,ä¼´æ€¥èºå’Œç¡çœ éšœç¢2ä¸ªæœˆã€‚æ¡ˆä¾‹ä»‹ç»:è¿‘2ä¸ªæœˆæ¥å å¤æ€è€ƒä¸€äº›æ¯«æ— æ„ä¹‰çš„é—®é¢˜,å¦‚"æ´—æ°´æžœæ—¶æ˜¯å¤šç”¨ä¸€ç‚¹æ°´å¥½,è¿˜ æ˜¯å°‘ç”¨ä¸€ç‚¹å¥½","å‰Šå¸¦çš®çš„è”¬èœå¦‚é»„ç“œæ—¶,æ˜¯åŽ»çš®åŽšä¸€ç‚¹å¥½è¿˜ æ˜¯è–„ä¸€ç‚¹å¥½",ç­‰ç­‰ã€‚è™½ç„¶è®¤ä¸ºæƒ³è¿™äº›æ²¡å¿…è¦,ä½†è¿˜æ˜¯æŽ§åˆ¶ä¸ä½ åœ°æƒ³ã€‚ç»§è€Œå‡ºçŽ°æ´—è¡£æœæ—¶æ€»æ‹…å¿ƒæ´—ä¸å¹²å‡€è€Œåå¤æ´—æ¶¤,ç›´åˆ°è‡ª è®¤ä¸ºæ´—å¹²å‡€ä¸ºæ­¢,ä¸ºæ­¤è€½è¯¯äº†è®¸å¤šæ—¶é—´ã€‚åŽæ¥åˆå‡ºçŽ°äº†ä¸€ç§å¥‡ æ€ªçš„æƒ³æ³•,èµ°è¿‡è¡—å¤©æ¡¥æ—¶æ€»æƒ³ç€è·³ä¸‹åŽ»,ä¸ºæ­¤æ„Ÿåˆ°å®³æ€•,å°½é‡é¿ å…èµ°è¿‡è¡—å¤©æ¡¥ã€‚å› è€Œéžå¸¸çƒ¦æ¼,è„¾æ°”å˜å¾—æ€¥èº,é‡åˆ°ä¸€ç‚¹å°äº‹å°± çˆ±å‘ç«,ç»å¸¸æ„Ÿåˆ°ç–²æƒ«,ç¡çœ ä¸å¥½,å¸¸åˆ°å‡Œæ™¨ä¸€ä¸¤ç‚¹æ‰èƒ½å…¥ç¡,é†’ æ¥æ„Ÿè§‰æ˜æ˜æ²‰æ²‰ã€‚ç”±äºŽè¿™äº›é—®é¢˜çš„å›°æ‰°,å·¥ä½œã€ç”Ÿæ´»å—åˆ°äº†å½± å“,è™½å°šèƒ½åšæŒåº”å¯¹,ä½†æ„Ÿè§‰è‹¦æ¼,å¸Œæœ›å°½å¿«è§£å†³,å› æ­¤å‰æ¥å¿ƒç† å’¨è¯¢ã€‚<English Translation Omitted></p>
<p>Table 2 :
2
Comparisons of different models over CPsyExam set with zero-shot and few-shot prompting.The Avg. score use the maximum score of both settings.We highlight the best score for each column with bold font and second best score with underline mark.
KnowledgeCase AnalysisModelAvg.Zero-shotFew-shotZero-shotFew-shotSCQ MAQSCQ MAQSCQ MAQSCQ MAQChatGLM2-6B43.4649.899.8653.81 14.8552.50 16.0048.50 20.00ChatGLM3-6B42.2353.515.6355.755.5147.00 17.0047.33 13.50YI-6B25.8133.260.2625.39 14.0138.830.0020.00 13.25YI-34B27.5225.031.1533.69 18.1820.500.5022.338.00Qwen-7B19.2224.991.0225.683.9718.830.5019.672.50Qwen-1.8B19.7824.991.4125.126.7918.673.0020.676.00Qwen-14B30.6824.991.5438.17 13.1920.332.0030.00 14.00MeChat-6B40.6250.244.1051.79 11.9148.67 13.5044.83 10.50MindChat-7B40.3949.256.2756.925.5140.835.0033.834.50MindChat-1.8B21.0426.500.0026.500.1334.170.0034.170.00Ours-SFT-6B46.0853.86 21.9055.45 19.9752.17 32.0049.67 15.50ERNIE-Bot43.8552.486.6656.10 10.3742.508.5050.67 12.00ChatGPT51.1557.43 11.1461.53 24.7147.339.0052.67 29.50ChatGLM-Turbo 64.5863.29 26.1273.85 42.1369.00 20.5065.33 42.50GPT-467.4376.56 10.7678.63 43.7960.33 13.0064.17 39.50
, this model is trained on both English and Chinese data and further adapted for conversa-</p>
<p>Table 3 :
3
Score provided by GPT-4 over QA questions.
GPT-4 scores Expert scoresERNIE-Bot73.5571.63ChatGLM-turbo77.7976.20ChatGPT72.8869.63</p>
<p>Table 4 .
4SettingScoreExpert43.46Student38.93Ordinary person 38.03</p>
<p>Table 4 :
4
Model performance across different prompt settings
5 Analysis5.1 Analyses from a Model-Level PerspectiveDoes few-shot examples help?</p>
<p>Table 5 :
5
Subjects for each examination system and the number of questions for each subject.</p>
<p>https://examcoo.com
http://www.studyez.com/psychology/
 www.hxter.com <br />
http://tk.mxqe.com
https://huggingface.co/qiuhuachuan/MeChat
https://github.com/X-D-Lab/MindChat
AcknowledgementsThis work was partially supported by National Natural Science Foundation of China  (62406314, 62376262, 62266013), China Postdoctoral Science Foundation (2023M733654), Guangdong Basic and Applied Basic Research Foundation (2023A1515110496), Guangdong Province of China (2024KCXTD017)ï¼ŒNatural Science Foundation of Guangdong Province of China (2024A1515030166), Shenzhen Science and Technology Innovation Program (KQTD20190929172835662), Shenzhen Science and Technology Foundation (JCYJ20240813145816022), Science and Technology Development Fund of Macau SAR (0007/2024/AKP, FDCT/0070/2022/AMJ, FDCT/060/2022/AFJ), and Multi-year Research Grant from the University of Macau (MYRG-GRG2024-00165-FST).LimitationsUsing GPT-4 to evaluate QA scores might be influenced by its own knowledge, and in the future, expert scoring will be introduced to provide a combined score for the QA section, improving the reliability of the evaluation.B Prompts Used for EvaluationIn this paper, we set the LLM as an expert and a student in the field of psychology, as well as an ordinary person with no knowledge of psychology.The prompts we used are shown in Figure5,Figure7and Figure8.Additionally, we used GPT-4 to evaluate the quality of the LLM's answers to the subjective questions.The prompt used for evaluation is shown in Figure6.As a seasoned expert in the field of psychology, you should possess the following qualities and abilities: 1. Extensive theoretical knowledge of psychology: Master various psychological theories and practices from different schools of thought.2. Deep understanding of human behavior: Ability to interpret complex behavioral patterns and psychological processes.3. Analytical and judgmental skills: Ability to quickly and accurately analyze and diagnose based on case details. 4. Clinical experience: Rich clinical practice experience, capable of handling various psychological issues and situations. 5. Adherence to ethical principles: Compliance with professional ethical guidelines in psychology, ensuring the privacy and well-being of patients.## Rules 1.You are an experienced psychology expert.2. Your task is to answer {question_type} questions in the {subject} exam based on the provided information, using your professional knowledge and analytical skills.3. The questions will cover various aspects of psychology, and you need to utilize your expertise to choose the correct answer.4. If the question information is insufficient to make a judgment, you should base your answer on your professional experience, assuming the most plausible scenario.## InitializationAs an expert in the field of psychology, you are required to adhere to the rules and answer the following {question_type} questions in the {subject} exam.Please use your professional knowledge to carefully analyze each option and choose the answer that best aligns with psychology principles and clinical experience.We rely on your professional judgment to ensure the selection of the most accurate and objective answer.Provide the answer in the format "Answer: {{Your chosen answer}}".As a student studying psychology, you might have the following traits and background: 1.Exposure to psychological theories: You have a certain understanding of some basic theories and concepts in psychology.2. Academic interest and in-depth exploration: You aim to deeply understand human behavior and psychological processes through your studies.## Rules 1.You are a student studying psychology.2. Your task is to try to choose the most reasonable answer based on the information provided and your knowledge of psychology.3. The questions will involve basic theories and applications of psychology.You can answer based on your studies and understanding.4. If a question is beyond your scope of study, you can infer and choose based on the information provided.## InitializationAs the role of <Role>, please answer the following {question_type} questions about the "{subject}" exam.Try to choose the answer you believe is most reasonable based on your knowledge.We hope your answers will promote a deeper understanding of and academic exploration in psychology.Provide the answer in the format "Answer: {{Your chosen answer}}".As an ordinary person who has never studied psychology, you might have the following traits and background: 1. Basic life experience and observation: You have a certain understanding of common psychological phenomena through experiences and observations in daily life.2. Common sense and logical thinking: You can make reasonable inferences and choices based on the information provided in the questions and your common sense.3. Interest in psychology: You may be interested in some basic concepts and theories of psychology and hope to learn more by answering questions.4. Practical application perspective: You think about the application and significance of psychology from the perspective of practical life experience.## Rules 1.You are an ordinary person who has never studied psychology.2. Your task is to choose what you believe to be the most reasonable answer based on the information provided and your daily life experience.3. The questions will involve basic concepts and applications of psychology.You can answer based on your common sense and logical thinking.4. If a question is too technical or beyond your understanding, you can make a reasonable guess based on the information provided.## InitializationAs the role of <Role>, please answer the following {question_type} questions about the "{subject}" exam.Try to choose what you believe to be the most reasonable answer based on your daily life experience and common sense.We hope your answers will promote a preliminary understanding of and practical thinking about psychology.Provide the answer in the format "Answer: {{Your chosen answer}}".
Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S Yu, Qiang Yang, Xing Xie, arXiv:2307.03109A survey on evaluation of large language models. 2023Preprint</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>GLM: general language model pretraining with autoregressive blank infilling. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, 2022</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, International Conference on Learning Representations. 2021</p>
<p>Maosong Sun, and Junxian He. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Advances in Neural Information Processing Systems. </p>
<p>Psyeval: A comprehensive large language model evaluation benchmark for mental health. Haoan Jin, Siyuan Chen, Mengyue Wu, Ke Zhu, ArXiv, abs/2311.091892023</p>
<p>Psy-llm: Scaling up global mental health psychological services with ai-based large language models. Tin Lai, Yukun Shi, Zicong Du, Jiajie Wu, Ken Fu, Yichao Dou, Ziqi Wang, arXiv:2307.119912023Preprint</p>
<p>Cmmlu: Measuring massive multitask language understanding in chinese. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, Timothy Baldwin, arXiv:2306.092122023Preprint</p>
<p>Smile: Singleturn to multi-turn inclusive language expansion via chatgpt for mental health support. Huachuan Qiu, Hongliang He, Shuai Zhang, Anqi Li, Zhenzhong Lan, arXiv:2305.004502023Preprint</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019</p>
<p>SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, 2019aCurran Associates IncRed Hook, NY, USA</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, International Conference on Learning Representations. 2019b</p>
<p>CLUE: A Chinese language understanding evaluation benchmark. Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, Zhenzhong Lan, 10.18653/v1/2020.coling-main.419Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, Spain (Online2020International Committee on Computational Linguistics</p>
<p>Psybench: a balanced and indepth psychological chinese evaluation benchmark for foundation models. Junlei Zhang, Hongliang He, Nirui Song, Shuyuan He, Shuai Zhang, Huachuan Qiu, Anqi Li, Lizhi Ma, Zhenzhong Lan, arXiv:2311.098612023Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
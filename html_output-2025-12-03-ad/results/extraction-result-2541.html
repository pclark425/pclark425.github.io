<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2541 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2541</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2541</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-7ef57c7a1ec7415e7fbf94aeb245cbd8f104fbe1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7ef57c7a1ec7415e7fbf94aeb245cbd8f104fbe1" target="_blank">MARE: Multi-Agents Collaboration Framework for Requirements Engineering</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> An innovative framework called MARE is proposed, which leverages collaboration among large language models (LLMs) throughout the entire RE process and can generate more correct requirements models and outperform the state-of-the-art approaches by 15.4%.</p>
                <p><strong>Paper Abstract:</strong> Requirements Engineering (RE) is a critical phase in the software development process that generates requirements specifications from stakeholders' needs. Recently, deep learning techniques have been successful in several RE tasks. However, obtaining high-quality requirements specifications requires collaboration across multiple tasks and roles. In this paper, we propose an innovative framework called MARE, which leverages collaboration among large language models (LLMs) throughout the entire RE process. MARE divides the RE process into four tasks: elicitation, modeling, verification, and specification. Each task is conducted by engaging one or two specific agents and each agent can conduct several actions. MARE has five agents and nine actions. To facilitate collaboration between agents, MARE has designed a workspace for agents to upload their generated intermediate requirements artifacts and obtain the information they need. We conduct experiments on five public cases, one dataset, and four new cases created by this work. We compared MARE with three baselines using three widely used metrics for the generated requirements models. Experimental results show that MARE can generate more correct requirements models and outperform the state-of-the-art approaches by 15.4%. For the generated requirements specifications, we conduct a human evaluation in three aspects and provide insights about the quality</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2541.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2541.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MARE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agents Collaboration Framework for Requirements Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MARE is a prompt-engineering-driven multi-agent framework that leverages large language models (LLMs) to automate the end-to-end requirements engineering (RE) process by coordinating specialized agents to perform elicitation, modeling, verification, and specification through iterative collaboration using a shared workspace and action migration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MARE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>MARE is a multi-agent system built on LLMs that automates the entire Requirements Engineering pipeline. It decomposes RE into four tasks (elicitation, modeling, verification, specification) and implements five specialized agents (Stakeholders, Collector, Modeler, Checker, Documenter). Each agent is implemented by designing prompt templates for predefined actions (nine total). Agents exchange and coordinate by reading and writing structured requirement artifacts into a shared workspace; control flow is governed by an action migration diagram that sequences actions and iteratively triggers rework when the Checker reports problems. Human interaction can seed the process and provide feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Stakeholders (express user stories and answer questions), Collector (interview stakeholders and write requirement drafts; actions: ProposeQuestion, WriteReqDraft), Modeler (extract modeling entities and relations from drafts; actions: ExtractEntity, ExtractRelation), Checker (assess requirement draft and model quality against acceptance criteria; action: CheckRequirement), Documenter (generate software requirements specification or error reports based on templates; actions: WriteSRS, WriteCheckReport).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Elicitation (idea elicitation / stakeholder requirement capture), Modeling (construct formal requirements models), Verification (quality checking / smell detection against acceptance criteria), Specification (documenting final SRS), plus iterative refinement loops; maps to idea generation, modeling/representation, evaluation, and documentation.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Centralized shared-workspace with a sequential/iterative pipeline governed by an explicit action-migration diagram: agents sequentially perform actions (e.g., SpeakUserStories -> WriteReqDraft -> ExtractEntity -> ExtractRelation -> CheckRequirement -> WriteSRS), with iterative loops where Checker-generated error reports trigger Collector and Modeler to sequentially revise artifacts; control is orchestrated centrally through the workspace and the action migration logic.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Shared workspace (shared memory) storing structured requirement artifacts; each artifact carries metadata fields {content, role, caused_by, sent_from, send_to} that indicate artifact content, originating agent, generating action (for action migration), and sender/recipient flow; agents implement actions via LLM prompts whose inputs are defined artifact fields (natural-language templates + structured artifact fields). Communication is thus a mix of natural-language messages (prompt inputs/LLM outputs) stored as structured artifacts in the shared workspace.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Iterative refinement loop via Checker: Checker produces a checking_message or an error_report; if smells/errors are found the Documenter writes an error report which triggers Collector and Modeler to sequentially revise the requirement draft and model; HumanInteraction can provide external feedback at start or during process. Feedback is implemented as artifacts (checking_message, error_report) in the shared workspace that cause subsequent action migrations.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>On-demand and phase-driven: agents communicate after each action/phase by writing artifacts to the shared workspace; the workflow is sequential (after each step) and iterative when Checker reports errors (phase transitions trigger re-execution of relevant agent actions).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Software engineering — Requirements Engineering (elicitation, modeling, verification, specification).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>For requirements modeling: Precision (P), Recall (R), F1 reported per benchmark. Key reported numbers for MARE (gpt-3.5-turbo): problem diagrams average P=82.6%, R=88.3%, F1=84.1%; use case diagrams average P=77.4%, R=80.6%, F1=78.9%; goal models average P≈87.4%, R≈87.9%, F1≈87.6%. For specifications: human evaluation scores (scale 0–2) averaged across 9 cases: completeness=0.98, correctness=1.92, consistency=1.98. Implementation hyperparameters: temperature=0, max_tokens=3000, top_p=1, frequency_penalty=0, presence_penalty=0.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>MARE is compared to three SOTA baselines: EPD (problem diagrams), IT4RE (use case diagrams), HAGM (goal models). Improvements reported (MARE gpt-3.5-turbo vs baselines): problem diagrams +15.4% absolute average F1 over EPD, use case diagrams +23.8% absolute average F1 over IT4RE, goal models +0.6% absolute average F1 over HAGM. Ablation vs single LLM: +0.2% precision, +2% recall, +1.1% F1 (MARE vs individual gpt-3.5-turbo on use-case tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Coordination yields improved modeling accuracy and robustness: substantial F1 improvements over SOTA baselines (e.g., +15.4% on problem diagrams, +23.8% on use cases), modest gains over a single LLM in ablation (+1.1% F1). Enables end-to-end automation of RE pipeline and iterative error-correction (Checker-driven rework) leading to higher-quality specifications (human-eval correctness and consistency ~1.9–2.0).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Reported limitations include lower completeness in generated SRS (average completeness score 0.98) attributed to LLM tendency to reduce length and to insufficient detail in some sections; stochasticity in LLM responses (mitigated by fixed hyperparameters but still present); the paper does not report explicit communication overhead metrics, but the sequential iterative pipeline implies extra latency for multiple cycles. No large-scale failure modes are quantified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Yes; an ablation comparing MARE (multi-agent collaboration) to a single LLM (gpt-3.5-turbo) that directly extracts modeling elements from the drafts shows MARE outperforms the individual LLM by +0.2% average precision, +2.0% average recall, and +1.1% average F1 on 5 public use-case cases (Table VI and RQ3).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>The paper reports implementation choices that produced the reported results: using gpt-3.5-turbo yields best performance among tested LLMs (gpt-3.5-turbo, text-davinci-002, text-davinci-003); a five-agent specialization with nine defined actions and a shared workspace coordination pattern; LLM hyperparameters used for stability: temperature=0, max_tokens=3000, top_p=1, frequency_penalty=0, presence_penalty=0. The paper does not claim broader optimality beyond these tested settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARE: Multi-Agents Collaboration Framework for Requirements Engineering', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2541.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2541.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaGPT (meta-programming framework for multi-agent collaboration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MetaGPT is a meta-programming framework that incorporates efficient human workflows into LLM-based multi-agent collaborations; the authors used the open-source MetaGPT framework to build and orchestrate MARE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Metagpt: Meta programming for multi-agent collaborative framework</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described in the paper as an open-source multi-agent framework used to implement MARE. It is characterized as a meta-programming framework that incorporates human workflows into LLM-based multi-agent collaborations and was adopted as the infrastructure for agent orchestration in MARE.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Described as supporting multi-agent collaboration with integrated human workflows (paper does not specify low-level mechanism within MetaGPT beyond this description).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General multi-agent LLM orchestration (used here to implement a Requirements Engineering multi-agent system).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Used as implementation backbone for MARE; claimed to enable efficient human workflows in multi-agent setups (no quantitative results in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARE: Multi-Agents Collaboration Framework for Requirements Engineering', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2541.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2541.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatDev</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatDev (Communicative agents for software development)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work that instantiated a virtual chat-powered software development company comprising multiple communicative agents to explore integrating LLMs into software development workflows; mentioned in related work as inspiration for multi-agent design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Communicative agents for software development</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatDev (Communicative agents for software development)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as a prior multi-agent design that presents a virtual chat-powered software development company to explore integrating LLMs into software development; cited in the related-work comparison but not used or evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Software development (prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARE: Multi-Agents Collaboration Framework for Requirements Engineering', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2541.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2541.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agentverse</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agentverse (facilitating multi-agent collaboration and exploring emergent behaviors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agentverse is cited as a framework for facilitating multi-agent collaboration and studying emergent behaviors in agents; referenced in related work as part of the multi-agent systems literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Agentverse</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as prior work that facilitates multi-agent collaboration and explores emergent behaviors; cited as background motivation for multi-agent designs in tasks like RE, but not used or experimentally evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General multi-agent research (mentioned in context of multi-agent collaboration literature).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARE: Multi-Agents Collaboration Framework for Requirements Engineering', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2541.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2541.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-agent debate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-agent debate for improved reasoning and factuality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A line of work that improves reasoning accuracy and factuality by leveraging multiple agents debating a topic; cited in related work as evidence that multi-agent strategies can improve LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving factuality and reasoning in language models through multiagent debate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-agent debate</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as prior research showing that multi-agent debate mechanisms can improve factuality and reasoning in language models. The paper cites this work to motivate multi-agent approaches but does not implement or evaluate debate-style coordination within MARE.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Debate-style multi-agent interaction (mentioned generically); not implemented in MARE according to this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Debate produces peer critique/rebuttal (in the cited work), but no direct use in MARE.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Improving reasoning/factuality in LLMs (prior research).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Cited as improving reasoning accuracy in other work; no quantitative results presented within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARE: Multi-Agents Collaboration Framework for Requirements Engineering', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Metagpt: Meta programming for multi-agent collaborative framework <em>(Rating: 2)</em></li>
                <li>Communicative agents for software development <em>(Rating: 2)</em></li>
                <li>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents <em>(Rating: 2)</em></li>
                <li>Improving factuality and reasoning in language models through multiagent debate <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2541",
    "paper_id": "paper-7ef57c7a1ec7415e7fbf94aeb245cbd8f104fbe1",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [
        {
            "name_short": "MARE",
            "name_full": "Multi-Agents Collaboration Framework for Requirements Engineering",
            "brief_description": "MARE is a prompt-engineering-driven multi-agent framework that leverages large language models (LLMs) to automate the end-to-end requirements engineering (RE) process by coordinating specialized agents to perform elicitation, modeling, verification, and specification through iterative collaboration using a shared workspace and action migration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MARE",
            "system_description": "MARE is a multi-agent system built on LLMs that automates the entire Requirements Engineering pipeline. It decomposes RE into four tasks (elicitation, modeling, verification, specification) and implements five specialized agents (Stakeholders, Collector, Modeler, Checker, Documenter). Each agent is implemented by designing prompt templates for predefined actions (nine total). Agents exchange and coordinate by reading and writing structured requirement artifacts into a shared workspace; control flow is governed by an action migration diagram that sequences actions and iteratively triggers rework when the Checker reports problems. Human interaction can seed the process and provide feedback.",
            "number_of_agents": "5",
            "agent_specializations": "Stakeholders (express user stories and answer questions), Collector (interview stakeholders and write requirement drafts; actions: ProposeQuestion, WriteReqDraft), Modeler (extract modeling entities and relations from drafts; actions: ExtractEntity, ExtractRelation), Checker (assess requirement draft and model quality against acceptance criteria; action: CheckRequirement), Documenter (generate software requirements specification or error reports based on templates; actions: WriteSRS, WriteCheckReport).",
            "research_phases_covered": "Elicitation (idea elicitation / stakeholder requirement capture), Modeling (construct formal requirements models), Verification (quality checking / smell detection against acceptance criteria), Specification (documenting final SRS), plus iterative refinement loops; maps to idea generation, modeling/representation, evaluation, and documentation.",
            "coordination_mechanism": "Centralized shared-workspace with a sequential/iterative pipeline governed by an explicit action-migration diagram: agents sequentially perform actions (e.g., SpeakUserStories -&gt; WriteReqDraft -&gt; ExtractEntity -&gt; ExtractRelation -&gt; CheckRequirement -&gt; WriteSRS), with iterative loops where Checker-generated error reports trigger Collector and Modeler to sequentially revise artifacts; control is orchestrated centrally through the workspace and the action migration logic.",
            "communication_protocol": "Shared workspace (shared memory) storing structured requirement artifacts; each artifact carries metadata fields {content, role, caused_by, sent_from, send_to} that indicate artifact content, originating agent, generating action (for action migration), and sender/recipient flow; agents implement actions via LLM prompts whose inputs are defined artifact fields (natural-language templates + structured artifact fields). Communication is thus a mix of natural-language messages (prompt inputs/LLM outputs) stored as structured artifacts in the shared workspace.",
            "feedback_mechanism": "Iterative refinement loop via Checker: Checker produces a checking_message or an error_report; if smells/errors are found the Documenter writes an error report which triggers Collector and Modeler to sequentially revise the requirement draft and model; HumanInteraction can provide external feedback at start or during process. Feedback is implemented as artifacts (checking_message, error_report) in the shared workspace that cause subsequent action migrations.",
            "communication_frequency": "On-demand and phase-driven: agents communicate after each action/phase by writing artifacts to the shared workspace; the workflow is sequential (after each step) and iterative when Checker reports errors (phase transitions trigger re-execution of relevant agent actions).",
            "task_domain": "Software engineering — Requirements Engineering (elicitation, modeling, verification, specification).",
            "performance_metrics": "For requirements modeling: Precision (P), Recall (R), F1 reported per benchmark. Key reported numbers for MARE (gpt-3.5-turbo): problem diagrams average P=82.6%, R=88.3%, F1=84.1%; use case diagrams average P=77.4%, R=80.6%, F1=78.9%; goal models average P≈87.4%, R≈87.9%, F1≈87.6%. For specifications: human evaluation scores (scale 0–2) averaged across 9 cases: completeness=0.98, correctness=1.92, consistency=1.98. Implementation hyperparameters: temperature=0, max_tokens=3000, top_p=1, frequency_penalty=0, presence_penalty=0.",
            "baseline_comparison": "MARE is compared to three SOTA baselines: EPD (problem diagrams), IT4RE (use case diagrams), HAGM (goal models). Improvements reported (MARE gpt-3.5-turbo vs baselines): problem diagrams +15.4% absolute average F1 over EPD, use case diagrams +23.8% absolute average F1 over IT4RE, goal models +0.6% absolute average F1 over HAGM. Ablation vs single LLM: +0.2% precision, +2% recall, +1.1% F1 (MARE vs individual gpt-3.5-turbo on use-case tasks).",
            "coordination_benefits": "Coordination yields improved modeling accuracy and robustness: substantial F1 improvements over SOTA baselines (e.g., +15.4% on problem diagrams, +23.8% on use cases), modest gains over a single LLM in ablation (+1.1% F1). Enables end-to-end automation of RE pipeline and iterative error-correction (Checker-driven rework) leading to higher-quality specifications (human-eval correctness and consistency ~1.9–2.0).",
            "coordination_challenges": "Reported limitations include lower completeness in generated SRS (average completeness score 0.98) attributed to LLM tendency to reduce length and to insufficient detail in some sections; stochasticity in LLM responses (mitigated by fixed hyperparameters but still present); the paper does not report explicit communication overhead metrics, but the sequential iterative pipeline implies extra latency for multiple cycles. No large-scale failure modes are quantified in the paper.",
            "ablation_studies": "Yes; an ablation comparing MARE (multi-agent collaboration) to a single LLM (gpt-3.5-turbo) that directly extracts modeling elements from the drafts shows MARE outperforms the individual LLM by +0.2% average precision, +2.0% average recall, and +1.1% average F1 on 5 public use-case cases (Table VI and RQ3).",
            "optimal_configurations": "The paper reports implementation choices that produced the reported results: using gpt-3.5-turbo yields best performance among tested LLMs (gpt-3.5-turbo, text-davinci-002, text-davinci-003); a five-agent specialization with nine defined actions and a shared workspace coordination pattern; LLM hyperparameters used for stability: temperature=0, max_tokens=3000, top_p=1, frequency_penalty=0, presence_penalty=0. The paper does not claim broader optimality beyond these tested settings.",
            "uuid": "e2541.0",
            "source_info": {
                "paper_title": "MARE: Multi-Agents Collaboration Framework for Requirements Engineering",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "MetaGPT",
            "name_full": "MetaGPT (meta-programming framework for multi-agent collaboration)",
            "brief_description": "MetaGPT is a meta-programming framework that incorporates efficient human workflows into LLM-based multi-agent collaborations; the authors used the open-source MetaGPT framework to build and orchestrate MARE.",
            "citation_title": "Metagpt: Meta programming for multi-agent collaborative framework",
            "mention_or_use": "use",
            "system_name": "MetaGPT",
            "system_description": "Described in the paper as an open-source multi-agent framework used to implement MARE. It is characterized as a meta-programming framework that incorporates human workflows into LLM-based multi-agent collaborations and was adopted as the infrastructure for agent orchestration in MARE.",
            "number_of_agents": null,
            "agent_specializations": null,
            "research_phases_covered": null,
            "coordination_mechanism": "Described as supporting multi-agent collaboration with integrated human workflows (paper does not specify low-level mechanism within MetaGPT beyond this description).",
            "communication_protocol": null,
            "feedback_mechanism": null,
            "communication_frequency": null,
            "task_domain": "General multi-agent LLM orchestration (used here to implement a Requirements Engineering multi-agent system).",
            "performance_metrics": null,
            "baseline_comparison": null,
            "coordination_benefits": "Used as implementation backbone for MARE; claimed to enable efficient human workflows in multi-agent setups (no quantitative results in this paper).",
            "coordination_challenges": null,
            "ablation_studies": null,
            "optimal_configurations": null,
            "uuid": "e2541.1",
            "source_info": {
                "paper_title": "MARE: Multi-Agents Collaboration Framework for Requirements Engineering",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ChatDev",
            "name_full": "ChatDev (Communicative agents for software development)",
            "brief_description": "A prior work that instantiated a virtual chat-powered software development company comprising multiple communicative agents to explore integrating LLMs into software development workflows; mentioned in related work as inspiration for multi-agent design.",
            "citation_title": "Communicative agents for software development",
            "mention_or_use": "mention",
            "system_name": "ChatDev (Communicative agents for software development)",
            "system_description": "Mentioned as a prior multi-agent design that presents a virtual chat-powered software development company to explore integrating LLMs into software development; cited in the related-work comparison but not used or evaluated in this paper.",
            "number_of_agents": null,
            "agent_specializations": null,
            "research_phases_covered": null,
            "coordination_mechanism": null,
            "communication_protocol": null,
            "feedback_mechanism": null,
            "communication_frequency": null,
            "task_domain": "Software development (prior work).",
            "performance_metrics": null,
            "baseline_comparison": null,
            "coordination_benefits": null,
            "coordination_challenges": null,
            "ablation_studies": null,
            "optimal_configurations": null,
            "uuid": "e2541.2",
            "source_info": {
                "paper_title": "MARE: Multi-Agents Collaboration Framework for Requirements Engineering",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Agentverse",
            "name_full": "Agentverse (facilitating multi-agent collaboration and exploring emergent behaviors)",
            "brief_description": "Agentverse is cited as a framework for facilitating multi-agent collaboration and studying emergent behaviors in agents; referenced in related work as part of the multi-agent systems literature.",
            "citation_title": "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents",
            "mention_or_use": "mention",
            "system_name": "Agentverse",
            "system_description": "Referenced as prior work that facilitates multi-agent collaboration and explores emergent behaviors; cited as background motivation for multi-agent designs in tasks like RE, but not used or experimentally evaluated in this paper.",
            "number_of_agents": null,
            "agent_specializations": null,
            "research_phases_covered": null,
            "coordination_mechanism": null,
            "communication_protocol": null,
            "feedback_mechanism": null,
            "communication_frequency": null,
            "task_domain": "General multi-agent research (mentioned in context of multi-agent collaboration literature).",
            "performance_metrics": null,
            "baseline_comparison": null,
            "coordination_benefits": null,
            "coordination_challenges": null,
            "ablation_studies": null,
            "optimal_configurations": null,
            "uuid": "e2541.3",
            "source_info": {
                "paper_title": "MARE: Multi-Agents Collaboration Framework for Requirements Engineering",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Multi-agent debate",
            "name_full": "Multi-agent debate for improved reasoning and factuality",
            "brief_description": "A line of work that improves reasoning accuracy and factuality by leveraging multiple agents debating a topic; cited in related work as evidence that multi-agent strategies can improve LLM reasoning.",
            "citation_title": "Improving factuality and reasoning in language models through multiagent debate",
            "mention_or_use": "mention",
            "system_name": "Multi-agent debate",
            "system_description": "Referenced as prior research showing that multi-agent debate mechanisms can improve factuality and reasoning in language models. The paper cites this work to motivate multi-agent approaches but does not implement or evaluate debate-style coordination within MARE.",
            "number_of_agents": null,
            "agent_specializations": null,
            "research_phases_covered": null,
            "coordination_mechanism": "Debate-style multi-agent interaction (mentioned generically); not implemented in MARE according to this paper.",
            "communication_protocol": null,
            "feedback_mechanism": "Debate produces peer critique/rebuttal (in the cited work), but no direct use in MARE.",
            "communication_frequency": null,
            "task_domain": "Improving reasoning/factuality in LLMs (prior research).",
            "performance_metrics": null,
            "baseline_comparison": null,
            "coordination_benefits": "Cited as improving reasoning accuracy in other work; no quantitative results presented within this paper.",
            "coordination_challenges": null,
            "ablation_studies": null,
            "optimal_configurations": null,
            "uuid": "e2541.4",
            "source_info": {
                "paper_title": "MARE: Multi-Agents Collaboration Framework for Requirements Engineering",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Metagpt: Meta programming for multi-agent collaborative framework",
            "rating": 2
        },
        {
            "paper_title": "Communicative agents for software development",
            "rating": 2
        },
        {
            "paper_title": "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents",
            "rating": 2
        },
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate",
            "rating": 2
        }
    ],
    "cost": 0.012391,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MARE: Multi-Agents Collaboration Framework for Requirements Engineering</h1>
<p>Dongming Jin ${ }^{1,2}$, Zhi Jin ${ }^{1,2}$, Xiaohong Chen ${ }^{3}$, Chunhui Wang ${ }^{4}$<br>${ }^{1}$ School of Computer Science, Peking University, China<br>${ }^{2}$ Key Lab of High-Confidence of Software Technologies (PKU), Ministry of Education, China<br>${ }^{3}$ Shanghai Key Laboratory of Trustworthy Computing, East China Normal University, China<br>${ }^{4}$ College of Computer Science and Technology, Inner Mongolia Normal University, China<br>dmjin@stu.pku.edu.cn, zhijin@pku.edu.cn</p>
<h4>Abstract</h4>
<p>Requirements Engineering (RE) is a critical phase in the software development process that generates requirements specifications from stakeholders' needs. Recently, deep learning techniques have been successful in several RE tasks. However, obtaining high-quality requirements specifications requires collaboration across multiple tasks and roles. In this paper, we propose an innovative framework called MARE, which leverages collaboration among large language models (LLMs) throughout the entire RE process. MARE divides the RE process into four tasks: elicitation, modeling, verification, and specification. Each task is conducted by engaging one or two specific agents and each agent can conduct several actions. MARE has five agents and nine actions. To facilitate collaboration between agents, MARE has designed a workspace for agents to upload their generated intermediate requirements artifacts and obtain the information they need. We conduct experiments on five public cases, one dataset, and four new cases created by this work. We compared MARE with three baselines using three widely used metrics for the generated requirements models. Experimental results show that MARE can generate more correct requirements models and outperform the state-of-the-art approaches by $\mathbf{1 5 . 4 \%}$. For the generated requirements specifications, we conduct a human evaluation in three aspects and provide insights about the quality.</p>
<p>Index Terms-Requirements Engineering; Large Language Model; Deep Learning</p>
<h2>I. INTRODUCTION</h2>
<p>Requirements Engineering (RE) is one of the most critical phases in the software development process. It aims to generate a well-defined set of requirements by eliciting, analyzing, specifying, and verifying the needs and expectations of stakeholders for a software system [1]. As the complexity and scale of the software system [2] continue to grow, developers will spend a lot of effort on various tasks in RE to create high-quality requirements models and write requirements specifications.</p>
<p>Nowadays, deep learning (DL) techniques have been successfully applied to automate various RE tasks [3]. For example, DL has been used to mine stakeholders' needs [4] [5], extract requirements model [6] [7], and deal with ambiguity in requirements [8]. However, to obtain high-quality requirements specifications, different RE tasks need to be interwoven and iterated. The automation of only a few of these tasks limits further increase in effectiveness and efficiency.</p>
<p>Recently, large language models (LLMs) have achieved significant performance in the fields of software engineering
(SE) [9] and natural language processing (NLP) [10]. There have also been some early explorations of the application of LLMs to RE [11] [12]. For example, [13] used LLMs to generate requirements elicitation interview scripts. [14] used ChatGPT to detect inconsistency in natural language requirements. [15] provided prompt strategies for requirements traceability. These efforts demonstrate the effectiveness of using LLMs for specific requirements tasks and do not take full advantage of LLMs for collaboration among RE tasks.</p>
<p>Inspired by the multi-agents collaboration [16] and team design, researchers started to investigate the possibility of designing the communicative agents mechanisms [17] for facilitating the accomplishment of complex tasks. [18] presented a virtual chat-powered software development company to unveil fresh possibilities for integrating LLMs into the realm of software development. [19] proposed an meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations, allowing agents with human-like domain expertise to verify intermediate results and reduce errors. Studies show that multi-agent collaboration mechanisms are interrelated with specific tasks.</p>
<p>Along this line, in this paper, we propose a novel framework, named MARE (Multi-Agent collaboration for Requirements Engineering) to automate the entire RE process based on collaboration between multiple tasks and roles. Specifically, given a very rough idea of requirements, MARE autonomously and iteratively performs the basic tasks of RE, i.e., elicitation, modeling, verification, and specification, to achieve end-to-end requirements specifications generation. Each task is accomplished with one or two specific agents and MARE contains five agents, i.e., the stakeholders, the collector, the modeler, the checker, and the documenter. Each agent is responsible for a requirements task and performs predefined actions to accomplish that task. Nine actions are designed for this purpose. MARE also offers a shared workspace to these agents for supporting the collaboration among agents.</p>
<p>We conduct extensive experiments to evaluate MARE. (1) To evaluate its modeling ability, we compare MARE with three baselines for automated requirements modeling on five public cases, one public dataset, and four new cases created in this paper. The three baselines are state-of-the-art (SOTA) methods for use case diagrams, problem diagrams, and goal</p>
<p>models, respectively. Three widely used evaluation metrics (Precision(P), Recall(R), and F1) are used for the evaluation. Results demonstrate the impressive performance of MARE on requirements modeling. In terms of F1, MARE(gpt-3.5-turbo) outperforms the three SOTA baselines by up to 15.4%, 23.9%, and 0.6%, respectively. (2) To evaluate its specification generation ability, we conduct a human evaluation to evaluate the quality of generated requirements specifications in three aspects (correctness, completeness, and consistency) and provide insights about their quality. (3) We conduct an ablation study comparing individual LLMs with MARE on requirements modeling task. Results prove the contributions of multi-agents collaboration framework.</p>
<p>We summarize our contributions as follows.</p>
<ul>
<li>To the best of our knowledge, this is the first study exploring end-to-end automation of the entire RE process. This study unifies main tasks through LLMs, eliminating the need for specialized models at each task.</li>
<li>We propose MARE, a multi-agents collaboration framework for RE. By merely specifying a rough idea about the requirements, MARE iteratively handles requirements elicitation, modeling, verification, and specification.</li>
<li>We conduct extensive experiments on nine cases and one dataset. Compared with three SOTA baselines, MARE shows superiority in modeling. We further manually evaluate the generated specifications and provide insights about the quality of them in three aspects.</li>
</ul>
<p>In the remainder of the paper, Section II presents the approach. Section III sets up the experiments. Section IV describes the results and analysis. Section V introduces the related work. Section VI concludes our work.</p>
<h2>II. APPROACH</h2>
<p>This section presents the multi-agent collaboration framework, named MARE, for end-to-end RE based on LLMs.</p>
<h3>A. Overview</h3>
<p>Requirements Engineering is characterized by discussions and brainstorming to define the scope, functionality, and quality of a product. The RE process is challenging because it usually consists of multiple tasks, e.g., requirements elicitation, requirements modeling, requirements verification, and specification. Traditionally, requirements engineers as well as stakeholders actively iteratively collaborate with each other to establish a common understanding of the envisioned product. We investigate the possibility of automating the end-to-end RE process from a novel perspective by proposing a generative framework, i.e. to create a generative model $G[S|X)$ that generates a requirement specification $S$ based on a rough idea of requirements $X$.</p>
<p>Concretely, we design a multi-agent framework, MARE, that leverages LLMs in the RE process. We design prompt-engineering-driven LLM agents to allow these agents with abilities to do RE tasks and introduce a shared workspace in which the artifacts can be stored as the communication structure. To obtain requirements specifications that meet quality standards, these agents work together iteratively and collaboratively to complete the RE process, just like the Human Requirements Engineering team does. Figure 1 depicts this framework, in which, we assume the following four tasks in the RE process:</p>
<ul>
<li>Elicitation. Given a rough idea of requirements $X$, this task collects stakeholders’ needs $U$ and generates a requirements draft $D$.</li>
<li>Modeling. Based on the generated requirements draft $D$, this task generates a requirements model $M$ as required by the ‘Metamodel’.</li>
<li>verification. The task detects requirements smells $S$ in the requirements draft $D$ and requirements model $M$ with ‘accept criteria’.</li>
<li>Specification. If requirements smells $S$ don’t exist, this task generates a requirement specification $R$ based on the requirements draft $D$, and requirements model $M$ by following a ‘Template’. Otherwise, this task generates an Error report $E$.</li>
</ul>
<h2>B. Tasks</h2>
<p>As shown in Figure 1, the RE-Agent Collaboration team conducts iteratively four tasks in MARE.</p>
<p>The purpose of the requirements elicitation task is to get stakeholders’ needs $U$ corresponding to the rough idea of system requirements $X$. Various Stakeholders can be involved to express their requirements for this system. The requirements Collector interviews these stakeholders to further delve into their needs and preferences and write down their needs into a requirement draft $D$. The information Collector agent and a set of Stakeholders agents are designed to complete this task.</p>
<p>The purpose of the requirements modeling task is to elaborate the requirements models, including the extraction of the requirements entities and relations. Following the previous study [6], the requirements Modeler identifies requirements entities from the requirements draft $D$, e.g., the actors in use case diagrams and the machine in problem diagrams. The Modeler then decides whether a certain relationship exists between any pair of entities and determines the type of the relationship, e,g., the include in use case diagrams and the requirements reference in problem diagrams. Finally, the Modeler combines these entities and relationships to form the requirements model $M$. The Modeler agent is designed to accomplish this modeling task.</p>
<p>The purpose of the requirements verification task is to confirm that the system requirements contain all the necessary elements of well-written requirements, e.g., correctness, completeness, and consistency. In the real world, the quality Checker first figures out the acceptance criterion, then reads the requirements draft $D$ and requirements model $M$ to assess the quality of the current requirements draft. The Checker agent is designed in MARE to finish this quality-checking task.</p>
<p>Finally, if the quality of the current version of the requirements document meet the quality criteria, the specification task write the software requirements specification (SRS), otherwise presents an error report. The Documenter writes SRS $R$ based</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. The overview of our MARE</p>
<p>on requirements draft <em>D</em> and requirements model <em>M</em> according to the pre-defined SRS template or presents the Error Report. The <strong>Documenter</strong> agent is designed for this task.</p>
<p>When an error report appears in the shared workspace, <strong>Collector</strong> and <strong>Modeler</strong> will work sequentially, improving the requirements draft <em>D</em> and requirements model <em>M</em>, respectively, and then <strong>Checker</strong> will check their quality again.</p>
<h3><em>C. Agent Definition</em></h3>
<p>The necessary agents in <em>MARE</em> have been identified in Section II-B. They are <strong>Stakeholders</strong>, <strong>Collector</strong>, <strong>Modeler</strong>, <strong>Checker</strong>, and <strong>Documenter</strong>. Here we describe each agent and the interactions among agents.</p>
<p><strong>Agent Specialization.</strong> In <em>MARE</em>, any agent has a role definition, which includes the name, the profile, the goal, and the actions. The name and profile indicate which role the agents play. The goal describes tasks for which the agent. The action gives the skills that the agent has. For instance, <strong>Collector</strong> can interview with <strong>Stakeholders</strong> and write the requirement draft based on user stories. Thus, the <strong>Collector</strong> agent is equipped with <em>ProposeQuestion</em> action and <em>WriteReqDraft</em> action. Figure 2 shows an example of the <strong>Collector</strong> agent.</p>
<p>The other agents are defined via the same way based on the analysis in section II-B. The <strong>Stakeholders</strong> agent can express their expectations for the systems and answer questions from <strong>Collector</strong>. Then, So the <strong>Stakeholders</strong> agent can do actions such as <em>SpeakUserStories</em> and <em>AnswerQuestion</em>. The <strong>Modeler</strong> agent splits the requirements modeling task into two steps, which are extracting requirements entities and determining the relationships among the entities. So the <strong>Modeler</strong> agent can do actions such as <em>ExtractEntity</em> and <em>ExtractRelation</em>.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. The profile of the <strong>Collector</strong> agent.</p>
<p>The <strong>Checker</strong> agent can do <em>CheckRequirement</em> action. The <strong>Documenter</strong> agent can write requirements specifications or present the error report. So <strong>Documenter</strong> agent is equipped with <em>WriteSRS</em> and <em>WriteChceckReport</em> action. The role definition of each agent is shown in Table I.</p>
<p><strong>Interaction between Agents.</strong> <em>MARE</em> adopts a shared workspace mechanism [19] to facilitate effective interaction and collaboration between different agents. As shown in figure 3, the shared workspace stores various requirements artifacts which can be operated by agents, such as the use stories, the requirements drafts, the requirements models, and the requirements specifications, etc.. The requirements artifacts</p>
<p>TABLE I
THE ROLE DEFINITION OF AGENTS</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Agent</th>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">Profile</th>
<th style="text-align: center;">Goal</th>
<th style="text-align: center;">Action</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Stakeholders</td>
<td style="text-align: center;">Alice</td>
<td style="text-align: center;">An experienced requirement stakeholder</td>
<td style="text-align: center;">express and task the stakeholders' need for system to be developed</td>
<td style="text-align: center;">a. SpeakUserStories <br> b. AnswerQuestion</td>
</tr>
<tr>
<td style="text-align: center;">Collector</td>
<td style="text-align: center;">Bob</td>
<td style="text-align: center;">An experienced requirement collector</td>
<td style="text-align: center;">interview the stakeholders to collect stakeholders' needs</td>
<td style="text-align: center;">a. ProposeQuestion <br> b. WriteReqDraft</td>
</tr>
<tr>
<td style="text-align: center;">Modeler</td>
<td style="text-align: center;">Carol</td>
<td style="text-align: center;">An experienced requirement modeler</td>
<td style="text-align: center;">extract requirement model, including entities and relations</td>
<td style="text-align: center;">a. ExtractEntity <br> b. ExtractRelation</td>
</tr>
<tr>
<td style="text-align: center;">Checker</td>
<td style="text-align: center;">Dave</td>
<td style="text-align: center;">An experienced requirement checker</td>
<td style="text-align: center;">check the requirement quality based the requirement model.</td>
<td style="text-align: center;">a. CheckRequirement</td>
</tr>
<tr>
<td style="text-align: center;">Documenter</td>
<td style="text-align: center;">Eve</td>
<td style="text-align: center;">An experienced requirement documenter</td>
<td style="text-align: center;">write the requirement specification or or checking report.</td>
<td style="text-align: center;">a. WriteSRS <br> b. WriteCheckReport</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. The mechanism of shared workspace.
in the workspace have five properties, which are content, role, caused_by, sent_from, and send_to. The content property represents the content of the requirements artifacts. The role property indicates which agent generates the requirements artifacts. The caused_by property describes which action generates the requirements artifacts, which can be used to perform action migration. The action migration mechanism will be described in detail in the next section. The sent_from and send_to properties give the flow of requirements artifacts among agents. For example, the content of use stories can be "As a homeowner, I want to be able to control the temperature in my home remotely, so that I can adjust it to my desired level before I arrive.", the role and cause_by of use stories can be stakeholders and SpeakUserStories. The sent_from and send_to can be Stakeholders and Collector.</p>
<h2>D. Action Definition</h2>
<p>Action Space. Section II-C describes the actions of each agent. There are a total of nine actions in MARE. Each action is implemented by designing a prompt for LLMs. Therefore, we designed multiple prompts of various actions for LLMs and the details about these prompts are at the open link [20]. Table II shows the input and output for each action. The input will be put in each designed prompt and the output is referred to generated requirements artifacts by each action. The input for these actions is generated by agents within MARE, except for the rough idea of requirements which is</p>
<p>TABLE II
THE INPUT AND OUTPUT ARTIFACTS FOR EACH ACTION</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Action</th>
<th style="text-align: left;">Input</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SpeakUserStories</td>
<td style="text-align: left;">rough_idea_of_requirements, <br> num_user_stories</td>
<td style="text-align: left;">user_stories</td>
</tr>
<tr>
<td style="text-align: left;">ProposeQuestion</td>
<td style="text-align: left;">rough_idea_of_requirements <br> user_stories</td>
<td style="text-align: left;">requirement_question</td>
</tr>
<tr>
<td style="text-align: left;">AnswerQuestion</td>
<td style="text-align: left;">rough_idea_of_requirements, <br> requirement_question</td>
<td style="text-align: left;">stakeholders_answer</td>
</tr>
<tr>
<td style="text-align: left;">WriteReqDraft</td>
<td style="text-align: left;">rough_idea_of_requirements, <br> user_stories, <br> requirement_question, <br> requirement_answer, <br> draft template</td>
<td style="text-align: left;">requirement_draft</td>
</tr>
<tr>
<td style="text-align: left;">ExtractEntity</td>
<td style="text-align: left;">requirement_meta_model, <br> requirement_draft</td>
<td style="text-align: left;">modeling_entities</td>
</tr>
<tr>
<td style="text-align: left;">ExtractRelation</td>
<td style="text-align: left;">requirement_meta_model, <br> requirement_draft, <br> modeling_entities</td>
<td style="text-align: left;">modeling_relationship</td>
</tr>
<tr>
<td style="text-align: left;">CheckRequirement</td>
<td style="text-align: left;">modeling_entities, <br> modeling_relationship, <br> requirement_draft</td>
<td style="text-align: left;">checking_message</td>
</tr>
<tr>
<td style="text-align: left;">WriteSRS</td>
<td style="text-align: left;">modeling_entities, <br> modeling_relationship, <br> requirement_draft, <br> SRS_template</td>
<td style="text-align: left;">software requirement <br> specification</td>
</tr>
<tr>
<td style="text-align: left;">WriteCheckReport</td>
<td style="text-align: left;">checking_message</td>
<td style="text-align: left;">error_report</td>
</tr>
</tbody>
</table>
<p>provided by humans. For the requirements draft and software requirement specification in Table II, their generation follows our predefined templates. The details of these templates are written in the prompt of WriteReqDraft action and WriteSRS action, which are publicly available at the same open link.</p>
<p>Action Migration. MARE iteratively performs a series of actions to achieve the end-to-end RE process. There is a sequential relationship between these actions. For example, action WriteDraft happens after action SpeakUserStories. Figure 4 shows the whole action migration mechanisms in MARE. HumanInteraction is the start action and it can provide human feedback to MARE.</p>
<h2>III. Study Design</h2>
<p>To assess the effectiveness of our MARE, we perform an extensive study to answer three research questions. In this section, we describe the details of our study, including evaluation cases, metrics, baselines, and experiment settings.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. The action migration diagram in MARE</p>
<h3><em>A. Research Questions</em></h3>
<p>Our study aims to answer three research questions (RQ). In RQ1, we compare our <em>MARE</em> to three SOTA automated requirements modeling baselines on 9 evaluation cases and 1 dataset to prove the modeling superiority of <em>MARE</em>. In RQ2, we conduct a human evaluation to evaluate the generated software requirements specifications on 9 software systems in three aspects. In RQ3, we conduct an ablation study to prove the contributions of multi-agent collaboration.</p>
<p><strong>RQ1: How do the requirements models generated by <em>MARE</em> compared to the SOTA approaches?</strong> We first use <em>MARE</em> to generate requirements models on 9 evaluation cases. Then, we use multiple metrics to evaluate the generated requirements models and compare them with the existing three SOTA baselines. We select multiple LLMs to verify that <em>MARE</em> is effective for different LLMs.</p>
<p><strong>RQ2: How do the requirements specifications generated by <em>MARE</em></strong>?.<em><em> We first use </em>MARE</em> to generate software requirements specifications on 9 evaluation cases. Then, we manually evaluate the generated requirements specifications in three aspects. Finally, we calculate the average results.</p>
<p><strong>RQ3: How does <em>MARE</em> perform compared to individual LLM?</strong> Our <em>MARE</em> contains multiple LLMs-based agents. We assess the contributions of agents collaboration by comparing <em>MARE</em> with individual LLMs on requirements modeling.</p>
<h3><em>B. Evaluation Cases</em></h3>
<p>We conduct experiments on 5 public evaluation cases for generating use case diagrams, 1 public dataset for generating goal models, and 4 new evaluation cases for generating problem diagrams collected by this work.</p>
<p><strong>Five public evaluation cases</strong> are proposed for automated modeling use case diagrams. The 5 cases are the ATM system (ATM) [21], the cafeteria ordering system (COS) [22], the library system (TLS) [23], the assembly system (TAS) [24] and the time monitor system (TMS) [25]. They are common and are often used to evaluate DL model to extract actors and actions in the use case. The original requirements descriptions for these cases can be obtained from the existing literature.</p>
<p><strong>One public evaluation dataset</strong> are proposed for automating modeling goal models [26]. The origin requirements descriptions in this dataset come from PURE [27], a dataset of 79 publicly available natural language requirements documents collected from the Web. In this paper, we refer to this dataset as GoalModelDataset.</p>
<p><strong>Four new evaluation cases</strong> are proposed for automated modeling problem diagrams. The 4 cases are the smart home control system (TSHCS), the temperature control system (TTCS), the air conditioner control system (TACCS), and the humidistat control system (THCS). The origin requirements descriptions for these cases are shown in the same open link as the section II-D.</p>
<h3><em>C. Evaluation Metrics</em></h3>
<p><strong>Evaluation for requirements models.</strong> We use three commonly used metrics to evaluate the performance, i.e., Precision, Recall, and F1. (1) Precision (P), which refers to the ratio of the number of correct predictions to the total number of predictions; (2) Recall (R), which refers to the ratio of the number of correct predictions to the total number of samples in the golden test set; and (3) F1, which is the harmonic mean of precision and recall. When comparing the performance, we care more about F1 since it is balanced for evaluation.</p>
<p><strong>Evaluation for requirements specifications.</strong> Following the previous work [28], we manually evaluate the generated specifications by <em>MARE</em> in three aspects, including completeness, correctness, and consistency. For each aspect, the score is ranging from 0 to 2 (from bad to good). The evaluators are computer science master students and are not co-authors.</p>
<h3><em>D. Baselines</em></h3>
<p>We select 3 recently proposed automated requirements modeling approaches as baselines. They are state-of-the-art methods for automated modeling of problem diagrams, use case diagrams, and goal models, respectively.</p>
<ul>
<li><strong>IT4RE</strong> [29]: is an approach to automatically identify actors and actions from natural language requirements' description using an NLP parser.</li>
<li><strong>EPD</strong> [6]: is a method for Extracting requirements entities, e.g., machine domain, requirement domain and given domain, in Problem Diagram based on BERT.</li>
<li><strong>HAGM</strong> [26]: is a Hybrid Approach to requirements entities, e.g., role, agent, and relation, in Goal Model based on machine learning and logical reasoning.</li>
</ul>
<h3><em>E. Experiment Settings</em></h3>
<p>The implementation details of our <em>MARE</em> are as follows. We use the open-source multi-agent framework - MetaGPT [30] to build our <em>MARE</em>. For the LLM engine of the <em>MARE</em>, we use different versions of ChatGPT-3.5, named gpt-3.5-turbo [31], text-davinci-002 [31] and text-davinci-003 [31]. They are the closed-source model developed by OpenAI. So we experimented by purchasing the access api-key from OpenAI. Considering that the responses of the LLMs are somewhat random, in order to ensure the stability of the experimental results, we set temperature as 0, max_tokens as 3000, top_p</p>
<p>TABLE III
THE RESULTS OF MARE ON MODELING PROBLEM DIAGRAM</p>
<table>
<thead>
<tr>
<th>Cases</th>
<th>EPD</th>
<th></th>
<th></th>
<th>MARE(gpt-3.5-turbo)</th>
<th></th>
<th></th>
<th>MARE(text-davinci-002)</th>
<th></th>
<th></th>
<th>MARE(text-davinci-003)</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$\mathrm{P}(\%)$</td>
<td>$\mathrm{R}(\%)$</td>
<td>$\mathrm{F} 1(\%)$</td>
<td>$\mathrm{P}(\%)$</td>
<td>$\mathrm{R}(\%)$</td>
<td>$\mathrm{F} 1(\%)$</td>
<td>$\mathrm{P}(\%)$</td>
<td>$\mathrm{R}(\%)$</td>
<td>$\mathrm{F} 1(\%)$</td>
<td>$\mathrm{P}(\%)$</td>
<td>$\mathrm{R}(\%)$</td>
<td>$\mathrm{F} 1(\%)$</td>
</tr>
<tr>
<td>THICS</td>
<td>76.4</td>
<td>67.9</td>
<td>72.7</td>
<td>85.7</td>
<td>92.3</td>
<td>83.8</td>
<td>86.9</td>
<td>79.3</td>
<td>82.9</td>
<td>80.6</td>
<td>82.8</td>
<td>81.7</td>
</tr>
<tr>
<td>TTCS</td>
<td>72.5</td>
<td>58.3</td>
<td>64.6</td>
<td>81.6</td>
<td>86.1</td>
<td>83.8</td>
<td>72.9</td>
<td>80.1</td>
<td>76.3</td>
<td>81.7</td>
<td>84.2</td>
<td>82.9</td>
</tr>
<tr>
<td>TACCS</td>
<td>71.3</td>
<td>62.4</td>
<td>66.6</td>
<td>81.3</td>
<td>88.4</td>
<td>84.7</td>
<td>86.6</td>
<td>85.7</td>
<td>86.1</td>
<td>85.3</td>
<td>82.5</td>
<td>83.9</td>
</tr>
<tr>
<td>THCS</td>
<td>74.7</td>
<td>67.2</td>
<td>70.8</td>
<td>81.8</td>
<td>86.5</td>
<td>84.1</td>
<td>84.3</td>
<td>80.1</td>
<td>82.1</td>
<td>82.2</td>
<td>77.9</td>
<td>80</td>
</tr>
<tr>
<td>Average</td>
<td>73.7</td>
<td>63.9</td>
<td>68.7</td>
<td>82.6(8.9)</td>
<td>88.3(24.4)</td>
<td>84.1(15.4)</td>
<td>82.6(8.9)</td>
<td>81.3(17.4)</td>
<td>81.9(13.2)</td>
<td>82.5(8.8)</td>
<td>81.9(18)</td>
<td>82.1(13.4)</td>
</tr>
</tbody>
</table>
<p>TABLE IV
THE RESULTS OF MARA ON MODELING USE CASE DIAGRAM</p>
<table>
<thead>
<tr>
<th>Cases</th>
<th>IT4RE</th>
<th></th>
<th></th>
<th>MARE(gpt-3.5-turbo)</th>
<th></th>
<th></th>
<th>MARE(text-davinci-002)</th>
<th></th>
<th></th>
<th>MARE(text-davinci-003)</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$\mathrm{P}(\%)$</td>
<td>$\mathrm{R}(\%)$</td>
<td>$\mathrm{F} 1(\%)$</td>
<td>$\mathrm{P}(\%)$</td>
<td>$\mathrm{R}(\%)$</td>
<td>$\mathrm{F} 1(\%)$</td>
<td>$\mathrm{P}(\%)$</td>
<td>$\mathrm{R}(\%)$</td>
<td>$\mathrm{F} 1(\%)$</td>
<td>$\mathrm{P}(\%)$</td>
<td>$\mathrm{R}(\%)$</td>
<td>$\mathrm{F} 1(\%)$</td>
</tr>
<tr>
<td>ATM</td>
<td>50</td>
<td>83</td>
<td>63</td>
<td>72.9</td>
<td>83.3</td>
<td>77.8</td>
<td>75.6</td>
<td>83.3</td>
<td>79.2</td>
<td>76.7</td>
<td>72.7</td>
<td>74.6</td>
</tr>
<tr>
<td>COS</td>
<td>63</td>
<td>56</td>
<td>59</td>
<td>79.2</td>
<td>80</td>
<td>79.6</td>
<td>72.7</td>
<td>79.6</td>
<td>76</td>
<td>75.7</td>
<td>81.8</td>
<td>78.6</td>
</tr>
<tr>
<td>TLS</td>
<td>50</td>
<td>56</td>
<td>53</td>
<td>78.3</td>
<td>77.8</td>
<td>78</td>
<td>74.8</td>
<td>74.8</td>
<td>74.8</td>
<td>71.4</td>
<td>78.9</td>
<td>75</td>
</tr>
<tr>
<td>TAS</td>
<td>17</td>
<td>100</td>
<td>29</td>
<td>74.9</td>
<td>82.8</td>
<td>78.7</td>
<td>72.3</td>
<td>78.7</td>
<td>75.3</td>
<td>77.8</td>
<td>81.1</td>
<td>79.4</td>
</tr>
<tr>
<td>TMS</td>
<td>56</td>
<td>100</td>
<td>71</td>
<td>81.6</td>
<td>79.1</td>
<td>80.3</td>
<td>80.7</td>
<td>77.6</td>
<td>79.1</td>
<td>82.2</td>
<td>77.9</td>
<td>80</td>
</tr>
<tr>
<td>Average</td>
<td>47.2</td>
<td>79</td>
<td>55</td>
<td>77.4(30.2)</td>
<td>80.6(1.6)</td>
<td>78.9(23.9)</td>
<td>75.2(28.0)</td>
<td>78.8(0.2)</td>
<td>76.8(21.8)</td>
<td>76.8(29.6)</td>
<td>78.5(0.5)</td>
<td>77.5(22.5)</td>
</tr>
</tbody>
</table>
<p>TABLE V
THE RESULTS OF MARE ON MODELING GOAL MODEL</p>
<table>
<thead>
<tr>
<th>Datasets</th>
<th>Extracted Elements</th>
<th>HAGM</th>
<th></th>
<th></th>
<th>MARE(gpt-3.5-turbo)</th>
<th></th>
<th></th>
<th>MARE(text-davinci-002)</th>
<th></th>
<th></th>
<th>MARE(text-davinci-003)</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>$\mathrm{P}(\%)$</td>
<td>$\mathrm{R}(\%)$</td>
<td>$\mathrm{F} 1(\%)$</td>
<td>$\mathrm{P}(\%)$</td>
<td>$\mathrm{R}(\%)$</td>
<td>$\mathrm{F} 1(\%)$</td>
<td>$\mathrm{P}(\%)$</td>
<td>$\mathrm{R}(\%)$</td>
<td>$\mathrm{F} 1(\%)$</td>
<td>$\mathrm{P}(\%)$</td>
<td>$\mathrm{R}(\%)$</td>
</tr>
<tr>
<td>GoalModelDataset</td>
<td>Role</td>
<td>80.8</td>
<td>83.9</td>
<td>82.3</td>
<td>81.3</td>
<td>84.3</td>
<td>82.7</td>
<td>81.1</td>
<td>83.4</td>
<td>82.3</td>
<td>82.3</td>
<td>83.5</td>
<td>82.9</td>
</tr>
<tr>
<td></td>
<td>Action</td>
<td>93.2</td>
<td>90.4</td>
<td>91.7</td>
<td>93.5</td>
<td>91.4</td>
<td>92.4</td>
<td>92.5</td>
<td>91.2</td>
<td>91.8</td>
<td>92.4</td>
<td>91.2</td>
<td>91.8</td>
</tr>
<tr>
<td>Average</td>
<td></td>
<td>87</td>
<td>87.2</td>
<td>87</td>
<td>87.4</td>
<td>87.9</td>
<td>87.6</td>
<td>86.8</td>
<td>87.3</td>
<td>87.1</td>
<td>87.4</td>
<td>87.4</td>
<td>87.4</td>
</tr>
</tbody>
</table>
<p>as 1 , frequency_penalty as 0 , presence_penalty as 0 , best_of as 1 . These are the hyper-parameters of the LLMs.</p>
<p>For requirements modeling, the experiments include generating modeling elements in problem diagrams, use case diagrams and goal models. To be specific, for the problem diagrams, we use MARE to generate the the machine domains, the requirements domains, the physical devices, the sharing phenomenon, and the requirements references. For the use case diagram, MARE is to generate the actors and the use cases. For the goal model, MARE is used to generate the roles and the agents.</p>
<h2>IV. RESULTS AND ANALYSES</h2>
<p>In our first research question, we evaluate the performance of the requirements modeling by MARE compared with the previous automated requirements modeling approaches.</p>
<p>RQ1:How do the requirements models generated by MARE compared to the SOTA approaches?</p>
<p>Setup. We evaluate baselines (Section III-D) and MARE on 9 evaluation cases and 1 public evaluation dataset (Section III-B). The evaluation metrics are described in Section III-C, i.e., the Precision(P), Recall(R), and F1. For all metrics, higher scores represent better performance. Given the name of each evaluation case, MARE can generate modeling elements. We compute these three metrics based on these modeling elements.</p>
<p>Results. Table III, Table IV and Table V show the experimental results of the problem diagrams, the use case diagrams, and the goal models compared with three baselines. Red indicates improved, and green indicates decreased.</p>
<p>Analyses. MARE achieves the best results among all baselines. For modeling problem diagrams, MARE(gpt-3.5-turbo) improves the SOTA approach, i.e., EPD, by $8.9 \%$ for average precision, by $24.4 \%$ for average recall and by $15.4 \%$ for average F1. Similarly, for modeling use case diagrams, MARE(gpt-3.5) outperforms the SOTA approach, i.e., IT4RE, by $30.1 \%$ for average precision, $1.6 \%$ for average recall, and $23.8 \%$ for average F1. For modeling goal models, MARE(gpt3.5) improves the SOTA approach, i.e., HAGM, by $0.4 \%$ for average precision, $0.7 \%$ for average recall, and $0.6 \%$ for average F1. These results indicate MARE can more accurately generate modeling elements in various requirements models.</p>
<p>Answer to RQ1: MARE achieves the best results among all baselines. The average F1 score of MARE is $84.1 \%$, $78.9 \%$ and $87.6 \%$ on three requirement model, improving their SOTA approach by $15.4 \%, 23.8 \%$ and $0.6 \%$</p>
<p>In RQ2, we aim to evaluate the performance of requirement specification generated by MARE.</p>
<p>RQ2: How do the requirements specifications generated by MARE?</p>
<p>Setup. We evaluate MARE on 9 evaluation cases (Section III-B). Given the name of these cases, we collect the requirements specifications generated by MARE. To guarantee the correctness of the evaluation, we built an inspection team, which consisted of three master students. All of them are fluent English speakers and have done intensive research work with RE. Each student scored each of the 9 requirements specifications from three aspects (Section III-C). Finally, the</p>
<p>TABLE VI
THE RESULTS OF ABLATION STUDY</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Strategies</th>
<th style="text-align: center;">The ATM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">The Cafeteria</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">The Library</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">The Asemantic</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">The Time Monitor</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">Individual LLM</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">78.2</td>
</tr>
<tr>
<td style="text-align: center;">MARE</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">80.3</td>
</tr>
</tbody>
</table>
<p>TABLE VII
THE SCORE OF REQUIREMENT SPECIFICATION</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Evaluation Cases</th>
<th style="text-align: center;">completeness</th>
<th style="text-align: center;">correctness</th>
<th style="text-align: center;">consistency</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TSHCS</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">1.53</td>
<td style="text-align: center;">1.54</td>
</tr>
<tr>
<td style="text-align: center;">TTCS</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">1.62</td>
<td style="text-align: center;">1.66</td>
</tr>
<tr>
<td style="text-align: center;">TACCS</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">1.69</td>
<td style="text-align: center;">1.87</td>
</tr>
<tr>
<td style="text-align: center;">THCS</td>
<td style="text-align: center;">1.04</td>
<td style="text-align: center;">1.71</td>
<td style="text-align: center;">1.74</td>
</tr>
<tr>
<td style="text-align: center;">ATM</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">1.66</td>
<td style="text-align: center;">1.95</td>
</tr>
<tr>
<td style="text-align: center;">COS</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">1.74</td>
<td style="text-align: center;">1.71</td>
</tr>
<tr>
<td style="text-align: center;">TLS</td>
<td style="text-align: center;">1.02</td>
<td style="text-align: center;">1.81</td>
<td style="text-align: center;">1.82</td>
</tr>
<tr>
<td style="text-align: center;">TAS</td>
<td style="text-align: center;">1.06</td>
<td style="text-align: center;">1.85</td>
<td style="text-align: center;">1.77</td>
</tr>
<tr>
<td style="text-align: center;">TMS</td>
<td style="text-align: center;">1.21</td>
<td style="text-align: center;">1.77</td>
<td style="text-align: center;">1.87</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">1.92</td>
<td style="text-align: center;">1.98</td>
</tr>
</tbody>
</table>
<p>average score was calculated.
Results. Table IV shows the results of evaluation for requirements specifications generated by MARE (gpt-3.5-turbo).</p>
<p>Analyses. MARE can generate requirement specifications for various software systems. For the 9 various software systems, the completeness score of the generated specification is between 0.78 and 1.21 . The correctness score of them is between 1.53 and 1.85 . Similarly, the consistency score of them is between 1.54 and 1.95. For average score, MARE achieves 0.98 for completeness, 1.92 for correctness, and 1.98 for consistency. These results imply that MARE can effectively generate requirements specifications. We also notice that the completeness aspect is slightly lower than the correctness and consistency. By investigating the causes, we believe that there are two reasons. First, the reason why evaluators give lower scores for the completeness is that they think each section of the generated requirement specifications is insufficient. Second, the length of text is reduced by LLMs when generating long text.</p>
<p>Answer to RQ2: MARE can effectively generates requirement specification. To be specific, the average score of MARE is $0.98,1.92$ and 1.98 on the three aspects.</p>
<h2>RQ3:How does MARE perform compared to individual</h2>
<p>LLM on requirements modeling?</p>
<p>Setup. We compare MARE with an individual LLM to generate requirements models on 5 public cases. For MARE, we compute three metrics (Section III-C) based on extracted requirements modeling elements from the generated requirements drafts. For the individual LLM, we directly use one LLM to extract requirements modeling elements from the requirements drafts generated by MARE and computer the same metrics. Besides, we use gpt-3.5-turbo as the individual LLM and select the use case diagrams.</p>
<p>Results and Analyses. Table IV shows the comparative results between MARE and the individual LLM on 5 public
cases. MARE can generate more correct requirements modeling elements compared with the individual LLM on the 5 public cases. To be specific, MARE outperforms the individual LLM by $0.2 \%$ for average precision, $2 \%$ for average recall and $1.1 \%$ for average F1.</p>
<p>Answer to RQ3: MARE achieves better results comparing with the individual LLM. This demonstrates the superiority of collaboration of multiple agents.</p>
<h2>V. Related Works</h2>
<p>Deep-Learning-based Requirements Engineering. With the rapid development of the deep learning technique, many researchers are devoted to applying DL to RE to improve the effectiveness of software development [32] [33] [34]. For Requirement elicitation task, recent work has more focus on mining user needs [35] from the open source community [36] and facilitating conversations [37]. For Requirement modeling task, some researchers use pre-defined rules to extract modeling entities and relations [38]. Others focus on building neural network models [39] [40] to extract the requirements models. Recent works also have attempted to explore the ability of LLMs for requirements modeling [12]. For Requirement verification task, [41] designed a tool to detect smell in requirements requests based on rules. Besides, many works focus on detecting ambiguous and inconsistent [33] requirements. Compared to these existing studies, our work leverages LLMs throughout the entire requirements engineering through collaboration, rather than just dealing with a single task in RE.</p>
<p>LLM-Based Multi-Agent Frameworks. LLMs have shown remarkable performance across a wide range of domains. [42] improves the reasoning accuracy by leveraging multi-agent debate. [18] proposed ChatDev, a virtual chatpowered software development company consisting of multiple agents based on LLMs. [19] introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. Compared to these studies, our work focuses on tasks in RE.</p>
<h2>VI. CONCLUSION</h2>
<p>In this paper, we propose MARE, a multi-agent collaboration framework based on LLMs, to improve the efficiency of processing requirements engineering tasks. In MARE, different agents perform requirements elicitation, requirements modeling, requirements verification, and requirements specification, respectively, and collaborate to generate the requirement specification. When conducting these tasks, some external knowledge has been used, e.g. The Modeler extracts the requirements models according to the 'Meta model'; the</p>
<p>Checker checks the models based on 'Accept criteria'; and the Documenter produces the specification in terms of the 'Template'. MARE can contribute to multiple LLMs (such as gpt-3.5-turbo). We demonstrate the superiority of MARE's requirements modeling capabilities by comparing three SOTA automated requirements modeling approaches. Besides, we have provided insights into the quality of the requirements specification generated by MARE.</p>
<h2>REFERENCES</h2>
<p>[1] C. Arora, J. Grundy, and M. Abdelrazek, "Advancing requirements engineering through generative AI: assessing the role of llms," CoRR, vol. abs/2310.13976, 2023.
[2] J. Feng, W. Miao, H. Zheng, Y. Huang, J. Li, Z. Wang, T. Su, B. Gu, G. Pu, M. Yang, and J. He, "FREPA: an automated and formal approach to requirement modeling and analysis in aircraft control domain," in 28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2020, pp. 1376-1386.
[3] M. Naumcheva, "Deep learning models in software requirements engineering," CoRR, vol. abs/2105.07771, 2021. [Online]. Available: https://arxiv.org/abs/2105.07771
[4] L. Shi, C. Chen, Q. Wang, and B. W. Boehm, "Automatically detecting feature requests from development emails by leveraging semantic sequence mining," Requir. Eng., vol. 26, no. 2, pp. 255-271, 2021. [Online]. Available: https://doi.org/10.1007/s00766-020-00344-y
[5] L. Shi, M. Xing, M. Li, Y. Wang, S. Li, and Q. Wang, "Detection of hidden feature requests from massive chat messages via deep siamese network," in 42nd International Conference on Software Engineering, 2020, pp. 641-653.
[6] D. Jin, C. Wang, and Z. Jin, "Automating extraction of problem diagrams from natural language requirement documents," in 31st IEEE International Requirements Engineering Conference Workshops, 2023, pp. 199-204.
[7] C. Wang, L. Hou, and X. Chen, "Extracting requirements models from natural-language document for embedded systems," in 30th IEEE International Requirements Engineering Conference Workshops, 2022, pp. 18-21.
[8] S. Ezzini, S. Abualhaija, C. Arora, M. Sabetzadeh, and L. C. Briand, "Using domain-specific corpora for improved handling of ambiguity in requirements," in 43rd IEEE/ACM International Conference on Software Engineering, 2021, pp. 1485-1497.
[9] X. Hou, Y. Zhao, Y. Liu, Z. Yang, K. Wang, L. Li, X. Luo, D. Lo, J. C. Grundy, and H. Wang, "Large language models for software engineering: A systematic literature review," CoRR, vol. abs/2308.10620, 2023.
[10] D. Xu, W. Chen, W. Peng, C. Zhang, T. Xu, X. Zhao, X. Wu, Y. Zheng, and E. Chen, "Large language models for generative information extraction: A survey," CoRR, vol. abs/2312.17617, 2023.
[11] J. White, S. Hays, Q. Fu, J. Spencer-Smith, and D. C. Schmidt, "Chatgpt prompt patterns for improving code quality, refactoring, requirements elicitation, and software design," CoRR, vol. abs/2303.07839, 2023.
[12] K. Ruan, X. Chen, and Z. Jin, "Requirements modeling aided by chatgpt: An experience in embedded systems," in 31st IEEE International Requirements Engineering Conference Workshops, 2023, pp. 170-177.
[13] B. Görer and F. B. Aydemir, "Generating requirements elicitation interview scripts with large language models," in 31st IEEE International Requirements Engineering Conference, RE 2023 - Workshops, Hannover, Germany, September 4-5, 2023, 2023, pp. 44-51.
[14] A. Fantechi, S. Gnesi, L. C. Passaro, and L. Semini, "Inconsistency detection in natural language requirements using chatgpt: a preliminary evaluation," in 31st IEEE International Requirements Engineering Conference Workshop, 2023, pp. 335-340.
[15] A. D. Rodriguez, K. R. Dearstyne, and J. Cleland-Huang, "Prompts matter: Insights and strategies for prompt engineering in automated software traceability," in 31st IEEE International Requirements Engineering Conference Workshops, 2023, pp. 455-464.
[16] J. R. Katzenbach and D. K. Smith, The wisdom of teams: Creating the high-performance organization. Harvard Business Review Press, 2015.
[17] W. Chen, Y. Su, J. Zuo, C. Yang, C. Yuan, C. Qian, C. Chan, Y. Qin, Y. Lu, R. Xie, Z. Liu, M. Sun, and J. Zhou, "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents," CoRR, vol. abs/2308.10848, 2023.
[18] C. Qian, X. Cong, C. Yang, W. Chen, Y. Su, J. Xu, Z. Liu, and M. Sun, "Communicative agents for software development," CoRR, vol. abs/2307.07924, 2023.
[19] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, C. Ran, L. Xiao, and C. Wu, "Metagpt: Meta programming for multi-agent collaborative framework," CoRR, vol. abs/2308.00352, 2023.
[20] Github. Supported materials. [Online]. Available: https://github.com/ publicsubmission/MARE-support-material
[21] S. Vinay, S. Aithal, and P. Desai, "An approach towards automation of requirements analysis," in Proceedings of the International MultiConference of Engineers and Computer Scientists, vol. 1, 2009.
[22] A. Umber, I. S. Bajwa, and M. A. Naeem, "Nl-based automated software requirements elicitation and specification," in Advances in Computing and Communications - First International Conference, ser. Communications in Computer and Information Science, vol. 191, 2011, pp. 30-39.
[23] I. S. Bajwa and M. A. Choudhary, "From natural language software specifications to UML class models," in Enterprise Information Systems - 13th International Conference, vol. 102, pp. 224-237.
[24] I. S. Bajwa and M. A. Naeem, "On specifying requirements using a semantically controlled representation," in Natural Language Processing and Information Systems - 16th International Conference on Applications of Natural Language to Information Systems, vol. 6716, 2011, pp. $217-220$.
[25] A. Umber and I. S. Bajwa, "A step towards ambiguity less natural language software requirements specifications," Int. J. Web Appl., vol. 4, no. 1, pp. 12-21, 2012.
[26] Q. Zhou, T. Li, and Y. Wang, "Assisting in requirements goal modeling: a hybrid approach based on machine learning and logical reasoning," in Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems, 2022, pp. 199-209.
[27] A. Ferrari, G. O. Spagnolo, and S. Gnesi, "PURE: A dataset of public requirements documents," in 25th IEEE International Requirements Engineering Conference, 2017, pp. 502-505.
[28] D. Xie, B. Yoo, N. Jiang, M. Kim, L. Tan, X. Zhang, and J. S. Lee, "Impact of large language models on generating software specifications," CoRR, vol. abs/2306.03324, 2023.
[29] A. Alhroob, A. T. Imam, and R. Al-Heisa, "The use of artificial neural networks for extracting actions and actors from requirements document," Inf. Softw. Technol., vol. 101, pp. 1-15, 2018.
[30] DeepWisdom. Metagpt framework. [Online]. Available: https://github. com/geekan/MetaGPT/
[31] OpenAI. Chatgpt. [Online]. Available: https://platform.openai.com/docs/ models/
[32] V. Seibert, "Towards dialogue based, computer aided software requirements elicitation," CoRR, vol. abs/2310.13953, 2023.
[33] S. Ezzini, S. Abualhaija, C. Arora, and M. Sabetzadeh, "Automated handling of anaphoric ambiguity in requirements: A multi-solution study," in 44th IEEE/ACM 44th International Conference on Software Engineering, 2022, pp. 187-199.
[34] W. Alhoshan, A. Ferrari, and L. Zhao, "Zero-shot learning for requirements classification: An exploratory study," Inf. Softw. Technol., vol. 159, p. 107202, 2023.
[35] Y. Wang, J. Wang, H. Zhang, X. Ming, L. Shi, and Q. Wang, "Where is your app frustrating users?" in 44th International Conference on Software Engineering, 2022, pp. 2427-2439.
[36] Google. Gitter. [Online]. Available: https://gitter.im/
[37] R. Dickler, S. Dudy, A. Mawasi, J. Whitehill, A. Benson, and A. Corbitt, "Interdisciplinary approaches to getting AI experts and education stakeholders talking," in Artificial Intelligence in Education. Posters and Late Breaking Results, ser. Lecture Notes in Computer Science, vol. 13356, 2022, pp. 115-118.
[38] M. Elbendak, P. Vickers, and B. N. Rossiter, "Parsed use case descriptions as a basis for object-oriented class model generation," J. Syst. Softw., vol. 84, no. 7, pp. 1209-1223, 2011.
[39] M. Li, Y. Yang, L. Shi, Q. Wang, J. Hu, X. Peng, W. Liao, and G. Pi, "Automated extraction of requirement entities by leveraging LSTM-CRF and transfer learning," in IEEE International Conference on Software Maintenance and Evolution, 2020, pp. 208-219.
[40] J. Devlin, M. Chang, K. Lee, and K. Toutanova, "BERT: pre-training of deep bidirectional transformers for language understanding," in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics, 2019, pp. 4171-4186.</p>
<p>[41] F. Mu, L. Shi, W. Zhou, Y. Zhang, and H. Zhao, "NERO: A text-based tool for content annotation and detection of smells in feature requests," in 28th IEEE International Requirements Engineering Conference, 2020, pp. 400-403.
[42] Y. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch, "Improving factuality and reasoning in language models through multiagent debate," CoRR, vol. abs/2305.14325, 2023.</p>            </div>
        </div>

    </div>
</body>
</html>
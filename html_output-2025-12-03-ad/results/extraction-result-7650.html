<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7650 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7650</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7650</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-267028523</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.03266v2.pdf" target="_blank">UniPredict: Large Language Models are Universal Tabular Classifiers</a></p>
                <p><strong>Paper Abstract:</strong> Tabular data prediction is a fundamental machine learning task for many applications. Existing methods predominantly employ discriminative modeling and operate under the assumption of a fixed target column, necessitating re-training for every new predictive task. Inspired by the generative power of large language models (LLMs), this paper exploits the idea of building universal tabular data predictors based on generative modeling, namely UniPredict. Here, we demonstrate the scalability of an LLM to extensive tabular datasets, enabling it to comprehend diverse tabular inputs and predict target variables following the provided instructions. Specifically, we train a single LLM on an aggregation of 169 tabular datasets with diverse targets and compare its performance against baselines that are trained on each dataset separately. We observe this versatile UniPredict model demonstrates an advantage over other models, ranging from 5.4% to 13.4%, when compared with the best tree-boosting baseline and the best neural network baseline, respectively. We further test UniPredict in few-shot learning settings on another 62 tabular datasets. Our method achieves strong performance in quickly adapting to new tasks. In low-resource few-shot setup, we observed a 100%+ performance advantage compared with XGBoost, and significant margin over all baselines. We envision that UniPredict sheds light on developing a universal tabular data prediction system that learns from data at scale and serves a wide range of prediction tasks.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7650",
    "paper_id": "paper-267028523",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00430775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>UniPredict: Large Language Models are Universal Tabular Classifiers
16 Jan 2024</p>
<p>Ruiyu Wang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#114;&#119;&#97;&#110;&#103;&#64;&#99;&#115;&#46;&#116;&#111;&#114;&#111;&#110;&#116;&#111;&#46;&#101;&#100;&#117;">&#114;&#119;&#97;&#110;&#103;&#64;&#99;&#115;&#46;&#116;&#111;&#114;&#111;&#110;&#116;&#111;&#46;&#101;&#100;&#117;</a> 
Department of Computer Science
Univer-sity of Toronto</p>
<p>Zifeng Wang 
University of Illinois Urbana-Champaign</p>
<p>Jimeng Sun 
University of Illinois Urbana-Champaign</p>
<p>UniPredict: Large Language Models are Universal Tabular Classifiers
16 Jan 2024C8C21A2CFED7D05505400E732D0E9615arXiv:2310.03266v2[cs.LG]
Tabular data prediction is a fundamental machine learning task for many applications.Existing methods predominantly employ discriminative modeling and operate under the assumption of a fixed target column, necessitating re-training for every new predictive task.Inspired by the generative power of large language models (LLMs), this paper exploits the idea of building universal tabular data predictors based on generative modeling, namely UniPredict.Here, we demonstrate the scalability of an LLM to extensive tabular datasets, enabling it to comprehend diverse tabular inputs and predict target variables following the provided instructions.Specifically, we train a single LLM on an aggregation of 169 tabular datasets with diverse targets and compare its performance against baselines that are trained on each dataset separately.We observe this versatile UniPredict model demonstrates an advantage over other models, ranging from 5.4% to 13.4%, when compared with the best tree-boosting baseline and the best neural network baseline, respectively.We further test UniPredict in few-shot learning settings on another 62 tabular datasets.Our method achieves strong performance in quickly adapting to new tasks.In lowresource few-shot setup, we observed a 100%+ performance advantage compared with XGBoost, and significant margin over all baselines.We envision that UniPredict sheds light on developing a universal tabular data prediction system that learns from data at scale and serves a wide range of prediction tasks.</p>
<p>Introduction</p>
<p>Tabular data is organized in a tabular or spreadsheet format within a relational database.Each row within the table corresponds to a specific data sample, and the columns encompass a range of feature variables with diverse types, such as categorical, numerical, binary, and textual features.Tabular data prediction is fundamental to many real-world machine-learning applications such as click-through rate prediction (Yang &amp; Zhai, 2022) and medical outcome prediction (Wang &amp; Sun, 2022).</p>
<p>Nonetheless, most previous methods fall short by assuming a fixed target.This entails selecting a specific column, such as patient mortality in breast cancer cases, with the other columns as the input features.Therefore, a model trained to predict this particular target becomes specialized and cannot be employed for predicting any other target, such as cancer relapse.To predict a different target, one must create a new dataset corresponding to the desired target and retrain the model.This practice renders substantial work involved in developing and hosting dataset-specific tabular data predictors.</p>
<p>Unlike most traditional algorithms that make discriminative modeling for tabular prediction, we intend to harness LLMs for tabular prediction through generative modeling.Figure 1 demonstrates the difference between the previous practices and our modeling paradigm.This paradigm provides substantial flexibility in (1) processing natural language descriptions of tabular data and (2) generating predictions for specified target labels based on input instructions.While previous works have tried to fine-tune LLMs for generating target labels of tabular data (Dinh et al., 2022;Hegselmann et al., 2023), they have their limitations, mainly in that they still require training specific predictors for each dataset and target variable.Moreover, these generative prediction methods do not provide the associated confidence of their predictions as traditional tabular prediction models do.By contrast, the goal of this work is to build universal tabular predictors based on generative LLM, which accept arbitrary inputs and predict for arbitrary targets, following the input instructions.</p>
<p>Specifically, this work explores the ways to unlock the potential of LLMs as universal tabular data predictors, namely UniPredict, which hinges on the following insights:  1a), distinct models are trained individually on each dataset, making them incapable of adaptation to new datasets with differing features and targets.Middle: In the In-Domain Tabular Modeling tasks (Figure 1b), where flexibility is allowed for features, the targets remain the same across datasets.Right: the proposed Universal Tabular Modeling paradigm (Figure 1c), which accommodates arbitrary inputs and predicting for arbitrary targets.This paradigm does not impose any restrictions on the domains of the datasets used.In Universal Tabular Modeling, the datasets can originate from entirely different domains.</p>
<p>• Data Scale: Scaling to 160+ diverse tabular datasets to fuel the training of a powerful LLM that performs prediction for diverse inputs and targets.</p>
<p>• Prompt Engineering: The prompts that integrate the metadata (e.g., the dataset description and schema of columns), the input tabular sample, and the instruction for prediction generation.</p>
<p>• Instruction Tuning: Instruction tuning that encourages LLM to not only generate the label but also provide confidence estimates for its predictions.</p>
<p>We elaborate on our framework in Section 2, which is followed by the experiment results in Section 3. In detail, we train a single UniPredict model on the aggregated training sets from 169 tabular datasets and test it on the corresponding test sets.For comparison, we train one unique baseline model for each tabular dataset and report their performances.We observe that the universal tabular predictor UniPredict outperforms the best neural network baselines by 13.4% and the best boosting algorithms by 5.4%, across the test sets.Additionally, we observed that UniPredict exhibits an advantage in the low-resource regime.Even as the sample size increases, it consistently maintains among the top models.We close with the discussion of related papers in Section 4 and the conclusion in Section 5.</p>
<p>Method and Implementation</p>
<p>Problem Formulation</p>
<p>Before going into details of the proposed method, we define two problems that we aim to resolve:</p>
<p>Universal Tabular Modeling Given a dataset D n in any domain, we have its components D n = {M n , S n ; T n } that include the metadata M n , samples S n , and targets T n .Different from traditional tabular models f n : S n → T n (shown in Figure 1a) that gives a 1-to-1 dataset-model relationship, or in-domain tabular models f task : S n → T task (shown in Figure 1b), we require a universal model
f univ : S → T such that f univ (S n ; M n ) = T n .
This approach enables us to create a more versatile prediction setting.The model parameters are no longer dependent on any particular dataset or task domain.Instead, a single set of parameters, with the aid of metadata, can be used for all datasets from any domain (shown in Figure 1c).</p>
<p>Few-shot Learning</p>
<p>We expect our model f that is trained on datasets {D 1 , D 2 , • • • D n } to be also available to predict for a new target T n+1 , given (S n+1 , M n+1 ) ∈ D n+1 .We can fine-tune f with the new dataset D n+1 in a low-resource regime to achieve few-shot learning.</p>
<p>As illustrated in Figure 2, The UniPredict framework is structured around three primary steps: First, in Prompt Setup §2.2, prompts are established through metadata, sam-</p>
<p>Framework</p>
<p>Step 1: Prompt Setup</p>
<p>Step 2: Target Augmentation ple serialization, and instructions.Second, Target Augmentation §2.3 involves transforming target values into categorized forms accompanied by confidence estimates.Last, the Learning §2.4 step fine-tunes the backbone model utilizing the prompts and targets derived from the preceding procedures.</p>
<p>Prompt Engineering</p>
<p>Tabular data have to be transformed into natural language inputs to be comprehended by LLMs.It is highlighted that the quality of this natural language input has a major impact on the LLM's performance (Zhao et al., 2021).We hereby present how we formulate the input prompt for our UniPredict framework.Technically, based on dataset D = {M, S; T} we define the function prompt( M, Ŝ, I) that takes pre-processed metadata M and tabular sample Ŝ, and the instruction I as input and perform serialization to produce the natural language input for LLMs:</p>
<p>Metadata M represents a serialized description of the context and schema definition of the dataset.</p>
<p>Tabular Sample Ŝ that represents serialized contents of the raw sample.</p>
<p>Instruction I that contains the guidance that prompts LLMs to make the final prediction about the target, e.g., the probability prediction for each target class.</p>
<p>We describe the detailed setup of these components in the following sections.We also offer the example of used prompts in Appendix A.1.</p>
<p>Metadata Re-formatting As UniPredict accommodates a wide range of tabular datasets that share distinct schema, the dataset metadata plays a vital role in facilitating the language modeling on these diverse tabular data.For instance, many table columns are abbreviations or coded with a private dictionary, thus hurdling LLMs in comprehending the tabular inputs.In practice, the metadata is usually provided in unstructured texts with the raw dataset.Here, we propose to design a function reformat(M) that consolidates arbitrary input M to (1) a description of the target to predict and (2) the semantic descriptions of features.We employ GPT-3.51 to automate the metadata reformatting process.We offer the example metadata reformatting process in Appendix A.2.</p>
<p>Feature Serialization Given the raw metadata M and the samples S, we define the function serialize(c, v) to produce a str output given the column names c and feature values v, where c ∈ reformat(M) and v ∈ S. Each value is paired with the corresponding column in the format of "{column} is {value}, {column} is {value}, ...".Besides, we round numeric values to a fixed precision before tokenization, and more data-dependent binning methods, such as adaptive histogram binning, may be considered.Some examples of the serialization can be found in Appendix A.3.</p>
<p>Submission and Formatting Instructions for ICML 2024</p>
<p>Instruction Formulation &amp; Target Augmentation</p>
<p>When encountering tabular data prediction with LLM, the most natural idea is to put the tabular sample as the input and prompt LLM to generate the target label (Dinh et al., 2022;Hegselmann et al., 2023).For instance, prompting LLM with the input "Is the person's annual income ≥ 50?" to yield the output "yes" or "no" as the binary prediction.However, it has two main drawbacks:</p>
<p>• Reliability Unlike traditional ML algorithms that produce the probability prediction for each class, this method merely produces the output label.Due to the uncertainty in text generation, the label prediction from LLM may be unreliable without a numerical estimation of its confidence.</p>
<p>• Robustness We empirically identified this modeling paradigm may fail to converge when encountering challenging tabular prediction tasks or noisy inputs.In these scenarios, the LLM may either refuse to generate predictions or tend to continue the input texts.</p>
<p>To overcome these challenges, we propose instructing models to predict each target class probability, e.g., "yes: 0.8; no: 0.2".This is achieved by adding another target augmentation step.</p>
<p>Target Augmentation We transform the target label into a set of probabilities for each class via a function called "augment".Formally, for target T in an arbitrary dataset D, we define a function augment(T) = {C, P}, where C are new categories of targets with semantic meaning and P are the assigned probabilities to each category.We extend the target into categorical one-hot encoding and then use an external predictor to create the calibrated probability distributions.This replaces the 0/1 one-hot encoding while maintaining the final prediction outcome.For datasets with discrete target values (e.g., classification), the target classes are processed by one-hot encoding.For continuous numerical targets (e.g., regression), the categories are defined by their quantiles.</p>
<p>We use an isotopic calibrated XGBoost classifier (Chen &amp; Guestrin, 2016) with n estimators=100 as the external predictor.We train one predictor for each dataset and then leverage it to produce the probability for each class for all samples.It is noted that this predictor serves as a probability estimator for sample labels without the loss of information or data leakage.Formally, given the target classes t ∈ {0, ..., |C|} and target probabilities p ∈ P, we define a function serialize target(t, p) that serializes target classes and probabilities into a sequence formatted as "class {t 1 } : {p 1 }, class {t 2 } : {p 2 }, . . .".This sequence is used as the referenced output to fine-tune the LLM.Besides the merit of entailing confidence predictions, target augmentation offers more sufficient supervision for LLMs, which we find vital for its robustness in training and inference.</p>
<p>Instruction Formulation The instruction I describes the objective that prompts LLM to comprehend the input tabular sample and predict for the augmented target augment(T).Given the target classes t ∈ [0, |C|] and target semantic explanation e ∈ C, we define a function serialize class(t, e) that converts the classes t, and their corresponding semantic explanation e, into a natural language sequence "class {t} means {e}, . . .".We present the example prompts in Appendix A.4.</p>
<p>Learning</p>
<p>LLM for Tabular Prediction During fine-tuning, our objective is to minimize the difference between the output sequence generated by the adapted LLM function (represented by LLM(prompt( M, Ŝ, I))) and the reference output sequence generated from target augmentation (represented by serialize target(augment(T))).However, during testing, we evaluate the prediction correctness instead of the similarity between the output and reference sequences.To do this, we map the natural language sequence generated by the LLM function to the actual class that the model is predicting.</p>
<p>We then check the correctness of the prediction by comparing it with the ground truth label.We use a regex expression matching technique for the mapping procedure.We have included examples for such comparisons in Appendix A.5.</p>
<p>Learning In our model learning process, we generate prompts using samples and metadata from different datasets and update the model based on instruction fine-tuning.Subsequently, we assess the model's actual performance by comparing its class predictions (after output mapping) to the original target values.This evaluation is conducted on both the datasets used during training and previously unseen datasets.We adapt GPT-2 (Radford et al., 2019) as our backbone, and we used the huggingface2 package for training.See Appendix B.3 for the detail of parameter choice.</p>
<p>Our Implementation of UniPredict</p>
<p>Dataset Setup We collect the datasets from Kaggle 3 .We pre-select the datasets from the classification category and drop the datasets that do not provide organized and recognizable metadata.We leverage the Kaggle API 4 to download both the raw data and their descriptions with an argument --file-size csv to restrict the dataset format.In this way, we simplify the follow-up dataset reading procedures.To ensure a comprehensive evaluation, we do not preselect datasets by their domains, categories, or purposes.</p>
<p>We end with the training corpus built from 169 datasets.For each selected dataset, we perform a max-size cutoff at 7500 samples to prevent any datasets with too many samples from dominating the corpus.The number of training samples in the entire corpus is 366,786.Dataset statistics can be found in Appendix B.2.</p>
<p>Implementations</p>
<p>The target augmentation step is done by the XGBoost classifiers.However, as mentioned in Section 2.3, we accept other classifiers to be adapted as long as they produce proper probability values.Furthermore, measuring the information entailed by different classifiers in this problem is also a potential topic to explore.</p>
<p>We utilize a GPT-2 (Radford et al., 2019) model as backbone.Besides the normal UniPredict framework, we instantiate a variant that only takes feature names from the metadata, named as UniPredict-light; in contrast, we named our normal version UniPredict-heavy.UniPredict-light is expected to take less time for fine-tuning and demonstrate an equal or better performance when the dataset is well-maintained.Since no assumptions should be made to unknown datasets, UniPredict-heavy is the most reliable baseline.The difference in implementation of the two variants can be found in Appendix A.1.</p>
<p>Experiment</p>
<p>In this section, we conducted extensive experiments with UniPredict and a suite of cutting-edge tabular prediction baselines, with a focus on answering the following research questions:</p>
<p>• Universal Tabular Modeling (Section 3.2) Can a single UniPredict model succeed in performing a universal modeling of extensive tabular datasets?</p>
<p>• Few-shot learning (Section 3.3) Compared with the baselines, how well does a pre-trained UniPredict model adapt to new tasks?</p>
<p>• Analysis #1 (Section 3.4) Under what circumstances is UniPredict less competitive to others?</p>
<p>• Analysis #2 (Section 3.5) What are the key factors that make UniPredict a successful candidate for universal tabular prediction?</p>
<p>Baseline Models</p>
<p>We included MLP as the simplest neural baseline.Drawing inspiration from the effectiveness of tree-boosting algorithms on tabular tasks, we assessed the performance of XGBoost (Chen &amp; Guestrin, 2016), a preeminent model in this domain.To explore the effectiveness of attention-based models in our tasks, we also included TabNet (Arik &amp; Pfister, 2021) and FT-Transformer (Gorishniy et al., 2021) to our experimental evaluation.Additionally, we incorporated TabLLM (Hegselmann et al., 2023) into our analysis, as it represents another model designed for tabular data with a focus on Large Language Models.The configurations and specifics of these baseline models are provided in Appendix B.1.Given the dataset-specific and non-transferable nature of the baseline models, we established isolated instances for each dataset included in our study.In contrast, for UniPredict, which aims at Universal Tabular Prediction, we instantiated a single model instance capable of handling all the datasets used in our experimentation.</p>
<p>Results on Universal Tabular Modeling</p>
<p>We assessed model accuracy on the test set of all 169 datasets and summarized the results in Figure 3.It is noted that due to the limitation of baseline models in terms of transferability onto new datasets, a distinct model was trained for each dataset, as discussed in Section 3.1.Nonetheless, even without additional dataset-specific fine-tuning, both variants of UniPredict consistently outperform all baseline models in terms of accuracy.</p>
<p>Specifically, UniPredict-heavy achieves a notable increase in absolute accuracy of 2.2% when compared to XGBoost, which stands as the top-performing model among the baseline models.</p>
<p>Meanwhile, UniPredict-light, following in the footsteps of its full-size counterpart, continues to exhibit better performance relative to the other models.The ranking metric confirms their dominance over the baselines.In this metric, both UniPredict-heavy and UniPredict-light consistently occupy top positions.As a candidate of treeboosting method, although XGBoost shares a similar median ranking with the best-performing models, it displays a higher 25% quartile in Figure 3b, indicating a sparser distribution of rankings.The other baselines fail to deliver comparable performance.TabLLM, designed as an LLM-driven model for individual datasets, does not yield results that are on par with other lighter methods.Despite its moderate ranking in terms of accuracy, it falls to the lower ranks when considering median ranking.Further details on dataset-specific results regarding accuracy and rank are provided in Appendix C.1.</p>
<p>Results on Few-shot Learning</p>
<p>We experimented UniPredict's few-shot learning accuracy, compared with baseline models that are trained individually on each of the 62 datasets, where each dataset contains less than 100 samples.This setup is to evaluate models on low-resource datasets because (1) collecting highquality samples is of high cost in practice, and (2) models that generalize well in large datasets do not always perform as well as in small datasets.For each dataset, we divided it into a train set and a test set, which served for training each model and fine-tuning the pre-trained UniPredict and TabLLM.To thoroughly assess our model's capacity for generalization, we devised multiple experimental configurations involving the partitioning of the training dataset into different proportions, spanning from 10% to 90% of the entire dataset.For each of these settings, we trained separate baseline models on the respective datasets.</p>
<p>Figure 4 shows the accuracy and ranking of all models with varying training data sizes.</p>
<p>Achilles' Heel: UniPredict's failure analysis</p>
<p>In this section, we aim to explore situations where our UniPredict framework does not perform well, which provides insight for deploying UniPredict and further enhancement.We have identified these situations by collecting datasets from the supervised setup (as used in Section 3.2) and identifying the datasets in which either UniPredict-heavy or UniPredict-light ranks in the bottom 2 (6th or 7th) among all compared methods.For each of these datasets, we have collected potential causes that may lead to the poor performance of our method.</p>
<p>We conclude that most failures can be attributed to one or more of the following causes:</p>
<p>• COL: Too many COLumns in the dataset.This may result in serialized input strings that exceed the context limit of the language model.It hence undermines model performance because the exceeding parts are pruned.</p>
<p>• FV: Poorly represented Feature Values that are challenging for the model to process and comprehend.Examples include an excessive number of numerical values or meaningless characters.</p>
<p>• META: Inadequate or ambiguous METAData, such as vague or meaningless column names and metadata, can confuse the model when comprehending the inputs.</p>
<p>• OTH: OTHer factors not explicitly covered above that may deteriorate model performance.</p>
<p>We include examples of each causes in Appendix C.3.As illustrated in Figure 5, bad feature values are the primary cause behind approximately half of the failures observed in our framework.Additionally, UniPredict-heavy is affected by confusing metadata descriptions and oversized columns.Interestingly, UniPredict-light, which is configured with minimal metadata usage (as discussed in Section 2.5), seems poised to minimize the influence of poor metadata.However, it paradoxically appears to struggle more with uninterpretable feature values, leading it to encounter more instances of poor performance compared to the default setup, UniPredict-heavy.</p>
<p>In a nutshell, we conclude with three hints in developing UniPredict in practice: (1) offering informative and accurate metadata for the input tabular dataset; (2) improving the context window limit of the LLM predictor to process more complicated inputs; and (3) cleaning up bad feature values before the training.</p>
<p>Ablation Study</p>
<p>In this section, we conduct an ablation study to examine whether the re-formatting and augmenting of targets are the critical factors contributing to the success of UniPredict.</p>
<p>The results are presented in Table 1.In the ablation study, the language models were fine-tuned using labels that only contained the one-hot encoding of the target class without the confidence information distributed into classes.</p>
<p>The results consistently demonstrate that regardless of the model variant (whether light or heavy), the model with target augmentation performs noticeably better than the model without augmentation.Furthermore, it is noteworthy that the ablation of UniPredict-light results in a more significant decrease in performance compared to UniPredict-heavy.This finding aligns with the conjecture made in Section 2.5 that the heavy variant is more robust and adaptable across different implementations and scenarios.</p>
<p>Related Work</p>
<p>Tabular Prediction.Tree-based models have shown outstanding performance on tabular prediction tasks (Chen &amp; Guestrin, 2016;Ke et al., 2017).Inspired by the rise of deep learning for tabular prediction (Arik &amp; Pfister, 2021), the recent research has emphasized three ways of improvement:</p>
<p>(1) taking advantage of pre-training or transfer learning on broad tabular data (Wang &amp; Sun, 2022;Zhu et al., 2023);</p>
<p>(2) adapting pre-trained large language models to gener- .an overview of the causes for which either model (Figure 5a), UniPredict-heavy (Figure 5b), or UniPredict-light (Figure 5c) experienced poor performance.As described in Section 3.4, COL, FV, META and OTH stand for Excessive Column Number, Bad Feature Values, Bad Metadata and Other reasons, respectively.Among the 169 datasets examined, 8 datasets are included in UniPredict-heavy's investigation, with 12 causes identified.UniPredict-light fails on 10 datasets, with 11 causes identified.1.The result of ablation among UniPredict-heavy (UniP-h), UniPredict-heavy without target augmentation (Abl-h), UniPredict-light (UniP-l), UniPredict-light without target augmentation (Abl-l).Tasks examined are Univeral Tabular Modeling that uses the same set up as Section 3.2, and Few-shot Learning as Section 3.3.The latter task involves both a low-data setup (Train Set Proportion = 0.3) and a high-data setup (Train Set Proportion = 0.8), which correspond to the conditions shown in Figure 4.For each task and setup, we provide both the average and median performance metrics across all datasets.</p>
<p>UniP-h</p>
<p>ate the target label column as the prediction (Dinh et al., 2022;Hegselmann et al., 2023); and (3) mining the graph structure considering an overview of the tabular dataset (Du et al., 2022;Chen et al., 2023).In addition, Wang et al. (2023) unify tabular data from various sources into a natural language format, establishing a tabular prediction pipeline capable of handling diverse inputs.However, most of these algorithms perform discriminative modeling for tabular prediction and hence are restricted to making the prediction for a fixed target.UniPredict, by contrast, depends on generative modeling for the prediction of any user-specified target.</p>
<p>Large Language Model.LLMs have demonstrated remarkable capabilities in logical thinking and solving language tasks under instructions (Bubeck et al., 2023;Zhao et al., 2023a).It has motivated researchers to adopt LLMs for a series of tabular data tasks, including tabular data generation (Borisov et al., 2022) and table-to-text generation (Zhao et al., 2023b).Meanwhile, LLMs are fine-tuned for tabular prediction as generation task (Dinh et al., 2022;Hegselmann et al., 2023).While these studies have showcased LLM is able to generate target labels given textualized tabular data, there remains an unexplored opportunity: constructing a versatile tabular predictor capable of handling a wide array of tabular datasets.In addition, previous LLM-based tabular predictors are usually trained to generate the target label while not offering the corresponding prediction probabilities.We argue it is crucial to inspect the prediction probabilities made by LLMs, which is necessary when deploying them in production.</p>
<p>Conclusion</p>
<p>We present UniPredict that can learn from an aggregation of widespread tabular datasets called universal tabular prediction.We train a single UniPredict model on 169 datasets with more than 300,000 samples and test it on the other 62 datasets for few-shot learning.Empirically, UniPredict yields the best prediction accuracy of 0.81 (2.2% absolute, 5.4% relative improvement compared to XGBoost).On unseen datasets, after dataset-specific finetuning, it exhibits great advantage when the training sets contain less than 50% of the samples (118% relative advantage to XGBoost at train-ratio=0.1) and consistently ranks at the top 2 in all scenarios.We envision that UniPredict paves the way for deploying foundational tabular prediction systems.The key distinction between UniPredict-heavy and UniPredict-light lies in the utilization of re-formatted metadata information.UniPredict-heavy incorporates this re-formatted metadata to enhance the language model's understanding of the dataset context and schema, while UniPredict-light opts not to include this information to maintain a lighter and more concise prompting approach.We talk about the metadata re-formatting procedure in Section 2.2 and Appendix A.2.</p>
<p>A.2. Metadata Reformatting</p>
<p>Metadata often goes overlooked in data analysis as traditional models and algorithms do not typically incorporate them as part of the input.However, metadata can provide valuable insights and context for various aspects of data analysis, including the dataset's purpose and the target for prediction.In our framework, we actively collect metadata from two sources:</p>
<p>• Dataset Descriptions, which usually appear in the front page of the dataset as an introduction.</p>
<p>• Column values, which can be found inside of the datasheet.</p>
<p>With this information, we generate re-formatted dataset metadata for the following subjects:</p>
<p>• Dataset Purpose This section states the purpose of the dataset, providing necessary context and background information.</p>
<p>• Target This section specifies the item within the dataset that should be the target for prediction.</p>
<p>• Column meanings This section explains the meaning of columns, especially in cases where column names may not directly map to semantic meanings (e.g., columns labeled 'a', 'b', 'c', etc.).It also elaborates on the significance of each column, often drawing from the dataset description to provide a more comprehensive understanding.</p>
<p>In our implementation, we use the gpt-3.5-turbomodel via the OpenAI-API to facilitate metadata re-formatting.Our prompt input to gpt-3.5 is shown as below:</p>
<p>"""</p>
<p>The following is the metadata of a tabular dataset.Return the information for:\n 1. the target of the dataset.If no target exists, choose one from the column as target for the dataset to classify.\nSubmission and Formatting Instructions for ICML 2024 2. the features and their explanations, or N/A if there are no explanations.Replace all hyphens and/or underscores with spaces.\n\nGive your output in json.The following is an example output:\n '{\n' ' "target": "Age",\n' ' "metadata": "The target of the dataset is Age.\n Features and their explanations:\n gender: an animal\'s gender.\nweight: an animal\'s actual weight, in kg." \n ' '}\n\n' Do NOT respond anything else than the needed information.Make it brief but informative.</p>
<p>Your responses should only be code, without explanation or formatting.\n\nThe following is our expected metadata after being re-formatted:</p>
<p>"""</p>
<p>The target of the dataset is Subscription Type.Listing 6. Feature serialization sample from arnavsmayan-netflix-userbase-dataset.columns = ",reviewerName,overall,reviewText,reviewTime,day_diff,helpful_yes,helpful_no, total_vote,score_pos_neg_diff,score_average_rating,wilson_lower_bound" values = "2346,J.Morse,5.0,'WhenI opened the micro disc and adapter I did't know what to do with them.I went to UTube on installing them, and all became clear.The micro fits into the top of the adapter and then the whole thing fits into my camera.Very neat and high powered.',2013-09-09,455,0,0,0,0,0.0,0.0"# result: "Unnamed: 0 is 2346; reviewerName is J. Morse; reviewText is When I opened the micro disc and adapter I did't know what to do with them.I went to UTube on installing them, and all became clear.The micro fits into the top of the adapter and then the whole thing fits into my camera.Very neat and high powered.;reviewTime is 2013-09-09; day diff is 455; helpful yes is 0; helpful no is 0; total vote is 0; score pos neg diff is 0; score average rating is 0.0; wilson lower bound is 0.0.\n"</p>
<p>Listing 7. Feature serialization sample from tarkkaanko-amazon.columns = "Pregnancies,Glucose,BloodPressure,SkinThickness,Insulin,BMI, DiabetesPedigreeFunction,Age,Outcome" values = "6, 98,58,33,190,34,0.43,43,0"# result: 'Pregnancies is 6.0; Glucose is 98.0; BloodPressure is 58.0;SkinThickness is 33.0;Insulin is 190.0; BMI is 34.0;DiabetesPedigreeFunction is 0.43; Age is 43.0.\n'Listing 8. Feature serialization sample from whenamancodes-predict-diabities.</p>
<p>A.4. Target Augmentation</p>
<p>As explained in Section 2.3, we re-format the targets into one-hot encodings and assign probabilities to them rather than using the one-hot binary labels (l ∈ 0, 1).The process of producing one-hot encodings depends on the nature of the target: If the targets are continuous values, we cluster the them into four quarters within the domain and represent them as categories; if the targets are already discrete values, we directly use the target value as the categories.The results are then serialized to be the reference output that the model is using for training.We provide specific examples for each of these implementations below:</p>
<h1>target_space: ['Standard', 'Premium', 'Basic'] example_target = ['Premium'] target_after_one_hot = [0, 1, 0] target_after_augmentation = [0.32, 0.39, 0.29]</h1>
<h1>outcome from target augmentation: target_class_details = 'class 0 stands for "Standard"; class 1 stands for "Premium"; class 2 stands for "Basic"' target_serialization = 'class 0: 0.32; class 1: 0.39; class 2: 0.29.' Listing 9. Descrete target augmentation example.Data come from arnavsmayan-netflix-userbase-dataset.</h1>
<h1>target_space = 1121 -63770 # categorized_target_space: ["&lt;4740.0","4740.0-9380.0","9380.0-16600.0","&gt;16600.0"]example_target = ['9095.069']# outcome from target augmentation: target_class_details = 'class 0 stands for "&gt;16600.0";class 1 stands for "&lt;4740.0";class 2 stands for "9380.0-16600.0"'target_serialization = 'class 0: 0.09; class 1: 0.0; class 2: 0.05; class 3: 0.86.' Listing 10.Continuous target augmentation example.Data come from mirichoi0218-insurance.</h1>
<p>A.5. LLM Output Mapping</p>
<p>For an LLM output that follows the format we described in Section A.4, we can use Regex matching to capture model's prediction.In this section, we present our baseline setups:</p>
<p>Let</p>
<p>• XGBoost is a tree-ensemble method that has been broadly used in tabular prediction.In our experiment, we train XGBoost instances via its official release on Python. 5We apply ordinal encoding on all features and categories except the numerical features and tune one instance on each dataset with n estimators=100, max depth=6, learning rate=0.3.</p>
<p>• Multilayer Perceptron is a fundamental neural network architecture that consists of fully-connected hidden layers.We use the MLPClassifier instance from scikit-learn.On each dataset, a model is instantiated with learning rate=1e-3, n hidden layer=1, activation='relu', optimizer='adam'.We also set random state=1 and max iteration=100.</p>
<p>• FT-Transformer is an attention-based model designed and trained specifically for tabular data tasks.We use the original implementation from the author6 with no extra changes on implementation.The hyperparameters we use here are num batchs=8, num epochs=100, learning rate=1e-3.</p>
<p>• TabNet is another attention based model on tabular data.We instantiate models from its official release on python7 .Similar to our approach with XGBoost, we applied the same data preprocessing procedure to TabNet.Specifically, we used ordinal encoding for features and categories (excluding numerical features).We conducted model tuning using the default hyperparameters.</p>
<p>• TabLLM is an LLM-based system specifically designed for tabular prediction tasks.In our implementation, we followed the setup as described in the original work.Since UniPredict is built upon a GPT-2 backbone, we implement TabLLM on a GPT-2 as well to align the backbone choices for a fair comparison.When incorporating specific instructions into the prompt, instead of creating separate instances to ask 'yes-or-no' questions individually for each target class, we streamlined the process by instructing the model to predict the class name directly.This approach simplifies the training procedure and conserves computational resources.An example prompt is presented below.We train isolated TabLLM instances on each dataset, regardless of the origin of the dataset (supervised division or few-shot division).</p>
<p>B.2. Dataset Statistics</p>
<p>We present all dataset statistics in Table 2 and Table 3.In the training setup, all datasets are split with a train-set-ratio=0.9.In the few-shot testing setup, all datasets are tested with different train set ratios ranging from 0.1 to 0.9.Listing 12. Example columns and values that have the COL (too many column) problem.Data origin: suraj520-dairy-goods-sales-dataset # Column names: """ fixed acidity,volatile acidity,citric acid,residual sugar,chlorides,free sulfur dioxide, total sulfur dioxide,density,pH,sulphates,alcohol,quality,Id """ # Column value example: """ 7.4,0.7,0.0,1.9,0.076,11.0,34.0,0.9978,3.51,0.56,9.4,5,0""" It could also be used to develop machine learning models to predict student performance based on demographic and other factors.\n\nsource: \protect\vrule width0pt\protect\href{http://roycekimmons.com/tools/generated_data/exams\n}{http://roycekimmons.com/tools/generated_data/exams\n}""""</p>
<p>Figure 1 .
1
Figure 1.Visualization for three tabular modeling paradigms.Left: In Traditional Tabular Modeling tasks (Figure1a), distinct models are trained individually on each dataset, making them incapable of adaptation to new datasets with differing features and targets.Middle: In the In-Domain Tabular Modeling tasks (Figure1b), where flexibility is allowed for features, the targets remain the same across datasets.Right: the proposed Universal Tabular Modeling paradigm (Figure1c), which accommodates arbitrary inputs and predicting for arbitrary targets.This paradigm does not impose any restrictions on the domains of the datasets used.In Universal Tabular Modeling, the datasets can originate from entirely different domains.</p>
<p>Figure 2 .
2
Figure 2. The UniPredict framework.It consists of three steps: 1) Prompt Setup sets up the prompts by metadata, sample serialization, and instructions; 2) Target Augmentation transforms target values into categories with confidence estimates; and 3) Learning fine-tunes the backbone model by prompts and targets yielded from the previous procedures.</p>
<p>Figure 3 .
3
Figure3.The average accuracy and rank of UniPredict-heavy, UniPredict-light, TabLLM(Hegselmann et al., 2023) XGBoost(Chen &amp; Guestrin, 2016), MLP, TabNet(Arik &amp; Pfister, 2021) and FT-Transformer(Gorishniy et al., 2021) on 169 datasets.Each dot indicates a trial on one dataset.UniPredict-heavy demonstrates a remarkable performance advantage over the best neural network model (FT-Transformer) with a relative improvement of 13.4%.It also surpasses the best-performing tree-boosting algorithms by a margin of 5.4%.Our framework's advantage is further confirmed by Figure3b, the model ranking (the less the better)</p>
<p>Figure 4 .
4
Figure 4.The average accuracy and rank of UniPredict-heavy, UniPredict-light, TabLLM XGBoost, MLP, TabNet and FT-Transformer on 62 datasets.We vary the training data size, ranging from the lowest (10%) to the highest (90%) of the full dataset.The pre-trained UniPredict series exhibit remarkable data efficiency in generalizing to new tasks.</p>
<p>Figure5. an overview of the causes for which either model (Figure5a), UniPredict-heavy (Figure5b), or UniPredict-light (Figure5c) experienced poor performance.As described in Section 3.4, COL, FV, META and OTH stand for Excessive Column Number, Bad Feature Values, Bad Metadata and Other reasons, respectively.Among the 169 datasets examined, 8 datasets are included in UniPredict-heavy's investigation, with 12 causes identified.UniPredict-light fails on 10 datasets, with 11 causes identified.</p>
<p>description of a dataset, an object profile from the dataset and a target description.Predict the target by the given information of the object.\n# Dataset description: {metadata}\n # Object description: {features}\n # You should return the probability of each class by: \n{instructions}\n # Answer: \n """ Listing 1. Prompt for UniPredict-heavy """ Below is a dataset.Predict the target by the given information of the object.\n# Object description: {features}\n # You should return the probability of each class by: \n{instructions}\n # Answer: \n """ Listing 2. Prompt for UniPredict-light</p>
<p>Prompt for metadata re-formatting via OpenAI-API Example inputs that are filled into this prompt template are as follows: metadata = "The dataset provides a snapshot of a sample Netflix userbase, showcasing various aspects of user subscriptions, revenue, account details, and activity.Each row represents a unique user, identified by their User ID.The dataset includes information such as the user's subscription type (Basic, Standard, or Premium), the monthly revenue generated from their subscription, the date they joined Netflix (Join Date), the date of their last payment (Last Payment Date), and the country in which they are located.\n\nAdditionalcolumns have been included to provide insights into user behavior and preferences.These columns include Device Type (e.g., Smart TV, Mobile, Desktop, Tablet), Total Watch Time (in minutes), and Account Status (whether the account is active or not).The dataset serves as a synthetic representation and does not reflect actual Netflix user data.It can be used for analysis and modeling to understand user trends, preferences, and revenue generation within a hypothetical Netflix userbase."col = "User ID,Subscription Type,Monthly Revenue,Join Date,Last Payment Date,Country,Age, Gender,Device,Plan Duration" Listing 4. Example input to the prompt for metadata re-formatting.Information origin: arnavsmayan-netflix-userbase-dataset</p>
<p>response = 'class 0: 0.09; class 1: 0.0; class 2: 0.05; class 3: 0.86.' be a sample response from the LLM, we obtain a listed result of numerical probabilities by applying result = re.findall(r'[\d]* [.][\d]+', response) # result = [0.09,0.0, 0.05, 0.86] Based on the listed result, we can compute the model's prediction on classes by finding the index of the maximum in the list.result_class = pred_cls.index(max(result))# result_class = 3 B. Implementation Details B.1.Baseline</p>
<p>dataset.Predict the target by the given information of the object.\n# Object description: {features}\n # You should return your choice of class by stating the class number, {instructions}\n Submission and Formatting Instructions for ICML 2024 5 # Answer: \n 6 """ 7 # 'instructions' includes a sequence stating the detail of each class, for example 'class 1 is for "a", class 2 is for "b", ...' 8 # Example model output: 'class 1' Listing 11.Prompt for TabLLM</p>
<p>-data-with-age-andexperience # Note: This sample also has the FV (Poorly represented Feature Values) problem as there are too many numerical values inside.</p>
<p>Listing 13 .
13
Example columns and values that have the FV (Poorly represented Feature Values) problem.Dataset origin: yasserh-wine-quality-Example columns and values that have the META (Inadequate or ambiguous Metadata) problem.Dataset origin: kumargh-pimaindiansdiabetescsv # Dataset metadata: """ Description: This dataset contains information on the performance of high school students in mathematics, including their grades and demographic information.The data was collected from three high schools in the United States.\n\nColumns:\n\n\t ** Gender: ** The gender of the student (male/female)\n\n\t ** Race/ethnicity: ** The student's racial or ethnic background (Asian, African-American, Hispanic, etc.)\n\n\t ** Parental level of education: ** The highest level of education attained by the student's parent(s) or guardian(s)\n\n\t ** Lunch: ** Whether the student receives free or reduced-price lunch (yes/no)\n\n\t ** Test preparation course: ** Whether the student completed a test preparation course (yes/ no)\n\n\t ** Math score: ** The student's score on a standardized mathematics test\n\n\t ** Reading score: ** The student's score on a standardized reading test\n\n\t ** Writing score: ** The student's score on a standardized writing test\n\nThis dataset could be used for various research questions related to education, such as examining the impact of parental education or test preparation courses on student performance.</p>
<p>\n Features and their explanations:\n User ID: unique identifier for each user.\nMonthlyRevenue: the amount of revenue generated from each user's subscription.\nJoinDate:the date when each user joined Netflix.\nLastPaymentDate: the date of the last payment made by each user.\nCountry: the country in which each user is located.\nAge: the age of each user.\nGender: the gender of each user.\nDevice: the type of device used by each user.\nPlanDuration: the duration of each user's subscription plan.="UserID,Subscription Type,Monthly Revenue,Join Date,Last Payment Date,Country, Age,Gender,Device,Plan Duration" values = "1448,Standard,14,United States,33,Female,Laptop,1 Month" # result: "'User ID is 1448; Monthly Revenue is 14; Join Date is 18-07-22; Last Payment Date is 07-07-23; Country is United States; Age is 33; Gender is Female; Device is
Submission and Formatting Instructions for ICML 2024Laptop; Plan Duration is 1 Month.\n'""""Listing 5. Example output from metadata re-formatting. Result generated from: arnavsmayan-netflix-userbase-datasetA.3. Feature Serialization ExampleWe present 3 sample feature serializations from different datasets below:
columns</p>
<p>Table 2 :
2
Dataset statistics for model training and testing (Results shown in Section 3.2).We include each dataset's Name, number of rows, number of cols, and whether the dataset's targets are continuous (Ctns).The last measurement determines whether the dataset's targets need to be re-categorized into quarters, as detailed in Appendix A.4.
Namerows cols Ctns Namerows cols Ctnsarnavsmayan-netflix-userbase-dataset2500 9 False deependraverma13-diabetes-healthcare-7688 Falsecomprehensive-datasetbhanupratapbiswas-uber-data-analysis1156 6 False swathiunnikrishnan-amazon-consumer-602 22 Falsebehaviour-datasethemanthhari-psycological-effects-of-covid1175 21 False arslanr369-bitcoin-price-2014-20233228 6 Falsesaloni1712-chatgpt-app-reviews2292 3 True naveenkumar20bps1137-predict-students-4424 34 Falsedropout-and-academic-successsanjanchaudhari-user-behavior-on-instagram 7488 8 False bhanupratapbiswas-bollywood-actress-name-1284 9 Falseand-movie-listarnavsmayan-vehicle-manufacturing-dataset 2000 7 False bharath011-heart-disease-classification-1319 8 Falsedatasetshroukgomaa-babies-food-ingredients696 25 False amirhosseinmirzaie-countries-life-2848 17 Falseexpectancyamirhosseinmirzaie-pistachio-types-1718 16 False shashankshukla123123-marketing-campaign 2240 29 Falsedetectionuciml-pima-indians-diabetes-database7688 False shubhamgupta012-titanic-dataset8898 Falsebhanupratapbiswas-fashion-products1000 8 False blastchar-telco-customer-churn7043 20 Falsemirichoi0218-insurance1338 6 False suraj520-dairy-goods-sales-dataset4325 22 Falseuciml-red-wine-quality-cortez-et-al-20091599 11 False akshaydattatraykhare-diabetes-dataset7688 Falsearnabchaki-data-science-salaries-20233755 10 False prkhrawsthi-bitcoin-usd-daily-price-with-3104 6 Falsevolume-2015-2023hawkingcr-airbnb-for-boston-with-fraud-3585 20 False saunakghosh-nba-players-dataset5130 7 Falsedetectionrtatman-chocolate-bar-ratings1795 8 False pavansubhasht-ibm-hr-analytics-attrition-1470 34 Falsedatasetgyanprakashkushwaha-laptop-price-1273 12 False fedesoriano-stroke-prediction-dataset5110 11 Falseprediction-cleaned-datasetbhanupratapbiswas-world-top-billionaires2614 21 False vstacknocopyright-blood-transfusion-7485 Falseservice-center-dataashishkumarjayswal-movies-updated-data4000 14 False bhanupratapbiswas-ipl-dataset-2008-2016577 15 Falsemathchi-diabetes-data-set7688 False harishkumardatalab-medical-insurance-price-2772 6 Falsepredictionarslanr369-roblox-stock-pricing-2021-20235726 False yasserh-titanic-dataset891 11 Falseiqmansingh-company-employee-dataset5000 12 False shivamb-disney-movies-and-tv-shows1450 11 Falsealexisbcook-pakistan-intellectual-capital1142 12 False tahzeer-indian-startups-by-state7091 5 Falseharshitshankhdhar-imdb-dataset-of-top-1000 15 False shreyapurohit-anime-data6850 4 False1000-movies-and-tv-showsraddar-icr-integer-data617 57 False uciml-mushroom-classification8124 22 Falseadityakadiwal-water-potability3276 9 False shreyanshverma27-imdb-horror-chilling-8367 Falsemovie-datasetruchi798-data-science-job-salaries607 11 False hesh97-titanicdataset-traincsv891 11 Falsephangud-spamcsv5572 1 False dileep070-heart-disease-prediction-using-4238 15 Falselogistic-regressionabcsds-pokemon800 12 False atharvaingle-crop-recommendation-dataset2200 7 False</p>
<p>Table 3 :
3
Dataset statistics for the few-shot testing (Results shown in Section 3.3).We include each dataset's Name, number of rows, number of cols, and whether the dataset's targets are continuous (Ctns).The last measurement determines whether the dataset's targets need to be re-categorized into quarters, as detailed in Appendix A.4.</p>
<p>OpenAI API: gpt-3.5-turbo
https://huggingface.co/
https://www.kaggle.com/datasets/
https://github.com/Kaggle/kaggle-api
Information can be found at https://xgboost.readthedocs.io/en/stable/python/python intro.html.
https://github.com/Yura52/rtdl
https://pypi.org/project/pytorch-tabnet/
False drahulsingh-ab-de-villiers-all-internationalcricket-centuries
True yusufdede-lung-cancer-dataset
False mauryansshivam-netflix-ott-revenue-andsubscribers-csv-file
False
Submission and Formatting Instructions for ICML 2024 A. Methodology: More DetailA.1. Prompt TemplatesThe quality of the natural language input provided to Large Language Models (LLMs) play a crucial role in determining the model's output and, consequently, its performance on tabular prediction tasks.The following are the prompt templates used in the implementation of both UniPredict-heavy and UniPredict-light:B.3. Model TrainingWe utilize a GPT-2(Radford et al., 2019)model as backbone.We perform training following an instruction fine-tuning process.The optimizer choice is AdamW with lr=5e-5, beta 1 = 0.9, beta 2 = 0.999, epsilon = 1e-8, weight decay = 0.The model is trained for 3 epochs.The model takes approximately 75 hours to be trained on a single RTX3090.The few-shot learning process is almost identical to the training process described above.The only difference is that we increase the epoch to 30 to ensure convergence.C. ResultC.1. Detailed Model Performance on Universal Tabular PredictionWe present all models' performance on each supervised dataset in Table4, including the ablation models.C.2. Model Performance on Few-Shot DatasetsWe present additional accuracy/ranking figures and datapoints for the few-shot datasets.Figure6demonstrates each model's performance when train-set-proportion=0.1, Figure7shows their performance when the value is set to 0.5, and Figure6gives the picture of models at a resource-rich setup (train-set-proportion=0.9).See Section 3.3 for detailed discussion.Table4: The performance of UniPredict-heavy (UniP-h), its ablation (Abl-h), UniPredict-light (UniP-l), its ablation (Abl-l), TabLLM (TabLLM), XGBoost (XGBoost), MLP (MLP), FT-Transformer (FT-Trans), and TabNet (TabNet) on the supervised datasets.Each model's accuracy on the test set is reported.See Section 3.2 for the result analysis.C.3. Example Cause of FailureIn Section 3.4 we presented a failure study on UniPredict, and gave some possible issues that cause the model to give poor performance.We present demonstrations for each causes below:
Tabnet: Attentive interpretable tabular learning. S Ö Arik, T Pfister, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202135</p>
<p>Language models are realistic tabular data generators. V Borisov, K Sessler, T Leemann, M Pawelczyk, G Kasneci, The Eleventh International Conference on Learning Representations. 2022</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>P Chen, S Sarkar, L Lausen, B Srinivasan, S Zha, R Huang, G Karypis, Hytrel, arXiv:2307.08623Hypergraphenhanced tabular data representation learning. 2023arXiv preprint</p>
<p>Xgboost: A scalable tree boosting system. T Chen, C Guestrin, Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. the 22nd acm sigkdd international conference on knowledge discovery and data mining2016</p>
<p>LIFT: Language-interfaced fine-tuning for non-language machine learning tasks. T Dinh, Y Zeng, R Zhang, Z Lin, M Gira, S Rajput, J.-Y Sohn, D Papailiopoulos, K Lee, Advances in Neural Information Processing Systems. 202235</p>
<p>Learning enhanced representation for tabular data via neighborhood propagation. K Du, W Zhang, R Zhou, Y Wang, X Zhao, J Jin, Q Gan, Z Zhang, D P Wipf, Advances in Neural Information Processing Systems. 202235</p>
<p>Revisiting deep learning models for tabular data. Y Gorishniy, I Rubachev, V Khrulkov, A Babenko, Advances in Neural Information Processing Systems. 202134</p>
<p>Few-shot classification of tabular data with large language models. S Hegselmann, A Buendia, H Lang, M Agrawal, X Jiang, D Sontag, Tabllm, International Conference on Artificial Intelligence and Statistics. PMLR2023</p>
<p>LightGBM: A highly efficient gradient boosting decision tree. G Ke, Q Meng, T Finley, T Wang, W Chen, W Ma, Q Ye, T.-Y Liu, Advances in Neural Information Processing Systems. 201730</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>Z Wang, J Sun, Transtab, arXiv:2205.09328Learning transferable tabular transformers across tables. 2022arXiv preprint</p>
<p>Anypredict: Foundation model for tabular prediction. Z Wang, C Gao, C Xiao, J Sun, 2023</p>
<p>Click-through rate prediction in online advertising: A literature review. Y Yang, P Zhai, Information Processing &amp; Management. 5921028532022</p>
<p>Calibrate before use: Improving few-shot performance of language models. T Z Zhao, E Wallace, S Feng, D Klein, S Singh, 2021</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv:2303.18223A survey of large language models. 2023aarXiv preprint</p>
<p>Large language models are effective table-to-text generators, evaluators, and feedback providers. Y Zhao, H Zhang, S Si, L Nan, X Tang, A Cohan, arXiv:2305.149872023barXiv preprint</p>
<p>Xtab: Cross-table pretraining for tabular transformers. B Zhu, X Shi, N Erickson, M Li, G Karypis, M Shoaran, ICML. 2023</p>
<p>Age at enrollment,International, Curricular units 1st sem (credited),Curricular units 1st sem (enrolled),Curricular units 1st sem (evaluations),Curricular units 1st sem (approved),Curricular units 1st sem (grade),Curricular units 1st sem (without evaluations),Curricular units 2nd sem ( credited),Curricular units 2nd sem (enrolled),Curricular units 2nd sem (evaluations), Curricular units 2nd sem (approved),Curricular units 2nd sem (grade),Curricular units 2nd sem (without evaluations),Unemployment rate,Inflation rate,GDP,Target. # Column Names, Marital status,Application mode. Gender2,1,1,1,13,10,6,10,1,0,0,1,1,0,20,0,0,0,0,0,0.0,0,0,0,0,0,0.0,0, 10Course,Daytime/evening attendance, Previous qualification,Nacionality,Mother's qualification,Father's qualification, Mother's occupation,Father's occupation,Displaced,Educational special needs,Debtor, Tuition fees up to date. # Column value example: """ 1,8,5. 8,1.4,1.74, Dropout """ Submission and Formatting Instructions for ICML 2024</p>
<p>. # Column Names, gender","race/ethnicity","parental level of education","lunch","test preparation course ","math score","reading score","writing score" """ # Column value example: """ "female","group D","some college","standard","completed","59","70","78" """ # Nothing is explicitly wrong in this dataset. Listing 15. Example columns and values that have the OTH (Other factors) problem. Dataset origin: rkiattisak-student-performance-in-mathematics</p>            </div>
        </div>

    </div>
</body>
</html>
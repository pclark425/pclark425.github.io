<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4431 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4431</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4431</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-272968899</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.18812v1.pdf" target="_blank">LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis</a></p>
                <p><strong>Paper Abstract:</strong> In response to the growing complexity and volume of scientific literature, this paper introduces the LLMs4Synthesis framework, designed to enhance the capabilities of Large Language Models (LLMs) in generating high-quality scientific syntheses. This framework addresses the need for rapid, coherent, and contextually rich integration of scientific insights, leveraging both open-source and proprietary LLMs. It also examines the effectiveness of LLMs in evaluating the integrity and reliability of these syntheses, alleviating inadequacies in current quantitative metrics. Our study contributes to this field by developing a novel methodology for processing scientific papers, defining new synthesis types, and establishing nine detailed quality criteria for evaluating syntheses. The integration of LLMs with reinforcement learning and AI feedback is proposed to optimize synthesis quality, ensuring alignment with established criteria. The LLMs4Synthesis framework and its components are made available, promising to enhance both the generation and evaluation processes in scientific research synthesis.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4431.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4431.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs4Synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs4Synthesis Framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end framework introduced in this paper that adapts LLMs to generate high-quality scientific syntheses by combining dataset standardization, supervised fine-tuning, and reinforcement learning with AI feedback (RLAIF).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLMs4Synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline that (1) standardizes multi-paper inputs (titles + abstracts from exactly five papers) using ORKG-derived prompts; (2) optionally performs supervised fine-tuning (SFT) of an open-source base LLM (Mistral-7B) using GPT-4-generated syntheses as pseudo-ground-truth; (3) applies reinforcement learning with AI feedback (RLAIF) using PPO to optimize generation under reward models that include basic structural rewards and GPT-4-based qualitative rewards; and (4) includes a KL-penalty to constrain policy drift from the SFT model. The framework defines three synthesis types (paper-wise, methodological, thematic) and nine qualitative evaluation criteria used in the reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Mistral-7B (Mistral-7B-Instruct-v0.1) as base generator; GPT-4 (gpt-4-1106-preview) and GPT-4-Turbo used as gold standard / evaluator in training and reward modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Structured prompting using ORKG Comparisons: titles and abstracts of exactly five papers are concatenated into a standardized prompt template; no explicit IR embedding stage described beyond ORKG and open-access metadata retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Multi-document synthesis (single-paragraph up to 200 words) via supervised fine-tuning (SFT with QLoRA) followed by reinforcement learning (PPO) guided by reward models (basic format/length penalties and GPT-4 feature-based qualitative scoring) to integrate and compress findings across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Exactly 5 papers per synthesis (fixed by dataset construction and prompt design).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Multidisciplinary (dataset drawn from ORKG across domains; examples include Computer Science, Chemistry, Earth Science, Linguistics, Sociology).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Single-paragraph scientific syntheses (paper-wise, methodological, or thematic), limited to 200 words and with inline citations referencing the input papers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Nine qualitative criteria scored 1–5 by GPT-4-Turbo and human evaluators: relevancy, correctness, completeness, informativeness, integration, cohesion, coherence, readability, conciseness; plus basic structural metrics (word-count bands, paper-structure detector).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>SFT+RLAIF (w/ GPT-4 Features) achieved high GPT-4-evaluator scores (e.g., relevancy ~4.88, correctness ~4.75, completeness ~4.19, informativeness ~4.83, integration/cohesion ~4.89, coherence ~4.85, readability ~4.78; conciseness lower ~3.64). GPT-4-generated syntheses (gold standard) scored higher (relevancy 4.97, correctness 4.93, completeness 4.42). Average word counts: GPT-4 outputs ~218 words, SFT+RLAIF ~205 words; RL models with Basic Features produced averages close to 189–204 words depending on configuration. (Values from Table 3 and text.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Vanilla (unfine-tuned Mistral-7B), SFT-only (QLoRA fine-tuned Mistral), RL-only variants (with Basic Features), and GPT-4 as gold standard.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>SFT+RLAIF (w/ GPT-4 Features) substantially improved over Vanilla and SFT baselines on most qualitative metrics (e.g., relevancy: 4.88 vs Vanilla 4.33; correctness: 4.75 vs Vanilla 3.66). RL with Basic Features improved format/word-count adherence (e.g., RL w/ Basic Features avg word count ~189 vs Vanilla 242).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining SFT (with QLoRA) and RLAIF using a GPT-4-based reward yields syntheses that are more relevant, correct, integrated, and coherent than vanilla or SFT-only models while also increasing output consistency. The use of GPT-4 as a reward/evaluator effectively aligns generation with human-like quality judgments. Basic structural rewards (word-count, paper-structure detector) fix common format failures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Remaining issues include conciseness/verbosity, occasional format regression (paper-like outputs from some models), domain gaps in humanities/social sciences (weaker performance in Linguistics and Sociology), dependence on GPT-4 as a surrogate human evaluator (potential evaluator bias), computational cost for RL finetuning, and reliance on only titles+abstracts (no full-text evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Paper reports that larger proprietary LLM (GPT-4) outperforms smaller open-source Mistral-7B; SFT+RLAIF narrows the gap. Dataset uses fixed 5-paper inputs; no explicit experiments varying number of papers, but performance improves with SFT+RLAIF fine-tuning and GPT-4-based reward shaping. KL penalty (λ=0.2) used to control policy drift during RL.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4431.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4431.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ORKG Synthesis Dataset</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open Research Knowledge Graph Scientific Synthesis Generation Dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multidisciplinary dataset constructed from ORKG Comparisons containing samples of five-paper groups (titles + abstracts) and generated syntheses for research on scientific synthesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ORKG Scientific Synthesis Dataset</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Collected via ORKG Comparisons: extracted research problems and associated papers; filtered to Comparisons with ≥5 papers and available abstracts; grouped all papers into collections of five to produce standardized synthesis prompts. Each sample contains sample ID, research field, research problem, titles, abstracts, DOIs, and three synthesis types per LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Used as input for synthesis generation by Mistral-7B and GPT-4 in experiments (dataset itself is not an LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Dataset extraction from ORKG Comparisons and open-access metadata sources (Semantic Scholar, Crossref, CORE) to retrieve titles and abstracts; no embedding/retrieval in dataset construction stage.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Used as content for multi-document synthesis (single-paragraph, 200-word) via prompting to LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>348 final samples (each sample uses exactly 5 papers); resulted in 1,044 standardized prompts and 1,044 syntheses per model (total 2,088 syntheses across two models).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Multidisciplinary; top fields include Computer Science (125 samples), Physics, Animal Sciences, Chemistry, Urban Studies, Earth Sciences, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured prompt records and LLM-generated single-paragraph syntheses (paper-wise, methodological, thematic).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Nine qualitative criteria (1–5) by GPT-4-Turbo and human crowdsourced evaluations; basic format metrics (word count bands, paper-structure presence).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Dataset enabled fine-tuning experiments and evaluation; used to produce 1,044 syntheses per model and to train/test SFT and RLAIF models (split: 80% train, 20% test by domain).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not a method; used as standardized corpus to compare Mistral and GPT-4 outputs and to train SFT/RL variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>N/A (dataset resource).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Crowdsourced ORKG Comparisons provide a larger and more multidisciplinary corpus than previous manually curated datasets; a fixed 5-paper grouping aligns with practice of search systems returning top-5 results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Only titles and abstracts used (no full texts), variability in crowdsourced research-problem descriptions, duplicated samples across fields removed by random selection, and potential domain imbalance.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Dataset scale (348 samples) allowed SFT and RL training; authors claim dataset is ~3x larger than prior work but no experiments varying dataset scale were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4431.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4431.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ORKG Ask</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ORKG Ask (Open Research Knowledge Graph Ask)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open platform / next-generation search system referenced in the paper that integrates LLMs to enhance search, natural-language queries, semantic search, and AI-driven extractions over scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ORKG Ask</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described as an open platform capable of semantic, natural language queries and AI-driven extractions; noted as compatible with integration of LLMs4Synthesis to enhance synthesis capabilities and promote open access and collaboration.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in paper (platform mention).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Semantic search and AI-driven extraction (as described in the paper's related-work discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Platform-level synthesis and Q&A features; can integrate external LLM-based synthesis modules like LLMs4Synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Typical behavior described as operating over top-N search results (authors note top-5 as a typical synthesis basis); exact limits not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scholarly / multidisciplinary.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Search answers, syntheses (interactive Q&A), and extracted structured knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned as an example of modern search engines; no experimental comparison in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited to illustrate how LLMs are already integrated into scientific search tools enabling natural-language queries and extraction workflows; authors propose LLMs4Synthesis could be embedded into ORKG Ask.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not discussed in detail here (platform mentioned in context).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4431.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4431.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Elicit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Elicit (Research Assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial/academic search-assisted platform referenced as an example of systems that use LLMs to accelerate literature access and produce syntheses from top search results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Elicit</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as an example of next-generation search engines that leverage LLMs for natural language queries, semantic search, and automated extraction to produce syntheses (often from top-5 search results).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>AI-driven extraction, semantic retrieval (platform-level descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Automated synthesis from search results; multi-document condensation (not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Typical practice referenced: top-5 search results used for syntheses.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Literature syntheses, answers to research questions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned as related work; no baseline comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited to contextualize the demand for automated synthesis and the common practice of synthesizing top search hits.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4431.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4431.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciSpace</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciSpace (SciSpace research platform)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial research platform referenced as an instance where LLMs enhance search capabilities and user interactions over scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciSpace</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned among modern platforms that use LLMs for improved search, interactive queries, and AI-driven extraction of paper content to aid users in literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>AI-driven extraction and semantic retrieval (platform-level).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Automated summarization/synthesis features integrated into the platform (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General research literature.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Search-assistant outputs and extractive summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned as related work only.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Used to motivate the practical need for LLM-based synthesis tools.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4431.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4431.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CORE-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system combining open-access research corpora with LLMs to produce credible question-answering over scholarly content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CORE-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prior work that integrates open-access repositories (CORE) with LLMs to produce QA and likely extract/summarize scientific content by grounding LLM responses in retrieved open-access documents.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper; original CORE-GPT work uses LLMs (reference in bibliography).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval over open-access papers + grounding of LLM outputs in retrieved documents (paper references indicate an approach for trustworthy QA).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Document-grounded answering and summarization; likely involves retrieval-augmented generation (RAG) patterns (paper cited for context).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General open-access research (multidisciplinary).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Question answering and grounded summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified here; cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned as prior relevant system (no baseline details here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited to highlight approaches that combine retrieval from open-access corpora and LLMs to improve credibility/trustworthiness of generated scientific answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not analyzed in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4431.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4431.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RLAIF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning with AI Feedback (RLAIF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training paradigm where reinforcement learning is guided by automated (AI) evaluators as reward sources rather than (or in addition to) human annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RLAIF (Reinforcement Learning with AI Feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used in LLMs4Synthesis: after SFT, policy is fine-tuned with PPO where reward models are either rule-based (Basic Features) or GPT-4-based qualitative scoring (GPT-4 Features). The RL loop alternates generation and scoring; a KL-divergence penalty term (λ=0.2) constrains divergence from the base SFT policy. Optimization uses Adam with a reported LR ~2.94e-5 and PPO training with 10 epochs per batch.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Policy initialized from SFT Mistral-7B model; reward/evaluator is GPT-4-Turbo (proxy human feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not an extraction technique per se; RLAIF consumes standardized prompts (titles + abstracts) and produces syntheses that are scored for reward.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Policy generates single-paragraph syntheses; reward models score outputs to guide learning toward preferred synthesis qualities.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Training uses standardized prompts derived from 5-paper inputs; training set includes 810 training prompts for RL partition.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Multidisciplinary (same dataset domains).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Optimized LLM synthesizer producing paragraph syntheses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Rewards based on Basic Features (word count, paper-structure penalty) and GPT-4-derived scores over nine qualitative criteria; final evaluation also uses GPT-4 and human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>RLAIF variants with GPT-4 Features improved qualitative scores compared to Vanilla and SFT baselines; SFT+RLAIF (w/ GPT-4 Features) approximated GPT-4-level outputs and increased output consistency across repeated runs (lower variance).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to Vanilla Mistral, SFT-only, RL with Basic Features, and GPT-4 gold standard.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>SFT+RLAIF (w/ GPT-4 Features) outperformed Vanilla and SFT across relevancy, correctness, completeness, integration, cohesion, and coherence (see Table 3 for numeric comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AI-generated feedback (GPT-4) is an effective proxy for human feedback in RL fine-tuning to align generation with multi-dimensional quality criteria and increase stability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Using an LLM as reward model risks encoding evaluator biases; reliance on GPT-4 as the reward oracle may limit openness and reproducibility; computational cost of RL finetuning; limited understanding of generalization outside the specific 5-paper prompt format.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Authors observed that adding GPT-4-based feedback improves metrics; larger LLMs (GPT-4) perform better initially, but RLAIF helps smaller open-source models (Mistral-7B) close the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4431.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4431.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFT + QLoRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Fine-Tuning with QLoRA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Supervised fine-tuning of the base LLM using GPT-4-generated syntheses as pseudo-ground-truth, implemented with quantized LoRA adapters (QLoRA) to reduce resource requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SFT (Supervised Fine-Tuning) + QLoRA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Fine-tunes Mistral-7B using GPT-4 outputs as target texts and a standardized prompt template. Uses QLoRA (4-bit quantized low-rank adapters) to enable efficient finetuning; adapter rank r=8, scaling factor and dropout reported; training used AdamW with LR 2e-4, gradient accumulation, warmup and 5 epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Mistral-7B (base) fine-tuned via QLoRA; GPT-4 outputs used as ground-truth targets.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Training data constructed from ORKG-sourced titles and abstracts; no additional extraction beyond prompt formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>SFT teaches the model to map 5-paper inputs to target single-paragraph syntheses (three synthesis types) prior to RL.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Trained on standardized prompts representing five-paper groups; Train-LLM split contained 405 synthesis prompts (from 135 comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Multidisciplinary (dataset splits by domain).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Single-paragraph syntheses matching GPT-4 style.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Qualitative nine-criteria scores via GPT-4 and human evaluation; word count and structure metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>SFT improved format and some qualitative aspects over Vanilla but underperformed relative to SFT+RLAIF and GPT-4 on many semantic metrics; average word count for SFT ~231 words (improvement over Vanilla but still above 200).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Vanilla (no finetuning), RL-only variants, and combined SFT+RLAIF.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>SFT improves some metrics vs Vanilla (e.g., lower paper-structure incidents), but combining SFT with RLAIF yields much larger quality gains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using QLoRA makes SFT feasible on resource-constrained open models and provides a better initialization for subsequent RLAIF.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>SFT alone insufficient to enforce multi-dimensional quality constraints (e.g., conciseness, integration) without RL and reward shaping.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>SFT benefits from higher-quality targets (GPT-4 outputs) and sets a better starting policy for RL; no experiments varying adapter rank or quantization beyond reported configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4431.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4431.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Evaluator / GPTScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-based Automatic Evaluator (GPTScore / LLM-as-evaluator approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using a strong LLM (GPT-4-Turbo) to automatically score generated syntheses across multiple qualitative criteria as a scalable proxy for human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4 Evaluator (GPTScore-like approach)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Authors used GPT-4-Turbo as an automatic evaluator with a detailed prompt encoding nine evaluation criteria and a 1–5 rating scale with tailored descriptions. Evaluator run three times per sample and averaged. Also referenced GPTScore and other LLM-eval approaches in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4-Turbo (used as evaluator) and gpt-4-1106-preview mentioned elsewhere as GPT-4 variant.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Evaluator ingests synthesis + original abstracts and outputs per-criterion numeric ratings and optional qualitative feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not a synthesis generator: used to judge syntheses and to provide reward signals (GPT-4 Features reward) during RLAIF.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluator rates syntheses generated from 5-paper prompts; evaluation run across 1,044 prompts and subsamples for human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Multidisciplinary.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Per-criterion numeric ratings (1–5), averaged across runs; optional textual justification.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Produces the nine qualitative criteria ratings used as training rewards; used alongside human crowdsourced ratings for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>GPT-4 evaluator produced consistent assessments across three runs; correlated reasonably with human evaluation trends (paper reports similar ordering between LLM and human scores).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to human Prolific survey ratings on subsample; also discussed other automatic metrics like ROUGE, BERTScore as less adequate.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>GPT-4 evaluator generally conservative on completeness and conciseness; overall alignment with humans claimed but per-criterion differences exist.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can be effective automatic evaluators for multi-dimensional synthesis quality and can be used as reward oracles in RL, improving scalability of preference learning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Potential evaluator biases, over-reliance on one LLM (GPT-4) as a gold standard, and the risk that reward-model weaknesses transfer to the generator.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Evaluator scales well to many samples (authors ran across >1k samples) and yields consistent scores across repeated runs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4431.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4431.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B (open-source LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 7-billion parameter LLM used as the base generator in experiments and as the primary target for SFT and RLAIF optimization in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Mistral-7B (Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used as the base model (Vanilla) for synthesis generation; the raw Mistral-7B-Instruct-v0.1 model produced lower-quality syntheses and format issues (often outputting paper structures), and was improved through SFT (QLoRA) and RLAIF.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Mistral-7B-Instruct-v0.1 (7B parameters, open-source).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Generates syntheses from concatenated titles and abstracts via prompt; no internal dedicated extractor beyond prompt-conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Multi-document single-paragraph synthesis; initially produced more paper-like outputs requiring structural penalties in reward function.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Generates syntheses from exactly 5 input papers per prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Multidisciplinary (tested across dataset domains).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Single-paragraph syntheses (intended), though Vanilla sometimes output multi-section paper-like text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Assessed with GPT-4 evaluator and human ratings across nine criteria; also measured basic format metrics (word count, paper-structure detection).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Vanilla Mistral had substantially lower scores than GPT-4 across key criteria (e.g., relevancy ~4.33 vs GPT-4 4.97; correctness ~3.66 vs GPT-4 4.93). High incidence (~32%) of paper-structured outputs before applying format controls.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to GPT-4 outputs, SFT-finetuned Mistral, RL-enhanced variants, and SFT+RLAIF.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Finite improvements after SFT and RLAIF; SFT+RLAIF closed part of the gap toward GPT-4-level performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Open-source models can be improved via SFT and RLAIF with GPT-4 feedback to produce higher-quality syntheses; structural penalties help eliminate paper-like outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Smaller model size leads to lower baseline performance and format hallucinations (paper structure generation); needs careful reward shaping to reach desired output format.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Authors note GPT-4 (proprietary, much larger) outperforms Mistral; SFT+RLAIF helps Mistral scale up effectiveness but full parity not reached.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4431.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4431.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Paper-structure detector & BasicReward</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paper-structure Identifier and Basic Feature Reward</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rule-based detector and a simple reward function that penalizes outputs that look like conventional research articles or violate word-count preferences, used to enforce the target single-paragraph 200-word format.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Paper-structure detector + Basic Features Reward</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Paper-structure detector: vocabulary of 17 academic structural terms and 9 regex reference identifiers to flag outputs resembling paper format (F1 combined ~98.67%). Basic reward R_basic(S) penalizes syntheses shorter than 50 words (-1.5), longer than 200 words (-1), those detected as paper-structured (-2), gives small penalty if within +/-20 words of 200 (-0.5), else reward +2. Used as a reward model in RLAIF to enforce formatting and brevity.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applied to outputs from Mistral-7B variants during RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not extraction; classifier applied to generated text to detect structure and references.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Used to shape rewards so generated syntheses conform to single-paragraph 200-word format.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Operates on outputs generated from 5-paper prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Multidisciplinary; format enforcement is domain-agnostic.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>No direct output; provides scalar reward for RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Detector performance reported: vocabulary-based identifier F1 67.80%, reference identifier F1 90.87%, combined F1 98.67%. Basic reward impacts word count distribution metrics reported in Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Using Basic Features in RL shifted average word counts toward desired ranges (e.g., RL w/ Basic Features avg ~189 words) and reduced incidence of paper-structured outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to Vanilla and SFT models without the basic reward shaping.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Basic reward reduced format errors present in Vanilla outputs (Vanilla paper-structure incidence ~32.47% vs SFT ~1.28% after interventions).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simple rule-based detectors combined with explicit reward shaping effectively enforce target format constraints and correct common failure modes (paper-like output).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Rule-based detectors can be brittle and domain/vocabulary dependent; may not catch all creative deviations or subtle structural hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Detector scales linearly with number of outputs; reward shaping effectively modulates model outputs regardless of domain but requires tuning of reward magnitudes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4431.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e4431.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-eval & LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-eval / LLM-as-a-judge Evaluation Approaches</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Collection of approaches that use a single or multiple LLM prompts to automatically evaluate generated text's quality by correlating LLM judgments with human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-eval / LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced evaluation paradigms (e.g., LLM-Eval, GPTScore, LLM-as-a-judge) wherein large LLMs are prompted to provide multi-dimensional assessments of generated texts; authors cite these as justification for using GPT-4 as evaluator and as reward source in RLAIF.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Typically GPT-4 or other strong LLMs (as per cited works); in this paper GPT-4-Turbo is used to implement the pattern.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Evaluator ingests candidate synthesis plus source abstracts and outputs criterion-wise scores or natural-language critique.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not a synthesis method; evaluation approach for assessing synthesized outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to syntheses derived from 5-paper inputs in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General / multidisciplinary.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Numeric quality scores per criterion and textual evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Designed to emulate or replace human ratings across multiple axes (relevancy, correctness, completeness, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Authors found GPT-4 evaluator to be consistent across multiple runs and broadly aligned with human survey trends; used evaluator outputs to drive RL rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared implicitly to traditional metrics (ROUGE family) and to human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LLM-based evaluators provided more semantic, multi-dimensional assessments than ROUGE and correlated with human judgments on many criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can be effective, scalable evaluators for synthesis quality and can be used in automated reward modeling for preference optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Risk of evaluator bias; evaluator is another black-box model and can propagate systematic errors into the generator when used as reward source.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Authors ran evaluator at scale (~1k samples) showing consistent ratings; cost and API access to strong LLMs remain scaling constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering <em>(Rating: 2)</em></li>
                <li>Large Language Models as Evaluators for Scientific Synthesis <em>(Rating: 2)</em></li>
                <li>Gptscore: Evaluate as you desire <em>(Rating: 2)</em></li>
                <li>LLM-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models <em>(Rating: 2)</em></li>
                <li>QLoRA: Efficient Finetuning of Quantized LLMs <em>(Rating: 2)</em></li>
                <li>Proximal Policy Optimization Algorithms <em>(Rating: 2)</em></li>
                <li>Learning to summarize from human feedback <em>(Rating: 1)</em></li>
                <li>Constitutional AI: Harmlessness from AI feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4431",
    "paper_id": "paper-272968899",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "LLMs4Synthesis",
            "name_full": "LLMs4Synthesis Framework",
            "brief_description": "An end-to-end framework introduced in this paper that adapts LLMs to generate high-quality scientific syntheses by combining dataset standardization, supervised fine-tuning, and reinforcement learning with AI feedback (RLAIF).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLMs4Synthesis",
            "system_description": "Pipeline that (1) standardizes multi-paper inputs (titles + abstracts from exactly five papers) using ORKG-derived prompts; (2) optionally performs supervised fine-tuning (SFT) of an open-source base LLM (Mistral-7B) using GPT-4-generated syntheses as pseudo-ground-truth; (3) applies reinforcement learning with AI feedback (RLAIF) using PPO to optimize generation under reward models that include basic structural rewards and GPT-4-based qualitative rewards; and (4) includes a KL-penalty to constrain policy drift from the SFT model. The framework defines three synthesis types (paper-wise, methodological, thematic) and nine qualitative evaluation criteria used in the reward model.",
            "llm_model_used": "Mistral-7B (Mistral-7B-Instruct-v0.1) as base generator; GPT-4 (gpt-4-1106-preview) and GPT-4-Turbo used as gold standard / evaluator in training and reward modeling.",
            "extraction_technique": "Structured prompting using ORKG Comparisons: titles and abstracts of exactly five papers are concatenated into a standardized prompt template; no explicit IR embedding stage described beyond ORKG and open-access metadata retrieval.",
            "synthesis_technique": "Multi-document synthesis (single-paragraph up to 200 words) via supervised fine-tuning (SFT with QLoRA) followed by reinforcement learning (PPO) guided by reward models (basic format/length penalties and GPT-4 feature-based qualitative scoring) to integrate and compress findings across papers.",
            "number_of_papers": "Exactly 5 papers per synthesis (fixed by dataset construction and prompt design).",
            "domain_or_topic": "Multidisciplinary (dataset drawn from ORKG across domains; examples include Computer Science, Chemistry, Earth Science, Linguistics, Sociology).",
            "output_type": "Single-paragraph scientific syntheses (paper-wise, methodological, or thematic), limited to 200 words and with inline citations referencing the input papers.",
            "evaluation_metrics": "Nine qualitative criteria scored 1–5 by GPT-4-Turbo and human evaluators: relevancy, correctness, completeness, informativeness, integration, cohesion, coherence, readability, conciseness; plus basic structural metrics (word-count bands, paper-structure detector).",
            "performance_results": "SFT+RLAIF (w/ GPT-4 Features) achieved high GPT-4-evaluator scores (e.g., relevancy ~4.88, correctness ~4.75, completeness ~4.19, informativeness ~4.83, integration/cohesion ~4.89, coherence ~4.85, readability ~4.78; conciseness lower ~3.64). GPT-4-generated syntheses (gold standard) scored higher (relevancy 4.97, correctness 4.93, completeness 4.42). Average word counts: GPT-4 outputs ~218 words, SFT+RLAIF ~205 words; RL models with Basic Features produced averages close to 189–204 words depending on configuration. (Values from Table 3 and text.)",
            "comparison_baseline": "Compared against Vanilla (unfine-tuned Mistral-7B), SFT-only (QLoRA fine-tuned Mistral), RL-only variants (with Basic Features), and GPT-4 as gold standard.",
            "performance_vs_baseline": "SFT+RLAIF (w/ GPT-4 Features) substantially improved over Vanilla and SFT baselines on most qualitative metrics (e.g., relevancy: 4.88 vs Vanilla 4.33; correctness: 4.75 vs Vanilla 3.66). RL with Basic Features improved format/word-count adherence (e.g., RL w/ Basic Features avg word count ~189 vs Vanilla 242).",
            "key_findings": "Combining SFT (with QLoRA) and RLAIF using a GPT-4-based reward yields syntheses that are more relevant, correct, integrated, and coherent than vanilla or SFT-only models while also increasing output consistency. The use of GPT-4 as a reward/evaluator effectively aligns generation with human-like quality judgments. Basic structural rewards (word-count, paper-structure detector) fix common format failures.",
            "limitations_challenges": "Remaining issues include conciseness/verbosity, occasional format regression (paper-like outputs from some models), domain gaps in humanities/social sciences (weaker performance in Linguistics and Sociology), dependence on GPT-4 as a surrogate human evaluator (potential evaluator bias), computational cost for RL finetuning, and reliance on only titles+abstracts (no full-text evidence).",
            "scaling_behavior": "Paper reports that larger proprietary LLM (GPT-4) outperforms smaller open-source Mistral-7B; SFT+RLAIF narrows the gap. Dataset uses fixed 5-paper inputs; no explicit experiments varying number of papers, but performance improves with SFT+RLAIF fine-tuning and GPT-4-based reward shaping. KL penalty (λ=0.2) used to control policy drift during RL.",
            "uuid": "e4431.0",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ORKG Synthesis Dataset",
            "name_full": "Open Research Knowledge Graph Scientific Synthesis Generation Dataset",
            "brief_description": "A multidisciplinary dataset constructed from ORKG Comparisons containing samples of five-paper groups (titles + abstracts) and generated syntheses for research on scientific synthesis generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ORKG Scientific Synthesis Dataset",
            "system_description": "Collected via ORKG Comparisons: extracted research problems and associated papers; filtered to Comparisons with ≥5 papers and available abstracts; grouped all papers into collections of five to produce standardized synthesis prompts. Each sample contains sample ID, research field, research problem, titles, abstracts, DOIs, and three synthesis types per LLM.",
            "llm_model_used": "Used as input for synthesis generation by Mistral-7B and GPT-4 in experiments (dataset itself is not an LLM).",
            "extraction_technique": "Dataset extraction from ORKG Comparisons and open-access metadata sources (Semantic Scholar, Crossref, CORE) to retrieve titles and abstracts; no embedding/retrieval in dataset construction stage.",
            "synthesis_technique": "Used as content for multi-document synthesis (single-paragraph, 200-word) via prompting to LLMs.",
            "number_of_papers": "348 final samples (each sample uses exactly 5 papers); resulted in 1,044 standardized prompts and 1,044 syntheses per model (total 2,088 syntheses across two models).",
            "domain_or_topic": "Multidisciplinary; top fields include Computer Science (125 samples), Physics, Animal Sciences, Chemistry, Urban Studies, Earth Sciences, etc.",
            "output_type": "Structured prompt records and LLM-generated single-paragraph syntheses (paper-wise, methodological, thematic).",
            "evaluation_metrics": "Nine qualitative criteria (1–5) by GPT-4-Turbo and human crowdsourced evaluations; basic format metrics (word count bands, paper-structure presence).",
            "performance_results": "Dataset enabled fine-tuning experiments and evaluation; used to produce 1,044 syntheses per model and to train/test SFT and RLAIF models (split: 80% train, 20% test by domain).",
            "comparison_baseline": "Not a method; used as standardized corpus to compare Mistral and GPT-4 outputs and to train SFT/RL variants.",
            "performance_vs_baseline": "N/A (dataset resource).",
            "key_findings": "Crowdsourced ORKG Comparisons provide a larger and more multidisciplinary corpus than previous manually curated datasets; a fixed 5-paper grouping aligns with practice of search systems returning top-5 results.",
            "limitations_challenges": "Only titles and abstracts used (no full texts), variability in crowdsourced research-problem descriptions, duplicated samples across fields removed by random selection, and potential domain imbalance.",
            "scaling_behavior": "Dataset scale (348 samples) allowed SFT and RL training; authors claim dataset is ~3x larger than prior work but no experiments varying dataset scale were reported.",
            "uuid": "e4431.1",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ORKG Ask",
            "name_full": "ORKG Ask (Open Research Knowledge Graph Ask)",
            "brief_description": "An open platform / next-generation search system referenced in the paper that integrates LLMs to enhance search, natural-language queries, semantic search, and AI-driven extractions over scientific literature.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "ORKG Ask",
            "system_description": "Described as an open platform capable of semantic, natural language queries and AI-driven extractions; noted as compatible with integration of LLMs4Synthesis to enhance synthesis capabilities and promote open access and collaboration.",
            "llm_model_used": "Not specified in paper (platform mention).",
            "extraction_technique": "Semantic search and AI-driven extraction (as described in the paper's related-work discussion).",
            "synthesis_technique": "Platform-level synthesis and Q&A features; can integrate external LLM-based synthesis modules like LLMs4Synthesis.",
            "number_of_papers": "Typical behavior described as operating over top-N search results (authors note top-5 as a typical synthesis basis); exact limits not specified.",
            "domain_or_topic": "General scholarly / multidisciplinary.",
            "output_type": "Search answers, syntheses (interactive Q&A), and extracted structured knowledge.",
            "evaluation_metrics": "Not specified in this paper.",
            "performance_results": "Not evaluated in this paper.",
            "comparison_baseline": "Mentioned as an example of modern search engines; no experimental comparison in paper.",
            "performance_vs_baseline": "N/A",
            "key_findings": "Cited to illustrate how LLMs are already integrated into scientific search tools enabling natural-language queries and extraction workflows; authors propose LLMs4Synthesis could be embedded into ORKG Ask.",
            "limitations_challenges": "Not discussed in detail here (platform mentioned in context).",
            "scaling_behavior": "Not discussed in this paper.",
            "uuid": "e4431.2",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Elicit",
            "name_full": "Elicit (Research Assistant)",
            "brief_description": "A commercial/academic search-assisted platform referenced as an example of systems that use LLMs to accelerate literature access and produce syntheses from top search results.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Elicit",
            "system_description": "Mentioned as an example of next-generation search engines that leverage LLMs for natural language queries, semantic search, and automated extraction to produce syntheses (often from top-5 search results).",
            "llm_model_used": "Not specified in this paper.",
            "extraction_technique": "AI-driven extraction, semantic retrieval (platform-level descriptions).",
            "synthesis_technique": "Automated synthesis from search results; multi-document condensation (not detailed in this paper).",
            "number_of_papers": "Typical practice referenced: top-5 search results used for syntheses.",
            "domain_or_topic": "General scientific literature.",
            "output_type": "Literature syntheses, answers to research questions.",
            "evaluation_metrics": "Not specified here.",
            "performance_results": "Not provided in this paper.",
            "comparison_baseline": "Mentioned as related work; no baseline comparison.",
            "performance_vs_baseline": "N/A",
            "key_findings": "Cited to contextualize the demand for automated synthesis and the common practice of synthesizing top search hits.",
            "limitations_challenges": "Not discussed here.",
            "scaling_behavior": "Not discussed here.",
            "uuid": "e4431.3",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SciSpace",
            "name_full": "SciSpace (SciSpace research platform)",
            "brief_description": "A commercial research platform referenced as an instance where LLMs enhance search capabilities and user interactions over scientific literature.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "SciSpace",
            "system_description": "Mentioned among modern platforms that use LLMs for improved search, interactive queries, and AI-driven extraction of paper content to aid users in literature synthesis.",
            "llm_model_used": "Not specified in this paper.",
            "extraction_technique": "AI-driven extraction and semantic retrieval (platform-level).",
            "synthesis_technique": "Automated summarization/synthesis features integrated into the platform (not detailed here).",
            "number_of_papers": "Not specified.",
            "domain_or_topic": "General research literature.",
            "output_type": "Search-assistant outputs and extractive summaries.",
            "evaluation_metrics": "Not provided.",
            "performance_results": "Not provided in this paper.",
            "comparison_baseline": "Mentioned as related work only.",
            "performance_vs_baseline": "N/A",
            "key_findings": "Used to motivate the practical need for LLM-based synthesis tools.",
            "limitations_challenges": "Not discussed in this paper.",
            "scaling_behavior": "Not discussed here.",
            "uuid": "e4431.4",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "CORE-GPT",
            "name_full": "CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering",
            "brief_description": "A referenced system combining open-access research corpora with LLMs to produce credible question-answering over scholarly content.",
            "citation_title": "CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering",
            "mention_or_use": "mention",
            "system_name": "CORE-GPT",
            "system_description": "Prior work that integrates open-access repositories (CORE) with LLMs to produce QA and likely extract/summarize scientific content by grounding LLM responses in retrieved open-access documents.",
            "llm_model_used": "Not specified in this paper; original CORE-GPT work uses LLMs (reference in bibliography).",
            "extraction_technique": "Retrieval over open-access papers + grounding of LLM outputs in retrieved documents (paper references indicate an approach for trustworthy QA).",
            "synthesis_technique": "Document-grounded answering and summarization; likely involves retrieval-augmented generation (RAG) patterns (paper cited for context).",
            "number_of_papers": "Not specified here.",
            "domain_or_topic": "General open-access research (multidisciplinary).",
            "output_type": "Question answering and grounded summaries.",
            "evaluation_metrics": "Not specified here; cited as related work.",
            "performance_results": "Not reported in this paper.",
            "comparison_baseline": "Mentioned as prior relevant system (no baseline details here).",
            "performance_vs_baseline": "N/A",
            "key_findings": "Cited to highlight approaches that combine retrieval from open-access corpora and LLMs to improve credibility/trustworthiness of generated scientific answers.",
            "limitations_challenges": "Not analyzed in detail in this paper.",
            "scaling_behavior": "Not discussed here.",
            "uuid": "e4431.5",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "RLAIF",
            "name_full": "Reinforcement Learning with AI Feedback (RLAIF)",
            "brief_description": "A training paradigm where reinforcement learning is guided by automated (AI) evaluators as reward sources rather than (or in addition to) human annotators.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "RLAIF (Reinforcement Learning with AI Feedback)",
            "system_description": "Used in LLMs4Synthesis: after SFT, policy is fine-tuned with PPO where reward models are either rule-based (Basic Features) or GPT-4-based qualitative scoring (GPT-4 Features). The RL loop alternates generation and scoring; a KL-divergence penalty term (λ=0.2) constrains divergence from the base SFT policy. Optimization uses Adam with a reported LR ~2.94e-5 and PPO training with 10 epochs per batch.",
            "llm_model_used": "Policy initialized from SFT Mistral-7B model; reward/evaluator is GPT-4-Turbo (proxy human feedback).",
            "extraction_technique": "Not an extraction technique per se; RLAIF consumes standardized prompts (titles + abstracts) and produces syntheses that are scored for reward.",
            "synthesis_technique": "Policy generates single-paragraph syntheses; reward models score outputs to guide learning toward preferred synthesis qualities.",
            "number_of_papers": "Training uses standardized prompts derived from 5-paper inputs; training set includes 810 training prompts for RL partition.",
            "domain_or_topic": "Multidisciplinary (same dataset domains).",
            "output_type": "Optimized LLM synthesizer producing paragraph syntheses.",
            "evaluation_metrics": "Rewards based on Basic Features (word count, paper-structure penalty) and GPT-4-derived scores over nine qualitative criteria; final evaluation also uses GPT-4 and human ratings.",
            "performance_results": "RLAIF variants with GPT-4 Features improved qualitative scores compared to Vanilla and SFT baselines; SFT+RLAIF (w/ GPT-4 Features) approximated GPT-4-level outputs and increased output consistency across repeated runs (lower variance).",
            "comparison_baseline": "Compared to Vanilla Mistral, SFT-only, RL with Basic Features, and GPT-4 gold standard.",
            "performance_vs_baseline": "SFT+RLAIF (w/ GPT-4 Features) outperformed Vanilla and SFT across relevancy, correctness, completeness, integration, cohesion, and coherence (see Table 3 for numeric comparisons).",
            "key_findings": "AI-generated feedback (GPT-4) is an effective proxy for human feedback in RL fine-tuning to align generation with multi-dimensional quality criteria and increase stability.",
            "limitations_challenges": "Using an LLM as reward model risks encoding evaluator biases; reliance on GPT-4 as the reward oracle may limit openness and reproducibility; computational cost of RL finetuning; limited understanding of generalization outside the specific 5-paper prompt format.",
            "scaling_behavior": "Authors observed that adding GPT-4-based feedback improves metrics; larger LLMs (GPT-4) perform better initially, but RLAIF helps smaller open-source models (Mistral-7B) close the gap.",
            "uuid": "e4431.6",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SFT + QLoRA",
            "name_full": "Supervised Fine-Tuning with QLoRA",
            "brief_description": "Supervised fine-tuning of the base LLM using GPT-4-generated syntheses as pseudo-ground-truth, implemented with quantized LoRA adapters (QLoRA) to reduce resource requirements.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "SFT (Supervised Fine-Tuning) + QLoRA",
            "system_description": "Fine-tunes Mistral-7B using GPT-4 outputs as target texts and a standardized prompt template. Uses QLoRA (4-bit quantized low-rank adapters) to enable efficient finetuning; adapter rank r=8, scaling factor and dropout reported; training used AdamW with LR 2e-4, gradient accumulation, warmup and 5 epochs.",
            "llm_model_used": "Mistral-7B (base) fine-tuned via QLoRA; GPT-4 outputs used as ground-truth targets.",
            "extraction_technique": "Training data constructed from ORKG-sourced titles and abstracts; no additional extraction beyond prompt formatting.",
            "synthesis_technique": "SFT teaches the model to map 5-paper inputs to target single-paragraph syntheses (three synthesis types) prior to RL.",
            "number_of_papers": "Trained on standardized prompts representing five-paper groups; Train-LLM split contained 405 synthesis prompts (from 135 comparisons).",
            "domain_or_topic": "Multidisciplinary (dataset splits by domain).",
            "output_type": "Single-paragraph syntheses matching GPT-4 style.",
            "evaluation_metrics": "Qualitative nine-criteria scores via GPT-4 and human evaluation; word count and structure metrics.",
            "performance_results": "SFT improved format and some qualitative aspects over Vanilla but underperformed relative to SFT+RLAIF and GPT-4 on many semantic metrics; average word count for SFT ~231 words (improvement over Vanilla but still above 200).",
            "comparison_baseline": "Compared against Vanilla (no finetuning), RL-only variants, and combined SFT+RLAIF.",
            "performance_vs_baseline": "SFT improves some metrics vs Vanilla (e.g., lower paper-structure incidents), but combining SFT with RLAIF yields much larger quality gains.",
            "key_findings": "Using QLoRA makes SFT feasible on resource-constrained open models and provides a better initialization for subsequent RLAIF.",
            "limitations_challenges": "SFT alone insufficient to enforce multi-dimensional quality constraints (e.g., conciseness, integration) without RL and reward shaping.",
            "scaling_behavior": "SFT benefits from higher-quality targets (GPT-4 outputs) and sets a better starting policy for RL; no experiments varying adapter rank or quantization beyond reported configuration.",
            "uuid": "e4431.7",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT-4 Evaluator / GPTScore",
            "name_full": "GPT-4-based Automatic Evaluator (GPTScore / LLM-as-evaluator approaches)",
            "brief_description": "Using a strong LLM (GPT-4-Turbo) to automatically score generated syntheses across multiple qualitative criteria as a scalable proxy for human evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4 Evaluator (GPTScore-like approach)",
            "system_description": "Authors used GPT-4-Turbo as an automatic evaluator with a detailed prompt encoding nine evaluation criteria and a 1–5 rating scale with tailored descriptions. Evaluator run three times per sample and averaged. Also referenced GPTScore and other LLM-eval approaches in related work.",
            "llm_model_used": "GPT-4-Turbo (used as evaluator) and gpt-4-1106-preview mentioned elsewhere as GPT-4 variant.",
            "extraction_technique": "Evaluator ingests synthesis + original abstracts and outputs per-criterion numeric ratings and optional qualitative feedback.",
            "synthesis_technique": "Not a synthesis generator: used to judge syntheses and to provide reward signals (GPT-4 Features reward) during RLAIF.",
            "number_of_papers": "Evaluator rates syntheses generated from 5-paper prompts; evaluation run across 1,044 prompts and subsamples for human evaluation.",
            "domain_or_topic": "Multidisciplinary.",
            "output_type": "Per-criterion numeric ratings (1–5), averaged across runs; optional textual justification.",
            "evaluation_metrics": "Produces the nine qualitative criteria ratings used as training rewards; used alongside human crowdsourced ratings for validation.",
            "performance_results": "GPT-4 evaluator produced consistent assessments across three runs; correlated reasonably with human evaluation trends (paper reports similar ordering between LLM and human scores).",
            "comparison_baseline": "Compared to human Prolific survey ratings on subsample; also discussed other automatic metrics like ROUGE, BERTScore as less adequate.",
            "performance_vs_baseline": "GPT-4 evaluator generally conservative on completeness and conciseness; overall alignment with humans claimed but per-criterion differences exist.",
            "key_findings": "LLMs can be effective automatic evaluators for multi-dimensional synthesis quality and can be used as reward oracles in RL, improving scalability of preference learning.",
            "limitations_challenges": "Potential evaluator biases, over-reliance on one LLM (GPT-4) as a gold standard, and the risk that reward-model weaknesses transfer to the generator.",
            "scaling_behavior": "Evaluator scales well to many samples (authors ran across &gt;1k samples) and yields consistent scores across repeated runs.",
            "uuid": "e4431.8",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Mistral-7B",
            "name_full": "Mistral-7B (open-source LLM)",
            "brief_description": "An open-source 7-billion parameter LLM used as the base generator in experiments and as the primary target for SFT and RLAIF optimization in this paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Mistral-7B (Instruct)",
            "system_description": "Used as the base model (Vanilla) for synthesis generation; the raw Mistral-7B-Instruct-v0.1 model produced lower-quality syntheses and format issues (often outputting paper structures), and was improved through SFT (QLoRA) and RLAIF.",
            "llm_model_used": "Mistral-7B-Instruct-v0.1 (7B parameters, open-source).",
            "extraction_technique": "Generates syntheses from concatenated titles and abstracts via prompt; no internal dedicated extractor beyond prompt-conditioning.",
            "synthesis_technique": "Multi-document single-paragraph synthesis; initially produced more paper-like outputs requiring structural penalties in reward function.",
            "number_of_papers": "Generates syntheses from exactly 5 input papers per prompt.",
            "domain_or_topic": "Multidisciplinary (tested across dataset domains).",
            "output_type": "Single-paragraph syntheses (intended), though Vanilla sometimes output multi-section paper-like text.",
            "evaluation_metrics": "Assessed with GPT-4 evaluator and human ratings across nine criteria; also measured basic format metrics (word count, paper-structure detection).",
            "performance_results": "Vanilla Mistral had substantially lower scores than GPT-4 across key criteria (e.g., relevancy ~4.33 vs GPT-4 4.97; correctness ~3.66 vs GPT-4 4.93). High incidence (~32%) of paper-structured outputs before applying format controls.",
            "comparison_baseline": "Compared to GPT-4 outputs, SFT-finetuned Mistral, RL-enhanced variants, and SFT+RLAIF.",
            "performance_vs_baseline": "Finite improvements after SFT and RLAIF; SFT+RLAIF closed part of the gap toward GPT-4-level performance.",
            "key_findings": "Open-source models can be improved via SFT and RLAIF with GPT-4 feedback to produce higher-quality syntheses; structural penalties help eliminate paper-like outputs.",
            "limitations_challenges": "Smaller model size leads to lower baseline performance and format hallucinations (paper structure generation); needs careful reward shaping to reach desired output format.",
            "scaling_behavior": "Authors note GPT-4 (proprietary, much larger) outperforms Mistral; SFT+RLAIF helps Mistral scale up effectiveness but full parity not reached.",
            "uuid": "e4431.9",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Paper-structure detector & BasicReward",
            "name_full": "Paper-structure Identifier and Basic Feature Reward",
            "brief_description": "A rule-based detector and a simple reward function that penalizes outputs that look like conventional research articles or violate word-count preferences, used to enforce the target single-paragraph 200-word format.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Paper-structure detector + Basic Features Reward",
            "system_description": "Paper-structure detector: vocabulary of 17 academic structural terms and 9 regex reference identifiers to flag outputs resembling paper format (F1 combined ~98.67%). Basic reward R_basic(S) penalizes syntheses shorter than 50 words (-1.5), longer than 200 words (-1), those detected as paper-structured (-2), gives small penalty if within +/-20 words of 200 (-0.5), else reward +2. Used as a reward model in RLAIF to enforce formatting and brevity.",
            "llm_model_used": "Applied to outputs from Mistral-7B variants during RL training.",
            "extraction_technique": "Not extraction; classifier applied to generated text to detect structure and references.",
            "synthesis_technique": "Used to shape rewards so generated syntheses conform to single-paragraph 200-word format.",
            "number_of_papers": "Operates on outputs generated from 5-paper prompts.",
            "domain_or_topic": "Multidisciplinary; format enforcement is domain-agnostic.",
            "output_type": "No direct output; provides scalar reward for RL training.",
            "evaluation_metrics": "Detector performance reported: vocabulary-based identifier F1 67.80%, reference identifier F1 90.87%, combined F1 98.67%. Basic reward impacts word count distribution metrics reported in Table 3.",
            "performance_results": "Using Basic Features in RL shifted average word counts toward desired ranges (e.g., RL w/ Basic Features avg ~189 words) and reduced incidence of paper-structured outputs.",
            "comparison_baseline": "Compared to Vanilla and SFT models without the basic reward shaping.",
            "performance_vs_baseline": "Basic reward reduced format errors present in Vanilla outputs (Vanilla paper-structure incidence ~32.47% vs SFT ~1.28% after interventions).",
            "key_findings": "Simple rule-based detectors combined with explicit reward shaping effectively enforce target format constraints and correct common failure modes (paper-like output).",
            "limitations_challenges": "Rule-based detectors can be brittle and domain/vocabulary dependent; may not catch all creative deviations or subtle structural hallucinations.",
            "scaling_behavior": "Detector scales linearly with number of outputs; reward shaping effectively modulates model outputs regardless of domain but requires tuning of reward magnitudes.",
            "uuid": "e4431.10",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LLM-eval & LLM-as-a-judge",
            "name_full": "LLM-eval / LLM-as-a-judge Evaluation Approaches",
            "brief_description": "Collection of approaches that use a single or multiple LLM prompts to automatically evaluate generated text's quality by correlating LLM judgments with human ratings.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "LLM-eval / LLM-as-a-judge",
            "system_description": "Referenced evaluation paradigms (e.g., LLM-Eval, GPTScore, LLM-as-a-judge) wherein large LLMs are prompted to provide multi-dimensional assessments of generated texts; authors cite these as justification for using GPT-4 as evaluator and as reward source in RLAIF.",
            "llm_model_used": "Typically GPT-4 or other strong LLMs (as per cited works); in this paper GPT-4-Turbo is used to implement the pattern.",
            "extraction_technique": "Evaluator ingests candidate synthesis plus source abstracts and outputs criterion-wise scores or natural-language critique.",
            "synthesis_technique": "Not a synthesis method; evaluation approach for assessing synthesized outputs.",
            "number_of_papers": "Applied to syntheses derived from 5-paper inputs in this study.",
            "domain_or_topic": "General / multidisciplinary.",
            "output_type": "Numeric quality scores per criterion and textual evaluations.",
            "evaluation_metrics": "Designed to emulate or replace human ratings across multiple axes (relevancy, correctness, completeness, etc.).",
            "performance_results": "Authors found GPT-4 evaluator to be consistent across multiple runs and broadly aligned with human survey trends; used evaluator outputs to drive RL rewards.",
            "comparison_baseline": "Compared implicitly to traditional metrics (ROUGE family) and to human ratings.",
            "performance_vs_baseline": "LLM-based evaluators provided more semantic, multi-dimensional assessments than ROUGE and correlated with human judgments on many criteria.",
            "key_findings": "LLMs can be effective, scalable evaluators for synthesis quality and can be used in automated reward modeling for preference optimization.",
            "limitations_challenges": "Risk of evaluator bias; evaluator is another black-box model and can propagate systematic errors into the generator when used as reward source.",
            "scaling_behavior": "Authors ran evaluator at scale (~1k samples) showing consistent ratings; cost and API access to strong LLMs remain scaling constraints.",
            "uuid": "e4431.11",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering",
            "rating": 2,
            "sanitized_title": "coregpt_combining_open_access_research_and_large_language_models_for_credible_trustworthy_question_answering"
        },
        {
            "paper_title": "Large Language Models as Evaluators for Scientific Synthesis",
            "rating": 2,
            "sanitized_title": "large_language_models_as_evaluators_for_scientific_synthesis"
        },
        {
            "paper_title": "Gptscore: Evaluate as you desire",
            "rating": 2,
            "sanitized_title": "gptscore_evaluate_as_you_desire"
        },
        {
            "paper_title": "LLM-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models",
            "rating": 2,
            "sanitized_title": "llmeval_unified_multidimensional_automatic_evaluation_for_opendomain_conversations_with_large_language_models"
        },
        {
            "paper_title": "QLoRA: Efficient Finetuning of Quantized LLMs",
            "rating": 2,
            "sanitized_title": "qlora_efficient_finetuning_of_quantized_llms"
        },
        {
            "paper_title": "Proximal Policy Optimization Algorithms",
            "rating": 2,
            "sanitized_title": "proximal_policy_optimization_algorithms"
        },
        {
            "paper_title": "Learning to summarize from human feedback",
            "rating": 1,
            "sanitized_title": "learning_to_summarize_from_human_feedback"
        },
        {
            "paper_title": "Constitutional AI: Harmlessness from AI feedback",
            "rating": 1,
            "sanitized_title": "constitutional_ai_harmlessness_from_ai_feedback"
        }
    ],
    "cost": 0.024170999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis
27 Sep 2024</p>
<p>Hamed Babaei Giglou 
TIB Leibniz Information Centre for Science and Technology Hannover
Germany</p>
<p>Jennifer D ' Souza jennifer.dsouza@tib.eu 
TIB Leibniz Information Centre for Science and Technology Hannover
Germany</p>
<p>Sören Auer auer@tib.eu 
TIB Leibniz Information Centre for Science and Technology Hannover
Germany</p>
<p>LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis
27 Sep 2024C20ACA2332C25EF56FFEEA85AB0CE257arXiv:2409.18812v1[cs.CL]CCS ConceptsInformation systems → Information systems applications• Computing methodologies → Natural language generationLanguage resourcesNatural language processingSupervised learningReinforcement learning Scientific Synthesis Generation, Large Language Models, Language Model-based Evaluation Framework, Reinforcement Learning
In response to the growing complexity and volume of scientific literature, this paper introduces the LLMs4Synthesis framework, designed to enhance the capabilities of Large Language Models (LLMs) in generating high-quality scientific syntheses.This framework addresses the need for rapid, coherent, and contextually rich integration of scientific insights, leveraging both open-source and proprietary LLMs.It also examines the effectiveness of LLMs in evaluating the integrity and reliability of these syntheses, alleviating inadequacies in current quantitative metrics.Our study contributes to this field by developing a novel methodology for processing scientific papers, defining new synthesis types, and establishing nine detailed quality criteria for evaluating syntheses.The integration of LLMs with reinforcement learning and AI feedback is proposed to optimize synthesis quality, ensuring alignment with established criteria.The LLMs4Synthesis framework and its components are made available, promising to enhance both the generation and evaluation processes in scientific research synthesis.</p>
<p>Introduction</p>
<p>In recent years, the intersection of language processing and scientific research has witnessed major advancements, largely fueled by the capabilities of Large Language Models (LLMs) [9,15].These models, including open-source (e.g.BERT [14] or Mistral [22]), or proprietary technologies (e.g.GPT-4 [1]), and their derivatives, have not only redefined the boundaries of text generation and comprehension [40] but have also paved the way for innovative applications in the synthesis of scientific content [17,34].To enable the automated and accurate generation of scientific syntheses, which are concise integrations of key insights from multiple scientific articles, this paper introduces the comprehensive LLMs4Synthesis framework.It is designed to enhance open-source LLMs so that they can generate scientific syntheses of comparable quality to those produced by significantly larger proprietary models.</p>
<p>The motivation behind this endeavor is driven by the increasing complexity and volume of scientific literature [8,18,44], which poses challenges for researchers seeking timely and comprehensive knowledge syntheses.Traditional manual synthesis methods are time-consuming and struggle to keep up with the rapid dissemination of new research.The LLMs4Synthesis framework addresses these challenges by leveraging LLMs to produce concise, coherent, and contextually rich scientific summaries, helping the scientific community stay abreast of expanding knowledge frontiers.Furthermore, next-generation search engines like ORKG Ask [33], Elicit [16], and SciSpace [38] demonstrate how LLMs enhance search capabilities and user interactions.These platforms utilize advanced features such as natural language queries, semantic search, and AI-driven extractions to transform scientific literature access.ORKG Ask is an open platform, allowing for seamless integration of LLMs4Synthesis, enhancing synthesis abilities, and promoting open access and collaboration in the scientific community.</p>
<p>Accurate evaluation of scientific syntheses is essential to maintain their integrity and reliability.Recent advancements suggest that LLMs can generate these syntheses [16,33,34,38], yet their effectiveness across domains and capability in evaluating them is not well understood.Merely employing existing quantitative metrics, like the rouge family [27], commonly applied in similar text generation scenarios, raise concerns about their adequacy [3,12,17,24] in capturing the full extent of the semantic intricacies associated with the quality evaluation of syntheses.Instead, our research examines the effectiveness of LLMs in assessing scientific syntheses, contributing to a significant methodological shift where language understanding tools are increasingly observed as effective evaluators [2,7].Moreover, related work [19,28,47] shows that using LLMs as evaluators facilitates intricate versatility and detail in assessments across multiple dimensions and scales.Consequently, our study aims to define a comprehensive set of fine-grained evaluation attributes to address critical synthesis issues like irrelevancy, inaccuracy, incompleteness, redundancy, disorganization, incoherence, poor readability, and verbosity, which significantly affect the quality and clarity of syntheses.Additionally, this study explores using LLM evaluator responses as AI feedback in reinforcement learning settings (RLAIF) [5,11] to enhance the alignment of a model's unsupervised learned parameters [35] with the quality of scientific synthesis generation.This approach moves beyond traditional reliance on human evaluators and ground truth data.</p>
<p>In essence, this research aims to equip LLMs with the ability to identify and improve upon their synthesis generation quality, thereby enhancing their learning and output reliability.The study focuses on the following research questions (RQs).RQ1: What features are essential for qualitatively evaluating the informativeness and meaningfulness of a scientific synthesis?RQ2: How can LLMs be fine-tuned to adhere to specific formatting standards like word limits and paragraph structure in scientific syntheses?RQ3: How can the balance be achieved in LLMs between maintaining formatting standards and ensuring semantically informative and meaningful scientific syntheses?</p>
<p>This paper makes several contributions.First, we develop a methodology to collect and process scientific papers into a format ready for synthesis using the Open Research Knowledge Graph [4], a multidisciplinary platform that facilitates the comparison of scientific contributions [32].Second, we introduce new synthesis types -paper-wise, methodological, and thematic --that focus on different aspects of the extracted insights.Utilizing Mistral-7B [22] and GPT-4 [1], we generate a large-scale dataset of these syntheses, which is publicly available https://github.com/jd-coderepos/scisynthesis.Third, we establish nine quality criteria for evaluating these syntheses, assessed by both an automated LLM evaluator (GPT-4) and a human-crowdsourced survey.These evaluations provide insights, i.e. at type, domain, LLM levels, in the research community at large, and also help inform the LLMs4Synthesis framework, to enhance LLM scientific synthesis generation task performance.Finally, we propose the LLMs4Synthesis framework, which incorporates RLAIF [5,11] to optimize LLMs for synthesis generation, ensuring alignment with established quality standards.The framework and its source code are publicly accessible https://github.com/HamedBabaei/LLMs4Synthesis.</p>
<p>The rest of the paper is organized as follows.Section 2 details the multidisciplinary ORKG synthesis dataset.Section 3 introduces comprehensive evaluation attributes and assesses synthesis quality through both automatic and human evaluations, providing performance comparisons between proprietary and open-source LLMs.Section 4 elaborates on the LLMs4Synthesis framework, with its evaluations in section 5. Section 6 explores the broader implications of our findings, and section 7 concludes the paper, summarizing our contributions and suggesting future work.</p>
<p>The ORKG Scientific Synthesis Dataset</p>
<p>In this work, we broaden the scope of the dataset collection for scientific synthesis.The primary requirement for building a corpus of scientific syntheses is access to research problems or questions along with their associated papers.Previous efforts involved manually curating this information through a team of human annotators [34].Here, we systematize this approach by utilizing the Open Research Knowledge Graph (ORKG), a crowdsourcing platform that provides structured research contributions and comparisons.The subsequent paragraphs detail our method for compiling a reliable collection of research problems and corresponding papers to generate a multidisciplinary scientific syntheses corpus.Our corpus surpasses previous work in both multidisciplinarity and size, containing three times as many data samples, as a direct consequence of relying on the ORKG crowdsourced data.</p>
<p>Synthesis Generation Data Preparation</p>
<p>The ORKG data source.The ORKG is a web-based service that structures scholarly research contributions into a knowledge graph, using a crowdsourcing approach where users add and semantically describe paper contributions (of which each paper may have one or more).A key feature, Comparisons, allows users to select and compare multiple research contributions in a tabular format [32].These Comparisons, curated by users, include papers addressing specific research themes.For defining a corpus for scientific synthesis generation, we utilized ORKG Comparisons to extract human-annotated research problems and their associated papers, providing an ideal data source.</p>
<p>Data processing.The ORKG Python package (https://pypi.org/project/orkg/) was used to collect the data.First, we found all research problems with Comparisons, which produced 1,300 Comparisons for 708 research problems.From these Comparisons, we eliminated those with fewer than five unique papers, which left 495 Comparisons.The minimum threshold of five papers per comparison is a fixed criterion in this study for generating scientific syntheses.This criterion is based on popular search systems like Elicit or ORKG Ask, which typically generate syntheses from the top five results of their respective search engines.Next, we sourced abstracts for each paper in the Comparisons using the open-access platforms Semantic Scholar, Crossref, and CORE.This resulted in 329 Comparisons with five or more papers with abstracts.This step was essential since in this work, paper titles and abstracts are the context from which the syntheses are generated.Finally, for each Comparison, all papers were grouped as all possible collections of five contributions, with each collection representing one sample in the data, for a total of 541 samples.However, it is not uncommon for a Comparison to be tagged in multiple research fields, resulting in duplicated samples that differ only in research field.In such cases, only one Comparison was randomly selected to be kept, yielding a final dataset of 348 samples with each sample as a candidate for scientific synthesis generation.</p>
<p>Intermediate ORKG scientific synthesis generation dataset.The ORKG scientific synthesis generation dataset including the respective accompanying generated syntheses (details in subsection 2.2) is publicly released at https://github.com/jd-coderepos/scisynthesis.Each sample in the dataset consists of the following: sample ID, research field, research problem, and the title, abstract, and DOI (if available) of five papers.The research field is selected from the ORKG's own taxonomic schema (https://orkg.org/fields), with users permitted to choose any level within the hierarchy, whereas research problems are entered as free-form text.Due to the flexibility allowed in user inputs, the research problems in the ORKG vary widely in structure and specificity.For example, both "text</p>
<p>Scientific Synthesis Generation</p>
<p>Task description.Inspired by prior work [17], the task of scientific synthesis is defined as a specialized form of multi-document summarization.It involves combining the main insights from multiple research papers-five in this work-into a coherent paragraph that addresses a specific research problem or question.The scientific synthesis generation task encompasses the following five key characteristics.1) Use of scientific literature: This process involves synthesizing information from the scientific literature, primarily from titles and abstracts of research papers.The task requires summarizing these texts and evaluating their relevance, correctness, and completeness concerning the research problem.2) Synthesis format: The synthesis should be concisely presented in a single paragraph, limited to 200 words.This limit aligns with the standard guidelines for scientific abstracts as recommended by APA [10], MLA [45], Harvard [43], and Chicago [21] style guides, aiming for brevity and precision in summarizing key information [31,41].The format requires distilling and integrating diverse scientific insights into a coherent, comprehensive summary that addresses the research problem directly.This single-paragraph approach emphasizes the need for concise and cohesive communication of complex information.3) Synthesize vs. summarize: The objective is to synthesize-meaning to combine elements into a coherent whole-rather than merely summarizing each source individually.This involves the integration, cohesion, and coherence of information from multiple sources to produce new insights or understanding in response to the research problem.4) Referencing source material: Each claim or piece of information in the synthesis must be traceable back to the source material (the abstracts) to ensure the synthesis's accuracy and reliability.5) Adherence to quality characteristics: Building on approaches from linguistic and semantic quality evaluation studies of generated texts [19], the quality of the synthesis should satisfy the following nine key criteria: relevancy, correctness, completeness, informativeness, integration, cohesion, coherence, readability, and conciseness.These criteria, identified as apt for synthesis quality evaluation, collectively ensure effective communication of the synthesized information.The role and the application of these criteria in practice in this work are introduced in subsection 3.1.</p>
<p>To accommodate the diverse information scope typically found in research papers, and to tailor the synthesis to the specific interests of the user, we developed three distinct types of synthesis: 1) Paperwise synthesis, which provides a general overview; 2) Methodological synthesis, which concentrates on the methods and their pertinent details; and 3) Thematic synthesis, which focuses on identifying and summarizing recurring themes or patterns.Each synthesis type is designed to address a specific research problem while aligning with the intended focus of information.For example, a methodological synthesis will include only details about the methods discussed in the paper abstracts.In contrast, a thematic synthesis will specifically target the repetitive overarching themes present in the research.</p>
<p>As alluded to in the Introduction, the goal of this work is to develop a systematic framework for generating scientific syntheses using Large Language Models (LLMs4Synthesis).To our knowledge, this is the first framework to showcase the integration of both generation and evaluation components, aimed at optimizing the downstream synthesis model.While implemented in the context of ORKG Ask, LLMs4Synthesis is designed to be easily adaptable to other similar platforms like Elicit.The remainder of this paper discusses our use of generative AI technology, specifically LLMs, to achieve this goal.We start by describing the LLMs used for scientific synthesis generation and their application in the next paragraph.</p>
<p>Synthesis generation task instruction, models, and the final ORKG syntheses dataset.This task involved using an LLM to generate a synthesis from the titles and abstracts of five scientific papers, tailored to the synthesis type and research problem.</p>
<p>Previous studies on scientific synthesis generation [17,34], have not extensively discussed how LLMs are prompted for this task.We designed the scientific synthesis generation prompt based on prompt engineering best practices [36] and various tested prompts.Our prompt for synthesis generation is detailed in Table 2.The prompt includes two main parts: a detailed task specification and placeholders for input papers.The first part specifies the task to the LLM, filling in the "[research-problem]" placeholder with the research problem from the ORKG synthesis dataset and "[prompttype-input-instruction]" with instructions corresponding to the synthesis type.For example, the methodological synthesis type instruction is: "The objective of this synthesis is to focus on the methodology.Therefore, compare and integrate the methodologies used in each paper content, emphasizing how they contribute to Part I -Task Instruction: Synthesis Task Specification Generate a synthesis from the provided papers as content on the research problem "[research-problem]" into a concise single paragraph of no more than 200 words.Follow these instructions: -Only the titles and abstracts from exactly five scientific papers will be provided, to be used as content for the synthesis.</p>
<p>-"[prompt-type-input-instruction]".</p>
<p>-Support each claim with citations, formatted as (1) or (3,5) to refer to the respective papers' content.</p>
<p>-Ensure the output is a single cohesive paragraph without section headings, titles, abstracts, or any paper-like structure.</p>
<p>-Focus on essential information, maintaining clarity and precision.</p>
<p>-Do not include additional information or exceed the specified word count of 200 words.</p>
<p>Part II -Task Input: Five Papers to Synthesize \n\n Papers \n 1. "
[content-1]" \n 2. "[content-2]" \n 3. "[content-3]" \n 4. "[content- 4]" \n 5. "[content-5]"
the research problem."The prompts for all three synthesis types are available at https://github.com/jd-coderepos/scisynthesis/tree/main/synthesis-generation-prompts.Based on initial feedback from the LLM's responses on a subset of the dataset, the first part of the prompt was iteratively refined to enhance specificity and instructional emphasis (notably, the 200-word limit is reiterated for clarity).The second part of the prompt includes placeholders for the titles and abstracts of five papers which are to be given as input to the LLM to perform the synthesis generation task.</p>
<p>We applied two prominent state-of-the-art LLMs for the scientific synthesis generation task: the open-source Mistral-7B [22], and the proprietary GPT-4 [1].Thus the resulting ORKG syntheses dataset comprises three syntheses data files, per synthesis type, per LLM (six files overall).Each file contains the raw synthesis response and the text of the synthesis.So total of 1044 syntheses for each model and an overall total of 2088 syntheses.</p>
<p>Synthesis Quality Evaluation</p>
<p>To enhance our understanding of the weaknesses of LLMs concerning the synthesis objective and toward realizing potential improvements within the LLMs4Synthesis framework, we carried out a two-part qualitative evaluation of the generated syntheses.This evaluation included: 1) an LLM-based assessment using the most powerful variant of the GPT models, specifically GPT-4-Turbo, and 2) a human survey evaluation of a subsample from the synthesis dataset.Training moderate-sized open-source models effectively we demonstrate with Mistral 7B.We aim to make synthesis generation tasks more accessible and decrease dependence on proprietary models, which, while highly effective as indicated by popular LLM leaderboards, often lack transparency in their pre-training methods, datasets, and parameters.This is a barrier to deeper research insights and wider community participation.Consequently, our work is dedicated to developing methods that equip open-source LLMs to handle essential tasks in scientific language processing.</p>
<p>As a first step, we established a comprehensive set of evaluation criteria for the generated syntheses, detailed in the next section.</p>
<p>Nine Criteria of Synthesis Quality</p>
<p>Prior work [17] has provided a thorough analysis of the limitations associated with existing quantitative evaluation metrics, such as the rouge family [27], traditionally used for text summarization tasks--a broader context for the synthesis objective.The limitations highlighted the necessity to establish evaluation criteria that focus on linguistic and semantic qualitative aspects.In this vein, inspired by a prior study that assessed synthesis quality using just three criteria-comprehensive, trust, and utility--considered crucial for high-quality synthesis [34], this work aims to expand on this foundation.We seek to develop a broader set of criteria for synthesis quality evaluation, incorporating thorough linguistic and semantic assessments, informed by insights from related research on text quality evaluation in summarization [19].After filtering out unrelated criteria from existing work, we identified nine criteria to evaluate the quality and effectiveness of the synthesized information.Each criterion is presented to the evaluator in the form of a question, as outlined below.</p>
<ol>
<li>Relevancy: Is the information in the answer relevant to the problem? 2. Correctness: Is the information in the answer a correct representation of the content of the provided abstracts?3. Completeness: Is the answer a comprehensive encapsulation of the relevant information in the provided abstracts?4. Informativeness: Is the answer a useful and informative reply to the problem? 5. Integration: Are the sources structurally and linguistically wellintegrated, using appropriate markers of provenance/quotation and logical connectors for each reference?In addition, are the sources integrated within a single paragraph?6. Cohesion: Are the sentences connected appropriately such that the resulting synthesis is cohesive?7. Coherence: Are the ideas connected in a sound and logical manner?8. Readability: Does the answer follow appropriate style and structure conventions for academic writing and use language correctly?9. Conciseness: Is the answer short and clear, without redundant statements?Furthermore, is the synthesis approximately 200 words long?</li>
</ol>
<p>Our initial five criteria were crafted to enhance objectivity and precision in evaluating syntheses, expanding on the three criteria used earlier in synthesis evaluation [34].Specifically, our '3.completeness' corresponds to their 'comprehensive, ' our '2.correctness' and '5.integration' relate to their 'trust, ' and our '1.relevancy' and '4.informativeness' align with their 'utility.' Thus, we expanded the three foundational criteria into five.Our remaining four criteria, three-'6.cohesion, ' '7.coherence, ' and '8.readability'-are rooted in established text evaluation practices for linguistic quality, which are, for instance, commonly applied in summarization scenarios.The final criterion, '9.conciseness, ' is unique to this work.It emphasizes both the quality of the synthesized information and adherence to the specified word limit, essential for presenting a succinct representation of synthesized research insights to the reader.</p>
<p>LLM Evaluation of Synthesis Quality</p>
<p>Prior work has demonstrated the effectiveness of LLMs as automatic evaluators for scientific synthesis [6,17,19], arguing that they are becoming both the tools and the standards for language assessment.Thus, for the evaluation task, we utilized GPT-4-Turbo, the most advanced LLM available at the time of writing.The evaluation prompt and script are available athttps://github.com/jd-coderepos/ scisynthesis/tree/main/gpt-4%20synthesis-evaluator.The prompt includes all nine evaluation criteria along with their corresponding questions.Each evaluation criteria was assessed based on a rating scale from 1 to 5 as follows: 1. Very Bad, 2. Bad, 3. Moderate, 4. Good, and 5. Very Good.Each rating level was specifically tailored with a description to aid in the assessment of each criterion.For instance, consider the rating scale description for the first criteria i.e. "1.Relevancy: is the information in the answer relevant to the problem?" as follows: Rating 1. Very bad: The information provided does not relate to the research problem, showing a lack of understanding or connection to the topic.Rating 2. Bad: The information occasionally relates to the research problem but lacks direct and consistent relevance.Rating 3. Moderate: The information is generally related to the research problem, with occasional lapses in direct relevance.Rating 4. Good: The information is consistently relevant to the research problem, with only minor exceptions.Rating 5. Very good: The synthesis is directly and consistently relevant to the research problem, demonstrating a deep understanding of the topic and its nuances.For the full prompts, including detailed descriptions of the rating scale for all criteria, please refer to the system prompt.Finally, similar to 3-fold cross-validation, the GPT-4 synthesis evaluator was run thrice on the same data samples to ensure consistent assessment.The resulting scores, which did not show significant divergence largely due to our detailed prompt specification of the expected evaluation task leaving little room for ambiguity in interpretation, were averaged and reported.</p>
<p>Results</p>
<p>. The results are depicted as the purple and green bars in Figure 1.These results per synthesis quality criteria are averaged for the three synthesis types, i.e. paper-wise, methodological, and thematic since the differences between their scores were observed to be minimal.Comparing the two synthesis generators, GPT-4 outperforms Mistral on all characteristics.On the GPT-4-generated syntheses, the highest-scored characteristics are integration, cohesion, coherence, and readability, all of which have average scores of 4.95 or greater, reflecting strong overall structure, logical flow, and ease of comprehension.Informativeness, correctness, and relevancy were also scored highly as indicators of the trust and utility of the answers.Furthermore, the evaluator was conservative in scoring completeness and conciseness suggesting that there might be some redundancy or verbosity in the syntheses.Mistral-7B was evaluated substantially lower than GPT-4.Notably, Mistral and GPT-4 differ significantly in size, with GPT-4 rumored to exceed a billion parameters, which likely contributes to its strong performance in text generation and synthesis tasks.Completeness, correctness, and conciseness received the lowest scores, attributed to the unexpected output format of the syntheses produced by Mistral.In these cases, syntheses were wrongly structured in the shape of original research articles comprising title, abstract, keywords, introduction, results, methodology, and conclusion rather than as a paragraph-format synthesis of multiple other works.</p>
<p>In conclusion, both models performed best on readability, integration, cohesion, and coherence, demonstrating the fitness of LLMs to generate clear, logically structured texts.However, there may be a difference in the suitability of each LLM to the varying synthesis types, with GPT-4 performing slightly better on methodological, followed closely by thematic, whereas Mistral performs best on thematic, followed by paper-wise.</p>
<p>Human Evaluation of Synthesis Quality</p>
<p>We recruited human evaluators to conduct a survey assessing synthesis quality.</p>
<p>3.3.1 Survey setup.Prolific (https://www.prolific.com/),a paid crowdworker platform specifically for academic studies, was used to conduct the survey.To reduce effort, we subsampled the syntheses dataset to include a subset of syntheses from the domains of Chemistry, Computer Science, Earth Science, Linguistics, and Sociology.These domains were selected because from surface observations the Mistral syntheses received a wide range of scores by the LLM evaluator (cf.subsection 3.2), and represent scholarly diversity.For each domain, two data samples were selected, each including a research problem, sets of five papers, and six generated syntheses--three each from Mistral-7B and GPT-4, corresponding to the three different synthesis types.The samples were chosen to exhibit distinct performance levels: one with high average evaluation scores and the other with low scores from the LLM evaluator for the Mistral syntheses.This approach aimed to compare the scoring variability between human and LLM evaluators.Furthermore, Mistral, our open-source LLM, is targeted for performance enhancement in synthesis generation through the LLMs4Synthesis framework, which justifies our focus on its generated syntheses' evaluation scores.Five surveys were created for each of the five domains, with each survey including both data samples thus a total of 12 syntheses.Each survey started with an introduction screen that defined the task as well as detailed the nine evaluation criteria and the 5-rating scale.In a subsequent screen, participants were shown the research problem and paper titles and abstracts before evaluating the six LLM-generated syntheses on sequential screens.They could revisit the abstracts at any time and optionally provide free text feedback for each characteristic.Links to the original surveys underlying the domain names are: Chemistry, Computer Science, Earth Science, Linguistics, Sociology.Evaluations were obtained from three participants for each domain, involving 15 participants in total.The survey for the human evaluator and the LLM evaluator prompt were closely aligned to ensure nearly identical evaluation setups.</p>
<p>Survey participant characteristics.</p>
<p>Prolific filters were used to screen for eligible individuals, specifically: 1) those fluent in English; 2) those who have completed at least an undergraduate degree, and; 3) those whose subject is in the appropriate domain.Prolific only verifies participants' identity and country of residence; all other demographic details, such as language proficiency and academic specialization, are self-reported.</p>
<p>Our survey participants, aged 22 to 33, included both undergraduate and graduate students.Detailed demographic information is available online.Participants were paid £9 (GBP) per hour, with an estimated survey duration of 2 hours.</p>
<p>Survey Results</p>
<p>. The results are depicted as the red and blue bars in Figure 1.In line with the LLM evaluator results, overall, GPT-4-generated syntheses consistently outperform those from Mistral across all evaluated characteristics, although some differences are slight.The scores from both LLM and human evaluators are based on the same data subsample used for the Prolific survey.Performance across the evaluation criteria varied.In the case of readability, both generators performed comparably; however, this was Mistral's strongest characteristic while for GPT-4 the average scores for relevancy, correctness, informativeness, and conciseness exceeded that of readability.The most significant difference in model performance is in the characteristics of completeness, informativeness, and integration.Additionally, we can gain an overview of participants' perceptions of the syntheses by examining the optional feedback provided for some texts.Below is a summary of the comments left for each characteristic.</p>
<ol>
<li>Relevancy.Both LLMs are generally positively received for relevancy.GPT-4's syntheses are more detailed and directly relevant to research problems, while Mistral may generalize findings.Both models need to improve on making connections and providing deeper insights, but GPT-4 slightly outperforms Mistral.2. Correctness.GPT-4 shows a high degree of correctness, whereas Mistral's performance is variable, with reports of "multiple serious inaccuracies." While GPT-4 reliably captures abstract details, it occasionally misses specific study details or statistical values.Mistral sometimes mislabels studies and includes incorrect information.3. Completeness.GPT-4 generally covers abstracts better and includes more specific information than Mistral, although both can miss key quantitative details.Mistral has more frequent gaps and occasionally misses entire studies, often being too brief.Both need to enhance their depth and detail, but GPT-4 maintains slightly better completeness.4. Informativeness.GPT-4 is generally more informative, though more explicit details on methods and influencing factors would be beneficial.Mistral's syntheses tend to be general and lack depth, with both models critiqued for insufficient insight into the research problem. 5. Integration.GPT-4 effectively integrates sources into a structured synthesis, whereas Mistral tends to list sources without transitions.GPT-4 is more consistent in creating a unified narrative, though both could improve paragraph organization.6. Cohesion.GPT-4 offers more cohesive syntheses with smoother transitions and well-connected ideas, whereas Mistral's work often reads like separate summaries.GPT-4 effectively groups studies thematically, enhancing synthesis cohesion.7. Coherence.GPT-4 shows higher overall coherence, while Mistral's syntheses can seem disjointed.Both models could strengthen connections between ideas, but GPT-4 is closer to achieving this.8. Readability.Both models produce clear syntheses, but Mistral's style is slightly preferred for its simplicity.GPT-4 adheres well to stylistic norms but can be overly complex.Despite structural issues, Mistral's writing quality is praised.9. Conciseness.GPT-4 generally adheres to word limits better, while Mistral often exceeds or falls short of the 200-word target.Both are praised for being non-repetitive.</li>
</ol>
<p>Finally, the domain-specific survey evaluations reveal that GPT-4 performs best in Earth Science and Computer Science, while Mistral excels in Earth Science and Chemistry.Both LLMs show weaker performance in Linguistics and Sociology, suggesting a better fit for engineering and hard sciences.For more insights, visit our repository.</p>
<p>This first part of the paper has introduced the scientific synthesis task, LLM synthesis generators, the multidisciplinary ORKG dataset, and insights from both an automatic LLM evaluator and a human survey.These elements contribute to defining the LLMs4Synthesis framework, which optimizes open-source LLMs for synthesis generation.The second part evaluates this framework, particularly against Mistral-7B, focusing on improvements in output format and addressing weaknesses in the nine evaluation criteria.</p>
<p>The LLMs4Synthesis Framework</p>
<p>The proposed LLMs4Synthesis adopts a reinforcement learning with AI feedback (RLAIF) [5,11] paradigm to fine-tune LLM synthesizers for scientific synthesis generation.As illustrated in Figure 2, the first module attempts to construct a standardized dataset.The next step of LLMs4Synthesis is supervised fine-tuning with the aim of learning the task distributions by instructing Mistral-7B as the base LLM and GPT-4 synthesis as the gold standard, attempting to shift the general LLM probability distributions toward scientific domains while maintaining the general knowledge.Finally, LLMs4Synthesis uses reinforcement learning (RL) with Proximal Policy Optimization (PPO) [37] algorithm to leverage human/AI feedback to guide the model's learning process, aiming to enhance the model's performance on synthesis generation by aligning its outputs with human preferences.Figure 2: LLMs4Synthesis Framework using Supervised Fine-Tuning and Reinforcement Learning [26].Note: SFT is optional, but we achieved better performance when it was included.</p>
<p>Supervised Finetuning (SFT)</p>
<p>Supervised fine-tuning (SFT) of LLMs [30] for scientific synthesis involves adapting the model to generate synthesized content from input papers using labeled training data.This process is crucial for customizing a pre-trained model, originally trained on a diverse range of texts, to handle more specific tasks like scientific synthesis generation based on provided research papers.The SFT module in the LLMs4Synthesis framework, illustrated in Figure 2, uses a standardized prompt template that incorporates abstracts and titles from five given papers as input   ℎ , with the synthesis generated by GPT-4 serving as the ground truth   4− ℎ .We used a Quantized Low-Rank Adapter (QLoRA) [13] for SFT to adapt LLM to a synthesis generation task while minimizing computational resources and memory usage.Let's consider  ( ) as a pretrained Mistral-7B LLM, where  ∈ R  *  represents the model's parameters, where  is the dimension of the LLM hidden state.Adapter layers with low-rank parameterization [20]  Later, the AdamW optimizer [29] was employed with a learning rate of 2 * 10 −4 , a gradient accumulation step of 4, and a warmup step of 0.03.This configuration was used to optimize both the LLM parameters  ( ) and the low-rank adapter parameters   and   over 5 epochs.</p>
<p>Modeling Feedback</p>
<p>Incorporating desired alignment preferences through reward modeling is essential for enhancing the performance of language models.This section presents two reward functions aimed at guiding the generation of syntheses.The first approach employs basic features to align with synthesis format as a paragraph and word limit constraints as structural preferences using feedback, while the second utilizes detailed qualitative scoring to encourage high-quality output using AI feedback, reflecting user objectives and enhancing alignment with human values by using GPT-4 as a proxy to the reward function [25].</p>
<p>Basic Features.</p>
<p>In synthesizing scientific content using LLMs, a key challenge is ensuring that the generated synthesis aligns with desired format and constraints.As noted in the earlier evaluations, Mistral produced synthesis following a conventional paper structure which is not per our specified synthesis format i.e. as a paragraph and within 200 words.To address this, a rule-based paper structure identifier was developed with two goals: (1) to discourage overly structured synthesis, and (2) to promote succinctness.We found that 35.44% of the Mistral generated synthesis dataset exhibited a paper structure, detected through 17 specific academic terms (e.g."Title", "Abstract", "Conclusion", etc.) and fabricated author names in the citations.To address this, we used nine regular expressions to identify various reference forms, helping us quantify the adherence to academic styles in LLM-generated texts.The word count limit used a standard space-based splitting method.</p>
<p>Let's consider  as a synthesis,   as a word counter function, and  as a paper structure identifier function that returns 1 if the synthesis has the paper structure and 0 otherwise.The basic features try to adapt human preferences for obtaining an ideal structural format of synthesis  by the following reward function:
𝑅 𝑏𝑎𝑠𝑖𝑐 (𝑆) =                    −1.5 if 𝑊 𝐶 (𝑆) &lt; 50 −1 if 𝑊 𝐶 (𝑆) &gt; 200 −2 if 𝑃𝑆 (𝑆) = 1 −0.5 if |𝑊 𝐶 (𝑆) − 200| ≤ 20 2 otherwise
The overall objective of this reward function   () is to encourage syntheses that are of moderate length (close to 200 words) and do not adhere strictly to a rigid paper structure.It penalizes both very short and very long syntheses, as well as those that follow a specific structure while rewarding those that meet the criteria for ideal synthesis length and flexibility.This approach reflects a balance between length, structure, and flexibility, aligning with human preferences for synthesis quality.</p>
<p>GPT-4 Features.</p>
<p>To reward the quality of synthesis based on nine qualitative criteria, we introduce a reward function designed to reflect how closely the provided synthesis is aligned with ideal qualitative standards obtained by GPT-4.We defined the   function that asses how close each score is per criteria to the preferred value, which in our case is five (the optimal value that one synthesis could get).The function is given by:
𝑃𝑉 𝐹 𝑠𝑐𝑜𝑟𝑒 (𝐶, 𝑝𝑣) = − 1 𝑛 𝑛 ∑︁ 𝑖=1 |𝐶 𝑖 − 𝑝𝑣 |
Here  represents a list of scores for nine qualitative criteria, denoted as { 1 ,  2 ...,   } (where  = 9), with each score   within the range of [1,5].This reward aims to maximize the average score of .To achieve this, we set the preferred value of  = 5.By using    (, ) function, we aim to encourage LLM to produce a synthesis with higher overall scores based on qualitative criteria.Finally, the following reward function is derived, which rewards the synthesis based on the synthesis scores  using the GPT-4 Evaluator.
𝑅 𝐺𝑃𝑇 −4 (𝐶) = 4.0 if 𝑃𝑉 𝐹 𝑠𝑐𝑜𝑟𝑒 ≥ −0.125
   otherwise Withing this reward function   −4 (), we aim to provide positive rewards for values of    that are closer to zero, as this indicates scores that are near the preferred value.To this, we set a threshold of −0.125.If the computed    is greater than or equal to this threshold, which means it is closer to zero and therefore represents better alignment with the preferred value, the reward is adjusted to a higher fixed value of 4.0.Otherwise, the    will be used as a reward score to punish the LLM for better synthesis generation.</p>
<p>Reinforcement Learning</p>
<p>To train a policy that generates higher-quality synthesis, we utilized reward models within an RLAIF framework.AI is utilized as a substitute for the traditional human feedback typically employed in similar contexts.In RL, given the current state (standardized prompts from the ORKG synthesis dataset), the LLM as a synthesizer produces an action (synthesis), that modifies the current state by adding a token to the current sequence.Once a full textual sequence has been produced, we can obtain a reward by rating the quality of the synthesis using reward models to mimic human preferences.To fine-tune the model with RL, we simply alternate between collecting data from the environment -done by generating text with the LLM and then scoring it with the reward model -and updating the policy according to the Proximal Policy Optimization (PPO) algorithm [37].</p>
<p>Importantly, we incorporate a term in the reward function that penalizes the Kullback-Leibler (KL) divergence [23] by measuring the entropy of the token probability distributions learned by the RL policy   (|), where this policy is derived from the tuned LLM synthesizer, compared to the initial LLM synthesis   (|) from the frozen SFT synthesis.The final reward function will be calculated as follows:
𝑅 = 𝑟 Θ (𝑦|𝑥) − 𝜆𝐷𝐿 𝐾𝐿 (𝜋 𝑃𝑃𝑂 (𝑦|𝑥)||𝜋 𝑏𝑎𝑠𝑒 (𝑦|𝑥))
Where  Θ (|) represents the reward model, which can be either   or   −4 .The KL penalty coefficient parameter, denoted as , is set to 0.2 for adaptive KL divergence control.We employed the Adam optimizer with a learning rate of 2.94 × 10 −5 in conjunction with PPO policy, which involves 10 epochs per batch of samples.The   [42] is defined as follows:
𝐷𝐿 𝐾𝐿 (𝜋 PPO ∥𝜋 base ) = log 𝜋 PPO (𝑦|𝑥)
 base (|) Intuitively, the entropy value captures how much information is stored within a probability distribution by measuring the dissimilarity between two probability distributions, with the primary goal of maximizing the reward achieved by the model during RL policy training.Another advantage of KL divergence is that it constrains the model to ensure that it doesn't drift too far from the pre-trained policy.</p>
<p>5 The LLMs4Synthesis Evaluations 5.1 Experimental Setup 5.1.1Dataset Split.The ORKG synthesis dataset is divided into training and testing splits to facilitate both training and evaluation processes.The overall dataset consists of 348 comparisons and 1044 standardized synthesis prompts.We partitioned the dataset by domain, allocating 20% of the comparisons for testing and the remaining 80% for training.This resulted in 78 comparisons with 234 standardized synthesis prompts for testing and 270 comparisons with 810 standardized synthesis prompts for training.For training purposes, the data is further divided into two equal parts: 135 comparisons and 405 synthesis prompts each for training LLMs (denoted as Train-LLM) and RL (denoted as Train-RL).Additionally, a small subset from the test set, comprising 10 comparisons and 30 standardized synthesis prompts, is used for Prolific human evaluations.This structured division ensures a comprehensive approach to model training and evaluation, leveraging diverse methodologies and human feedback.</p>
<p>Experimental Models.</p>
<p>In our experiments, we utilized a variety of models to assess the effectiveness of different fine-tuning and RL strategies.We used seven models as follows: (1) GPT-4 is a</p>
<p>Results</p>
<p>To study the research questions (RQs) of this work, we examine the results reported for the experimental models in Table 3.This analysis will provide insights into how each model performs across different evaluation metrics and criteria.RQ1: What features are essential for qualitatively evaluating the informativeness and meaningfulness of a scientific synthesis?Given the results in Table 3, we analyze this question using GPT-4 evaluation metrics and comparison of results for w/ GPT-4 Features based models with Systems models.Assessing semantic quality.GPT-4 and SFT + RLAIF (w/ GPT-4 Features) excel in several key areas.They lead with the highest scores in relevancy (4.97 and 4.88), demonstrating their ability to produce highly relevant synthesis content.These models also score highest in correctness (4.93 and 4.75), reflecting their strong performance in generating accurate information.In terms of completeness, GPT-4 and SFT + RLAIF (w/ GPT-4 Features) again lead with scores of 4.42 and 4.19, indicating more comprehensive topic coverage.Moreover, GPT-4 and SFT + RLAIF (w/ GPT-4 Features) consistently perform well across additional criteria such as informativeness, integration, cohesion, coherence, readability, and conciseness, showcasing their superior ability to produce well-rounded and high-quality outputs.In contrast, Vanilla and SFT show lower scores in these areas, suggesting they are less effective in relevancy, correctness, completeness, and overall content quality.Impact of adding w/ GPT-4 Features-based feedbacks.Incorporating GPT-4 Features-based feedback into the RLAIF and SFT+RLAIF models significantly enhances its performance across nearly all metrics compared to the baseline Vanilla and SFT models.This improvement indicates that the addition of GPT-4 Features feedback results in more semantically rich and coherent syntheses.By leveraging the   −4 () reward function, the model effectively aligns with human preferences and achieves superior quality and informativeness through RLAIF-based fine-tuning.</p>
<p>RQ2: How can LLMs be fine-tuned to adhere to specific formatting standards like word limits and paragraph structure in scientific syntheses?We address this question by analyzing the findings presented in Table 3 using results for Basic evaluation metrics and comparison between w/ Basic Features based models with Systems models.Adherence to the word limit.GPT-4 with an average word count of 218 obtains fairly close to the ideal of 200 word limit.However, the Vanilla model with a higher average word count of 242, shows less control over the adherence to the word limit, and even supervised fine-tuning (SFT averaged word count of 231) did not show an improvement.Nevertheless, GPT-4 majority of outputs are within the 150-250 word range (91.02%), however, Vanilla (48.71%) and SFT (81.62%) fail to surpass this performance.When it comes to minimizing overgenerating (  &gt; 250), GPT-4 performs exceptionally well with a score of 8.97%.SFT also shows impressive performance, scoring 17.94%, which is twice as good as the Vanilla model's score of 34.18%.Similarly, in generating less content (50 ≤   &lt; 150), the Vanilla model falls short with an average score of 17.09%.Using the   () reward function, both the RL (w/ Basic Features) and SFT + RL (w/ Basic Features) models produce word counts close to the ideal average.RL (w/ Basic Features) averages 189 words, while RQ3: How can the balance be achieved in LLMs between maintaining formatting standards and ensuring semantically informative and meaningful scientific syntheses?According to the SFT+RLAIF (w/ GPT-4 Features) results in Table 3, balancing the basic characteristics and semantic quality in LLM scientific synthesis involves leveraging advanced fine-tuning methods such as RLAIF with specialized reward functions such as   () and   −4 () accordingly.The SFT+RLAIF (w/ GPT-4 Features) model demonstrates an effective balance, excelling in both basic and qualitative metrics.It maintains optimal word count distribution and avoids paper-like structures while achieving high scores in all qualitative criteria.This balance is likely achieved through iterative fine-tuning that focuses on both structural adherence and semantic quality, indicating that an integrated approach combining SFT and RLAIF with model feedback can effectively balance these aspects.</p>
<p>Discussion</p>
<p>An LLM as a Quality Evaluator.As language models like BERT and BART have advanced, evaluation methods have evolved.Traditional metrics have been supplemented by approaches such as BERTScore [46], which uses contextual embeddings to assess semantic similarity effectively, and BLEURT [39], which leverages human-annotated data for more accurate text quality predictions.The advent of GPT-4 introduced more sophisticated methods like GPTScore [19], utilizing its zero-shot capabilities for versatile text evaluation, and LLM-Eval [28], which employs a single LLM prompt to robustly evaluate conversational quality, correlating strongly with human judgments.Additionally, the LLM-as-a-judge approach [47] finds GPT-4 approximating human evaluations with high agreement rates, offering a scalable alternative where traditional methods are impractical.These advancements highlight the increasing use of LLMs not only in generating text but also in evaluating it, a direction that aligns with this work and enhances the scalability, versatility, and precision of assessment methods.Human Preferences.The rule-based method was manually developed based on observations of the training set and incorporating human preferences via the   () reward model described in section 4.2.1.Our training set comprised 810 samples, with 296 of these identified as having paper structures.To detect these structures seen in 296 syntheses, we implemented a rule-based binary classifier using a set of predefined rules.This classifier utilized a paper structure vocabulary comprising 17 terms and 9 reference identifier regular expressions.The vocabulary-based identifier achieved an F1-score of 67.80%, while the reference identifier detection reached an F1-score of 90.87%.When both identifiers were combined, the final model achieved an overall F1-score of 98.67%, with only 10 misclassified cases.Further analysis involved annotating outputs from the SFT and Vanilla models and comparing them to our model's results.Human annotations indicated that the Vanilla model had 31.62% of its syntheses with paper structure, while the SFT model had 2.99%.Our model identified 32.47% of the Vanilla syntheses and 1.28% of the SFT syntheses as having paper structures.This demonstrates that our paper structure identifier model effectively contributes to the reward mechanism, with a focus on improving paper structure adherence in synthesis.Consistency Comparison.In Figure 3, we analyzed GPT-4 evaluator behavior comparing the Vanilla baseline model to the SFT+RLAIF (w/ GPT-4 Features) optimized model output generated thrice on the same test set.The results reveal that the Vanilla model's output is inconsistent across three different runs.This variability suggests that the Vanilla model is less reliable as a synthesizer.In contrast, the SFT+RLAIF model exhibits much greater stability, outperforming the Vanilla model, with syntheses evaluations closely clustered and demonstrating consistent performance of syntheses evaluations.This enhanced consistency highlights the effectiveness of RLAIF in improving model reliability.The GPT-4 evaluator's ability to provide reliable and consistent assessments further supports these findings, making it an effective tool for evaluating model performance.Overall, the experiment underscores the benefits of RLAIF in delivering both consistent and higher-quality results.</p>
<p>Conclusion</p>
<p>In conclusion, this work presents the LLMs4Synthesis framework, a comprehensive approach designed to enhance open-source LLMs for generating high-quality scientific syntheses.Addressing the challenges posed by the growing volume and complexity of scientific literature, the framework introduces new synthesis types, and quality evaluation criteria, and leverages RLAIF to improve synthesis generation.The study not only demonstrates the effectiveness of LLMs in producing and evaluating scientific summaries but also provides publicly available resources to further advance research in this domain, fostering greater accessibility and collaboration within the scientific community.</p>
<p>4 Figure 1 :
41
Figure 1: Evaluation results from the GPT-4 LLM evaluator (purple and green bars) and a Prolific human survey (red and blue bars) for syntheses generated by Mistral and GPT-4.The data includes averaged scores across three synthesis types and five domains-Chemistry, Computer Science, Earth Science, Linguistics, and Sociology.</p>
<p>x:</p>
<p>Generate a synthesis from ... The methodologies employed ... y: In recent studies, researchers ...</p>
<p>introduce the transformation based on  ′ =   , where  ∈ R  *  and  ∈ R  *  are low-rank matrices with  representative of the rank, which set to  = 8.  is the transformation of the hidden state of LLM,  is the further transform the output of .Later,  and  quantized via   =  () and   =  () to reduce the memory footprint using 4-bit quantitation at  (.).In forward pass generation the model output ℎ is been obtained based on input sequence  ∈   ℎ by ℎ =  ( , )+ ( *   (    ( , ))), Where  ( , ) is the output of  for  ∈   ℎ ,   (    ( , )) is the task-specific adaptation introduced by quantized adapter layer,  is the scaling factor for balancing the contribution the model final output which  = 16, and  (.) denotes the dropout operation which applied to the output of the low-rank adapter with dropout rate of 0.05 to regularize the lowrank adapters, reducing overfitting by randomly dropping parts of the adapter's output during training.</p>
<p>Table 1 :
1
Top 10research fields in the ORKG synthesis dataset.
Research fieldFrequencyComputer Sciences125Physics28Animal Sciences19Chemistry17Urban Studies and Planning16Earth Sciences14Oceanography and Atmospheric Sciences and Meteorology14Science and Technology Studies12Materials Science and Engineering12Engineering10classification" and "Automated construction of health knowledgegraphs from medical records" are included as research problems inour dataset. However, the diverse and subjective nature of theseentries, stemming from the dataset's crowdsourced origins, reflectsand is representative of user behavior on online search platformssuch as Elicit or ORKG Ask. This diversity is a specific strength ofour dataset, setting it apart from previous datasets [34] created by asingle team of human annotators. Similarly, since users may assigna research field from any level in the hierarchy, they range frombroad, high-level fields like "Chemistry" to more specified fieldslike "Medicinal Chemistry and Pharmaceuticals." Therefore, eachsample has two research field columns: the original label selectedby the user, and a mapping of the selected field to the third level ofthe ORKG's taxonomy. We chose the third level as representativeenough to generalize to specific research field assignments. Note, re-search fields are not used in synthesis generation but are included asdata points to help organize and understand the dataset. Table 1 liststhe top 10 research fields in the ORKG synthesis generation dataset.The complete distribution is available at https://github.com/jd-coderepos/scisynthesis/blob/main/corpus/domain_counts.xlsx.</p>
<p>Table 2 :
2
Scientific synthesis generation task prompt.</p>
<p>Table 3 :
3
Results from LLMs4Synthesis using both Basic and GPT-4 evaluations across various metrics.Word Count metrics are reported as percentages, except for the average value.Paper Structure measures the percentage of observed synthesis with the paper's structure.GPT-4 results are averaged scores across nine characteristics, obtained from three separate evaluations.-the-art LLM developed by OpenAI.For our experiments, we used the gpt-4-1106-preview version.(2) Vanilla is a Mistral-7B model that has not undergone any fine-tuning, representing the raw capabilities of the Mistral-7B LLM.For our experiments, we utilized the Mistral-7B-Instruct-v0.1 version.(3) SFT is a fine-tuned version of the Vanilla model.The fine-tuning was performed using the QLoRA method, with the GPT-4 based synthesis outputs serving as the ground truth text.We used the Train-LLM split from the training data for fine-tuning.(4) RL (w/ Basic Features) involves fine-tuning the Vanilla model using RL with a basic reward function   ().
Evaluation CriteriaSystemsw/ Basic Featuresw/ GPT-4 FeaturesTypeMetricsGPT-4 VanillaSFTRLSFT + RL RLAIF SFT + RLAIF𝑊 𝐶 &lt; 500.00.00.00.00.00.420.050 ≤ 𝑊 𝐶 &lt; 1500.017.090.42 15.810.8511.960.0BasicWord Count150 ≤ 𝑊 𝐶 ≤ 250 𝑊 𝐶 &gt; 25091.02 8.9748.71 81.62 83.76 34.18 17.94 0.4298.71 0.4287.17 0.42100 0.0Average218242231189204194205Paper Structure1.2832.471.281.700.00.850.0Relevancy4.974.334.653.974.484.754.88Correctness4.933.663.632.973.094.394.75Completeness4.423.003.132.262.553.544.19Informativeness4.953.944.213.413.844.284.83GPT-4Integration4.994.414.733.674.544.744.89Cohesion4.994.444.733.814.514.784.89Coherence4.984.384.693.764.474.734.85Readability4.964.594.434.334.164.904.78Conciseness3.893.423.323.113.083.653.64state-of(5) SFT+RL (w/ Basic Features)is created by further fine-tuning the SFT model using RL with a ba-sic features reward function 𝑅 𝑏𝑎𝑠𝑖𝑐 (𝑆) that combines SFT and RL toenhance the performance w.r.t human preferences. (6) RLAIF (w/GPT-4 Features) is the result of further fine-tuning the RLAIF (w/Basic Features) model using a GPT-4 features reward function𝑅 𝐺𝑃𝑇 −4 (𝐶). (7) SFT+RLAIF (w/ GPT-4 Features) represents thefinal stage of our experiments, where the SFT+RLAIF (w/ BasicFeatures) model is further fine-tuned using RLAIF with GPT-4features reward function 𝑅 𝐺𝑃𝑇 −4 (𝐶).
AcknowledgmentsWe thank Julia Evans for her invaluable work on the "Human Evaluation of Synthesis Quality".This work is jointly supported by the NFDI4DataScience initiative (DFG, German Research Foundation, Grant ID: 460234259) and the SCINEXT project (BMBF, German Federal Ministry of Education and Research, Grant ID: 01lS22070).
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Decoding LLM Performance: A Guide to Evaluating LLM Applications. Amogh Agastya, 2023</p>
<p>Revisiting Automatic Evaluation of Extractive Summarization Task: Can We Do Better than ROUGE?. Mousumi Akter, Naman Bansal, Shubhra Kanti, Karmaker , 10.18653/v1/2022.findings-acl.122Findings of the Association for Computational Linguistics: ACL 2022. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Improving access to scientific literature with knowledge graphs. Sören Auer, Allard Oelen, Muhammad Haris, Markus Stocker, D' Jennifer, Kheir Eddine Souza, Lars Farfar, Manuel Vogt, Vitalis Prinz, Mohamad Wiens, Jaradeh Yaser, Bibliothek Forschung und Praxis. 442020. 2020</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022. 2022arXiv preprint</p>
<p>Benchmarking foundation models with language-model-as-an-examiner. Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Benchmarking Foundation Models with Language-Model-as-an-Examiner. Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, Lei Hou, Advances in Neural Information Processing Systems. A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references. Lutz Bornmann, Rüdiger Mutz, Journal of the association for information science and technology. 662015. 2015</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023. 2023arXiv preprint</p>
<p>Kendra Cherry, How to Write an Abstract in APA Format. 2023</p>
<p>Deep reinforcement learning from human preferences. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, 2017. 201730Advances in neural information processing systems</p>
<p>Revisiting Summarization Evaluation for Scientific Articles. Arman Cohan, Nazli Goharian, ; , Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Sara Goggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). Stelios Odijk, Piperidis, the Tenth International Conference on Language Resources and Evaluation (LREC'16)Asuncion Moreno; Portorož, Slovenia2016. JanEuropean Language Resources Association (ELRA)</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, arXiv:2305.14314[cs.LGQLoRA: Efficient Finetuning of Quantized LLMs. 2023</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies20191</p>
<p>D' Jennifer, Souza, 10.48366/R609337A Catalog of Transformer Models. 2023</p>
<p>Elicit: Analyze research papers at superhuman speed. 2024</p>
<p>Julia Evans, D' Jennifer, Sören Souza, Auer, arXiv:2407.02977Large Language Models as Evaluators for Scientific Synthesis. 2024. 2024arXiv preprint</p>
<p>. Santo Fortunato, Carl T Bergstrom, Katy Börner, James A Evans, Dirk Helbing, Staša Milojević, Filippo Alexander M Petersen, Roberta Radicchi, Brian Sinatra, Uzzi, Science of science. 3591852018. 2018Science</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, arXiv:2302.04166Gptscore: Evaluate as you desire. 2023. 2023arXiv preprint</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, arXiv:2106.09685[cs.CL2021</p>
<p>Authors and Abstracts -IEEE PVSC. IEEE</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825[cs.CLMistral 7B. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023</p>
<p>Kullback-Leibler Divergence. James M Joyce, 10.1007/978-3-642-04898-2_3272011SpringerBerlin Heidelberg; Berlin, Heidelberg</p>
<p>Neural Text Summarization: A Critical Evaluation. Wojciech Kryscinski, Nitish Shirish Keskar, Bryan Mccann, Caiming Xiong, Richard Socher, 10.18653/v1/D19-1051Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Vincent Jiang, Xiaojun Ng, Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh, arXiv:2303.00001[cs.LGReward Design with Language Models. 2023</p>
<p>Illustrating Reinforcement Learning from Human Feedback (RLHF). Nathan Lambert, Louis Castricato, 2022. 2022Hugging Face Blog</p>
<p>ROUGE: A Package for Automatic Evaluation of Summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. Yen-Ting Lin, Yun-Nung Chen, arXiv:2305.137112023. 2023arXiv preprint</p>
<p>Ilya Loshchilov, Frank Hutter, arXiv:1711.05101[cs.LGDecoupled Weight Decay Regularization. 2019</p>
<p>Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, Jianfeng Gao, arXiv:2402.06196[cs.CLLarge Language Models: A Survey. 2024</p>
<p>How to Write a Scientific Abstract. Suhasini Nagda, 10.1007/s13191-013-0299-xThe Journal of Indian Prosthodontic Society. 132013. 2013</p>
<p>Comparing Research Contributions in a Scholarly Knowledge Graph. Mohamad Yaser Allard Oelen, Kheir Eddine Jaradeh, Markus Farfar, Sören Stocker, Auer, Proceedings of the Third International Workshop on Capturing Scientific Knowledge co-located with the 10th International Conference on Knowledge Capture. the Third International Workshop on Capturing Scientific Knowledge co-located with the 10th International Conference on Knowledge CaptureMarina Del Rey, CA, USA2019. 2019</p>
<p>Open Research Knowledge Graph Ask Team. 2024. ORKG Ask -Find research you are actually looking for. </p>
<p>CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering. David Pride, Matteo Cancellieri, Petr Knoth, ; Omar Alonso, Helena Cousijn, Gianmaria Silvello, Mónica Marrero, 10.1007/978-3-031-43849-3_13Linking Theory and Practice of Digital Libraries. Carla Teixeira, Lopes , Stefano Marchesin, Nature SwitzerlandSpringer2023</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 192019. 2019</p>
<p>. Elvis Saravia, 2022. 12 2022Prompt Engineering Guide</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347[cs.LGProximal Policy Optimization Algorithms. 2017</p>
<p>Scispace Team, SciSpace -The Fastest Research Platform Ever. 2024</p>
<p>BLEURT: Learning Robust Metrics for Text Generation. Thibault Sellam, Dipanjan Das, Ankur Parikh, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Beyond the Imitation Game: Quantifying and extrapolatingthe capabilities of language models. Aarohi Srivastava, Denis Kleyjo, Ziyi Wu, Transactions on Machine Learning Research. 52023. 2023</p>
<p>How to Write a Science Fair Project Abstract. Science Buddies Staff</p>
<p>Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano, arXiv:2009.01325[cs.CLLearning to summarize from human feedback. 2022</p>
<p>Formatting Your Dissertation. </p>
<p>Global scientific output doubles every nine years. Richard Van Noorden, Nature News Blog. 2014</p>
<p>/Write-an-Abstract-in-MLA-Style Accessed. How to Write an Abstract in MLA Style</p>
<p>BERTScore: Evaluating Text Generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. </p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362024. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
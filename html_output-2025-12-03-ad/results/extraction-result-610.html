<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-610 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-610</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-610</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-dc2b36fb20490c0d540e01e74efd868d2e17faa3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/dc2b36fb20490c0d540e01e74efd868d2e17faa3" target="_blank">Bring Your Own Data! Self-Supervised Evaluation for Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a framework for self-supervised evaluation of LLMs by analyzing their sensitivity or invariance to transformations on the input text, and demonstrates self- supervised evaluation strategies for measuring closed-book knowledge, toxicity, and long-range context dependence.</p>
                <p><strong>Paper Abstract:</strong> With the rise of Large Language Models (LLMs) and their ubiquitous deployment in diverse domains, measuring language model behavior on realistic data is imperative. For example, a company deploying a client-facing chatbot must ensure that the model will not respond to client requests with profanity. Current evaluations approach this problem using small, domain-specific datasets with human-curated labels. These evaluation sets are often sampled from a narrow and simplified distribution, and data sources can unknowingly be leaked into the training set which can lead to misleading evaluations. To bypass these drawbacks, we propose a framework for self-supervised evaluation of LLMs by analyzing their sensitivity or invariance to transformations on the input text. Self-supervised evaluation can directly monitor LLM behavior on datasets collected in the wild or streamed during live model deployment. We demonstrate self-supervised evaluation strategies for measuring closed-book knowledge, toxicity, and long-range context dependence, in addition to sensitivity to grammatical structure and tokenization errors. When comparisons to similar human-labeled benchmarks are available, we find strong correlations between self-supervised and human-supervised evaluations. The self-supervised paradigm complements current evaluation strategies that rely on labeled data.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e610.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e610.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Train/Test Leakage / Memorization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training-data overlap, memorization and dataset leakage</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Discussion and recognition that overlap between evaluation data and pretraining corpora (memorization) is a source of variability and can confound evaluation results; the paper cites prior work showing models can regurgitate training data and that evaluation datasets hosted online may be scraped into training sets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple (Pythia, GPT-2, GPT-J, MPT, LLaMA, OPT, Cohere, OpenAI API models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language processing / LLM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Robustly evaluating LLM behavior using self-supervised transformations (to avoid confounds from memorization)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>pretraining corpus overlap / memorized training examples, dataset leakage from internet-hosted benchmarks into model training data</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>qualitative comparison and argumentation; recommendation to evaluate on novel/streamed data to avoid memorization confounds</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>No numeric replication study reported; authors state that self-supervised evaluation on novel or live data can 'circumvent' memorization effects but provide no quantitative reproducibility numbers</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>train/test contamination (datasets scraped into pretraining corpora), memorization causing inflated benchmark performance, difficulty removing leaked examples from training sets</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>use self-supervised evaluation on novel or streaming data (not present in pretraining corpora) to eliminate memorization confounds; avoid relying solely on static human-labeled benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Qualitative claim only: authors state this strategy can eliminate the possibility of memorization affecting sensitivity scores; no quantitative reduction numbers provided</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Memorization and dataset leakage are important sources of variability that can mislead benchmark-based evaluation; evaluating sensitivities on novel/streaming data is proposed as a way to avoid these confounds, but no numeric reproducibility experiments were performed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bring Your Own Data! Self-Supervised Evaluation for Large Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e610.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e610.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perspective API Instability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instability and versioning changes in black-box toxicity APIs (Perspective API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper notes that toxicity evaluation using external black-box APIs (e.g., Perspective API) is a reproducibility risk because these services change over time, altering toxicity scores and hindering longitudinal comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>n/a (applies to auxiliary evaluation APIs used with many LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP safety / toxicity evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Measuring model toxicity and comparing self-supervised toxicity metrics to Perspective API scores</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>external API version changes, opaque scoring updates by third-party services</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>comparison of the proposed self-supervised toxicity metric against Perspective API scores (correlation plotted qualitatively/graphically)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Authors report a close correlation between their self-supervised toxicity metric (fraction of toxic generations and change in next-token profane probability) and Perspective API toxicity scores (figures show correlation), but no numerical correlation coefficient is provided in the text</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>reliance on third-party black-box APIs whose scoring functions change over time, making cross-time comparisons unreliable</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>use self-supervised, reproducible tests (e.g., appending deterministic trigger phrases and measuring toxicity using model-internal probabilities or deterministic word-list checks) rather than external black-box APIs</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Qualitative: self-supervised metrics correlate well with the Perspective API on the examined generations, providing a more stable, reproducible alternative; no numeric improvement in reproducibility reported</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>varied; toxicity tests computed on Wikipedia sentence prompts (1000 examples reported elsewhere), models >6B run in FP16</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Black-box toxicity APIs change over time and threaten reproducibility; self-supervised toxicity metrics (triggering with F-bombs and measuring generation/probability changes) correlate with Perspective API scores and provide a more reproducible evaluation path.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bring Your Own Data! Self-Supervised Evaluation for Large Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e610.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e610.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model Entropy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shannon entropy of model output distributions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper measures and discusses model output entropy (next-token and mean sentence token entropy) as a factor that can influence sensitivity metrics: lower entropy models may show lower measured sensitivity and affect calibration and diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple (LLaMA, MPT, Pythia, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / model evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Assessing how output distribution entropy relates to sensitivity/invariance metrics across LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>intrinsic model entropy (sharpness of next-token distributions), vocabulary size differences</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Shannon entropy H(x) = -∑ p(x) log p(x); reported for next-token prediction and mean token over a sentence</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Qualitative results: figure shows LLaMA has the lowest next-token and sentence entropy and large models generally have lower entropy than smaller ones; no per-model numeric entropy values are tabulated in the text, but the paper notes entropy differences likely explain some observed sensitivity score differences</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Entropy differences can confound sensitivity comparisons across models (e.g., low-entropy models may appear less sensitive), complicating reproducible interpretation</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Authors recommend future work to explore how to incorporate entropy into invariance/sensitivity scores (no concrete mitigation tested here)</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>entropy computed over Wikipedia sentences used as corpus (exact sample size not restated for entropy), general experiments used 1000 examples</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Model output entropy varies across models and scale and can materially affect sensitivity/invariance metrics; interpreting sensitivity requires accounting for entropy (future work suggested).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bring Your Own Data! Self-Supervised Evaluation for Large Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e610.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e610.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sample-size / measurement uncertainty</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reporting of sample sizes and standard errors for sensitivity metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports sample sizes and standard errors for its self-supervised metrics (e.g., 1000 examples yielding standard error < 0.002; tokenization test standard error ≈ 0.005) and uses median vs mean to mitigate long-tailed distributions—practical measures to make results reproducible and statistically reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple (evaluated models listed in Appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / experimental methodology</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Self-supervised evaluation of LLM sensitivities (negation, toxicity, context, word order, tokenization) with statistical reporting</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>sampling variability from finite numbers of examples; long-tailed score distributions across examples; outliers in perplexity</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>standard error of the mean, use of median for long-tailed distributions, reporting of sample sizes (n=1000 for many tests, n=5000 for word-order)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Reported: sensitivity score over 1000 examples had standard error < 0.002; LRS (context) computed over 1000 examples with standard error 2e-3; word-order mean standard error would be 2e-3 but authors report median due to long-tailed distributions; tokenization experiments report standard error ≈ 5e-3</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>statistical uncertainty (standard error), median reporting to reduce sensitivity to outliers</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>By reporting sample sizes and standard errors, the authors provide measures of statistical reliability; e.g., small standard errors (<0.002) indicate stable estimates over their sample sizes, but no cross-lab replication reported</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>long-tailed distributions and outliers (e.g., perplexity outliers) can bias mean estimates and complicate comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>use sufficiently large sample sizes (typically 1000 examples), report standard errors, use median for long-tailed distributions, use outlier-robust metrics (e.g., Percent PPL Drops as robust to outliers), normalize sensitivity scores using benign corpora</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Quantitative: reported standard errors show low sampling uncertainty (e.g., <0.002) for 1000-sample experiments; median choice mitigates long-tailed effects (qualitative description only)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>1000 examples for most metrics; 5000 examples for word-order experiments; reported per-experiment</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Careful reporting of sample size and standard error (and using medians when distributions are long-tailed) yields stable, reproducible metric estimates—authors report small standard errors for their 1000-sample evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bring Your Own Data! Self-Supervised Evaluation for Large Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e610.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e610.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction Fine-tuning Effects</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of instruction finetuning (instruction-tuned vs pretrained models) on sensitivity and variability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper analyzes how instruction finetuning changes model sensitivity to transformations: instruction-tuned models are often more sensitive to negations, context changes, and word order, but effects vary by model and metric (some outliers like Cohere command behave differently).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>instruction-finetuned variants and their base models (e.g., Dolly-V2 vs Pythia, MPT-Instruct vs MPT, Cohere command vs base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / model fine-tuning effects</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Comparing sensitivity/invariance metrics between pretrained and instruction-finetuned versions of models</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>differences introduced by finetuning dataset composition and procedure, instruction finetuning altering output calibration and sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>comparative sensitivity scores (negation sensitivity, LRS, word order, tokenization scores) plotted per-model pair; fraction of PPL drops; qualitative comparisons and plots</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Findings: instruction-finetuned models are on average more sensitive to negations, context, and word order; normalization can correct instruction-tuned models' overconfidence in negation task; tokenization robustness showed no reliable trend after instruction finetuning. No single numeric aggregate (e.g., average delta) is provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>paired comparisons between pretrained and instruction-finetuned model variants across the same self-supervised metrics (plots in Figures 5, 8, 9, 10)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Qualitative: consistent trends across multiple model families (instruction tuning increases sensitivity for several metrics), but with notable exceptions (e.g., Cohere command less sensitive after finetuning; tokenization robustness shows no consistent effect). No numerical replication statistics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>finetuning dataset composition and procedure may introduce variability that is hard to generalize; different instruction datasets (e.g., filtered vs unfiltered ShareGPT) yield different toxicity and sensitivity behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>compare paired pretrained/finetuned model variants using identical evaluation pipelines; normalize metrics to control for general terms (e.g., measure sensitivity to 'not' on a benign corpus to normalize negation sensitivity)</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Normalization qualitatively improved alignment with supervised benchmarks and corrected certain misbehaviors (authors state normalization 'further improving this trend' for negation), but no numeric before/after normalization values are tabulated</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>per-metric sample sizes (e.g., 1000 examples for negation and context metrics, 5000 for word-order) applied to both variants</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Instruction finetuning typically increases sensitivity to negation, context, and word order—implying finetuning alters model calibration and behavior—though effects depend on finetuning data and there are notable exceptions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bring Your Own Data! Self-Supervised Evaluation for Large Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e610.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e610.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tokenization Sensitivity (Broken Token Metric)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tokenization sensitivity assessed by character-split re-tokenization and next-token JSD</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised test that simulates 'broken' tokenization by splitting raw text at fixed character strides, re-tokenizing chunks independently, concatenating tokens back, and measuring divergence (JSD) in next-token distributions as a tokenization-robustness metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple (Pythia, GPT-2, GPT-Neo, GPT-J, MPT, LLaMA, OPT, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / tokenizer robustness</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Evaluating sensitivity of models to alternative tokenizations by computing JSD between original and re-tokenized inputs' next-token distributions</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>tokenizer behavior and pre-tokenization preprocessing differences (character splits, extra spaces, misspellings), amount of training data / tokens seen during model pretraining (training FLOPs/tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Median/mean Jensen-Shannon Divergence (JSD) on next-token distributions; standard error (~5e-3 reported); correlation plots vs approx. FLOPs and vs tokens seen during training</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Reported quantitative observations: tokenization test run on 1000 examples with standard error ≈ 5e-3; negative correlation observed between tokenization sensitivity and number of training tokens/FLOPs (more training => lower sensitivity); OPT models (trained on fewest tokens) are strong outliers with high sensitivity. Exact JSD numbers per model are plotted in figures but not tabulated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>mean JSD over 1000 examples; comparison across models and correlation with approximate FLOPs / tokens seen during training</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Authors report consistent cross-model trends (e.g., LLaMA and MPT are least sensitive; OPT family most sensitive) and a clear negative trend with training FLOPs/tokens seen; numerical standard error given (~5e-3) supports reproducibility of the estimate over sample</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>limited types of tokenization error tested (character splits only); variability due to different vocabulary sizes and tokenizer implementations across models</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>simulate broken tokenization deterministically (fixed split stride), use next-token JSD which is architecture-agnostic, evaluate across many models and report standard errors; recommend evaluating additional tokenization error types in future</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Quantitative trends show models trained on more tokens are empirically more robust (lower JSD); specific mitigation (e.g., more training data) correlates with lower sensitivity but no causal intervention experiment was run here</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>1000 examples (tokenization); split stride typically 5 in reported experiment; standard error ≈ 5e-3</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Tokenization sensitivity is a measurable source of variability; models that have seen more training tokens / FLOPs exhibit lower sensitivity (more invariant to broken tokenizations), and results are reproducible with small standard error across 1000-example samples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bring Your Own Data! Self-Supervised Evaluation for Large Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Extracting training data from large language models <em>(Rating: 2)</em></li>
                <li>Quantifying memorization across neural language models <em>(Rating: 2)</em></li>
                <li>On the challenges of using black-box apis for toxicity evaluation in research <em>(Rating: 2)</em></li>
                <li>A framework for few-shot language model evaluation <em>(Rating: 2)</em></li>
                <li>Holistic evaluation of language models <em>(Rating: 2)</em></li>
                <li>Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models <em>(Rating: 1)</em></li>
                <li>What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-610",
    "paper_id": "paper-dc2b36fb20490c0d540e01e74efd868d2e17faa3",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "Train/Test Leakage / Memorization",
            "name_full": "Training-data overlap, memorization and dataset leakage",
            "brief_description": "Discussion and recognition that overlap between evaluation data and pretraining corpora (memorization) is a source of variability and can confound evaluation results; the paper cites prior work showing models can regurgitate training data and that evaluation datasets hosted online may be scraped into training sets.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "multiple (Pythia, GPT-2, GPT-J, MPT, LLaMA, OPT, Cohere, OpenAI API models)",
            "model_size": null,
            "scientific_domain": "Natural language processing / LLM evaluation",
            "experimental_task": "Robustly evaluating LLM behavior using self-supervised transformations (to avoid confounds from memorization)",
            "variability_sources": "pretraining corpus overlap / memorized training examples, dataset leakage from internet-hosted benchmarks into model training data",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": true,
            "reproducibility_metrics": "qualitative comparison and argumentation; recommendation to evaluate on novel/streamed data to avoid memorization confounds",
            "reproducibility_results": "No numeric replication study reported; authors state that self-supervised evaluation on novel or live data can 'circumvent' memorization effects but provide no quantitative reproducibility numbers",
            "reproducibility_challenges": "train/test contamination (datasets scraped into pretraining corpora), memorization causing inflated benchmark performance, difficulty removing leaked examples from training sets",
            "mitigation_methods": "use self-supervised evaluation on novel or streaming data (not present in pretraining corpora) to eliminate memorization confounds; avoid relying solely on static human-labeled benchmarks",
            "mitigation_effectiveness": "Qualitative claim only: authors state this strategy can eliminate the possibility of memorization affecting sensitivity scores; no quantitative reduction numbers provided",
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "Memorization and dataset leakage are important sources of variability that can mislead benchmark-based evaluation; evaluating sensitivities on novel/streaming data is proposed as a way to avoid these confounds, but no numeric reproducibility experiments were performed.",
            "uuid": "e610.0",
            "source_info": {
                "paper_title": "Bring Your Own Data! Self-Supervised Evaluation for Large Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Perspective API Instability",
            "name_full": "Instability and versioning changes in black-box toxicity APIs (Perspective API)",
            "brief_description": "The paper notes that toxicity evaluation using external black-box APIs (e.g., Perspective API) is a reproducibility risk because these services change over time, altering toxicity scores and hindering longitudinal comparisons.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "n/a (applies to auxiliary evaluation APIs used with many LLMs)",
            "model_size": null,
            "scientific_domain": "NLP safety / toxicity evaluation",
            "experimental_task": "Measuring model toxicity and comparing self-supervised toxicity metrics to Perspective API scores",
            "variability_sources": "external API version changes, opaque scoring updates by third-party services",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": true,
            "reproducibility_metrics": "comparison of the proposed self-supervised toxicity metric against Perspective API scores (correlation plotted qualitatively/graphically)",
            "reproducibility_results": "Authors report a close correlation between their self-supervised toxicity metric (fraction of toxic generations and change in next-token profane probability) and Perspective API toxicity scores (figures show correlation), but no numerical correlation coefficient is provided in the text",
            "reproducibility_challenges": "reliance on third-party black-box APIs whose scoring functions change over time, making cross-time comparisons unreliable",
            "mitigation_methods": "use self-supervised, reproducible tests (e.g., appending deterministic trigger phrases and measuring toxicity using model-internal probabilities or deterministic word-list checks) rather than external black-box APIs",
            "mitigation_effectiveness": "Qualitative: self-supervised metrics correlate well with the Perspective API on the examined generations, providing a more stable, reproducible alternative; no numeric improvement in reproducibility reported",
            "comparison_with_without_controls": true,
            "number_of_runs": "varied; toxicity tests computed on Wikipedia sentence prompts (1000 examples reported elsewhere), models &gt;6B run in FP16",
            "key_findings": "Black-box toxicity APIs change over time and threaten reproducibility; self-supervised toxicity metrics (triggering with F-bombs and measuring generation/probability changes) correlate with Perspective API scores and provide a more reproducible evaluation path.",
            "uuid": "e610.1",
            "source_info": {
                "paper_title": "Bring Your Own Data! Self-Supervised Evaluation for Large Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Model Entropy",
            "name_full": "Shannon entropy of model output distributions",
            "brief_description": "The paper measures and discusses model output entropy (next-token and mean sentence token entropy) as a factor that can influence sensitivity metrics: lower entropy models may show lower measured sensitivity and affect calibration and diversity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple (LLaMA, MPT, Pythia, etc.)",
            "model_size": null,
            "scientific_domain": "NLP / model evaluation",
            "experimental_task": "Assessing how output distribution entropy relates to sensitivity/invariance metrics across LLMs",
            "variability_sources": "intrinsic model entropy (sharpness of next-token distributions), vocabulary size differences",
            "variability_measured": true,
            "variability_metrics": "Shannon entropy H(x) = -∑ p(x) log p(x); reported for next-token prediction and mean token over a sentence",
            "variability_results": "Qualitative results: figure shows LLaMA has the lowest next-token and sentence entropy and large models generally have lower entropy than smaller ones; no per-model numeric entropy values are tabulated in the text, but the paper notes entropy differences likely explain some observed sensitivity score differences",
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Entropy differences can confound sensitivity comparisons across models (e.g., low-entropy models may appear less sensitive), complicating reproducible interpretation",
            "mitigation_methods": "Authors recommend future work to explore how to incorporate entropy into invariance/sensitivity scores (no concrete mitigation tested here)",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": "entropy computed over Wikipedia sentences used as corpus (exact sample size not restated for entropy), general experiments used 1000 examples",
            "key_findings": "Model output entropy varies across models and scale and can materially affect sensitivity/invariance metrics; interpreting sensitivity requires accounting for entropy (future work suggested).",
            "uuid": "e610.2",
            "source_info": {
                "paper_title": "Bring Your Own Data! Self-Supervised Evaluation for Large Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Sample-size / measurement uncertainty",
            "name_full": "Reporting of sample sizes and standard errors for sensitivity metrics",
            "brief_description": "The paper reports sample sizes and standard errors for its self-supervised metrics (e.g., 1000 examples yielding standard error &lt; 0.002; tokenization test standard error ≈ 0.005) and uses median vs mean to mitigate long-tailed distributions—practical measures to make results reproducible and statistically reliable.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple (evaluated models listed in Appendix)",
            "model_size": null,
            "scientific_domain": "NLP / experimental methodology",
            "experimental_task": "Self-supervised evaluation of LLM sensitivities (negation, toxicity, context, word order, tokenization) with statistical reporting",
            "variability_sources": "sampling variability from finite numbers of examples; long-tailed score distributions across examples; outliers in perplexity",
            "variability_measured": true,
            "variability_metrics": "standard error of the mean, use of median for long-tailed distributions, reporting of sample sizes (n=1000 for many tests, n=5000 for word-order)",
            "variability_results": "Reported: sensitivity score over 1000 examples had standard error &lt; 0.002; LRS (context) computed over 1000 examples with standard error 2e-3; word-order mean standard error would be 2e-3 but authors report median due to long-tailed distributions; tokenization experiments report standard error ≈ 5e-3",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "statistical uncertainty (standard error), median reporting to reduce sensitivity to outliers",
            "reproducibility_results": "By reporting sample sizes and standard errors, the authors provide measures of statistical reliability; e.g., small standard errors (&lt;0.002) indicate stable estimates over their sample sizes, but no cross-lab replication reported",
            "reproducibility_challenges": "long-tailed distributions and outliers (e.g., perplexity outliers) can bias mean estimates and complicate comparisons",
            "mitigation_methods": "use sufficiently large sample sizes (typically 1000 examples), report standard errors, use median for long-tailed distributions, use outlier-robust metrics (e.g., Percent PPL Drops as robust to outliers), normalize sensitivity scores using benign corpora",
            "mitigation_effectiveness": "Quantitative: reported standard errors show low sampling uncertainty (e.g., &lt;0.002) for 1000-sample experiments; median choice mitigates long-tailed effects (qualitative description only)",
            "comparison_with_without_controls": true,
            "number_of_runs": "1000 examples for most metrics; 5000 examples for word-order experiments; reported per-experiment",
            "key_findings": "Careful reporting of sample size and standard error (and using medians when distributions are long-tailed) yields stable, reproducible metric estimates—authors report small standard errors for their 1000-sample evaluations.",
            "uuid": "e610.3",
            "source_info": {
                "paper_title": "Bring Your Own Data! Self-Supervised Evaluation for Large Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Instruction Fine-tuning Effects",
            "name_full": "Effect of instruction finetuning (instruction-tuned vs pretrained models) on sensitivity and variability",
            "brief_description": "The paper analyzes how instruction finetuning changes model sensitivity to transformations: instruction-tuned models are often more sensitive to negations, context changes, and word order, but effects vary by model and metric (some outliers like Cohere command behave differently).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "instruction-finetuned variants and their base models (e.g., Dolly-V2 vs Pythia, MPT-Instruct vs MPT, Cohere command vs base)",
            "model_size": null,
            "scientific_domain": "NLP / model fine-tuning effects",
            "experimental_task": "Comparing sensitivity/invariance metrics between pretrained and instruction-finetuned versions of models",
            "variability_sources": "differences introduced by finetuning dataset composition and procedure, instruction finetuning altering output calibration and sensitivity",
            "variability_measured": true,
            "variability_metrics": "comparative sensitivity scores (negation sensitivity, LRS, word order, tokenization scores) plotted per-model pair; fraction of PPL drops; qualitative comparisons and plots",
            "variability_results": "Findings: instruction-finetuned models are on average more sensitive to negations, context, and word order; normalization can correct instruction-tuned models' overconfidence in negation task; tokenization robustness showed no reliable trend after instruction finetuning. No single numeric aggregate (e.g., average delta) is provided in text.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "paired comparisons between pretrained and instruction-finetuned model variants across the same self-supervised metrics (plots in Figures 5, 8, 9, 10)",
            "reproducibility_results": "Qualitative: consistent trends across multiple model families (instruction tuning increases sensitivity for several metrics), but with notable exceptions (e.g., Cohere command less sensitive after finetuning; tokenization robustness shows no consistent effect). No numerical replication statistics reported.",
            "reproducibility_challenges": "finetuning dataset composition and procedure may introduce variability that is hard to generalize; different instruction datasets (e.g., filtered vs unfiltered ShareGPT) yield different toxicity and sensitivity behavior",
            "mitigation_methods": "compare paired pretrained/finetuned model variants using identical evaluation pipelines; normalize metrics to control for general terms (e.g., measure sensitivity to 'not' on a benign corpus to normalize negation sensitivity)",
            "mitigation_effectiveness": "Normalization qualitatively improved alignment with supervised benchmarks and corrected certain misbehaviors (authors state normalization 'further improving this trend' for negation), but no numeric before/after normalization values are tabulated",
            "comparison_with_without_controls": true,
            "number_of_runs": "per-metric sample sizes (e.g., 1000 examples for negation and context metrics, 5000 for word-order) applied to both variants",
            "key_findings": "Instruction finetuning typically increases sensitivity to negation, context, and word order—implying finetuning alters model calibration and behavior—though effects depend on finetuning data and there are notable exceptions.",
            "uuid": "e610.4",
            "source_info": {
                "paper_title": "Bring Your Own Data! Self-Supervised Evaluation for Large Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Tokenization Sensitivity (Broken Token Metric)",
            "name_full": "Tokenization sensitivity assessed by character-split re-tokenization and next-token JSD",
            "brief_description": "A self-supervised test that simulates 'broken' tokenization by splitting raw text at fixed character strides, re-tokenizing chunks independently, concatenating tokens back, and measuring divergence (JSD) in next-token distributions as a tokenization-robustness metric.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple (Pythia, GPT-2, GPT-Neo, GPT-J, MPT, LLaMA, OPT, etc.)",
            "model_size": null,
            "scientific_domain": "NLP / tokenizer robustness",
            "experimental_task": "Evaluating sensitivity of models to alternative tokenizations by computing JSD between original and re-tokenized inputs' next-token distributions",
            "variability_sources": "tokenizer behavior and pre-tokenization preprocessing differences (character splits, extra spaces, misspellings), amount of training data / tokens seen during model pretraining (training FLOPs/tokens)",
            "variability_measured": true,
            "variability_metrics": "Median/mean Jensen-Shannon Divergence (JSD) on next-token distributions; standard error (~5e-3 reported); correlation plots vs approx. FLOPs and vs tokens seen during training",
            "variability_results": "Reported quantitative observations: tokenization test run on 1000 examples with standard error ≈ 5e-3; negative correlation observed between tokenization sensitivity and number of training tokens/FLOPs (more training =&gt; lower sensitivity); OPT models (trained on fewest tokens) are strong outliers with high sensitivity. Exact JSD numbers per model are plotted in figures but not tabulated in text.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "mean JSD over 1000 examples; comparison across models and correlation with approximate FLOPs / tokens seen during training",
            "reproducibility_results": "Authors report consistent cross-model trends (e.g., LLaMA and MPT are least sensitive; OPT family most sensitive) and a clear negative trend with training FLOPs/tokens seen; numerical standard error given (~5e-3) supports reproducibility of the estimate over sample",
            "reproducibility_challenges": "limited types of tokenization error tested (character splits only); variability due to different vocabulary sizes and tokenizer implementations across models",
            "mitigation_methods": "simulate broken tokenization deterministically (fixed split stride), use next-token JSD which is architecture-agnostic, evaluate across many models and report standard errors; recommend evaluating additional tokenization error types in future",
            "mitigation_effectiveness": "Quantitative trends show models trained on more tokens are empirically more robust (lower JSD); specific mitigation (e.g., more training data) correlates with lower sensitivity but no causal intervention experiment was run here",
            "comparison_with_without_controls": true,
            "number_of_runs": "1000 examples (tokenization); split stride typically 5 in reported experiment; standard error ≈ 5e-3",
            "key_findings": "Tokenization sensitivity is a measurable source of variability; models that have seen more training tokens / FLOPs exhibit lower sensitivity (more invariant to broken tokenizations), and results are reproducible with small standard error across 1000-example samples.",
            "uuid": "e610.5",
            "source_info": {
                "paper_title": "Bring Your Own Data! Self-Supervised Evaluation for Large Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Extracting training data from large language models",
            "rating": 2
        },
        {
            "paper_title": "Quantifying memorization across neural language models",
            "rating": 2
        },
        {
            "paper_title": "On the challenges of using black-box apis for toxicity evaluation in research",
            "rating": 2
        },
        {
            "paper_title": "A framework for few-shot language model evaluation",
            "rating": 2
        },
        {
            "paper_title": "Holistic evaluation of language models",
            "rating": 2
        },
        {
            "paper_title": "Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models",
            "rating": 1
        },
        {
            "paper_title": "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
            "rating": 1
        }
    ],
    "cost": 0.015034249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Bring Your Own Data! Self-Supervised Evaluation of Large Language Models</h1>
<p>Neel Jain<em> Khalid Saifullah</em> Yuxin Wen John Kirchenbauer Manli Shu<br>Aniruddha Saha Micah Goldblum ${ }^{\dagger}$ Jonas Geiping Tom Goldstein<br>University of Maryland ${ }^{\dagger}$ New York University</p>
<h4>Abstract</h4>
<p>With the rise of Large Language Models (LLMs) and their ubiquitous deployment in diverse domains, measuring language model behavior on realistic data is imperative. For example, a company deploying a client-facing chatbot must ensure that the model will not respond to client requests with profanity. Current evaluations approach this problem using small, domain-specific datasets with human-curated labels. These evaluation sets are often sampled from a narrow and simplified distribution, and data sources can unknowingly be leaked into the training set, which can lead to misleading evaluations. To bypass these drawbacks, we propose a framework for self-supervised evaluation of LLMs by analyzing their sensitivity or invariance to transformations on the input text. Self-supervised evaluation can directly monitor LLM behavior on datasets collected in the wild or streamed during live model deployment. We demonstrate self-supervised evaluation strategies for measuring closed-book knowledge, toxicity, and long-range context dependence, in addition to sensitivity to grammatical structure and tokenization errors. When comparisons to similar human-labeled benchmarks are available, we find strong correlations between self-supervised and human-supervised evaluations. The self-supervised paradigm complements current evaluation strategies that rely on labeled data. Code is available at https://github.com/neelsjain/BYOD.</p>
<h2>1 Introduction</h2>
<p>As Large Language Models (LLMs) continue to advance rapidly, there has been a growing demand for new evaluation metrics that can accurately capture their capabilities and limitations [Ethayarajh and Jurafsky, 2020, Birhane et al., 2022, Kiela et al., 2021, Bowman and Dahl, 2021]. As a result, there has been a constant need to create new datasets as newer models continuously make the existing datasets obsolete. Recent approaches such as BIG-Bench [Srivastava et al., 2022] and HELM [Liang et al., 2022] aim to address this issue by providing an ever-increasing, diverse set of accumulating micro-benchmarks to measure the performance of LLMs. However, these approaches still rely heavily on dataset creation and curation, which is time-consuming and expensive.
Furthermore, evaluation is generally dataset-centric, meaning that evaluations are based on some human-labeled or generated metric evaluated on a fixed dataset. For modern LLMs, this conventional approach comes with new complications. First, evaluation data is hosted on the internet (for example on sites like GitHub). This makes them accessible to scraping bots that generate training data for</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: In our proposed self-supervised evaluation, pairs are created from a corpus. Each pair contains the original and perturbed text, which in the figure above is creating a negation via applying a "not." These pairs are then fed into the network, and the outputs (perplexity, probability distributions, or text) are compared for each pair. These measures are then aggregated to produce an invariance or sensitivity score.</p>
<p>LLMs, making older datasets unreliable unless they are painstakingly removed from the training set, which does not reliably happen [Brown et al., 2020, Gao et al., 2021]. Second, LLM evaluation is by its nature multi-faceted, since different LLM applications rely on distinct capabilities, and an ever-increasing number of such capabilities needs to be tested in modern LLMs. As dataset curation is expensive, each test in a large benchmark like HELM [Liang et al., 2022], uses only a small dataset – carefully created to test a particular capability in a particular scenario. However, models are then deployed in much broader contexts and settings, and the applicability of these evaluations to deployment usage can be uncertain.</p>
<p>To complement conventional evaluation, we propose a framework for <em>self-supervised model evaluation</em>. In this framework, metrics are defined as invariances and sensitivities that can be checked in a self-supervised fashion using interventions based only on the model in question rather than external labels. Self-supervised evaluation pipelines are <em>dataset-agnostic</em>, and so they can be utilized over larger corpora of evaluation data than conventional metrics, or even directly in production systems to monitor day-to-day performance. In this work, we develop this framework, discuss desiderata for such metrics, and provide several case studies for self-supervised metrics: measuring knowledge through negations, toxicity detection, long-range dependency, word-order, and tokenization sensitivity. By developing these new metrics, we hope to provide a more comprehensive and nuanced understanding of the strengths and limitations of LLMs.</p>
<h1>2 A Procedure for Self-Supervised Evaluation</h1>
<p>Our goal is to measure properties of LLMs such as toxicity, closed-book knowledge, and word order sensitivity without relying on benchmark-specific datasets or human annotations. Rather than measuring model accuracy against known ground truth labels, we choose a simple transformation that can be applied to text. We then measure the level of invariance that a model's output has under that transformation. If we choose our transformations carefully, we can obtain useful information about model behavior in a completely self-supervised way.</p>
<p>More concretely, given a corpus $D$ (e.g., Wikipedia), we construct pairs of original passages/sentences $x$, and transformed counterparts $x^{\prime}$. An example is seen in Figure 1, where we negate the original sentence $x$ to construct $x^{\prime}$. $X$ is the set of all transformed pairs. We then feed input pairs into the language model, $f$, to extract a pair of outputs. Depending on the construction, the output being considered can be the softmax probability vector over tokens, a perplexity score, or a feature vector. We then compare the outputs $f(x)$ and $f(x')$ using a similarity metric, $\mathcal{M}$. Finally, we aggregate the results over all pairs in the data corpus using an aggregation operator, $A$, to produce an invariance/sensitivity score.</p>
<p>$$
\text{SCORE} = A\left{\mathcal{M}(f(x), f(x'))\ \forall (x, x') \in X\right}
$$</p>
<p>In this work, we bring wikipedia as our own dataset, but note that we do so to enable comparisons to existing metrics that use human labels on similar data. We use this methodology to study several</p>
<p>Efforts such as https://github.com/hitz-zentroa/lm-contamination are trying to catalog this phenomenon for ChatGPT.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Spider plots showing sensitivity scores for the Knowledge Probing via Negations, Toxicity, Context (Long-Range), Word Order, and Tokenization metrics introduced in the paper. A larger area corresponds to a better model in terms of the sensitivity scores. (Left) Comparison between Pythia 1.4B and Pythia 7B models. The larger model performs better for all the metrics. (Right) Comparison between the instruction finetuned version (Dolly-V2) and the vanilla Pythia model. The instruction finetuned model is better than the vanilla model for all metrics except tokenization robustness.
case studies, namely knowledge via negations (Section 4), toxicity (Section 5), context sensitivity (Section 6), word order sensitivity (Section 7), and tokenization robustness (Section 8) culminating in sensitivity scores as seen in Figure 2. In practice, these metrics should not be constrained to this data source, but evaluated directly on application-relevant sources.</p>
<h1>3 Related Work</h1>
<p>HELM adopts a multi-metric approach: accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency over each of the datasets proposed [Liang et al., 2022]. These metrics build on the work of Ribeiro et al. [2020] and subsequent studies such as, [Mille et al., 2021, Wu et al., 2021, Ross et al., 2021, Dhole et al., 2021, Yang et al., 2022] which augment inputs from a dataset to measure properties beyond the classical metric of accuracy. While these methods rely on existing datasets and labels, our method departs from these previous works as we analyze invariances using a data-agnostic procedure.
Knowledge Probing via Negation: The MMLU benchmark [Hendrycks et al., 2021] is widely used to assess the knowledge base of language models, evaluating their performance on task-specific micro datasets. In production, the GPT-4 technical report [OpenAI, 2023] advertises the model's capabilities across various knowledge categories, yet the evaluation suite used in the report is not publicly available. Furthermore, Wu et al. [2021] introduces a general-purpose counterfactual generator, Polyjuice, that allows for control over perturbation types and locations and is trained by finetuning GPT-2 on multiple labeled datasets of paired sentences. In contrast, we focus on evaluating the knowledge base of LLMs through invariances where no labeled data is required. Negations: Ettinger [2020] utilize psycholinguistic tests to explore the general linguistic knowledge and contextual impacts of negation in language models. Our evaluation method allows us to assess the model's understanding and knowledge representation by examining its ability to handle negations without the need for in-domain labeled datasets or model finetuning.
Toxicity: RealToxicityPrompts is the most prominent benchmark for toxicity in LLMs [Gehman et al., 2020]. This method relies on the Perspective API ${ }^{2}$ to score the model's generation based on a series of prompts. This API is also used as the toxicity metric for HELM. However, with the proprietary API constantly changing, comparing evaluations across time is difficult [Pozzobon et al., 2023]. Another common benchmark is BOLD [Dhamala et al., 2021]. BOLD trains another model to classify toxic generations. This approach of utilizing another model to measure toxicity is common [Sun et al., 2022]. Our approach differs from these methods as we do not build a dataset nor rely on auxiliary models to classify the generations.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Word Order: While previous efforts have made significant contributions to testing the compositional and word order understanding of language models [O’Connor and Andreas, 2021, Thrush et al., 2022], these efforts predominantly rely on small sets of hand-crafted examples. Moreover, these tests often encompass a wide range of knowledge types, making it challenging to isolate and evaluate the specific role of word order knowledge. Our work aims to investigate the word order sensitivity of LLMs from the lens of invariance in a data-agnostic manner.</p>
<p>Long-Range Dependency: As conversational AI models become more prevalent [Ouyang et al., 2022, Anthropic, 2023b], the importance of accommodating large context lengths has become evident. Recent endeavors have focused on developing chat models with extensive context capabilities, such as 32k and 100k [OpenAI, 2023, Anthropic, 2023a], utilizing techniques like memory-efficient attention [Dao et al., 2022]. However, it is equally crucial to gauge how far back into the context the model truly operates and can refer to. LAMBADA [Paperno et al., 2016], addresses this by assessing language models’ comprehension of broad contexts. In contrast, our self-supervised approach creates texts through closed-form transformations that evaluate language models’ grasp of long-range sensitivity.</p>
<p>Tokenization Sensitivity: HELM approaches this problem by inducing spaces, misspellings, etc. over the datasets in question to determine if these slight changes can affect changes when evaluating over established datasets [Liang et al., 2022]. Additionally, Rumbelow and Mwatkins [2023] found a set of anomalous tokens which result in a previously undocumented failure mode for GPT-2 and GPT-3 models. Inspired by these works, we designed a test to see how the same text tokenized differently affects model behavior without changing the underlying text.</p>
<h1>4 Knowledge Probing via Negations: Au Contraire Metric</h1>
<p>This section presents a simple self-supervised evaluation for knowledge probing. Knowledge probing in specific target domains is an important way to assess how a model will behave in different deployment scenarios. OpenAI approached this problem by constructing nine adversarial datasets on varying areas such as Law and Technology to evaluate GPT-4 [OpenAI, 2023]. While OpenAI's approach and others like MMLU [Hendrycks et al., 2021] are a step forward, these datasets do not cover all possible domain-specific areas. Therefore, when deploying a model, it is important to understand its ability to comprehend the potentially narrow domain-specific information of its use case. We probe this capability by testing whether the model is actually surprised (in terms of perplexity) by negated facts in a target domain.</p>
<p>Self-Supervised Approach: We construct a simple transformation over factual information like definitions by applying negations to facts. This is done in a trivial self-supervised way: We search for the first occurrence of is, was, or were, and place the word not after it provided a negation is not already present. For example, given the fact "April is the fourth month of the year in the Julian and Gregorian calendars and comes between March and May", we apply the negation transformation to this sentence and construct: "April is not the fourth month of the year in the Julian and Gregorian calendars and comes between March and May".</p>
<p>Based on this intervention, we measure the change in the log-perplexity $(\log (\operatorname{ppl}(x)))$, between the original and negated sentence. Formally, we define the sensitivity score as the following:</p>
<p>$$
\text { SENSITIVITY SCORE }=\frac{1}{n} \sum_{i}^{n} \log \left(\operatorname{ppl}\left(x_{i}^{\prime}\right)\right)-\log \left(\operatorname{ppl}\left(x_{i}\right)\right)
$$</p>
<p>One possible confounding variable is how sensitive a model is to the term "not" in a sentence. One way to normalize this behavior is to approximately measure the model's sensitivity to "not" over a benign corpus, where the meaning of "not" should not have a sizable impact on the perplexity over sentences nor have a known expected direction:</p>
<p>NORMALIZED SENSITIVITY SCORE $=$ sensitivity score $-\frac{1}{m} \sum_{i}^{m}\left|\log \left(\operatorname{ppl}\left(y_{i}^{\prime}\right)\right)-\log \left(\operatorname{ppl}\left(y_{i}\right)\right)\right|$,
where $y$ is a sample from a benign corpus like bookcorpus with $m$ total samples for which there is not a clearly defined truth value. Note that we use the absolute value of the difference, as it is unclear which direction is expected from the model for a given input in the benign corpus. To evaluate the relationship of these metrics to model confidence in our analysis, we also</p>
<p>record the fraction of inputs for which perplexity decreases after introducing a negation, which represents, for a typical sample, the error that the model is making: PERCENT PPL Drops $=$ $\frac{1}{n} \sum_{i}^{n} \max \left{\operatorname{sign}\left(\operatorname{ppl}\left(x_{i}\right)\right)-\log \left(\operatorname{ppl}\left(x_{i}^{\prime}\right)\right)\right), 0}$.</p>
<h1>4.1 Experimental Set-up</h1>
<p>To verify that this self-supervised evaluation is sensible, we compare our method to accuracy on TriviaQA, as both evaluations gauge an LLM's world knowledge [Joshi et al., 2017]. We do not penalize the length of the output. More details on exactly how we calculate accuracy can be found in the Appendix. Since TriviaQA asks general knowledge questions, we apply our self-supervised metric to topic sentences from Wikipedia to get a comparable general knowledge score. A human inspection of 100 samples verified that the proposed transformation resulted in grammatically correct sentences that were counterfactuals for the original sentence. To calculate our metric, we measure the sensitivity score over 1000 examples, where the standard error for these scores was less than 0.002 . Since we use perplexity, we can also utilize API models, such as those from OpenAI and Cohere, and publicly available models from the Hugging Face Hub, such as Pythia and GPT-2 [Biderman et al., 2023, Brown et al., 2020, Radford et al., 2019]. A full list of the models we evaluate can be found in the Appendix. We run all models greater than 6B parameters in their FP16 configuration.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: (Left) Sensitivity Score (negations) compared to accuracy on TriviaQA over various model sizes and families. (Right) Normalized Sensitivity Score compared to accuracy on TriviaQA over various model sizes and families. Larger markers correspond to bigger models, and " $x$ " markers represent instruction finetuned models.</p>
<h3>4.2 Results</h3>
<p>Figure 3 shows that the self-supervised SENSITIVITY SCORE, which measures the change in $\log (\mathrm{ppl})$ over the pair of sentences, closely tracks accuracy on the human-curated TriviaQA dataset, especially for non-instruction finetuned models. It also maps closely to a squareroot relationship, with normalization further improving this trend. Normalization corrects the instruction-tuned models to a larger degree, possibly due to their innate overconfidence. We can further hone in on why correct normalization is important by cross-referencing the frequency with which perplexity goes down rather than up, in Figure 4. This ablation metric is robust to outlier perplexity values. Here, instruction-tuned models are well-behaved. Further, we notice that outliers in Figure 3 are indicative of important model properties and weaknesses of the TriviaQA benchmark. For example, consider Cohere's instruction model (Cohere command), which has low sensitivity score relative to its TriviaQA performance and appears as a dark turquoise " $\times$ " on the middle right of the chart, and text-ada-001 (OpenAI's smallest instruction model), which appears as an orange " $\times$ " on the upper left side of the chart. To investigate these outliers further, we applied negations to questions in TriviaQA and found that Cohere command model rarely changed its answer when a negation was introduced, whereas text-ada-001 changed its answer frequently. We show</p>
<p>Table 1: Example outputs of text-ada-001, text-davinci-003 and Cohere command. These examples are selected where text-ada-001 would produce a sensible answer to both the original question and the negated question. The Cohere model is sometimes entirely insensitive to negations, compared to the OpenAI models, although even text-davinci can fail at this task. This trend was observed over several generations, from which we show two qualitative examples here.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Original</th>
<th style="text-align: center;">Transformed</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">A sterlet is what type of creature?</td>
<td style="text-align: center;">A sterlet is not what type of creature?</td>
</tr>
<tr>
<td style="text-align: center;">text-ada-001</td>
<td style="text-align: center;">A sterlet is a creature that has <br> a spiny body and a long, sharp tongue.</td>
<td style="text-align: center;">A sterlet is not a creature.</td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-003</td>
<td style="text-align: center;">A sterlet is a type of sturgeon.</td>
<td style="text-align: center;">A sterlet is a type of sturgeon.</td>
</tr>
<tr>
<td style="text-align: center;">Cohere command</td>
<td style="text-align: center;">Fish</td>
<td style="text-align: center;">Fish</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">What is the only natural food <br> that never goes bad?</td>
<td style="text-align: center;">What is not the only natural <br> food that never goes bad?</td>
</tr>
<tr>
<td style="text-align: center;">text-ada-001</td>
<td style="text-align: center;">The only natural food that <br> never goes bad is sugar.</td>
<td style="text-align: center;">There is no one natural food that <br> never goes bad. There are, however, <br> some foods that are more likely to do so. <br> These include: milk, yogurt, ice cream, <br> and cake.</td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-003</td>
<td style="text-align: center;">Honey.</td>
<td style="text-align: center;">There is no single natural food <br> that never goes bad.</td>
</tr>
<tr>
<td style="text-align: center;">Cohere command</td>
<td style="text-align: center;">Honey never goes bad.</td>
<td style="text-align: center;">Honey never goes bad.</td>
</tr>
</tbody>
</table>
<p>examples of this behavior in Table 1. This implies that the Cohere model is insensitive to sentence structure when the negation is present - it has memorized the associations between concepts and answers based on the context alone, even if the construction of the question makes its answer incorrect. This inability to answer grammatically complex questions is not reflected in the TriviaQA results, due to its reliance on simple sentence structures and nearly uniform question formats. Text-ada-001 is the opposite, it is exceedingly sensitive to sentence structure and nearly always flips its answer when faced with a negation. This also highlights a weakness of TriviaQA - its simple and predictable sentence constructs yield a benchmark that rewards correct concept associations rather than correct answers.</p>
<p>In summary, we find that we can predict benchmark performance exceedingly well with a simple self-supervised scheme, validating the effectiveness of this metric.</p>
<p>Effect of Instruction Finetuning: In general, we find that instruction-tuned models are more sensitive to negations than other LLMs as seen in Figure 5, regardless of the source of instruction data. The outlier here is again the Cohere command model, which is less sensitive than Cohere's base model after finetuning.</p>
<p>Limitations: For the sensitivity score to measure truthfulness, the dataset being used must contain a large fraction of sentences whose truth value is true, rather than neutral or false. This is likely to hold for many corpora, if only to varying degrees. As such, this metric might be less meaningful on a fan-fiction corpus, but more meaningful on a collection of medical or legal textbooks. Finally,
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Sensitivity Score (negation) comparing pretrained LLMs with their instruction finetuned counterparts. It can be seen that on average, instruction finetuning increases the Sensitivity Score.</p>
<p>we chose a simple construction for the negation transform and found it to be effective. LLMs like ChatGPT could be utilized to construct the counterfactual sentence when doing so would otherwise be non-trivial. However, our simple construction has the benefit of reproducibility, as it does not depend on a commercial model that may change over time.</p>
<h2>5 Toxicity: F-Bomb Metric</h2>
<p>Before LLMs are deployed in commercial settings, for example as a customer service chatbot, it is important to audit their potential to produce profanity or other toxic language. Most methods for measuring toxicity involve feeding an LLM toxic prompts and then analyzing the outputs using a black-box commercial tool (e.g., the Perspective API) or an additional trained model (usually an encoder). However, using a model to measure the generation may be problematic. For example, although work like <em>Fortuna et al. (2020)</em> has tried to understand how the Perspective API classifies toxic text, the API continues to change, and as it changes our understanding of how toxic generations are being classified starts to dissipate <em>(Pozzobon et al., 2023)</em>.</p>
<p>Self-Supervised Approach: One simple and reproducible approach is to analyze toxic generation through invariance. We will construct a metric that quantifies how <em>stoic</em> the model is to profanity, i.e., whether the model will respond to profane and aggressive comments with its own profanity or aggression. Although we study profanity, this can be extended to other forms of toxicity as well, or more broadly to model behaviors, such as tone, that the model should not mimic from user queries.</p>
<p>To build a self-supervised evaluation, we transform each of our prompts by "F-bombing:" we append trigger words (e.g., "F<strong>k F</strong>K") at the end of the original text. We then feed these modified prompts into the model, examining the generated output.</p>
<p>We construct two sensitivity scores: one that analyzes open-ended generated text, and another that analyzes the softmax probability distribution for the next predicted token. For the text generation score, we examine the output to determine if a profane word from the LDNOOBW dataset is contained in the generated text. The softmax score metric computes the next-token predictions at each position in a block of text and sums the probabilities of an LDNOOBW word occurring. We validate that our measurement procedure is sensible by comparing our metric with Perspective API toxicity scores over the generations that contained the appended trigger phrase. This approach enables us to understand the model’s stoicism towards profanity and extends to other forms of toxicity.</p>
<h3>5.1 Experimental Set-up</h3>
<p>We employ this self-supervised approach using the wikipedia dataset, which we split into prompts on a per-sentence level. For the generated text, we report the fraction of times a generation contains a "bad" word, minus the fraction of times the prompt contains a bad word. For the next word prediction metric, we compute the change in the average probability of a profane word between $x^{\prime}$ and $x$. We compared our metric with the Perspective API <em>(Fortuna et al., 2020)</em> toxic scores on the prompts. We consider a generation toxic when the toxicity score is greater than or equal to 0.5. We run all models greater than 6B parameters in their FP16 configuration. The full list of models can be found in the Appendix.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: The fraction of generated outputs that are toxic (i.e., the proposed metric) for instruction finetuned models and their pretrained counterparts. The rightmost bar is WizardLM, which is trained on uncensored ShareGPT data, containing toxic content.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: (Left) The fraction of toxic generations under our metric versus Perspective API. (Right) The change in the next-token probability of a profane word (token) versus the fraction of toxic generations according to Perspective API.</p>
<h1>5.2 Results</h1>
<p>The results of our toxicity metric, evaluated in both text space and logit space, are presented in Figure 7. The figures clearly demonstrate a close correlation between our metric, which measures the fraction of generated toxic word counts and the change in probabilities over the profane words, and the toxicity scores obtained from the Perspective API. We conducted tests using models of different types and scales (Figure 6 and Figure 7). Furthermore, from Figure 7, there appears to be no relation between the sensitivity of models to profane words and model size.
Effect of Instruction Finetuning: From Figure 6, we see that seems to be no effect on average of instruction finetuning compared to their pretrained counterparts over the six models examined. The LLM with the lowest score is Dolly-V2 (7B), making it the least toxic model with respect to both our scores. Additionally, we see that MPT-Instruct is less toxic, which we suspect is due to the Harmless and Helpful dataset from Anthropic the model was trained on [Bai et al., 2022]. Furthermore, we see that WizardLM, which is trained on an uncensored version of ShareGPT, is more toxic than a model trained on the filtered version of ShareGPT. While <em>Ouyang et al. [2022]</em> reported that RLHF decreases the toxicity of the model, this is ultimately highly dependent on the composition of the feedback data used to train the RLHF reward function.
Limitations: Our analysis focuses on explicit profanity and may not capture nuanced forms of toxicity beyond explicit language. We rely on predefined lists of profane words, which may not encompass all variations of toxicity. The effectiveness of our metric and the model's stoicism could vary with different datasets and prompt distributions.</p>
<h2>6 Context (Long-Range) Sensitivity: Back to the Future Metric</h2>
<p>As LLM context window sizes have increased in recent models, it is important to understand how changes in the previous context can affect the representations and generation across long ranges. Datasets like Long-Range Arena [Tay et al., 2020] offer a very broad set of tasks, focusing on context lengths ranging from $1 k$ to, $16 k$ and aim to evaluate architectural choices. There are other datasets like LAMBADA that focus on the capability to successfully predict the conclusion to a paragraph [Paperno et al., 2016]. The dataset is designed such that the prediction of the word is clear given the full context, but it is impossible to predict given just the last sentence. This measures an LLM's ability to comprehend text beyond locally attending to a sentence.
Self-Supervised Approach: We can utilize self-supervised evaluation to understand how the model's predictions change when a prior sentence or multiple sentences from a passage are altered. We conduct this test by taking three sentences from a stream of data in order and replacing the first two sentences with two random sentences from the corpus. For example, if the original passage had three sentences, $\left{S_{3}, S_{2}, S_{1}\right}$, where $S_{3}$ is the first sentence of the input passage, then the altered passage would be $\left{S_{X}^{\prime}, S_{Y}^{\prime}, S_{1}\right}$, where $S_{X}^{\prime}, S_{Y}^{\prime}$ are random sentences from another passage in the corpus. A more concrete example can be found in the Appendix. We then look at the probability distribution at each position of $S_{1}$ for both $x$ and $x^{\prime}$, and compare them using the Jensen-Shannon divergence. This is to determine how the representations of the last sentence change as different context is presented.</p>
<p>The Jensen-Shannon divergence (JSD) is a symmetric variation of KL-divergence, defined as:</p>
<p>$$
\operatorname{JSD}(P | Q)=\frac{1}{2} K L(P | M)+\frac{1}{2} K L(Q | M) \text {, where } M=\frac{1}{2}(P+Q)
$$</p>
<p>For our invariance/sensitivity score, we take the mean of JSD over the last sentence, averaging over all samples. Concretely,</p>
<p>$$
\text { LRS SCORE }=\frac{1}{n} \sum_{i}^{n} \frac{1}{m} \sum_{j}^{m} \operatorname{JSD}\left(f\left(x_{j}^{i}\right) | f\left(\left(x^{\prime}\right)_{j}^{i}\right)\right)
$$</p>
<p>where $m$ represents the sentence length and $x_{j}^{i}$ is the $i$ th sample in the set at token position $j$ in the last sentence.</p>
<h1>6.1 Experimental Set-up</h1>
<p>For this sensitivity test, we compare our method to LAMBADA using EleutherAI's Language Model Evaluation Harness [Gao et al., 2021]. It is worth noting that the tests here are different. The LAMBADA dataset measures long-range dependency on fiction and its ability to comprehend the previous passage. On the other hand, we analyze the invariance of the probability distributions over the last sentence when the passage has been altered. To calculate our metric, we use the same corpus as the other tests and calculate over 1000 examples with the standard error $2 \mathrm{e}-3$ of the mean value record. We report the JSD for a range of models including Pythia, Neo, GPT-2, and others. We run all models greater than 6B parameters in their FP16 configuration.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Left LRS Score vs LAMBADA (OpenAI) across various model sizes and families. Right LRS Score of instruction finetuned models and their pretrained counterparts.</p>
<h3>6.2 Results</h3>
<p>From Figure 8 (Left), we see that as our LRS Score increases, the model performs better on LAMBADA. Furthermore, bigger models generally tend to be more sensitive to changes in the context. We see that Pythia and GPT-J are more sensitive to changes in the context compared to MPT and LLaMA. Whereas, smaller models like Pythia-70M and GPT-2 small produce a lower LRS Score.</p>
<p>Effect of Instruction Tuning: On average, we see that instruction-finetuned models are more sensitive to changes in context than their pretrained counterparts, suggesting that they may be sensitive to long-range changes (beyond locally attending to a sentence). Moreover, we find this gain appears independent of base model size. Both the smaller and larger Pythia base models have a similar sensitivity, and finetuning on Dolly-V2 ("human-curated" in Figure 8) leads to a similar gain in sensitivity.</p>
<p>Limitations: Although we are analyzing long-range sensitivity in token probability space, for transformers in particular, analyzing attention probabilities may be more effective. However, to make the metric applicable to generic architectures, including RNNs, LSTMs, efficient attention variants, etc., we believe that the token probability space is more appropriate.</p>
<h1>7 Word Order: Word Salad Metric</h1>
<p>Close adherence to word order is a requirement for accurate factual responses beyond simple completions based on associative recall. Large Language Models have an incredible ability to understand association but have been shown to lack the necessary representations for certain types of reasoning. One of many potential reasons for this is their occasional inability to understand word order. [yuksekgomul et al. [2023] showed that multimodal models trained on image captions exhibit this behavior. People have also demonstrated that BERT can often behave like a bag-of-words classifier [Juneja et al., 2023].
Self-Supervised Approach: To evaluate a model's sensitivity to word order, we utilize sentences from a given corpus and apply a transformation where two random words are swapped in each sentence, creating modified versions denoted as $x^{\prime}$. Next, we analyze the impact of word order changes on the model's predictions by examining the predicted token softmax probability distribution from the original sentence $x$ and its modified counterpart $x^{\prime}$. Specifically, we examine the JSD between the two distributions to quantify the divergence in attention or focus resulting from the random word swaps in $x^{\prime}$. Since there are no datasets that study word order, we compare our self-supervised approach to the LRS Score established in the previous section.</p>
<p>$$
\text { Word Order Score }=\operatorname{median}\left{\operatorname{JSD}\left(f(x)<em j_prime="j^{\prime">{j+1} | f\left(x^{\prime}\right)</em>\right) \in X\right}
$$}+1}\right) \forall\left(x, x^{\prime</p>
<p>where $j$ is the last token for the input sequence for $x$ and $j^{\prime}$ is the last token for $x^{\prime}$.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: (Left) Word Order Score vs LRS Score across various model sizes and families. (Right) Word Order Score of instruction finetuned models and their pretrained counterparts.</p>
<h3>7.1 Experimental Set-up</h3>
<p>For this experiment, we take our corpus and break it down into sentences. Then, for every sentence, we swap two random words (not tokens) to construct our $x^{\prime}$ over 5000 examples. Due to the long-tailed distribution in scores that were observed over the 5000 examples, we report the median, as described. For reference, if we had computed the mean, we would observe a standard error $2 \mathrm{e}-3$. We report the median JSD for each model, again including Pythia, Neo, GPT-2, and others. We run all models greater than 6B parameters in their FP16 configuration.</p>
<h3>7.2 Results</h3>
<p>From Figure 9 (Left), we can see that there is a positive correlation between Word Order Score and LRS Score. The higher the Word Order Score, the higher the LRS Score. Nevertheless, we can see that there appears to be a plateau for Word Score. Similar to the LRS Score, we see that larger models are more sensitive to word order, with the Mosaic MPT-7B and GPT-J model being the most sensitive to word order.
Effect of Instruction Finetuning: Figure 9 (Right) shows that most instruction finetuning approaches make the model more sensitive to word order over the five models studied. Particularly, we see that only finetuning on the human-curated databricks-dolly-15k seems to make the model more sensitive irrespective of the size.</p>
<p>Limitations: For this Word Order Score, we make the assumption that the next token prediction when swapping two words randomly is a good proxy to measure a model’s sensitivity to word order.</p>
<h1>8 Tokenization Sensitivity: Broken Token Metric</h1>
<p>Text pre-processing is rarely perfect. Raw text often contains extra spaces, weird formatting, and other quirks that affect how the tokenization of the text occurs. HELM explored some of these phenomena [Liang et al., 2022]. Others, such as Rumbelow and Mwatkins [2023], found anomalous tokens that represent failure modes in GPT-2 and GPT-3 models, showing that our understanding of how different tokenization impacts the model behavior is still limited.</p>
<p>Self-Supervised Approach: To quantify this phenomenon, we randomly chop strings of raw input text at regular intervals of $x$, and then we tokenize each of the chopped strings independently. This way, we mimic a "broken" tokenization, that might occur in the pretraining corpus due to document breaks and misspellings. A broken tokenization can also occur during model generation when incomplete user input is provided [Microsoft, 2023]. After tokenizing each chopped string separately, we concatenate these tokenizations back together. Note that the original content is unchanged - the alternative tokenization still decodes to the same raw input text. We then compare the concatenation of chopped tokenization to the original text over the next token prediction using JSD, similar to our Word Order Metric.</p>
<p>$$
\text { Tokenization Sensitivity Score }=\frac{1}{n} \sum \operatorname{JSD}\left(f(x)<em j_prime="j^{\prime">{j+1} | f\left(x^{\prime}\right)</em>\right)
$$}+1</p>
<h3>8.1 Experimental Set-up</h3>
<p>For this experiment, we take our corpus and break it down into sentences. Then, for every sentence, we apply our procedure (described above) to construct $x^{\prime}$ over 1000 examples. We report the mean JSD for each different model like Pythia, Neo, GPT-2, and others, where the standard error is about $5 \mathrm{e}-3$ for all models. We run all models greater than 6B parameters in their FP16 configuration. Here, we specifically explore a split stride of 5 , splitting every 5 th character.</p>
<h3>8.2 Results</h3>
<p>From Figure 10 (Left), we see that MPT and LLaMA are the least sensitive (lower is better) to changes in token inputs. More broadly, we observe a negative trend with training FLOPs (i.e increasing the FLOPs decreases the sensitivity to tokenization changes). We suspect that as the amount of training increases, alternative tokenizations are more likely to be observed, and invariance to these abnormal tokenizations increases. This is supported by measurements on the OPT models, which are strong outliers in the trend observed above. Each of these models was trained on only 180B tokens, less than
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: (Left) Tokenization Sensitivity Score with a split stride of five versus Approx. FLOPS - lower is better. Note that the OPT models have seen the fewest tokens during training, c.f. Figure 22. (Right) Impact of different instruction-tuned methods.</p>
<p>a fifth of the tokens seen by MPT and LLaMA (1 Trillion) and about half of what GPT-2, GPT-Neo, and Pythia have seen. We include Figure 22 for a variant of Figure 10 in terms of tokens observed during training in the appendix.
Effect of Instruction Finetuning: Figure 10 (Right) shows the impact of different instruction finetuning methods. In contrast to previously observed metrics, there seems to be no reliable trend in tokenization robustness after instruction finetuning. Furthermore, even when only model size differs (Dolly-V2s) the instruction finetuned dataset can have a different impact on this metric. It is worth noting that the Dolly-V2s were only trained on 15 k instructions.</p>
<p>Limitations We test a limited type - character splits - of tokenization error, particularly the same text just being processed differently by the tokenizer. There are additional tokenization errors to consider as well, based on minor edits of the raw input text (i.e explicit word splits, extra spaces, unusual punctuation, etc), that could also be considered. Additionally, we examined the change in the next token probabilities, as we believe it is a good proxy to measure this phenomenon.</p>
<h1>9 Discussion</h1>
<p>In this paper, we introduce a new framework for Self-Supervised Evaluation for LLMs using sensitivity (invariance) metrics. We show that sensitivity measurements like the ones explored in this paper knowledge via negations, toxicity, context, word order, and tokenization robustness - can correlate with existing evaluation datasets, as we verify for the knowledge and context sensitivity metrics. We conclude that sensitivity metrics can provide meaningful insights into model behavior, which we also verify qualitatively in our study of the Cohere command model. Additionally, we see generally, except for toxicity, that larger models have better sensitivity scores compared to smaller models, mirroring other benchmarks that verify that model performance generally increases with scale. However, there are still things to consider when analyzing these models using Self-Supervised Evaluation, which we will outline in this section.</p>
<p>For example, in some instances like text-ada-001 in knowledge, we see that being more sensitive is a byproduct of some other phenomena. Similarly, it may be that certain models are just insensitive in general to any transformations. This may be the case for tiny models, like the toxicity metric for GPT-2 (small) and the tokenization metric for Pythia-160M. This implies that there is a lower limit of model size where certain sensitivity metrics cease to be meaningful predictors of model qualities.</p>
<p>Model Entropy. The entropy of a model's output distribution can impact many aspects of text generation. A lower entropy may require a more aggressive sampling strategy for text generation to achieve a diverse set of generations from the model, or might indicate a miscalibration of the output distribution. Similarly, the model's entropy can affect sensitivity scores. If the entropy of the model is low, then the sensitivity may naturally be lower as well. The exact impact of the model's entropy on these sensitivity scores and how to appropriately incorporate it into invariances/sensitivity scores should be explored in future work. Figure 11 shows the Shannon Entropy of the Next Token Prediction and Sentence Entropy (the mean token entropy over a sentence of the model). We use the Wikipedia (our corpus) sentences to calculate the Shannon Entropy, defined as $H(x)=-\sum p(x) \log (p(x))$. From Figure 11, we see that LLaMA has the lowest entropy on both the next token and mean token over a sentence, with large models having a lower entropy than smaller models on average. This may partially explain why the sensitivity scores for LLaMA are lower. ${ }^{4}$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 12: (Left) shows LLaMA, MPT, and Pythia sensitivity scores across the five metrics studied in this paper. (Right) shows the instruction-tuned counterparts of these models across the five metrics. The more area that is covered, the better the model according to our SSE scheme. All models 7B were run in FP16 configurations.</p>
<p>Memorization. Machine learning evaluation benchmarks for studying statistical generalization almost always assume idealized train and test set separation. However, in reality, some amount of overlap often exists in modern web-scale pre-training corpora. As a result, there have been various efforts to measure and address the impact of these overlaps on the training and evaluation of large models [Brown et al., 2020, Gao et al., 2021]. Investigating the same relationship, purely from a training support perspective, Kandpal et al. [2022] showed that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. In a different but fundamentally related line of work, Carlini et al. [2022] demonstrated that LLMs regurgitate training data in specific scenarios, often based on repetition rates in training corpora. Further, their own prior work [Carlini et al., 2020] quantifies the underlying relationship between train and test data in yet another way by showing that simple loss-based membership inference methods are capable of discriminating whether a test query was present in the training dataset. In the context of sensitivity scores, this collection of results in the literature suggests that it is hard to make strong statements about whether training-time exposure to certain documents or token sequences would confound the trends observed in our proposed sensitivity metrics. We leave a detailed analysis of the interactions between memorization behaviors based on training data and our sensitivity metrics for future research. We suspect that developing a more complete understanding of these interactions is an important step towards more informative and robust sensitivity metrics.</p>
<p>An advantage of self-supervised sensitivity scores is that we can circumvent the potential effects of memorization by evaluating sensitivities on novel text, i.e., the latest news articles, as no labeling and additional curation of data sources is required. With this strategy, the possibility of memorization can be eliminated.</p>
<h1>10 Conclusion</h1>
<p>We introduce a procedure for self-supervised evaluation by analyzing invariances for Large Language Models. The key advantage of self-supervised evaluation is that it removes the need to laboriously label new data, leading to more efficient forms of evaluation in real deployment settings. We showcase several case studies, where we empirically validate this approach to be reliably tracking existing supervised metrics. Additionally, there are a number of future questions to consider when measuring a model's sensitivity that we did not fully explore yet - like entropy and memorization. Nevertheless, these self-supervised evaluation approaches have the potential to measure properties beyond what is currently capable of the traditional dataset approach - like sensitivity to word order. We hope that this is only a starting point for self-supervised metrics in the future that can lead to a deeper understanding of how LLMs behave and complement classical supervised benchmarks.</p>
<h1>11 Acknowledgements</h1>
<p>This work was made possible by the ONR MURI program, the Office of Naval Research (N000142112557), and the AFOSR MURI program. Commercial support was provided by Capital One Bank, the Amazon Research Award program, and Open Philanthropy. Further support was provided by the National Science Foundation (IIS-2212182), and by the NSF TRAILS Institute (2229885).</p>
<h2>References</h2>
<p>Anthropic. Introducing 100k context windows, May 2023a. URL https://www.anthropic.com/ index/100k-context-windows.</p>
<p>Anthropic. Introducing claude, March 2023b. URL https://www.anthropic.com/index/ introducing-claude.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.</p>
<p>Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373, 2023.</p>
<p>Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao. The values encoded in machine learning research. In 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 173-184, 2022.</p>
<p>Samuel R Bowman and George E Dahl. What will it take to fix benchmarking in natural language understanding? arXiv preprint arXiv:2104.02145, 2021.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>N Carlini, F Tramer, E Wallace, M Jagielski, A Herbert-Voss, K Lee, A Roberts, T Brown, D Song, Ú Erlingsson, et al. Extracting training data from large language models. arxiv. Preprint posted online December, 14, 2020.</p>
<p>Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. arXiv preprint arXiv:2202.07646, 2022.</p>
<p>Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022.</p>
<p>Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, page 862-872, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445924. URL https://doi.org/10.1145/3442188.3445924.</p>
<p>Kaustubh D Dhole, Varun Gangal, Sebastian Gehrmann, Aadesh Gupta, Zhenhao Li, Saad Mahamood, Abinaya Mahendiran, Simon Mille, Ashish Shrivastava, Samson Tan, et al. Nl-augmenter: A framework for task-sensitive natural language augmentation. arXiv preprint arXiv:2112.02721, 2021.</p>
<p>Kawin Ethayarajh and Dan Jurafsky. Utility is in the eye of the user: A critique of NLP leaderboards. arXiv preprint arXiv:2009.13888, 2020.</p>
<p>Allyson Ettinger. What bert is not: Lessons from a new suite of psycholinguistic diagnostics for language models. Transactions of the Association for Computational Linguistics, 8:34-48, 2020.</p>
<p>Paula Fortuna, Juan Soler, and Leo Wanner. Toxic, hateful, offensive or abusive? what are we really classifying? an empirical analysis of hate speech datasets. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 6786-6794, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https: //aclanthology.org/2020.lrec-1.838.</p>
<p>Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021. URL https://doi.org/10.5281/zenodo.5371628.</p>
<p>Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models, September 2020. URL http://arxiv.org/abs/2009.11462. arXiv:2009.11462 [cs].</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, 2017.</p>
<p>Jeevesh Juneja, Rachit Bansal, Kyunghyun Cho, João Sedoc, and Naomi Saphra. Linear connectivity reveals generalization strategies. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=hY6M0JH13uL.</p>
<p>Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models struggle to learn long-tail knowledge. arXiv preprint arXiv:2211.08411, 2022.</p>
<p>Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, et al. Dynabench: Rethinking benchmarking in nlp. arXiv preprint arXiv:2104.14337, 2021.</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.</p>
<p>Microsoft. Guidance. Microsoft, June 2023. URL https://github.com/microsoft/guidance.
Simon Mille, Kaustubh D. Dhole, Saad Mahamood, Laura Perez-Beltrachini, Varun Gangal, Mihir Kale, Emiel van Miltenburg, and Sebastian Gehrmann. Automatic construction of evaluation suites for natural language generation datasets. ArXiv, abs/2106.09069, 2021.</p>
<p>Joe O'Connor and Jacob Andreas. What context features can transformer language models use? arXiv preprint arXiv:2106.08367, 2021.</p>
<p>OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: $27730-27744,2022$.</p>
<p>Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1525-1534, 2016.</p>
<p>Luiza Pozzobon, Beyza Ermis, Patrick Lewis, and Sara Hooker. On the challenges of using black-box apis for toxicity evaluation in research. arXiv preprint arXiv:2304.12397, 2023.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy: Behavioral testing of NLP models with CheckList. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902-4912, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.442. URL https://aclanthology. org/2020.acl-main. 442.</p>
<p>Alexis Ross, Tongshuang Sherry Wu, Hao Peng, Matthew E. Peters, and Matt Gardner. Tailor: Generating and perturbing text with semantic controls. In Annual Meeting of the Association for Computational Linguistics, 2021.</p>
<p>Jessica Rumbelow and Mwatkins. Solidgoldmagikarp (plus, prompt generation), 2023. URL https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/ solidgoldmagikarp-plus-prompt-generation.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.</p>
<p>Hao Sun, Guangxuan Xu, Jiawen Deng, Jiale Cheng, Chujie Zheng, Hao Zhou, Nanyun Peng, Xiaoyan Zhu, and Minlie Huang. On the safety of conversational models: Taxonomy, dataset, and benchmark. In Findings of the Association for Computational Linguistics: ACL 2022, pages 3906-3923, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/ v1/2022.findings-acl.308. URL https://aclanthology.org/2022.findings-acl.308.</p>
<p>Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. In International Conference on Learning Representations, 2020.</p>
<p>Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality, 2022.</p>
<p>Tongshuang Sherry Wu, Marco Tulio Ribeiro, Jeffrey Heer, and Daniel S. Weld. Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models. In Annual Meeting of the Association for Computational Linguistics, 2021.</p>
<p>Guanqun Yang, Mirazul Haque, Qiaochu Song, Wei Yang, and Xueqing Liu. Testaug: A framework for augmenting capability-based nlp tests. In International Conference on Computational Linguistics, 2022.</p>
<p>Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it? In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=KRLUvxh8uaX.</p>
<h1>A Appendix</h1>
<h2>A. 1 Knowledge Probing via Negations</h2>
<p>Example: Figure 13 shows an example of the original $x$ and the transformed $x^{\prime}$ for the Knowledge Probing via Negations experiments.</p>
<p>Original $(x)$ : April is the fourth month of the year in the Julian and Gregorian calendars and comes between March and May.</p>
<p>Perturbed $\left(x^{\prime}\right)$ : April is not the fourth month of the year in the Julian and Gregorian calendars and comes between March and May.</p>
<p>Figure 13: Knowledge probing via negations example over topic sentences in wikipedia. (Top) is the original, $x$, from wikipedia. (Bottom) is the transformed, $x^{\prime}$, where we add a "not" according to the rules described in the main paper.</p>
<p>Adding Negations in TriviaQA To understand whether adding negations and measuring the change in log perplexity is a reasonable assessment of probing the knowledge in an LLM, we added negations to questions following the same rule described in the main paper. We then recorded the change in perplexity for each of the models given the question-answer pair. This was to understand how different models may understand negations. Figure 14 (Left) shows that adding a negation in the question and observing the change in perplexity can give us an indication of performance on TriviaQA.</p>
<p>TriviaQA Accuracy We calculate the accuracy for TriviaQA for the unfiltered-web-dev split by simply counting a correct answer from the model if one of the given answers was contained in the output string. Additionally, since we found that the answer list sometimes had the answer entity in the question, we excluded these answers when calculating accuracy. We use the template "Question: [input question]'nAnswer:".
Models From Huggingface: gpt2, gpt2-large, gpt2-medium, gpt2-xl, EleutherAI/gpt-neo-1.3B, EleutherAI/gpt-neo-2.7B, EleutherAI/gpt-j-6b, EleutherAI/pythia-1.4b, EleutherAI/pythia-2.8b, EleutherAI/pythia-6.9b, mosaicml/mpt-7b, mosaicml/mpt-7b-instruct, databricks/dolly-v1-6b, databricks/dolly-v2-3b, databricks/dolly-v2-7b
Other Models: LLaMA-base-7B, Vicuna-7B
OpenAI API Models: ada, babbage, curie, davinci, text-ada-001, text-babbage-001, text-curie-001,text-davinci-003
Cohere API Models: xlarge, command-xlarge-beta
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 14: (Left) The change in perplexity in the question-answer pair when a negation is applied to the question versus TriviaQA Acc. There appears to be a square-root relationship between the Sensitivity Score on TriviaQA versus TriviaQA Acc. (Right) The percentage of times when the correct answer was contained in the solution even when applying the negation versus Sensitivity Score (Wikipedia) for a few instruction models. We see that text-ada-001 changes its answer often, whereas the Cohere model does not.</p>
<h1>A. 2 Toxicity</h1>
<p>Example: Figure 15 shows an example of the original $x$ and the transformed $x^{\prime}$ for the toxicity experiments.</p>
<p>Original ( $x$ ): Lionel Andrés Messi is an Argentine professional footballer who plays as a forward for Ligue 1 club Paris Saint-Germain and captains the Argentina national team.</p>
<p>Transformed ( $x^{\prime}$ ): Lionel Andrés Messi is an Argentine professional footballer who plays as a forward for Ligue 1 club Paris Saint-Germain and captains the Argentina national team. $\underline{\mathrm{F}}^{<em> * </em>} \underline{\mathrm{~F}}^{<em> * </em>}$</p>
<p>Figure 15: Toxicity test with one sentence. (Top) is the original, $x$, from wikipedia. (Bottom) is the transformed, $x^{\prime}$, where we add the trigger phrase to the original input.</p>
<p>Increasing the number of F-bombs: From Figure 16, we see, that on average, increasing the number of F-bombs increases the toxicity of the model. However, here two F-bombs induce more toxic behavior than three F-bombs, suggesting that on average, the two F-bombs might be a trigger for toxic generation.
Models From Huggingface: gpt2, gpt2-large, gpt2-medium, gpt2-xl, EleutherAI/gpt-j-6b, EleutherAI/gpt-neo-1.3B, EleutherAI/gpt-neo-2.7B, EleutherAI/pythia-1.4b, EleutherAI/pythia-2.8b, EleutherAI/pythia-6.9b, mosaicml/mpt-7b, mosaicml/mpt-7b-instruct, databricks/dolly-v1-6b, databricks/dolly-v2-3b, databricks/dolly-v2-7b
Other Models: LLaMA-base-7B, Vicuna-7B, WizardLM-7B</p>
<h2>A. 3 Context (Long-Range) Sensitivity</h2>
<p>Example: Figure 17 shows an example of the original $x$ and the transformed $x^{\prime}$ for the LRS experiments.</p>
<p>Increasing the Amount of Context: From Figure 18, we see that increasing the context (or the number of sentences swapped) increases the sensitivity. For the 7B parameter range, we see that Pythia (6.9B) is the most sensitive.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 16: As we increase the number of F-bombs, the toxicity of the generation increases except when two F-bombs are present, which is a notable outlier. This suggests that to most models this is a toxic trigger. We measure toxicity over the generated text by observing whether a term from LDNOOBW is contained in the generation. From this figure, we see GPT-Neo (2.7B) is the most toxic according to our metric.</p>
<p>Original $(x)$ : Lyrically, the song begins with the absence of her man, but then, in the chorus, transitions into a warning not to fall in love with material things. The second track, "Lágrimas Cálidas" ("Warm Tears"), is a vallenato-stylized pop ballad, expressing her suffering due to being abandoned by her lover."Te Arrepentiras" ("You'll Regret"), is about a woman who surrendered completely to a man who did not appreciate her.</p>
<p>Transformed $\left(x^{\prime}\right)$ : Ireland has won more medals in boxing than in any other Olympic sport. Boxing is governed by the Irish Amateur Boxing Association. "Te Arrepentiras" ("You'll Regret"), is about a woman who surrendered completely to a man who did not appreciate her.</p>
<p>Figure 17: Long-Range Sensitivity test with four sentences. (Top) is the original, $x$, from wikipedia. (Bottom) is the transformed, $x^{\prime}$, where the first two sentences are replaced with random two sentences from wikipedia.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 18: Increasing the context length (the number of swapped sentences) increases, the change in the probability distribution over the last sentence.</p>
<p>Models From Huggingface: gpt2, gpt2-large, gpt2-medium, gpt2-xl, facebook/opt-1.3b, facebook/opt-2.7b, EleutherAI/gpt-neo-125M, EleutherAI/gpt-neo-1.3B, EleutherAI/gpt-neo-2.7B, EleutherAI/gpt-j-6b, EleutherAI/pythia-70M, EleutherAI/pythia-160m, EleutherAI/pythia-410m, EleutherAI/pythia-1b, EleutherAI/pythia-1.4b, EleutherAI/pythia-2.8b, EleutherAI/pythia-6.9b, mosaicml/mpt-7b, mosaicml/mpt-7b-instruct, databricks/dolly-v1-6b, databricks/dolly-v2-3b, databricks/dolly-v2-7b, databricks/dolly-v2-7b
Other Models: LLaMA-base-7B, Vicuna-7B</p>
<h1>A. 4 Word Order Sensitivity</h1>
<p>Example: Figure 19 shows an example of the original $x$ and the transformed $x^{\prime}$ for the word order experiments.</p>
<p>Original $(x)$ : Media. Vision would return to the franchise with the development of Valkyria: Azure Revolution for the PlayStation 4.</p>
<p>Transformed $\left(x^{\prime}\right)$ : Media. Vision would return PlayStation the franchise with the development of Valkyria : Azure Revolution for the to 4 .</p>
<p>Figure 19: Word Order Sensitivity test over one sentence. (Top) is the original, $x$, from wikipedia. (Bottom) is the transformed, $x^{\prime}$, where two words are randomly flipped. This is a 1-Swap.</p>
<p>Table 2: Example sentence of the transformation with a split stride of 10. (Left) shows the original unaltered sentence. (Right) shows the transformed sentence after splitting every 10th character. The underlined dashes are where the sentence is split.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Original $(x)$</th>
<th style="text-align: left;">Transformed $\left(x^{\prime}\right)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Media. Vision would return to the franchise with <br> the development of Valkyria: Azure Revolution <br> for the PlayStation.</td>
<td style="text-align: left;">Media. Visi on would r eturn to t he franchi se <br> with th e developm ent of Val kyria: Azu re <br> Revolut ion for th e PlayStat ion 4.</td>
</tr>
</tbody>
</table>
<p>Different Number of Swaps: Figure 20 shows the median JSD on the next token as we increase the swaps. Here, we see increasing the number of swaps increases the sensitivity.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 20: We plot JSD on the next token prediction against the number of swaps for the token.</p>
<p>Models From Huggingface: gpt2, gpt2-large, gpt2-medium, gpt2-xl, EleutherAI/gpt-neo-125M, EleutherAI/gpt-neo-1.3B, EleutherAI/gpt-neo-2.7B, EleutherAI/gpt-j-6b, EleutherAI/pythia-1.4b, EleutherAI/pythia-2.8b, EleutherAI/pythia-6.9b, mosaicml/mpt-7b, mosaicml/mpt-7b-instruct, databricks/dolly-v1-6b, databricks/dolly-v2-3b, databricks/dolly-v2-7b
Other Models: LLaMA-base-7B, Vicuna-7B</p>
<h1>A. 5 Tokenization Sensitivity</h1>
<p>Example: Figure 19 shows an example of the original $x$ and the transformed $x^{\prime}$ for the tokenization experiments.</p>
<p>Incresing Split Stride: Figure 21 shows the median JSD on the next token as we increase the split stride. Here, we see that LLaMA and MPT are much less sensitive (better at handling tokenization changes) regarding the change in the probability distribution over the next token as we increase the split stride. Figure 22 shows the number of tokens seen versus the tokenization sensitivity score. Here, we see that there is a negative correlation.
Models From Huggingface: gpt2, gpt2-large, gpt2-medium, gpt2-xl, facebook/opt-1.3b, facebook/opt-2.7b, EleutherAI/gpt-neo-125M, EleutherAI/gpt-neo-1.3B, EleutherAI/gpt-neo-2.7B, EleutherAI/gpt-j-6b, EleutherAI/pythia-160m, EleutherAI/pythia-410m, EleutherAI/pythia-1b, EleutherAI/pythia-1.4b, EleutherAI/pythia-2.8b, EleutherAI/pythia-6.9b, mosaicml/mpt-7b,mosaicml/mpt-7b-instruct, databricks/dolly-v1-6b, databricks/dolly-v2-3b, databricks/dolly-v2-7b, databricks/dolly-v2-7b
Other Models: LLaMA-base-7B, Vicuna-7B</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 21: Increasing the split stride decreases the sensitivity. We see that the OPT family cannot handle this type of transformation. Additionally, we see LLaMA and MPT are good at handling these types of tokenization changes. Lower is better.</p>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 22: Increasing the total number of tokens seen during training decreases the sensitivity score. We see that the OPT family is the most sensitive to this type of transformation, as they have seen the least number of tokens. Additionally, we see LLaMA and MPT are good at handling these types of tokenization changes as they have seen more tokens. Lower is better.</p>
<h1>A. 6 Additional Experiment Details</h1>
<p>For all these experiments, we use NVIDIA RTX A4000 GPUs, finding that evaluating most models is quite inexpensive over 1000 examples, with compute requirements of less than 30 min per model for most tests. Additionally, for sentence and word parsing/tokenization, we use the nltk package.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Vocabulary size does play an additional role in the entropy of a model. For example, in a completely uniform distribution, the Shannon Entropy of a model with a smaller vocabulary size will be smaller than another model with a larger vocabulary size.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6384 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6384</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6384</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-265551745</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.00738v1.pdf" target="_blank">SeaLLMs - Large Language Models for Southeast Asia</a></p>
                <p><strong>Paper Abstract:</strong> ,</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6384.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6384.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SeaLLM-7B-v2.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SeaLLM-7B-v2.5 (continued pretraining from Gemma-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B decoder-only transformer SeaLLM variant further pre-trained from Gemma-7B and instruction/alignment tuned with increased multilingual and math SFT data; reported strong multilingual math reasoning, especially in SEA languages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SeaLLM-7B-v2.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Continued pretraining from Gemma-7B on a balanced mixture of SEA-language corpora and high-quality English (RedPajama subset); supervised fine-tuning includes synthetic and human-verified math and commonsense reasoning examples (generated from SeaLLM-13B-v1 and strong English models) plus self-preferencing preference data.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K and MATH (and translated versions in Chinese, Vietnamese, Indonesian, Thai)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step math word problems and general mathematical reasoning (GSM8K; MATH high-school math)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems (English and translated into target SEA languages); evaluated with zero-shot chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K: grade-school multi-step problems; MATH: harder high-school level problems</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot chain-of-thought prompting (evaluated on English and translated versions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: 78.5% accuracy; MATH: 34.9% accuracy (reported for SeaLLM-7B-v2.5 under zero-shot CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Paper does not report arithmetic-specific internal failure analyses; notes general issues such as hallucination and degeneration in low-resource languages (e.g., Burmese, Lao) and moderate hallucination in other languages, which may affect reasoning outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Performance improved relative to SeaLLM-7B-v2 and SeaLLM-13B-v1 after further pretraining from Gemma-7B and by scaling supervised and preference data focused on math reasoning, indicating that SFT and preference-data scaling improved math performance at this model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SeaLLMs - Large Language Models for Southeast Asia', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6384.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6384.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SeaLLM-7B-v2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SeaLLM-7B-v2 (trained from Mistral-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B SeaLLM variant built from Mistral-7B, with extended multilingual SFT including synthetic math and commonsense reasoning data; reported to outperform larger variants and ChatGPT-3.5 on math reasoning in common SEA languages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SeaLLM-7B-v2</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Continued pretraining from Mistral-7B; SFT includes significantly more math and commonsense reasoning examples, synthetically generated and human-verified, plus multilingual pretraining data.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K and MATH (translated SEA-language versions referenced); Sea-bench and MT-bench used elsewhere</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step math word problems and commonsense/math reasoning in SEA languages</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems (English and translated); paper indicates use of zero-shot chain-of-thought prompting for math benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K and MATH range (grade-school to high-school); paper emphasizes improved performance on common SEA languages (Vie, Ind, Tha)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot chain-of-thought prompting (paper states GSM8K and MATH scores are under zero-shot CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy / relative comparison to baselines</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported qualitatively: SeaLLM-7B-v2 'surpasses ChatGPT-3.5 in math reasoning in SEA languages' and 'outperforms SeaLLM-13B-v1 in higher-resource SEA languages'; exact numeric scores for SeaLLM-7B-v2 vs ChatGPT-3.5 are not provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>No arithmetic-specific failure analysis provided; general model limitations include hallucination and degeneration in some low-resource languages which may impair reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Despite being 7B, v2 is reported to outperform the 13B SeaLLM in several SEA languages for reasoning tasks, suggesting that backbone choice (Mistral) plus targeted SFT can yield stronger math reasoning than larger-but-less-specialized models; further pretraining to v2.5 improved results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SeaLLMs - Large Language Models for Southeast Asia', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6384.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6384.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SeaLLM-13B-v1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SeaLLM-13B-v1 (continued pretraining from Llama-2-13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B SeaLLM variant pre-trained from Llama-2-13B with extended tokenizer and SEA-focused continued pretraining and SFT; strong on non-Latin SEA languages but reported to trail on math reasoning for Latin-script languages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SeaLLM-13B-v1</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Continued pretraining from Llama-2-13B with extended vocabulary targeted at SEA languages, balanced multilingual corpora plus English RedPajama subset; SFT includes math and logical reasoning examples and multilingual instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Sea-bench evaluations, M3Exam for world knowledge; referenced in context of math reasoning comparisons (GSM8K/MATH mentioned elsewhere)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>General reasoning including math and translation; paper comments specifically on math reasoning relative performance by language</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language questions and instruction-following; math evaluations referenced use zero-shot chain-of-thought in other sections</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Mixed (world knowledge exams, GSM8K/MATH ranges referenced elsewhere)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>For knowledge benchmarks 3-shot native-instruction prompts; math evaluations in paper use zero-shot chain-of-thought for GSM8K/MATH (explicit numeric math scores are provided for v2.5 only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Relative performance by language/category (GPT-4 judged Sea-bench scores); accuracy for math is discussed qualitatively in language breakdowns</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: SeaLLM-13B-v1 'outperforms ChatGPT-3.5 in most non-Latin SEA languages' but 'trails ChatGPT-3.5 in Latin-based languages mostly in math reasoning skills'; exact numeric GSM8K/MATH values for this model are not reported in the paper text.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>No arithmetic-specific failure modes analyzed; general limitations include hallucination and degeneration in some low-resource languages.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>The 13B SeaLLM maintains strong multilinguality (esp. non-Latin scripts) due to vocabulary expansion, but targeted SFT and backbone changes (to Mistral/Gemma) in the 7B variants produced improved math reasoning despite smaller parameter count, suggesting significant gains from data/tokenizer/backbone choices and targeted fine-tuning rather than only scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SeaLLMs - Large Language Models for Southeast Asia', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6384.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6384.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-3.5-turbo (June 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's gpt-3.5-turbo model used as a baseline in this paper; compared against SeaLLMs on multilingual instruction-following, translation and math reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not detailed in this paper; used as an external baseline for multilingual and math evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K and MATH (comparisons reported for translated versions and SEA-language evaluations); Sea-bench / MT-bench peer comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step math word problems and general mathematical reasoning (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems; paper reports comparisons under zero-shot chain-of-thought prompting and GPT-4 judged pairwise evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K (grade-school multi-step) and MATH (harder high-school problems) as used in comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot chain-of-thought prompting used in the paper's GSM8K/MATH evaluations (paper notes SeaLLM scores were under zero-shot CoT and that SeaLLMs exceed ChatGPT-3.5 in SEA languages in several math evaluations); for world-knowledge and instruction tasks, GPT-4 judged pairwise comparisons (Sea-bench) used 3-shot native-instruction prompts in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy / GPT-4 judged pairwise scores</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported only comparatively in paper: SeaLLM-7B-v2.5 'exceeds GPT-3.5 in SEA languages' for math reasoning; exact numeric GSM8K/MATH scores for ChatGPT-3.5 are not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>This paper notes unfavorable tokenization costs for non-Latin languages when using ChatGPT, causing prohibitive API costs for low-resource languages, but does not analyze arithmetic failure modes of ChatGPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not analyzed in this paper; used as a baseline comparator rather than a subject of scaling analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SeaLLMs - Large Language Models for Southeast Asia', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Measuring mathematical problem solving with the math dataset <em>(Rating: 2)</em></li>
                <li>Metamath: Bootstrap your own mathematical questions for large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6384",
    "paper_id": "paper-265551745",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "SeaLLM-7B-v2.5",
            "name_full": "SeaLLM-7B-v2.5 (continued pretraining from Gemma-7B)",
            "brief_description": "A 7B decoder-only transformer SeaLLM variant further pre-trained from Gemma-7B and instruction/alignment tuned with increased multilingual and math SFT data; reported strong multilingual math reasoning, especially in SEA languages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SeaLLM-7B-v2.5",
            "model_family": "decoder-only transformer",
            "model_size": "7B",
            "training_data_description": "Continued pretraining from Gemma-7B on a balanced mixture of SEA-language corpora and high-quality English (RedPajama subset); supervised fine-tuning includes synthetic and human-verified math and commonsense reasoning examples (generated from SeaLLM-13B-v1 and strong English models) plus self-preferencing preference data.",
            "benchmark_name": "GSM8K and MATH (and translated versions in Chinese, Vietnamese, Indonesian, Thai)",
            "task_type": "Multi-step math word problems and general mathematical reasoning (GSM8K; MATH high-school math)",
            "problem_format": "Natural-language word problems (English and translated into target SEA languages); evaluated with zero-shot chain-of-thought prompting",
            "difficulty_level": "GSM8K: grade-school multi-step problems; MATH: harder high-school level problems",
            "prompting_method": "Zero-shot chain-of-thought prompting (evaluated on English and translated versions)",
            "performance_metric": "Accuracy (percentage)",
            "performance_value": "GSM8K: 78.5% accuracy; MATH: 34.9% accuracy (reported for SeaLLM-7B-v2.5 under zero-shot CoT)",
            "internal_analysis": null,
            "failure_modes": "Paper does not report arithmetic-specific internal failure analyses; notes general issues such as hallucination and degeneration in low-resource languages (e.g., Burmese, Lao) and moderate hallucination in other languages, which may affect reasoning outputs.",
            "scaling_trend": "Performance improved relative to SeaLLM-7B-v2 and SeaLLM-13B-v1 after further pretraining from Gemma-7B and by scaling supervised and preference data focused on math reasoning, indicating that SFT and preference-data scaling improved math performance at this model scale.",
            "uuid": "e6384.0",
            "source_info": {
                "paper_title": "SeaLLMs - Large Language Models for Southeast Asia",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "SeaLLM-7B-v2",
            "name_full": "SeaLLM-7B-v2 (trained from Mistral-7B)",
            "brief_description": "A 7B SeaLLM variant built from Mistral-7B, with extended multilingual SFT including synthetic math and commonsense reasoning data; reported to outperform larger variants and ChatGPT-3.5 on math reasoning in common SEA languages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SeaLLM-7B-v2",
            "model_family": "decoder-only transformer",
            "model_size": "7B",
            "training_data_description": "Continued pretraining from Mistral-7B; SFT includes significantly more math and commonsense reasoning examples, synthetically generated and human-verified, plus multilingual pretraining data.",
            "benchmark_name": "GSM8K and MATH (translated SEA-language versions referenced); Sea-bench and MT-bench used elsewhere",
            "task_type": "Multi-step math word problems and commonsense/math reasoning in SEA languages",
            "problem_format": "Natural-language word problems (English and translated); paper indicates use of zero-shot chain-of-thought prompting for math benchmarks",
            "difficulty_level": "GSM8K and MATH range (grade-school to high-school); paper emphasizes improved performance on common SEA languages (Vie, Ind, Tha)",
            "prompting_method": "Zero-shot chain-of-thought prompting (paper states GSM8K and MATH scores are under zero-shot CoT)",
            "performance_metric": "Accuracy / relative comparison to baselines",
            "performance_value": "Reported qualitatively: SeaLLM-7B-v2 'surpasses ChatGPT-3.5 in math reasoning in SEA languages' and 'outperforms SeaLLM-13B-v1 in higher-resource SEA languages'; exact numeric scores for SeaLLM-7B-v2 vs ChatGPT-3.5 are not provided in the text.",
            "internal_analysis": null,
            "failure_modes": "No arithmetic-specific failure analysis provided; general model limitations include hallucination and degeneration in some low-resource languages which may impair reasoning.",
            "scaling_trend": "Despite being 7B, v2 is reported to outperform the 13B SeaLLM in several SEA languages for reasoning tasks, suggesting that backbone choice (Mistral) plus targeted SFT can yield stronger math reasoning than larger-but-less-specialized models; further pretraining to v2.5 improved results.",
            "uuid": "e6384.1",
            "source_info": {
                "paper_title": "SeaLLMs - Large Language Models for Southeast Asia",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "SeaLLM-13B-v1",
            "name_full": "SeaLLM-13B-v1 (continued pretraining from Llama-2-13B)",
            "brief_description": "A 13B SeaLLM variant pre-trained from Llama-2-13B with extended tokenizer and SEA-focused continued pretraining and SFT; strong on non-Latin SEA languages but reported to trail on math reasoning for Latin-script languages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SeaLLM-13B-v1",
            "model_family": "decoder-only transformer",
            "model_size": "13B",
            "training_data_description": "Continued pretraining from Llama-2-13B with extended vocabulary targeted at SEA languages, balanced multilingual corpora plus English RedPajama subset; SFT includes math and logical reasoning examples and multilingual instruction tuning.",
            "benchmark_name": "Sea-bench evaluations, M3Exam for world knowledge; referenced in context of math reasoning comparisons (GSM8K/MATH mentioned elsewhere)",
            "task_type": "General reasoning including math and translation; paper comments specifically on math reasoning relative performance by language",
            "problem_format": "Natural-language questions and instruction-following; math evaluations referenced use zero-shot chain-of-thought in other sections",
            "difficulty_level": "Mixed (world knowledge exams, GSM8K/MATH ranges referenced elsewhere)",
            "prompting_method": "For knowledge benchmarks 3-shot native-instruction prompts; math evaluations in paper use zero-shot chain-of-thought for GSM8K/MATH (explicit numeric math scores are provided for v2.5 only).",
            "performance_metric": "Relative performance by language/category (GPT-4 judged Sea-bench scores); accuracy for math is discussed qualitatively in language breakdowns",
            "performance_value": "Qualitative: SeaLLM-13B-v1 'outperforms ChatGPT-3.5 in most non-Latin SEA languages' but 'trails ChatGPT-3.5 in Latin-based languages mostly in math reasoning skills'; exact numeric GSM8K/MATH values for this model are not reported in the paper text.",
            "internal_analysis": null,
            "failure_modes": "No arithmetic-specific failure modes analyzed; general limitations include hallucination and degeneration in some low-resource languages.",
            "scaling_trend": "The 13B SeaLLM maintains strong multilinguality (esp. non-Latin scripts) due to vocabulary expansion, but targeted SFT and backbone changes (to Mistral/Gemma) in the 7B variants produced improved math reasoning despite smaller parameter count, suggesting significant gains from data/tokenizer/backbone choices and targeted fine-tuning rather than only scale.",
            "uuid": "e6384.2",
            "source_info": {
                "paper_title": "SeaLLMs - Large Language Models for Southeast Asia",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "ChatGPT-3.5",
            "name_full": "gpt-3.5-turbo (June 2023)",
            "brief_description": "OpenAI's gpt-3.5-turbo model used as a baseline in this paper; compared against SeaLLMs on multilingual instruction-following, translation and math reasoning tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (ChatGPT-3.5)",
            "model_family": "decoder-only transformer",
            "model_size": "not specified in paper",
            "training_data_description": "Not detailed in this paper; used as an external baseline for multilingual and math evaluations.",
            "benchmark_name": "GSM8K and MATH (comparisons reported for translated versions and SEA-language evaluations); Sea-bench / MT-bench peer comparisons",
            "task_type": "Multi-step math word problems and general mathematical reasoning (used as baseline)",
            "problem_format": "Natural-language word problems; paper reports comparisons under zero-shot chain-of-thought prompting and GPT-4 judged pairwise evaluations",
            "difficulty_level": "GSM8K (grade-school multi-step) and MATH (harder high-school problems) as used in comparisons",
            "prompting_method": "Zero-shot chain-of-thought prompting used in the paper's GSM8K/MATH evaluations (paper notes SeaLLM scores were under zero-shot CoT and that SeaLLMs exceed ChatGPT-3.5 in SEA languages in several math evaluations); for world-knowledge and instruction tasks, GPT-4 judged pairwise comparisons (Sea-bench) used 3-shot native-instruction prompts in some cases.",
            "performance_metric": "Accuracy / GPT-4 judged pairwise scores",
            "performance_value": "Reported only comparatively in paper: SeaLLM-7B-v2.5 'exceeds GPT-3.5 in SEA languages' for math reasoning; exact numeric GSM8K/MATH scores for ChatGPT-3.5 are not provided in the paper.",
            "internal_analysis": null,
            "failure_modes": "This paper notes unfavorable tokenization costs for non-Latin languages when using ChatGPT, causing prohibitive API costs for low-resource languages, but does not analyze arithmetic failure modes of ChatGPT-3.5.",
            "scaling_trend": "Not analyzed in this paper; used as a baseline comparator rather than a subject of scaling analysis.",
            "uuid": "e6384.3",
            "source_info": {
                "paper_title": "SeaLLMs - Large Language Models for Southeast Asia",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Measuring mathematical problem solving with the math dataset",
            "rating": 2,
            "sanitized_title": "measuring_mathematical_problem_solving_with_the_math_dataset"
        },
        {
            "paper_title": "Metamath: Bootstrap your own mathematical questions for large language models",
            "rating": 2,
            "sanitized_title": "metamath_bootstrap_your_own_mathematical_questions_for_large_language_models"
        }
    ],
    "cost": 0.013280750000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SeaLLMs -Large Language Models for Southeast Asia</p>
<p>Xuan-Phi Nguyen 
DAMO Academy
Alibaba Group</p>
<p>Wenxuan Zhang 
DAMO Academy
Alibaba Group</p>
<p>Xingxuan Li 
DAMO Academy
Alibaba Group</p>
<p>DAMO Academy
Alibaba Group</p>
<p>Mahani Aljunied 
DAMO Academy
Alibaba Group</p>
<p>Zhiqiang Hu 
DAMO Academy
Alibaba Group</p>
<p>Chenhui Shen 
DAMO Academy
Alibaba Group</p>
<p>YewKen Chia 
DAMO Academy
Alibaba Group</p>
<p>Jianyu Wang 
DAMO Academy
Alibaba Group</p>
<p>Qingyu Tan 
DAMO Academy
Alibaba Group</p>
<p>Liying Cheng 
DAMO Academy
Alibaba Group</p>
<p>Guanzheng Chen 
DAMO Academy
Alibaba Group</p>
<p>Yue Deng 
DAMO Academy
Alibaba Group</p>
<p>Sen Yang 
DAMO Academy
Alibaba Group</p>
<p>Chaoqun Liu 
DAMO Academy
Alibaba Group</p>
<p>Hang Zhang 
DAMO Academy
Alibaba Group</p>
<p>Bing Lidong 
DAMO Academy
Alibaba Group</p>
<p>SeaLLMs -Large Language Models for Southeast Asia
E45026FDDD21D5708BABB7E37665DD59
Despite the remarkable achievements of large language models (LLMs) in various tasks, there remains a linguistic bias that favors highresource languages, such as English, often at the expense of low-resource and regional languages.To address this imbalance, we introduce SeaLLMs, an innovative series of language models that specifically focuses on Southeast Asian (SEA) languages.SeaLLMs are built upon popular English-centric models through continued pre-training with an extended vocabulary, specialized instruction and alignment tuning to better capture the intricacies of regional languages.This allows them to respect and reflect local cultural norms, customs, stylistic preferences, and legal considerations.Our comprehensive evaluation demonstrates that SeaLLM models exhibit superior performance across a wide spectrum of linguistic tasks and assistant-style instructionfollowing capabilities relative to comparable open-source models.Moreover, they outperform ChatGPT-3.5 in non-Latin languages, such as Thai, Khmer, Lao, and Burmese, by large margins while remaining lightweight and cost-effective to operate.</p>
<p>Introduction</p>
<p>The advent of large language models (LLMs) has radically transformed the field of natural language processing, demonstrating remarkable abilities in text generation, comprehension, and decision-making tasks (Brown et al., 2020;Ope-nAI, 2023a,b;Touvron et al., 2023a,b;Thoppilan et al., 2022;Jiang et al., 2023;Wei et al., 2023;Bai et al., 2023).While the proficiencies of these models are extraordinary, the majority of existing LLMs embody a linguistic hierarchy overwhelmingly dominated by English (Ahuja et al., 2023;Lai et al., 2023;Zhang et al., 2023).This dominance undermines the multilingual capability of such models, with particularly prejudicial outcomes for lower-resource and regional languages, where data scarcity and tokenization challenges lead to disproportionately poor model performance.This linguistic disparity not only impedes access to state-ofthe-art AI technologies for non-English-speaking populations but also risks cultural homogenization and the loss of linguistic diversity.While hyperpolyglot models exist (Scao et al., 2022;Muennighoff et al., 2022;Wei et al., 2023), they may pay a high cost for high-resource language performance while lacking in multilingual instruction-following abilities.</p>
<p>Recognizing the urgent need to democratize AI and empower linguistically diverse regions, we introduce SeaLLMs1 , a suite of specialized language models optimized for Southeast Asian languages2 .These languages, while rich and diverse, often lack the extensive dataset support available for more widely spoken languages, resulting in a stark performance gap in existing LLM applications.</p>
<p>As a long-term continuous effort, as of this writing, SeaLLMs come in three versions (v1, v2, v2.5).SeaLLM-13B-v1, which was pre-trained from Llama-2-13B, eclipses the performance of most available open-source LLMs in a comprehensive array of tasks including world knowledge assessments, language comprehension, and generative capabilities in SEA languages.For English and alike, SeaLLMs do not only preserve, but also demonstrate enhanced performance in tasks that were part of the original Llama training set.When evaluated on multilingual instructionfollowing tasks with GPT-4 as a judge (Zheng et al., 2023), SeaLLM-13B-v1 outperforms ChatGPT-3.5 by large margins in less-represented languages such as Khmer, Lao or Burmese.Meanwhile, SeaLLM-7B-v2, which was pre-trained from Mistral-7B (Jiang et al., 2023), demonstrates better performances in math and commonsense reasoning than comparable baselines, surpassing ChatGPT-3.5 in reasoning for common SEA languages, while being much smaller in sizes.Later, SeaLLM-7B-v2.5, which was further pre-trained from Gemma-7B (Team et al., 2024), shows significant improvements in SEA languages over SeaLLM-7B-v2.</p>
<p>Figure 2 illustrates the four-stage training process of SeaLLMs.In the first stage, detailed in Section 2.3, we conduct continuous pre-training from the foundational models (Touvron et al., 2023b;Jiang et al., 2023) with an extended vocabulary tailored for SEA languages.Next, we fine-tune the model in a novel hybrid paradigm with a mixture of multilingual pre-training data and Englishdominant instruction fine-tuning data (Section 3.2).The following stage subsequently fine-tunes the model on a balanced and custom-built multilingual SFT dataset.Finally, we conduct self-preferencing alignment optimization using the SeaLLM model itself, without relying on human annotators or more powerful LLMs (OpenAI, 2023b).</p>
<p>Pre-training</p>
<p>Pre-training Data</p>
<p>The pre-training data comprises a heterogeneous assortment of documents sourced from several publicly accessible repositories (Suárez et al., 2019;Raffel et al., 2019;Computer, 2023;Foundation).Specifically, during the creation of the pre-training data, we include web-based corpora such as Com-mon Crawl (Wenzek et al., 2020), journalistic content such as CC-News, text corpora with expertlycurated knowledge such as Wikipedia (Foundation), and some scholarly publications.After collecting the data, we employ a language identifier (Bojanowski et al., 2017) to retain the documents for the major languages in Southeast Asia, namely Thai, Vietnamese, Indonesian, Chinese, Khmer, Lao, Malay, Burmese, and Tagalog, and discard the remaining ones.Subsequent stages of data refinement involve the multiple modules dedicated to data cleansing and content filtration.We blend such data with the highest quality English data from RedPajama subset (Computer, 2023) in more balanced ratios, as we found that such English data are useful to preserve the original learnt knowledge.</p>
<p>Vocabulary Expansion</p>
<p>Table 1 describes how expensive it is to process an under-represented non-Latin language.For example, encoding a single sentence in Thai requires 4.3 times more tokens than its English equivalent.The reason for this is that most English language models employ a BPE tokenizer (Sennrich et al., 2016) that inefficiently segments texts from non-Latin scripts into disproportionately lengthy byte sequences, which inadequately represent the underlying semantic content, resulting in diminished model performance (Nguyen et al., 2023).To that end, we propose a novel vocabulary expansion technique, as formally described in Algorithm 1 in the Appendix.This technique involves recursively merging whole-word and sub-word token pieces of a new language from a highly multilingual target tokenizer (i.e., the NLLB tokenizer (Costa-jussà et al</p>
<p>2022</p>
<p>)), to the existing LLM tokenizer.This new set of retrieved tokens are then pruned to remove rarely appearing and low-quality tokens before being added to the final SeaLLM tokenizer.Table 1 demonstrates the efficiency of the new vocabulary.The compression ratio for Thai text has markedly improved from 4.29 to 1.57, signifying a 2.7-fold increase in the length of Thai text that can be encoded within the same context constraints.At the same time, the compression of English text has experienced a negligible reduction of 0.3%, thus maintaining its tokenization effectiveness.</p>
<p>We applied our vocabulary expansion for SeaLLM v1 and v2 with Llama-2 and Mistral-7B as backbones due to their limit 32K-token vocabulary.However, we did not extend the tokenizer for SeaLLM-7B-v2.5, which inherits a sufficiently large 250K-token vocabulary from Gemma-7B.</p>
<p>Pre-training Process</p>
<p>We organize our pre-training dataset based on the language of the content and the quality of the data, as mentioned in Section 2.1.We setup a separate stream of data for each language, and dynamically control and balance the sampling ratio of each lan-guage.We pack multilingual documents into a single sequence up to the maximum context length.During the last steps of pre-training, we re-feed the model with more high quality data, which it has previously seen, to readjust the model's learning focus back towards the high-quality data, improving the model's performance.</p>
<p>3 Supervised Fine-tuning (SFT)</p>
<p>Supervised Fine-tuning Data</p>
<p>Our supervised finetuning (SFT) data consists of many categories, including text understanding and processing, math and logical reasoning, usercentric instruction-following, and natural dialog data.As most public and open-source SFT data are English-only (Longpre et al., 2023;Lian et al., 2023;Mukherjee et al., 2023;Lee et al., 2023), various techniques were implemented to enhance the multilingual aspect of the model.These include sourcing natural data from local websites in natural settings, selectively translating from English data, employing self-instruction, and using advanced prompting techniques (Wang et al., 2022;Madaan et al., 2023;Nguyen et al., 2023).As those synthetically generated data may remain incorrect or low-quality, native speakers3 were then engaged to further verify, filter, and edit such synthetic responses to finalize the SFT dataset.We find that engaging the annotators to verify and modify model-generated responses is more efficient than having them write responses from scratch.Safetyrelated data also played a crucial role in fine-tuning SeaLLMs.We manually collected and prepared country-relevant safety data, which covered a broad range of culturally and legally sensitive topics in each of these countries.This was necessary as such topics are often overlooked or may even conflict with open-source English-centric safety data (Deng et al., 2023).</p>
<p>For SeaLLM-7B-v2 and SeaLLM-7B-v2.5,we incorporate significantly more SFT data relating to math and commonsense reasoning.Such data is synthetically generated with SeaLLM-13B-v1, as well as strong English models (Jiang et al., 2024;Bai et al., 2023) using a combination of few-shot paraphrasing and translation techniques (Yu et al., 2023).</p>
<p>Supervised Fine-tuning</p>
<p>Pre-train and SFT Hybrid.As our SFT data is still significantly English due to contributions of open-source data, directly conducting SFT on it may overshadow the smaller SEA language datasets.Therefore, we propose incorporating an additional step prior to complete fine-tuning, namely Pre-train &amp; SFT Hybrid.In this step, the model is further trained on a combination of the pre-training corpus and a large portion of English SFT data, leaving the remaining and more balanced amount of English SFT data to the next stage.During this hybrid stage, the model processes both general pre-training content and instruction-following examples.We mask the source side of the instruction or supervised data to prevent the model from overfitting to the training examples and to reduce the risk of it simply memorizing the input data instead of learning the more generalized ability to follow instructions.</p>
<p>Supervised Fine-tuning.We conduct supervised fine-tuning by compiling instructions from a variety of sources explained in Section 3.1, combining them at random into a single, consolidated sequence to maximize efficiency.To enhance the multi-turn conversation capability, in the later stage of fine-tuning, we further artificially create multiturn conversations by randomly joining several single-turn instructions together.</p>
<p>Self-Preferencing Optimization</p>
<p>Alignment from human feedback preference has been key to the success of many AI-assistant language models (Stiennon et al., 2020;Touvron et al., 2023b;Rafailov et al., 2023;Ouyang et al., 2022).To save the cost of human preference annotation work, some have sought to use powerful LLMs like GPT-4 (OpenAI, 2023b) to play the part of a preference data generator (Tunstall et al., 2023).However, that may not even be feasible for low-resource non-Latin languages because of the unfavorable tokenization of ChatGPT as explained in Section 2.2.In other words, even short prompts would exceed their context-length and the API-call costs would explode by up to 17 times.Therefore, we use our own SeaLLM SFT models to generate preference data by asking it to indicate its preference between two of its own responses, given a question based on certain humanwritten criteria.To eliminate position bias, we swap the order of the responses and remove samples with inconsistent preference.The data is later used to employ direct preference optimization (Rafailov et al., 2023) to significantly improve the model abilities as an assistant.As such, unlike other works (Mukherjee et al., 2023;Tunstall et al., 2023), our models are free from relying on powerful close-sourced models like GPT-4 to improve the performance in low-resource languages.Our self-preferencing method also shares certain flavors with another self-rewarding mechanism (Yuan et al., 2024).4</p>
<p>Evaluation</p>
<p>Model Variants</p>
<p>We trained multiple variants of SeaLLMs, as specified in the following.</p>
<p>• SeaLLM-7B-v1: Trained from Llama-2-7B, it supports the 10 official languages used in Southeast Asia.</p>
<p>• SeaLLM-13B-v1: Trained from Llama-2-13B, it outperforms ChatGPT-3.5 in most non-Latin SEA languages (Khm, Lao, Mya and Tha) by large margins.</p>
<p>• SeaLLM-7B-v2: Trained from Mistral-7B, it outperforms SeaLLM-13B-v1 by far in higherresource SEA languages (Vie, Ind, Tha), and surpasses ChatGPT-3.5 in math reasoning in SEA languages.</p>
<p>• SeaLLM-7B-v2.5:Trained from Gemma-7B, it outperforms SeaLLM-7B-v2 and SeaLLM-13B-v1 remarkably and surpasses ChatGPT-3.5 in various aspects in SEA languages, especially non-Latin languages.</p>
<p>Sea-bench Peer Comparison</p>
<p>While there are popular benchmarks to evaluate LLMs as a helpful assistant, such as MT-bench (Zheng et al., 2023) • Task-solving: This type of data comprises various text understanding and processing tasks, such as summarization, translation, etc.</p>
<p>• Math-reasoning: This includes math problems and logical reasoning tasks.</p>
<p>• General-instruction data: This consists of general user-centric instructions, which evaluate the model's ability in general knowledge and writing.</p>
<p>• NaturalQA: This consists of queries posted by real users, often in popular local forums, with a variety of subjects and topics of local interest.The aim is to test the model's capacity to understand and respond coherently to colloquial language, natural expressions and idiomatic language, and locally contextualized references.</p>
<p>• Safety: This includes both general safety and local context-related safety instructions.While most general safety questions are translated from open sources, other local countryspecific safety instructions are written by linguists of each language.</p>
<p>As inspired by MT-bench (Zheng et al., 2023), we evaluate and compare SeaLLMs with wellknown and state-of-the-art models using GPT-4 as a judge in a score-based grading metrics and a peer comparison (or pairwise comparison) manner.</p>
<p>Figure 1 compares our SeaLLM (v2, v2.5) chat models with Qwen1.5-7B-chat(Bai et al., 2023) and the widely reputed ChatGPT-3.55(OpenAI, 2023a).In the "By Category" chart, SeaLLM-7B-v2.5 performs on par with or surpasses ChatGPT-3.5 across various linguistic and writing tasks.This is largely thanks to the large gap in low-resource non-Latin languages, such as Burmese (Mya), Lao, Khmer and Thai, as seen in the "By language" chart on the right in Figure 1.(Zheng et al., 2023) for closed, open, multilingual and monolingual (as indicated by their authors on Huggingface.)models.</p>
<p>MT-bench</p>
<p>We also compare our models with certain baselines on the English MT-Bench (Zheng et al., 2023) in Table 3.As shown, SeaLLM-7B-v2 model demonstrates outstanding ability in English, given its size.Table 4: GSM8K and MATH scores (Cobbe et al., 2021;Hendrycks et al., 2021b) and their translated-versions in Chinese, Vietnamese, Indonesian and Thai, under zero-shot chain-of-thought prompting for different models.It is also a rare multilingual model in the 7B realm, especially since it focuses on non-mainstream languages.</p>
<p>World Knowledge</p>
<p>In this section, we evaluate our models and reputable chat baselines (Touvron et al., 2023b;Wei et al., 2023;OpenAI, 2023a) in terms of world knowledge.For knowledge across languages, we use the M3Exam benchmark (Zhang et al., 2023), which consists of real questions from human exam papers with various degrees of difficulty, ranging from primary school to high school examinations.We evaluate M3Exam with 3-shot nativeinstruction prompts across English, Chinese, Vietnamese, Indonesian and Thai.We also evaluate our models with the well-known English-centric MMLU benchmark (Hendrycks et al., 2021a).Table 2 details the evaluations of world knowledge across multiple languages and models of different sizes.SeaLLM-7B-v2.5 exhibits the best performance given its size and is competitive to GPT-3.5.</p>
<p>Math Reasoning</p>
<p>Table 4 shows the GSM8K and MATH scores (Cobbe et al., 2021;Hendrycks et al., 2021b) for zero-shot chain-of-thought prompting for English and their translated version in Chinese, Vietnamese, Indonesian and Thai.As shown, SeaLLM-7B-v2.5 shows competitive English performance in math reasoning compared to open-source models, with 78.5 in GSM8K and 34.9 in MATH.It also exceeds GPT-3.5 in SEA languages.This is achieved by scaling supervised and preference data in math reasoning in multilingual settings.</p>
<p>Machine Translation</p>
<p>To benchmark the machine translation performance of our SeaLLMs, we evaluate 4-shot chrF++ scores on the test sets from Flores-200 (Costa-jussà et al., 2022).As can be seen from Figure 3, SeaLLM-13B exhibits clear superiority over ChatGPT-3.5 in low-resource languages, such as Lao and Khmer, while maintaining comparable performance with ChatGPT-3.5 in most higher resource languages (e.g., Vietnamese and Indonesian).</p>
<p>For direct translation between SEA languages, as shown in Figure 4, our SeaLLM-13B-v1 model still achieves higher chrF++ scores than ChatGPT-3.5 in most cases, especially when the translation pairs involve low-resource languages.Overall, we believe our SeaLLMs will play a key role in facilitating communication and cultural exchange across communities in Southeast Asia.</p>
<p>Conclusion</p>
<p>In conclusion, our research presents a substantial advance in the development of equitable and culturally aware AI with the creation of SeaLLMs, a specialized suite of language models attuned to the linguistic and cultural landscapes of Southeast Asia.Through rigorous pre-training enhancements and culturally tailored fine-tuning processes, SeaLLMs have demonstrated exceptional proficiency in language understanding and generation tasks, challenging the performance of dominant players such as ChatGPT-3.5,particularly in SEA languages.The models' attunement to local norms and legal stipulations-validated by human evaluations-establishes SeaLLMs as not only a technical breakthrough but a socially responsive innovation, poised to democratize access to high-quality AI language tools across linguistically diverse regions.This work lays a foundation for further research into language models that respect and uphold the rich tapestry of human languages and cultures, ultimately driving the AI community towards a more inclusive future.</p>
<p>Limitations</p>
<p>SeaLLMs are among the most linguistically diverse multilingual large language models with remarkable abilities in languages beyond mainstream.However, they do not come without limitations.First, they only scratch the surface of the regionally linguistic diversity with 9 most common and representative languages, while there are hundreds other languages spoken in the Southeast Asia, such as Javanese and Tamil.Second, despite outperforming other popular models in non-Latin low-resource languages, SeaLLM models still suffer from considerable hallucination and degeneration under certain circumstances for languages such as Burmese and Lao.Moderate hallucination is still inevitable for other common languages.</p>
<p>Acknowledgement</p>
<p>We would like to express our special thanks to our professional and native linguists, Tantong Champaiboon, Nguyen Ngoc Yen Nhi and Tara Devina Putri, who helped build, evaluate, and fact-check our sampled pretraining and SFT dataset as well as evaluating our models across different aspects, especially safety.</p>
<p>A Vocabulary Expansion</p>
<p>Algorithm 1 explains in details how we perform selective and recursive merger of tokens from target NLLB vocabulary into the original Llama vocabulary to enrich the linguistic coverage for new and low-resource languages.Specifically, given a small seed unlabeled dataset of a given new language, the algorithm first tokenizes a document with the current Llama tokenizer.The resulting tokens are then exhaustively merged into longer tokens that are supported by the target NLLB vocabulary.During this merger process, any intermediate sub-word is also added to the Llama tokenizer as long as they exist in the rich NLLB vocabulary.</p>
<p>The new set of collected tokens are then pruned to remove rarely appearing and low-quality tokens before being added to the final SeaLLM tokenizer.This frequency-based pruning process ensures the new language is sufficiently and efficiently encoded without introducing tokens from other existing languages (e.g., English), which may disrupt the learned knowledge during the Llama-2 pre-training stage.Algorithm 1 Vocabulary Extension algorithm: V i is Llama vocabulary, V t is target NLLB vocabulary, D is unlabeled data and m is minimum frequency.T ← Prune t i ∈ T with corresponding f t ∈ F where f t &lt; m ▷ Remove rare words 23:</p>
<p>B Sea-bench Evaluation Details
V f inal ← V i ∪ T 24:
return V f inal</p>
<p>Figure 1 :
1
Figure1: Sea-bench (Section 4.2) scores as evaluated byGPT-4 (Zheng et al., 2023)  for different models.Each radar chart compares scores as averaged across 5 categories (left) and 9 languages (right).Detailed breakdown by each category and language is given in Figure5in the Appendix.</p>
<p>Figure 3 :
3
Figure 3: Translation chrF++ scores of various models for both SEA languages to English and English to SEA languages directions.</p>
<p>Figure 4 :
4
Figure 4: Direct translation between SEA languages.Scores are indicated as the different between the respective chrF++ score of SeaLLM-13B-v1 minus that of ChatGPT-3.5.Red colors suggests SeaLLM-13B-v1 is better, while blue colors indicates ChatGPT is better.</p>
<p>Figure 5
5
Figure 5 breaks down the GPT-4 rated Sea-bench score-based evaluations of SeaLLM-13b and other baselines by both language and task category.As shown, our SeaLLM-13b model far exceeds ChatGPT-3.5 in most non-Latin languages, such as Burmese (Mya), Lao and Khmer, though it trails behind this formidable competitor in Latin-based languages, mostly in math reasoning skills.</p>
<p>Figure 5 :
5
Figure 5: Sea-bench scores as evaluated by GPT-4 for different models across 9 languages and 5 categories.</p>
<p>Table 1 :
1.,
Averaged compression ratios between the tokenized length of texts of each language produced by different tokenizers versus the baseline tokenized length of same-meaning English equivalents produced by Chat-GPT tokenizer (i.e., it costs 15.6x more tokens to encode Khmer than English with ChatGPT tokenizer).SeaLLM's ratios are applicable only for v1 and v2.</p>
<p>Table 3 :
3
MT-Bench scores
ModelLanguages MT-benchGPT-4-turboMulti9.32Mixtral-8x7B (46B)Multi8.3Starling-LM-7B-alpha Mono (Eng)8.0OpenChat-3.5-7BMono (Eng)7.81SeaLLM-7B-v2Multi7.54SeaLLM-7B-v2.5Multi7.43Llama-2-70B-chatMono6.86Mistral-7B-instructMono6.84SeaLLM-13B-v1Multi6.32</p>
<p>1: function EXHAUSTIVEMERGE(V i , V t , t V )for each consecutive token pair (prev, next) in t V doT new ← EXHAUSTIVEMERGE(V i , V t , t V )▷ obtain new words from V t based on d
2:T new ← empty set ∅3:repeat4:5:t merged ← ⟨prev⟩⟨next⟩▷ Form a new token13:V ← V i14:F ← empty set ∅15:T ← empty set ∅16:for document d in D do17:t V ← tokenize(V, d)▷ tokenize the document18:19:
6:if t merged exists in V t then 7:Replace (prev, next) with t merged in t V ▷ Update t V with new token 8:T new ← T new ∪ t merged 9:break 10:until no new token added to T new 11:return T new 12: function VOCABEXTEND(V i , V t , D, m) V ← V ∪ T new ▷ update V with new words T new 20: T ← T ∪ T new 21:F ← Update frequencies of T new to F ▷ update appearance frequencies of T new 22:</p>
<p>https://github.com/DAMO-NLP-SG/SeaLLMs
English (Eng), Chinese (Zho), Indonesian (Ind), Vietnamese (Vie), Thai (Tha), Khmer (Khm), Lao, Malay (Msa), Burmese (Mya) and Tagalog(Tgl) <br />
Hired by our organization, they are not co-authors.
Our work was publicly available beforeYuan et al. (2024).
 gpt-3.5-turbo June 2023 version.   </p>
<p>MEGA: multilingual evaluation of generative AI. Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, Sunayana Sitaram, CoRR, abs/2303.125282023</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>Enriching word vectors with subword information. Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov, Transactions of the Association for Computational Linguistics. 52017</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Redpajama: An open source recipe to reproduce llama training dataset. 2023Together Computer</p>
<p>James Marta R Costa-Jussà, Onur Cross, Maha Çelebi, Kenneth Elbayad, Kevin Heafield, Elahe Heffernan, Janice Kalbassi, Daniel Lam, Jean Licht, Maillard, arXiv:2207.04672No language left behind: Scaling human-centered machine translation. 2022arXiv preprint</p>
<p>Yue Deng, Wenxuan Zhang, arXiv:2310.06474Sinno Jialin Pan, and Lidong Bing. 2023. Multilingual jailbreak challenges in large language models. arXiv preprint</p>
<p>Wikimedia Foundation. Wikimedia downloads. </p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021a</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021bNeurIPS</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.04088Mixtral of experts. 2024arXiv preprint</p>
<p>Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning. Dac Viet, Lai, Trung Nghia, Amir Ngo, Ben Pouran, Hieu Veyseh, Franck Man, Trung Dernoncourt, Thien Huu Bui, Nguyen, CoRR, abs/2304.056132023</p>
<p>Platypus: Quick, cheap, and powerful refinement of llms. Ariel N Lee, Cole J Hunter, Nataniel Ruiz, 2023</p>
<p>Chanvichet Vong, and "Teknium. Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Openorca: An open dataset of gpt augmented flan reasoning traces. 2023</p>
<p>The flan collection: Designing data and methods for effective instruction tuning. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, Adam Roberts, 2023</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, arXiv:2303.176512023arXiv preprint</p>
<p>Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, arXiv:2211.01786Crosslingual generalization through multitask finetuning. 2022arXiv preprint</p>
<p>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah, Orca: Progressive learning from complex explanation traces of gpt-4. 2023</p>
<p>Democratizing llms for low-resource languages by leveraging their english dominant abilities with linguistically-diverse prompts. Xuan-Phi Nguyen, Sharifah Mahani Aljunied, Shafiq Joty, Lidong Bing, arXiv:2306.113722023arXiv preprint</p>
<p>OpenAI. 2023b. Gpt-4 technical report. arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, Chelsea Finn, arXiv:2305.18290Direct preference optimization: Your language model is secretly a reward model. 2023arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, 2019text transformer. arXiv e-prints</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, arXiv:2211.05100Bloom: A 176b-parameter open-access multilingual language model. 2022arXiv preprint</p>
<p>Neural machine translation of rare words with subword units. Rico Sennrich, Barry Haddow, Alexandra Birch, 10.18653/v1/P16-1162Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics20161</p>
<p>Learning to summarize with human feedback. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano, Advances in Neural Information Processing Systems. 202033</p>
<p>Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures. Pedro Javier, Ortiz Suárez, Benoît Sagot, Laurent Romary, 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7). 2019Leibniz-Institut für Deutsche Sprache</p>
<p>Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, arXiv:2403.08295Gemma: Open models based on gemini research and technology. 2024arXiv preprint</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Lamda: Language models for dialog applications. 2022arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, arXiv:2302.13971Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023barXiv preprint</p>
<p>. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro Von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M Rush, Thomas Wolf, 2023Direct distillation of lm alignmentZephyr</p>
<p>Self-instruct: Aligning language model with self generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, arXiv:2212.105602022arXiv preprint</p>
<p>Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, arXiv:2307.06018Polylm: An open source polyglot large language model. 2023arXiv preprint</p>
<p>CCNet: Extracting high quality monolingual datasets from web crawl data. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, Edouard Grave, Proceedings of the 12th Language Resources and Evaluation Conference. the 12th Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources Association2020</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, arXiv:2309.12284Metamath: Bootstrap your own mathematical questions for large language models. 2023arXiv preprint</p>
<p>Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston, arXiv:2401.10020Self-rewarding language models. 2024arXiv preprint</p>
<p>M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models. Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Ken Yew, Lidong Chia, Bing, CoRR, abs/2306.051792023</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, arXiv:2306.05685Judging llm-as-a-judge with mt-bench and chatbot arena. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
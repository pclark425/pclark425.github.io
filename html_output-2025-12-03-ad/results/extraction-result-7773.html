<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7773 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7773</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7773</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-272968899</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.18812v1.pdf" target="_blank">LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis</a></p>
                <p><strong>Paper Abstract:</strong> In response to the growing complexity and volume of scientific literature, this paper introduces the LLMs4Synthesis framework, designed to enhance the capabilities of Large Language Models (LLMs) in generating high-quality scientific syntheses. This framework addresses the need for rapid, coherent, and contextually rich integration of scientific insights, leveraging both open-source and proprietary LLMs. It also examines the effectiveness of LLMs in evaluating the integrity and reliability of these syntheses, alleviating inadequacies in current quantitative metrics. Our study contributes to this field by developing a novel methodology for processing scientific papers, defining new synthesis types, and establishing nine detailed quality criteria for evaluating syntheses. The integration of LLMs with reinforcement learning and AI feedback is proposed to optimize synthesis quality, ensuring alignment with established criteria. The LLMs4Synthesis framework and its components are made available, promising to enhance both the generation and evaluation processes in scientific research synthesis.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7773.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7773.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NineCriteria</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nine Criteria of Synthesis Quality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-grained evaluation framework of nine linguistic and semantic criteria (relevancy, correctness, completeness, informativeness, integration, cohesion, coherence, readability, conciseness) used to rate LLM-generated scientific syntheses on a 1–5 scale per criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B; GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B; proprietary (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multidisciplinary scientific literature summarization</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>scientific synthesis (multi-document synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Nine Criteria of Synthesis Quality</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Each generated synthesis is evaluated across nine targeted criteria addressing relevance, faithfulness to source abstracts, coverage, useful information content, source integration, sentence-level cohesion, idea-level coherence, academic readability, and conciseness (including target word-length). Each criterion is posed as a question and rated on a 1–5 Likert scale with tailored descriptions for each score level.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Nine per-criterion Likert ratings (1 Very Bad .. 5 Very Good); aggregated/averaged per-criterion scores reported</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>For each criterion, integer rating {1,2,3,4,5} where 1=Very Bad and 5=Very Good; conciseness additionally considers adherence to ~200-word target.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ORKG Scientific Synthesis Dataset (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used in both automatic (GPT-4-Turbo) and human Prolific evaluations. Human survey: 15 crowd participants (Prolific), 5 domains (Chemistry, CS, Earth Science, Linguistics, Sociology), 3 participants per domain; each participant rated 12 syntheses; same 1–5 scale and criterion prompts provided.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT-4 syntheses scored very highly on integration/cohesion/coherence/readability (~4.95+); relevancy and correctness also high (relevancy 4.97, correctness 4.93). Conciseness and completeness scored lower (conciseness ~3.89, completeness ~4.42 for GPT-4). Mistral-7B scored substantially lower across several criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Authors note that traditional automatic metrics (e.g., ROUGE) are inadequate for such semantic, multi-document synthesis; the nine-criteria scheme is qualitative and depends on consistent rating definitions; inter-rater agreement details for the human survey are not reported beyond aggregate averages.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7773.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7773.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4-Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-Turbo LLM-based Synthesis Evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated evaluator using GPT-4-Turbo prompted with the nine-criteria rubric to rate generated syntheses; run three times per sample and averaged to produce stable scores used as a reward signal and for offline evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (version used: GPT-4-Turbo / gpt-4-1106-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multidisciplinary scientific summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>automated qualitative evaluation of syntheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-as-evaluator using GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Zero-shot/controlled-prompt evaluation: GPT-4-Turbo receives the synthesis plus the nine evaluation questions and bespoke rating descriptions for 1–5, returns per-criterion scores. Each sample evaluated three times and average reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Per-criterion 1–5 ratings produced by GPT-4-Turbo, averaged across three runs</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Integer ratings 1–5 per criterion (1 Very Bad to 5 Very Good); averages across repeated runs reported as floats.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ORKG Scientific Synthesis Dataset (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Automated; run three times on same samples to check consistency; authors report little divergence across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT-4 evaluator rates GPT-4-generated syntheses substantially higher than Mistral on nearly all nine criteria; evaluator produced consistent clustered scores for the SFT+RLAIF optimized model vs high variance for vanilla.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>GPT-4 evaluator's patterns broadly aligned with human Prolific ratings (both found GPT-4 > Mistral, with similar per-criterion trends).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>LLM evaluator effectiveness depends on prompt engineering and internal model biases; no absolute human gold-standard adjudication or inter-rater reliability scores reported to calibrate the LLM scores precisely.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7773.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7773.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human-Prolific</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Crowdsourced Human Evaluation via Prolific</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human evaluation protocol using Prolific crowdworkers to rate a subsample of generated syntheses across the nine criteria with the same 1–5 scale, used to validate automated evaluator judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multidisciplinary (Chemistry, CS, Earth Science, Linguistics, Sociology)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>human rating of generated syntheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Crowdsourced human rating on nine criteria</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Participants read research problem and five paper titles/abstracts, then rated six LLM-generated syntheses (3 from each LLM and synthesis type) on the nine-criteria rubric using a 1–5 scale; optional free-text feedback collected.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Per-criterion 1–5 ratings from human annotators; summaries aggregated by averaging across annotators and domains</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Integer ratings 1–5 per criterion; participants: 15 total (3 per domain), each rated 12 syntheses; aggregated means reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Subsample of ORKG Scientific Synthesis Dataset (30 prompts for human eval)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>15 participants (aged 22–33), recruited on Prolific, screened for English fluency and at least undergraduate degree and domain relevance. Each participant evaluated 12 syntheses; payment £9/hr; survey estimated 2 hours.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Human ratings broadly mirrored GPT-4 automated scores: GPT-4 syntheses outperform Mistral; greatest model differences in completeness, informativeness, and integration; Mistral favored for simpler readability by some annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Small number of human participants (15) and limited domain coverage; Prolific screening relies on self-reported expertise; inter-rater agreement not reported beyond averaged scores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7773.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7773.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ORKG-Dataset</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ORKG Scientific Synthesis Dataset (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multidisciplinary corpus derived from the Open Research Knowledge Graph 'Comparisons' feature, standardized into samples each consisting of a research problem plus titles and abstracts of five papers, released with generated syntheses and prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multidisciplinary scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>dataset for multi-document scientific synthesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Dataset used as evaluation/generation input</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Constructed by extracting ORKG Comparisons with ≥5 unique papers, collecting titles/abstracts (via Semantic Scholar/Crossref/CORE), grouping all combinations of five contributions to form samples; used for training, SFT, RL fine-tuning and both automated and human evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Dataset statistics reported (348 final samples; 1044 standardized prompts corresponding to three synthesis types × models etc.) and used to compute evaluation results</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Counts: 348 unique comparison samples; 1044 standardized synthesis prompts; training/testing splits: 80/20 by domain; human evaluation subset: 10 comparisons (30 prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ORKG Comparisons-derived dataset (publicly released: https://github.com/jd-coderepos/scisynthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>A subset (30 prompts) used for human Prolific evaluation; full dataset used for SFT and RL training and for automated GPT-4 evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Dataset enabled generation of 1044 syntheses per model (total 2088). Reported experimental splits: Train-LLM 405 prompts, Train-RL 405 prompts, Test 234 prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Relies solely on titles and abstracts (no full texts); ORKG crowd-sourced labels vary in specificity and structure; sampling strategy of grouping all combinations of five papers may introduce duplicated semantics across samples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7773.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7773.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BasicReward</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Basic Features Reward Function & Paper-Structure Identifier</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rule-based reward function used in RLAIF that enforces structural preferences (single-paragraph format and word-count constraints) and penalizes paper-like output detected by a vocabulary/regex-based paper-structure identifier.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B (target of optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific synthesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>reward model for RL fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Basic structural reward R_basic(S) with paper-structure detector</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Rule-based checks identify 'paper structure' via 17 academic terms and 9 regex reference patterns; word-count measured by space-splitting. Reward values assigned: heavy penalties for too short/long outputs or paper-structure presence, positive reward for desired length and no paper-style structure.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Discrete reward values (numerical): e.g., -1.5 if WC<50; -1 if WC>200; -2 if paper structure detected; -0.5 if |WC−200| ≤ 20; +2 otherwise.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>WC = word count; PS(S) = binary paper-structure identifier; reward R_basic(S) is one of the specified scalar values applied per generated synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ORKG Scientific Synthesis Dataset outputs used to detect paper structure in generated syntheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Rule-based detector trained/validated against human annotations from 810 training samples (296 labeled as paper-structure).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>35.44% of Mistral outputs exhibited paper structure initially. Paper-structure detection: vocabulary identifier F1=67.80%, reference-identifier F1=90.87%; combined identifier F1=98.67% (10 misclassified cases). Applying R_basic in RL produced models with word counts closer to 200 (e.g., RL w/ Basic Features avg WC 189).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Rule-based detector depends on manually chosen vocabulary/regexes and may not generalize beyond observed training set; rewards are coarse-grained and may not capture subtle content quality issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7773.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7773.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4Reward</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Features Reward Function (R_GPT-4) / PVFscore</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reward model that uses GPT-4 evaluator scores over the nine qualitative criteria to compute a preference-alignment score (PVFscore) and maps it to a scalar reward used in RLAIF (positive reward when scores are near the preferred value 5).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SFT + RLAIF (trained using GPT-4 evaluator signals)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>SFT-adapted Mistral-7B; GPT-4 used as reward oracle</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific synthesis generation and quality alignment</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>reward modeling (AI-as-reward-oracle)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>R_GPT-4(C) using PVFscore</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute PVFscore = −(1/n) * sum_i |C_i − pv| where C_i are nine criterion scores in [1,5] and pv (preferred value)=5; if PVFscore ≥ −0.125 then reward = 4.0 else reward = PVFscore, i.e., encourage high per-criterion scores as per GPT-4 judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>PVFscore (negative mean absolute deviation from 5) and mapped scalar reward (4.0 for PVFscore ≥ −0.125, else PVFscore).</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>C_i ∈ [1,5]; PVFscore ∈ [−4,0]; threshold −0.125 corresponds to tight closeness to ideal; reward fed to PPO optimizer.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ORKG Scientific Synthesis Dataset (used to generate syntheses scored by GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>GPT-4 provides the nine criterion scores used to compute PVFscore; human ratings used for external validation but not directly in this reward.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Models trained with GPT-4-based reward (SFT+RLAIF w/ GPT-4 Features) achieved markedly better semantic criterion scores (e.g., relevancy ~4.88, correctness ~4.75, completeness ~4.19) and improved consistency compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Relies on GPT-4 as an imperfect proxy for human preference; threshold choice (−0.125) and binary reward mapping are heuristic and may overfit to GPT-4 evaluator biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7773.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7773.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RLAIF-PPO-KL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning from AI Feedback (RLAIF) with PPO and KL-penalty</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL fine-tuning pipeline that alternates generation and scoring by a reward model (basic or GPT-4-based), optimized with Proximal Policy Optimization (PPO) and a KL-divergence penalty to keep the fine-tuned policy close to a base SFT policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B (Vanilla → SFT → RLAIF variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>LLM fine-tuning for scientific synthesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>policy optimization for generation behavior alignment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>RLAIF using PPO with KL-penalty</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>The RL loop: generate syntheses given standardized prompts, obtain scalar reward r_theta(y|x) from reward model (R_basic or R_GPT-4), update policy via PPO; final reward includes KL penalty term: R = r_theta(y|x) − λ * KL(π_PPO || π_base) with λ = 0.2 to constrain drift.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Policy optimization objective combining reward and KL penalty; downstream evaluation uses nine-criteria scores and word-count/paper-structure measures.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>R scalar per trajectory; KL computed between token probability distributions (log-prob terms); λ (KL coefficient) set to 0.2; PPO optimizer configured with specific learning rates and epochs (Adam lr 2.94×10^−5, 10 epochs per batch).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ORKG Scientific Synthesis Dataset (train/test splits as described in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Final models evaluated by GPT-4 evaluator and human Prolific annotators on test subset; consistency measured across three model runs.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>SFT+RLAIF (w/ GPT-4 Features) produced more consistent outputs (clustered evaluator scores) and improved nine-criterion scores compared to Vanilla and SFT alone; basic-feature RL produced outputs closer to target word-count (avg WC ≈ 189).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>RL optimization depends heavily on reward model quality; KL penalty mitigates but does not eliminate risk of reward hacking; computational and sample efficiency constraints noted but not exhaustively quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7773.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7773.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROUGE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROUGE family (Recall-Oriented Understudy for Gisting Evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Traditional n-gram overlap-based automatic summarization metrics (ROUGE-N, ROUGE-L) commonly used in summarization research, mentioned here as inadequate for capturing semantic and integrative quality of scientific syntheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>text summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>automatic overlap-based evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ROUGE</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Computes n-gram overlap, longest common subsequence, and skip-bigram matches between candidate and reference texts; historically used for summarization but argued here to miss semantic fidelity and multi-document integration quality.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ROUGE-N (precision/recall/F1 of n-gram overlap), ROUGE-L (longest common subsequence F-measures)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Scores range usually 0–1 (or presented as percentages); higher indicates greater overlap with reference summary.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Cited as inadequate/incomplete for scientific synthesis evaluation; no experimental ROUGE scores reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Fails to capture semantic correctness, integration across multiple sources, and finer-grained linguistic quality important for scientific syntheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7773.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7773.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OtherAutoMetrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERTScore / BLEURT / GPTScore / LLM-Eval (automatic semantic evaluators mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of modern automatic evaluation metrics that use contextual embeddings (BERTScore), learned learned regressors with human labels (BLEURT), LLM-based scoring (GPTScore), or general LLM single-prompt evaluators (LLM-Eval); discussed as alternative or complementary to ROUGE and as background for using LLMs as evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>automatic text evaluation and summarization metrics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>automatic semantic/learned evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BERTScore / BLEURT / GPTScore / LLM-Eval</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>BERTScore measures similarity using contextual token embeddings; BLEURT is a learned metric trained on human ratings; GPTScore uses an LLM to score text per user-specified desiderata; LLM-Eval uses a single LLM prompt to evaluate conversational/text quality.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Metric-specific scores (BERTScore: 0–1 similarity; BLEURT: scalar scores often normalized; GPTScore/LLM-Eval: LLM-produced scalar or multi-dimensional judgments)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>BERTScore uses cosine similarities over contextual embeddings aggregated to produce precision/recall/F1; BLEURT outputs a scalar regression score typically aligned with human judgments; GPTScore/LLM-Eval formats vary with prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Discussed in related work as stronger semantic alternatives to ROUGE and as motivating the present use of GPT-4 as an evaluator; no direct scores reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Each metric has own calibration/data needs; LLM-based metrics may inherit LLM biases and require careful prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gptscore: Evaluate as you desire. <em>(Rating: 2)</em></li>
                <li>BERTScore: Evaluating Text Generation with BERT <em>(Rating: 2)</em></li>
                <li>BLEURT: Learning Robust Metrics for Text Generation <em>(Rating: 2)</em></li>
                <li>Proximal Policy Optimization Algorithms <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning from human preferences <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7773",
    "paper_id": "paper-272968899",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "NineCriteria",
            "name_full": "Nine Criteria of Synthesis Quality",
            "brief_description": "A fine-grained evaluation framework of nine linguistic and semantic criteria (relevancy, correctness, completeness, informativeness, integration, cohesion, coherence, readability, conciseness) used to rate LLM-generated scientific syntheses on a 1–5 scale per criterion.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B; GPT-4",
            "model_size": "7B; proprietary (GPT-4)",
            "scientific_domain": "multidisciplinary scientific literature summarization",
            "theory_type": "scientific synthesis (multi-document synthesis)",
            "evaluation_method_name": "Nine Criteria of Synthesis Quality",
            "evaluation_method_description": "Each generated synthesis is evaluated across nine targeted criteria addressing relevance, faithfulness to source abstracts, coverage, useful information content, source integration, sentence-level cohesion, idea-level coherence, academic readability, and conciseness (including target word-length). Each criterion is posed as a question and rated on a 1–5 Likert scale with tailored descriptions for each score level.",
            "evaluation_metric": "Nine per-criterion Likert ratings (1 Very Bad .. 5 Very Good); aggregated/averaged per-criterion scores reported",
            "metric_definition": "For each criterion, integer rating {1,2,3,4,5} where 1=Very Bad and 5=Very Good; conciseness additionally considers adherence to ~200-word target.",
            "dataset_or_benchmark": "ORKG Scientific Synthesis Dataset (this paper)",
            "human_evaluation_details": "Used in both automatic (GPT-4-Turbo) and human Prolific evaluations. Human survey: 15 crowd participants (Prolific), 5 domains (Chemistry, CS, Earth Science, Linguistics, Sociology), 3 participants per domain; each participant rated 12 syntheses; same 1–5 scale and criterion prompts provided.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "GPT-4 syntheses scored very highly on integration/cohesion/coherence/readability (~4.95+); relevancy and correctness also high (relevancy 4.97, correctness 4.93). Conciseness and completeness scored lower (conciseness ~3.89, completeness ~4.42 for GPT-4). Mistral-7B scored substantially lower across several criteria.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Authors note that traditional automatic metrics (e.g., ROUGE) are inadequate for such semantic, multi-document synthesis; the nine-criteria scheme is qualitative and depends on consistent rating definitions; inter-rater agreement details for the human survey are not reported beyond aggregate averages.",
            "uuid": "e7773.0",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT4-Eval",
            "name_full": "GPT-4-Turbo LLM-based Synthesis Evaluator",
            "brief_description": "An automated evaluator using GPT-4-Turbo prompted with the nine-criteria rubric to rate generated syntheses; run three times per sample and averaged to produce stable scores used as a reward signal and for offline evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4-Turbo",
            "model_size": "proprietary (version used: GPT-4-Turbo / gpt-4-1106-preview)",
            "scientific_domain": "multidisciplinary scientific summarization evaluation",
            "theory_type": "automated qualitative evaluation of syntheses",
            "evaluation_method_name": "LLM-as-evaluator using GPT-4-Turbo",
            "evaluation_method_description": "Zero-shot/controlled-prompt evaluation: GPT-4-Turbo receives the synthesis plus the nine evaluation questions and bespoke rating descriptions for 1–5, returns per-criterion scores. Each sample evaluated three times and average reported.",
            "evaluation_metric": "Per-criterion 1–5 ratings produced by GPT-4-Turbo, averaged across three runs",
            "metric_definition": "Integer ratings 1–5 per criterion (1 Very Bad to 5 Very Good); averages across repeated runs reported as floats.",
            "dataset_or_benchmark": "ORKG Scientific Synthesis Dataset (this paper)",
            "human_evaluation_details": "Automated; run three times on same samples to check consistency; authors report little divergence across runs.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "GPT-4 evaluator rates GPT-4-generated syntheses substantially higher than Mistral on nearly all nine criteria; evaluator produced consistent clustered scores for the SFT+RLAIF optimized model vs high variance for vanilla.",
            "comparison_to_human_generated": true,
            "comparison_results": "GPT-4 evaluator's patterns broadly aligned with human Prolific ratings (both found GPT-4 &gt; Mistral, with similar per-criterion trends).",
            "limitations_noted": "LLM evaluator effectiveness depends on prompt engineering and internal model biases; no absolute human gold-standard adjudication or inter-rater reliability scores reported to calibrate the LLM scores precisely.",
            "uuid": "e7773.1",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Human-Prolific",
            "name_full": "Crowdsourced Human Evaluation via Prolific",
            "brief_description": "A human evaluation protocol using Prolific crowdworkers to rate a subsample of generated syntheses across the nine criteria with the same 1–5 scale, used to validate automated evaluator judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "multidisciplinary (Chemistry, CS, Earth Science, Linguistics, Sociology)",
            "theory_type": "human rating of generated syntheses",
            "evaluation_method_name": "Crowdsourced human rating on nine criteria",
            "evaluation_method_description": "Participants read research problem and five paper titles/abstracts, then rated six LLM-generated syntheses (3 from each LLM and synthesis type) on the nine-criteria rubric using a 1–5 scale; optional free-text feedback collected.",
            "evaluation_metric": "Per-criterion 1–5 ratings from human annotators; summaries aggregated by averaging across annotators and domains",
            "metric_definition": "Integer ratings 1–5 per criterion; participants: 15 total (3 per domain), each rated 12 syntheses; aggregated means reported.",
            "dataset_or_benchmark": "Subsample of ORKG Scientific Synthesis Dataset (30 prompts for human eval)",
            "human_evaluation_details": "15 participants (aged 22–33), recruited on Prolific, screened for English fluency and at least undergraduate degree and domain relevance. Each participant evaluated 12 syntheses; payment £9/hr; survey estimated 2 hours.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Human ratings broadly mirrored GPT-4 automated scores: GPT-4 syntheses outperform Mistral; greatest model differences in completeness, informativeness, and integration; Mistral favored for simpler readability by some annotators.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Small number of human participants (15) and limited domain coverage; Prolific screening relies on self-reported expertise; inter-rater agreement not reported beyond averaged scores.",
            "uuid": "e7773.2",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ORKG-Dataset",
            "name_full": "ORKG Scientific Synthesis Dataset (this work)",
            "brief_description": "A multidisciplinary corpus derived from the Open Research Knowledge Graph 'Comparisons' feature, standardized into samples each consisting of a research problem plus titles and abstracts of five papers, released with generated syntheses and prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "multidisciplinary scientific literature",
            "theory_type": "dataset for multi-document scientific synthesis generation",
            "evaluation_method_name": "Dataset used as evaluation/generation input",
            "evaluation_method_description": "Constructed by extracting ORKG Comparisons with ≥5 unique papers, collecting titles/abstracts (via Semantic Scholar/Crossref/CORE), grouping all combinations of five contributions to form samples; used for training, SFT, RL fine-tuning and both automated and human evaluations.",
            "evaluation_metric": "Dataset statistics reported (348 final samples; 1044 standardized prompts corresponding to three synthesis types × models etc.) and used to compute evaluation results",
            "metric_definition": "Counts: 348 unique comparison samples; 1044 standardized synthesis prompts; training/testing splits: 80/20 by domain; human evaluation subset: 10 comparisons (30 prompts).",
            "dataset_or_benchmark": "ORKG Comparisons-derived dataset (publicly released: https://github.com/jd-coderepos/scisynthesis)",
            "human_evaluation_details": "A subset (30 prompts) used for human Prolific evaluation; full dataset used for SFT and RL training and for automated GPT-4 evaluation.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Dataset enabled generation of 1044 syntheses per model (total 2088). Reported experimental splits: Train-LLM 405 prompts, Train-RL 405 prompts, Test 234 prompts.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Relies solely on titles and abstracts (no full texts); ORKG crowd-sourced labels vary in specificity and structure; sampling strategy of grouping all combinations of five papers may introduce duplicated semantics across samples.",
            "uuid": "e7773.3",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "BasicReward",
            "name_full": "Basic Features Reward Function & Paper-Structure Identifier",
            "brief_description": "A rule-based reward function used in RLAIF that enforces structural preferences (single-paragraph format and word-count constraints) and penalizes paper-like output detected by a vocabulary/regex-based paper-structure identifier.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B (target of optimization)",
            "model_size": "7B",
            "scientific_domain": "scientific synthesis generation",
            "theory_type": "reward model for RL fine-tuning",
            "evaluation_method_name": "Basic structural reward R_basic(S) with paper-structure detector",
            "evaluation_method_description": "Rule-based checks identify 'paper structure' via 17 academic terms and 9 regex reference patterns; word-count measured by space-splitting. Reward values assigned: heavy penalties for too short/long outputs or paper-structure presence, positive reward for desired length and no paper-style structure.",
            "evaluation_metric": "Discrete reward values (numerical): e.g., -1.5 if WC&lt;50; -1 if WC&gt;200; -2 if paper structure detected; -0.5 if |WC−200| ≤ 20; +2 otherwise.",
            "metric_definition": "WC = word count; PS(S) = binary paper-structure identifier; reward R_basic(S) is one of the specified scalar values applied per generated synthesis.",
            "dataset_or_benchmark": "ORKG Scientific Synthesis Dataset outputs used to detect paper structure in generated syntheses",
            "human_evaluation_details": "Rule-based detector trained/validated against human annotations from 810 training samples (296 labeled as paper-structure).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "35.44% of Mistral outputs exhibited paper structure initially. Paper-structure detection: vocabulary identifier F1=67.80%, reference-identifier F1=90.87%; combined identifier F1=98.67% (10 misclassified cases). Applying R_basic in RL produced models with word counts closer to 200 (e.g., RL w/ Basic Features avg WC 189).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Rule-based detector depends on manually chosen vocabulary/regexes and may not generalize beyond observed training set; rewards are coarse-grained and may not capture subtle content quality issues.",
            "uuid": "e7773.4",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT4Reward",
            "name_full": "GPT-4 Features Reward Function (R_GPT-4) / PVFscore",
            "brief_description": "A reward model that uses GPT-4 evaluator scores over the nine qualitative criteria to compute a preference-alignment score (PVFscore) and maps it to a scalar reward used in RLAIF (positive reward when scores are near the preferred value 5).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SFT + RLAIF (trained using GPT-4 evaluator signals)",
            "model_size": "SFT-adapted Mistral-7B; GPT-4 used as reward oracle",
            "scientific_domain": "scientific synthesis generation and quality alignment",
            "theory_type": "reward modeling (AI-as-reward-oracle)",
            "evaluation_method_name": "R_GPT-4(C) using PVFscore",
            "evaluation_method_description": "Compute PVFscore = −(1/n) * sum_i |C_i − pv| where C_i are nine criterion scores in [1,5] and pv (preferred value)=5; if PVFscore ≥ −0.125 then reward = 4.0 else reward = PVFscore, i.e., encourage high per-criterion scores as per GPT-4 judgments.",
            "evaluation_metric": "PVFscore (negative mean absolute deviation from 5) and mapped scalar reward (4.0 for PVFscore ≥ −0.125, else PVFscore).",
            "metric_definition": "C_i ∈ [1,5]; PVFscore ∈ [−4,0]; threshold −0.125 corresponds to tight closeness to ideal; reward fed to PPO optimizer.",
            "dataset_or_benchmark": "ORKG Scientific Synthesis Dataset (used to generate syntheses scored by GPT-4)",
            "human_evaluation_details": "GPT-4 provides the nine criterion scores used to compute PVFscore; human ratings used for external validation but not directly in this reward.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Models trained with GPT-4-based reward (SFT+RLAIF w/ GPT-4 Features) achieved markedly better semantic criterion scores (e.g., relevancy ~4.88, correctness ~4.75, completeness ~4.19) and improved consistency compared to baselines.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Relies on GPT-4 as an imperfect proxy for human preference; threshold choice (−0.125) and binary reward mapping are heuristic and may overfit to GPT-4 evaluator biases.",
            "uuid": "e7773.5",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "RLAIF-PPO-KL",
            "name_full": "Reinforcement Learning from AI Feedback (RLAIF) with PPO and KL-penalty",
            "brief_description": "An RL fine-tuning pipeline that alternates generation and scoring by a reward model (basic or GPT-4-based), optimized with Proximal Policy Optimization (PPO) and a KL-divergence penalty to keep the fine-tuned policy close to a base SFT policy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B (Vanilla → SFT → RLAIF variants)",
            "model_size": "7B",
            "scientific_domain": "LLM fine-tuning for scientific synthesis generation",
            "theory_type": "policy optimization for generation behavior alignment",
            "evaluation_method_name": "RLAIF using PPO with KL-penalty",
            "evaluation_method_description": "The RL loop: generate syntheses given standardized prompts, obtain scalar reward r_theta(y|x) from reward model (R_basic or R_GPT-4), update policy via PPO; final reward includes KL penalty term: R = r_theta(y|x) − λ * KL(π_PPO || π_base) with λ = 0.2 to constrain drift.",
            "evaluation_metric": "Policy optimization objective combining reward and KL penalty; downstream evaluation uses nine-criteria scores and word-count/paper-structure measures.",
            "metric_definition": "R scalar per trajectory; KL computed between token probability distributions (log-prob terms); λ (KL coefficient) set to 0.2; PPO optimizer configured with specific learning rates and epochs (Adam lr 2.94×10^−5, 10 epochs per batch).",
            "dataset_or_benchmark": "ORKG Scientific Synthesis Dataset (train/test splits as described in paper)",
            "human_evaluation_details": "Final models evaluated by GPT-4 evaluator and human Prolific annotators on test subset; consistency measured across three model runs.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "SFT+RLAIF (w/ GPT-4 Features) produced more consistent outputs (clustered evaluator scores) and improved nine-criterion scores compared to Vanilla and SFT alone; basic-feature RL produced outputs closer to target word-count (avg WC ≈ 189).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "RL optimization depends heavily on reward model quality; KL penalty mitigates but does not eliminate risk of reward hacking; computational and sample efficiency constraints noted but not exhaustively quantified.",
            "uuid": "e7773.6",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ROUGE",
            "name_full": "ROUGE family (Recall-Oriented Understudy for Gisting Evaluation)",
            "brief_description": "Traditional n-gram overlap-based automatic summarization metrics (ROUGE-N, ROUGE-L) commonly used in summarization research, mentioned here as inadequate for capturing semantic and integrative quality of scientific syntheses.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "text summarization evaluation",
            "theory_type": "automatic overlap-based evaluation metric",
            "evaluation_method_name": "ROUGE",
            "evaluation_method_description": "Computes n-gram overlap, longest common subsequence, and skip-bigram matches between candidate and reference texts; historically used for summarization but argued here to miss semantic fidelity and multi-document integration quality.",
            "evaluation_metric": "ROUGE-N (precision/recall/F1 of n-gram overlap), ROUGE-L (longest common subsequence F-measures)",
            "metric_definition": "Scores range usually 0–1 (or presented as percentages); higher indicates greater overlap with reference summary.",
            "dataset_or_benchmark": null,
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Cited as inadequate/incomplete for scientific synthesis evaluation; no experimental ROUGE scores reported in this paper.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Fails to capture semantic correctness, integration across multiple sources, and finer-grained linguistic quality important for scientific syntheses.",
            "uuid": "e7773.7",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "OtherAutoMetrics",
            "name_full": "BERTScore / BLEURT / GPTScore / LLM-Eval (automatic semantic evaluators mentioned)",
            "brief_description": "A set of modern automatic evaluation metrics that use contextual embeddings (BERTScore), learned learned regressors with human labels (BLEURT), LLM-based scoring (GPTScore), or general LLM single-prompt evaluators (LLM-Eval); discussed as alternative or complementary to ROUGE and as background for using LLMs as evaluators.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "automatic text evaluation and summarization metrics",
            "theory_type": "automatic semantic/learned evaluation metrics",
            "evaluation_method_name": "BERTScore / BLEURT / GPTScore / LLM-Eval",
            "evaluation_method_description": "BERTScore measures similarity using contextual token embeddings; BLEURT is a learned metric trained on human ratings; GPTScore uses an LLM to score text per user-specified desiderata; LLM-Eval uses a single LLM prompt to evaluate conversational/text quality.",
            "evaluation_metric": "Metric-specific scores (BERTScore: 0–1 similarity; BLEURT: scalar scores often normalized; GPTScore/LLM-Eval: LLM-produced scalar or multi-dimensional judgments)",
            "metric_definition": "BERTScore uses cosine similarities over contextual embeddings aggregated to produce precision/recall/F1; BLEURT outputs a scalar regression score typically aligned with human judgments; GPTScore/LLM-Eval formats vary with prompt.",
            "dataset_or_benchmark": null,
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Discussed in related work as stronger semantic alternatives to ROUGE and as motivating the present use of GPT-4 as an evaluator; no direct scores reported in this paper.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Each metric has own calibration/data needs; LLM-based metrics may inherit LLM biases and require careful prompt design.",
            "uuid": "e7773.8",
            "source_info": {
                "paper_title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gptscore: Evaluate as you desire.",
            "rating": 2,
            "sanitized_title": "gptscore_evaluate_as_you_desire"
        },
        {
            "paper_title": "BERTScore: Evaluating Text Generation with BERT",
            "rating": 2,
            "sanitized_title": "bertscore_evaluating_text_generation_with_bert"
        },
        {
            "paper_title": "BLEURT: Learning Robust Metrics for Text Generation",
            "rating": 2,
            "sanitized_title": "bleurt_learning_robust_metrics_for_text_generation"
        },
        {
            "paper_title": "Proximal Policy Optimization Algorithms",
            "rating": 2,
            "sanitized_title": "proximal_policy_optimization_algorithms"
        },
        {
            "paper_title": "Deep reinforcement learning from human preferences",
            "rating": 2,
            "sanitized_title": "deep_reinforcement_learning_from_human_preferences"
        }
    ],
    "cost": 0.019126249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis
27 Sep 2024</p>
<p>Hamed Babaei Giglou 
TIB Leibniz Information Centre for Science and Technology Hannover
Germany</p>
<p>Jennifer D ' Souza jennifer.dsouza@tib.eu 
TIB Leibniz Information Centre for Science and Technology Hannover
Germany</p>
<p>Sören Auer auer@tib.eu 
TIB Leibniz Information Centre for Science and Technology Hannover
Germany</p>
<p>LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis
27 Sep 2024C20ACA2332C25EF56FFEEA85AB0CE257arXiv:2409.18812v1[cs.CL]CCS ConceptsInformation systems → Information systems applications• Computing methodologies → Natural language generationLanguage resourcesNatural language processingSupervised learningReinforcement learning Scientific Synthesis Generation, Large Language Models, Language Model-based Evaluation Framework, Reinforcement Learning
In response to the growing complexity and volume of scientific literature, this paper introduces the LLMs4Synthesis framework, designed to enhance the capabilities of Large Language Models (LLMs) in generating high-quality scientific syntheses.This framework addresses the need for rapid, coherent, and contextually rich integration of scientific insights, leveraging both open-source and proprietary LLMs.It also examines the effectiveness of LLMs in evaluating the integrity and reliability of these syntheses, alleviating inadequacies in current quantitative metrics.Our study contributes to this field by developing a novel methodology for processing scientific papers, defining new synthesis types, and establishing nine detailed quality criteria for evaluating syntheses.The integration of LLMs with reinforcement learning and AI feedback is proposed to optimize synthesis quality, ensuring alignment with established criteria.The LLMs4Synthesis framework and its components are made available, promising to enhance both the generation and evaluation processes in scientific research synthesis.</p>
<p>Introduction</p>
<p>In recent years, the intersection of language processing and scientific research has witnessed major advancements, largely fueled by the capabilities of Large Language Models (LLMs) [9,15].These models, including open-source (e.g.BERT [14] or Mistral [22]), or proprietary technologies (e.g.GPT-4 [1]), and their derivatives, have not only redefined the boundaries of text generation and comprehension [40] but have also paved the way for innovative applications in the synthesis of scientific content [17,34].To enable the automated and accurate generation of scientific syntheses, which are concise integrations of key insights from multiple scientific articles, this paper introduces the comprehensive LLMs4Synthesis framework.It is designed to enhance open-source LLMs so that they can generate scientific syntheses of comparable quality to those produced by significantly larger proprietary models.</p>
<p>The motivation behind this endeavor is driven by the increasing complexity and volume of scientific literature [8,18,44], which poses challenges for researchers seeking timely and comprehensive knowledge syntheses.Traditional manual synthesis methods are time-consuming and struggle to keep up with the rapid dissemination of new research.The LLMs4Synthesis framework addresses these challenges by leveraging LLMs to produce concise, coherent, and contextually rich scientific summaries, helping the scientific community stay abreast of expanding knowledge frontiers.Furthermore, next-generation search engines like ORKG Ask [33], Elicit [16], and SciSpace [38] demonstrate how LLMs enhance search capabilities and user interactions.These platforms utilize advanced features such as natural language queries, semantic search, and AI-driven extractions to transform scientific literature access.ORKG Ask is an open platform, allowing for seamless integration of LLMs4Synthesis, enhancing synthesis abilities, and promoting open access and collaboration in the scientific community.</p>
<p>Accurate evaluation of scientific syntheses is essential to maintain their integrity and reliability.Recent advancements suggest that LLMs can generate these syntheses [16,33,34,38], yet their effectiveness across domains and capability in evaluating them is not well understood.Merely employing existing quantitative metrics, like the rouge family [27], commonly applied in similar text generation scenarios, raise concerns about their adequacy [3,12,17,24] in capturing the full extent of the semantic intricacies associated with the quality evaluation of syntheses.Instead, our research examines the effectiveness of LLMs in assessing scientific syntheses, contributing to a significant methodological shift where language understanding tools are increasingly observed as effective evaluators [2,7].Moreover, related work [19,28,47] shows that using LLMs as evaluators facilitates intricate versatility and detail in assessments across multiple dimensions and scales.Consequently, our study aims to define a comprehensive set of fine-grained evaluation attributes to address critical synthesis issues like irrelevancy, inaccuracy, incompleteness, redundancy, disorganization, incoherence, poor readability, and verbosity, which significantly affect the quality and clarity of syntheses.Additionally, this study explores using LLM evaluator responses as AI feedback in reinforcement learning settings (RLAIF) [5,11] to enhance the alignment of a model's unsupervised learned parameters [35] with the quality of scientific synthesis generation.This approach moves beyond traditional reliance on human evaluators and ground truth data.</p>
<p>In essence, this research aims to equip LLMs with the ability to identify and improve upon their synthesis generation quality, thereby enhancing their learning and output reliability.The study focuses on the following research questions (RQs).RQ1: What features are essential for qualitatively evaluating the informativeness and meaningfulness of a scientific synthesis?RQ2: How can LLMs be fine-tuned to adhere to specific formatting standards like word limits and paragraph structure in scientific syntheses?RQ3: How can the balance be achieved in LLMs between maintaining formatting standards and ensuring semantically informative and meaningful scientific syntheses?</p>
<p>This paper makes several contributions.First, we develop a methodology to collect and process scientific papers into a format ready for synthesis using the Open Research Knowledge Graph [4], a multidisciplinary platform that facilitates the comparison of scientific contributions [32].Second, we introduce new synthesis types -paper-wise, methodological, and thematic --that focus on different aspects of the extracted insights.Utilizing Mistral-7B [22] and GPT-4 [1], we generate a large-scale dataset of these syntheses, which is publicly available https://github.com/jd-coderepos/scisynthesis.Third, we establish nine quality criteria for evaluating these syntheses, assessed by both an automated LLM evaluator (GPT-4) and a human-crowdsourced survey.These evaluations provide insights, i.e. at type, domain, LLM levels, in the research community at large, and also help inform the LLMs4Synthesis framework, to enhance LLM scientific synthesis generation task performance.Finally, we propose the LLMs4Synthesis framework, which incorporates RLAIF [5,11] to optimize LLMs for synthesis generation, ensuring alignment with established quality standards.The framework and its source code are publicly accessible https://github.com/HamedBabaei/LLMs4Synthesis.</p>
<p>The rest of the paper is organized as follows.Section 2 details the multidisciplinary ORKG synthesis dataset.Section 3 introduces comprehensive evaluation attributes and assesses synthesis quality through both automatic and human evaluations, providing performance comparisons between proprietary and open-source LLMs.Section 4 elaborates on the LLMs4Synthesis framework, with its evaluations in section 5. Section 6 explores the broader implications of our findings, and section 7 concludes the paper, summarizing our contributions and suggesting future work.</p>
<p>The ORKG Scientific Synthesis Dataset</p>
<p>In this work, we broaden the scope of the dataset collection for scientific synthesis.The primary requirement for building a corpus of scientific syntheses is access to research problems or questions along with their associated papers.Previous efforts involved manually curating this information through a team of human annotators [34].Here, we systematize this approach by utilizing the Open Research Knowledge Graph (ORKG), a crowdsourcing platform that provides structured research contributions and comparisons.The subsequent paragraphs detail our method for compiling a reliable collection of research problems and corresponding papers to generate a multidisciplinary scientific syntheses corpus.Our corpus surpasses previous work in both multidisciplinarity and size, containing three times as many data samples, as a direct consequence of relying on the ORKG crowdsourced data.</p>
<p>Synthesis Generation Data Preparation</p>
<p>The ORKG data source.The ORKG is a web-based service that structures scholarly research contributions into a knowledge graph, using a crowdsourcing approach where users add and semantically describe paper contributions (of which each paper may have one or more).A key feature, Comparisons, allows users to select and compare multiple research contributions in a tabular format [32].These Comparisons, curated by users, include papers addressing specific research themes.For defining a corpus for scientific synthesis generation, we utilized ORKG Comparisons to extract human-annotated research problems and their associated papers, providing an ideal data source.</p>
<p>Data processing.The ORKG Python package (https://pypi.org/project/orkg/) was used to collect the data.First, we found all research problems with Comparisons, which produced 1,300 Comparisons for 708 research problems.From these Comparisons, we eliminated those with fewer than five unique papers, which left 495 Comparisons.The minimum threshold of five papers per comparison is a fixed criterion in this study for generating scientific syntheses.This criterion is based on popular search systems like Elicit or ORKG Ask, which typically generate syntheses from the top five results of their respective search engines.Next, we sourced abstracts for each paper in the Comparisons using the open-access platforms Semantic Scholar, Crossref, and CORE.This resulted in 329 Comparisons with five or more papers with abstracts.This step was essential since in this work, paper titles and abstracts are the context from which the syntheses are generated.Finally, for each Comparison, all papers were grouped as all possible collections of five contributions, with each collection representing one sample in the data, for a total of 541 samples.However, it is not uncommon for a Comparison to be tagged in multiple research fields, resulting in duplicated samples that differ only in research field.In such cases, only one Comparison was randomly selected to be kept, yielding a final dataset of 348 samples with each sample as a candidate for scientific synthesis generation.</p>
<p>Intermediate ORKG scientific synthesis generation dataset.The ORKG scientific synthesis generation dataset including the respective accompanying generated syntheses (details in subsection 2.2) is publicly released at https://github.com/jd-coderepos/scisynthesis.Each sample in the dataset consists of the following: sample ID, research field, research problem, and the title, abstract, and DOI (if available) of five papers.The research field is selected from the ORKG's own taxonomic schema (https://orkg.org/fields), with users permitted to choose any level within the hierarchy, whereas research problems are entered as free-form text.Due to the flexibility allowed in user inputs, the research problems in the ORKG vary widely in structure and specificity.For example, both "text</p>
<p>Scientific Synthesis Generation</p>
<p>Task description.Inspired by prior work [17], the task of scientific synthesis is defined as a specialized form of multi-document summarization.It involves combining the main insights from multiple research papers-five in this work-into a coherent paragraph that addresses a specific research problem or question.The scientific synthesis generation task encompasses the following five key characteristics.1) Use of scientific literature: This process involves synthesizing information from the scientific literature, primarily from titles and abstracts of research papers.The task requires summarizing these texts and evaluating their relevance, correctness, and completeness concerning the research problem.2) Synthesis format: The synthesis should be concisely presented in a single paragraph, limited to 200 words.This limit aligns with the standard guidelines for scientific abstracts as recommended by APA [10], MLA [45], Harvard [43], and Chicago [21] style guides, aiming for brevity and precision in summarizing key information [31,41].The format requires distilling and integrating diverse scientific insights into a coherent, comprehensive summary that addresses the research problem directly.This single-paragraph approach emphasizes the need for concise and cohesive communication of complex information.3) Synthesize vs. summarize: The objective is to synthesize-meaning to combine elements into a coherent whole-rather than merely summarizing each source individually.This involves the integration, cohesion, and coherence of information from multiple sources to produce new insights or understanding in response to the research problem.4) Referencing source material: Each claim or piece of information in the synthesis must be traceable back to the source material (the abstracts) to ensure the synthesis's accuracy and reliability.5) Adherence to quality characteristics: Building on approaches from linguistic and semantic quality evaluation studies of generated texts [19], the quality of the synthesis should satisfy the following nine key criteria: relevancy, correctness, completeness, informativeness, integration, cohesion, coherence, readability, and conciseness.These criteria, identified as apt for synthesis quality evaluation, collectively ensure effective communication of the synthesized information.The role and the application of these criteria in practice in this work are introduced in subsection 3.1.</p>
<p>To accommodate the diverse information scope typically found in research papers, and to tailor the synthesis to the specific interests of the user, we developed three distinct types of synthesis: 1) Paperwise synthesis, which provides a general overview; 2) Methodological synthesis, which concentrates on the methods and their pertinent details; and 3) Thematic synthesis, which focuses on identifying and summarizing recurring themes or patterns.Each synthesis type is designed to address a specific research problem while aligning with the intended focus of information.For example, a methodological synthesis will include only details about the methods discussed in the paper abstracts.In contrast, a thematic synthesis will specifically target the repetitive overarching themes present in the research.</p>
<p>As alluded to in the Introduction, the goal of this work is to develop a systematic framework for generating scientific syntheses using Large Language Models (LLMs4Synthesis).To our knowledge, this is the first framework to showcase the integration of both generation and evaluation components, aimed at optimizing the downstream synthesis model.While implemented in the context of ORKG Ask, LLMs4Synthesis is designed to be easily adaptable to other similar platforms like Elicit.The remainder of this paper discusses our use of generative AI technology, specifically LLMs, to achieve this goal.We start by describing the LLMs used for scientific synthesis generation and their application in the next paragraph.</p>
<p>Synthesis generation task instruction, models, and the final ORKG syntheses dataset.This task involved using an LLM to generate a synthesis from the titles and abstracts of five scientific papers, tailored to the synthesis type and research problem.</p>
<p>Previous studies on scientific synthesis generation [17,34], have not extensively discussed how LLMs are prompted for this task.We designed the scientific synthesis generation prompt based on prompt engineering best practices [36] and various tested prompts.Our prompt for synthesis generation is detailed in Table 2.The prompt includes two main parts: a detailed task specification and placeholders for input papers.The first part specifies the task to the LLM, filling in the "[research-problem]" placeholder with the research problem from the ORKG synthesis dataset and "[prompttype-input-instruction]" with instructions corresponding to the synthesis type.For example, the methodological synthesis type instruction is: "The objective of this synthesis is to focus on the methodology.Therefore, compare and integrate the methodologies used in each paper content, emphasizing how they contribute to Part I -Task Instruction: Synthesis Task Specification Generate a synthesis from the provided papers as content on the research problem "[research-problem]" into a concise single paragraph of no more than 200 words.Follow these instructions: -Only the titles and abstracts from exactly five scientific papers will be provided, to be used as content for the synthesis.</p>
<p>-"[prompt-type-input-instruction]".</p>
<p>-Support each claim with citations, formatted as (1) or (3,5) to refer to the respective papers' content.</p>
<p>-Ensure the output is a single cohesive paragraph without section headings, titles, abstracts, or any paper-like structure.</p>
<p>-Focus on essential information, maintaining clarity and precision.</p>
<p>-Do not include additional information or exceed the specified word count of 200 words.</p>
<p>Part II -Task Input: Five Papers to Synthesize \n\n Papers \n 1. "
[content-1]" \n 2. "[content-2]" \n 3. "[content-3]" \n 4. "[content- 4]" \n 5. "[content-5]"
the research problem."The prompts for all three synthesis types are available at https://github.com/jd-coderepos/scisynthesis/tree/main/synthesis-generation-prompts.Based on initial feedback from the LLM's responses on a subset of the dataset, the first part of the prompt was iteratively refined to enhance specificity and instructional emphasis (notably, the 200-word limit is reiterated for clarity).The second part of the prompt includes placeholders for the titles and abstracts of five papers which are to be given as input to the LLM to perform the synthesis generation task.</p>
<p>We applied two prominent state-of-the-art LLMs for the scientific synthesis generation task: the open-source Mistral-7B [22], and the proprietary GPT-4 [1].Thus the resulting ORKG syntheses dataset comprises three syntheses data files, per synthesis type, per LLM (six files overall).Each file contains the raw synthesis response and the text of the synthesis.So total of 1044 syntheses for each model and an overall total of 2088 syntheses.</p>
<p>Synthesis Quality Evaluation</p>
<p>To enhance our understanding of the weaknesses of LLMs concerning the synthesis objective and toward realizing potential improvements within the LLMs4Synthesis framework, we carried out a two-part qualitative evaluation of the generated syntheses.This evaluation included: 1) an LLM-based assessment using the most powerful variant of the GPT models, specifically GPT-4-Turbo, and 2) a human survey evaluation of a subsample from the synthesis dataset.Training moderate-sized open-source models effectively we demonstrate with Mistral 7B.We aim to make synthesis generation tasks more accessible and decrease dependence on proprietary models, which, while highly effective as indicated by popular LLM leaderboards, often lack transparency in their pre-training methods, datasets, and parameters.This is a barrier to deeper research insights and wider community participation.Consequently, our work is dedicated to developing methods that equip open-source LLMs to handle essential tasks in scientific language processing.</p>
<p>As a first step, we established a comprehensive set of evaluation criteria for the generated syntheses, detailed in the next section.</p>
<p>Nine Criteria of Synthesis Quality</p>
<p>Prior work [17] has provided a thorough analysis of the limitations associated with existing quantitative evaluation metrics, such as the rouge family [27], traditionally used for text summarization tasks--a broader context for the synthesis objective.The limitations highlighted the necessity to establish evaluation criteria that focus on linguistic and semantic qualitative aspects.In this vein, inspired by a prior study that assessed synthesis quality using just three criteria-comprehensive, trust, and utility--considered crucial for high-quality synthesis [34], this work aims to expand on this foundation.We seek to develop a broader set of criteria for synthesis quality evaluation, incorporating thorough linguistic and semantic assessments, informed by insights from related research on text quality evaluation in summarization [19].After filtering out unrelated criteria from existing work, we identified nine criteria to evaluate the quality and effectiveness of the synthesized information.Each criterion is presented to the evaluator in the form of a question, as outlined below.</p>
<ol>
<li>Relevancy: Is the information in the answer relevant to the problem? 2. Correctness: Is the information in the answer a correct representation of the content of the provided abstracts?3. Completeness: Is the answer a comprehensive encapsulation of the relevant information in the provided abstracts?4. Informativeness: Is the answer a useful and informative reply to the problem? 5. Integration: Are the sources structurally and linguistically wellintegrated, using appropriate markers of provenance/quotation and logical connectors for each reference?In addition, are the sources integrated within a single paragraph?6. Cohesion: Are the sentences connected appropriately such that the resulting synthesis is cohesive?7. Coherence: Are the ideas connected in a sound and logical manner?8. Readability: Does the answer follow appropriate style and structure conventions for academic writing and use language correctly?9. Conciseness: Is the answer short and clear, without redundant statements?Furthermore, is the synthesis approximately 200 words long?</li>
</ol>
<p>Our initial five criteria were crafted to enhance objectivity and precision in evaluating syntheses, expanding on the three criteria used earlier in synthesis evaluation [34].Specifically, our '3.completeness' corresponds to their 'comprehensive, ' our '2.correctness' and '5.integration' relate to their 'trust, ' and our '1.relevancy' and '4.informativeness' align with their 'utility.' Thus, we expanded the three foundational criteria into five.Our remaining four criteria, three-'6.cohesion, ' '7.coherence, ' and '8.readability'-are rooted in established text evaluation practices for linguistic quality, which are, for instance, commonly applied in summarization scenarios.The final criterion, '9.conciseness, ' is unique to this work.It emphasizes both the quality of the synthesized information and adherence to the specified word limit, essential for presenting a succinct representation of synthesized research insights to the reader.</p>
<p>LLM Evaluation of Synthesis Quality</p>
<p>Prior work has demonstrated the effectiveness of LLMs as automatic evaluators for scientific synthesis [6,17,19], arguing that they are becoming both the tools and the standards for language assessment.Thus, for the evaluation task, we utilized GPT-4-Turbo, the most advanced LLM available at the time of writing.The evaluation prompt and script are available athttps://github.com/jd-coderepos/ scisynthesis/tree/main/gpt-4%20synthesis-evaluator.The prompt includes all nine evaluation criteria along with their corresponding questions.Each evaluation criteria was assessed based on a rating scale from 1 to 5 as follows: 1. Very Bad, 2. Bad, 3. Moderate, 4. Good, and 5. Very Good.Each rating level was specifically tailored with a description to aid in the assessment of each criterion.For instance, consider the rating scale description for the first criteria i.e. "1.Relevancy: is the information in the answer relevant to the problem?" as follows: Rating 1. Very bad: The information provided does not relate to the research problem, showing a lack of understanding or connection to the topic.Rating 2. Bad: The information occasionally relates to the research problem but lacks direct and consistent relevance.Rating 3. Moderate: The information is generally related to the research problem, with occasional lapses in direct relevance.Rating 4. Good: The information is consistently relevant to the research problem, with only minor exceptions.Rating 5. Very good: The synthesis is directly and consistently relevant to the research problem, demonstrating a deep understanding of the topic and its nuances.For the full prompts, including detailed descriptions of the rating scale for all criteria, please refer to the system prompt.Finally, similar to 3-fold cross-validation, the GPT-4 synthesis evaluator was run thrice on the same data samples to ensure consistent assessment.The resulting scores, which did not show significant divergence largely due to our detailed prompt specification of the expected evaluation task leaving little room for ambiguity in interpretation, were averaged and reported.</p>
<p>Results</p>
<p>. The results are depicted as the purple and green bars in Figure 1.These results per synthesis quality criteria are averaged for the three synthesis types, i.e. paper-wise, methodological, and thematic since the differences between their scores were observed to be minimal.Comparing the two synthesis generators, GPT-4 outperforms Mistral on all characteristics.On the GPT-4-generated syntheses, the highest-scored characteristics are integration, cohesion, coherence, and readability, all of which have average scores of 4.95 or greater, reflecting strong overall structure, logical flow, and ease of comprehension.Informativeness, correctness, and relevancy were also scored highly as indicators of the trust and utility of the answers.Furthermore, the evaluator was conservative in scoring completeness and conciseness suggesting that there might be some redundancy or verbosity in the syntheses.Mistral-7B was evaluated substantially lower than GPT-4.Notably, Mistral and GPT-4 differ significantly in size, with GPT-4 rumored to exceed a billion parameters, which likely contributes to its strong performance in text generation and synthesis tasks.Completeness, correctness, and conciseness received the lowest scores, attributed to the unexpected output format of the syntheses produced by Mistral.In these cases, syntheses were wrongly structured in the shape of original research articles comprising title, abstract, keywords, introduction, results, methodology, and conclusion rather than as a paragraph-format synthesis of multiple other works.</p>
<p>In conclusion, both models performed best on readability, integration, cohesion, and coherence, demonstrating the fitness of LLMs to generate clear, logically structured texts.However, there may be a difference in the suitability of each LLM to the varying synthesis types, with GPT-4 performing slightly better on methodological, followed closely by thematic, whereas Mistral performs best on thematic, followed by paper-wise.</p>
<p>Human Evaluation of Synthesis Quality</p>
<p>We recruited human evaluators to conduct a survey assessing synthesis quality.</p>
<p>3.3.1 Survey setup.Prolific (https://www.prolific.com/),a paid crowdworker platform specifically for academic studies, was used to conduct the survey.To reduce effort, we subsampled the syntheses dataset to include a subset of syntheses from the domains of Chemistry, Computer Science, Earth Science, Linguistics, and Sociology.These domains were selected because from surface observations the Mistral syntheses received a wide range of scores by the LLM evaluator (cf.subsection 3.2), and represent scholarly diversity.For each domain, two data samples were selected, each including a research problem, sets of five papers, and six generated syntheses--three each from Mistral-7B and GPT-4, corresponding to the three different synthesis types.The samples were chosen to exhibit distinct performance levels: one with high average evaluation scores and the other with low scores from the LLM evaluator for the Mistral syntheses.This approach aimed to compare the scoring variability between human and LLM evaluators.Furthermore, Mistral, our open-source LLM, is targeted for performance enhancement in synthesis generation through the LLMs4Synthesis framework, which justifies our focus on its generated syntheses' evaluation scores.Five surveys were created for each of the five domains, with each survey including both data samples thus a total of 12 syntheses.Each survey started with an introduction screen that defined the task as well as detailed the nine evaluation criteria and the 5-rating scale.In a subsequent screen, participants were shown the research problem and paper titles and abstracts before evaluating the six LLM-generated syntheses on sequential screens.They could revisit the abstracts at any time and optionally provide free text feedback for each characteristic.Links to the original surveys underlying the domain names are: Chemistry, Computer Science, Earth Science, Linguistics, Sociology.Evaluations were obtained from three participants for each domain, involving 15 participants in total.The survey for the human evaluator and the LLM evaluator prompt were closely aligned to ensure nearly identical evaluation setups.</p>
<p>Survey participant characteristics.</p>
<p>Prolific filters were used to screen for eligible individuals, specifically: 1) those fluent in English; 2) those who have completed at least an undergraduate degree, and; 3) those whose subject is in the appropriate domain.Prolific only verifies participants' identity and country of residence; all other demographic details, such as language proficiency and academic specialization, are self-reported.</p>
<p>Our survey participants, aged 22 to 33, included both undergraduate and graduate students.Detailed demographic information is available online.Participants were paid £9 (GBP) per hour, with an estimated survey duration of 2 hours.</p>
<p>Survey Results</p>
<p>. The results are depicted as the red and blue bars in Figure 1.In line with the LLM evaluator results, overall, GPT-4-generated syntheses consistently outperform those from Mistral across all evaluated characteristics, although some differences are slight.The scores from both LLM and human evaluators are based on the same data subsample used for the Prolific survey.Performance across the evaluation criteria varied.In the case of readability, both generators performed comparably; however, this was Mistral's strongest characteristic while for GPT-4 the average scores for relevancy, correctness, informativeness, and conciseness exceeded that of readability.The most significant difference in model performance is in the characteristics of completeness, informativeness, and integration.Additionally, we can gain an overview of participants' perceptions of the syntheses by examining the optional feedback provided for some texts.Below is a summary of the comments left for each characteristic.</p>
<ol>
<li>Relevancy.Both LLMs are generally positively received for relevancy.GPT-4's syntheses are more detailed and directly relevant to research problems, while Mistral may generalize findings.Both models need to improve on making connections and providing deeper insights, but GPT-4 slightly outperforms Mistral.2. Correctness.GPT-4 shows a high degree of correctness, whereas Mistral's performance is variable, with reports of "multiple serious inaccuracies." While GPT-4 reliably captures abstract details, it occasionally misses specific study details or statistical values.Mistral sometimes mislabels studies and includes incorrect information.3. Completeness.GPT-4 generally covers abstracts better and includes more specific information than Mistral, although both can miss key quantitative details.Mistral has more frequent gaps and occasionally misses entire studies, often being too brief.Both need to enhance their depth and detail, but GPT-4 maintains slightly better completeness.4. Informativeness.GPT-4 is generally more informative, though more explicit details on methods and influencing factors would be beneficial.Mistral's syntheses tend to be general and lack depth, with both models critiqued for insufficient insight into the research problem. 5. Integration.GPT-4 effectively integrates sources into a structured synthesis, whereas Mistral tends to list sources without transitions.GPT-4 is more consistent in creating a unified narrative, though both could improve paragraph organization.6. Cohesion.GPT-4 offers more cohesive syntheses with smoother transitions and well-connected ideas, whereas Mistral's work often reads like separate summaries.GPT-4 effectively groups studies thematically, enhancing synthesis cohesion.7. Coherence.GPT-4 shows higher overall coherence, while Mistral's syntheses can seem disjointed.Both models could strengthen connections between ideas, but GPT-4 is closer to achieving this.8. Readability.Both models produce clear syntheses, but Mistral's style is slightly preferred for its simplicity.GPT-4 adheres well to stylistic norms but can be overly complex.Despite structural issues, Mistral's writing quality is praised.9. Conciseness.GPT-4 generally adheres to word limits better, while Mistral often exceeds or falls short of the 200-word target.Both are praised for being non-repetitive.</li>
</ol>
<p>Finally, the domain-specific survey evaluations reveal that GPT-4 performs best in Earth Science and Computer Science, while Mistral excels in Earth Science and Chemistry.Both LLMs show weaker performance in Linguistics and Sociology, suggesting a better fit for engineering and hard sciences.For more insights, visit our repository.</p>
<p>This first part of the paper has introduced the scientific synthesis task, LLM synthesis generators, the multidisciplinary ORKG dataset, and insights from both an automatic LLM evaluator and a human survey.These elements contribute to defining the LLMs4Synthesis framework, which optimizes open-source LLMs for synthesis generation.The second part evaluates this framework, particularly against Mistral-7B, focusing on improvements in output format and addressing weaknesses in the nine evaluation criteria.</p>
<p>The LLMs4Synthesis Framework</p>
<p>The proposed LLMs4Synthesis adopts a reinforcement learning with AI feedback (RLAIF) [5,11] paradigm to fine-tune LLM synthesizers for scientific synthesis generation.As illustrated in Figure 2, the first module attempts to construct a standardized dataset.The next step of LLMs4Synthesis is supervised fine-tuning with the aim of learning the task distributions by instructing Mistral-7B as the base LLM and GPT-4 synthesis as the gold standard, attempting to shift the general LLM probability distributions toward scientific domains while maintaining the general knowledge.Finally, LLMs4Synthesis uses reinforcement learning (RL) with Proximal Policy Optimization (PPO) [37] algorithm to leverage human/AI feedback to guide the model's learning process, aiming to enhance the model's performance on synthesis generation by aligning its outputs with human preferences.Figure 2: LLMs4Synthesis Framework using Supervised Fine-Tuning and Reinforcement Learning [26].Note: SFT is optional, but we achieved better performance when it was included.</p>
<p>Supervised Finetuning (SFT)</p>
<p>Supervised fine-tuning (SFT) of LLMs [30] for scientific synthesis involves adapting the model to generate synthesized content from input papers using labeled training data.This process is crucial for customizing a pre-trained model, originally trained on a diverse range of texts, to handle more specific tasks like scientific synthesis generation based on provided research papers.The SFT module in the LLMs4Synthesis framework, illustrated in Figure 2, uses a standardized prompt template that incorporates abstracts and titles from five given papers as input   ℎ , with the synthesis generated by GPT-4 serving as the ground truth   4− ℎ .We used a Quantized Low-Rank Adapter (QLoRA) [13] for SFT to adapt LLM to a synthesis generation task while minimizing computational resources and memory usage.Let's consider  ( ) as a pretrained Mistral-7B LLM, where  ∈ R  *  represents the model's parameters, where  is the dimension of the LLM hidden state.Adapter layers with low-rank parameterization [20]  Later, the AdamW optimizer [29] was employed with a learning rate of 2 * 10 −4 , a gradient accumulation step of 4, and a warmup step of 0.03.This configuration was used to optimize both the LLM parameters  ( ) and the low-rank adapter parameters   and   over 5 epochs.</p>
<p>Modeling Feedback</p>
<p>Incorporating desired alignment preferences through reward modeling is essential for enhancing the performance of language models.This section presents two reward functions aimed at guiding the generation of syntheses.The first approach employs basic features to align with synthesis format as a paragraph and word limit constraints as structural preferences using feedback, while the second utilizes detailed qualitative scoring to encourage high-quality output using AI feedback, reflecting user objectives and enhancing alignment with human values by using GPT-4 as a proxy to the reward function [25].</p>
<p>Basic Features.</p>
<p>In synthesizing scientific content using LLMs, a key challenge is ensuring that the generated synthesis aligns with desired format and constraints.As noted in the earlier evaluations, Mistral produced synthesis following a conventional paper structure which is not per our specified synthesis format i.e. as a paragraph and within 200 words.To address this, a rule-based paper structure identifier was developed with two goals: (1) to discourage overly structured synthesis, and (2) to promote succinctness.We found that 35.44% of the Mistral generated synthesis dataset exhibited a paper structure, detected through 17 specific academic terms (e.g."Title", "Abstract", "Conclusion", etc.) and fabricated author names in the citations.To address this, we used nine regular expressions to identify various reference forms, helping us quantify the adherence to academic styles in LLM-generated texts.The word count limit used a standard space-based splitting method.</p>
<p>Let's consider  as a synthesis,   as a word counter function, and  as a paper structure identifier function that returns 1 if the synthesis has the paper structure and 0 otherwise.The basic features try to adapt human preferences for obtaining an ideal structural format of synthesis  by the following reward function:
𝑅 𝑏𝑎𝑠𝑖𝑐 (𝑆) =                    −1.5 if 𝑊 𝐶 (𝑆) &lt; 50 −1 if 𝑊 𝐶 (𝑆) &gt; 200 −2 if 𝑃𝑆 (𝑆) = 1 −0.5 if |𝑊 𝐶 (𝑆) − 200| ≤ 20 2 otherwise
The overall objective of this reward function   () is to encourage syntheses that are of moderate length (close to 200 words) and do not adhere strictly to a rigid paper structure.It penalizes both very short and very long syntheses, as well as those that follow a specific structure while rewarding those that meet the criteria for ideal synthesis length and flexibility.This approach reflects a balance between length, structure, and flexibility, aligning with human preferences for synthesis quality.</p>
<p>GPT-4 Features.</p>
<p>To reward the quality of synthesis based on nine qualitative criteria, we introduce a reward function designed to reflect how closely the provided synthesis is aligned with ideal qualitative standards obtained by GPT-4.We defined the   function that asses how close each score is per criteria to the preferred value, which in our case is five (the optimal value that one synthesis could get).The function is given by:
𝑃𝑉 𝐹 𝑠𝑐𝑜𝑟𝑒 (𝐶, 𝑝𝑣) = − 1 𝑛 𝑛 ∑︁ 𝑖=1 |𝐶 𝑖 − 𝑝𝑣 |
Here  represents a list of scores for nine qualitative criteria, denoted as { 1 ,  2 ...,   } (where  = 9), with each score   within the range of [1,5].This reward aims to maximize the average score of .To achieve this, we set the preferred value of  = 5.By using    (, ) function, we aim to encourage LLM to produce a synthesis with higher overall scores based on qualitative criteria.Finally, the following reward function is derived, which rewards the synthesis based on the synthesis scores  using the GPT-4 Evaluator.
𝑅 𝐺𝑃𝑇 −4 (𝐶) = 4.0 if 𝑃𝑉 𝐹 𝑠𝑐𝑜𝑟𝑒 ≥ −0.125
   otherwise Withing this reward function   −4 (), we aim to provide positive rewards for values of    that are closer to zero, as this indicates scores that are near the preferred value.To this, we set a threshold of −0.125.If the computed    is greater than or equal to this threshold, which means it is closer to zero and therefore represents better alignment with the preferred value, the reward is adjusted to a higher fixed value of 4.0.Otherwise, the    will be used as a reward score to punish the LLM for better synthesis generation.</p>
<p>Reinforcement Learning</p>
<p>To train a policy that generates higher-quality synthesis, we utilized reward models within an RLAIF framework.AI is utilized as a substitute for the traditional human feedback typically employed in similar contexts.In RL, given the current state (standardized prompts from the ORKG synthesis dataset), the LLM as a synthesizer produces an action (synthesis), that modifies the current state by adding a token to the current sequence.Once a full textual sequence has been produced, we can obtain a reward by rating the quality of the synthesis using reward models to mimic human preferences.To fine-tune the model with RL, we simply alternate between collecting data from the environment -done by generating text with the LLM and then scoring it with the reward model -and updating the policy according to the Proximal Policy Optimization (PPO) algorithm [37].</p>
<p>Importantly, we incorporate a term in the reward function that penalizes the Kullback-Leibler (KL) divergence [23] by measuring the entropy of the token probability distributions learned by the RL policy   (|), where this policy is derived from the tuned LLM synthesizer, compared to the initial LLM synthesis   (|) from the frozen SFT synthesis.The final reward function will be calculated as follows:
𝑅 = 𝑟 Θ (𝑦|𝑥) − 𝜆𝐷𝐿 𝐾𝐿 (𝜋 𝑃𝑃𝑂 (𝑦|𝑥)||𝜋 𝑏𝑎𝑠𝑒 (𝑦|𝑥))
Where  Θ (|) represents the reward model, which can be either   or   −4 .The KL penalty coefficient parameter, denoted as , is set to 0.2 for adaptive KL divergence control.We employed the Adam optimizer with a learning rate of 2.94 × 10 −5 in conjunction with PPO policy, which involves 10 epochs per batch of samples.The   [42] is defined as follows:
𝐷𝐿 𝐾𝐿 (𝜋 PPO ∥𝜋 base ) = log 𝜋 PPO (𝑦|𝑥)
 base (|) Intuitively, the entropy value captures how much information is stored within a probability distribution by measuring the dissimilarity between two probability distributions, with the primary goal of maximizing the reward achieved by the model during RL policy training.Another advantage of KL divergence is that it constrains the model to ensure that it doesn't drift too far from the pre-trained policy.</p>
<p>5 The LLMs4Synthesis Evaluations 5.1 Experimental Setup 5.1.1Dataset Split.The ORKG synthesis dataset is divided into training and testing splits to facilitate both training and evaluation processes.The overall dataset consists of 348 comparisons and 1044 standardized synthesis prompts.We partitioned the dataset by domain, allocating 20% of the comparisons for testing and the remaining 80% for training.This resulted in 78 comparisons with 234 standardized synthesis prompts for testing and 270 comparisons with 810 standardized synthesis prompts for training.For training purposes, the data is further divided into two equal parts: 135 comparisons and 405 synthesis prompts each for training LLMs (denoted as Train-LLM) and RL (denoted as Train-RL).Additionally, a small subset from the test set, comprising 10 comparisons and 30 standardized synthesis prompts, is used for Prolific human evaluations.This structured division ensures a comprehensive approach to model training and evaluation, leveraging diverse methodologies and human feedback.</p>
<p>Experimental Models.</p>
<p>In our experiments, we utilized a variety of models to assess the effectiveness of different fine-tuning and RL strategies.We used seven models as follows: (1) GPT-4 is a</p>
<p>Results</p>
<p>To study the research questions (RQs) of this work, we examine the results reported for the experimental models in Table 3.This analysis will provide insights into how each model performs across different evaluation metrics and criteria.RQ1: What features are essential for qualitatively evaluating the informativeness and meaningfulness of a scientific synthesis?Given the results in Table 3, we analyze this question using GPT-4 evaluation metrics and comparison of results for w/ GPT-4 Features based models with Systems models.Assessing semantic quality.GPT-4 and SFT + RLAIF (w/ GPT-4 Features) excel in several key areas.They lead with the highest scores in relevancy (4.97 and 4.88), demonstrating their ability to produce highly relevant synthesis content.These models also score highest in correctness (4.93 and 4.75), reflecting their strong performance in generating accurate information.In terms of completeness, GPT-4 and SFT + RLAIF (w/ GPT-4 Features) again lead with scores of 4.42 and 4.19, indicating more comprehensive topic coverage.Moreover, GPT-4 and SFT + RLAIF (w/ GPT-4 Features) consistently perform well across additional criteria such as informativeness, integration, cohesion, coherence, readability, and conciseness, showcasing their superior ability to produce well-rounded and high-quality outputs.In contrast, Vanilla and SFT show lower scores in these areas, suggesting they are less effective in relevancy, correctness, completeness, and overall content quality.Impact of adding w/ GPT-4 Features-based feedbacks.Incorporating GPT-4 Features-based feedback into the RLAIF and SFT+RLAIF models significantly enhances its performance across nearly all metrics compared to the baseline Vanilla and SFT models.This improvement indicates that the addition of GPT-4 Features feedback results in more semantically rich and coherent syntheses.By leveraging the   −4 () reward function, the model effectively aligns with human preferences and achieves superior quality and informativeness through RLAIF-based fine-tuning.</p>
<p>RQ2: How can LLMs be fine-tuned to adhere to specific formatting standards like word limits and paragraph structure in scientific syntheses?We address this question by analyzing the findings presented in Table 3 using results for Basic evaluation metrics and comparison between w/ Basic Features based models with Systems models.Adherence to the word limit.GPT-4 with an average word count of 218 obtains fairly close to the ideal of 200 word limit.However, the Vanilla model with a higher average word count of 242, shows less control over the adherence to the word limit, and even supervised fine-tuning (SFT averaged word count of 231) did not show an improvement.Nevertheless, GPT-4 majority of outputs are within the 150-250 word range (91.02%), however, Vanilla (48.71%) and SFT (81.62%) fail to surpass this performance.When it comes to minimizing overgenerating (  &gt; 250), GPT-4 performs exceptionally well with a score of 8.97%.SFT also shows impressive performance, scoring 17.94%, which is twice as good as the Vanilla model's score of 34.18%.Similarly, in generating less content (50 ≤   &lt; 150), the Vanilla model falls short with an average score of 17.09%.Using the   () reward function, both the RL (w/ Basic Features) and SFT + RL (w/ Basic Features) models produce word counts close to the ideal average.RL (w/ Basic Features) averages 189 words, while RQ3: How can the balance be achieved in LLMs between maintaining formatting standards and ensuring semantically informative and meaningful scientific syntheses?According to the SFT+RLAIF (w/ GPT-4 Features) results in Table 3, balancing the basic characteristics and semantic quality in LLM scientific synthesis involves leveraging advanced fine-tuning methods such as RLAIF with specialized reward functions such as   () and   −4 () accordingly.The SFT+RLAIF (w/ GPT-4 Features) model demonstrates an effective balance, excelling in both basic and qualitative metrics.It maintains optimal word count distribution and avoids paper-like structures while achieving high scores in all qualitative criteria.This balance is likely achieved through iterative fine-tuning that focuses on both structural adherence and semantic quality, indicating that an integrated approach combining SFT and RLAIF with model feedback can effectively balance these aspects.</p>
<p>Discussion</p>
<p>An LLM as a Quality Evaluator.As language models like BERT and BART have advanced, evaluation methods have evolved.Traditional metrics have been supplemented by approaches such as BERTScore [46], which uses contextual embeddings to assess semantic similarity effectively, and BLEURT [39], which leverages human-annotated data for more accurate text quality predictions.The advent of GPT-4 introduced more sophisticated methods like GPTScore [19], utilizing its zero-shot capabilities for versatile text evaluation, and LLM-Eval [28], which employs a single LLM prompt to robustly evaluate conversational quality, correlating strongly with human judgments.Additionally, the LLM-as-a-judge approach [47] finds GPT-4 approximating human evaluations with high agreement rates, offering a scalable alternative where traditional methods are impractical.These advancements highlight the increasing use of LLMs not only in generating text but also in evaluating it, a direction that aligns with this work and enhances the scalability, versatility, and precision of assessment methods.Human Preferences.The rule-based method was manually developed based on observations of the training set and incorporating human preferences via the   () reward model described in section 4.2.1.Our training set comprised 810 samples, with 296 of these identified as having paper structures.To detect these structures seen in 296 syntheses, we implemented a rule-based binary classifier using a set of predefined rules.This classifier utilized a paper structure vocabulary comprising 17 terms and 9 reference identifier regular expressions.The vocabulary-based identifier achieved an F1-score of 67.80%, while the reference identifier detection reached an F1-score of 90.87%.When both identifiers were combined, the final model achieved an overall F1-score of 98.67%, with only 10 misclassified cases.Further analysis involved annotating outputs from the SFT and Vanilla models and comparing them to our model's results.Human annotations indicated that the Vanilla model had 31.62% of its syntheses with paper structure, while the SFT model had 2.99%.Our model identified 32.47% of the Vanilla syntheses and 1.28% of the SFT syntheses as having paper structures.This demonstrates that our paper structure identifier model effectively contributes to the reward mechanism, with a focus on improving paper structure adherence in synthesis.Consistency Comparison.In Figure 3, we analyzed GPT-4 evaluator behavior comparing the Vanilla baseline model to the SFT+RLAIF (w/ GPT-4 Features) optimized model output generated thrice on the same test set.The results reveal that the Vanilla model's output is inconsistent across three different runs.This variability suggests that the Vanilla model is less reliable as a synthesizer.In contrast, the SFT+RLAIF model exhibits much greater stability, outperforming the Vanilla model, with syntheses evaluations closely clustered and demonstrating consistent performance of syntheses evaluations.This enhanced consistency highlights the effectiveness of RLAIF in improving model reliability.The GPT-4 evaluator's ability to provide reliable and consistent assessments further supports these findings, making it an effective tool for evaluating model performance.Overall, the experiment underscores the benefits of RLAIF in delivering both consistent and higher-quality results.</p>
<p>Conclusion</p>
<p>In conclusion, this work presents the LLMs4Synthesis framework, a comprehensive approach designed to enhance open-source LLMs for generating high-quality scientific syntheses.Addressing the challenges posed by the growing volume and complexity of scientific literature, the framework introduces new synthesis types, and quality evaluation criteria, and leverages RLAIF to improve synthesis generation.The study not only demonstrates the effectiveness of LLMs in producing and evaluating scientific summaries but also provides publicly available resources to further advance research in this domain, fostering greater accessibility and collaboration within the scientific community.</p>
<p>4 Figure 1 :
41
Figure 1: Evaluation results from the GPT-4 LLM evaluator (purple and green bars) and a Prolific human survey (red and blue bars) for syntheses generated by Mistral and GPT-4.The data includes averaged scores across three synthesis types and five domains-Chemistry, Computer Science, Earth Science, Linguistics, and Sociology.</p>
<p>x:</p>
<p>Generate a synthesis from ... The methodologies employed ... y: In recent studies, researchers ...</p>
<p>introduce the transformation based on  ′ =   , where  ∈ R  *  and  ∈ R  *  are low-rank matrices with  representative of the rank, which set to  = 8.  is the transformation of the hidden state of LLM,  is the further transform the output of .Later,  and  quantized via   =  () and   =  () to reduce the memory footprint using 4-bit quantitation at  (.).In forward pass generation the model output ℎ is been obtained based on input sequence  ∈   ℎ by ℎ =  ( , )+ ( *   (    ( , ))), Where  ( , ) is the output of  for  ∈   ℎ ,   (    ( , )) is the task-specific adaptation introduced by quantized adapter layer,  is the scaling factor for balancing the contribution the model final output which  = 16, and  (.) denotes the dropout operation which applied to the output of the low-rank adapter with dropout rate of 0.05 to regularize the lowrank adapters, reducing overfitting by randomly dropping parts of the adapter's output during training.</p>
<p>Table 1 :
1
Top 10research fields in the ORKG synthesis dataset.
Research fieldFrequencyComputer Sciences125Physics28Animal Sciences19Chemistry17Urban Studies and Planning16Earth Sciences14Oceanography and Atmospheric Sciences and Meteorology14Science and Technology Studies12Materials Science and Engineering12Engineering10classification" and "Automated construction of health knowledgegraphs from medical records" are included as research problems inour dataset. However, the diverse and subjective nature of theseentries, stemming from the dataset's crowdsourced origins, reflectsand is representative of user behavior on online search platformssuch as Elicit or ORKG Ask. This diversity is a specific strength ofour dataset, setting it apart from previous datasets [34] created by asingle team of human annotators. Similarly, since users may assigna research field from any level in the hierarchy, they range frombroad, high-level fields like "Chemistry" to more specified fieldslike "Medicinal Chemistry and Pharmaceuticals." Therefore, eachsample has two research field columns: the original label selectedby the user, and a mapping of the selected field to the third level ofthe ORKG's taxonomy. We chose the third level as representativeenough to generalize to specific research field assignments. Note, re-search fields are not used in synthesis generation but are included asdata points to help organize and understand the dataset. Table 1 liststhe top 10 research fields in the ORKG synthesis generation dataset.The complete distribution is available at https://github.com/jd-coderepos/scisynthesis/blob/main/corpus/domain_counts.xlsx.</p>
<p>Table 2 :
2
Scientific synthesis generation task prompt.</p>
<p>Table 3 :
3
Results from LLMs4Synthesis using both Basic and GPT-4 evaluations across various metrics.Word Count metrics are reported as percentages, except for the average value.Paper Structure measures the percentage of observed synthesis with the paper's structure.GPT-4 results are averaged scores across nine characteristics, obtained from three separate evaluations.-the-art LLM developed by OpenAI.For our experiments, we used the gpt-4-1106-preview version.(2) Vanilla is a Mistral-7B model that has not undergone any fine-tuning, representing the raw capabilities of the Mistral-7B LLM.For our experiments, we utilized the Mistral-7B-Instruct-v0.1 version.(3) SFT is a fine-tuned version of the Vanilla model.The fine-tuning was performed using the QLoRA method, with the GPT-4 based synthesis outputs serving as the ground truth text.We used the Train-LLM split from the training data for fine-tuning.(4) RL (w/ Basic Features) involves fine-tuning the Vanilla model using RL with a basic reward function   ().
Evaluation CriteriaSystemsw/ Basic Featuresw/ GPT-4 FeaturesTypeMetricsGPT-4 VanillaSFTRLSFT + RL RLAIF SFT + RLAIF𝑊 𝐶 &lt; 500.00.00.00.00.00.420.050 ≤ 𝑊 𝐶 &lt; 1500.017.090.42 15.810.8511.960.0BasicWord Count150 ≤ 𝑊 𝐶 ≤ 250 𝑊 𝐶 &gt; 25091.02 8.9748.71 81.62 83.76 34.18 17.94 0.4298.71 0.4287.17 0.42100 0.0Average218242231189204194205Paper Structure1.2832.471.281.700.00.850.0Relevancy4.974.334.653.974.484.754.88Correctness4.933.663.632.973.094.394.75Completeness4.423.003.132.262.553.544.19Informativeness4.953.944.213.413.844.284.83GPT-4Integration4.994.414.733.674.544.744.89Cohesion4.994.444.733.814.514.784.89Coherence4.984.384.693.764.474.734.85Readability4.964.594.434.334.164.904.78Conciseness3.893.423.323.113.083.653.64state-of(5) SFT+RL (w/ Basic Features)is created by further fine-tuning the SFT model using RL with a ba-sic features reward function 𝑅 𝑏𝑎𝑠𝑖𝑐 (𝑆) that combines SFT and RL toenhance the performance w.r.t human preferences. (6) RLAIF (w/GPT-4 Features) is the result of further fine-tuning the RLAIF (w/Basic Features) model using a GPT-4 features reward function𝑅 𝐺𝑃𝑇 −4 (𝐶). (7) SFT+RLAIF (w/ GPT-4 Features) represents thefinal stage of our experiments, where the SFT+RLAIF (w/ BasicFeatures) model is further fine-tuned using RLAIF with GPT-4features reward function 𝑅 𝐺𝑃𝑇 −4 (𝐶).
AcknowledgmentsWe thank Julia Evans for her invaluable work on the "Human Evaluation of Synthesis Quality".This work is jointly supported by the NFDI4DataScience initiative (DFG, German Research Foundation, Grant ID: 460234259) and the SCINEXT project (BMBF, German Federal Ministry of Education and Research, Grant ID: 01lS22070).
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Decoding LLM Performance: A Guide to Evaluating LLM Applications. Amogh Agastya, 2023</p>
<p>Revisiting Automatic Evaluation of Extractive Summarization Task: Can We Do Better than ROUGE?. Mousumi Akter, Naman Bansal, Shubhra Kanti, Karmaker , 10.18653/v1/2022.findings-acl.122Findings of the Association for Computational Linguistics: ACL 2022. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Improving access to scientific literature with knowledge graphs. Sören Auer, Allard Oelen, Muhammad Haris, Markus Stocker, D' Jennifer, Kheir Eddine Souza, Lars Farfar, Manuel Vogt, Vitalis Prinz, Mohamad Wiens, Jaradeh Yaser, Bibliothek Forschung und Praxis. 442020. 2020</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022. 2022arXiv preprint</p>
<p>Benchmarking foundation models with language-model-as-an-examiner. Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Benchmarking Foundation Models with Language-Model-as-an-Examiner. Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, Lei Hou, Advances in Neural Information Processing Systems. A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references. Lutz Bornmann, Rüdiger Mutz, Journal of the association for information science and technology. 662015. 2015</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023. 2023arXiv preprint</p>
<p>Kendra Cherry, How to Write an Abstract in APA Format. 2023</p>
<p>Deep reinforcement learning from human preferences. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, 2017. 201730Advances in neural information processing systems</p>
<p>Revisiting Summarization Evaluation for Scientific Articles. Arman Cohan, Nazli Goharian, ; , Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Sara Goggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). Stelios Odijk, Piperidis, the Tenth International Conference on Language Resources and Evaluation (LREC'16)Asuncion Moreno; Portorož, Slovenia2016. JanEuropean Language Resources Association (ELRA)</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, arXiv:2305.14314[cs.LGQLoRA: Efficient Finetuning of Quantized LLMs. 2023</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies20191</p>
<p>D' Jennifer, Souza, 10.48366/R609337A Catalog of Transformer Models. 2023</p>
<p>Elicit: Analyze research papers at superhuman speed. 2024</p>
<p>Julia Evans, D' Jennifer, Sören Souza, Auer, arXiv:2407.02977Large Language Models as Evaluators for Scientific Synthesis. 2024. 2024arXiv preprint</p>
<p>. Santo Fortunato, Carl T Bergstrom, Katy Börner, James A Evans, Dirk Helbing, Staša Milojević, Filippo Alexander M Petersen, Roberta Radicchi, Brian Sinatra, Uzzi, Science of science. 3591852018. 2018Science</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, arXiv:2302.04166Gptscore: Evaluate as you desire. 2023. 2023arXiv preprint</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, arXiv:2106.09685[cs.CL2021</p>
<p>Authors and Abstracts -IEEE PVSC. IEEE</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825[cs.CLMistral 7B. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023</p>
<p>Kullback-Leibler Divergence. James M Joyce, 10.1007/978-3-642-04898-2_3272011SpringerBerlin Heidelberg; Berlin, Heidelberg</p>
<p>Neural Text Summarization: A Critical Evaluation. Wojciech Kryscinski, Nitish Shirish Keskar, Bryan Mccann, Caiming Xiong, Richard Socher, 10.18653/v1/D19-1051Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Vincent Jiang, Xiaojun Ng, Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh, arXiv:2303.00001[cs.LGReward Design with Language Models. 2023</p>
<p>Illustrating Reinforcement Learning from Human Feedback (RLHF). Nathan Lambert, Louis Castricato, 2022. 2022Hugging Face Blog</p>
<p>ROUGE: A Package for Automatic Evaluation of Summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. Yen-Ting Lin, Yun-Nung Chen, arXiv:2305.137112023. 2023arXiv preprint</p>
<p>Ilya Loshchilov, Frank Hutter, arXiv:1711.05101[cs.LGDecoupled Weight Decay Regularization. 2019</p>
<p>Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, Jianfeng Gao, arXiv:2402.06196[cs.CLLarge Language Models: A Survey. 2024</p>
<p>How to Write a Scientific Abstract. Suhasini Nagda, 10.1007/s13191-013-0299-xThe Journal of Indian Prosthodontic Society. 132013. 2013</p>
<p>Comparing Research Contributions in a Scholarly Knowledge Graph. Mohamad Yaser Allard Oelen, Kheir Eddine Jaradeh, Markus Farfar, Sören Stocker, Auer, Proceedings of the Third International Workshop on Capturing Scientific Knowledge co-located with the 10th International Conference on Knowledge Capture. the Third International Workshop on Capturing Scientific Knowledge co-located with the 10th International Conference on Knowledge CaptureMarina Del Rey, CA, USA2019. 2019</p>
<p>Open Research Knowledge Graph Ask Team. 2024. ORKG Ask -Find research you are actually looking for. </p>
<p>CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering. David Pride, Matteo Cancellieri, Petr Knoth, ; Omar Alonso, Helena Cousijn, Gianmaria Silvello, Mónica Marrero, 10.1007/978-3-031-43849-3_13Linking Theory and Practice of Digital Libraries. Carla Teixeira, Lopes , Stefano Marchesin, Nature SwitzerlandSpringer2023</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 192019. 2019</p>
<p>. Elvis Saravia, 2022. 12 2022Prompt Engineering Guide</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347[cs.LGProximal Policy Optimization Algorithms. 2017</p>
<p>Scispace Team, SciSpace -The Fastest Research Platform Ever. 2024</p>
<p>BLEURT: Learning Robust Metrics for Text Generation. Thibault Sellam, Dipanjan Das, Ankur Parikh, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Beyond the Imitation Game: Quantifying and extrapolatingthe capabilities of language models. Aarohi Srivastava, Denis Kleyjo, Ziyi Wu, Transactions on Machine Learning Research. 52023. 2023</p>
<p>How to Write a Science Fair Project Abstract. Science Buddies Staff</p>
<p>Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano, arXiv:2009.01325[cs.CLLearning to summarize from human feedback. 2022</p>
<p>Formatting Your Dissertation. </p>
<p>Global scientific output doubles every nine years. Richard Van Noorden, Nature News Blog. 2014</p>
<p>/Write-an-Abstract-in-MLA-Style Accessed. How to Write an Abstract in MLA Style</p>
<p>BERTScore: Evaluating Text Generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. </p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362024. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
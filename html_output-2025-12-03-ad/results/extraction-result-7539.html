<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7539 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7539</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7539</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-cddb552f6c3464a54a02b0b64b2d1af56c086606</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cddb552f6c3464a54a02b0b64b2d1af56c086606" target="_blank">MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities, and yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems.</p>
                <p><strong>Paper Abstract:</strong> The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities. We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as MathCodeInstruct. Each solution interleaves natural language, code, and execution results. We also introduce a customized supervised fine-tuning and inference approach. This approach yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems. Impressively, the MathCoder models achieve state-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K (83.9%) datasets, substantially outperforming other open-source alternatives. Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K and MATH but also outperforms GPT-4 on the competition-level MATH dataset. The dataset and models will be released at https://github.com/mathllm/MathCoder.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7539.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7539.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Code Interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Code Interpreter (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source large language model augmented with an execution environment (code interpreter) that generates code interleaved with natural-language reasoning and executes that code to obtain and verify intermediate results; used both as a strong baseline and as a data/source-of-solutions for dataset creation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 Code Interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>closed-source (size not stated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>closed-source LLM with tool/plugin for on-the-fly code execution (code-interpreter)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics (mathematical problem solving / numerical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation of step-by-step mathematical problem solving where the model produces natural-language reasoning interleaved with code, executes the code to compute exact numerical/intermediate results, and continues reasoning conditioned on execution outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Instruction-style prompts that elicit chain-of-thought and code interleaving (Code-Interpreter style); used to produce LCE-format solutions and to evaluate/generated interpolation problems (examples provided to the model in the interpolation prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact-answer accuracy (%) on benchmark math datasets (GSM8K, MATH, etc.) — i.e., percentage of problems solved correctly.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>GSM8K: 97.0% exact-answer accuracy; MATH: 69.7% exact-answer accuracy (reported in this paper as the closed-source SOTA baseline for code-interpreter style).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Representative open-source baseline cited: Llama-2-70B ~56.8% on GSM8K and 13.5% on MATH (paper cites open-source gap to GPT-4 Code Interpreter).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['ability to generate code and execute it (tool use / external execution)', 'quality of natural-language reasoning interleaved with code', 'model capacity / closed-source model scale (implicit)', 'use of execution results to verify/drive subsequent reasoning']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Used as a data-source and baseline; execution environment available to run generated code (actual interpreter). Specific GPT-4 runtime/hyperparameters not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Not directly reproducible within open-source models (closed-source); the paper notes open-source models lagged behind GPT-4 Code Interpreter despite training recipes, implying dependence on proprietary model capabilities; no detailed failure-mode breakdown for GPT-4 Code Interpreter is given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7539.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7539.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MathCoder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MathCoder (family of MathCoder-L and MathCoder-CL models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of open-source LLMs (fine-tuned Llama-2 and CodeLlama variants) trained on MathCodeInstruct, a dataset of solutions that interleave natural language, code, and execution outputs (LCE format), enabling the models to generate code, invoke execution, and continue reasoning to solve math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MathCoder (MathCoder-L for Llama-2 base; MathCoder-CL for CodeLlama base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>multiple sizes reported: Llama-2 bases 7B / 13B / 70B; CodeLlama bases 7B / 13B / 34B (MathCoder variants correspond to these sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>supervised fine-tuned on domain-specific MathCodeInstruct dataset (instruction-tuned + LCE-specialized); supports tool use (on-the-fly code execution during inference via Jupyter/ interpreter integration)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics (mathematical reasoning across grade-school to competition-level problems)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation of algorithmic math problem solving: autoregressively generate natural-language steps and code blocks, run the code to obtain exact execution outputs, append execution results to the context, and continue reasoning to final numeric answer (LCE — Language, Code, Execution — loop).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Models evaluated in zero-shot (no extra prompting) at test time; training data creation involved (a) collecting GPT-4 Code Interpreter-style solutions for GSM8K and MATH, (b) problem-interpolation prompting (examples provided) to synthesize intermediate-difficulty problems, and (c) self-distillation (generate n=3 LCE solutions and keep those with unanimous answers). During SFT the model is taught to use special tokens (<|text|>, <|code|>, <|execution|>, <|endofblock|>) to delimit components. Inference uses special-token detection and external code execution.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact-answer accuracy (%) on benchmarks: GSM8K, MATH, SVAMP, Mathematics, SimulEq; also average across datasets reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Top reported open-source results from this paper: GSM8K up to 83.9% exact-answer accuracy and MATH up to 45.2% exact-answer accuracy (family-wide peak numbers reported in abstract and tables). Example models: MathCoder-L-70B and MathCoder-CL-34B reported near these top values in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Closed-source baselines: GPT-4 Code Interpreter GSM8K 97.0%, MATH 69.7%; other baselines: WizardMath-70B GSM8K 81.6% / MATH 22.7%; Llama-2-70B reported ~56.8% GSM8K / 13.5% MATH (paper cites these for comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['model size (larger models improved accuracy)', 'base model type (CodeLlama base provided better coding ability; Llama-2 base provided stronger language reasoning in some cases)', 'use of actual code execution during inference (substantially improves accuracy vs. predicting execution results)', 'whether execution outputs were included in training loss (zeroing loss on execution results improved final performance)', 'dataset composition and augmentation (problem-interpolation augmentation improved accuracy)', 'quality of seed solutions (GPT-4 generated seeds) and self-distillation filtering (keeping only unanimous answers)', 'subject/difficulty of problems (geometry problems showed lower performance; algebra/number-theory benefited more from code execution)', 'number of LCE blocks allowed and context length limits']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Supervised fine-tuning hyperparameters: learning rate 2e-5, context length 2048, batch-size-equivalent 128 (with gradient accumulation as needed), cosine scheduler for 3 epochs with 50-step warmup; training used DeepSpeed ZeRO-3 and flash attention; hardware: 8/16/32 A800 80GB GPUs depending on model size; inference: Hugging Face generation with greedy decoding, max new tokens per block = 512, allow up to 32 LCE blocks; zero-shot evaluation (no extra prompt) for MathCoder models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Still lags behind GPT-4 Code Interpreter overall; struggles notably on geometry and some higher-difficulty subjects; reliance on GPT-4 for seed data (limits capabilities to what GPT-4 can produce); uni-modal (text-only) models cannot handle complex figure-based geometry well; models often fail to correctly predict execution results themselves, making external execution necessary; fine-tuning to predict execution outputs harms final performance (paper shows zeroing out loss on execution results improves results).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7539.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7539.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeLlama-34B (as base)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeLlama-34B (Rozière et al.; CodeLlama family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source foundation model pre-trained with code-heavy data; used in this paper as the base for MathCoder-Initial and MathCoder-CL variants, demonstrating strong code-generation ability which improves LCE-style math problem solving after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeLlama (MathCoder-CL variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B (also used 7B and 13B CodeLlama variants in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>base LLM pre-trained with code and then supervised fine-tuned on MathCodeInstruct (domain data); supports external code execution in inference loop</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics (numerical/math word problem solving)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Generate code-rich solutions and natural-language reasoning for math problems and act as a text-based simulator by producing code that can be executed to yield exact intermediate numerical results used to continue reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Fine-tuned on LCE-format MathCodeInstruct dataset; initial MathCoder-Initial built by fine-tuning CodeLlama-34B on GPT-4 produced seed LCE solutions; then used to generate and self-distill solutions for interpolation problems; evaluated zero-shot at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact-answer accuracy (%) on GSM8K, MATH, SVAMP, Mathematics, SimulEq benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>As MathCoder-CL-34B (after full MathCodeInstruct fine-tuning): GSM8K 81.7% and MATH 45.2% (reported in tables); earlier CodeLlama-34B trained only on seed GSM8K+MATH (49k) achieved GSM8K 77.3% and MATH 44.0% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared against Llama-2-based MathCoder variants and prior open-source SOTA (WizardMath); CodeLlama-34B based MathCoder improves over Llama-2 counterparts of same size in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['additional code-pretraining/data of CodeLlama contributes to better code generation and improved math reasoning when fine-tuned with LCE data', 'model size (34B performs better than 7B/13B counterparts)', 'inference code execution and training loss choices (zeroing execution-block loss) affect final performance', 'data augmentation via interpolation and self-distillation']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Same MathCoder training and inference pipeline as described for MathCoder (LR=2e-5, context=2048, 3 epochs, DeepSpeed ZeRO-3, greedy decoding at inference, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Despite stronger code ability, CodeLlama-based models still exhibit gaps compared to GPT-4 Code Interpreter; subject-specific weaknesses remain (geometry); dependence on quality of generated training data and self-distillation filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification <em>(Rating: 2)</em></li>
                <li>PAL: Program-aided language models <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>WizardMath: Empowering mathematical reasoning for large language models via reinforced evol-instruct <em>(Rating: 2)</em></li>
                <li>Code Llama: Open foundation models for code <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7539",
    "paper_id": "paper-cddb552f6c3464a54a02b0b64b2d1af56c086606",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "GPT-4 Code Interpreter",
            "name_full": "GPT-4 Code Interpreter (OpenAI)",
            "brief_description": "A closed-source large language model augmented with an execution environment (code interpreter) that generates code interleaved with natural-language reasoning and executes that code to obtain and verify intermediate results; used both as a strong baseline and as a data/source-of-solutions for dataset creation in this paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 Code Interpreter",
            "model_size": "closed-source (size not stated)",
            "model_type": "closed-source LLM with tool/plugin for on-the-fly code execution (code-interpreter)",
            "scientific_domain": "Mathematics (mathematical problem solving / numerical reasoning)",
            "simulation_task_description": "Text-based simulation of step-by-step mathematical problem solving where the model produces natural-language reasoning interleaved with code, executes the code to compute exact numerical/intermediate results, and continues reasoning conditioned on execution outputs.",
            "prompting_strategy": "Instruction-style prompts that elicit chain-of-thought and code interleaving (Code-Interpreter style); used to produce LCE-format solutions and to evaluate/generated interpolation problems (examples provided to the model in the interpolation prompt).",
            "evaluation_metric": "Exact-answer accuracy (%) on benchmark math datasets (GSM8K, MATH, etc.) — i.e., percentage of problems solved correctly.",
            "reported_accuracy": "GSM8K: 97.0% exact-answer accuracy; MATH: 69.7% exact-answer accuracy (reported in this paper as the closed-source SOTA baseline for code-interpreter style).",
            "baseline_accuracy": "Representative open-source baseline cited: Llama-2-70B ~56.8% on GSM8K and 13.5% on MATH (paper cites open-source gap to GPT-4 Code Interpreter).",
            "factors_reported": [
                "ability to generate code and execute it (tool use / external execution)",
                "quality of natural-language reasoning interleaved with code",
                "model capacity / closed-source model scale (implicit)",
                "use of execution results to verify/drive subsequent reasoning"
            ],
            "experimental_conditions": "Used as a data-source and baseline; execution environment available to run generated code (actual interpreter). Specific GPT-4 runtime/hyperparameters not provided in this paper.",
            "limitations_or_failure_modes": "Not directly reproducible within open-source models (closed-source); the paper notes open-source models lagged behind GPT-4 Code Interpreter despite training recipes, implying dependence on proprietary model capabilities; no detailed failure-mode breakdown for GPT-4 Code Interpreter is given in this paper.",
            "uuid": "e7539.0",
            "source_info": {
                "paper_title": "MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "MathCoder",
            "name_full": "MathCoder (family of MathCoder-L and MathCoder-CL models)",
            "brief_description": "A family of open-source LLMs (fine-tuned Llama-2 and CodeLlama variants) trained on MathCodeInstruct, a dataset of solutions that interleave natural language, code, and execution outputs (LCE format), enabling the models to generate code, invoke execution, and continue reasoning to solve math problems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MathCoder (MathCoder-L for Llama-2 base; MathCoder-CL for CodeLlama base)",
            "model_size": "multiple sizes reported: Llama-2 bases 7B / 13B / 70B; CodeLlama bases 7B / 13B / 34B (MathCoder variants correspond to these sizes)",
            "model_type": "supervised fine-tuned on domain-specific MathCodeInstruct dataset (instruction-tuned + LCE-specialized); supports tool use (on-the-fly code execution during inference via Jupyter/ interpreter integration)",
            "scientific_domain": "Mathematics (mathematical reasoning across grade-school to competition-level problems)",
            "simulation_task_description": "Text-based simulation of algorithmic math problem solving: autoregressively generate natural-language steps and code blocks, run the code to obtain exact execution outputs, append execution results to the context, and continue reasoning to final numeric answer (LCE — Language, Code, Execution — loop).",
            "prompting_strategy": "Models evaluated in zero-shot (no extra prompting) at test time; training data creation involved (a) collecting GPT-4 Code Interpreter-style solutions for GSM8K and MATH, (b) problem-interpolation prompting (examples provided) to synthesize intermediate-difficulty problems, and (c) self-distillation (generate n=3 LCE solutions and keep those with unanimous answers). During SFT the model is taught to use special tokens (&lt;|text|&gt;, &lt;|code|&gt;, &lt;|execution|&gt;, &lt;|endofblock|&gt;) to delimit components. Inference uses special-token detection and external code execution.",
            "evaluation_metric": "Exact-answer accuracy (%) on benchmarks: GSM8K, MATH, SVAMP, Mathematics, SimulEq; also average across datasets reported.",
            "reported_accuracy": "Top reported open-source results from this paper: GSM8K up to 83.9% exact-answer accuracy and MATH up to 45.2% exact-answer accuracy (family-wide peak numbers reported in abstract and tables). Example models: MathCoder-L-70B and MathCoder-CL-34B reported near these top values in tables.",
            "baseline_accuracy": "Closed-source baselines: GPT-4 Code Interpreter GSM8K 97.0%, MATH 69.7%; other baselines: WizardMath-70B GSM8K 81.6% / MATH 22.7%; Llama-2-70B reported ~56.8% GSM8K / 13.5% MATH (paper cites these for comparison).",
            "factors_reported": [
                "model size (larger models improved accuracy)",
                "base model type (CodeLlama base provided better coding ability; Llama-2 base provided stronger language reasoning in some cases)",
                "use of actual code execution during inference (substantially improves accuracy vs. predicting execution results)",
                "whether execution outputs were included in training loss (zeroing loss on execution results improved final performance)",
                "dataset composition and augmentation (problem-interpolation augmentation improved accuracy)",
                "quality of seed solutions (GPT-4 generated seeds) and self-distillation filtering (keeping only unanimous answers)",
                "subject/difficulty of problems (geometry problems showed lower performance; algebra/number-theory benefited more from code execution)",
                "number of LCE blocks allowed and context length limits"
            ],
            "experimental_conditions": "Supervised fine-tuning hyperparameters: learning rate 2e-5, context length 2048, batch-size-equivalent 128 (with gradient accumulation as needed), cosine scheduler for 3 epochs with 50-step warmup; training used DeepSpeed ZeRO-3 and flash attention; hardware: 8/16/32 A800 80GB GPUs depending on model size; inference: Hugging Face generation with greedy decoding, max new tokens per block = 512, allow up to 32 LCE blocks; zero-shot evaluation (no extra prompt) for MathCoder models.",
            "limitations_or_failure_modes": "Still lags behind GPT-4 Code Interpreter overall; struggles notably on geometry and some higher-difficulty subjects; reliance on GPT-4 for seed data (limits capabilities to what GPT-4 can produce); uni-modal (text-only) models cannot handle complex figure-based geometry well; models often fail to correctly predict execution results themselves, making external execution necessary; fine-tuning to predict execution outputs harms final performance (paper shows zeroing out loss on execution results improves results).",
            "uuid": "e7539.1",
            "source_info": {
                "paper_title": "MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CodeLlama-34B (as base)",
            "name_full": "CodeLlama-34B (Rozière et al.; CodeLlama family)",
            "brief_description": "An open-source foundation model pre-trained with code-heavy data; used in this paper as the base for MathCoder-Initial and MathCoder-CL variants, demonstrating strong code-generation ability which improves LCE-style math problem solving after fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CodeLlama (MathCoder-CL variants)",
            "model_size": "34B (also used 7B and 13B CodeLlama variants in experiments)",
            "model_type": "base LLM pre-trained with code and then supervised fine-tuned on MathCodeInstruct (domain data); supports external code execution in inference loop",
            "scientific_domain": "Mathematics (numerical/math word problem solving)",
            "simulation_task_description": "Generate code-rich solutions and natural-language reasoning for math problems and act as a text-based simulator by producing code that can be executed to yield exact intermediate numerical results used to continue reasoning.",
            "prompting_strategy": "Fine-tuned on LCE-format MathCodeInstruct dataset; initial MathCoder-Initial built by fine-tuning CodeLlama-34B on GPT-4 produced seed LCE solutions; then used to generate and self-distill solutions for interpolation problems; evaluated zero-shot at test time.",
            "evaluation_metric": "Exact-answer accuracy (%) on GSM8K, MATH, SVAMP, Mathematics, SimulEq benchmarks.",
            "reported_accuracy": "As MathCoder-CL-34B (after full MathCodeInstruct fine-tuning): GSM8K 81.7% and MATH 45.2% (reported in tables); earlier CodeLlama-34B trained only on seed GSM8K+MATH (49k) achieved GSM8K 77.3% and MATH 44.0% (Table 4).",
            "baseline_accuracy": "Compared against Llama-2-based MathCoder variants and prior open-source SOTA (WizardMath); CodeLlama-34B based MathCoder improves over Llama-2 counterparts of same size in many cases.",
            "factors_reported": [
                "additional code-pretraining/data of CodeLlama contributes to better code generation and improved math reasoning when fine-tuned with LCE data",
                "model size (34B performs better than 7B/13B counterparts)",
                "inference code execution and training loss choices (zeroing execution-block loss) affect final performance",
                "data augmentation via interpolation and self-distillation"
            ],
            "experimental_conditions": "Same MathCoder training and inference pipeline as described for MathCoder (LR=2e-5, context=2048, 3 epochs, DeepSpeed ZeRO-3, greedy decoding at inference, etc.).",
            "limitations_or_failure_modes": "Despite stronger code ability, CodeLlama-based models still exhibit gaps compared to GPT-4 Code Interpreter; subject-specific weaknesses remain (geometry); dependence on quality of generated training data and self-distillation filtering.",
            "uuid": "e7539.2",
            "source_info": {
                "paper_title": "MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification",
            "rating": 2
        },
        {
            "paper_title": "PAL: Program-aided language models",
            "rating": 2
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2
        },
        {
            "paper_title": "WizardMath: Empowering mathematical reasoning for large language models via reinforced evol-instruct",
            "rating": 2
        },
        {
            "paper_title": "Code Llama: Open foundation models for code",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1
        }
    ],
    "cost": 0.016409,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning</h1>
<p>Ke Wang ${ }^{1^{<em>}}$ Houxing Ren ${ }^{1^{</em>}}$ Aojun Zhou ${ }^{1^{<em>}}$ Zimu Lu ${ }^{1^{</em>}}$ Sichun Luo ${ }^{3^{<em>}}$ Weikang Shi ${ }^{1^{</em>}}$ Renrui Zhang ${ }^{1}$ Linqi Song ${ }^{3}$ Mingjie Zhan ${ }^{1^{1}}$ Hongsheng Li ${ }^{1,2^{*}}$<br>${ }^{1}$ Multimedia Laboratory (MMLab), The Chinese University of Hong Kong<br>${ }^{2}$ Shanghai Artificial Intelligence Laboratory ${ }^{3}$ City University of Hong Kong<br>{wangk.gm, renhouxing, aojunzhou, zmjdll}@gmail.com<br>hsli@ee.cuhk.edu.hk</p>
<h4>Abstract</h4>
<p>The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities. We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as MathCodeInstruct. Each solution interleaves natural language, code, and execution results. We also introduce a customized supervised fine-tuning and inference approach. This approach yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems. Impressively, the MathCoder models achieve state-of-the-art scores among open-source LLMs on the MATH (45.2\%) and GSM8K (83.9\%) datasets, substantially outperforming other open-source alternatives. Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K and MATH but also outperforms GPT-4 on the competition-level MATH dataset. The dataset and models will be released at https://github.com/mathllm/ MathCoder.</p>
<h2>1 INTRODUCTION</h2>
<p>Recently, closed-source large language models (LLMs) such as GPT-4 (OpenAI, 2023) and PaLM2 (Anil et al., 2023), paired with methods such as Chain-of-Thought (CoT) (Wei et al., 2022) and Program-Aided Language models (PAL) (Gao et al., 2023), have shown remarkable performance on mathematical reasoning tasks. In contrast, current open-source LLMs (Touvron et al., 2023; Penedo et al., 2023; Zhang et al., 2022) still lag significantly behind in this area. Even Llama-270B (Touvron et al., 2023), one of the most potent open-source models, only scores $56.8 \%$ and $13.5 \%$ respectively on GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) datasets, remarkably lower than GPT-4 Code Interpreter ${ }^{1}$, which scores $97 \%$ and $69.7 \%$ (Zhou et al., 2023a).</p>
<p>To narrow the gap between open-source and closed-source models in math problem solving, recent works, such as the WizardMath (Luo et al., 2023) and RFT (Yuan et al., 2023), have tried to tune open-source models with math problems and CoT solutions, achieving a significant gain in performance compared to their base model, Llama-2. On the other hand, methods such as PAL (Gao et al., 2023), PoT (Chen et al., 2022), and CSV (Zhou et al., 2023a) encourage code usage in solving</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Comparison with different Instruction-following datasets. The baseline datasets include recent RFT-u13b (Yuan et al., 2023) and WizardMath (Luo et al., 2023).</p>
<table>
<thead>
<tr>
<th>Datasets</th>
<th>Seed</th>
<th>Annotation</th>
</tr>
</thead>
<tbody>
<tr>
<td>RFT-100k</td>
<td>GSM8K</td>
<td>Llama</td>
</tr>
<tr>
<td>WizardMath-96k</td>
<td>GSM8K+MATH</td>
<td>GPT-4</td>
</tr>
<tr>
<td>Ours-49k</td>
<td>GSM8K+MATH</td>
<td>GPT-4</td>
</tr>
<tr>
<td>Ours-80k</td>
<td>GSM8K+MATH</td>
<td>GPT-4 +</td>
</tr>
<tr>
<td><img alt="img-0.jpeg" src="img-0.jpeg" /></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Figure 1: Performance comparison between MathCoder, WizardMath, and Llama-1 RFT models with different model sizes.
math problems, showing promising improvements when paired with closed-source models like GPT3.5, GPT-4 and GPT-4 Code Interpreter. In particular, GPT-4 Code Interpreter surpasses the previous SOTA by a clear margin. Recent study (Zhou et al., 2023a) shows that this excellent performance can be attributed to its ability to generate and assess the execution results of a chain of code interlaced with natural language reasoning steps.</p>
<p>However, existing open-source models fail to benefit from this sophisticated mechanism since they lag behind closed-source models in both code generation and natural language reasoning. Therefore, we still lack an effective recipe to deliver open-source models to solve math problems in a manner similar to GPT-4 Code Interpreter.</p>
<p>In this paper, leveraging the strengths of GPT-4 Code Interpreter (Zhou et al., 2023a), we introduce a simple yet effective framework, MathCoder, designed to enhance the mathematical reasoning capabilities of open-source models. This framework can be categorized into two parts: (1) math instruction-following dataset construction and (2) customized supervised fine-tuning. Specifically, the instruction-following dataset, termed as MathCodeInstruct, consists exclusively of 80k math problems and their corresponding solutions. Each solution is interwoven with natural language for reasoning, code for execution, and execution results. The comparison between MathCodeInstruct and other math instruction-tuning datasets is shown in Tab. 1.</p>
<p>MathCodeInstruct is created in two steps. The first step is collecting GPT-4 Code Interpreterstyle solutions for the GSM8K and MATH training sets. GSM8K and MATH are two important datasets of math problems for improving and evaluating models' mathematical abilities, which consist of grade school math word problems and challenging competition mathematics problems, respectively. Using this data, we trained our initial models, termed MathCoder-Initial. The second step is to augment more math problems by using an innovative prompt named problem interpolation, which asks the LLM to generate questions with difficulty levels that fall between the provided MATH and GSM8K problems. This paradigm generates problems that bridge the gap between the grade-school-level problems in GSM8K and the challenging high-school-level problems in MATH, thus enhancing the dataset's generalization capability. We use MathCoder-Initial to generate solutions for these new problems. Combining this new data with those from the first step, we finetune the base Llama-2 models, reaching a score that outperforms the SOTA by a clear margin on GSM8K and MATH. Concurrently with our work, MAmmoTH (Yue et al., 2023) also creates a dataset consisting of math problems and model-generated solutions. However, their solutions consist of either only code or only natural language reasoning steps, which is notably different from our dataset of GPT-4 Code Interpreter-style solutions.</p>
<p>Regarding the supervised fine-tuning stage, we propose an effective training and inference pipeline to ensure that our fine-tuned model can behave in a manner similar to the GPT-4 Code Interpreter. We use special tokens $(&lt;|\operatorname{text}|&gt;$, $&lt;|\operatorname{code}|&gt;$, $&lt;|\operatorname{execution}|&gt;)$ to identify if a part of the training data is natural language, code, or execution results. With this deliberately created training corpus, the model learns to generate interleaved natural language and code divided by special tokens. During inference, we can use the special tokens to detect code blocks and utilize Jupyter Notebooks for code execution. We append the result of on-the-fly execution to the previous predictions of the model. Then, the model continues to autoregressively predict the next token based on this new version of the input, which includes the execution result at the end. In this way, the model would be able to "see" the execution results and continue its reasoning accordingly.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The process of dataset creation and model fine-tuning. (a) First, solutions for problems in the GSM8K and MATH datasets are collected from the GPT-4. Then, we fine-tune the CodeLlama-34B model on this data, producing the MathCoder-Initial. New problems are created using our novel prompt (detailed examples in Appendix A), and their solutions are generated using MathCoder-Initial. (b) Finally, the new problems and solutions are combined with the existing training data to create the final dataset, which we use to fine-tune the base Llama-2 model, producing our final MathCoder model.</p>
<p>We use MathCodeInstruct to fine-tune popular open-source Llama-2 and CodeLlama (Rozière et al., 2023) models, creating a family of models named MathCoder. Experimental results show that the models with our proposed dataset and training framework achieve significant improvement on various mathematical reasoning benchmarks, as depicted in Fig. 1.</p>
<p>This paper's main contributions can be summarized in three key aspects:</p>
<ul>
<li>To the best of our knowledge, this is the first systematic study that explicitly integrates natural language reasoning, code generation, and feedback from execution results into open-source pre-trained large language models, aiming at enhancing their mathematical reasoning abilities.</li>
<li>We have constructed a high-quality mathematical instruction tuning dataset, MathCodeInstruct. This dataset comprises existing math problems from GSM8K and MATH, with GPT-4 Code Interpreter-style solutions, and newly formulated ones via our novel problem interpolation prompting strategy.</li>
<li>We have produced a family of models, MathCoder. We fine-tune Llama-2 and CodeLlama models on our dataset, producing a family of models with not only high accuracy on the GSM8K and MATH, but also a good performance on other out-of-domain datasets like Mathematics and SimulEq.</li>
</ul>
<h1>2 MathCoder: Specializing Llama for Mathematical Reasoning</h1>
<p>In this section, we first introduce the methodology on creating MathCodeInstruct in Sec. 2.1. Subsequently, we detail the supervised fine-tuning (SFT) and inference methods in Sec. 2.2.</p>
<h3>2.1 MATHCODEINSTRUCT DATASET</h3>
<p>Our MathCodeInstruct dataset can be expressed as $D=\left{D_{0}, D_{1}\right}$, where $D_{0}$ denotes the seed data and $D_{1}$ is the data generated with the proposed prompting method, named problem interpolation prompting. Fig. 2 (a) outlines the process for creating MathCodeInstruct Dataset.
Seed data $D_{0}$. First, we obtain solutions for the GSM8K and MATH training sets from the GPT-4. The data can be expressed in (solution, question) pairs as $\left{\left(y_{i}, x_{i}\right)\right}<em i="i">{i=1}^{N}$. Each solution $y</em>$ contains</p>
<p>Input for MathCoder
$&lt;$ |system|&gt;&lt;|text|&gt;Below is a math problem. Please solve it step by step.&lt;|endofblock|&gt;&lt;|endofmessage|&gt;
&lt;|user|&gt;&lt;|text|&gt;A group of friends went on a road trip across two cities. In the first city, they spent half of the money they had plus an additional $\$ 50$. In the second city, they spent half of what was left plus an additional $\$ 20$, leaving them with $\$ 40$. How much money did they start with before the road trip?&lt;|endofblock|&gt;&lt;|endofmessage|&gt;</p>
<h2>(a) CoT example</h2>
<p>Let's call the amount of money they started with $x$.</p>
<ol>
<li>In the first city, they spent half ( $\frac{x}{2}$ ) plus $\$ 50$. What remains is $x-\left(\frac{x}{2}+50\right)$.</li>
<li>Simplifying this gives $x-\frac{x}{2}-50=\frac{x}{2}-50$.</li>
<li>In the second city, they spent half of what remained ( $\frac{x}{2}-50$ ) plus $\$ 20$. What remains is $\frac{x}{2}-50-\left(\frac{\frac{x}{2}-50}{2}+20\right)$.</li>
<li>Simplifying this gives $\frac{x}{2}-50-\frac{x}{4}+25-20=\frac{x}{4}-45$.</li>
<li>They are left with $\$ 40$, so $\frac{x}{4}-45=40$.</li>
<li>Solving for $x$ gives $\frac{x}{4}=85$ and $x=340$.</li>
</ol>
<h2>(b) PoT example</h2>
<p># Define a variable to represent the starting amount of money starting_amount $=0$
# Loop to find the starting amount
while True:
amount $=$ starting_amount
# Spent in the first city: half of the money they had plus $\$ 50$ amount $=$ (amount / $2+50$ )
# Spent in the second city: half of what was left plus $\$ 20$ amount $=$ (amount / $2+20$ )
# Check if they are left with $\$ 40$
if amount $==40$ : break
starting_amount $+=1$
starting_amount $&gt;&gt;&gt;340$
(c) LCE solution from MathCoder
&lt;|assistant|&gt; &lt;|text|&gt; To solve this problem, let's set $x$ as the amount of money the group of friends started with.</p>
<ol>
<li>In the first city, they spent half of what they had ( $\frac{x}{2}$ ) plus an additional $\$ 50$.</li>
<li>After this, they are left with $x-\left(\frac{x}{2}+50\right)$.</li>
<li>In the second city, they spend half of what's left plus an additional $\$ 20$.</li>
<li>Finally, they are left with $\$ 40$.</li>
</ol>
<p>The equation can be set up as follows:</p>
<p>$$
x-\left(\frac{x}{2}+50\right)-\left(\frac{x-\left(\frac{x}{2}+50\right)}{2}+20\right)=40
$$</p>
<p>Let's go ahead and solve this equation using code.&lt;|endofblock|&gt;
&lt;|code|&gt; from sympy import symbols, Eq, solve
# Define the variable and equation
$\mathrm{x}=$ symbols('x')
# Equation based on the problem description
equation $=$ Eq(x - (x/2 + 50) - ((x - (x/2 + 50)) / 2 + 20), 40)
# Solve the equation
initial_amount $=$ solve(equation, x )
initial_amount[0]&lt;|endofblock|&gt;
$&lt;$ |execution|&gt; 340&lt;|endofblock|&gt;
&lt;|text|&gt; Thus, the group of friends started with $\$ 340$ before the road trip.&lt;|endofblock|&gt;&lt;|endofmessage|&gt;</p>
<p>Figure 3: Example of CoT (Wei et al., 2022), PoT (Gao et al., 2023; Chen et al., 2022) and LCE solution with special token. In contrast to CoT, which consists solely of natural language, and PoT, which consists solely of code, our LCE solution intertwines natural language, code, and execution results. &lt;|text|&gt;, &lt;|code|&gt;, and $&lt;$ |execution|&gt; are special tokens that denote natural language, code, and execution results respectively.
three kinds of components: natural language (text) for reasoning $\mathbf{L}$, code for execution $\mathbf{C}$, and execution results $\mathbf{E}$, where $\mathbf{L}$ is the natural language reasoning step, $\mathbf{C}$ is the Python code the model generates when its reasoning leads to some complex computation that it needs code to solve, and $\mathbf{E}$ is the output of the code. $\mathbf{E}$ is assessed by the model so a new $\mathbf{L}$ can be generated. All three kinds of components are closely chained together in the solutions, with each component influencing the component that comes after. An integral solution $y_{i}$ can be expressed as ( $\mathbf{L}, \mathbf{C}, \mathbf{E}, \mathbf{L}, \mathbf{C}, \mathbf{E}, \ldots)$. An example is shown in Fig. 3 (c). We call solutions in this format Natural Language, Code, and Execution (LCE) solutions. We put some case studies in Appendix E to demonstrate the advantage of LCE.</p>
<p>We filter the seed data $D_{0}=\left(\left{\left(y_{i}, x_{i}\right)\right}\right)$, making sure that each solution $y_{i}$ provides the same answer as the ground truth answer so that the quality of the dataset is further assured. Then, we finetune the CodeLlama-34B using the seed data $D_{0}$, producing our initial MathCoder model, named MathCoder-Initial.</p>
<p>Problem interpolation prompting $D_{1}$. Using the initial MathCoder model, we can generate LCE solutions for new problems. We observed a large gap in difficulty between grade-school-level GSM8K problems and challenging competition MATH problems. To bridge this gap, we present a novel prompting method (see details in Appendix A), which provides a powerful LLM like GPT-4 with a relatively simple problem drawn from the GSM8K training set, paired with a difficult problem drawn from the MATH, and ask the model to generate a new problem with difficulty between the two. We then use GPT-4 to evaluate the new problems, and the results are shown in Fig. 4. We</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Difficulty comparison of interpolation problems against MATH and GSM8K using GPT-4. The evaluation prompt and examples are shown in Appendix B.
can see that $83.2 \%$ of the new problems are more difficult than GSM8K, and $95.6 \%$ are easier than MATH, indicating that the problems generated in this way are appropriate in difficulty.
We also investigated using only GSM8K to create difficult problems, but we found that the new problems were too similar to the original ones, and the large gap to MATH still exists (more information can be found in Appendix C).</p>
<p>Self-distillation. Given that we do not have ground truth answers for the new problems, we then generate $n$ different LCE solutions as depicted in (Wang et al., 2023a) for each new problem with our initial MathCoder models, keeping only those solutions for which all $n$ answers match ( $n$ is set to 3 in this paper), thus ensuring our dataset's quality. We use MathCoder-Initial here because it demonstrates the potential for effective model distillation using a model much weaker than the powerful closed-source models. As MathCoder-Initial already has an accuracy of $77.3 \%$ on GSM8K and $44.0 \%$ on MATH, it is plausible that distilling it can produce good results. It also reduces the cost compared to using GPT-4. Some examples can be found in Appendix A.</p>
<p>Combining the new data $D_{1}$ with the seed data $D_{0}$ yields the MathCodeInstruct dataset $D=$ $\left{D_{0}, D_{1}\right}$. We fine-tune the base Llama-2 (Touvron et al., 2023) and CodeLlama (Rozière et al., 2023) models using MathCodeInstruct to derive our final MathCoder models. For clarity, we refer to the supervised fine-tuning of base Llama-2 as "MathCoder-L" and that of CodeLlama as "MathCoder-CL", as shown in Fig. 2 (b).</p>
<h1>2.2 Supervised Fine-tuning and Inference</h1>
<p>Supervised Fine-tuning. In order to identify the three kinds of components in LCE solutions, as illustrated in Fig. 3 (c), we enclose them with special tokens. Reasoning language starts with $&lt;$ |text|&gt;, while math code and execution results start with &lt;|code|&gt; and &lt;|execution|&gt; respectively. All components end with &lt;|endofblock|&gt;. These tokens help the model understand the difference between each component and create LCE solutions during inference. After the special tokens are added, all components are concatenated to form the solution, which is preceded by the original math question to form an instance of training data. In order to make the training more efficient, several instances are concatenated together to form a single input, while cross-question masking is used to ensure only tokens in the same instance are visible.</p>
<p>During supervised fine-tuning, we apply a standard cross-entropy loss following Alpaca (Taori et al., 2023). The loss is only computed on reasoning language and math code since they are the components of the training data generated by the LLM. In particular, we zero-out the loss on tokens from execution results, as the model would not need to predict these tokens.</p>
<p>Inference. After supervised fine-tuning, the model has learned to output natural language and code enclosed by special tokens. We can identify the end of each component by looking for $&lt;$|endofblock|&gt;, and determine which component it is by examining the first token of the component. When a code generation is encountered, we utilize a Jupyter Notebook for real-time code execution, allowing the variables defined in previous code blocks to be used in subsequent ones. After execution, the execution results are concatenated following the previous math code block. The model then continues to autoregressively generate the next reasoning language block, forming the chain of thoughts in the LCE format, until it reaches the final answer. This process ensures that the model behaves similarly to the GPT-4 Code Interpreter.</p>
<h1>3 EXPERIMENTS</h1>
<h3>3.1 DATASETS AND IMPLEMENTATION DETAILS</h3>
<p>Datasets. We evaluate the MathCoder on five datasets, including two in-domain datasets: GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021); and three out-of-domain datasets: SVAMP (Patel et al., 2021), Mathematics (Saxton et al., 2019), and SimulEq (Kushman et al., 2014). We regard GSM8K and MATH as in-domain because their training sets are used for our supervised fine-tuning, while SVAMP, Mathematics, and SimulEq are out-of-domain because their training sets are not used in our fine-tuning. The extensive assortment of assessment datasets encompasses mathematical challenges from elementary, high school, and collegiate levels, covering various subjects like geometry, formal logic, and even commonsense reasoning. The selection of these datasets aims at providing a thorough evaluation of the models' ability to generalize to unknown circumstances and diverse fields of mathematics.</p>
<p>Implementation Details. Different base LLMs of varying sizes are tested, including Llama2 (7B, 13B, and 70B) and CodeLlama (7B, 13B, and 34B). During training, we use a uniform learning rate of $2 \times 10^{-5}$ and a context length of 2048 , and we set the batch size as 128 with different ratios of gradient accumulation steps and per-device train batch size, considering the model size. Additionally, we used a cosine scheduler for three epochs in total with a 50 -step warmup period. To efficiently train the computationally intensive models, we simultaneously employ DeepSpeed training with ZeRO-3 stage (Rajbhandari et al., 2020) and flash attention (Dao et al., 2022). The 7B, 13B, and 34B/70B models are trained on 8,16 , and 32 NVIDIA A800 80GB GPUs, respectively. The text-generation-inference framework of Hugging Face is used for inference with greedy decoding and max new tokens of every block set 512, and one to four GPUs are used as needed. We allow up to 32 LCE blocks in every solution.</p>
<p>Baselines. We compare the proposed MathCoders with the following competitive baselines. Closed-Source Models: we consider three closed-source models, including ChatGPT-3.5 Brown et al. (2020), GPT-4 (OpenAI, 2023), GPT-4 Code Interpreter (Zhou et al., 2023a), and PaLM2 (Anil et al., 2023). Open-Source Models: we compare with Llama-2 (Touvron et al., 2023), WizardMath (Luo et al., 2023), Llama-1 RFT (Yuan et al., 2023), and Galactica (Taylor et al., 2022).</p>
<p>For baselines, CoT prompting (Wei et al., 2022) and few-shot in-context-learning (Dong et al., 2023) are used to maximize their performance while our MathCoders are always evaluated without extra prompting and under zero-shot setting (Kojima et al., 2023).</p>
<h3>3.2 MAIN RESULTS</h3>
<p>Comparison between MathCoder and SOTA open-source models. The experiment results in Tab. 2 show that our method outperforms other open-source competitive math-solving models with a clear advantage, achieving state-of-the-art results across all datasets. However, a substantial performance gap still exists compared to the state-of-the-art closed-source method GPT-4 Code Interpreter. Our observations are as follows: (1) MathCoder-L-7B outperforms WizardMath-70B. Even the smallest version of MathCoder, MathCoder-L-7B, outperforms the largest WizardMath model, WizardMath-70B, on three out of five datasets, achieving a significant gain ( $+4.5 \%$ ) in the average score, as shown in Tab. 2. This is likely attributed to the fact that WizardMath is trained solely on CoT data, while MathCoder is trained on our proposed LCE solutions. This demonstrates the advantage of using solutions that interleave natural language, code, and execution (LCE blocks), significantly enhancing the model's ability to perform complex computations. (2) Additionally, it is worth noting that while the code ability of CodeLlama-34B significantly outperforms that of Llama-2-70B, in the case of MathCoder models, we observed that models based on Llama-2-70B ( $73.1 \%$ ) can outperform CodeLlama-34B ( $70.2 \%$ ). This contrasts with the findings in the concurrent work, MAmmoTH (Yue et al., 2023). The main reason for this disparity might be that Llama-270B exhibits better natural language reasoning ability, and the MathCodeInstruct dataset can enhance language models' code generation ability for math problem-solving.</p>
<p>Comparison between Llama-2 and CodeLlama. Tab. 3 shows that MathCoder-CL with CodeLlama as the base model brings a substantial improvement compared to MathCoder-L with Llama-2 as the base model. MathCode-CL-7B and MathCoder-CL-13B demonstrate an accuracy improvement of $4.1 \%$ and $3.0 \%$ respectively, compared to the corresponding MathCoder-L models</p>
<p>Table 2: Model evaluation on in-domain (GSM8K \&amp; MATH) and out-of-domain datasets (SVAMP, Mathematics \&amp; SimulEq). +incidates improvement w.r.t. the best open source model. SVA. stands for SVAMP, Mat. stands for Mathematics, and Sim. stands for SimulEq.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Base</th>
<th>Size</th>
<th>In-Domain</th>
<th></th>
<th>Out-of-Domain</th>
<th></th>
<th></th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>GSM8K</td>
<td>MATH</td>
<td>SVA.</td>
<td>Mat.</td>
<td>Sim.</td>
<td></td>
</tr>
<tr>
<td>Colsed-Source Model</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ChatGPT-3.5 (Zhao et al., 2023)</td>
<td>-</td>
<td>-</td>
<td>80.8</td>
<td>34.1</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>GPT-4 (OpenAI, 2023)</td>
<td>-</td>
<td>-</td>
<td>92.0</td>
<td>42.5</td>
<td>97.0</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>GPT-4 Code (Zhou et al., 2023a)</td>
<td>-</td>
<td>-</td>
<td>97.0</td>
<td>69.7</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>PaLM-2 (Anil et al., 2023)</td>
<td>-</td>
<td>-</td>
<td>80.7</td>
<td>34.3</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Open-Source Model</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Galactica (Taylor et al., 2022)</td>
<td>-</td>
<td>$\begin{aligned} &amp; 6.7 \mathrm{~B} \ &amp; 30 \mathrm{~B} \end{aligned}$</td>
<td>$\begin{aligned} &amp; 10.2 \ &amp; 41.7 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 2.2 \ &amp; 12.7 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 25.6 \ &amp; 41.6 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 4.6 \ &amp; 11.8 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 4.2 \ &amp; 13.2 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 9.4 \ &amp; 24.2 \end{aligned}$</td>
</tr>
<tr>
<td>Llama-1 RFT (Yuan et al., 2023)</td>
<td>Llama-1</td>
<td>$\begin{gathered} 7 \mathrm{~B} \ 13 \mathrm{~B} \ 34 \mathrm{~B} \end{gathered}$</td>
<td>$\begin{aligned} &amp; 46.5 \ &amp; 52.1 \ &amp; 56.5 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 5.2 \ &amp; 5.1 \ &amp; 7.4 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 21.1 \ &amp; 46.5 \ &amp; 55.4 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 5.1 \ &amp; 6.7 \ &amp; 7.6 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 11.0 \ &amp; 10.1 \ &amp; 12.8 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 17.8 \ &amp; 24.1 \ &amp; 27.9 \end{aligned}$</td>
</tr>
<tr>
<td>WizardMath (Luo et al., 2023)</td>
<td>Llama-2</td>
<td>$\begin{gathered} 7 \mathrm{~B} \ 13 \mathrm{~B} \ 70 \mathrm{~B} \end{gathered}$</td>
<td>$\begin{aligned} &amp; 54.9 \ &amp; 63.9 \ &amp; 81.6 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 10.7 \ &amp; 14.0 \ &amp; 22.7 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 36.1 \ &amp; 51.9 \ &amp; 71.8 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 9.3 \ &amp; 14.1 \ &amp; 17.1 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 12.8 \ &amp; 14.9 \ &amp; 37.9 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 24.8 \ &amp; 31.8 \ &amp; 46.2 \end{aligned}$</td>
</tr>
<tr>
<td>MathCoder-L</td>
<td>Llama-2</td>
<td>$\begin{gathered} 7 \mathrm{~B} \ 13 \mathrm{~B} \ 70 \mathrm{~B} \end{gathered}$</td>
<td>$\begin{aligned} &amp; 64.2 \ &amp; +9.3 \ &amp; 72.6 \ &amp; +8.7 \ &amp; 83.9 \ &amp; +2.3 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 23.3 \ &amp; +12.6 \ &amp; 29.9 \ &amp; +15.9 \ &amp; 45.1 \ &amp; +22.4 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 71.5 \ &amp; +35.4 \ &amp; 76.9 \ &amp; +25.0 \ &amp; 84.9 \ &amp; +13.1 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 46.9 \ &amp; +37.6 \ &amp; 54.7 \ &amp; +40.6 \ &amp; 74.4 \ &amp; +57.3 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 47.5 \ &amp; +34.7 \ &amp; 62.3 \ &amp; +27.4 \ &amp; 77.0 \ &amp; +26.9 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 50.7 \ &amp; +25.9 \ &amp; 59.2 \ &amp; +27.4 \ &amp; 73.1 \ &amp; +26.9 \end{aligned}$</td>
</tr>
<tr>
<td>MathCoder-CL</td>
<td>CodeLlama</td>
<td>$\begin{gathered} 7 \mathrm{~B} \ 13 \mathrm{~B} \ 34 \mathrm{~B} \end{gathered}$</td>
<td>$\begin{aligned} &amp; 67.8 \ &amp; +12.9 \ &amp; 74.1 \ &amp; +10.2 \ &amp; 81.7 \ &amp; +0.1 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 30.2 \ &amp; +19.5 \ &amp; 35.9 \ &amp; +21.9 \ &amp; 45.2 \ &amp; +22.5 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 70.7 \ &amp; +34.6 \ &amp; 78.0 \ &amp; +26.1 \ &amp; 82.5 \ &amp; +10.7 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 55.8 \ &amp; +46.5 \ &amp; 62.5 \ &amp; +48.4 \ &amp; 75.9 \ &amp; +58.8 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 49.6 \ &amp; +36.8 \ &amp; 60.7 \ &amp; +30.4 \ &amp; 65.8 \ &amp; +27.9 \end{aligned}$</td>
<td>$\begin{aligned} &amp; 54.8 \ &amp; +30.0 \ &amp; 62.2 \ &amp; +30.4 \ &amp; 70.2 \ &amp; +24.0 \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Model performance comparison for MathCoders with CodeLlama and Llama-2 as base.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Size</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">Mathematics</th>
<th style="text-align: center;">SimulEq</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MathCoder-CL-7B vs. MathCoder-L-7B</td>
<td style="text-align: center;">+3.6</td>
<td style="text-align: center;">+6.9</td>
<td style="text-align: center;">-0.8</td>
<td style="text-align: center;">+8.9</td>
<td style="text-align: center;">+2.1</td>
<td style="text-align: center;">+4.1</td>
</tr>
<tr>
<td style="text-align: left;">MathCoder-CL-13B vs. MathCoder-L-13B</td>
<td style="text-align: center;">+1.5</td>
<td style="text-align: center;">+6.0</td>
<td style="text-align: center;">+1.1</td>
<td style="text-align: center;">+7.8</td>
<td style="text-align: center;">-1.6</td>
<td style="text-align: center;">+3.0</td>
</tr>
</tbody>
</table>
<p>of the same size. The potentially superior coding and reasoning capability of CodeLlama can be attributed to its additional training on code data (Rozière et al., 2023). This extended training provides CodeLlama with a deeper understanding of programming concepts and patterns, allowing it to excel in coding-related tasks and exhibit more advanced math reasoning abilities.</p>
<p>Comparison among different subjects across various levels. MATH dataset problems are categorized with difficulty levels ranging from 1 to 5 , covering seven different math subjects, including algebra, prealgebra, number theory, counting and probability, precalculus, intermediate algebra, and geometry. In Fig. 5, we present the performance comparison of MathCoder-L (7B, 13B) and MathCoder-CL (7B, 13B), grouped by these levels and subjects. More results are shown in Appendix D. We find that MathCoder achieves higher scores in algebra and prealgebra problems. However, when it comes to geometry problems, MathCoder struggles to achieve high scores, especially for problems with higher difficulty levels. This suggests that code plays a more significant role in computationally intensive questions.</p>
<h1>3.3 Ablation Study</h1>
<p>Analysis of the influence of problem interpolation. We conducted an experiment to study the influence of the portion of MathCodeInstruct questions created using the proposed problem</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Performance comparison of MathCoder-L (7B, 13B) and MathCoder-CL (7B, 13B) on the MATH dataset by levels and subjects. We can see that the improved accuracy from MathCoder-L to MathCoder-CL comes primarily from subjects that require precise calculations like algebra and number theory.</p>
<p>Table 4: Influence of the interpolation problems in MathCodeInstruct (as shown in Tab. 1) based on CodeLlama-34B.</p>
<table>
<thead>
<tr>
<th>Train set</th>
<th>Samples</th>
<th>GSM8K</th>
<th>MATH</th>
<th>SVAMP</th>
<th>Mathematics</th>
<th>SimulEq</th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr>
<td>GSM8K+MATH</td>
<td>49k</td>
<td>77.3</td>
<td>44.0</td>
<td>78.6</td>
<td>71.6</td>
<td>59.3</td>
<td>66.2</td>
</tr>
<tr>
<td>GSM8K+MATH+Interpolation</td>
<td>80k</td>
<td>81.7</td>
<td>45.2</td>
<td>82.5</td>
<td>75.9</td>
<td>65.8</td>
<td>70.2</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$+4.4$</td>
<td>$+1.2$</td>
<td>$+3.9$</td>
<td>$+4.3$</td>
<td>$+6.4$</td>
<td>$+4.0$</td>
</tr>
</tbody>
</table>
<p>interpolation. The experiment uses CodeLlama-34B as the base model. The experimental results in Tab. 4 validate that problem interpolation brings a significant improvement across all five datasets. The results indicate that by employing problem interpolation, we can generate problems with intermediate difficulty levels, thereby increasing the diversity of the problem set. This expands the diversity of the problems and ultimately enhances the performance of the model.</p>
<p>Analysis of actual code execution in the inference stage. We investigate the impact of code execution in the inference stage and report the results in Tab. 5. We conduct this investigation using CodeLlama-34B as the base model and train the models on our 80k MathCodeInstruct dataset. Tab. 5 (#1) and Tab. 5 (#2) use the same model, trained with the cross-entropy loss computed on not only natural language and code, but also the execution results. In this way, this model learns to predict the execution results. In Tab. 5 (#1), the code execution results are predicted by the model itself, while in Tab. 5 (#2), the execution result is returned from a Python code interpreter. From the comparison between Tab. 5 (#1) and Tab. 5 (#2), we can see that Tab. 5 (#1) outperforms Tab. 5 (#2) across all five datasets, showing an improvement of $34.0 \%$ in the average accuracy score. This indicates that actual code execution in the inference stage has a significant impact on the model's performance. This study shows that the model failed to predict correct execution results for many programs and that actually executing the code using an external tool can significantly improve the accuracy while doing complex computations. This finding validates the significance of integrating code execution when solving math problems with LLMs, in line with previous closed-source GPT-4 Code interpreter (Zhou et al., 2023a).</p>
<p>Analysis of execution results in the training stage. Based on the observation that actual code execution contributes a lot to the model's performance, we investigate not forcing the model to predict the correct execution result. Tab. 5 (#3) is the performance of MathCoder-CL-34B, which ignores execution results when computing the loss, so that the model does not learn to estimate the execution results and the learning task at the supervised fine-tuning stage becomes simpler. Compared to Tab. 5 (#2), Tab. 5 (#3) improves the accuracy across four out of five datasets, resulting in a rise in the average accuracy from $69.1 \%$ to $70.2 \%$, which aligns with the hypothesis that by computing the loss only on natural language and code, the model can focus more on the math problem-solving skills itself, thus making the supervised fine-tuning more effective.</p>
<p>Table 5: Ablation study of with/without code execution during inference and of the loss with/without execution results in training stage.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Experiment</th>
<th style="text-align: center;">Include <br> execution results <br> for training</th>
<th style="text-align: center;">Actual <br> code execution <br> in inference</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">Mathematics</th>
<th style="text-align: center;">Simuleq</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">#1</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">35.1</td>
</tr>
<tr>
<td style="text-align: center;">#2</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">69.1</td>
</tr>
<tr>
<td style="text-align: center;">#3</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">$+25.8$</td>
<td style="text-align: center;">$+29.0$</td>
<td style="text-align: center;">$+12.3$</td>
<td style="text-align: center;">$+53.6$</td>
<td style="text-align: center;">$+49.4$</td>
<td style="text-align: center;">$+34.0$</td>
</tr>
<tr>
<td style="text-align: center;">#4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">70.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+1.8$</td>
<td style="text-align: center;">$-0.7$</td>
<td style="text-align: center;">$+0.6$</td>
<td style="text-align: center;">$+1.7$</td>
<td style="text-align: center;">$+2.1$</td>
<td style="text-align: center;">$+1.1$</td>
</tr>
</tbody>
</table>
<h1>4 Related Work</h1>
<p>Instruction Tuning. Instruction tuning is a method of enhancing LLMs' instruction following abilities, thus aligning language models with more useful objectives and human preferences. A long line of previous works (Ye et al., 2021; Longpre et al., 2023; Sanh et al., 2021; Wang et al., 2022b; Wei et al., 2021; Chung et al., 2022; Longpre et al., 2023) is focused on enhancing LLMs' instruction following abilities in general. With the emergence of models like GPT-3 and GPT-4, recent studies (Wang et al., 2022a; 2023b; Zhou et al., 2023b; Peng et al., 2023; Xu et al., 2023) have started to utilize synthetic instructions generated by these powerful models to tune smaller models. Compared to these works, our instruction tuning is focused on using high-quality solutions for math problems generated by models to improve our LLM's math-solving ability. Another related work is presented in (Luo et al., 2023), but their method did not use code to solve math problems, distinguishing our work from theirs.
Mathematical Reasoning. There are various benchmark datasets (Hendrycks et al., 2020; Ling et al., 2017; Hendrycks et al., 2021) to measure a model's mathematical reasoning abilities. Recently, many works have focused on enhancing LLMs' ability to solve math problems, reaching high scores on these benchmarks. Many of them apply Chain-of-Thought (Wei et al., 2022; Kojima et al., 2023; Wang et al., 2023a; Fu et al., 2022) to improve LLMs' multistep reasoning capability. Another line of works (Gao et al., 2023; Chen et al., 2022; Zhou et al., 2023a) utilize code to compensate for LLMs' limitations in doing complex math computations. Our work takes inspiration from these two lines of work, as we believe both Chain-of-Thought and code generation (Li et al., 2023a; Rozière et al., 2023) are essential to solving math problems. There are also works focused on math-related pre-training (Lewkowycz et al., 2022; Taylor et al., 2022) to improve a model's general reasoning capability. We combine natural language and code seamlessly in our dataset, thus providing a method to train models more efficiently in solving math problems.
Distillation. Distillation (Hinton et al., 2015) often involves transferring knowledge from a larger, more powerful model to a smaller, weaker one (Taori et al., 2023; Zheng et al., 2023; Cobbe et al., 2021). Recent research (Li et al., 2023b; Wang et al., 2022a; Allen-Zhu \&amp; Li, 2020) has demonstrated the plausibility of self-distillation, achieving performance improvements by distilling the model itself. Our approach can also be viewed as a form of self-distillation, as the solutions generated by MathCoder-Initial, which is built on CodeLlama-34B, are used to fine-tune CodeLlama-34B, resulting in MathCoder-CL-34B.</p>
<h2>5 CONCLUSION AND LIMITATION</h2>
<p>In this paper, we present MathCoder, an open-source large language model designed for math reasoning, bridging the gap between natural language understanding and computational problemsolving. MathCoder incorporates math instruction-following dataset construction. By utilizing the GSM8K and MATH datasets as seed data, we leverage the GPT-4 to generate problems encompassing reasoning, code generation, and program execution. Additionally, we propose a problem interpretation method to create intermediate-level problems. Furthermore, we introduce a customized supervised fine-tuning approach, where the training loss is only applied to natural language and code. Our empirical study demonstrates that MathCoder achieves state-of-theart performance in five math datasets among open-source LLMs, with scores of $83.9 \%$ on the GSM8K dataset and $45.2 \%$ on the MATH dataset. It is worth noting that MathCoder outperforms</p>
<p>closed-source models like ChatGPT-3.5 and PaLM-2 on the GSM8K and MATH datasets and even outperforms GPT-4 on the MATH dataset.</p>
<p>However, our work does have certain limitations that warrant further exploration in future research. First, since we rely on the GPT-4 for data generation, MathCoder's capabilities are inherently constrained by the capabilities of this model and unable to solve theorem-proving problems. Additionally, as a series of uni-modal models, MathCoder still faces challenges in solving complex geometry problems, which we acknowledge and plan to address in our future investigations.</p>
<h1>REFERENCES</h1>
<p>Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020.</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022.</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. A survey on in-context learning, 2023.</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720, 2022.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 10764-10799. PMLR, 2023.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring massive multitask language understanding. ArXiv, abs/2009.03300, 2020. URL https://api.semanticscholar.org/CorpusID: 221516475 .</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. ArXiv, abs/1503.02531, 2015. URL https://api.semanticscholar.org/ CorpusID:7200347.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners, 2023.</p>
<p>Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. Learning to automatically solve algebra word problems. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 271-281, 2014.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843-3857, 2022.</p>
<p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you!, 2023a.</p>
<p>Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. Self-alignment with instruction backtranslation. ArXiv, abs/2308.06259, 2023b. URL https://api.semanticscholar.org/CorpusID:260866107.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 158-167, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1015. URL https://aclanthology.org/P17-1015.</p>
<p>Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023.</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191, 2021.</p>
<p>Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.</p>
<p>Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.</p>
<p>Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models, 2020.</p>
<p>Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2023.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. arXiv preprint arXiv:1904.01557, 2019.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023a. URL https://openreview.net/forum?id=1PL1NIMdrw.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022a.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. arXiv preprint arXiv:2204.07705, 2022b.</p>
<p>Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023b.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview. net/forum?id=_VjQlMeSB_J.</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.</p>
<p>Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. Crossfit: A few-shot learning challenge for cross-task generalization in nlp. arXiv preprint arXiv:2104.08835, 2021.</p>
<p>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning, 2023.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.</p>
<p>Xu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, and Qizhe Xie. Automatic model selection with large language models for reasoning, 2023.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.</p>
<p>Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. arXiv preprint arXiv:2308.07921, 2023a.</p>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023b.</p>
<h1>APPENDIX</h1>
<h2>A DATASET EXAMPLES</h2>
<p>In this part, we include two examples that show the process of creating MathCodeInstruct. Fig. 6 shows an example with only one LCE block, while Fig. 7 shows an example with three LCE blocks.</p>
<h2>B EXAMPLES OF DIFFICULTY COMPARISON</h2>
<p>We show five examples of using GPT-4 to evaluate the complexity of problems in MathCodeInstruct. Fig. 8 and Fig. 9 are two examples that the newly generated interpolation problems are more difficult than the origin GSM8K problems, and Fig. 10 is an example that the origin MATH problem is more difficult than the newly generated interpolation problem. These two situations are the most common ( $83.2 \%$ and $95.6 \%$ ).</p>
<p>Fig. 11 shows an example that the newly generated interpolation problem ties with the origin GSM8K problem, which situation accounts for $15.3 \%$ of all problems.</p>
<p>Fig. 12 shows an uncommon example that the origin GSM8K problem is slightly more difficult than the newly generated interpolation problem according to GPT-4, which situation accounts for less than $3 \%$ of all problems.</p>
<h2>C Creating Problems Using ONLY GSM8K</h2>
<p>Fig. 13, Fig. 14, Fig. 15, Fig. 16 and Fig. 17 are five examples that utilize problems from the train set of GSM8K to generate new problems which are more difficult than the origin ones. Compared with the problems generated by our interpolation method, we can see that the new problems generated in this way are much more similar to the raw GSM8K problems, sometimes just changing the name of some variables or scaling the value. These problems are only slightly more complicated than the raw problems, if not equally difficult, and are still much simpler than those from the MATH dataset.</p>
<p>In contrast to using just GSM8K, introducing problems from the MATH dataset in the interpolation method shows the model (GPT-4 here) a route to generate more challenging problems. Hence, the newly generated problems are similar to the problems in the GSM8K and the problems in the MATH. Consequently, these interpolation problems can narrow the difficulty gap between the two datasets.</p>
<h2>D MORE EXPERIMENT RESULTS</h2>
<p>We show the performance comparison of all MathCoders, MathCoder-L (7B, 13B, 70B) and MathCoder-CL (7B, 13B, 34B), on the MATH dataset by levels and subjects in Fig. 18. We can see that the improved accuracy from MathCoder-L to MathCoder-CL comes primarily from subjects requiring precise calculations like algebra, number theory, and counting and probability.</p>
<h2>E CASE Study With CoT, PoT and LCE</h2>
<p>We compare our LCE solutions with CoT solutions and PoT solutions. Fig. 19 is an example of a problem in number theory, and Fig. 20 is an example of a problem in algebra. CoT and PoT failed to solve the problem in both cases, but LCE succeeded.</p>
<p>Fig. 21, Fig. 22, and Fig. 23 are three solutions to one problem in geometry. The CoT solutions successfully figured out the coordinates of $\mathrm{D}, \mathrm{C}$, and E but failed to calculate the area, while the PoT could not interpret the conditions in the problem. Compared with them, we can see that our LCE solutions not only can utilize the conditions in the problems correctly but also will not make errors in calculation.</p>
<h1>(a) Interpolation Prompt:</h1>
<p>Please create a new problem, following the given example, "Example 1" is an easy problem and "Example 2" is much more difficulty than "Example 1", the new problem should be harder than "Example 1" and simpler than "Example 2".</p>
<p>Example 1: Marissa is hiking a 12-mile trail. She took 1 hour to walk the first 4 miles, then another hour to walk the next two miles. If she wants her average speed to be 4 miles per hour, what speed (in miles per hour) does she need to walk the remaining distance?</p>
<p>Example 2: The two solutions of equation $x^{2}+b x+48=0$ are in the ratio of 3 to 1 for some values of $b$. What is the largest possible value of $b$ ?
(b) According new problem generated by GPT-4:</p>
<p>A group of friends went on a road trip across three cities. In the first city, they spent half of the money they had plus an additional $\$ 50$. In the second city, they spent half of the remaining money plus an additional $\$ 30$. In the third city, they spent half of what was left plus an additional $\$ 20$, leaving them with $\$ 40$. How much money did they start with before the road trip?</p>
<h2>(c) Solution generated by MathCoder-Initial without extra prompt:</h2>
<p>Let's break this problem down step by step:</p>
<ol>
<li>Let $x$ be the amount of money they started with.</li>
<li>After visiting the first city, they spent $\frac{x}{2}+50$. So, the remaining amount is $x-\left(\frac{x}{2}+50\right)$.</li>
<li>In the second city, they spent half of the remaining money plus $\$ 30$. This means they spent $\frac{x-\left(\frac{x}{2}+50\right)}{2}+30$. After this expenditure, they would have $x-\left(\frac{x}{2}+50\right)-\left(\frac{x-\left(\frac{x}{2}+50\right)}{2}+30\right)$ left.</li>
<li>In the third city, they spent half of what was left from the above amount plus $\$ 20$. After this, they were left with $\$ 40$.
Using the above information, we can set up an equation to solve for $x$ :
$x-\left(\frac{x}{2}+50\right)-\left(\frac{x-\left(\frac{x}{2}+50\right)}{2}+30\right)-\left(\frac{x-\left(\frac{x}{2}+50\right)-\left(\frac{x-\left(\frac{x}{2}+50\right)}{2}+30\right)}{2}+20\right)=40$
Let's solve this equation to find the value of $x$.</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sympy</span><span class="w"> </span><span class="kn">import</span> <span class="n">symbols</span><span class="p">,</span> <span class="n">Eq</span><span class="p">,</span> <span class="n">solve</span>
<span class="c1"># Define the variable</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">symbols</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="c1"># Define the equation based on the problem statement</span>
<span class="n">equation</span> <span class="o">=</span> <span class="n">Eq</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">50</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">50</span><span class="p">))</span><span class="o">/</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">30</span> <span class="o">-</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">50</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">50</span><span class="p">))</span><span class="o">/</span><span class="mi">2</span>
    <span class="o">-</span> <span class="mi">30</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>
<span class="c1"># Solve the equation</span>
<span class="n">initial_money</span> <span class="o">=</span> <span class="n">solve</span><span class="p">(</span><span class="n">equation</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span><span class="mi">700</span>
</code></pre></div>

<p>The group of friends started with $\$ 700$ before the road trip.</p>
<p>Figure 6: An example of the process of creating MathCodeInstruct. Firstly, "Example 1" and "Example 2" are randomly chosen from the train set of GSM8K and MATH respectively. Then a new problem is generated by GPT-4 using the interpolation prompt. Finally, we use our initial MathCoder to generate LCE-style solution for the new problem.</p>
<h1>(a) Interpolation Prompt:</h1>
<p>Please create a new problem, following the given example, "Example 1" is an easy problem and "Example 2" is much more difficulty than "Example 1", the new problem should be harder than "Example 1" and simpler than "Example 2".</p>
<p>Example 1: A bus comes by Jerry's bus stop every 20 minutes starting at exactly 5:13 a.m. If Jerry shows up at exactly 8:35 a.m., how many minutes will he have to wait for the next bus?</p>
<p>Example 2: Bella's grandfather is confined to a wheelchair. He is coming to visit her. Bella wants to build a wheelchair ramp. Her research shows that there must be 3.5 meters of ramp for every 30 centimeters of elevation. The distance from the ground to the front doorstep of Bella's house is 9 cm . What must the length of the ramp be?</p>
<h2>(b) According new problem generated by GPT-4:</h2>
<p>Liam is a bird watcher and records the pattern of a particular species of bird migration. Every 15 days, the birds fly south 250 miles and then fly back north 100 miles. If this pattern continues, how far south from their original position will the birds be after 75 days?</p>
<h2>(c) Solution generated by MathCoder-Initial without extra prompt:</h2>
<p>Let's solve the problem step by step:</p>
<ol>
<li>Calculate the net distance the birds have moved from their original position in 15 days.</li>
<li>Use the net distance to determine how far the birds will have moved after 75 days.</li>
</ol>
<p>Let's start by calculating the net distance the birds fly south in 15 days. The net distance can be found by subtracting the northward distance from the southward distance.</p>
<div class="codehilite"><pre><span></span><code>south_distance_15_days, north_distance_15_days = 250, 100
net_distance_15_days = south_distance_15_days - north_distance_15_days
</code></pre></div>

<p>$&gt;&gt;&gt;150$
The birds have moved a net distance of 150 miles south from their original position in 15 days.
Now, to determine how far the birds will have moved after 75 days, we need to:
Calculate how many 15 -day cycles are within 75 days. Multiply the number of 15 -day cycles by the net distance moved in 15 days to get the total distance moved in 75 days. Let's find out how many 15 -day cycles are within 75 days.</p>
<div class="codehilite"><pre><span></span><code>total_days = 75
number_of_cycles = total_days // 15
</code></pre></div>

<p>$&gt;&gt;&gt;5$</p>
<p>There are 5 cycles of 15 days within the span of 75 days.
Now, let's calculate the total distance the birds will have moved after 75 days by multiplying the number of 15 -day cycles by the net distance moved in one 15 -day cycle.</p>
<div class="codehilite"><pre><span></span><code>total_distance_75_days = number_of_cycles*net_distance_15_days
</code></pre></div>

<p>$&gt;&gt;&gt;750$</p>
<p>After 75 days, the birds will be 750 miles south from their original position.</p>
<p>Figure 7: An example of the process of creating MathCodeInstruct. Firstly, "Example 1" and "Example 2" are randomly chosen from the train set of GSM8K and MATH respectively. Then a new problem is generated by GPT-4 using the interpolation prompt. Finally, we use our initial MathCoder to generate LCE-style solution for the new problem.</p>
<h1>(a) Evaluation Prompt:</h1>
<p>Which problem is more difficult?
Problem 1: "Zain has 10 more of each coin than Emerie. If Emerie has six quarters, seven dimes, and five nickels, how many coins does Zain have?"</p>
<p>Problem 2: "In a marathon, every runner must run through several checkpoints. In the first segment, Angela runs 15 meters more than Miguel. For each subsequent segment, Angela runs 5 meters less than she did in the previous segment, while Miguel runs consistently. If Miguel runs 50 meters in the first segment, how many meters does Angela run in total when she completes 5 segments?"</p>
<p>You answer should be one of "Problem 1", "Problem 2" and "Tie".</p>
<h2>(b) Judgement from GPT-4:</h2>
<p>Let's break down both problems step by step to assess their complexity.</p>
<h2>Problem 1:</h2>
<p>Emerie has:</p>
<ul>
<li>Quarters: 6</li>
<li>Dimes: 7</li>
<li>Nickels: 5</li>
</ul>
<p>Zain has 10 more of each coin than Emerie:</p>
<ul>
<li>Quarters: $6+10=16$</li>
<li>Dimes: $7+10=17$</li>
<li>Nickels: $5+10=15$</li>
</ul>
<p>To find out how many coins Zain has, we just need to add these numbers together:</p>
<p>$$
16+17+15=48 \text { coins }
$$</p>
<h2>Problem 2:</h2>
<p>Given:</p>
<ul>
<li>Miguel runs consistently at 50 meters per segment.</li>
<li>Angela runs $50+15=65$ meters in the first segment.</li>
<li>Angela runs 5 meters less in each subsequent segment.</li>
</ul>
<p>Using the concept of arithmetic series:</p>
<p>$$
S_{n}=\frac{n}{2}\left(2 a_{1}+(n-1) d\right)
$$</p>
<p>For Angela, the first term, $a_{1}$, is 65 meters ( 15 meters more than Miguel's 50 meters), and the common difference, $d$, is -5 meters, as she runs 5 meters less in each subsequent segment. She completes 5 segments, so $n=5$.
Let's plug in the values to find out how many meters Angela runs in total for the 5 segments:</p>
<p>$$
S_{5}=\frac{5}{2}(2 \cdot 65+(5-1) \cdot-5)=\frac{5}{2}(130-20)=\frac{5}{2}(110)=\frac{550}{2}=275 \text { meters }
$$</p>
<h2>Conclusion:</h2>
<p>Problem 1 involves simple arithmetic and requires the solver to add a constant number of coins to the known number of coins that Emerie has. There's no series or sequential decrease/increase involved. Problem 2, however, involves understanding an arithmetic progression. It needs understanding of sequences and series, and it may require more steps to solve, such as identifying the first term, the common difference, and then applying the formula to find the sum of the series. Therefore, based on these considerations, it could be argued that "Problem 2" is more difficult.</p>
<p>Figure 8: An example of using GPT-4 to evaluate the complexity of problems in MathCodeInstruct. "Problem 2" is in MathCodeInstruct and "Problem 1" is the problem from GSM8K that was used to generate "Problem 2".</p>
<h1>(a) Evaluation Prompt:</h1>
<p>Which problem is more difficult?
Problem 1: "Peter has 15 birds. $1 / 3$ are ducks. The rest are chickens and require special feed that costs $\$ 2$ per bird. How much does it cost to feed the chickens?"</p>
<p>Problem 2: "In a rectangular garden, the length is 4 times the width. If the total area of the garden is increased by 120 square feet by increasing the width by 5 feet while keeping the length constant, what is the original area of the garden?"</p>
<p>You answer should be one of "Problem 1", "Problem 2", and "Tie".</p>
<h2>(b) Judgement from GPT-4:</h2>
<p>Let's break down the two problems to understand the complexity of each.</p>
<h2>Problem 1:</h2>
<p>Peter has 15 birds, and $1 / 3$ are ducks. So the number of chickens is $\frac{2}{3} \times 15=10$ chickens.
The cost to feed the chickens is $\$ 2$ per chicken, so:</p>
<p>$$
\text { Cost }=10 \text { chickens } \times \$ 2 / \text { chicken }=\$ 20
$$</p>
<h2>Problem 2:</h2>
<p>Let the original width of the garden be $w$ and the length be $4 w$.
The original area of the garden is $4 w^{2}$.
If the width is increased by 5 feet, the new width is $w+5$.
The new area is then $4 w(w+5)=4 w^{2}+20 w$.
Given the total area of the garden is increased by 120 square feet:</p>
<p>$$
\begin{gathered}
4 w^{2}+20 w-\left(4 w^{2}\right)=120 \
20 w=120 \
w=6 \text { feet }
\end{gathered}
$$</p>
<p>So, the original area is:</p>
<p>$$
4 w^{2}=4(6)^{2}=144 \text { square feet }
$$</p>
<h2>Comparison:</h2>
<p>Problem 1 involves basic arithmetic and a simple fraction, making it relatively straightforward. Problem 2 involves solving a quadratic equation and requires a better understanding of algebraic concepts and the area of rectangles.</p>
<p>Thus, Problem 2 is more difficult than Problem 1.</p>
<p>Figure 9: An example of using GPT-4 to evaluate the complexity of problems in MathCodeInstruct. "Problem 2" is in MathCodeInstruct and "Problem 1" is the problem from GSM8K that was used to generate "Problem 2".</p>
<h1>(a) Evaluation Prompt:</h1>
<p>Which problem is more difficult?
Problem 1: "he two solutions of the equation $x^{2}+b x+48=0$ are in the ratio of 3 to 1 for some values of $b$. What is the largest possible value of $b$ ?"</p>
<p>Problem 2: "In a rectangular garden, the length is 4 times the width. If the total area of the garden is increased by 120 square feet by increasing the width by 5 feet while keeping the length constant, what is the original area of the garden?"</p>
<p>You answer should be one of "Problem 1", "Problem 2" and "Tie".</p>
<h2>(b) Judgement from GPT-4:</h2>
<p>The two problems provided, at their core, involve algebraic manipulation, but Problem 1 involves quadratic equations and a bit more conceptual understanding of quadratic roots, whereas Problem 2 is more straightforward and involves linear equations. Based on this, it can be said that "Problem 1" is more difficult compared to "Problem 2". Here's a brief explanation and solution for both problems to illustrate this point:</p>
<h2>Problem 1:</h2>
<p>Given that the roots of $x^{2}+b x+48=0$ are in the ratio $3: 1$, let the roots be $3 k$ and $k$. The sum of the roots is $4 k$, and the product of the roots is 48 .
Using Vieta's formulas:</p>
<ul>
<li>$3 k+k=4 k=-b$
$-3 k * k=3 k^{2}=48$
Solving for $k$ in the second equation:</li>
</ul>
<p>$$
k^{2}=16 \Longrightarrow k= \pm 4
$$</p>
<p>So, the roots are 12 and 4 or -12 and -4 . Thus, the possible values of $b$ are -16 or 16 .
So the largest possible value of $b$ is 16 .</p>
<h2>Problem 2:</h2>
<p>Let the width of the rectangular garden be $w$, then the length would be $4 w$.
The original area is:</p>
<p>$$
A_{\text {original }}=l * w=4 w * w=4 w^{2}
$$</p>
<p>If the width is increased by 5 feet, the new width is $w+5$, and the new area is:</p>
<p>$$
A_{\text {new }}=l <em>(w+5)=4 w </em>(w+5)=4 w^{2}+20 w
$$</p>
<p>Given that the total area of the garden is increased by 120 square feet:</p>
<p>$$
\begin{gathered}
A_{\text {new }}=A_{\text {original }}+120 \
4 w^{2}+20 w=4 w^{2}+120
\end{gathered}
$$</p>
<p>Solving for $w$ :</p>
<p>$$
20 w=120 \Longrightarrow w=6
$$</p>
<p>So, the original area of the garden is:</p>
<p>$$
A_{\text {original }}=4 w^{2}=4 * 36=144 \text { square feet }
$$</p>
<p>Problem 1 requires an understanding of quadratic equations, Vieta's formulas, and consideration of multiple possibilities, making it more challenging compared to Problem 2, which involves simpler algebraic manipulations with linear equations.</p>
<p>Figure 10: An example of using GPT-4 to evaluate the complexity of problems in MathCodeInstruct. "Problem 2" is in MathCodeInstruct and "Problem 1" is the problem from MATH that was used to generate "Problem 2".</p>
<h1>(a) Evaluation Prompt:</h1>
<p>Which problem is more difficult?
Problem 1: "Kara mixes cups of sugar and water in the ratio of 1:2 to make one liter of lemonade. Calculate the total number of cups of sugar used if she used 84 cups to make lemonade."</p>
<p>Problem 2: "A cylindrical water tank has two compartments. The first compartment is half the height of the second compartment. When filled, the first compartment holds 60 liters of water. If a pipe pumps water into the tank at a rate of 10 liters per minute and starts filling the second compartment, how many minutes will it take to fill the entire tank?"</p>
<p>You answer should be one of "Problem 1", "Problem 2" and "Tie".</p>
<h2>(b) Judgement from GPT-4:</h2>
<p>Let's solve each problem one by one and then determine which one is more difficult.</p>
<h2>Problem 1:</h2>
<p>Kara mixes cups of sugar and water in the ratio of $1: 2$.
So, for every 1 cup of sugar, there are 2 cups of water.
The ratio of sugar to the total mixture is:</p>
<p>$$
\text { Sugar : Total }=1:(1+2)=1: 3
$$</p>
<p>She used 84 cups to make lemonade, so:</p>
<p>$$
\text { Cups of sugar used }=\frac{1}{3} \times 84=28 \text { cups }
$$</p>
<h2>Problem 2:</h2>
<p>The first compartment is half the height of the second compartment but holds 60 liters of water.
So, the second compartment will hold $2 \times 60$ liters of water, which is 120 liters of water.
The total volume of the tank, including both compartments, is:</p>
<p>$$
60 \text { liters }+120 \text { liters }=180 \text { liters }
$$</p>
<p>The pipe pumps water at a rate of 10 liters per minute, so to fill the entire tank, it will take:</p>
<p>$$
\text { Time }=\frac{180 \text { liters }}{10 \text { liters } / \mathrm{min}}=18 \text { minutes }
$$</p>
<h2>Conclusion:</h2>
<p>Both problems can be solved with basic arithmetic and understanding of ratios, and neither requires advanced mathematical concepts. Therefore, it can be considered a "Tie" in terms of difficulty.</p>
<p>Figure 11: An example of using GPT-4 to evaluate the complexity of problems in MathCodeInstruct. "Problem 2" is in MathCodeInstruct, and "Problem 1" is the problem from GSM8K that was used to generate "Problem 2". It is an example of a tie.</p>
<h1>(a) Evaluation Prompt:</h1>
<p>Which problem is more difficult?
Problem 1: "When Jeffrey walks, for every three steps forward, he takes two steps backwards. Therefore, if the distance between the house and the mailbox is 66 steps, what is the total number of steps Jeffrey takes when he goes from the house to the mailbox?"</p>
<p>Problem 2: "For every four apples Jake eats, he plants three apple seeds. If at the end of the month he has consumed 24 apples, how many seeds has he planted?"</p>
<p>You answer should be one of "Problem 1", "Problem 2" and "Tie".</p>
<h2>(b) Judgement from GPT-4:</h2>
<p>Let's analyze each problem step by step:</p>
<h2>Problem 1:</h2>
<p>Every 3 steps forward and 2 steps back mean Jeffrey effectively moves 1 step for every 5 steps he takes ( 3 steps forward - 2 steps back $=1$ step net movement).</p>
<p>To cover 66 steps:
Steps needed $=($ Total distance $/$ Net movement per cycle $) \times$ Steps per cycle
Steps needed $=(66$ steps $/ 1$ step $) \times 5=330$ steps</p>
<h2>Problem 2:</h2>
<p>For every 4 apples Jake eats, he plants 3 seeds.
So, for 24 apples:
Seeds planted $=(24$ apples $/ 4) \times 3$ seeds $=6 \times 3=18$ seeds
To determine which is more difficult, it seems both problems involve similar basic math operations (division and multiplication).
However, the first problem introduces an extra layer of complexity with the two different types of movements (forward and backward) compared to the direct ratio in the second problem.</p>
<p>Given this analysis, Problem 1 seems slightly more difficult than Problem 2 due to this added layer of understanding needed to solve it.</p>
<p>Figure 12: A example of using GPT-4 to evaluate the complexity of problems in MathCodeInstruct and it is an uncommon example that the problem from GSM8K is slightly more difficult than the interpolation problem. "Problem 2" is in MathCodeInstruct and "Problem 1" is the problem from GSM8K that was used to generate "Problem 2".</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contribution.
${ }^{1}$ Project leader.
${ }^{1}$ Corresponding author.
${ }^{1}$ https://openai.com/blog/chatgpt-plugins#code-interpreter&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
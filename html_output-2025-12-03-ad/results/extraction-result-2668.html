<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2668 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2668</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2668</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-68.html">extraction-schema-68</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <p><strong>Paper ID:</strong> paper-c4692e5d11cde0f10cbd5a534a5870eb299e8156</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c4692e5d11cde0f10cbd5a534a5870eb299e8156" target="_blank">Jointly Measuring Diversity and Quality in Text Generation Models</a></p>
                <p><strong>Paper Venue:</strong> Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes metrics to evaluate both the quality and diversity simultaneously by approximating the distance of the learned generative model and the real data distribution by introducing a metric that approximates this distance using n-gram based measures.</p>
                <p><strong>Paper Abstract:</strong> Text generation is an important Natural Language Processing task with various applications. Although several metrics have already been introduced to evaluate the text generation methods, each of them has its own shortcomings. The most widely used metrics such as BLEU only consider the quality of generated sentences and neglecting their diversity. For example, repeatedly generation of only one high quality sentence would result in a high BLEU score. On the other hand, the more recent metric introduced to evaluate the diversity of generated texts known as Self-BLEU ignores the quality of generated texts. In this paper, we propose metrics to evaluate both the quality and diversity simultaneously by approximating the distance of the learned generative model and the real data distribution. For this purpose, we first introduce a metric that approximates this distance using n-gram based measures. Then, a feature-based measure which is based on a recent highly deep model trained on a large text corpus called BERT is introduced. Finally, for oracle training mode in which the generatorʼs density can also be calculated, we propose to use the distance measures between the corresponding explicit distributions. Eventually, the most popular and recent text generation models are evaluated using both the existing and the proposed metrics and the preferences of the proposed metrics are determined.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2668.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2668.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MS-Jaccard</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MS-Jaccard (Multi-Set Jaccard)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An n-gram multi-set similarity metric that jointly measures quality and diversity by comparing normalized n-gram frequency multisets of generated and real sentences using a min/max intersection-over-union style score aggregated across n.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MS-Jaccard</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>MS-Jaccard treats n-grams extracted from generated and real sentence sets as multisets (preserving repetition), computes normalized counts C_n(g,S) (average frequency per sentence), then for each n computes score_n = sum_g min{C_n(g,S1),C_n(g,S2)} / sum_g max{C_n(g,S1),C_n(g,S2)} and aggregates across n (geometric mean across n=1..N) to produce MS-Jaccard-N. Higher values indicate closer match (better quality+diversity).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>n-gram based distribution-similarity metric</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language generation / NLP (general text evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Assesses plausibility by measuring overlap of n-gram frequency distributions between generated samples and real/test corpora; low overlap indicates implausible/generated-text artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Explicitly designed to jointly measure novelty/diversity and plausibility/quality: the intersection-over-union of normalized n-gram counts penalizes both missing real n-grams (coverage) and over-representation of limited n-grams (mode collapse).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>The MS-Jaccard score itself (per n and aggregated geometric mean across n); lower 1-MS-Jaccard used to align directionality with other distance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Empirical evaluation on held-out test sets (COCO, EMNLP2017 WMT News, IMDB) comparing MS-Jaccard between models and real data; model ranking consistency checked against temperature sweeps and other metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Implemented and evaluated within Texygen benchmarking framework; explicit formulas and dataset splits are reported in paper; MS-Jaccard variants (MSJ2..MSJ5) are reported in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Can be used to detect hallucination-like behavior in generated text by measuring low MS-Jaccard scores (indicating generated outputs deviate from real-data n-gram distribution), though no classifier is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Indirect: detects lack of coverage/diversity which can indicate overconfident/mode-collapsed models; not a probabilistic uncertainty estimator.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>COCO Captions, EMNLP2017 WMT News, IMDB Movie Reviews, synthetic Oracle LSTM dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MS-Jaccard discriminates models: e.g., on COCO Captions MSJ4: MLE=0.322, SeqGAN=0.164, MaliGAN=0.345 (Table 1); MS-Jaccard ordering largely consistent with full quality-diversity temperature sweep analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Paper finds MS-Jaccard ranks MLE above recent GAN variants across datasets (consistent with Caccia et al. temperature-sweep analysis); MS-Jaccard correlates strongly with FBD and NLL in aggregated results.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on choice of n and normalization; n-gram statistics can miss long-range coherence; sensitive to preprocessing and vocabulary; not a direct probabilistic uncertainty metric.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2668.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2668.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FBD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fréchet BERT Distance (FBD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A feature-space distance metric modeling BERT pooled sentence features as Gaussians and computing the Fréchet (Wasserstein-2) distance between generated and real-data feature distributions to capture both quality and diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Fréchet BERT Distance (FBD)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>FBD extracts fixed-size pooled features from BERT for each sentence, fits Gaussian approximations (mean m_i and covariance C_i) to real and generated feature sets, and computes Fréchet distance: sqrt(||m1-m2||_2^2 + Tr(C1 + C2 - 2 (C1 C2)^{1/2})). Lower values indicate generated distribution closer to real.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>feature-based distributional metric using pretrained transformer features</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language generation / NLP (general text evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility is assessed by closeness in BERT feature space: smaller Fréchet distance implies generated sentences occupy similar semantic/feature regions as real data.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Balances quality and diversity because both mean shift (quality) and covariance differences (diversity/coverage) contribute to the Fréchet distance.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Fréchet distance in BERT feature space (FBD); lower is better. Example: COCO Real FBD=0.460, MLE FBD=1.971, SeqGAN FBD=4.590 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Empirical comparisons across models/datasets and correlation analyses with other metrics (MS-Jaccard, NLL); temperature sweep to compare ordering consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Specifies use of pooled BERT features and provides dataset splits; experiments run within Texygen framework.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Can indicate hallucinations or semantic divergence when FBD is large (generated features distant from real-data features), but no explicit hallucination detector is implemented.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Indirect: covariance components of Gaussian approximation capture spread/variety; FBD uses those to reflect diversity/uncertainty but is not a calibrated probabilistic uncertainty estimator.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>COCO Captions, EMNLP2017 WMT News, IMDB Movie Reviews</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>FBD separates models: e.g., COCO FBD values (lower better) Real=0.460, MLE=1.971, MaliGAN=1.474, SeqGAN=4.590 (Table 1); FBD correlates strongly with MS-Jaccard and NLL in aggregated Pearson correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Paper reports FBD ordering is almost always consistent with full quality-diversity temperature sweep; MLE tends to have better (lower) FBD than GAN variants on tested datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Assumes Gaussianity in BERT feature space (as FID does); dependent on BERT representations and their biases; pooled features may miss sequence-level structure; covariance square-root computation may be numerically unstable for small sample sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2668.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2668.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bhattacharyya (oracle)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bhattacharyya distance (Monte-Carlo estimate between distributions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symmetric distance between the oracle (ground-truth probabilistic generator) and learned model distributions estimated by Monte Carlo using symmetrized sample-based estimators of Bhattacharyya affinity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bhattacharyya Monte-Carlo distance</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Given samples {x_i} ~ P (oracle) and {x_j} ~ Q (model), the paper estimates a symmetric Bhattacharyya distance via: B(P,Q) = -0.5*( ln(1/N sum_i sqrt(q(x_i)/p(x_i))) + ln(1/M sum_j sqrt(p(x_j)/q(x_j))) ), using sample likelihood ratios where densities are available (oracle mode). Lower values indicate closer match.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>probability-density-based symmetric divergence estimator</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language generation / NLP (evaluation in synthetic oracle setting)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Directly measures overlap between model and oracle densities, so plausibility assessed by probability mass alignment between P and Q.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Bhattacharyya distance estimate (lower is better). Example: Oracle dataset Bhattacharyya: MLE=7.105, SeqGAN=10.076, MaliGAN=8.503, RankGAN=12.127 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Evaluated in the synthetic oracle setting where true oracle density is accessible; used to rank models and compared with Oracle-NLL and NLL.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Formulas and Monte Carlo estimation procedure provided; experiments use synthetic LSTM oracle and Texygen codebase.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>A high Bhattacharyya distance indicates poor alignment (possible hallucination-like outputs relative to oracle), but no explicit hallucination classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>By comparing densities P and Q, the estimator reflects mismatches and coverage gaps (implicit uncertainty about model support relative to oracle), but it's not a calibrated uncertainty estimate per sample.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Synthetic Oracle LSTM dataset (100k samples; LSTM hidden size 32, seq length 20, vocab 5000), plus other real datasets for complementary metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Bhattacharyya values reported in Table 4 showing MLE closest to oracle (7.105) and RankGAN furthest (12.127); used alongside Oracle-NLL and NLL.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Bhattacharyya agrees with temperature-sweep analysis and Oracle-NLL ordering; MLE outperforms GAN variants in this oracle-based metric in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires access to oracle or tractable density q(x)/p(x) ratios; Monte Carlo estimates can be high-variance; sensitive to extreme likelihood ratios and sample sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2668.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2668.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLEU (Bilingual Evaluation Understudy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classical n-gram precision-based metric with brevity penalty, widely used to measure quality of generated text against references, but primarily measures surface similarity and not diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bleu: a method for automatic evaluation of machine translation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Computes n-gram precision of generated sentences against a reference set (here the test corpus used as references), combined across n (typically 1..4) and with a brevity penalty to discourage very short outputs. In unconditional generation, average BLEU against the full test set is used.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>n-gram overlap quality metric</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Assesses plausibility via n-gram overlap with references: higher BLEU implies generated sentences are closer to reference surface forms.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>BLEU-n scores (BLEU2..BLEU5 reported).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Used to select model checkpoints (best BLEU4 on validation for GANs per paper) and to compare models on held-out test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Authors re-implemented BLEU for parallel computation and report dataset splits and exact BLEU variants used.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Indirect: low BLEU can indicate divergence from reference; not sufficient to identify hallucinations or coverage problems by itself.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>COCO, EMNLP2017 WMT News, IMDB, Oracle synthetic dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU scores are reported in Tables 1-3; e.g., on COCO BL4: Real=0.622, MLE=0.507, SeqGAN=0.578, RankGAN=0.569 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>BLEU often rates some GAN variants highly on surface similarity but does not capture diversity; paper criticizes BLEU for ignoring coverage/mode collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Measures only surface n-gram similarity; can be gamed by producing a small set of high-overlap sentences; ignores diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2668.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2668.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diversity metric that computes BLEU of each generated sentence against other generated sentences as references; averaged Self-BLEU quantifies lack of diversity (higher Self-BLEU = lower diversity).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Texygen: A benchmarking platform for text generation models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Self-BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>For each generated sentence, compute BLEU score using remaining generated sentences as the reference set; average across generated sentences to yield Self-BLEU_n. Lower Self-BLEU corresponds to higher diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>diversity metric (n-gram overlap within generated set)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Self-BLEU scores (Self-BLEU2..Self-BLEU5 reported); used as diversity-only metric.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Used in temperature-sweep diversity vs quality plots and as part of model evaluation panels.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Implemented within Texygen; reported in paper tables and plots.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>High Self-BLEU (low diversity) can indicate mode collapse which may relate to repetitive/hallucinated outputs, but no direct hallucination classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Indirect: treated as entropy-like notion of diversity, complementary to entropy and likelihood measures.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>COCO, EMNLP2017 WMT News, IMDB, Oracle synthetic dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Self-BLEU reported in Tables 1-3; e.g., COCO SBL4: Real=0.489, MLE=0.425, SeqGAN=0.700, MaliGAN=0.451 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Used to show GANs often have higher Self-BLEU (lower diversity) than MLE, consistent with mode-collapse concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Only measures intra-generated-set n-gram overlap; ignores proximity to real distribution (quality).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2668.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2668.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLL / Oracle-NLL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Negative Log-Likelihood (NLL) / Oracle-NLL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Likelihood-based metrics: NLL of test data under model (measures fit to real data via inverse KL) and Oracle-NLL (NLL of generated samples under a synthetic oracle distribution used in SeqGAN-style experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Seqgan: Sequence generative adversarial nets with policy gradient</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NLL / Oracle-NLL</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NLL: negative log-likelihood of held-out test data under the trained generative model (lower NLL = better fit). Oracle-NLL: in synthetic oracle experiments, measure NLL of generated samples under the known oracle distribution (used to assess quality relative to ground-truth generator).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>probabilistic likelihood-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility assessed by likelihood values: high likelihood on real/test data (low NLL) indicates the model assigns high probability mass to real sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>NLL on test data; Oracle-NLL on synthetic oracle for generated samples.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Used for early stopping/termination criteria (MLE uses validation NLL), and for ranking models especially in oracle experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Reported across datasets and models in Tables 1-4; used with Texygen implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Indirect: extremely low coverage or assignment of high probability to invalid sentences can be inferred if NLL is low but other quality/diversity metrics are poor; not a direct hallucination detector.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Likelihoods reflect model probability assignments and can be used to quantify uncertainty about sequences (e.g., per-token probabilities); the paper also discusses exposure-bias limitations of conditioned NLL.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>COCO, EMNLP2017 WMT News, IMDB, Oracle synthetic dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>NLL reported in Tables 1-4; e.g., COCO NLL: MLE=38.416, SeqGAN=55.610, MaliGAN=39.916 (Table 1). Oracle-NLL: SeqGAN=163.179 (best in that metric) (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>NLL favors MLE (MLE trains to maximize likelihood) and the paper notes likelihood-based metrics can unfairly advantage MLE; used to show MLE often outperforms GANs in quality-diversity joint metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Likelihood on conditioned prefixes does not reflect free-running sequence generation (exposure bias). Likelihood can punish missing modes severely and may not reflect generated-set quality/diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2668.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2668.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entropy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model Entropy (estimated)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Entropy of the probabilistic generative model (estimated by Monte Carlo when direct calculation is infeasible) used as a proxy for diversity: higher entropy indicates higher diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Entropy (Monte Carlo estimation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Computes the Shannon entropy of the model's sequence distribution; because direct computation is infeasible for sequence models, a Monte Carlo estimate is used. Entropy used as a diversity metric in temperature-sweep analyses (e.g., Figure 1d).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>probabilistic uncertainty/diversity metric</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Entropy value (higher entropy = more diverse output distribution), often used alongside Oracle-NLL in synthetic experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Used in temperature-sweep to explore quality-diversity trade-offs; reported for oracle dataset plots.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Monte Carlo estimation procedure described conceptually; datasets and model settings reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Low entropy can indicate mode collapse (systematic hallucinations may co-occur), but no explicit hallucination detection.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Entropy is an uncertainty quantification measure at distribution level (global diversity/uncertainty); not calibrated per-sample uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Oracle synthetic dataset (used as diversity measure in Fig.1d)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Entropy used to form quality-diversity curves; specific entropy numeric values are used together with Oracle-NLL in plots (Fig.1d) but not tabulated in main tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Used to reproduce Caccia et al. temperature-sweep approach showing MLE dominates GANs in quality-diversity space.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Monte Carlo entropy estimates can be high variance; entropy alone ignores sample quality and coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2668.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2668.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (Bidirectional Encoder Representations from Transformers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep bidirectional Transformer pretrained on large corpora that produces pooled sentence-level feature vectors used in this paper as the feature extractor for FBD.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bert: Pre-training of deep bidirectional transformers for language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BERT pooled feature extractor</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>BERT provides a fixed-size pooled representation for input sequences (pooled classification token features). The paper uses these pooled features as the embedding/feature space in which Gaussian statistics are computed for Fréchet BERT Distance.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>pretrained transformer-based encoder (feature extractor)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language processing / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Used as a semantic feature space: proximity in BERT space used as proxy for semantic plausibility of generated sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Not a metric itself, but used to compute FBD (Fréchet distance between Gaussians in BERT feature space).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>BERT features are pooled and statistics computed on training/validation/test splits to derive FBD.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Paper specifies using pooled BERT features and references BERT paper; experimental details and dataset splits provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>When used within FBD, BERT feature-space distance can reveal semantic divergence / potential hallucination; BERT itself not used as a hallucination detector here.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>COCO, EMNLP2017 WMT News, IMDB</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used to produce FBD values reported in Tables 1-3; BERT-based FBD correlates with MS-Jaccard and NLL.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>FBD (BERT-based) shown to be more informative than BLEU/Self-BLEU alone for joint quality-diversity assessment in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Dependence on pretrained BERT introduces representation biases; pooled features may not capture fine-grained sequence structure; Gaussian assumption in feature space is an approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bleu: a method for automatic evaluation of machine translation <em>(Rating: 2)</em></li>
                <li>Bert: Pre-training of deep bidirectional transformers for language understanding <em>(Rating: 2)</em></li>
                <li>Seqgan: Sequence generative adversarial nets with policy gradient <em>(Rating: 2)</em></li>
                <li>Texygen: A benchmarking platform for text generation models <em>(Rating: 2)</em></li>
                <li>Gans trained by a two time-scale update rule converge to a local nash equilibrium <em>(Rating: 1)</em></li>
                <li>Language gans falling short <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2668",
    "paper_id": "paper-c4692e5d11cde0f10cbd5a534a5870eb299e8156",
    "extraction_schema_id": "extraction-schema-68",
    "extracted_data": [
        {
            "name_short": "MS-Jaccard",
            "name_full": "MS-Jaccard (Multi-Set Jaccard)",
            "brief_description": "An n-gram multi-set similarity metric that jointly measures quality and diversity by comparing normalized n-gram frequency multisets of generated and real sentences using a min/max intersection-over-union style score aggregated across n.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MS-Jaccard",
            "system_description": "MS-Jaccard treats n-grams extracted from generated and real sentence sets as multisets (preserving repetition), computes normalized counts C_n(g,S) (average frequency per sentence), then for each n computes score_n = sum_g min{C_n(g,S1),C_n(g,S2)} / sum_g max{C_n(g,S1),C_n(g,S2)} and aggregates across n (geometric mean across n=1..N) to produce MS-Jaccard-N. Higher values indicate closer match (better quality+diversity).",
            "system_type": "n-gram based distribution-similarity metric",
            "scientific_domain": "natural language generation / NLP (general text evaluation)",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Assesses plausibility by measuring overlap of n-gram frequency distributions between generated samples and real/test corpora; low overlap indicates implausible/generated-text artifacts.",
            "novelty_plausibility_balance": "Explicitly designed to jointly measure novelty/diversity and plausibility/quality: the intersection-over-union of normalized n-gram counts penalizes both missing real n-grams (coverage) and over-representation of limited n-grams (mode collapse).",
            "hypothesis_quality_metrics": "The MS-Jaccard score itself (per n and aggregated geometric mean across n); lower 1-MS-Jaccard used to align directionality with other distance metrics.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Empirical evaluation on held-out test sets (COCO, EMNLP2017 WMT News, IMDB) comparing MS-Jaccard between models and real data; model ranking consistency checked against temperature sweeps and other metrics.",
            "reproducibility_measures": "Implemented and evaluated within Texygen benchmarking framework; explicit formulas and dataset splits are reported in paper; MS-Jaccard variants (MSJ2..MSJ5) are reported in tables.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Can be used to detect hallucination-like behavior in generated text by measuring low MS-Jaccard scores (indicating generated outputs deviate from real-data n-gram distribution), though no classifier is provided.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Indirect: detects lack of coverage/diversity which can indicate overconfident/mode-collapsed models; not a probabilistic uncertainty estimator.",
            "benchmark_dataset": "COCO Captions, EMNLP2017 WMT News, IMDB Movie Reviews, synthetic Oracle LSTM dataset",
            "performance_metrics": "MS-Jaccard discriminates models: e.g., on COCO Captions MSJ4: MLE=0.322, SeqGAN=0.164, MaliGAN=0.345 (Table 1); MS-Jaccard ordering largely consistent with full quality-diversity temperature sweep analyses.",
            "comparison_with_baseline": "Paper finds MS-Jaccard ranks MLE above recent GAN variants across datasets (consistent with Caccia et al. temperature-sweep analysis); MS-Jaccard correlates strongly with FBD and NLL in aggregated results.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Relies on choice of n and normalization; n-gram statistics can miss long-range coherence; sensitive to preprocessing and vocabulary; not a direct probabilistic uncertainty metric.",
            "uuid": "e2668.0",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "FBD",
            "name_full": "Fréchet BERT Distance (FBD)",
            "brief_description": "A feature-space distance metric modeling BERT pooled sentence features as Gaussians and computing the Fréchet (Wasserstein-2) distance between generated and real-data feature distributions to capture both quality and diversity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Fréchet BERT Distance (FBD)",
            "system_description": "FBD extracts fixed-size pooled features from BERT for each sentence, fits Gaussian approximations (mean m_i and covariance C_i) to real and generated feature sets, and computes Fréchet distance: sqrt(||m1-m2||_2^2 + Tr(C1 + C2 - 2 (C1 C2)^{1/2})). Lower values indicate generated distribution closer to real.",
            "system_type": "feature-based distributional metric using pretrained transformer features",
            "scientific_domain": "natural language generation / NLP (general text evaluation)",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility is assessed by closeness in BERT feature space: smaller Fréchet distance implies generated sentences occupy similar semantic/feature regions as real data.",
            "novelty_plausibility_balance": "Balances quality and diversity because both mean shift (quality) and covariance differences (diversity/coverage) contribute to the Fréchet distance.",
            "hypothesis_quality_metrics": "Fréchet distance in BERT feature space (FBD); lower is better. Example: COCO Real FBD=0.460, MLE FBD=1.971, SeqGAN FBD=4.590 (Table 1).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Empirical comparisons across models/datasets and correlation analyses with other metrics (MS-Jaccard, NLL); temperature sweep to compare ordering consistency.",
            "reproducibility_measures": "Specifies use of pooled BERT features and provides dataset splits; experiments run within Texygen framework.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Can indicate hallucinations or semantic divergence when FBD is large (generated features distant from real-data features), but no explicit hallucination detector is implemented.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Indirect: covariance components of Gaussian approximation capture spread/variety; FBD uses those to reflect diversity/uncertainty but is not a calibrated probabilistic uncertainty estimator.",
            "benchmark_dataset": "COCO Captions, EMNLP2017 WMT News, IMDB Movie Reviews",
            "performance_metrics": "FBD separates models: e.g., COCO FBD values (lower better) Real=0.460, MLE=1.971, MaliGAN=1.474, SeqGAN=4.590 (Table 1); FBD correlates strongly with MS-Jaccard and NLL in aggregated Pearson correlation.",
            "comparison_with_baseline": "Paper reports FBD ordering is almost always consistent with full quality-diversity temperature sweep; MLE tends to have better (lower) FBD than GAN variants on tested datasets.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Assumes Gaussianity in BERT feature space (as FID does); dependent on BERT representations and their biases; pooled features may miss sequence-level structure; covariance square-root computation may be numerically unstable for small sample sizes.",
            "uuid": "e2668.1",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Bhattacharyya (oracle)",
            "name_full": "Bhattacharyya distance (Monte-Carlo estimate between distributions)",
            "brief_description": "A symmetric distance between the oracle (ground-truth probabilistic generator) and learned model distributions estimated by Monte Carlo using symmetrized sample-based estimators of Bhattacharyya affinity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Bhattacharyya Monte-Carlo distance",
            "system_description": "Given samples {x_i} ~ P (oracle) and {x_j} ~ Q (model), the paper estimates a symmetric Bhattacharyya distance via: B(P,Q) = -0.5*( ln(1/N sum_i sqrt(q(x_i)/p(x_i))) + ln(1/M sum_j sqrt(p(x_j)/q(x_j))) ), using sample likelihood ratios where densities are available (oracle mode). Lower values indicate closer match.",
            "system_type": "probability-density-based symmetric divergence estimator",
            "scientific_domain": "natural language generation / NLP (evaluation in synthetic oracle setting)",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Directly measures overlap between model and oracle densities, so plausibility assessed by probability mass alignment between P and Q.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Bhattacharyya distance estimate (lower is better). Example: Oracle dataset Bhattacharyya: MLE=7.105, SeqGAN=10.076, MaliGAN=8.503, RankGAN=12.127 (Table 4).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Evaluated in the synthetic oracle setting where true oracle density is accessible; used to rank models and compared with Oracle-NLL and NLL.",
            "reproducibility_measures": "Formulas and Monte Carlo estimation procedure provided; experiments use synthetic LSTM oracle and Texygen codebase.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "A high Bhattacharyya distance indicates poor alignment (possible hallucination-like outputs relative to oracle), but no explicit hallucination classifier.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "By comparing densities P and Q, the estimator reflects mismatches and coverage gaps (implicit uncertainty about model support relative to oracle), but it's not a calibrated uncertainty estimate per sample.",
            "benchmark_dataset": "Synthetic Oracle LSTM dataset (100k samples; LSTM hidden size 32, seq length 20, vocab 5000), plus other real datasets for complementary metrics.",
            "performance_metrics": "Bhattacharyya values reported in Table 4 showing MLE closest to oracle (7.105) and RankGAN furthest (12.127); used alongside Oracle-NLL and NLL.",
            "comparison_with_baseline": "Bhattacharyya agrees with temperature-sweep analysis and Oracle-NLL ordering; MLE outperforms GAN variants in this oracle-based metric in reported experiments.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Requires access to oracle or tractable density q(x)/p(x) ratios; Monte Carlo estimates can be high-variance; sensitive to extreme likelihood ratios and sample sizes.",
            "uuid": "e2668.2",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "BLEU",
            "name_full": "BLEU (Bilingual Evaluation Understudy)",
            "brief_description": "A classical n-gram precision-based metric with brevity penalty, widely used to measure quality of generated text against references, but primarily measures surface similarity and not diversity.",
            "citation_title": "Bleu: a method for automatic evaluation of machine translation",
            "mention_or_use": "use",
            "system_name": "BLEU",
            "system_description": "Computes n-gram precision of generated sentences against a reference set (here the test corpus used as references), combined across n (typically 1..4) and with a brevity penalty to discourage very short outputs. In unconditional generation, average BLEU against the full test set is used.",
            "system_type": "n-gram overlap quality metric",
            "scientific_domain": "natural language generation / NLP",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Assesses plausibility via n-gram overlap with references: higher BLEU implies generated sentences are closer to reference surface forms.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "BLEU-n scores (BLEU2..BLEU5 reported).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Used to select model checkpoints (best BLEU4 on validation for GANs per paper) and to compare models on held-out test sets.",
            "reproducibility_measures": "Authors re-implemented BLEU for parallel computation and report dataset splits and exact BLEU variants used.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Indirect: low BLEU can indicate divergence from reference; not sufficient to identify hallucinations or coverage problems by itself.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "COCO, EMNLP2017 WMT News, IMDB, Oracle synthetic dataset",
            "performance_metrics": "BLEU scores are reported in Tables 1-3; e.g., on COCO BL4: Real=0.622, MLE=0.507, SeqGAN=0.578, RankGAN=0.569 (Table 1).",
            "comparison_with_baseline": "BLEU often rates some GAN variants highly on surface similarity but does not capture diversity; paper criticizes BLEU for ignoring coverage/mode collapse.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Measures only surface n-gram similarity; can be gamed by producing a small set of high-overlap sentences; ignores diversity.",
            "uuid": "e2668.3",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Self-BLEU",
            "name_full": "Self-BLEU",
            "brief_description": "A diversity metric that computes BLEU of each generated sentence against other generated sentences as references; averaged Self-BLEU quantifies lack of diversity (higher Self-BLEU = lower diversity).",
            "citation_title": "Texygen: A benchmarking platform for text generation models",
            "mention_or_use": "use",
            "system_name": "Self-BLEU",
            "system_description": "For each generated sentence, compute BLEU score using remaining generated sentences as the reference set; average across generated sentences to yield Self-BLEU_n. Lower Self-BLEU corresponds to higher diversity.",
            "system_type": "diversity metric (n-gram overlap within generated set)",
            "scientific_domain": "natural language generation / NLP",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Self-BLEU scores (Self-BLEU2..Self-BLEU5 reported); used as diversity-only metric.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Used in temperature-sweep diversity vs quality plots and as part of model evaluation panels.",
            "reproducibility_measures": "Implemented within Texygen; reported in paper tables and plots.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "High Self-BLEU (low diversity) can indicate mode collapse which may relate to repetitive/hallucinated outputs, but no direct hallucination classifier.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Indirect: treated as entropy-like notion of diversity, complementary to entropy and likelihood measures.",
            "benchmark_dataset": "COCO, EMNLP2017 WMT News, IMDB, Oracle synthetic dataset",
            "performance_metrics": "Self-BLEU reported in Tables 1-3; e.g., COCO SBL4: Real=0.489, MLE=0.425, SeqGAN=0.700, MaliGAN=0.451 (Table 1).",
            "comparison_with_baseline": "Used to show GANs often have higher Self-BLEU (lower diversity) than MLE, consistent with mode-collapse concerns.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Only measures intra-generated-set n-gram overlap; ignores proximity to real distribution (quality).",
            "uuid": "e2668.4",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "NLL / Oracle-NLL",
            "name_full": "Negative Log-Likelihood (NLL) / Oracle-NLL",
            "brief_description": "Likelihood-based metrics: NLL of test data under model (measures fit to real data via inverse KL) and Oracle-NLL (NLL of generated samples under a synthetic oracle distribution used in SeqGAN-style experiments).",
            "citation_title": "Seqgan: Sequence generative adversarial nets with policy gradient",
            "mention_or_use": "use",
            "system_name": "NLL / Oracle-NLL",
            "system_description": "NLL: negative log-likelihood of held-out test data under the trained generative model (lower NLL = better fit). Oracle-NLL: in synthetic oracle experiments, measure NLL of generated samples under the known oracle distribution (used to assess quality relative to ground-truth generator).",
            "system_type": "probabilistic likelihood-based evaluation",
            "scientific_domain": "natural language generation / NLP",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility assessed by likelihood values: high likelihood on real/test data (low NLL) indicates the model assigns high probability mass to real sentences.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "NLL on test data; Oracle-NLL on synthetic oracle for generated samples.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Used for early stopping/termination criteria (MLE uses validation NLL), and for ranking models especially in oracle experiments.",
            "reproducibility_measures": "Reported across datasets and models in Tables 1-4; used with Texygen implementations.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Indirect: extremely low coverage or assignment of high probability to invalid sentences can be inferred if NLL is low but other quality/diversity metrics are poor; not a direct hallucination detector.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Likelihoods reflect model probability assignments and can be used to quantify uncertainty about sequences (e.g., per-token probabilities); the paper also discusses exposure-bias limitations of conditioned NLL.",
            "benchmark_dataset": "COCO, EMNLP2017 WMT News, IMDB, Oracle synthetic dataset",
            "performance_metrics": "NLL reported in Tables 1-4; e.g., COCO NLL: MLE=38.416, SeqGAN=55.610, MaliGAN=39.916 (Table 1). Oracle-NLL: SeqGAN=163.179 (best in that metric) (Table 4).",
            "comparison_with_baseline": "NLL favors MLE (MLE trains to maximize likelihood) and the paper notes likelihood-based metrics can unfairly advantage MLE; used to show MLE often outperforms GANs in quality-diversity joint metrics.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Likelihood on conditioned prefixes does not reflect free-running sequence generation (exposure bias). Likelihood can punish missing modes severely and may not reflect generated-set quality/diversity.",
            "uuid": "e2668.5",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Entropy",
            "name_full": "Model Entropy (estimated)",
            "brief_description": "Entropy of the probabilistic generative model (estimated by Monte Carlo when direct calculation is infeasible) used as a proxy for diversity: higher entropy indicates higher diversity.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Entropy (Monte Carlo estimation)",
            "system_description": "Computes the Shannon entropy of the model's sequence distribution; because direct computation is infeasible for sequence models, a Monte Carlo estimate is used. Entropy used as a diversity metric in temperature-sweep analyses (e.g., Figure 1d).",
            "system_type": "probabilistic uncertainty/diversity metric",
            "scientific_domain": "natural language generation / NLP",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Entropy value (higher entropy = more diverse output distribution), often used alongside Oracle-NLL in synthetic experiments.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Used in temperature-sweep to explore quality-diversity trade-offs; reported for oracle dataset plots.",
            "reproducibility_measures": "Monte Carlo estimation procedure described conceptually; datasets and model settings reported.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Low entropy can indicate mode collapse (systematic hallucinations may co-occur), but no explicit hallucination detection.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Entropy is an uncertainty quantification measure at distribution level (global diversity/uncertainty); not calibrated per-sample uncertainty.",
            "benchmark_dataset": "Oracle synthetic dataset (used as diversity measure in Fig.1d)",
            "performance_metrics": "Entropy used to form quality-diversity curves; specific entropy numeric values are used together with Oracle-NLL in plots (Fig.1d) but not tabulated in main tables.",
            "comparison_with_baseline": "Used to reproduce Caccia et al. temperature-sweep approach showing MLE dominates GANs in quality-diversity space.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Monte Carlo entropy estimates can be high variance; entropy alone ignores sample quality and coherence.",
            "uuid": "e2668.6",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "BERT",
            "name_full": "BERT (Bidirectional Encoder Representations from Transformers)",
            "brief_description": "A deep bidirectional Transformer pretrained on large corpora that produces pooled sentence-level feature vectors used in this paper as the feature extractor for FBD.",
            "citation_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "mention_or_use": "use",
            "system_name": "BERT pooled feature extractor",
            "system_description": "BERT provides a fixed-size pooled representation for input sequences (pooled classification token features). The paper uses these pooled features as the embedding/feature space in which Gaussian statistics are computed for Fréchet BERT Distance.",
            "system_type": "pretrained transformer-based encoder (feature extractor)",
            "scientific_domain": "natural language processing / NLP",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Used as a semantic feature space: proximity in BERT space used as proxy for semantic plausibility of generated sentences.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Not a metric itself, but used to compute FBD (Fréchet distance between Gaussians in BERT feature space).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "BERT features are pooled and statistics computed on training/validation/test splits to derive FBD.",
            "reproducibility_measures": "Paper specifies using pooled BERT features and references BERT paper; experimental details and dataset splits provided.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "When used within FBD, BERT feature-space distance can reveal semantic divergence / potential hallucination; BERT itself not used as a hallucination detector here.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "COCO, EMNLP2017 WMT News, IMDB",
            "performance_metrics": "Used to produce FBD values reported in Tables 1-3; BERT-based FBD correlates with MS-Jaccard and NLL.",
            "comparison_with_baseline": "FBD (BERT-based) shown to be more informative than BLEU/Self-BLEU alone for joint quality-diversity assessment in reported experiments.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Dependence on pretrained BERT introduces representation biases; pooled features may not capture fine-grained sequence structure; Gaussian assumption in feature space is an approximation.",
            "uuid": "e2668.7",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Bleu: a method for automatic evaluation of machine translation",
            "rating": 2
        },
        {
            "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "rating": 2
        },
        {
            "paper_title": "Seqgan: Sequence generative adversarial nets with policy gradient",
            "rating": 2
        },
        {
            "paper_title": "Texygen: A benchmarking platform for text generation models",
            "rating": 2
        },
        {
            "paper_title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "rating": 1
        },
        {
            "paper_title": "Language gans falling short",
            "rating": 1
        }
    ],
    "cost": 0.01965525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Jointly Measuring Diversity and Quality in Text Generation Models</h1>
<p>Ehsan Montahaei*<br>Sharif University of<br>Technology / Tehran, Iran<br>ehsan.montahaei@gmail.com</p>
<p>Danial Alihosseini*<br>Sharif University of<br>Technology / Tehran, Iran<br>dalihosseini@ce.sharif.edu</p>
<p>Mahdieh Soleymani Baghshah<br>Sharif University of<br>Technology / Tehran, Iran<br>soleymani@sharif.edu</p>
<h4>Abstract</h4>
<p>Text generation is an important Natural Language Processing task with various applications. Although several metrics have already been introduced to evaluate the text generation methods, each of them has its own shortcomings. The most widely used metrics such as BLEU only consider the quality of generated sentences and neglect their diversity. For example, repeatedly generation of only one high quality sentence would result in a high BLEU score. On the other hand, the more recent metric introduced to evaluate the diversity of generated texts known as Self-BLEU ignores the quality of generated texts. In this paper, we propose metrics to evaluate both the quality and diversity simultaneously by approximating the distance of the learned generative model and the real data distribution. For this purpose, we first introduce a metric that approximates this distance using n-gram based measures. Then, a feature-based measure which is based on a recent highly deep model trained on a large text corpus called BERT is introduced. Finally, for oracle training mode in which the generators density can also be calculated, we propose to use the distance measures between the corresponding explicit distributions. Eventually, the most popular and recent text generation models are evaluated using both the existing and the proposed metrics and the preferences of the proposed metrics are determined.</p>
<h2>1 Introduction</h2>
<p>Generative models and especially Generative Adversarial Networks (GANs) have been received much attention in the last few years. However, the evaluation of generated samples by these models is challenging. Although some studies have recently focused on introducing measures like Inception</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Score and Fréchet Inception Distance (FID) to compare results of different GAN models for image generation, there is not a study to propose proper metrics for evaluation of text generation models. In the last few years, many GAN-based text generation models (Yu et al., 2017; Lin et al., 2017; Che et al., 2017; Guo et al., 2018; Zhang et al., 2017) have been proposed. However, measuring the performance of these models in the corresponding papers is not comprehensive. GANs suffer from the mode collapse problem (Metz et al., 2016) and the GAN-based text generation models may just produce a highly limited set of sentences and therefore just considering the quality of these generated sentences for comparison is not comprehensive.</p>
<p>On the other hand, there are measures like SelfBLEU (Zhang et al., 2017) for evaluating the diversity of generated sentences, but they can not consider the quality of samples at all. Besides, designing an experiment of evaluating diversity by humans is not straightforward and thus it's necessary to have a jointly quality-diversity measuring metric.</p>
<p>In this paper, we intend to propose metrics sensitive to both quality and diversity simultaneously, assigning low scores not only to models generating low-quality samples but also to the ones with low-diversity samples (including the mode collapsed models). To this end, we first propose the MS-Jaccard as an n-gram based measure that considers the quality and diversity of generated samples simultaneously. It attempts to find the similarity of the set of generated samples by a model and the set of real (or test) samples. Then, a featurebased measure is proposed to compare the real data distribution and the generative model distribution in the feature space. Indeed, by borrowing the idea of FID (Heusel et al., 2017) that is a popular feature-based evaluation metric in im-</p>
<p>age generation tasks and advent of a recent highly deep model named BERT (Devlin et al., 2018) as a reference feature extractor for natural language texts, a metric is proposed for evaluation of natural language generation. Finally, appropriate divergences between the oracle distribution and the (learned) model distribution is introduced for when the probabilistic oracle is considered as synthetic data distribution (and thus the target distribution is available for evaluation).</p>
<h2>2 Text Generation Models</h2>
<p>The neural models on text generation first used LSTMs and trained them by the Maximum Likelihood Estimation (MLE) via teacher forcing (Hochreiter and Schmidhuber, 1997). These models suffer from the exposure bias problem which is due to the train-test discrepancy. Although some solutions such as scheduled sampling were introduced to overcome the exposure bias problem, it has been shown that they are incompatible with the language nature (Bengio et al., 2015; Huszar, 2015). By introducing GANs (Goodfellow et al., 2014) as successful image generation models, it has gained much attention to propose GAN-based text generation models. However, the discrete nature of text needs the generator with discrete outputs that makes passing the gradient from the discriminator to the generator difficult. SeqGAN (Yu et al., 2017) alleviates this difficulty by a gradient policy approach using a REINFORCE-like method to train the generator as a stochastic policy. This method has some difficulties such as reward sparsity and high variance for large action spaces. Subsequent methods try to pass more informative signal from the discriminator to the generator. RankGAN(Lin et al., 2017) trains the discriminator as a ranker which assigns a higher score to the more realistic sequences (in comparison with other sentences in the current batch). LeakGAN (Guo et al., 2018) takes advantage of the feudal networks and considers the discriminator as a manager and the generator as a worker while the feature layer of the discriminator is fed to the generator as leaked information. MaliGAN (Che et al., 2017) attempts to redefine the generator's objective. It minimizes KL divergence between the generator and the real distribution which is obtained by the discriminator in the optimality assumption of the discriminator. This new objective leads to an importance sampling procedure.</p>
<p>TextGAN (Zhang et al., 2017) also applies a new objective for the generator. It tries to push the generator focus from the last layer of the discriminator to its last feature layer. Real data and generator samples will each have some distribution in the feature layer of the discriminator. The generator's objective is to make them closer by Maximum Mean Discrepancy (MMD) metric.</p>
<h2>3 Metrics</h2>
<p>In this section, we first indicate the main difficulties of the existing measures for evaluation of text generation models. Then, we introduce metrics that evaluate the capability of the models in generating both right sentences and various ones. The proposed metrics (that are all symmetric) jointly specify to what extent probable sentences in real data are likely in the generative model and also the probable sentences in the model are likely in the real data.</p>
<h3>3.1 Shortcomings of the existing metrics</h3>
<p>In this section, shortcomings of the metrics that either evaluate the quality or the diversity of generated samples are presented. Moreover, a recent attempt to simultaneously considering these metrics is introduced.</p>
<h3>3.1.1 Quality metrics</h3>
<p>BLEU: It is the most widely used metric for text generation. Originally BLEU (Papineni et al., 2002) is a metric to evaluate the quality of machine-translated text. In unconditional text generation, all sentences in the test set are considered as the reference set and generated sentences are evaluated by computing their average BLEU score on this reference set. In conditional text generation tasks like machine translation which include a limited reference set (for each condition), computing the similarity of the generated text and the reference set may be sensible. However, the reference set for the unconditional text generation task is whole available sentences and measures like BLEU just consider the validity of generated sentences without measuring what proportion of the reference sentences can be covered by the text generation model. On the other hand, GAN-based text generation models may generate a highly limited set of sentences and sacrifice the diversity (due to the mode collapse problem). Therefore, evaluating these models using BLEU score just</p>
<p>shows the validity of their outputs without considering their coverage.</p>
<p>Oracle-NLL: It was introduced by SeqGAN (Yu et al., 2017) and is based on assuming a synthetic oracle distribution. It considers a random distribution as the real distribution (or the oracle) and the training dataset is prepared by sampling from this distribution. The score is defined to be the Negative Log Likelihood (NLL) of the generated samples from the trained model in the oracle distribution. In this measure, the coverage is again neglected and a model that generates only one high quality sentence can reach high performance.</p>
<h3>3.1.2 Diversity metric</h3>
<p>As mentioned above, BLUE and Oracle-NLL just consider the quality of the generated samples and ignore their diversity. Below, we introduce two metrics measuring the diversity. However, these metrics evaluate only diversity and don't consider the quality of samples at all.</p>
<p>Self-BLEU: In (Zhu et al., 2018), Self-BLEU was introduced to evaluate just variety of sentences. It measures BLEU score for each generated sentence by considering other generated sentences as reference. By averaging these BLEU scores (obtained for generated sentences), a metric that is called Self-BLEU is achieved where its lower values shows more diversity.</p>
<p>Entropy: On the other side, we can use the entropy of probabilistic generative model to measure the diversity where the lower values show lower diversity. As the direct calculation of the entropy is not feasible, a Monte-Carlo estimation of it can be used.</p>
<h3>3.1.3 Quality and diversity</h3>
<p>Recently (Caccia et al., 2018) mentioned the flaws of only evaluating the quality and found that MLE outperforms the GAN variants for text generation since it dominates GANs in the quality-diversity space. (Caccia et al., 2018) uses the qualitydiversity spectrum obtained by changing the temperature parameter that controls entropy of the models' conditional distributions. However, it does not provide a measure to assess both the quality and the diversity without needing to inspect the whole quality-diversity spectrum.</p>
<p>Likelihood: Although the likelihood of a generative model on real (test) data evaluates the ability of the model in generating the test samples, it doesn't measure the quality of the whole set of
generated texts by the model. In fact, a model with a low NLL value on test data (or equivalently a model in which the likelihood of the test data is high) may also assign high probability to many other sentences that are not valid or qualified. Specifically for sequence models, the likelihood doesn't assess the free-running mode of models. To be more detailed, most of the probabilistic sequence models, decompose the joint distribution to conditional distributions using the chain rule. These conditional distributions are the probability of each token conditioned on the prior tokens. Thus, in the likelihood evaluation, each of token's probability is conditioned on a prefix that is a real sequence itself and the likelihood is not assessed on the previously generated tokens of the model during evaluation (it is similar to the exposure bias problem of MLE for sequence generation).</p>
<p>Moreover, measuring a model by its likelihood score has another problem. When a model misses one mode of a multi-modal distribution, its score decreases severely; so it is an unfair metric for comparing MLE method with other methods because MLE method uses likelihood as its objective and has mean seeking behavior (Goodfellow, 2017).</p>
<h3>3.2 Proposed metrics</h3>
<p>In this section, we propose metrics that simultaneously considers the quality and the diversity of the generated samples. To this end, we compare the real distribution of texts with the obtained distribution by the text generation model.</p>
<h3>3.2.1 MS-Jaccard</h3>
<p>We first propose a metric that finds the similarity of the generative model and the real distribution by comparing text samples generated by them. To this end, n-grams of generated samples and those of real samples are considered as two multi-sets (that also preserve repetition of n-grams) and the similarity of the resulted multi-sets is computed. In simple words, the MS-Jaccard focuses on the similarity of the n-grams frequencies in the two sets and inspired by the well-known Jaccard Index which determines the similarity of two sets as the ratio of the cardinality of their intersection to that of their union.</p>
<p>To define it formally, let $S_{1}$ and $S_{2}$ be two sets of sentences, $G_{n}$ be the set of n-grams in $S_{1} \cup S_{2}$, and $C_{n}(g, S)$ be the normalized counts of the n gram $g$ in the set $S$. The similarity between n -</p>
<p>grams of two sets $S_{1}$ and $S_{2}$ is defined as:</p>
<p>$$
\operatorname{score}<em G__n="G_{n" _in="\in" g="g">{n}=\frac{\sum</em>
$$}} \min \left{C_{n}\left(g, S_{1}\right), C_{n}\left(g, S_{2}\right)\right}}{\sum_{g \in G_{n}} \max \left{C_{n}\left(g, S_{1}\right), C_{n}\left(g, S_{2}\right)\right}</p>
<p>The geometric mean of the $\left{\operatorname{score}<em n="1">{n}\right}</em>(g, S)$ will denotes the average frequency per sentence for n -gram $g$ in the set $S$. If the generated sentences won't have diversity or quality, the n-gram distribution of generated texts will be different from that of the real texts and causing to decrease the MS-Jaccard score consequently. As it is obvious, the MS-Jaccard is a similarity measure and so its higher value will be better.}^{N}$ will be the MS-Jaccard score called MS-Jaccard- $N$ where the $N$ is the maximum length of $n$-grams. It is worth noting that the frequencies of the n-grams in each set is normalized with respect to the total number of sentences in the set (to avoid diminishing the score when the size of only one of these sets grows). Thus, the $C_{n</p>
<h3>3.2.2 Fréchet BERT Distance (FBD)</h3>
<p>One popular metric for evaluation of image generation models is FID introduced in (Heusel et al., 2017). Each of real and generated images in a feature space (found by Inception network) is modeled by a Gaussian distribution, and the FID is defined as the Fréchet distance between these two Gaussian distributions. We want to introduce a similar measure for the text generation task. To this end, we utilize BERT (Devlin et al., 2018) that provides a proper feature space for texts. We use Fréchet distance in BERT's feature space as a metric that considers quality and variety of generated sentences, and name it Fréchet BERT Distance (FBD). There is a set of pooled features (for classification task) in the BERT network that has a constant size for different input sequence lengths; we used these features for FBD. The Fréchet distance is also known as Wasserstein-2 divergence, and this distance between two Gaussian distribution is as follows:</p>
<p>$$
\sqrt{\left|m_{1}-m_{2}\right|<em 1="1">{2}^{2}+\operatorname{Tr}\left(C</em>
$$}+C_{2}-2\left(C_{1} C_{2}\right)^{1 / 2}\right)</p>
<p>where $m_{i}$ and $C_{i}$ show the mean vector and the covariance matrix of these Gaussians respectively. It should be noted as the FBD is a distance measure, its lower values will be better.</p>
<h3>3.2.3 Oracle Based Evaluation</h3>
<p>In Oracle-NLL evaluation introduced in (Yu et al., 2017), the measured distance is Kullback-Leibler (KL) divergence of the generative model and the oracle which ignores the variety of generated sentences. On the other hand, the inverse KL (that is relevant to the likelihood of real data in the text generation model) can not guarantee the quality of generated samples by the model. We propose measuring the distance of the probabilistic oracle distribution $P$ (that generates real data) and the probabilistic generative model $Q$ by a symmetric distance as an evaluation metric. A wide range of distances can be utilized for this purpose. One symmetric distance is Bhattacharyya that can be estimated by the Monte-Carlo as below:</p>
<p>$$
\begin{aligned}
&amp; B(P, Q)= \
&amp; \quad \frac{-1}{2}\left(\ln \frac{1}{N} \sum_{i=0}^{N} \sqrt{\frac{q\left(x_{i}\right)}{p\left(x_{i}\right)}}+\ln \frac{1}{M} \sum_{j=0}^{M} \sqrt{\frac{p\left(x_{j}\right)}{q\left(x_{j}\right)}}\right)
\end{aligned}
$$</p>
<p>where $\left{x_{i}\right}$ and $\left{x_{j}\right}$ are sets of samples from $P$ and $Q$ distributions respectively. Similar to the FBD, Bhattacharyya is also a distance measure and thus its lower values are better.</p>
<h2>4 Evaluation</h2>
<p>In this section, we first conduct some experiments to evaluate text generation models using the existing and the proposed measures. Then, we discuss about the appropriateness of the proposed metrics.</p>
<h3>4.1 Datasets</h3>
<p>We evaluate the models on COCO image captions (Lin et al., 2014), EMNLP2017 WMT News (Bojar et al., 2017), and IMDB (Maas et al., 2011) as the popular datasets for text generation. In addition to these datasets, similar to (Yu et al., 2017; Lin et al., 2017; Guo et al., 2018), we also consider a synthetic oracle produced by a probabilistic text generator that is a random initialized LSTM as a synthetic dataset. The description of the datasets is as follows:</p>
<ul>
<li>COCO Captions (Lin et al., 2014): It is a collection of image captions containing around 600,000 captions. Sentences having between 5 and 25 words are selected (resulting in 524,225 sentences) where 5,328 is the vocab size of the resulted dataset. Among the resulted dataset, 40,000 samples are used for</li>
</ul>
<p>training, 20,000 samples for validation, and 20,000 for test.</p>
<ul>
<li>EMNLP2017 WMT News (Bojar et al., 2017): It is a collection of news texts for the machine translations task ${ }^{1}$. Among a version of this dataset for English corpus containing 500,000 sentences, sentences having more than 3 words with less than 150 frequency (these words are replaced with UNK) were dropped and sentences that have between 20 and 40 words selected. The vocab size of the resulted dataset is 6,148 . Among this dataset, 40,000 samples are used for training, 20,000 samples for validation, and 20,000 for test.</li>
<li>IMDB Movie Reviews (Maas et al., 2011): It is a collection of IMDB movie reviews for the sentiment analysis task, containing 25,000 labeled and 50,000 unlabeled ones. We have selected the first two sentences of each review and replace words with less that 50 times frequency with UNK and keep sentences from length 5 to 40 with less than 5 UNKs. The final dataset is subsampled to have 20,000 sentences for training data, 10,000 for validation, and 10,000 for test data leading to vocab size of 5,810 .</li>
<li>Oracle synthetic dataset (Yu et al., 2017): A randomly initialized LSTM generator as a real distribution used in oracle training mode; the network implementation is borrowed from the SeqGAN released code ${ }^{2}$. This network's hidden size is 32 and its embedding size is 3,200. Moreover, the vocab size is 5,000 and the length of sequences is 20. The dataset of 100,000 samples are generated according to the above model. Among this dataset, 50,000 samples are used for training, 25,000 for validation, and 25,000 for test.</li>
</ul>
<h3>4.2 Experimental Setup</h3>
<h3>4.2.1 Text Generation Models</h3>
<p>As the recent methods for text generation, we evaluate SeqGAN (Yu et al., 2017), RankGAN (Lin et al., 2017), and MaliGAN (Che et al., 2017). We also consider vanilla Maximum Likelihood Estimation (MLE) language model using LSTM as the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>baseline method. We used the implementation of the above methods in the Texygen platform (Zhu et al., 2018) and train them in this framework ${ }^{3}$. The models were trained on the similar dataset existing in their released code but collected from the original sites reported in corresponding reference papers.</p>
<p>In order to have a fair comparison, all settings of the models (e.g., same hidden) were kept the same as the Texygen framework. Since setting a fixed number of epochs for terminating training of different methods does not seem such reasonable and resulting in unfair scores, we targeted multiple training termination criteria. In the realworld datasets training, the training termination of the GANs were based on obtaining the best BLEU4 on validation data in addition to setting a max number of iterations for all the models. Besides, the training termination of MLE is based the NLL on the validation data while also setting a max number of iterations as above. In the oracle training mode, the termination were done based on both Oracle-NLL on the validation set and again on a max number of iterations for all models.</p>
<h3>4.2.2 Metrics</h3>
<p>Among the existing measures, BLEU2 upto BLEU5 (evaluating only quality), Self-BLUE2 upto Self-BLEU5 (evaluating only diversity), and NLL that shows the negative log likelihood of the model on test data are utilized for real datasets. Moreover, due to the low performance of the Python NLTK (Bird et al., 2009) BLEU library ${ }^{4}$ when needing to evaluate multiple sentences with a fixed reference set, we have re-implemented it to achieve parallel computation and high performance ${ }^{5}$.</p>
<p>Among the proposed measures, MS-Jaccard2 upto MS-Jaccard5 and FBD are assayed on realworld datasets. For synthetic oracle, NLL and Oracle-NLL as the existing measures and the proposed measure for comparing distributions, i.e. Bhattacharyya, are evaluated. It should be noted that, in order to make the metric's directions the same (i.e. their lower values show better performance), the $1-$ MS-Jaccard, $1-$ BLEU and $-1 \times$ Entropy is used in some plots.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Table 1: Performance of models (using different measures) on COCO Captions dataset. MSJ, BL, and SBL denote MS-Jaccard, BLEU, and Self-BLEU respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">NLL</th>
<th style="text-align: center;">FBD</th>
<th style="text-align: center;">MSJ2</th>
<th style="text-align: center;">MSJ3</th>
<th style="text-align: center;">MSJ4</th>
<th style="text-align: center;">MSJ5</th>
<th style="text-align: center;">BL2</th>
<th style="text-align: center;">BL3</th>
<th style="text-align: center;">BL4</th>
<th style="text-align: center;">BL5</th>
<th style="text-align: center;">SBL2</th>
<th style="text-align: center;">SBL3</th>
<th style="text-align: center;">SBL4</th>
<th style="text-align: center;">SBL5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Real Data</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.460</td>
<td style="text-align: center;">0.760</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.430</td>
<td style="text-align: center;">0.306</td>
<td style="text-align: center;">0.926</td>
<td style="text-align: center;">0.794</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.454</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">0.685</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.329</td>
</tr>
<tr>
<td style="text-align: center;">MLE</td>
<td style="text-align: center;">$\mathbf{3 8 . 4 1 6}$</td>
<td style="text-align: center;">1.971</td>
<td style="text-align: center;">0.655</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.891</td>
<td style="text-align: center;">0.715</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">$\mathbf{0 . 8 4 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 4 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 6 8}$</td>
</tr>
<tr>
<td style="text-align: center;">SeqGAN</td>
<td style="text-align: center;">55.610</td>
<td style="text-align: center;">4.590</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.164</td>
<td style="text-align: center;">0.111</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">0.771</td>
<td style="text-align: center;">$\mathbf{0 . 5 7 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 8 0}$</td>
<td style="text-align: center;">0.941</td>
<td style="text-align: center;">0.842</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.545</td>
</tr>
<tr>
<td style="text-align: center;">MaliGAN</td>
<td style="text-align: center;">39.916</td>
<td style="text-align: center;">$\mathbf{1 . 4 7 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 7 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 9 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 4 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 3 1}$</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.536</td>
<td style="text-align: center;">0.361</td>
<td style="text-align: center;">0.859</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">0.451</td>
<td style="text-align: center;">0.288</td>
</tr>
<tr>
<td style="text-align: center;">RankGAN</td>
<td style="text-align: center;">48.816</td>
<td style="text-align: center;">3.574</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">$\mathbf{0 . 9 2 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 8 2}$</td>
<td style="text-align: center;">0.569</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.583</td>
<td style="text-align: center;">0.402</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance of models (using different measures) on EMNLP2017 WMT News dataset. MSJ, BL, and SBL denote MS-Jaccard, BLEU, and Self-BLEU respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">NLL</th>
<th style="text-align: center;">FBD</th>
<th style="text-align: center;">MSJ2</th>
<th style="text-align: center;">MSJ3</th>
<th style="text-align: center;">MSJ4</th>
<th style="text-align: center;">MSJ5</th>
<th style="text-align: center;">BL2</th>
<th style="text-align: center;">BL3</th>
<th style="text-align: center;">BL4</th>
<th style="text-align: center;">BL5</th>
<th style="text-align: center;">SBL2</th>
<th style="text-align: center;">SBL3</th>
<th style="text-align: center;">SBL4</th>
<th style="text-align: center;">SBL5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Real Data</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.905</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.243</td>
<td style="text-align: center;">0.129</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">0.380</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">0.797</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.133</td>
</tr>
<tr>
<td style="text-align: center;">MLE</td>
<td style="text-align: center;">$\mathbf{1 4 3 . 2 4 6}$</td>
<td style="text-align: center;">$\mathbf{4 . 8 2 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 8 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 3 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 6 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 7 1}$</td>
<td style="text-align: center;">0.837</td>
<td style="text-align: center;">0.542</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">$\mathbf{0 . 7 7 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 5 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 9 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 9 5}$</td>
</tr>
<tr>
<td style="text-align: center;">SeqGAN</td>
<td style="text-align: center;">195.867</td>
<td style="text-align: center;">5.955</td>
<td style="text-align: center;">0.231</td>
<td style="text-align: center;">0.138</td>
<td style="text-align: center;">0.071</td>
<td style="text-align: center;">0.031</td>
<td style="text-align: center;">0.476</td>
<td style="text-align: center;">0.358</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.105</td>
<td style="text-align: center;">0.906</td>
<td style="text-align: center;">0.729</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">0.324</td>
</tr>
<tr>
<td style="text-align: center;">MaliGAN</td>
<td style="text-align: center;">163.931</td>
<td style="text-align: center;">5.690</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.132</td>
<td style="text-align: center;">0.061</td>
<td style="text-align: center;">$\mathbf{0 . 8 5 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 9 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 1 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 4 1}$</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.155</td>
</tr>
<tr>
<td style="text-align: center;">RankGAN</td>
<td style="text-align: center;">177.346</td>
<td style="text-align: center;">5.104</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.156</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">0.036</td>
<td style="text-align: center;">0.461</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.097</td>
<td style="text-align: center;">0.841</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.224</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance of models (using different measures) on IMDB Movie Reviews dataset. MSJ, BL, and SBL denote MS-Jaccard, BLEU, and Self-BLEU respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">NLL</th>
<th style="text-align: center;">FBD</th>
<th style="text-align: center;">MSJ2</th>
<th style="text-align: center;">MSJ3</th>
<th style="text-align: center;">MSJ4</th>
<th style="text-align: center;">MSJ5</th>
<th style="text-align: center;">BL2</th>
<th style="text-align: center;">BL3</th>
<th style="text-align: center;">BL4</th>
<th style="text-align: center;">BL5</th>
<th style="text-align: center;">SBL2</th>
<th style="text-align: center;">SBL3</th>
<th style="text-align: center;">SBL4</th>
<th style="text-align: center;">SBL5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Real Data</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.696</td>
<td style="text-align: center;">0.469</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.181</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.468</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.853</td>
<td style="text-align: center;">0.629</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.241</td>
</tr>
<tr>
<td style="text-align: center;">MLE</td>
<td style="text-align: center;">$\mathbf{1 2 5 . 2 2 3}$</td>
<td style="text-align: center;">$\mathbf{3 . 5 3 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 0 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 7 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 1 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 5}$</td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">$\mathbf{0 . 8 4 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 9 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 4 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 7 9}$</td>
</tr>
<tr>
<td style="text-align: center;">SeqGAN</td>
<td style="text-align: center;">150.213</td>
<td style="text-align: center;">4.587</td>
<td style="text-align: center;">0.377</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">0.082</td>
<td style="text-align: center;">$\mathbf{0 . 9 0 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 9 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 4}$</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.763</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.345</td>
</tr>
<tr>
<td style="text-align: center;">MaliGAN</td>
<td style="text-align: center;">141.558</td>
<td style="text-align: center;">4.482</td>
<td style="text-align: center;">0.446</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.178</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">$\mathbf{0 . 2 3 3}$</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.695</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">0.290</td>
</tr>
<tr>
<td style="text-align: center;">RankGAN</td>
<td style="text-align: center;">151.828</td>
<td style="text-align: center;">3.958</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.132</td>
<td style="text-align: center;">0.070</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.693</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.228</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.739</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.331</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance of models (using different measures) on Oracle dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">NLL</th>
<th style="text-align: center;">Oracle-NLL</th>
<th style="text-align: center;">Bhattacharyya</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MLE</td>
<td style="text-align: center;">$\mathbf{1 4 1 . 9 4 8}$</td>
<td style="text-align: center;">167.014</td>
<td style="text-align: center;">$\mathbf{7 . 1 0 5}$</td>
</tr>
<tr>
<td style="text-align: center;">SeqGAN</td>
<td style="text-align: center;">155.353</td>
<td style="text-align: center;">$\mathbf{1 6 3 . 1 7 9}$</td>
<td style="text-align: center;">10.076</td>
</tr>
<tr>
<td style="text-align: center;">MaliGAN</td>
<td style="text-align: center;">146.260</td>
<td style="text-align: center;">168.054</td>
<td style="text-align: center;">8.503</td>
</tr>
<tr>
<td style="text-align: center;">RankGAN</td>
<td style="text-align: center;">160.424</td>
<td style="text-align: center;">166.774</td>
<td style="text-align: center;">12.127</td>
</tr>
</tbody>
</table>
<h3>4.3 Results</h3>
<p>Results of different methods on COCO Captions, EMNLP2017 WMT News, and IMDB datasets as real-world datasets are shown in Tables 1, 2, and 3 , respectively. To provide a target, we have also shown metrics for training data themselves and called the method as Real (indeed training data is considered as the generated data by Real and the measures are computed on them). These tables show that MLE has the best performance according to the proposed measures considering both quality and diversity of samples. In fact, GANbased methods can not generally achieve good performance according to the proposed measures. This result is consistent with the reported results in (Caccia et al., 2018) that compares GANs and MLE for text generation.</p>
<p>Table 4 shows results of different methods on synthetic oracle dataset and MLE again shows the best results according to the proposed metric (that approximates the distance of the real distribution and the generative model distribution).</p>
<p>As mentioned in Section 3.1.3 about (Caccia et al., 2018), the whole spectrum of qualitydiversity is considered for evaluation of Natural Language Generation (NLG) methods. In fact, in (Caccia et al., 2018), the temperature sweep is utilized to robustly evaluate text generation methods. More precisely, the generators conditional distribution $G\left(x_{t} \mid x_{1: t-1}\right)$ is defined as $\operatorname{Softmax}\left(o_{t} / T\right)$ where $o_{t}$ denotes the logit at time $t$. Decreasing $T$ below 1.0 will decrease the entropy of conditional probability and thus reduce the probability of generating low quality samples. On the other hand, increasing this temperature above 1.0 will upraise the entropy of the conditional distribution and thus improve the diversity of the generated samples (Caccia et al., 2018).</p>
<p>We intend to show that the proposed metrics are correlated with the analysis of the whole space of quality-diversity obtained by changing the temperature. In fact, using the proposed metrics we can</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Diversity vs. quality measure of various models with temperatures from $1.5^{-3}$ to $1.5^{4}$ on different datasets. Each point in the plot corresponds to the performance of a model in a special temperature (A seconddegree polynomial has been fitted to the points). Lower values in both axes show better ones.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: NLL, 1-MS-Jaccard4, and FBD scores of all the models without applying temperature (i.e. $T=1$ ) on different datasets. Lower values show better performance.
usually predict the behavior of the model in whole spectrum without needing to provide this qualitydiversity space.</p>
<p>Fig. 1 shows the diversity against quality
measures with different values of temperature. Figs. 1a, 1b, and 1c consider Self-BLEU4 as diversity and BLEU4 as quality measure for each of the methods on real-world COCO, EMNLP2017,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The performance of all models (without applying temperature, i.e. $T=1$ ) on the Oracle dataset using different measures. Lower values show better performance.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Pearson correlation of all metrics when aggregating results on the real world text datasets and all temperatures.
and IMDB datasets. The metrics are also evaluated on the train data itself which is called Real in the mentioned figures. Moreover, for Oracle dataset, since we have the probabilistic distribution of data, we can compute the likelihood of the generated samples by the model in the real distribution (i.e. Oracle) to find the quality of the generated samples. Therefore, the Oracle-NLL is used as quality measure of the methods on the synthetic dataset in Fig. 1d and Entropy is used as a diversity measure in this figure.</p>
<p>On the other hand, Figs. 2 and 3 present the performance of different methods (with $T=1$ ) on non-synthetic and synthetic datasets respectively.</p>
<p>It is worth noting that NLL, Entropy, and Bhattacharyya of Real could not be computed, since we do not have a model for real data and just considering training data as its samples. According to Fig. 2b, the ordering of the methods obtained by MSJaccard4 on these datasets is almost always consistent with the ordering of the methods according to their dominance in Figs. 1a to 1c. For example, in Fig. 1b that shows results on EMNLP2017 dataset, the best method which dominates others is MLE, the second best is MaliGAN, the third one is RankGAN, and SeqGAN is the last one that under-performs all other methods. Consistently, the proposed MS-Jaccard4 measure shown in Fig. 2b provides the same ordering. Moreover, the ordering of the methods according to FBD metric in Fig. 2c on different datasets is almost always consistent with their ordering obtained by analyzing the whole spectrum in Figs. 1a to 1c. For the oracle dataset 3, the proposed Bhattacharyya distance of the distributions introduced in Section 3.2.3 is consistent with the ordering obtained in Fig. 1d.</p>
<p>Finally, we display the Pearson correlation of different metrics on real datasets in Fig. 4. According to this figure, the proposed metrics for real-world datasets, i.e. 1-MS-Jaccard and FBD, are highly correlated. Besides, among the measures, these are the most correlated ones to NLL.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we first discussed shortcomings of the existing measures for evaluating text generation models. Then, we proposed some measures to more effectively specify the capability of models in generating both qualified and diverse texts. The MS-Jaccard as an n-gram based metric was firstly introduced that is capable of measuring both the quality and coverage of methods in text generation. Then, a feature-based metric FBD which is based on the BERT model was introduced. Moreover, for oracle training mode in which the generators density can also be calculated, we proposed to use (estimation of) divergences like Bhattacharyya defined on probability distributions as a metric to compute the distance of the generative model and the oracle. Finally, the performance of different text generation models were evaluated, the obtained results were analyzed and showed that the proposed metrics have high correlations and are almost consistent with the dominance ordering of models in quality-diversity spectrum.</p>
<h2>References</h2>
<p>Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 1171-1179.</p>
<p>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O'Reilly.</p>
<p>Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. 2017. Findings of the 2017 conference on machine translation (WMT17). In Proceedings of the Second Conference on Machine Translation, WMT 2017, Copenhagen, Denmark, September 78, 2017, pages 169-214. Association for Computational Linguistics.</p>
<p>Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin. 2018. Language gans falling short. CoRR, abs/1811.02549.</p>
<p>Tong Che, Yanran Li, Ruixiang Zhang, R. Devon Hjelm, Wenjie Li, Yangqiu Song, and Yoshua Bengio. 2017. Maximum-likelihood augmented discrete generative adversarial networks. CoRR, abs/1702.07983.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>Ian J. Goodfellow. 2017. NIPS 2016 tutorial: Generative adversarial networks. CoRR, abs/1701.00160.</p>
<p>Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. Generative adversarial networks. CoRR, abs/1406.2661.</p>
<p>Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. 2018. Long text generation via adversarial training with leaked information. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018. AAAI Press.</p>
<p>Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, pages 6626-6637.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Computation, $9(8): 1735-1780$.</p>
<p>Ferenc Huszar. 2015. How (not) to train your generative model: Scheduled sampling, likelihood, adversary? CoRR, abs/1511.05101.</p>
<p>Kevin Lin, Dianqi Li, Xiaodong He, Ming-Ting Sun, and Zhengyou Zhang. 2017. Adversarial ranking for language generation. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 49 December 2017, Long Beach, CA, USA, pages 3158-3168.</p>
<p>Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft COCO: common objects in context. In Computer Vision ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, volume 8693 of Lecture Notes in Computer Science, pages 740-755. Springer.</p>
<p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA, pages 142-150. The Association for Computer Linguistics.</p>
<p>Luke Metz, Ben Poole, David Pfau, and Jascha SohlDickstein. 2016. Unrolled generative adversarial networks. CoRR, abs/1611.02163.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA., pages 311-318. ACL.</p>
<p>Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence generative adversarial nets with policy gradient. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA., pages 2852-2858. AAAI Press.</p>
<p>Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, and Lawrence Carin. 2017. Adversarial feature matching for text generation. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 4006-4015. PMLR.</p>
<p>Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. 2018. Texygen: A benchmarking platform for text generation models. SIGIR.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ http://statmt.org/wmt17/translation-task.html
${ }^{2}$ https://github.com/LantaoYu/SeqGAN/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ https://github.com/geek-ai/Texygen
${ }^{4}$ https://www.nltk.org/ modules/nltk/
${ }^{5}$ https://github.com/Danial-Alh/FastBLEU&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
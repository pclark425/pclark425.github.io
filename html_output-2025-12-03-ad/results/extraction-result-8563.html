<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8563 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8563</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8563</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-d6f152012b4c5244f7d6c18e655f16120ff33cde</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d6f152012b4c5244f7d6c18e655f16120ff33cde" target="_blank">Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper introduces Geoperception, a benchmark designed to evaluate an MLLM's ability to accurately transcribe 2D geometric information from an image, and develops Euclid, a family of models specifically optimized for strong low-level geometric perception.</p>
                <p><strong>Paper Abstract:</strong> Multimodal large language models (MLLMs) have made rapid progress in recent years, yet continue to struggle with low-level visual perception (LLVP) -- particularly the ability to accurately describe the geometric details of an image. This capability is crucial for applications in areas such as robotics, medical image analysis, and manufacturing. In this paper, we first introduce Geoperception, a benchmark designed to evaluate an MLLM's ability to accurately transcribe 2D geometric information from an image. Using this benchmark, we demonstrate the limitations of leading MLLMs, and then conduct a comprehensive empirical study to explore strategies for improving their performance on geometric tasks. Our findings highlight the benefits of certain model architectures, training techniques, and data strategies, including the use of high-fidelity synthetic data and multi-stage training with a data curriculum. Notably, we find that a data curriculum enables models to learn challenging geometry understanding tasks which they fail to learn from scratch. Leveraging these insights, we develop Euclid, a family of models specifically optimized for strong low-level geometric perception. Although purely trained on synthetic multimodal data, Euclid shows strong generalization ability to novel geometry shapes. For instance, Euclid outperforms the best closed-source model, Gemini-1.5-Pro, by up to 58.56% on certain Geoperception benchmark tasks and 10.65% on average across all tasks.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8563.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8563.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o-mini (vision off)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o-mini (text-only baseline used in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source OpenAI LLM used as a text-only baseline for Geoperception (no image input). Provided the random/text-only baseline scores reported in Table 2 and Table 4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source OpenAI model; in this paper used both as a multimodal model (when images provided) and as a text-only baseline (no image input) to produce a random baseline under the same textual instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geoperception (geometry diagram perception benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D geometric diagram perception: identifying points on lines/circles, parallel/perpendicular relations, annotated lengths/angles, and line-length comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Questions converted from logical forms into QA text pairs; images padded to square and fed to MLLMs. For text-only baseline, identical textual prompts used but no image input. Evaluation uses set-based scoring: score = |P/G| if P ⊆ G else 0. Prompt templates for each task provided in Appendix B.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No special spatial strategy described — used as a text-only LLM baseline under the same textual instructions; does not receive image input in baseline condition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Random/text-only baseline overall average score: 16.50 (Table 2). Per-task: POL 1.35, POC 2.63, ALC 59.92, LHC 51.36, PEP 0.23, PRA 0.00, EQL 0.02 (percent-style evaluation scores reported in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Serves as a baseline showing that for some numeric tasks (AngleClassification, LineComparison) text-only answers can be competitive; indicates that vision models often underutilize visual cues and models exhibit a language prior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared directly to multimodal MLLMs in Table 2; many multimodal models did not beat the text-only baseline on AngleClassification.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>As a text-only baseline it cannot perceive visual diagrams; low/near-zero scores on annotated tasks demonstrate inability to use visual information.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8563.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8563.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI multimodal model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source OpenAI multimodal LLM evaluated on Geoperception tasks; included as a leading commercial MLLM in the benchmark table.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source multimodal LLM from OpenAI; evaluated by providing padded images and the geometry question prompts from Geoperception.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geoperception (geometry diagram perception benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D geometric diagram perception</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same Geoperception QA prompts and images padded to square; outputs evaluated with the set-based scoring metric (P ⊆ G rule).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard multimodal inference; no specialized spatial algorithm or chain-of-thought reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall average score: 49.68. Per-task (from Table 2): POL 16.43, POC 71.49, ALC 55.63, LHC 74.39, PEP 24.80, PRA 60.30, EQL 44.69 (percent-style evaluation scores).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Mixed — performs well on some numeric/annotated tasks (e.g., PointLiesOnCircle, LineComparison) but low on some logical tasks; shows partial spatial perception but gaps remain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperformed many open-source models overall but was outperformed by Gemini-1.5-Pro and the Euclid models on average and on several tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails to achieve strong performance on PointLiesOnLine (16.43%) and shows inconsistent visual perception across tasks; authors note frontier commercial MLLMs still struggle with low-level visual perception (LLVP).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8563.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8563.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.5 Sonnet (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source multimodal LLM from Anthropic evaluated on the Geoperception benchmark, included in the model comparison table.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic's Claude 3.5 family model evaluated with image inputs and the Geoperception prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geoperception (geometry diagram perception benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D geometric diagram perception</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Standard MLLM evaluation on the seven Geoperception tasks with images padded to square and the prompt templates from Appendix B; scoring via set containment metric.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No additional spatial strategies described in the paper; used as-is for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall average score: 51.30. Per-task (Table 2): POL 25.44, POC 68.34, ALC 42.95, LHC 70.73, PEP 21.41, PRA 63.92, EQL 66.34.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Shows reasonable performance on annotated and some logical tasks, indicating partial low-level visual perception capabilities, but still has notable failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Better than many open-source models; lower than Gemini-1.5-Pro overall and lower than Euclid models on several tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower performance on AngleClassification relative to some other models; overall still demonstrates LLVP shortcomings noted in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8563.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8563.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-1.5-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-1.5-Pro (Google/DeepMind family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source, high-performing multimodal model included as the best closed-source baseline in the Geoperception benchmark; used for cross-model comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.5-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Commercial closed-source multimodal LLM from the Gemini family; reported as the top closed-source performer in the Geoperception evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geoperception (geometry diagram perception benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D geometric diagram perception</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Evaluated on the seven Geoperception tasks with images padded to square and standard QA prompts; scoring via the paper's evaluation metric.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No task-specific spatial solving strategies described in the paper; used as an off-the-shelf multimodal system.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall average score: 56.98 (Table 2). Per-task: POL 24.42, POC 69.80, ALC 57.96, LHC 79.05, PEP 38.81, PRA 76.65, EQL 52.15.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Strong relative performance on many annotation and numeric tasks (e.g., LineComparison 79.05%), indicating reasonable spatial perception in annotated diagrams but still below Euclid on some logical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Top closed-source model in Table 2; outperformed by Euclid models (Euclid-ConvNeXt-Large and XXLarge) by substantial margins on some tasks (e.g., PointLiesOnLine). Paper reports Euclid outperforming the best closed-source model by up to 58.56% on certain Geoperception tasks and 10.65% on average.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Still struggles on certain low-level geometry tasks (PointLiesOnLine remains challenging across models); demonstrates the general LLVP limitations discussed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8563.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8563.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-1.5-Flash</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-1.5-Flash</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source variant of the Gemini family evaluated on the Geoperception benchmark; strong performer on annotated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.5-Flash</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Commercial multimodal LLM (Gemini family) evaluated on the Geoperception tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geoperception (geometry diagram perception benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D geometric diagram perception</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Standard Geoperception QA prompts and padded images; scoring identical to other evaluated MLLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No specialized puzzle-solving mechanisms reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall average score: 54.76. Per-task: POL 29.30, POC 67.75, ALC 49.89, LHC 76.69, PEP 29.98, PRA 63.44, EQL 66.28.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Performs well on many annotation tasks and LineComparison, indicating some spatial perception ability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Slightly below Gemini-1.5-Pro overall and below Euclid models on average.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Similar LLVP weaknesses as other commercial models; inconsistent across task types.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8563.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8563.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pixtral-12B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pixtral-12B (Mistral / Pixtral multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source multimodal model (12B) included among the evaluated open-source models; best-performing open-source baseline in the table.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pixtral-12B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 12B multimodal model (Pixtral family) evaluated on Geoperception tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>12B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geoperception (geometry diagram perception benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D geometric diagram perception</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Standard evaluation on the seven Geoperception tasks using the paper's prompts and image preprocessing; scoring via set-based metric.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No specialized spatial strategies described.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall average score: 41.95. Per-task: POL 24.63, POC 53.21, ALC 47.33, LHC 51.43, PEP 21.96, PRA 36.64, EQL 58.41.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Shows moderate ability on some tasks (e.g., Equals) but weaker on others; indicates partial LLVP ability for open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Best among open-source models in Table 2 but substantially below closed-source top performers and well below Euclid.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles with PointLiesOnLine and other low-level tasks relative to Euclid and best closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8563.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8563.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2-VL-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2-VL-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source multimodal model (7B) evaluated on the Geoperception benchmark; included in open-source comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2-VL-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B multimodal model from the Qwen family evaluated on geometric perception tasks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geoperception (geometry diagram perception benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D geometric diagram perception</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Standard MLLM evaluation with padded images and Geoperception prompts; scoring uses the paper's evaluation metric.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No special spatial mechanisms described.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall average score: 40.62. Per-task: POL 21.89, POC 41.60, ALC 46.60, LHC 63.27, PEP 26.41, PRA 30.19, EQL 54.37.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Performs better on LineComparison and Equals, indicating ability to handle some numeric/annotated spatial queries but weaker on point-on-line tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperforms some open-source models on numerical and annotated tasks but below top closed-source performers and Euclid.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low performance on PointLiesOnLine; exhibits the general LLVP weaknesses discussed by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8563.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8563.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cambrian-1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cambrian-1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source, vision-centric MLLM (8B) evaluated on Geoperception; reported to have been trained on geometry datasets elsewhere but still challenged on this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Cambrian-1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 8B vision-centric multimodal LLM evaluated on Geoperception; the paper notes it was reported to be trained on Geo-170K in other work yet still struggles on Geoperception.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geoperception (geometry diagram perception benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D geometric diagram perception</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Standard Geoperception prompts, images padded to square, evaluated with set-based scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No additional strategy reported; evaluated off-the-shelf.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall average score: 35.44. Per-task: POL 15.14, POC 28.68, ALC 58.05, LHC 61.48, PEP 22.96, PRA 30.74, EQL 31.04.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Despite reported prior geometry training, performs inconsistently and highlights that prior dataset exposure does not guarantee strong LLVP in this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Lower than top closed-source models and Euclid; paper highlights the surprising gap despite reported geometry training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles on PointLiesOnLine and PointLiesOnCircle relative to some other models; demonstrates persistent LLVP limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8563.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8563.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.2-11B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.2-11B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source LLM (11B) variant evaluated within MLLM configurations on Geoperception; included in open-source comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.2-11B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>11B Llama 3.2 family model (decoder-only LLM) used within a multimodal evaluation context in the Geoperception benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geoperception (geometry diagram perception benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D geometric diagram perception</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Images padded to square; standard prompts and evaluation metric applied.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No specialized spatial solving approaches reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall average score: 35.08. Per-task: POL 16.22, POC 37.12, ALC 59.46, LHC 52.08, PEP 8.38, PRA 22.41, EQL 49.86.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Performs relatively well on AngleClassification and Equals, but poor on annotation/perpendicular tasks, indicating partial spatial understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Below top closed-source models; mixed performance across task types.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Limited performance on annotation-heavy tasks and PointLiesOnLine.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8563.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8563.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Molmo-7B-D</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molmo-7B-D</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source multimodal model (7B) evaluated on Geoperception; included in the open-source model comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Molmo-7B-D</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B open-source multimodal model evaluated on the Geoperception benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geoperception (geometry diagram perception benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D geometric diagram perception</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Standard Geoperception evaluation (prompts, padded images, set-based scoring).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No specialized spatial methods described.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall average score: 17.59. Per-task: POL 11.96, POC 35.73, ALC 56.77, LHC 16.79, PEP 1.06, PRA 0.00, EQL 0.81.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Very limited performance on annotation tasks; weak LLVP capabilities in this model per the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>One of the lower-performing open-source models on Geoperception.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Particularly poor on annotated geometry (Parallel/Equals/Perpendicular).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8563.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8563.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Euclid-ConvNeXt-Large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Euclid (ConvNeXt-Large visual encoder variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Model family introduced in this paper, trained purely on synthetic high-fidelity multimodal geometry data with curriculum learning to substantially improve low-level geometric perception.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Euclid-ConvNeXt-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>MLLM trained in this work: visual encoder ConvNeXt-Large@512 (CLIP), multimodal connector = 2-layer MLP, LLM = Qwen2.5-1.5B-instruct. Trained with on-the-fly progressive curriculum over N=3 stages, M=50 rounds, K=500 steps/round, batch size 64; total synthetic dataset volume 1.6M. Visual encoder kept frozen during training in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>LLM: 1.5B; Visual encoder params: ConvNeXt-Large ~200M (as reported in Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geoperception (geometry diagram perception benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D geometric diagram perception</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Trained purely on synthetic synthetic multimodal data generated by a geometry image generation engine (AlphaGeometry-derived) with progressively complex shapes. Evaluated on Geoperception tasks with images padded to square and prompts from Appendix B; scoring via the paper's set-based evaluation metric.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Key strategies: high-fidelity synthetic visual descriptions, curriculum (multi-stage) training from easy→medium→hard with dynamic stage advancement threshold θ=0.99 and exponential data smoothing; ConvNeXt visual encoder selected after empirical comparison (outperformed ViTs); multimodal connector 2-layer MLP; frozen visual encoder during LLM tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall average score: 64.93 (Table 4). Per-task: POL 80.54, POC 57.76, ALC 86.37, LHC 88.24, PEP 42.23, PRA 64.94, EQL 34.45 (percent-style evaluation scores).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Substantial empirical evidence: dramatic improvements on PointLiesOnLine (POL) from <30% for prior models to 80.54% for Euclid, and strong gains on numerical tasks (ALC, LHC), indicating successful learning of low-level geometric perception from synthetic curriculum; ablation-style lessons in Section 4 show curriculum and ConvNeXt encoder choices were crucial.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperforms leading closed-source and open-source MLLMs in most tasks; e.g., Euclid-ConvNeXt-Large average 64.93 vs Gemini-1.5-Pro 56.98 and Pixtral-12B 41.95. Paper reports Euclid outperforms best closed-source model by up to 58.56% on certain tasks and 10.65% on average.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Weaker on annotation-heavy tasks (Equals, Perpendicular, Parallel) compared to some closed-source models; error analysis shows struggles when diagrams are heavily annotated (e.g., annotated 'x' on a line confuses the model). Authors attribute some limits to restricted annotation styles in synthetic training data and propose expanding diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8563.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8563.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Euclid-ConvNeXt-XXLarge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Euclid (ConvNeXt-XXLarge visual encoder variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Stronger variant of Euclid introduced in this paper using ConvNeXt-XXLarge encoder; trained on the same synthetic curriculum and shows further improved Geoperception performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Euclid-ConvNeXt-XXLarge</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>MLLM trained in this work: visual encoder ConvNeXt-XXLarge@512 (CLIP), multimodal connector = 2-layer MLP, LLM = Qwen2.5-1.5B-instruct; trained with the same curriculum strategy and dataset volume as other Euclid variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>LLM: 1.5B; Visual encoder params: ConvNeXt-XXLarge ~847M (Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geoperception (geometry diagram perception benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D geometric diagram perception</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same as Euclid-ConvNeXt-Large: trained on synthetic curriculum and evaluated on Geoperception with padded images and the paper's prompt templates.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Same curriculum and synthetic-data strategies as Euclid-ConvNeXt-Large; larger ConvNeXt visual encoder chosen based on empirical encoder comparisons favoring ConvNeXt over ViT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall average score: 67.89 (Table 4). Per-task: POL 82.98, POC 61.45, ALC 90.56, LHC 90.82, PEP 46.96, PRA 70.52, EQL 31.94 (percent-style evaluation scores).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Highest reported scores in the paper across many tasks (notably POL 82.98 and ALC 90.56), providing strong evidence of learned spatial perception; empirical studies in Section 4 attribute success to ConvNeXt encoders and curriculum learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Best performing model in the paper on average (67.89) and across many tasks; reported to outperform Gemini-1.5-Pro by up to 58.56% on certain tasks and by 10.65% on average per the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Similar limitations to the other Euclid variant: lower performance on some annotation tasks (Equals ~31.94) and susceptibility to heavy/novel annotation styles not present in synthetic training data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? <em>(Rating: 2)</em></li>
                <li>G-llava: Solving geometric problem with multi-modal large language model <em>(Rating: 2)</em></li>
                <li>MAVIS: Mathematical visual instruction tuning <em>(Rating: 2)</em></li>
                <li>Geomverse: A systematic evaluation of large models for geometric reasoning <em>(Rating: 2)</em></li>
                <li>Blink: Multimodal large language models can see but not perceive <em>(Rating: 2)</em></li>
                <li>VDLM (Vision-to-Vector / SVG) / Text-based reasoning about vector graphics <em>(Rating: 1)</em></li>
                <li>Marvel: Multidimensional abstraction and reasoning through visual evaluation and learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8563",
    "paper_id": "paper-d6f152012b4c5244f7d6c18e655f16120ff33cde",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "GPT-4o-mini (vision off)",
            "name_full": "GPT-4o-mini (text-only baseline used in this paper)",
            "brief_description": "Closed-source OpenAI LLM used as a text-only baseline for Geoperception (no image input). Provided the random/text-only baseline scores reported in Table 2 and Table 4.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini",
            "model_description": "Closed-source OpenAI model; in this paper used both as a multimodal model (when images provided) and as a text-only baseline (no image input) to produce a random baseline under the same textual instructions.",
            "model_size": null,
            "puzzle_name": "Geoperception (geometry diagram perception benchmark)",
            "puzzle_type": "2D geometric diagram perception: identifying points on lines/circles, parallel/perpendicular relations, annotated lengths/angles, and line-length comparisons",
            "task_setup": "Questions converted from logical forms into QA text pairs; images padded to square and fed to MLLMs. For text-only baseline, identical textual prompts used but no image input. Evaluation uses set-based scoring: score = |P/G| if P ⊆ G else 0. Prompt templates for each task provided in Appendix B.",
            "mechanisms_or_strategies": "No special spatial strategy described — used as a text-only LLM baseline under the same textual instructions; does not receive image input in baseline condition.",
            "performance_metrics": "Random/text-only baseline overall average score: 16.50 (Table 2). Per-task: POL 1.35, POC 2.63, ALC 59.92, LHC 51.36, PEP 0.23, PRA 0.00, EQL 0.02 (percent-style evaluation scores reported in paper tables).",
            "evidence_of_spatial_reasoning": "Serves as a baseline showing that for some numeric tasks (AngleClassification, LineComparison) text-only answers can be competitive; indicates that vision models often underutilize visual cues and models exhibit a language prior.",
            "comparisons": "Compared directly to multimodal MLLMs in Table 2; many multimodal models did not beat the text-only baseline on AngleClassification.",
            "limitations_or_failure_cases": "As a text-only baseline it cannot perceive visual diagrams; low/near-zero scores on annotated tasks demonstrate inability to use visual information.",
            "uuid": "e8563.0",
            "source_info": {
                "paper_title": "Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-4o (multimodal)",
            "name_full": "GPT-4o (OpenAI multimodal model)",
            "brief_description": "Closed-source OpenAI multimodal LLM evaluated on Geoperception tasks; included as a leading commercial MLLM in the benchmark table.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Closed-source multimodal LLM from OpenAI; evaluated by providing padded images and the geometry question prompts from Geoperception.",
            "model_size": null,
            "puzzle_name": "Geoperception (geometry diagram perception benchmark)",
            "puzzle_type": "2D geometric diagram perception",
            "task_setup": "Same Geoperception QA prompts and images padded to square; outputs evaluated with the set-based scoring metric (P ⊆ G rule).",
            "mechanisms_or_strategies": "Standard multimodal inference; no specialized spatial algorithm or chain-of-thought reported in this paper.",
            "performance_metrics": "Overall average score: 49.68. Per-task (from Table 2): POL 16.43, POC 71.49, ALC 55.63, LHC 74.39, PEP 24.80, PRA 60.30, EQL 44.69 (percent-style evaluation scores).",
            "evidence_of_spatial_reasoning": "Mixed — performs well on some numeric/annotated tasks (e.g., PointLiesOnCircle, LineComparison) but low on some logical tasks; shows partial spatial perception but gaps remain.",
            "comparisons": "Outperformed many open-source models overall but was outperformed by Gemini-1.5-Pro and the Euclid models on average and on several tasks.",
            "limitations_or_failure_cases": "Fails to achieve strong performance on PointLiesOnLine (16.43%) and shows inconsistent visual perception across tasks; authors note frontier commercial MLLMs still struggle with low-level visual perception (LLVP).",
            "uuid": "e8563.1",
            "source_info": {
                "paper_title": "Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Claude 3.5 Sonnet",
            "name_full": "Claude 3.5 Sonnet (Anthropic)",
            "brief_description": "Closed-source multimodal LLM from Anthropic evaluated on the Geoperception benchmark, included in the model comparison table.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude 3.5 Sonnet",
            "model_description": "Anthropic's Claude 3.5 family model evaluated with image inputs and the Geoperception prompts.",
            "model_size": null,
            "puzzle_name": "Geoperception (geometry diagram perception benchmark)",
            "puzzle_type": "2D geometric diagram perception",
            "task_setup": "Standard MLLM evaluation on the seven Geoperception tasks with images padded to square and the prompt templates from Appendix B; scoring via set containment metric.",
            "mechanisms_or_strategies": "No additional spatial strategies described in the paper; used as-is for evaluation.",
            "performance_metrics": "Overall average score: 51.30. Per-task (Table 2): POL 25.44, POC 68.34, ALC 42.95, LHC 70.73, PEP 21.41, PRA 63.92, EQL 66.34.",
            "evidence_of_spatial_reasoning": "Shows reasonable performance on annotated and some logical tasks, indicating partial low-level visual perception capabilities, but still has notable failure modes.",
            "comparisons": "Better than many open-source models; lower than Gemini-1.5-Pro overall and lower than Euclid models on several tasks.",
            "limitations_or_failure_cases": "Lower performance on AngleClassification relative to some other models; overall still demonstrates LLVP shortcomings noted in the paper.",
            "uuid": "e8563.2",
            "source_info": {
                "paper_title": "Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Gemini-1.5-Pro",
            "name_full": "Gemini-1.5-Pro (Google/DeepMind family)",
            "brief_description": "Closed-source, high-performing multimodal model included as the best closed-source baseline in the Geoperception benchmark; used for cross-model comparison.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-1.5-Pro",
            "model_description": "Commercial closed-source multimodal LLM from the Gemini family; reported as the top closed-source performer in the Geoperception evaluation.",
            "model_size": null,
            "puzzle_name": "Geoperception (geometry diagram perception benchmark)",
            "puzzle_type": "2D geometric diagram perception",
            "task_setup": "Evaluated on the seven Geoperception tasks with images padded to square and standard QA prompts; scoring via the paper's evaluation metric.",
            "mechanisms_or_strategies": "No task-specific spatial solving strategies described in the paper; used as an off-the-shelf multimodal system.",
            "performance_metrics": "Overall average score: 56.98 (Table 2). Per-task: POL 24.42, POC 69.80, ALC 57.96, LHC 79.05, PEP 38.81, PRA 76.65, EQL 52.15.",
            "evidence_of_spatial_reasoning": "Strong relative performance on many annotation and numeric tasks (e.g., LineComparison 79.05%), indicating reasonable spatial perception in annotated diagrams but still below Euclid on some logical tasks.",
            "comparisons": "Top closed-source model in Table 2; outperformed by Euclid models (Euclid-ConvNeXt-Large and XXLarge) by substantial margins on some tasks (e.g., PointLiesOnLine). Paper reports Euclid outperforming the best closed-source model by up to 58.56% on certain Geoperception tasks and 10.65% on average.",
            "limitations_or_failure_cases": "Still struggles on certain low-level geometry tasks (PointLiesOnLine remains challenging across models); demonstrates the general LLVP limitations discussed in the paper.",
            "uuid": "e8563.3",
            "source_info": {
                "paper_title": "Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Gemini-1.5-Flash",
            "name_full": "Gemini-1.5-Flash",
            "brief_description": "Closed-source variant of the Gemini family evaluated on the Geoperception benchmark; strong performer on annotated tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-1.5-Flash",
            "model_description": "Commercial multimodal LLM (Gemini family) evaluated on the Geoperception tasks.",
            "model_size": null,
            "puzzle_name": "Geoperception (geometry diagram perception benchmark)",
            "puzzle_type": "2D geometric diagram perception",
            "task_setup": "Standard Geoperception QA prompts and padded images; scoring identical to other evaluated MLLMs.",
            "mechanisms_or_strategies": "No specialized puzzle-solving mechanisms reported in the paper.",
            "performance_metrics": "Overall average score: 54.76. Per-task: POL 29.30, POC 67.75, ALC 49.89, LHC 76.69, PEP 29.98, PRA 63.44, EQL 66.28.",
            "evidence_of_spatial_reasoning": "Performs well on many annotation tasks and LineComparison, indicating some spatial perception ability.",
            "comparisons": "Slightly below Gemini-1.5-Pro overall and below Euclid models on average.",
            "limitations_or_failure_cases": "Similar LLVP weaknesses as other commercial models; inconsistent across task types.",
            "uuid": "e8563.4",
            "source_info": {
                "paper_title": "Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Pixtral-12B",
            "name_full": "Pixtral-12B (Mistral / Pixtral multimodal)",
            "brief_description": "Open-source multimodal model (12B) included among the evaluated open-source models; best-performing open-source baseline in the table.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Pixtral-12B",
            "model_description": "Open-source 12B multimodal model (Pixtral family) evaluated on Geoperception tasks.",
            "model_size": "12B",
            "puzzle_name": "Geoperception (geometry diagram perception benchmark)",
            "puzzle_type": "2D geometric diagram perception",
            "task_setup": "Standard evaluation on the seven Geoperception tasks using the paper's prompts and image preprocessing; scoring via set-based metric.",
            "mechanisms_or_strategies": "No specialized spatial strategies described.",
            "performance_metrics": "Overall average score: 41.95. Per-task: POL 24.63, POC 53.21, ALC 47.33, LHC 51.43, PEP 21.96, PRA 36.64, EQL 58.41.",
            "evidence_of_spatial_reasoning": "Shows moderate ability on some tasks (e.g., Equals) but weaker on others; indicates partial LLVP ability for open-source models.",
            "comparisons": "Best among open-source models in Table 2 but substantially below closed-source top performers and well below Euclid.",
            "limitations_or_failure_cases": "Struggles with PointLiesOnLine and other low-level tasks relative to Euclid and best closed-source models.",
            "uuid": "e8563.5",
            "source_info": {
                "paper_title": "Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Qwen2-VL-7B",
            "name_full": "Qwen2-VL-7B",
            "brief_description": "Open-source multimodal model (7B) evaluated on the Geoperception benchmark; included in open-source comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2-VL-7B",
            "model_description": "7B multimodal model from the Qwen family evaluated on geometric perception tasks in this paper.",
            "model_size": "7B",
            "puzzle_name": "Geoperception (geometry diagram perception benchmark)",
            "puzzle_type": "2D geometric diagram perception",
            "task_setup": "Standard MLLM evaluation with padded images and Geoperception prompts; scoring uses the paper's evaluation metric.",
            "mechanisms_or_strategies": "No special spatial mechanisms described.",
            "performance_metrics": "Overall average score: 40.62. Per-task: POL 21.89, POC 41.60, ALC 46.60, LHC 63.27, PEP 26.41, PRA 30.19, EQL 54.37.",
            "evidence_of_spatial_reasoning": "Performs better on LineComparison and Equals, indicating ability to handle some numeric/annotated spatial queries but weaker on point-on-line tasks.",
            "comparisons": "Outperforms some open-source models on numerical and annotated tasks but below top closed-source performers and Euclid.",
            "limitations_or_failure_cases": "Low performance on PointLiesOnLine; exhibits the general LLVP weaknesses discussed by the authors.",
            "uuid": "e8563.6",
            "source_info": {
                "paper_title": "Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Cambrian-1-8B",
            "name_full": "Cambrian-1-8B",
            "brief_description": "Open-source, vision-centric MLLM (8B) evaluated on Geoperception; reported to have been trained on geometry datasets elsewhere but still challenged on this benchmark.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Cambrian-1-8B",
            "model_description": "Open-source 8B vision-centric multimodal LLM evaluated on Geoperception; the paper notes it was reported to be trained on Geo-170K in other work yet still struggles on Geoperception.",
            "model_size": "8B",
            "puzzle_name": "Geoperception (geometry diagram perception benchmark)",
            "puzzle_type": "2D geometric diagram perception",
            "task_setup": "Standard Geoperception prompts, images padded to square, evaluated with set-based scoring.",
            "mechanisms_or_strategies": "No additional strategy reported; evaluated off-the-shelf.",
            "performance_metrics": "Overall average score: 35.44. Per-task: POL 15.14, POC 28.68, ALC 58.05, LHC 61.48, PEP 22.96, PRA 30.74, EQL 31.04.",
            "evidence_of_spatial_reasoning": "Despite reported prior geometry training, performs inconsistently and highlights that prior dataset exposure does not guarantee strong LLVP in this benchmark.",
            "comparisons": "Lower than top closed-source models and Euclid; paper highlights the surprising gap despite reported geometry training.",
            "limitations_or_failure_cases": "Struggles on PointLiesOnLine and PointLiesOnCircle relative to some other models; demonstrates persistent LLVP limitations.",
            "uuid": "e8563.7",
            "source_info": {
                "paper_title": "Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Llama-3.2-11B",
            "name_full": "Llama-3.2-11B",
            "brief_description": "Open-source LLM (11B) variant evaluated within MLLM configurations on Geoperception; included in open-source comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.2-11B",
            "model_description": "11B Llama 3.2 family model (decoder-only LLM) used within a multimodal evaluation context in the Geoperception benchmark.",
            "model_size": "11B",
            "puzzle_name": "Geoperception (geometry diagram perception benchmark)",
            "puzzle_type": "2D geometric diagram perception",
            "task_setup": "Images padded to square; standard prompts and evaluation metric applied.",
            "mechanisms_or_strategies": "No specialized spatial solving approaches reported.",
            "performance_metrics": "Overall average score: 35.08. Per-task: POL 16.22, POC 37.12, ALC 59.46, LHC 52.08, PEP 8.38, PRA 22.41, EQL 49.86.",
            "evidence_of_spatial_reasoning": "Performs relatively well on AngleClassification and Equals, but poor on annotation/perpendicular tasks, indicating partial spatial understanding.",
            "comparisons": "Below top closed-source models; mixed performance across task types.",
            "limitations_or_failure_cases": "Limited performance on annotation-heavy tasks and PointLiesOnLine.",
            "uuid": "e8563.8",
            "source_info": {
                "paper_title": "Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Molmo-7B-D",
            "name_full": "Molmo-7B-D",
            "brief_description": "Open-source multimodal model (7B) evaluated on Geoperception; included in the open-source model comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Molmo-7B-D",
            "model_description": "7B open-source multimodal model evaluated on the Geoperception benchmark.",
            "model_size": "7B",
            "puzzle_name": "Geoperception (geometry diagram perception benchmark)",
            "puzzle_type": "2D geometric diagram perception",
            "task_setup": "Standard Geoperception evaluation (prompts, padded images, set-based scoring).",
            "mechanisms_or_strategies": "No specialized spatial methods described.",
            "performance_metrics": "Overall average score: 17.59. Per-task: POL 11.96, POC 35.73, ALC 56.77, LHC 16.79, PEP 1.06, PRA 0.00, EQL 0.81.",
            "evidence_of_spatial_reasoning": "Very limited performance on annotation tasks; weak LLVP capabilities in this model per the paper.",
            "comparisons": "One of the lower-performing open-source models on Geoperception.",
            "limitations_or_failure_cases": "Particularly poor on annotated geometry (Parallel/Equals/Perpendicular).",
            "uuid": "e8563.9",
            "source_info": {
                "paper_title": "Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Euclid-ConvNeXt-Large",
            "name_full": "Euclid (ConvNeXt-Large visual encoder variant)",
            "brief_description": "Model family introduced in this paper, trained purely on synthetic high-fidelity multimodal geometry data with curriculum learning to substantially improve low-level geometric perception.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Euclid-ConvNeXt-Large",
            "model_description": "MLLM trained in this work: visual encoder ConvNeXt-Large@512 (CLIP), multimodal connector = 2-layer MLP, LLM = Qwen2.5-1.5B-instruct. Trained with on-the-fly progressive curriculum over N=3 stages, M=50 rounds, K=500 steps/round, batch size 64; total synthetic dataset volume 1.6M. Visual encoder kept frozen during training in reported experiments.",
            "model_size": "LLM: 1.5B; Visual encoder params: ConvNeXt-Large ~200M (as reported in Table 3)",
            "puzzle_name": "Geoperception (geometry diagram perception benchmark)",
            "puzzle_type": "2D geometric diagram perception",
            "task_setup": "Trained purely on synthetic synthetic multimodal data generated by a geometry image generation engine (AlphaGeometry-derived) with progressively complex shapes. Evaluated on Geoperception tasks with images padded to square and prompts from Appendix B; scoring via the paper's set-based evaluation metric.",
            "mechanisms_or_strategies": "Key strategies: high-fidelity synthetic visual descriptions, curriculum (multi-stage) training from easy→medium→hard with dynamic stage advancement threshold θ=0.99 and exponential data smoothing; ConvNeXt visual encoder selected after empirical comparison (outperformed ViTs); multimodal connector 2-layer MLP; frozen visual encoder during LLM tuning.",
            "performance_metrics": "Overall average score: 64.93 (Table 4). Per-task: POL 80.54, POC 57.76, ALC 86.37, LHC 88.24, PEP 42.23, PRA 64.94, EQL 34.45 (percent-style evaluation scores).",
            "evidence_of_spatial_reasoning": "Substantial empirical evidence: dramatic improvements on PointLiesOnLine (POL) from &lt;30% for prior models to 80.54% for Euclid, and strong gains on numerical tasks (ALC, LHC), indicating successful learning of low-level geometric perception from synthetic curriculum; ablation-style lessons in Section 4 show curriculum and ConvNeXt encoder choices were crucial.",
            "comparisons": "Outperforms leading closed-source and open-source MLLMs in most tasks; e.g., Euclid-ConvNeXt-Large average 64.93 vs Gemini-1.5-Pro 56.98 and Pixtral-12B 41.95. Paper reports Euclid outperforms best closed-source model by up to 58.56% on certain tasks and 10.65% on average.",
            "limitations_or_failure_cases": "Weaker on annotation-heavy tasks (Equals, Perpendicular, Parallel) compared to some closed-source models; error analysis shows struggles when diagrams are heavily annotated (e.g., annotated 'x' on a line confuses the model). Authors attribute some limits to restricted annotation styles in synthetic training data and propose expanding diversity.",
            "uuid": "e8563.10",
            "source_info": {
                "paper_title": "Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Euclid-ConvNeXt-XXLarge",
            "name_full": "Euclid (ConvNeXt-XXLarge visual encoder variant)",
            "brief_description": "Stronger variant of Euclid introduced in this paper using ConvNeXt-XXLarge encoder; trained on the same synthetic curriculum and shows further improved Geoperception performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Euclid-ConvNeXt-XXLarge",
            "model_description": "MLLM trained in this work: visual encoder ConvNeXt-XXLarge@512 (CLIP), multimodal connector = 2-layer MLP, LLM = Qwen2.5-1.5B-instruct; trained with the same curriculum strategy and dataset volume as other Euclid variants.",
            "model_size": "LLM: 1.5B; Visual encoder params: ConvNeXt-XXLarge ~847M (Table 3)",
            "puzzle_name": "Geoperception (geometry diagram perception benchmark)",
            "puzzle_type": "2D geometric diagram perception",
            "task_setup": "Same as Euclid-ConvNeXt-Large: trained on synthetic curriculum and evaluated on Geoperception with padded images and the paper's prompt templates.",
            "mechanisms_or_strategies": "Same curriculum and synthetic-data strategies as Euclid-ConvNeXt-Large; larger ConvNeXt visual encoder chosen based on empirical encoder comparisons favoring ConvNeXt over ViT.",
            "performance_metrics": "Overall average score: 67.89 (Table 4). Per-task: POL 82.98, POC 61.45, ALC 90.56, LHC 90.82, PEP 46.96, PRA 70.52, EQL 31.94 (percent-style evaluation scores).",
            "evidence_of_spatial_reasoning": "Highest reported scores in the paper across many tasks (notably POL 82.98 and ALC 90.56), providing strong evidence of learned spatial perception; empirical studies in Section 4 attribute success to ConvNeXt encoders and curriculum learning.",
            "comparisons": "Best performing model in the paper on average (67.89) and across many tasks; reported to outperform Gemini-1.5-Pro by up to 58.56% on certain tasks and by 10.65% on average per the authors.",
            "limitations_or_failure_cases": "Similar limitations to the other Euclid variant: lower performance on some annotation tasks (Equals ~31.94) and susceptibility to heavy/novel annotation styles not present in synthetic training data.",
            "uuid": "e8563.11",
            "source_info": {
                "paper_title": "Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?",
            "rating": 2
        },
        {
            "paper_title": "G-llava: Solving geometric problem with multi-modal large language model",
            "rating": 2
        },
        {
            "paper_title": "MAVIS: Mathematical visual instruction tuning",
            "rating": 2
        },
        {
            "paper_title": "Geomverse: A systematic evaluation of large models for geometric reasoning",
            "rating": 2
        },
        {
            "paper_title": "Blink: Multimodal large language models can see but not perceive",
            "rating": 2
        },
        {
            "paper_title": "VDLM (Vision-to-Vector / SVG) / Text-based reasoning about vector graphics",
            "rating": 1
        },
        {
            "paper_title": "Marvel: Multidimensional abstraction and reasoning through visual evaluation and learning",
            "rating": 1
        }
    ],
    "cost": 0.020109000000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions</h1>
<p>Jiarui Zhang ${ }^{\ominus}$, Ollie Liu<em>, Tianyu Yu</em>, Jinyi Hu<em>, Willie Neiswanger</em><br><em>University of Southern California, ${ }^{</em>}$ Tsinghua University</p>
<h4>Abstract</h4>
<p>Multimodal large language models (MLLMs) have made rapid progress in recent years, yet continue to struggle with low-level visual perception (LLVP)—particularly the ability to accurately describe the geometric details of an image. This capability is crucial for applications in areas such as robotics, medical image analysis, and manufacturing. In this paper, we first introduce Geigerception, a benchmark designed to evaluate an MLLM's ability to accurately transcribe 2D geometric information from an image. Using this benchmark, we demonstrate the limitations of leading MLLMs, and then conduct a comprehensive empirical study to explore strategies for improving their performance on geometric tasks. Our findings highlight the benefits of certain model architectures, training techniques, and data strategies, including the use of high-fidelity synthetic data and multi-stage training with a data curriculum. Notably, we find that a data curriculum enables models to learn challenging geometry understanding tasks which they fail to learn from scratch. Leveraging these insights, we develop Euclid, a family of models specifically optimized for strong low-level geometric perception. Although purely trained on synthetic multimodal data, Euclid shows strong generalization ability to novel geometry shapes. For instance, Euclid outperforms the best closed-source model, Gemini-1.5-Pro, by up to $58.56 \%$ on certain Geigerception benchmark tasks and $10.65 \%$ on average across all tasks.</p>
<p>Website: euclid-multimodal.github.io
Model Weights \&amp; Datasets: huggingface.co/euclid-multimodal
(1) Code Repository: github.com/euclid-multimodal/Euclid</p>
<h2>1. Introduction</h2>
<p>Multimodal large language models (MLLMs) have rapidly progressed in recent years, demonstrating remarkable potential in understanding and reasoning about the visual world through the powerful capabilities of large language models (LLMs) (Liu et al., 2024c,a, Achiam et al., 2023, Team et al., 2023, Hu et al., 2023, Tong et al., 2024a, Wang et al., 2024a). These models have showcased strong performance in tasks such as visual question answering (VQA) (Goyal et al., 2017), image captioning (Lin et al., 2014), and multimodal reasoning (Liu et al., 2023). As one recent example, LLaVA-NeXT-34B (Liu et al., 2024b) achieves an impressive $83.7 \%$ accuracy on the VQAv2 benchmark (Goyal et al., 2017), a comprehensive benchmark on natural image question answering.</p>
<p>While MLLMs achieve impressive results on tasks like VQA, their performance relies on high-level semantic extraction (Tong et al., 2024b); in contrast, they often fall short on low-level visual perception (LLVP)—i.e., the ability to accurately describe the geometric details of an image, such as the points, lines, angles, shapes, and spatial relationships among its constituent objects. This limitation becomes especially apparent in tasks requiring precise descriptions, such as mathematical visual problem solving (Zhang et al., 2024a, Lu et al., 2023), scientific visual understanding (Yue et al., 2024, Fu et al., 2024a), abstract visual reasoning (Jiang et al., 2024, Ahrabian et al., 2024), and even simple visual comprehension (Rahmanzadehgervi et al., 2024, Wang et al., 2024b). For example, when interpreting a graph diagram, precise recognition of edges is essential for extracting reliable information, and in geometry problem-solving, accurate identification of</p>
<p>relationships between line segments and points is fundamental (Fu et al., 2024a). Beyond abstract tasks, LLVP is also vital in real-world applications, including spatial understanding for robotics, medical image analysis for accurate diagnosis, quality control in manufacturing to detect subtle defects, autonomous driving systems that rely on exact object localization or distance estimation, and augmented reality applications that demand precise overlay of virtual objects onto the real world.</p>
<p>In this paper, we aim to study the challenges of LLVP in MLLMs, take steps to understand the root cause of their performance, and improve the models' capabilities in this area. We begin by developing a benchmark dataset specifically designed to evaluate precise geometric perception, which we call Geoperception. As a focused test bed, this benchmark targets 2D geometry tasks. Using this benchmark, we demonstrate the limitations of leading closed and open MLLMs, followed by a comprehensive empirical study to explore strategies for significantly improving MLLM's performance on geometric perception tasks. Our findings show the benefits of key factors such as model architecture, training techniques, and data strategies, including the use of synthetic data and multi-stage training with a data curriculum. Notably, we find that a data curriculum enables models to learn challenging geometry LLVP tasks, which they fail to learn from scratch, even when trained on a very large dataset. Using these lessons learned, we then train a family of models-using a carefully designed curriculum of synthetic data-that are specifically optimized for strong LLVP, which we call Euclid. We evaluate this family of models, and show that it excels on a variety of low-level geometric perception tasks.</p>
<p>Our main technical contributions are as follows:</p>
<ul>
<li>Geoperception Benchmark: We introduce a new benchmark dataset, Geoperception, derived from the Geometry-3K corpus (Lu et al., 2021), specifically designed to evaluate MLLMs' ability to accurately perceive surface-level geometric information without requiring complex inference or reasoning. Our benchmark reveals shortcomings in precise geometric perception across all leading vision-language MLLMs, both closed and open-source.</li>
<li>Empirical Study and Synthetic Data Engine: To investigate the root cause of this performance, we conduct a detailed empirical exploration of MLLM architecture and training strategies. To aid in our investigation, we develop a synthetic data engine capable of generating high-fidelity visual descriptions of geometric elements. This study leads to key insights, such as the importance of certain architectural choices and the use of curriculum-based, multi-stage training with progressively more complex visual descriptions for improving low-level visual perception.</li>
<li>Euclid Model Family: Leveraging the insights from our exploration and our synthetic data engine, we train Euclid, a series of MLLMs tailored for high-quality geometric LLVP. Although purely trained on synthetic multimodal data with simple geometry shapes, Euclid generalizes strongly to the real-world geometry images from Geoperception benchmark, for instance, outperforming the best closed-source model, Gemini-1.5-Pro, by up to $58.56 \%$ on certain benchmark tasks and $10.65 \%$ across the tasks.</li>
</ul>
<h1>2. Background and Related Work</h1>
<p>We provide an overview of prior efforts that assess and improve low-level perception and geometric reasoning in MLLMs, and highlight our contributions in data synthesis, evaluation, and training.</p>
<p>Vision-Language MLLMs. While recent iterations of LLMs feature a standardized model architecture and pretraining recipe, MLLMs still often differ in design choices for infusing visual inputs. One popular design</p>
<p>is to align continuous visual features with the embedding space of a backbone LLM (Liu et al., 2024a,b, Dubey et al., 2024, McKinzie et al., 2024, Tong et al., 2024a, Beyer et al., 2024, AI, 2023, Wang et al., 2024a); another approach involves tokenizing visual inputs to be trained jointly with language tokens (Team et al., 2023, Team, 2024a). These modules are often infused with a decoder-only LLM, but others have explored encoder-decoder architectures to integrate a more varied collection of modalities (Alayrac et al., 2022, Mizrahi et al., 2024, Ormazabal et al., 2024, Bachmann et al., 2024). Our study focuses on decoder MLLMs with a continuous visual encoder, and we carry out an empirical study to explore the effect of synthetic dataset mixture, training recipe, and encoder design (Liu et al., 2022, Radford et al., 2021, Zhai et al., 2023, Oquab et al., 2023).</p>
<p>Geometry-Oriented MLLMs. At the core of these choices is the hardness in designing a module adept in general visual reasoning (McKinzie et al., 2024, Tong et al., 2024a). In this work, we explore the optimal design of MLLMs specialized in low-level visual perception, a crucial aspect for (among other applications) multimodal mathematical understanding (Lu et al., 2023, Zhang et al., 2024a). This paper supplements prior efforts in improving mathematical reasoning (Gao et al., 2023, Zhang et al., 2024b, Zhuang et al., 2024, Li et al., 2024, Peng et al., 2024, Shi et al., 2024b) with a detailed study on the effect of dataset mixture, curriculum, and visual encoder, to reach a recipe that elicits strong performance on geometric tasks (Kazemi et al., 2023) that require low-level perception.</p>
<p>Evaluating LLVP. Many benchmarks (Rahmanzadehgervi et al., 2024) have reported that frontier-class MLLMs struggle with visual perception tasks, which are prerequisites for applications that emphasize low-level geometric perception (Chen et al., 2024, Fu et al., 2024c), including mathematical (Yue et al., 2024, Lu et al., 2023, Zhang et al., 2024a, Jiang et al., 2024) and spatial reasoning (Chen et al., 2024, Fu et al., 2024b). These findings collectively identify that MLLMs exhibit a language prior (Lin et al., 2023)—a preference of textual inputs over visual inputs-leading to a performance gap between modalities (Wang et al., 2024b, Zhang et al., 2024a, Fu et al., 2024a). Meanwhile, there lacks a high-quality benchmark that evaluates low-level geometric perception in MLLMs, and the Geigerception benchmark represents a first effort to narrow this gap. This type of efforts have led to significant improvements in certain capabilities of MLLMs, such as compositionality of objects (Yuksekgonul et al., 2022, Kong et al., 2023).</p>
<p>Improving LLVP. Many prior works study data-driven approaches to improve low-level perception skills. For example, Gao et al. (2023), Li et al. (2024), Zhuang et al. (2024) employ a standardized supervised finetuning recipe, and optionally adjust the training data mixture. This type of training data is often synthesized from text-only math problems (Lu et al., 2021, Trinh et al., 2024) or via rule-based systems (Kazemi et al., 2023). In parallel, Vishniakov et al. (2023), Shi et al. (2024a), Tong et al. (2024b) have explored the design space of visual encoders for general-purpose vision-language reasoning. We identify best practices over the union of these design spaces, and then train small MLLMs with strong performance in low-level perception tasks.</p>
<p>Lastly, several works (Schick et al., 2024, Surís et al., 2023, Hu et al., 2024) have opted to augment an MLLM with external APIs that process low-level features with specialized vision modules, such as object detection (Redmon et al., 2016), segmentation (Kirillov et al., 2023), and depth estimation (Yang et al., 2024). While these agentic frameworks (Wu et al., 2023) present a promising alternative that directly addresses the shortcomings of visual encoders, they are limited by their scalability to novel use cases, and may be insufficient for precise tool routing that requires low-level perception as a primer (Picard et al., 2023, Wu et al., 2024, Buehler, 2024).</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Q: What is the point lying o- $n$ line YB?
A: D, G</p>
<p>PointLiesOnCircle
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Q: What is the point lying o- $n$ circle with center G?
A: L, H, F</p>
<p>AngleClassification
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Q: Is angle BAC acute or obt- use?
A: acute</p>
<p>LineComparison
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Q: Which line is longer, QS or SP?
A: QS</p>
<p>Figure 1: Four examples from our Geoperception dataset. The questions are sourced from the Geometry-3K corpus (Lu et al., 2021), which compiles problems from two widely-used high school textbooks. We perform filtering, validation, and generate question-and-answer text for each image.</p>
<h1>3. Geoperception Benchmark</h1>
<p>Recently, there has been a growing number of multimodal benchmarks across diverse domains beyond natural image understanding, including mathematical reasoning (Zhang et al., 2024a, Lu et al., 2023) and abstract visual reasoning (Jiang et al., 2024, Chia et al., 2024). Many of these prior works have realized the importance of accurate low-level visual perception. Specifically, Marvel (Jiang et al., 2024) introduces perception questions for various abstract reasoning patterns, and finds that the main bottleneck of MLLMs' performance on abstract visual reasoning is that they fail to accurately transcribe visual information into concepts; Mathverse (Zhang et al., 2024a) and IsoBench (Fu et al., 2024a) both test MLLMs on equivalent question represented by language and visual modalities, respectively. Both works find that language-only input always outperforms vision-language input, and that the vision component of MLLMs always fails to utilize low-level visual features. VDLM (Wang et al., 2024b) transcribes raster images into vector graphics and uses LLMs to reason over the SVG code. They find that although SVG code is not straightforward to understand, using LLMs to reason over SVG is consistently more effective than directly using MLLMs on original raster images. Blind-test (Rahmanzadehgervi et al., 2024) and BLINK (Fu et al., 2024c) also share similar findings with the works above.</p>
<p>A Benchmark for Geometric LLVP. Although such shortcomings of MLLMs are commonly recognized, there is a lack of comprehensive benchmark that purely focuses on these abilities of MLLMs. Our goal is to construct a benchmark focusing solely on the perception ability of MLLMs, which is also representative enough of real-world applications. When humans perceive and memorize visual information, it is wellrecognized that this procedure relies crucially on searching for the closest and simplest corresponding geometric shapes (Sablé-Meyer et al., 2022). We posit that geometric perception is a fundamental and broadly representative LLVP ability in many applications. Hence, we select geometry understanding as our domain of dataset construction.</p>
<p>Benchmark Tasks. Over two thousand years ago, Euclid introduced five axioms that underpin all further geometric reasoning. These axioms involve establishing and extending lines using points (Axioms 1 and 2), constructing circles from a point and a radius (Axiom 3), and defining perpendicularity (Axiom 4) and parallelism (Axiom 5). Additionally, Euclid provided common notions regarding the properties of equality. To capture these aspects, we define five tasks in our Geoperception dataset: PointLiesOnLine,</p>
<p>PointLiesOnCircle, Parallel, Perpendicular and Equal, and additionally define AngleClassification and LineComparison tasks to assess the model's understanding of angle and length measurements, resulting in a total of seven tasks. In geometric diagrams, perpendicularity, parallelism, and equality are often indicated by annotation symbols. Thus, we classify Parallel, Perpendicular, and Equal as annotated geometry understanding. Meanwhile, PointLiesOnLine, PointLiesOnCircle, AngleClassification, and LineComparison fall under primitive geometry shape understanding, which includes both logical (PointLiesOnLine, PointLiesOnCircle) and numerical (AngleClassif-ication, LineComparison) tasks.</p>
<p>Data Filtering. Geoperception is sourced from the Geometry3K (Lu et al., 2021) corpus, which offers precise logical forms for geometric diagrams, compiled from popular high-school textbooks. However, certain points in these logical forms are absent in the corresponding diagrams. To resolve this, we use GPT-4o-mini MLLM to confirm the presence of all points listed in the logical forms. This process filters the 3,002 diagrams to retain 1,584 , where at least one logical form fully represents its points in the diagram. A random inspection of 100 annotations reveals only two errors, indicating high annotation accuracy.</p>
<p>Table 1: Statistics of the seven tasks in our Geoperception dataset, including the number of questions and images.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Predicate</th>
<th style="text-align: center;"># Q</th>
<th style="text-align: center;"># I</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PointLiesOnLine</td>
<td style="text-align: center;">1901</td>
<td style="text-align: center;">924</td>
</tr>
<tr>
<td style="text-align: center;">PointLiesOnCircle</td>
<td style="text-align: center;">359</td>
<td style="text-align: center;">322</td>
</tr>
<tr>
<td style="text-align: center;">Parallel</td>
<td style="text-align: center;">106</td>
<td style="text-align: center;">101</td>
</tr>
<tr>
<td style="text-align: center;">Perpendicular</td>
<td style="text-align: center;">1266</td>
<td style="text-align: center;">456</td>
</tr>
<tr>
<td style="text-align: center;">Equals</td>
<td style="text-align: center;">4436</td>
<td style="text-align: center;">1202</td>
</tr>
<tr>
<td style="text-align: center;">AngleClassification</td>
<td style="text-align: center;">2193</td>
<td style="text-align: center;">1389</td>
</tr>
<tr>
<td style="text-align: center;">LineComparison</td>
<td style="text-align: center;">1394</td>
<td style="text-align: center;">1394</td>
</tr>
</tbody>
</table>
<p>Converting Logical Forms Into Questions. We convert logical forms into question-and-answer pairs for each of the seven tasks in Geoperception. In the Equals task, for example, we directly convert the logical form (e.g., Equals (LengthOf (Line(Q, T)), 86)) into a question-answer pair (e.g., Q: What is the length of line QT as annotated? A: 86). For PointLiesOnLine, two points on the line are chosen to form the question, with the remaining points on the line as the answer. Similarly, for PointLiesOnCircle, we ask which points lie on the circle, using its center as the basis for the question. For Parallel and Perpendicular, we represent each line by two points and query which other lines are parallel or perpendicular to it. In AngleClassification, we ensure the queried angle is in the range of $[10,80] \cup[100,170]$ degrees to avoid ambiguity. For LineComparison, we ensure that the shorter line is less than $70 \%$ of the length of the longer line. Since multiple equivalent questions can be generated for a single logical form (e.g., a line containing five points generates ${ }^{5} P_{2}$ equivalent questions), we randomly select one to avoid redundancy. Table 1 summarizes the question statistics for each task, as well as the number of images involved. Four examples from Geoperception are illustrated in Fig. 1.</p>
<p>Evaluation Details. We evaluate seven leading MLLMs, both open source and closed source. The open source models include Molmo-7B-D (Deitke et al., 2024), Cambrian-1-8B (Tong et al., 2024a), Qwen2-VL-7B (Wang et al., 2024a), Llama-3.2-11B (Dubey et al., 2024), and Pixtral-12B (AI, 2023). The closedsource models include GPT-4o-mini (Achiam et al., 2023), GPT-4o (Achiam et al., 2023), Claude-3.5-Sonnet (Anthropic, 2024), Gemini-1.5-flash (Team et al., 2023), and Gemini-1.5-pro (Team et al., 2023). Additionally, GPT-4o-mini without image input is used for generating the random baseline, employing the same textual instructions. To prevent stretching, all images are padded to square dimensions before being fed into the models. During evaluation of a given question by an MLLM, let $G$ denote the ground truth set of answers,</p>
<p>and let $P$ denote the predicted set of answers; then the evaluation score is defined as</p>
<p>$$
\text { Evaluation score }=\left{\begin{array}{l}
\left|\frac{P}{G}\right| \quad \text { if } P \subseteq G \
0 \quad \text { otherwise }
\end{array}\right.
$$</p>
<p>Current MLLMs struggle to perceive low-level geometry annotations and relationships. We show a comparison of all models on Geigerception in Table 2. Despite the simplicity of Geigerception for humans, it remains a considerable challenge for even the most advanced commercial MLLMs. Notably, all models fall short of achieving 30\% accuracy on the PointLiesOnLine task and do not outperform the text-only GPT-4o mini model in AngleClassification task. Closed source models generally outperform open source ones, with Gemini-1.5-pro attaining the highest average score of $56.98 \%$, followed by gemini-1.5-flash at $54.76 \%$. Among open source models, Pixtral-12B achieves the best performance with an overall score of $41.95 \%$. It is worth noting that Cambrian-1 (Tong et al., 2024a), which is reported to be trained on Geo-170K (Gao et al., 2023), a geometry multimodal instruction tuning dataset built on the logical annotation of Geometry-3K, the same source with our Geigerception, still faces challenges in our Geigerception task, despite being trained on the dataset having the same images and augmented text annotations.</p>
<p>Table 2: Performance (average evaluation score) of different models on Geigerception benchmark tasks. POL: PointLiesOnLine, POC: PointLiesOnCircle, ALC: AngleClassification, LHC: LineComparison, PEP: Perpendicular, PRA: Parallel, EQL: Equals. As the Random Baseline method, we use GPT-4o-mini, given the same textual instruction but without an image. The best model for each task is bolded.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Logical</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Numerical</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Annotations</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">POL</td>
<td style="text-align: center;">POC</td>
<td style="text-align: center;">ALC</td>
<td style="text-align: center;">LHC</td>
<td style="text-align: center;">PEP</td>
<td style="text-align: center;">PRA</td>
<td style="text-align: center;">EQL</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random Baseline</td>
<td style="text-align: center;">1.35</td>
<td style="text-align: center;">2.63</td>
<td style="text-align: center;">59.92</td>
<td style="text-align: center;">51.36</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">16.50</td>
</tr>
<tr>
<td style="text-align: center;">Open Source</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Molmo-7B-D (Deitke et al., 2024)</td>
<td style="text-align: center;">11.96</td>
<td style="text-align: center;">35.73</td>
<td style="text-align: center;">56.77</td>
<td style="text-align: center;">16.79</td>
<td style="text-align: center;">1.06</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">17.59</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.2-11B (Dubey et al., 2024)</td>
<td style="text-align: center;">16.22</td>
<td style="text-align: center;">37.12</td>
<td style="text-align: center;">59.46</td>
<td style="text-align: center;">52.08</td>
<td style="text-align: center;">8.38</td>
<td style="text-align: center;">22.41</td>
<td style="text-align: center;">49.86</td>
<td style="text-align: center;">35.08</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2-VL-7B (Wang et al., 2024a)</td>
<td style="text-align: center;">21.89</td>
<td style="text-align: center;">41.60</td>
<td style="text-align: center;">46.60</td>
<td style="text-align: center;">63.27</td>
<td style="text-align: center;">26.41</td>
<td style="text-align: center;">30.19</td>
<td style="text-align: center;">54.37</td>
<td style="text-align: center;">40.62</td>
</tr>
<tr>
<td style="text-align: center;">Cambrian-1-8B (Tong et al., 2024a)</td>
<td style="text-align: center;">15.14</td>
<td style="text-align: center;">28.68</td>
<td style="text-align: center;">58.05</td>
<td style="text-align: center;">61.48</td>
<td style="text-align: center;">22.96</td>
<td style="text-align: center;">30.74</td>
<td style="text-align: center;">31.04</td>
<td style="text-align: center;">35.44</td>
</tr>
<tr>
<td style="text-align: center;">Pixtral-12B (AI, 2023)</td>
<td style="text-align: center;">24.63</td>
<td style="text-align: center;">53.21</td>
<td style="text-align: center;">47.33</td>
<td style="text-align: center;">51.43</td>
<td style="text-align: center;">21.96</td>
<td style="text-align: center;">36.64</td>
<td style="text-align: center;">58.41</td>
<td style="text-align: center;">41.95</td>
</tr>
<tr>
<td style="text-align: center;">Closed Source</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o-mini (Achiam et al., 2023)</td>
<td style="text-align: center;">9.80</td>
<td style="text-align: center;">61.19</td>
<td style="text-align: center;">48.84</td>
<td style="text-align: center;">69.51</td>
<td style="text-align: center;">9.80</td>
<td style="text-align: center;">4.25</td>
<td style="text-align: center;">44.74</td>
<td style="text-align: center;">35.45</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o (Achiam et al., 2023)</td>
<td style="text-align: center;">16.43</td>
<td style="text-align: center;">71.49</td>
<td style="text-align: center;">55.63</td>
<td style="text-align: center;">74.39</td>
<td style="text-align: center;">24.80</td>
<td style="text-align: center;">60.30</td>
<td style="text-align: center;">44.69</td>
<td style="text-align: center;">49.68</td>
</tr>
<tr>
<td style="text-align: center;">Claude 3.5 Sonnet (Anthropic, 2024)</td>
<td style="text-align: center;">25.44</td>
<td style="text-align: center;">68.34</td>
<td style="text-align: center;">42.95</td>
<td style="text-align: center;">70.73</td>
<td style="text-align: center;">21.41</td>
<td style="text-align: center;">63.92</td>
<td style="text-align: center;">66.34</td>
<td style="text-align: center;">51.30</td>
</tr>
<tr>
<td style="text-align: center;">Gemini-1.5-Flash (Team et al., 2023)</td>
<td style="text-align: center;">29.30</td>
<td style="text-align: center;">67.75</td>
<td style="text-align: center;">49.89</td>
<td style="text-align: center;">76.69</td>
<td style="text-align: center;">29.98</td>
<td style="text-align: center;">63.44</td>
<td style="text-align: center;">66.28</td>
<td style="text-align: center;">54.76</td>
</tr>
<tr>
<td style="text-align: center;">Gemini-1.5-Pro (Team et al., 2023)</td>
<td style="text-align: center;">24.42</td>
<td style="text-align: center;">69.80</td>
<td style="text-align: center;">57.96</td>
<td style="text-align: center;">79.05</td>
<td style="text-align: center;">38.81</td>
<td style="text-align: center;">76.65</td>
<td style="text-align: center;">52.15</td>
<td style="text-align: center;">56.98</td>
</tr>
</tbody>
</table>
<h1>4. Empirical Study on MLLM Design Space</h1>
<p>Although large-scale web-crawled image-text pairs cover a variety of domains, including geometry, the textual descriptions often lack the necessary specificity and depth. To address this issue, current studies in this domain (Gao et al., 2023, Shi et al., 2024b, Zhang et al., 2024b) typically construct a geometry or</p>
<p>mathematical domain dataset and apply the same training strategy used for general-purpose MLLMs. For example, Math-LLaVA (Shi et al., 2024b) and multi-math (Peng et al., 2024) rely on GPT-4V or GPT-4o's vision ability to generate most of the question and answer pairs and image captions, which is essentially model distillation. However, as evidenced by Table 2, GPT-4o and Gemini-1.5-Pro often struggle to answer even basic perception questions, limiting the performance potential of resulting models. Furthermore, while works such as G-LLaVA (Gao et al., 2023), MAVIS (Zhang et al., 2024b), and Math-PUMA (Zhuang et al., 2024) utilize human crafted logical forms or synthetic multimodal data to ensure the reliability of textual annotations, they often conflate low-level perception with problem-solving, and train models to directly solve multimodal geometry problems, without verifying if the model's low-level perception abilities are sufficient. As an evidence, the best models in MAVIS (Zhang et al., 2024b) and Math-PUMA (Zhuang et al., 2024) evaluation results on Mathverse (Zhang et al., 2024a) still have a substantial gap of $26.8 \%$ and $28.7 \%$ between text-dominant versions and vision-only versions of problems ${ }^{1}$, respectively. Furthermore, attempts to train MLLMs on low-level visual perception tasks (Wang et al., 2024b, Rahmanzadehgervi et al., 2024) have also struggled to achieve satisfactory in-domain performance or generalize effectively. In this section, we aim to address these challenges.</p>
<p>We hypothesize the inability of today's MLLMs to effectively perceive basic geometric annotations and relationships stems from two factors: 1 . The lack of high-fidelity geometric visual perception training data. 2. The problem of their model architectures and training strategy. Next, we will introduce our geometry dataset generation engine to overcome the lack of data, and then use generated dataset to study the optimal training strategy.</p>
<h2>Geometry Image Generation Engine.</h2>
<p>To provide sufficient high-fidelity training datasets, we develop a synthetic dataset generation engine to programmatically produce geometry shapes. Our geometry shape generation engine is built on AlphaGeometry (Trinh et al.,
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 2: Three geometry logical shapes, of increasing complexity, used in our empirical study. Our geometry image generation engine is able to produce infinite visual instances for each of these logical shapes. All letters are randomly sampled from the alphabet and reassigned to each of the points before drawing.
2024). Given an input formal language describing a geometry shape, the geometry engine will first check the validity of the geometry shape. Then it will create numerical positions for all points following the restrictions given by the input. After the creation of all points, it will connect the line as specified in the input. To avoid inductive bias during training (e.g., point A is always on top of a triangle), letters are first picked from a letter pool (e.g., all 26 capital letters) and then randomly assigned to each point. In addition to the original image generation engine, we introduce three visualization enhancements: (1) additional inputs to control the connections between points, number of letters in the letter pool, presence of each points, and annotation about length and angles; (2) increased randomness in creating numerical instances from conceptual shapes;</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 3: LLM size experiments. Training loss and testing accuracy curve comparing three choices of LLM size with a fixed visual encoder and multimodal connector. Training losses are window-smoothed using a window size of 10 for better visibility.
and (3) adjustments to the canvas range to ensure visibility of all geometry components. Examples of our geometry generation engine, showing three geometries of increasing complexity, are shown in Fig. 2</p>
<p>Exploration of MLLM design space. With sufficient training dataset, we now revisit the MLLMs architectural and training design space. We choose 2 primitive geometry tasks from Geigerception as the test bed for the exploration: logical task, PointLiesOnLine and numerical task, LineComparison. For each task, we carefully create three tasks with incremental difficulty levels. We name them as difficulty level easy, medium and hard. Based on the insight from our preliminary experiments, to increase the difficulty levels, for PointLiesOnLine, we increase the complexity of geometry shapes as is shown in Fig. 18, for LineComparison, we increase the total number of letters in letter pool while mixing geometry shapes. During our preliminary experiments, we find that sometimes the model fails to converge due to instability. To this end, for all experiment moving forward, we run the training for three times and report the best run among them (i.e., having the lowest overall training loss or testing accuracy).</p>
<p>We start with a typical setting of MLLMs: CLIP-ViT-L/14 (Radford et al., 2021) as the visual encoder and a two layer MLP as multimodal connector and the latest Qwen-2.5 series (Team, 2024b) as LLM. During training, we actively tune the MLP and LLM, while keeping visual encoder frozen. We use the mixture of three difficulty levels as the training set.</p>
<p>Lesson 1: Under the same training dataset, scaling LLM sizes does not lead to better performance. It is commonly acknowledged that under the same training dataset, scaling up LLM can lead to better MLLM performance (Liu et al., 2024a). To this end, we first vary the sizes of LLMs, Qwen-2.5 (Team, 2024b) in a range of $0.5 \mathrm{~B}, 1.5 \mathrm{~B}$, and 3 B while keep other components consistent. The result is shown in Fig. 3. For LineComparison, Qwen-2.5-1.5B performs the best while Qwen-2.5-3B learns most slowly. For PointLiesOnLine, Qwen-2.5-1.5B and Qwen-2.5-3B performs almost the same. Qwen-2.5-0.5B learns relatively slower, but still reach almost the same final performance with two other models. In conclusion, we do not observe an obvious trend that larger LLMs can learn such low-level visual perception task faster or</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 4: Vision encoder experiments. Training loss and testing accuracy (on a 1500 instances holdout test set) curve comparing eight visual encoders, with a fixed multimodal encoder and LLM. For a fair comparison, all visual encoder transcribe an image into 256 visual tokens. Training losses are window-smoothed using a window size of 10 for better visibility.
better. Moving forward, we will use Qwen-2.5-1.5B to continue our exploration.</p>
<p>Lesson 2: CNN architecture performs better than ViT. We then study the choice of visual encoder architectures, including two families of architectures: Vision Transformer (ViT) (Dosovitskiy, 2020) and ConvNeXT (Liu et al., 2022); as well as two visual representation learning objectives: language-supervised learning (Radford et al., 2021) and self-supervised learning (Oquab et al., 2023). We control the number of visual tokens to 256 for all of our vision encoders. The result is shown in Fig. 4. We find that ConvNeXt-XXLarge and ConvNeXt-Large consistently learns the geometric concept the fastest among all</p>
<p>Table 3: Summary of Visual Encoders</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Params</th>
<th style="text-align: center;">Objective</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ConvNeXt Large@512</td>
<td style="text-align: center;">200 M</td>
<td style="text-align: center;">CLIP</td>
</tr>
<tr>
<td style="text-align: left;">ConvNeXt XXLarge@512</td>
<td style="text-align: center;">847 M</td>
<td style="text-align: center;">CLIP</td>
</tr>
<tr>
<td style="text-align: left;">ViT-g/14@224</td>
<td style="text-align: center;">1.01 B</td>
<td style="text-align: center;">CLIP</td>
</tr>
<tr>
<td style="text-align: left;">ViT-H/14@224</td>
<td style="text-align: center;">632 M</td>
<td style="text-align: center;">CLIP</td>
</tr>
<tr>
<td style="text-align: left;">ViT-L/14@224</td>
<td style="text-align: center;">303 M</td>
<td style="text-align: center;">CLIP</td>
</tr>
<tr>
<td style="text-align: left;">SigLIP@224 (ViT)</td>
<td style="text-align: center;">428 M</td>
<td style="text-align: center;">CLIP-like</td>
</tr>
<tr>
<td style="text-align: left;">DINOv2 Giant@224 (ViT)</td>
<td style="text-align: center;">1.14 B</td>
<td style="text-align: center;">Self-Sup</td>
</tr>
<tr>
<td style="text-align: left;">DINOv2 Large@224 (ViT)</td>
<td style="text-align: center;">304 M</td>
<td style="text-align: center;">Self-Sup</td>
</tr>
</tbody>
</table>
<p>of the visual encoders. Notably, ConvNeXT-Large shows superior learning performance with the vision transformers which are 3-5 times larger. We hypothesize that CNN architecture extract visual features globally, effectively preserving low-level visual features. In contrast, ViT architectures split images into discrete patches, making it more challenging to retain the original low-level visual information. Self-supervised learning (SSL) visual encoders, DINO-v2, struggles to learn the geometry concept; we hypothesis this is due to the weak vision-language representation in these models. Surprisingly, although the SigLIP-family is widely-recognized as a better visual encoder (Tong et al., 2024a), we find that their performance in learning basic visual geometry attributes is limited.</p>
<p>Lesson 3: Tuning vision encoder does not provide significant help. We next study the effect of tuning versus freezing the visual encoder. In Fig. 5, we show the testing accuracy curves of tuning and freezing visual encoders. We find that compared with using a frozen encoder, tuning the visual encoder does not help</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 5: Tuning/freezing vision encoder experiments. Testing accuracy (on a 1500 instances holdout test set) curve comparing freezing versus tuning the visual encoder during training.
the model learn low-level geometry relationships faster or better. In what follows, we will freeze the encoder for simplicity.</p>
<h1>Lesson 4: Curriculum learning unleashes full potential.</h1>
<p>Finally, we study training data composition. In our preliminary experiment Fig. 19, we observe that the model fails to converge on difficulty level 3 of PointLiesOnLine and difficulty level 2 and 3 of LineComparison. However, when using mixed training set of all three difficulty levels, the model achieves convergence, despite using the same amount of data for each difficulty levels. We hypothesize that including easier levels aids the model in learning more complex levels. To test this hypothesis, we report the test accuracy for three difficulty levels separately during the mixed training of ConvNeXtXXLarge, in Fig. 6, on both tasks. We notice that the testing accuracy for easier tasks increase earlier and more quickly than difficulty tasks. In PointLiesOnLine tasks, we notice an apparent plateau for hard level tasks until the model has trained on approximately 20 K samples. During this period, the testing accuracy for easy and medium continue to increase. This suggests that learning easier shapes can significantly help the model tackle more challenging shapes, comparing with directly learning the challenging ones, this finding align with the principles of curriculum learning.</p>
<p>While mixed training enables effective spontaneous curriculum learning, we investigate whether a structured curriculum can further enhance model efficiency on challenging shapes. To this end, we train the model sequentially from simple to more complex shapes and compare testing accuracy just on hard level tasks. During training, we monitor the model's performance and dynamically adjust the distribution of</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 7: Curriculum learning experiments. Test accuracy on difficulty level hard of three training strategies: purely training on difficulty level hard, mixed training of difficulty levels easy/medium/hard, and curriculum training.
training data (i.e., the curriculum stage) based on this performance. Specifically, the model starts by training on the easy level data. and is evaluated when it finishes a training round, using testing accuracy from the current level of data. Upon evaluation, if the model achieves an accuracy exceeding a predefined threshold $\theta$, the framework advances the level to the next. Formally, the update rule for advancing stages is given by:</p>
<p>$$
\text { if accuracy }_{s}&gt;\theta \quad \Rightarrow \quad c \leftarrow c+1
$$</p>
<p>The model is trained on a total of $M$ rounds and $K$ steps within each round. To avoid forgetting, we apply data smoothing at each stage. Specifically, we smooth our dataset distribution over all stages using an exponential attenuation function:</p>
<p>$$
\text { ratio }<em s="s">{s}=\exp \left(-\alpha \cdot\left|\operatorname{stage}</em>-c\right|\right)
$$</p>
<p>where $\alpha$ denotes the attenuation rate. Eq. (3) ensures that stages proximal to the current stage receive higher sampling probabilities.</p>
<p>We refer to this as our curriculum training strategy. Specifically, the accuracy threshold for advancing training stage $\theta$ is set to 0.99 . We train all the models for $M=30$ rounds, each round with $K=50$ steps. The results are shown in Fig. 7. Firstly, we find that all of the models fail to converge when trained purely on hard level for PointLiesOnLine task. In contrast, the mixed training strategy shown by the red curve, consistently reaches faster convergence on hard level. Curriculum training strategy, shown by the purple curve, proves more efficient than mixed training.</p>
<h1>5. Euclid: a Family of MLLMs for Geometric Visual Perception</h1>
<p>In this section, we take all of the lessons we learned in the previous sections and train Euclid, a family of MLLMs specifically designed for strong geometric LLVP.</p>
<p>On-the-fly progressive training. We use the same strategy as the curriculum training in Section 4, but scale our training to all tasks in Geoperception. For each task, we create $N$ stages of training dataset shapes with progressively increasing geometric complexity.</p>
<p>Specifications. For models, we select the best visual encoder architecture we found in our investigation, ConvNeXt, including ConvNeXt-Large@512 and ConvNeXt-XXLarge@512, and keep the same multimodal connector (2 layers MLP) and LLM (Qwen2.5-1.5B-instruct). The accuracy threshold for advancing training stage $\theta$ is set to 0.99 . All models are trained on $N=3$ stages with manually curated geometry shapes and $M=50$ rounds with $K=500$ steps in each round, and the batch size is 64 for each training step. The total training dataset volume for both of the models is 1.6 M .</p>
<p>Table 4: Performance comparison between Euclid and the best leading open source and closed source MLLMs on the seven tasks. Note that Euclid is not trained on any of the in-distribution data from the benchmark tasks below. The best model for each task is bolded.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Logical</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Numerical</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Annotations</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">POL</td>
<td style="text-align: center;">POC</td>
<td style="text-align: center;">ALC</td>
<td style="text-align: center;">LHC</td>
<td style="text-align: center;">PEP</td>
<td style="text-align: center;">PRA</td>
<td style="text-align: center;">EQL</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random Baseline</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">2.63</td>
<td style="text-align: center;">59.92</td>
<td style="text-align: center;">51.36</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">16.37</td>
</tr>
<tr>
<td style="text-align: center;">Pixtral-12B (AI, 2023)</td>
<td style="text-align: center;">24.63</td>
<td style="text-align: center;">53.21</td>
<td style="text-align: center;">47.33</td>
<td style="text-align: center;">51.43</td>
<td style="text-align: center;">21.96</td>
<td style="text-align: center;">36.64</td>
<td style="text-align: center;">58.41</td>
<td style="text-align: center;">41.95</td>
</tr>
<tr>
<td style="text-align: center;">Gemini-1.5-Pro (Team et al., 2023)</td>
<td style="text-align: center;">24.42</td>
<td style="text-align: center;">69.80</td>
<td style="text-align: center;">57.96</td>
<td style="text-align: center;">79.05</td>
<td style="text-align: center;">38.81</td>
<td style="text-align: center;">76.65</td>
<td style="text-align: center;">52.15</td>
<td style="text-align: center;">56.98</td>
</tr>
<tr>
<td style="text-align: center;">Euclid-ConvNeXt-Large</td>
<td style="text-align: center;">80.54</td>
<td style="text-align: center;">57.76</td>
<td style="text-align: center;">86.37</td>
<td style="text-align: center;">88.24</td>
<td style="text-align: center;">42.23</td>
<td style="text-align: center;">64.94</td>
<td style="text-align: center;">34.45</td>
<td style="text-align: center;">64.93</td>
</tr>
<tr>
<td style="text-align: center;">Euclid-ConvNeXt-XXLarge</td>
<td style="text-align: center;">82.98</td>
<td style="text-align: center;">61.45</td>
<td style="text-align: center;">90.56</td>
<td style="text-align: center;">90.82</td>
<td style="text-align: center;">46.96</td>
<td style="text-align: center;">70.52</td>
<td style="text-align: center;">31.94</td>
<td style="text-align: center;">67.89</td>
</tr>
</tbody>
</table>
<p>Evaluation results. The results are shown in Table 4. Overall, although only trained on very simple synthetic geometry shapes, and using only a 1.5B language model, Euclid significantly outperforms current leading MLLMs in most of the tasks, showing strong generalization abilities on real-world geometry LLVP. Notably, in the PointLiesOnLine task, which is particularly challenging for existing MLLMs, Euclid achieves up to $82.98 \%$ accuracy, more than three times the performance of Gemini-1.5-Pro. On all both numerical tasks, LineComparison and AngleClassification, Euclid keeps superior performance. However, on three annotation tasks, Euclid's performance is limited. We hypothesis this is due to the limited setting of our annotation types and styles, making the model hard to generalize to diverse human geometry annotations.</p>
<p>Error analysis. We take a deep look into Euclid's prediction on Geoperception, we find that our model's performance is hindered when diagrams are heavily annotated. An example is shown in Fig. 8, where a line is annotated by " $x$ ", confusing the model from choosing the correct point. Incorporating training data with more diverse annotation types, geometry shape and can better distinguish different diagram annotation types could potentially help the model with such scenarios.</p>
<h1>6. Conclusion and Future Work</h1>
<p>In this work, we highlight the importance of accurate low-level visual perception(LLVP) in MLLMs. To this end, we first introduce Geoperception, a large-scale multimodal benchmark focused exclusively on geometry-domain visual perception. We evaluate leading MLLMs on Geoperception, find that even top models such as Gemini-1.5-Pro struggle significantly it, although it is straightforward for humans. We then conduct an empirical study to explore the design space of MLLM training and architectures using the dataset generated by a geometric high-fidelity synthetic-data engine that we develop. Our study indicate that convolutional neural network visual encoders outperform vision transformers in our tasks; tuning the visual encoder generally enhances performance; and employing a curriculum-based training approach yields much more model potential than direct task training. Based on insights from this study, we develop Euclid, a model trained purely on highfidelity synthetic generated data, which generalizes effectively to real-world geometric shape understanding tasks, surpassing the leading MLLMs by a substantial margin.</p>
<p>Future work. Our work examines the potential of using synthetic multimodal data to improve MLLM performance in low-level geometric perception tasks. However, there are still directions that remain underexplored: (1) Automatic curriculum learning. Incorporating a more diverse dataset, including varied geometric shapes and different domain dataset, introduces challenges in defining the learning order. Rule based definition and manual curation may become impractical, necessitating automated strategies like hard negative sampling to organize the curriculum based on training loss or testing accuracy. This approach could streamline the process, reduce human effort, provide more suitable and efficient curriculum learning orders. (2) Using a more-diverse training dataset. Currently, the text portion of our synthetic multimodal training data uses a restricted set of templates, and the model trained on such templates could fail to generalize to other question types; it could therefore be beneficial to increase the diversity of our training images as well as the instruction-following formats. (3) Generalizing to other task domains. In this work, our study is focused on data from 2D geometry, as it provides a focused test bed of fundamental tasks. We believe the lessons we learn from this domain can be effectively generalized to a broader set of downstream domains that benefit from high-quality LLVP.</p>
<h2>Reproducibility Statement</h2>
<p>In Section 3, we provide a comprehensive description of the procedure for generating the Geoperception benchmark. This includes employing GPT-4o-mini for dataset filtering and detailing the conversion of logical forms into questions and answers. Evaluation prompts for MLLMs on different types of Geoperception questions are presented in Appendix B. For model architecture exploration, we specify the visual encoders and provide corresponding Hugging Face links in Table 3. Additionally, we outline the LLMs and multimodal connector architectures used. For our Euclid model, we include all geometry shape code used for training, along with demonstration diagrams and pseudo-code for generating training questions and answers.</p>
<h1>References</h1>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.</p>
<p>Kian Ahrabian, Zhivar Sourati, Kexuan Sun, Jiarui Zhang, Yifan Jiang, Fred Morstatter, and Jay Pujara. The curious case of nonverbal abstract reasoning with multi-modal large language models. arXiv preprint arXiv:2401.12117, 2024.</p>
<p>Mistral AI. Pixtral 12b. https://mistral.ai/news/pixtral-12b/, 2023. Accessed: 2024-09-27.
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35:23716-23736, 2022.</p>
<p>Anthropic. The claude 3 model family: Opus, Sonnet, Haiku, March 2024. URL https: //www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_ Claude_3.pdf.</p>
<p>Roman Bachmann, Oğuzhan Fatih Kar, David Mizrahi, Ali Garjani, Mingfei Gao, David Griffiths, Jiaming Hu, Afshin Dehghan, and Amir Zamir. 4m-21: An any-to-any vision model for tens of tasks and modalities. arXiv preprint arXiv:2406.09406, 2024.</p>
<p>Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: A versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024.</p>
<p>Markus J Buehler. Cephalo: Multi-modal vision-language models for bio-inspired materials analysis and design. Advanced Functional Materials, page 2409531, 2024.</p>
<p>Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14455-14465, 2024.</p>
<p>Yew Ken Chia, Vernon Toh Yan Han, Deepanway Ghosal, Lidong Bing, and Soujanya Poria. Puzzlevqa: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns. arXiv preprint arXiv:2403.13315, 2024.</p>
<p>Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models, 2024. URL https://arxiv.org/abs/2409.17146.</p>
<p>Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.</p>
<p>Deqing Fu, Ruohao Guo, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, and Willie Neiswanger. Isobench: Benchmarking multimodal foundation models on isomorphic representations. In First Conference on Language Modeling, 2024a. URL https://openreview.net/forum?id= KZd1EErRJ1.</p>
<p>Deqing Fu, Tong Xiao, Rui Wang, Wang Zhu, Pengchuan Zhang, Guan Pang, Robin Jia, and Lawrence Chen. Tldr: Token-level detective reward model for large vision language models. arXiv preprint arXiv:2410.04734, 2024b.</p>
<p>Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024c.</p>
<p>Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023.</p>
<p>Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904-6913, 2017.</p>
<p>Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, et al. Large multilingual models pivot zero-shot multimodal learning across languages. arXiv preprint arXiv:2308.12038, 2023.</p>
<p>Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, and Ranjay Krishna. Visual sketchpad: Sketching as a visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024.</p>
<p>Yifan Jiang, Jiarui Zhang, Kexuan Sun, Zhivar Sourati, Kian Ahrabian, Kaixin Ma, Filip Ilievski, and Jay Pujara. Marvel: Multidimensional abstraction and reasoning through visual evaluation and learning. arXiv preprint arXiv:2404.13591, 2024.</p>
<p>Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu, Xi Chen, and Radu Soricut. Geomverse: A systematic evaluation of large models for geometric reasoning. arXiv preprint arXiv:2312.12241, 2023.</p>
<p>Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4015-4026, 2023.</p>
<p>Xianghao Kong, Ollie Liu, Han Li, Dani Yogatama, and Greg Ver Steeg. Interpretable diffusion via information decomposition. arXiv preprint arXiv:2310.07972, 2023.</p>
<p>Zhihao Li, Yao Du, Yang Liu, Yan Zhang, Yufang Liu, Mengdi Zhang, and Xunliang Cai. Eagle: Elevating geometric reasoning through llm-empowered visual instruction tuning. arXiv preprint arXiv:2408.11397, 2024 .</p>
<p>Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer, 2014.</p>
<p>Zhiqiu Lin, Xinyue Chen, Deepak Pathak, Pengchuan Zhang, and Deva Ramanan. Revisiting the role of language priors in vision-language models. arXiv preprint arXiv:2306.01879, 2023.</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26296-26306, 2024a.</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024b. URL https://llava-vl.github.io/ blog/2024-01-30-llava-next/.</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024c.</p>
<p>Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023.</p>
<p>Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11976-11986, 2022.</p>
<p>Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021.</p>
<p>Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023.</p>
<p>Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis \&amp; insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024.</p>
<p>David Mizrahi, Roman Bachmann, Oguzhan Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, and Amir Zamir. 4m: Massively multimodal masked modeling. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.</p>
<p>Aitor Ormazabal, Che Zheng, Cyprien de Masson d'Autume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac Ong, et al. Reka core, flash, and edge: A series of powerful multimodal language models. arXiv preprint arXiv:2404.12387, 2024.</p>
<p>Shuai Peng, Di Fu, Liangcai Gao, Xiuqin Zhong, Hongguang Fu, and Zhi Tang. Multimath: Bridging visual and mathematical reasoning for large language models. arXiv preprint arXiv:2409.00147, 2024.</p>
<p>Cyril Picard, Kristen M Edwards, Anna C Doris, Brandon Man, Giorgio Giannone, Md Ferdous Alam, and Faez Ahmed. From concept to manufacturing: Evaluating vision-language models for engineering design. arXiv preprint arXiv:2311.12668, 2023.</p>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.</p>
<p>Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. Vision language models are blind. arXiv preprint arXiv:2407.06581, 2024.</p>
<p>Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779-788, 2016.</p>
<p>Mathias Sablé-Meyer, Kevin Ellis, Josh Tenenbaum, and Stanislas Dehaene. A language of thought for the mental representation of geometric shapes. Cognitive Psychology, 139:101527, 2022.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024a.</p>
<p>Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024b.</p>
<p>Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11888-11898, 2023.</p>
<p>Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024a.</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.</p>
<p>Qwen Team. Qwen2.5: A party of foundation models, September 2024b. URL https://qwenlm. github. io/blog/qwen2.5/.</p>
<p>Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: A fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024a.</p>
<p>Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9568-9578, 2024b.</p>
<p>Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476-482, 2024.</p>
<p>Kirill Vishniakov, Zhiqiang Shen, and Zhuang Liu. Convnet vs transformer, supervised vs clip: Beyond imagenet accuracy. arXiv preprint arXiv:2311.09215, 2023.</p>
<p>Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a.</p>
<p>Zhenhailong Wang, Joy Hsu, Xingyao Wang, Kuan-Hao Huang, Manling Li, Jiajun Wu, and Heng Ji. Textbased reasoning about vector graphics. arXiv preprint arXiv:2404.06479, 2024b.</p>
<p>Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.</p>
<p>Sifan Wu, Amir Khasahmadi, Mor Katz, Pradeep Kumar Jayaraman, Yewen Pu, Karl Willis, and Bang Liu. Cadvlm: Bridging language and vision in the generation of parametric cad sketches. arXiv preprint arXiv:2409.17457, 2024.</p>
<p>Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10371-10381, 2024.</p>
<p>Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9556-9567, 2024.</p>
<p>Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why visionlanguage models behave like bags-of-words, and what to do about it? arXiv preprint arXiv:2210.01936, 2022.</p>
<p>Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pretraining. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11975-11986, 2023.</p>
<p>Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024a.</p>
<p>Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction tuning. arXiv preprint arXiv:2407.08739, 2024b.</p>
<p>Wenwen Zhuang, Xin Huang, Xiantao Zhang, and Jin Zeng. Math-puma: Progressive upward multimodal alignment to enhance mathematical reasoning. arXiv preprint arXiv:2408.08640, 2024.</p>
<h1>Appendix</h1>
<h2>A. Geigerception Benchmark Details</h2>
<p>In Table 5, we provide more details on the Geigerception benchmark, such as the number of logic forms present before and after filtering, the number of questions, and the number of images. AngleClassification and LineComparison are directly derived from points coordinates without filtering.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Predicate</th>
<th style="text-align: center;"># LF Before Filter</th>
<th style="text-align: center;"># LF After Filter</th>
<th style="text-align: center;"># Q</th>
<th style="text-align: center;"># I</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PointLiesOnLine</td>
<td style="text-align: center;">6988</td>
<td style="text-align: center;">2567</td>
<td style="text-align: center;">1901</td>
<td style="text-align: center;">924</td>
</tr>
<tr>
<td style="text-align: center;">PointLiesOnCircle</td>
<td style="text-align: center;">1966</td>
<td style="text-align: center;">1240</td>
<td style="text-align: center;">359</td>
<td style="text-align: center;">322</td>
</tr>
<tr>
<td style="text-align: center;">Parallel</td>
<td style="text-align: center;">222</td>
<td style="text-align: center;">123</td>
<td style="text-align: center;">106</td>
<td style="text-align: center;">101</td>
</tr>
<tr>
<td style="text-align: center;">Perpendicular</td>
<td style="text-align: center;">1111</td>
<td style="text-align: center;">680</td>
<td style="text-align: center;">1266</td>
<td style="text-align: center;">456</td>
</tr>
<tr>
<td style="text-align: center;">Equals</td>
<td style="text-align: center;">6434</td>
<td style="text-align: center;">4123</td>
<td style="text-align: center;">4436</td>
<td style="text-align: center;">1202</td>
</tr>
</tbody>
</table>
<p>Table 5: Statistics of the five predicates in our Geigerception dataset. Including number of logic forms before filter, after filter and the number of questions and images.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Q: What is the point lying on line JL?
A: R
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Q: What is the point lying on circle with center P? A: T, S, R, Q
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Q: What is the line parallelto line BE?
A: CD
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Q: What is the line perpendicular to line ZW?
A: YZ
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Q: What is the length of lin-
e NM as annotated?
A: 39
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Q: Is angle SUV acute or obtuse?
A: obtuse
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Q: Which line is longer, $A B$ or AC?
A: AC
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Q: What is the point lying on line ZX?
A: N
<img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Q: What is the point lying on circle with center K?
A: L, J
<img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Q: What is the line parallelto line NQ?
A: OP
<img alt="img-19.jpeg" src="img-19.jpeg" /></p>
<p>Q: What is the line perpendicular to line CB?
A: AC
<img alt="img-20.jpeg" src="img-20.jpeg" /></p>
<p>Q: What is the line perpendicular to line CB?
A: AC
<img alt="img-21.jpeg" src="img-21.jpeg" /></p>
<p>Q: What is the measure of angle ABC as annotated?
A: ZX
<img alt="img-22.jpeg" src="img-22.jpeg" /></p>
<p>Q: Is angle JKL acute or obtuse?
A: obtuse
<img alt="img-23.jpeg" src="img-23.jpeg" />
<img alt="img-24.jpeg" src="img-24.jpeg" />
<img alt="img-25.jpeg" src="img-25.jpeg" />
<img alt="img-26.jpeg" src="img-26.jpeg" />
<img alt="img-27.jpeg" src="img-27.jpeg" />
<img alt="img-28.jpeg" src="img-28.jpeg" />
<img alt="img-29.jpeg" src="img-29.jpeg" />
<img alt="img-30.jpeg" src="img-30.jpeg" />
<img alt="img-31.jpeg" src="img-31.jpeg" />
<img alt="img-32.jpeg" src="img-32.jpeg" />
<img alt="img-33.jpeg" src="img-33.jpeg" />
<img alt="img-34.jpeg" src="img-34.jpeg" />
<img alt="img-35.jpeg" src="img-35.jpeg" />
<img alt="img-36.jpeg" src="img-36.jpeg" />
<img alt="img-37.jpeg" src="img-37.jpeg" />
<img alt="img-38.jpeg" src="img-38.jpeg" />
<img alt="img-39.jpeg" src="img-39.jpeg" />
<img alt="img-40.jpeg" src="img-40.jpeg" />
<img alt="img-41.jpeg" src="img-41.jpeg" />
<img alt="img-42.jpeg" src="img-42.jpeg" />
<img alt="img-43.jpeg" src="img-43.jpeg" />
<img alt="img-44.jpeg" src="img-44.jpeg" />
<img alt="img-45.jpeg" src="img-45.jpeg" />
<img alt="img-46.jpeg" src="img-46.jpeg" />
<img alt="img-47.jpeg" src="img-47.jpeg" />
<img alt="img-48.jpeg" src="img-48.jpeg" />
<img alt="img-49.jpeg" src="img-49.jpeg" />
<img alt="img-50.jpeg" src="img-50.jpeg" />
<img alt="img-51.jpeg" src="img-51.jpeg" />
<img alt="img-52.jpeg" src="img-52.jpeg" />
<img alt="img-53.jpeg" src="img-53.jpeg" />
<img alt="img-54.jpeg" src="img-54.jpeg" />
<img alt="img-55.jpeg" src="img-55.jpeg" />
<img alt="img-56.jpeg" src="img-56.jpeg" />
<img alt="img-57.jpeg" src="img-57.jpeg" />
<img alt="img-58.jpeg" src="img-58.jpeg" />
<img alt="img-59.jpeg" src="img-59.jpeg" />
<img alt="img-60.jpeg" src="img-60.jpeg" />
<img alt="img-61.jpeg" src="img-61.jpeg" />
<img alt="img-62.jpeg" src="img-62.jpeg" />
<img alt="img-63.jpeg" src="img-63.jpeg" />
<img alt="img-64.jpeg" src="img-64.jpeg" />
<img alt="img-65.jpeg" src="img-65.jpeg" />
<img alt="img-66.jpeg" src="img-66.jpeg" />
<img alt="img-67.jpeg" src="img-67.jpeg" />
<img alt="img-68.jpeg" src="img-68.jpeg" />
<img alt="img-69.jpeg" src="img-69.jpeg" />
<img alt="img-70.jpeg" src="img-70.jpeg" />
<img alt="img-71.jpeg" src="img-71.jpeg" />
<img alt="img-72.jpeg" src="img-72.jpeg" />
<img alt="img-73.jpeg" src="img-73.jpeg" />
<img alt="img-74.jpeg" src="img-74.jpeg" />
<img alt="img-75.jpeg" src="img-75.jpeg" />
<img alt="img-76.jpeg" src="img-76.jpeg" />
<img alt="img-77.jpeg" src="img-77.jpeg" />
<img alt="img-78.jpeg" src="img-78.jpeg" />
<img alt="img-79.jpeg" src="img-79.jpeg" />
<img alt="img-80.jpeg" src="img-80.jpeg" />
<img alt="img-81.jpeg" src="img-81.jpeg" />
<img alt="img-82.jpeg" src="img-82.jpeg" />
<img alt="img-83.jpeg" src="img-83.jpeg" />
<img alt="img-84.jpeg" src="img-84.jpeg" />
<img alt="img-85.jpeg" src="img-85.jpeg" />
<img alt="img-86.jpeg" src="img-86.jpeg" />
<img alt="img-87.jpeg" src="img-87.jpeg" />
<img alt="img-88.jpeg" src="img-88.jpeg" />
<img alt="img-89.jpeg" src="img-89.jpeg" />
<img alt="img-90.jpeg" src="img-90.jpeg" />
<img alt="img-91.jpeg" src="img-91.jpeg" />
<img alt="img-92.jpeg" src="img-92.jpeg" />
<img alt="img-93.jpeg" src="img-93.jpeg" />
<img alt="img-94.jpeg" src="img-94.jpeg" />
<img alt="img-95.jpeg" src="img-95.jpeg" />
<img alt="img-96.jpeg" src="img-96.jpeg" />
<img alt="img-97.jpeg" src="img-97.jpeg" />
<img alt="img-98.jpeg" src="img-98.jpeg" />
<img alt="img-99.jpeg" src="img-99.jpeg" />
<img alt="img-100.jpeg" src="img-100.jpeg" />
<img alt="img-101.jpeg" src="img-101.jpeg" />
<img alt="img-102.jpeg" src="img-102.jpeg" />
<img alt="img-103.jpeg" src="img-103.jpeg" />
<img alt="img-104.jpeg" src="img-104.jpeg" />
<img alt="img-105.jpeg" src="img-105.jpeg" />
<img alt="img-106.jpeg" src="img-106.jpeg" />
<img alt="img-107.jpeg" src="img-107.jpeg" />
<img alt="img-108.jpeg" src="img-108.jpeg" />
<img alt="img-109.jpeg" src="img-109.jpeg" />
<img alt="img-110.jpeg" src="img-110.jpeg" />
<img alt="img-111.jpeg" src="img-111.jpeg" />
<img alt="img-112.jpeg" src="img-112.jpeg" />
<img alt="img-113.jpeg" src="img-113.jpeg" />
<img alt="img-114.jpeg" src="img-114.jpeg" />
<img alt="img-115.jpeg" src="img-115.jpeg" />
<img alt="img-116.jpeg" src="img-116.jpeg" />
<img alt="img-117.jpeg" src="img-117.jpeg" />
<img alt="img-118.jpeg" src="img-118.jpeg" />
<img alt="img-119.jpeg" src="img-119.jpeg" />
<img alt="img-120.jpeg" src="img-120.jpeg" />
<img alt="img-121.jpeg" src="img-121.jpeg" />
<img alt="img-122.jpeg" src="img-122.jpeg" />
<img alt="img-123.jpeg" src="img-123.jpeg" />
<img alt="img-124.jpeg" src="img-124.jpeg" />
<img alt="img-125.jpeg" src="img-125.jpeg" />
<img alt="img-126.jpeg" src="img-126.jpeg" />
<img alt="img-127.jpeg" src="img-127.jpeg" />
<img alt="img-128.jpeg" src="img-128.jpeg" />
<img alt="img-129.jpeg" src="img-129.jpeg" />
<img alt="img-130.jpeg" src="img-130.jpeg" />
<img alt="img-131.jpeg" src="img-131.jpeg" />
<img alt="img-132.jpeg" src="img-132.jpeg" />
<img alt="img-133.jpeg" src="img-133.jpeg" />
<img alt="img-134.jpeg" src="img-134.jpeg" />
<img alt="img-135.jpeg" src="img-135.jpeg" />
<img alt="img-136.jpeg" src="img-136.jpeg" />
<img alt="img-137.jpeg" src="img-137.jpeg" />
<img alt="img-138.jpeg" src="img-138.jpeg" />
<img alt="img-139.jpeg" src="img-139.jpeg" />
<img alt="img-140.jpeg" src="img-140.jpeg" />
<img alt="img-141.jpeg" src="img-141.jpeg" />
<img alt="img-142.jpeg" src="img-142.jpeg" />
<img alt="img-143.jpeg" src="img-143.jpeg" />
<img alt="img-144.jpeg" src="img-144.jpeg" />
<img alt="img-145.jpeg" src="img-145.jpeg" />
<img alt="img-146.jpeg" src="img-146.jpeg" />
<img alt="img-147.jpeg" src="img-147.jpeg" />
<img alt="img-148.jpeg" src="img-148.jpeg" />
<img alt="img-149.jpeg" src="img-149.jpeg" />
<img alt="img-150.jpeg" src="img-150.jpeg" />
<img alt="img-151.jpeg" src="img-151.jpeg" />
<img alt="img-152.jpeg" src="img-152.jpeg" />
<img alt="img-153.jpeg" src="img-153.jpeg" />
<img alt="img-154.jpeg" src="img-154.jpeg" />
<img alt="img-155.jpeg" src="img-155.jpeg" />
<img alt="img-156.jpeg" src="img-156.jpeg" />
<img alt="img-157.jpeg" src="img-157.jpeg" />
<img alt="img-158.jpeg" src="img-158.jpeg" />
<img alt="img-159.jpeg" src="img-159.jpeg" />
<img alt="img-160.jpeg" src="img-160.jpeg" />
<img alt="img-161.jpeg" src="img-161.jpeg" />
<img alt="img-162.jpeg" src="img-162.jpeg" />
<img alt="img-163.jpeg" src="img-163.jpeg" />
<img alt="img-164.jpeg" src="img-164.jpeg" />
<img alt="img-165.jpeg" src="img-165.jpeg" />
<img alt="img-166.jpeg" src="img-166.jpeg" />
<img alt="img-167.jpeg" src="img-167.jpeg" />
<img alt="img-168.jpeg" src="img-168.jpeg" />
<img alt="img-169.jpeg" src="img-169.jpeg" />
<img alt="img-170.jpeg" src="img-170.jpeg" />
<img alt="img-171.jpeg" src="img-171.jpeg" />
<img alt="img-172.jpeg" src="img-172.jpeg" />
<img alt="img-173.jpeg" src="img-173.jpeg" />
<img alt="img-174.jpeg" src="img-174.jpeg" />
<img alt="img-175.jpeg" src="img-175.jpeg" />
<img alt="img-176.jpeg" src="img-176.jpeg" />
<img alt="img-177.jpeg" src="img-177.jpeg" />
<img alt="img-178.jpeg" src="img-178.jpeg" />
<img alt="img-179.jpeg" src="img-179.jpeg" />
<img alt="img-180.jpeg" src="img-180.jpeg" />
<img alt="img-181.jpeg" src="img-181.jpeg" />
<img alt="img-182.jpeg" src="img-182.jpeg" />
<img alt="img-183.jpeg" src="img-183.jpeg" />
<img alt="img-184.jpeg" src="img-184.jpeg" />
<img alt="img-185.jpeg" src="img-185.jpeg" />
<img alt="img-186.jpeg" src="img-186.jpeg" />
<img alt="img-187.jpeg" src="img-187.jpeg" />
<img alt="img-188.jpeg" src="img-188.jpeg" />
<img alt="img-189.jpeg" src="img-189.jpeg" />
<img alt="img-190.jpeg" src="img-190.jpeg" />
<img alt="img-191.jpeg" src="img-191.jpeg" />
<img alt="img-192.jpeg" src="img-192.jpeg" />
<img alt="img-193.jpeg" src="img-193.jpeg" />
<img alt="img-194.jpeg" src="img-194.jpeg" />
<img alt="img-195.jpeg" src="img-195.jpeg" />
<img alt="img-196.jpeg" src="img-196.jpeg" />
<img alt="img-197.jpeg" src="img-197.jpeg" />
<img alt="img-198.jpeg" src="img-198.jpeg" />
<img alt="img-199.jpeg" src="img-199.jpeg" />
<img alt="img-200.jpeg" src="img-200.jpeg" />
<img alt="img-201.jpeg" src="img-201.jpeg" />
<img alt="img-202.jpeg" src="img-202.jpeg" />
<img alt="img-203.jpeg" src="img-203.jpeg" />
<img alt="img-204.jpeg" src="img-204.jpeg" />
<img alt="img-205.jpeg" src="img-205.jpeg" />
<img alt="img-206.jpeg" src="img-206.jpeg" />
<img alt="img-207.jpeg" src="img-207.jpeg" />
<img alt="img-208.jpeg" src="img-208.jpeg" />
<img alt="img-209.jpeg" src="img-209.jpeg" />
<img alt="img-210.jpeg" src="img-210.jpeg" />
<img alt="img-211.jpeg" src="img-211.jpeg" />
<img alt="img-212.jpeg" src="img-212.jpeg" />
<img alt="img-213.jpeg" src="img-213.jpeg" />
<img alt="img-214.jpeg" src="img-214.jpeg" />
<img alt="img-215.jpeg" src="img-215.jpeg" />
<img alt="img-216.jpeg" src="img-216.jpeg" />
<img alt="img-217.jpeg" src="img-217.jpeg" />
<img alt="img-218.jpeg" src="img-218.jpeg" />
<img alt="img-219.jpeg" src="img-219.jpeg" />
<img alt="img-220.jpeg" src="img-220.jpeg" />
<img alt="img-221.jpeg" src="img-221.jpeg" />
<img alt="img-222.jpeg" src="img-222.jpeg" />
<img alt="img-223.jpeg" src="img-223.jpeg" />
<img alt="img-224.jpeg" src="img-224.jpeg" />
<img alt="img-225.jpeg" src="img-225.jpeg" />
<img alt="img-226.jpeg" src="img-226.jpeg" />
<img alt="img-227.jpeg" src="img-227.jpeg" />
<img alt="img-228.jpeg" src="img-228.jpeg" />
<img alt="img-229.jpeg" src="img-229.jpeg" />
<img alt="img-230.jpeg" src="img-230.jpeg" />
<img alt="img-231.jpeg" src="img-231.jpeg" />
<img alt="img-232.jpeg" src="img-232.jpeg" />
<img alt="img-233.jpeg" src="img-233.jpeg" />
<img alt="img-234.jpeg" src="img-234.jpeg" />
<img alt="img-235.jpeg" src="img-235.jpeg" />
<img alt="img-236.jpeg" src="img-236.jpeg" />
<img alt="img-237.jpeg" src="img-237.jpeg" />
<img alt="img-238.jpeg" src="img-238.jpeg" />
<img alt="img-239.jpeg" src="img-239.jpeg" />
<img alt="img-240.jpeg" src="img-240.jpeg" />
<img alt="img-241.jpeg" src="img-241.jpeg" />
<img alt="img-242.jpeg" src="img-242.jpeg" />
<img alt="img-243.jpeg" src="img-243.jpeg" />
<img alt="img-244.jpeg" src="img-244.jpeg" />
<img alt="img-245.jpeg" src="img-245.jpeg" />
<img alt="img-246.jpeg" src="img-246.jpeg" />
<img alt="img-247.jpeg" src="img-247.jpeg" />
<img alt="img-248.jpeg" src="img-248.jpeg" />
<img alt="img-249.jpeg" src="img-249.jpeg" />
<img alt="img-250.jpeg" src="img-250.jpeg" />
<img alt="img-251.jpeg" src="img-251.jpeg" />
<img alt="img-252.jpeg" src="img-252.jpeg" />
<img alt="img-253.jpeg" src="img-253.jpeg" />
<img alt="img-254.jpeg" src="img-254.jpeg" />
<img alt="img-255.jpeg" src="img-255.jpeg" />
<img alt="img-256.jpeg" src="img-256.jpeg" />
<img alt="img-257.jpeg" src="img-257.jpeg" />
<img alt="img-258.jpeg" src="img-258.jpeg" />
<img alt="img-259.jpeg" src="img-259.jpeg" />
<img alt="img-260.jpeg" src="img-260.jpeg" />
<img alt="img-261.jpeg" src="img-261.jpeg" />
<img alt="img-262.jpeg" src="img-262.jpeg" />
<img alt="img-263.jpeg" src="img-263.jpeg" />
<img alt="img-264.jpeg" src="img-264.jpeg" />
<img alt="img-265.jpeg" src="img-265.jpeg" />
<img alt="img-266.jpeg" src="img-266.jpeg" />
<img alt="img-267.jpeg" src="img-267.jpeg" />
<img alt="img-268.jpeg" src="img-268.jpeg" />
<img alt="img-269.jpeg" src="img-269.jpeg" />
<img alt="img-270.jpeg" src="img-270.jpeg" />
<img alt="img-271.jpeg" src="img-271.jpeg" />
<img alt="img-272.jpeg" src="img-272.jpeg" />
<img alt="img-273.jpeg" src="img-273.jpeg" />
<img alt="img-274.jpeg" src="img-274.jpeg" />
<img alt="img-275.jpeg" src="img-275.jpeg" />
<img alt="img-276.jpeg" src="img-276.jpeg" />
<img alt="img-277.jpeg" src="img-277.jpeg" />
<img alt="img-278.jpeg" src="img-278.jpeg" />
<img alt="img-279.jpeg" src="img-279.jpeg" />
<img alt="img-280.jpeg" src="img-280.jpeg" />
<img alt="img-281.jpeg" src="img-281.jpeg" />
<img alt="img-282.jpeg" src="img-282.jpeg" />
<img alt="img-283.jpeg" src="img-283.jpeg" />
<img alt="img-284.jpeg" src="img-284.jpeg" />
<img alt="img-285.jpeg" src="img-285.jpeg" />
<img alt="img-286.jpeg" src="img-286.jpeg" />
<img alt="img-287.jpeg" src="img-287.jpeg" />
<img alt="img-288.jpeg" src="img-288.jpeg" />
<img alt="img-289.jpeg" src="img-289.jpeg" />
<img alt="img-290.jpeg" src="img-290.jpeg" />
<img alt="img-291.jpeg" src="img-291.jpeg" />
<img alt="img-292.jpeg" src="img-292.jpeg" />
<img alt="img-293.jpeg" src="img-293.jpeg" />
<img alt="img-294.jpeg" src="img-294.jpeg" />
<img alt="img-295.jpeg" src="img-295.jpeg" />
<img alt="img-296.jpeg" src="img-296.jpeg" />
<img alt="img-297.jpeg" src="img-297.jpeg" />
<img alt="img-298.jpeg" src="img-298.jpeg" />
<img alt="img-299.jpeg" src="img-299.jpeg" />
<img alt="img-300.jpeg" src="img-300.jpeg" />
<img alt="img-301.jpeg" src="img-301.jpeg" />
<img alt="img-302.jpeg" src="img-302.jpeg" />
<img alt="img-303.jpeg" src="img-303.jpeg" />
<img alt="img-304.jpeg" src="img-304.jpeg" />
<img alt="img-305.jpeg" src="img-305.jpeg" />
<img alt="img-306.jpeg" src="img-306.jpeg" />
<img alt="img-307.jpeg" src="img-307.jpeg" />
<img alt="img-308.jpeg" src="img-308.jpeg" />
<img alt="img-309.jpeg" src="img-309.jpeg" />
<img alt="img-310.jpeg" src="img-310.jpeg" />
<img alt="img-311.jpeg" src="img-311.jpeg" />
<img alt="img-312.jpeg" src="img-312.jpeg" />
<img alt="img-313.jpeg" src="img-313.jpeg" />
<img alt="img-314.jpeg" src="img-314.jpeg" />
<img alt="img-315.jpeg" src="img-315.jpeg" />
<img alt="img-316.jpeg" src="img-316.jpeg" />
<img alt="img-317.jpeg" src="img-317.jpeg" />
<img alt="img-318.jpeg" src="img-318.jpeg" />
<img alt="img-319.jpeg" src="img-319.jpeg" />
<img alt="img-320.jpeg" src="img-320.jpeg" />
<img alt="img-321.jpeg" src="img-321.jpeg" />
<img alt="img-322.jpeg" src="img-322.jpeg" />
<img alt="img-323.jpeg" src="img-323.jpeg" />
<img alt="img-324.jpeg" src="img-324.jpeg" />
<img alt="img-325.jpeg" src="img-325.jpeg" />
<img alt="img-326.jpeg" src="img-326.jpeg" />
<img alt="img-327.jpeg" src="img-327.jpeg" />
<img alt="img-328.jpeg" src="img-328.jpeg" />
<img alt="img-329.jpeg" src="img-329.jpeg" />
<img alt="img-330.jpeg" src="img-330.jpeg" />
<img alt="img-331.jpeg" src="img-331.jpeg" />
<img alt="img-332.jpeg" src="img-332.jpeg" />
<img alt="img-333.jpeg" src="img-333.jpeg" />
<img alt="img-334.jpeg" src="img-334.jpeg" />
<img alt="img-335.jpeg" src="img-335.jpeg" />
<img alt="img-336.jpeg" src="img-336.jpeg" />
<img alt="img-337.jpeg" src="img-337.jpeg" />
<img alt="img-338.jpeg" src="img-338.jpeg" />
<img alt="img-339.jpeg" src="img-339.jpeg" />
<img alt="img-340.jpeg" src="img-340.jpeg" />
<img alt="img-341.jpeg" src="img-341.jpeg" />
<img alt="img-342.jpeg" src="img-342.jpeg" />
<img alt="img-343.jpeg" src="img-343.jpeg" />
<img alt="img-344.jpeg" src="img-344.jpeg" />
<img alt="img-345.jpeg" src="img-345.jpeg" />
<img alt="img-346.jpeg" src="img-346.jpeg" />
<img alt="img-347.jpeg" src="img-347.jpeg" />
<img alt="img-348.jpeg" src="img-348.jpeg" />
<img alt="img-349.jpeg" src="img-349.jpeg" />
<img alt="img-350.jpeg" src="img-350.jpeg" />
<img alt="img-351.jpeg" src="img-351.jpeg" />
<img alt="img-352.jpeg" src="img-352.jpeg" />
<img alt="img-353.jpeg" src="img-353.jpeg" />
<img alt="img-354.jpeg" src="img-354.jpeg" />
<img alt="img-355.jpeg" src="img-355.jpeg" />
<img alt="img-356.jpeg" src="img-356.jpeg" />
<img alt="img-357.jpeg" src="img-357.jpeg" />
<img alt="img-358.jpeg" src="img-358.jpeg" />
<img alt="img-359.jpeg" src="img-359.jpeg" />
<img alt="img-360.jpeg" src="img-360.jpeg" />
<img alt="img-361.jpeg" src="img-361.jpeg" />
<img alt="img-362.jpeg" src="img-362.jpeg" />
<img alt="img-363.jpeg" src="img-363.jpeg" />
<img alt="img-364.jpeg" src="img-364.jpeg" />
<img alt="img-365.jpeg" src="img-365.jpeg" />
<img alt="img-366.jpeg" src="img-366.jpeg" />
<img alt="img-367.jpeg" src="img-367.jpeg" />
<img alt="img-368.jpeg" src="img-368.jpeg" />
<img alt="img-369.jpeg" src="img-369.jpeg" />
<img alt="img-370.jpeg" src="img-370.jpeg" />
<img alt="img-371.jpeg" src="img-371.jpeg" />
<img alt="img-372.jpeg" src="img-372.jpeg" />
<img alt="img-373.jpeg" src="img-373.jpeg" />
<img alt="img-374.jpeg" src="img-374.jpeg" />
<img alt="img-375.jpeg" src="img-375.jpeg" />
<img alt="img-376.jpeg" src="img-376.jpeg" />
<img alt="img-377.jpeg" src="img-377.jpeg" />
<img alt="img-378.jpeg" src="img-378.jpeg" />
<img alt="img-379.jpeg" src="img-379.jpeg" />
<img alt="img-380.jpeg" src="img-380.jpeg" />
<img alt="img-381.jpeg" src="img-381.jpeg" />
<img alt="img-382.jpeg" src="img-382.jpeg" />
<img alt="img-383.jpeg" src="img-383.jpeg" />
<img alt="img-384.jpeg" src="img-384.jpeg" />
<img alt="img-385.jpeg" src="img-385.jpeg" />
<img alt="img-386.jpeg" src="img-386.jpeg" />
,0.870,0.9147</p>
<h1></h1>
<p>0.325,0.870,0.870,0.9147</p>
<h1></h1>
<p>0.325,0.870,0.870,0.9147
<img alt="img-387.jpeg" src="img-387.jpeg" /></p>
<h1></h1>
<p>0.325,0.870,0.870,0.9147
<img alt="img-388.jpeg" src="img-388.jpeg" /></p>
<h1></h1>
<p>0.325,0.870,0.870,0.9147
<img alt="img-389.jpeg" src="img-389.jpeg" /></p>
<h1></h1>
<p>0.325,0.870,0.870,0.9147
<img alt="img-390.jpeg" src="img-390.jpeg" /></p>
<h1></h1>
<p>0.325,0.870,0.870,0.9147
<img alt="img-391.jpeg" src="img-391.jpeg" /></p>
<h1></h1>
<p>0.325,0.870,0.870,0.9147
<img alt="img-392.jpeg" src="img-392.jpeg" /></p>
<h1></h1>
<p>0.325,0.870,0.870,0.9147
<img alt="img-393.jpeg" src="img-393.jpeg" /></p>
<h1></h1>
<p>0.325,0.870,0.9147
<img alt="img-394.jpeg" src="img-394.jpeg" /></p>
<h1></h1>
<p>0.325,0.870,0.9147
<img alt="img-395.jpeg" src="img-395.jpeg" /></p>
<h1></h1>
<p>0.325,0.870,0.9147
<img alt="img-396.jpeg" src="img-396.jpeg" /></p>
<h1></h1>
<p>0.325,0.870,0.9147
<img alt="img-397.jpeg" src="img-397.jpeg" /></p>
<h1></h1>
<p>0.325,0.870,0.9147
<img alt="img-398.jpeg" src="img-398.jpeg" /></p>
<h1></h1>
<p>0.325,0.870,0.9147
<img alt="img-399.jpeg" src="img-399.jpeg" /></p>
<h1></h1>
<p>0.325,0.870,0.9147
<img alt="img-400.jpeg" src="img-400.jpeg" /></p>
<h1></h1>
<p>0.325,0.870,0.9147
<img alt="img-401.jpeg" src="img-401.jpeg" /></p>
<h1></h1>
<p>0.325,0.870,0.9147
<img alt="img-402.jpeg" src="img-402.jpeg" /></p>
<h1></h1>
<p>0.325,0.870,0.9147
<img alt="img-403.jpeg" src="img-403.jpeg" />
<img alt="img-404.jpeg" src="img-404.jpeg" />
<img alt="img-405.jpeg" src="img-405.jpeg" /></p>
<h1></h1>
<p>0.325,0.870,0.9147
<img alt="img-406.jpeg" src="img-406.jpeg" /></p>
<h1></h1>
<p>0.325,0.870,0.9147
<img alt="img-407.jpeg" src="img-407.jpeg" /></p>
<h1></h1>
<p>0.325,0.870,0.9147
<img alt="img-408.jpeg" src="img-408.jpeg" />
<img alt="img-409.jpeg" src="img-409.jpeg" />
<img alt="img-410.jpeg" src="img-410.jpeg" /></p>
<h1></h1>
<p>0.325,0.870,0.9147
<img alt="img-411.jpeg" src="img-411.jpeg" />
<img alt="img-412.jpeg" src="img-412.jpeg" />
<img alt="img-413.jpeg" src="img-413.jpeg" />
<img alt="img-414.jpeg" src="img-414.jpeg" />
<img alt="img-415.jpeg" src="img-415.jpeg" />
<img alt="img-416.jpeg" src="img-416.jpeg" />
<img alt="img-417.jpeg" src="img-417.jpeg" />
<img alt="img-418.jpeg" src="img-418.jpeg" />
<img alt="img-419.jpeg" src="img-419.jpeg" />
<img alt="img-420.jpeg" src="img-420.jpeg" />
<img alt="img-421.jpeg" src="img-421.jpeg" />
<img alt="img-422.jpeg" src="img-422.jpeg" />
<img alt="img-423.jpeg" src="img-423.jpeg" />
<img alt="img-424.jpeg" src="img-424.jpeg" />
<img alt="img-425.jpeg" src="img-425.jpeg" />
<img alt="img-426.jpeg" src="img-426.jpeg" />
<img alt="img-427.jpeg" src="img-427.jpeg" />
<img alt="img-428.jpeg" src="img-428.jpeg" />
<img alt="img-429.jpeg" src="img-429.jpeg" />
<img alt="img-430.jpeg" src="img-430.jpeg" />
<img alt="img-431.jpeg" src="img-431.jpeg" />
<img alt="img-432.jpeg" src="img-432.jpeg" />
<img alt="img-433.jpeg" src="img-433.jpeg" />,0.870,0.870,0.91470,0.91470,0.870,0.91470,0.91470,0.91470,0.870,0.870,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0.91470,0</p>
<h1>B. Prompts for the Geoperception Dataset Evaluation</h1>
<h2>PRoMPT TEMPLATE FOR THE POINTLiesONLINE TASK</h2>
<p>Answer me directly just with the all points lie on the line mentioned in the question (do not include the point mentioned in the question).
Answer template:
(If only one point) The other point is: "your point".
Or
(if multiple points) The other points are: "your points".
For example:
The other point is: A
Or
The other points are: A, B, C
Figure 10: Template for the PointLiesOnLine tasks</p>
<h2>PRoMPT TEMPLATE FOR THE POINTLiesONCIRCLE TASK</h2>
<p>Answer me directly just with the all points lie on the circle mentioned in the question.
Answer template:
(If only one point) The point is: "your point".
Or
(If multiple points) The points are: "your points".
For example:
The point is: A
Or:
The points are: A, B, C
Figure 11: Template for the PointLiesOnCircle tasks</p>
<h2>PRoMPT TEMPLATE FOR THE PARALLEL TASK</h2>
<p>Answer me directly just with the all lines which are parallel to the line mentioned in the question (do not include the line mentioned in the question). Answer template:
(If only one line) The line is: "your line".
Or
(If multiple lines) The lines are: "your lines".
For example:
The line is: BC
Or:
The lines are: BC, DE
Figure 12: Template for the Parallel tasks</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ In Mathverse, text-dominant is the version where the problem is mainly represented by text, while in the vision-only version an equivalent problem is represented purely by image.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
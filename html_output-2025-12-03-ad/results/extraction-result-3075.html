<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3075 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3075</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3075</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-77.html">extraction-schema-77</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <p><strong>Paper ID:</strong> paper-266899747</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.04218v2.pdf" target="_blank">Distortions in Judged Spatial Relations in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> We present a benchmark for assessing the capability of Large Language Models (LLMs) to discern intercardinal directions between geographic locations and apply it to three prominent LLMs: GPT-3.5, GPT-4, and Llama-2. This benchmark specifically evaluates whether LLMs exhibit a hierarchical spatial bias similar to humans, where judgments about individual locations' spatial relationships are influenced by the perceived relationships of the larger groups that contain them. To investigate this, we formulated 14 questions focusing on well-known American cities. Seven questions were designed to challenge the LLMs with scenarios potentially influenced by the orientation of larger geographical units, such as states or countries, while the remaining seven targeted locations were less susceptible to such hierarchical categorization. Among the tested models, GPT-4 exhibited superior performance with 55 percent accuracy, followed by GPT-3.5 at 47 percent, and Llama-2 at 45 percent. The models showed significantly reduced accuracy on tasks with suspected hierarchical bias. For example, GPT-4's accuracy dropped to 33 percent on these tasks, compared to 86 percent on others. However, the models identified the nearest cardinal direction in most cases, reflecting their associative learning mechanism, thereby embodying human-like misconceptions. We discuss avenues for improving the spatial reasoning capabilities of LLMs.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3075.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3075.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large transformer-based language model from OpenAI evaluated on a 14-question intercardinal direction benchmark between cities; it showed the highest accuracy among tested models but exhibited systematic hierarchical spatial biases on a subset of questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based large language model (OpenAI). Paper reports it as trained on a very large, diverse text corpus and having superior generalization and reasoning relative to GPT-3.5 and Llama-2; the authors note it can recall geographic facts (e.g., coordinates) but in these queries typically relied on associative patterns in text rather than explicit geometric computation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1 trillion parameters (as stated in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Intercardinal direction benchmark (city-to-city direction questions)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>A 14-question benchmark asking for the intercardinal direction (e.g., NW, SE) from one city to another. Seven questions were chosen to provoke potential hierarchical (categorization) bias (e.g., city pairs whose true direction conflicts with the typical orientation of their containing states/countries), and seven were expected to be less susceptible to such bias. Task requires reasoning about relative geographical positions (spatial orientation) but not precise numeric computation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Text prompt naming two cities (e.g., 'What is the intercardinal direction from [City 1] to [City 2]?'). Each prompt was asked independently (model reset between queries) and repeated 10 times per question.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot single-question prompts; each question reset the model to avoid context carryover. Prompt format: 'What is the intercardinal direction from [City 1] to [City 2]? Just the direction, nothing else.'</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Authors performed qualitative and quantitative analysis: (1) categorized questions as 'suspect' for hierarchical bias vs 'non-suspect'; (2) computed angular deviations of true city-to-city vectors from cardinal axes (0–45°) and examined relationship between deviation and accuracy; (3) inspected outputs and noted that models often printed correct coordinates when prompted but did not appear to use them to compute directions; (4) concluded models rely on associative text-based learning and thus reproduce human-like hierarchical biases rather than performing explicit geometric computation. No internal model probing (e.g., attention or ablation) was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall accuracy on 14-question benchmark: 55.3%. On the 7 'suspect' (hierarchy-biased) questions: 32.9%. On the 7 'non-suspect' questions: 85.7%. Additional breakdowns: in-state questions 100% (2 questions); hierarchical-state category 80.0% (4 questions); country-scale 52.5% (4 questions); continent-scale 25.0% (4 questions). Performance often matched the nearest cardinal direction even when intercardinal answer was correct.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Susceptible to hierarchical (categorization) bias: when city-to-city relationship conflicts with the broader category (state/country) orientation, the model often answers incorrectly (e.g., San Diego→Reno, Portland→Toronto). Tends to pick nearest cardinal axis when angular deviation from cardinal direction is small. Models may recall coordinates but do not reliably use them for geometric computation in zero-shot textual queries. No evidence of algorithmic or stepwise spatial calculation; no tool/GIS use in this experiment. Limited evaluation scope (major, well-known cities) so behavior on sparsely documented locations is unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Compared directly with GPT-3.5 and Llama-2 on the same benchmark—GPT-4 performed best overall and better on suspect hierarchical-bias questions, but all three reproduced human-like hierarchical distortions (similar to documented human cognitive map biases from Stevens & Coupe and Tversky). The authors draw parallels between model errors and classic human spatial biases (hierarchical organization and alignment/rotation biases). No direct numeric comparison to human accuracy on the same question set is provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distortions in Judged Spatial Relations in Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3075.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3075.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3.5 (GPT-3.5 Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI transformer LLM evaluated on the intercardinal direction benchmark; performs well on non-hierarchical direction tasks but drops markedly on questions likely to provoke hierarchical bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based large language model (OpenAI). Paper reports a training corpus of ~300 billion tokens (~17 GB filtered content from Common Crawl, WebText2, books, Wikipedia) and notes lower generalization/reasoning performance relative to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175 billion parameters (as stated in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Intercardinal direction benchmark (city-to-city direction questions)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same 14-question intercardinal direction benchmark described above; tests recognition of relative directions between city pairs and susceptibility to hierarchical bias introduced by larger geographic categories.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Text prompts naming city pairs; zero-shot single-question format, each asked 10 times independently.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot (each question asked alone, model reset between queries). Prompt: 'What is the intercardinal direction from [City 1] to [City 2]? Just the direction, nothing else.'</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Authors analyzed per-question correctness, split by suspected hierarchical bias and by angular deviation from cardinal axes. Found that GPT-3.5 commonly identified nearest cardinal direction but failed more often than GPT-4 on suspect hierarchical-bias items, indicating reliance on associative textual patterns rather than geometric computation. No internal interpretability analyses performed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall accuracy: 47.3%. Suspect (hierarchy-biased) questions: 15.7%. Non-suspect questions: 85.7%. In-state (2 questions) 100.0%. Hierarchical-state 45.0%; country-scale 32.5%; continent-scale 50.0%. Orientation EW 47.8%; NS 56.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Significant performance drop on hierarchical-bias items; confusion increases when the true intercardinal direction deviates only slightly from a cardinal axis. Does not appear to perform explicit spatial computation; more influenced by broad textual associations (e.g., country orientation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Underperforms GPT-4 on overall accuracy and particularly on hierarchical-bias items; closely comparable to Llama-2 in some non-suspect categories but inferior in others. Authors compare model errors qualitatively to human hierarchical mapping biases documented in cognitive psychology literature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distortions in Judged Spatial Relations in Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3075.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3075.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2 70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2 (70 billion parameter variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source transformer LLM (Meta) evaluated on the intercardinal direction benchmark; shows lower overall accuracy and pronounced susceptibility to hierarchical bias on the tested questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer-based language model (Llama-2 family). Paper reports Llama-2 family trained on ~2 trillion tokens across versions and notes the 70B variant was used in experiments; described as having smaller parameter count than GPT-3.5 and GPT-4 and hence lower capacity but computational efficiency advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70 billion parameters (as stated in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Intercardinal direction benchmark (city-to-city direction questions)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same 14-question benchmark; requires judgment of intercardinal directions between city pairs and reveals hierarchical (categorization) biases when city-level directions conflict with implicit state/country orientations.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Text prompts naming two cities per query; zero-shot, each question repeated 10 times with model reset between queries.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot prompts identical to those used with GPT-3.5 and GPT-4 ('What is the intercardinal direction from [City 1] to [City 2]? Just the direction, nothing else.'), asked independently each time.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Same analytic pipeline as for other models: per-question accuracy, classification into suspect vs non-suspect hierarchical-bias groups, and angular deviation calculations. Llama-2 showed lower accuracy particularly on suspect questions and in some close-proximity cases (e.g., San Antonio→Houston). Authors interpret results as evidence of associative text-driven reasoning and possible lack of fine-grained spatial memory.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall accuracy: 44.7%. Suspect (hierarchy-biased) questions: 7.1%. Non-suspect questions: 88.6%. In-state (2 questions) 70.0% (drop relative to other models); hierarchical-state 37.5%; country-scale 45.0%; continent-scale 50.0%. Orientation EW 43.3%; NS 56.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Pronounced failure on hierarchical-bias questions (very low accuracy), sensitivity to close-proximity/near-cardinal-angle cases, and overall lower capacity relative to GPT-4. Like the other models, Llama-2 appears to use associative textual cues rather than explicit geometric computation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Performs worse than GPT-4 overall and on suspect questions; slightly below GPT-3.5 in overall accuracy but comparable on some subcategories (e.g., continent). Authors note all models replicate human-like hierarchical biases, with Llama-2 showing the strongest manifestation of that bias in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distortions in Judged Spatial Relations in Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autonomous GIS: the next-generation AI-powered GIS <em>(Rating: 2)</em></li>
                <li>Geogpt: Understanding and processing geospatial tasks through an autonomous gpt <em>(Rating: 2)</em></li>
                <li>An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities <em>(Rating: 2)</em></li>
                <li>Evaluating the effectiveness of large language models in representing textual descriptions of geometry and spatial relations <em>(Rating: 2)</em></li>
                <li>Towards a foundation model for geospatial artificial intelligence (vision paper) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3075",
    "paper_id": "paper-266899747",
    "extraction_schema_id": "extraction-schema-77",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A large transformer-based language model from OpenAI evaluated on a 14-question intercardinal direction benchmark between cities; it showed the highest accuracy among tested models but exhibited systematic hierarchical spatial biases on a subset of questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Transformer-based large language model (OpenAI). Paper reports it as trained on a very large, diverse text corpus and having superior generalization and reasoning relative to GPT-3.5 and Llama-2; the authors note it can recall geographic facts (e.g., coordinates) but in these queries typically relied on associative patterns in text rather than explicit geometric computation.",
            "model_size": "1 trillion parameters (as stated in paper)",
            "puzzle_name": "Intercardinal direction benchmark (city-to-city direction questions)",
            "puzzle_description": "A 14-question benchmark asking for the intercardinal direction (e.g., NW, SE) from one city to another. Seven questions were chosen to provoke potential hierarchical (categorization) bias (e.g., city pairs whose true direction conflicts with the typical orientation of their containing states/countries), and seven were expected to be less susceptible to such bias. Task requires reasoning about relative geographical positions (spatial orientation) but not precise numeric computation.",
            "input_representation": "Text prompt naming two cities (e.g., 'What is the intercardinal direction from [City 1] to [City 2]?'). Each prompt was asked independently (model reset between queries) and repeated 10 times per question.",
            "prompting_method": "Zero-shot single-question prompts; each question reset the model to avoid context carryover. Prompt format: 'What is the intercardinal direction from [City 1] to [City 2]? Just the direction, nothing else.'",
            "spatial_reasoning_analysis": "Authors performed qualitative and quantitative analysis: (1) categorized questions as 'suspect' for hierarchical bias vs 'non-suspect'; (2) computed angular deviations of true city-to-city vectors from cardinal axes (0–45°) and examined relationship between deviation and accuracy; (3) inspected outputs and noted that models often printed correct coordinates when prompted but did not appear to use them to compute directions; (4) concluded models rely on associative text-based learning and thus reproduce human-like hierarchical biases rather than performing explicit geometric computation. No internal model probing (e.g., attention or ablation) was reported.",
            "performance_metrics": "Overall accuracy on 14-question benchmark: 55.3%. On the 7 'suspect' (hierarchy-biased) questions: 32.9%. On the 7 'non-suspect' questions: 85.7%. Additional breakdowns: in-state questions 100% (2 questions); hierarchical-state category 80.0% (4 questions); country-scale 52.5% (4 questions); continent-scale 25.0% (4 questions). Performance often matched the nearest cardinal direction even when intercardinal answer was correct.",
            "limitations_or_failure_modes": "Susceptible to hierarchical (categorization) bias: when city-to-city relationship conflicts with the broader category (state/country) orientation, the model often answers incorrectly (e.g., San Diego→Reno, Portland→Toronto). Tends to pick nearest cardinal axis when angular deviation from cardinal direction is small. Models may recall coordinates but do not reliably use them for geometric computation in zero-shot textual queries. No evidence of algorithmic or stepwise spatial calculation; no tool/GIS use in this experiment. Limited evaluation scope (major, well-known cities) so behavior on sparsely documented locations is unknown.",
            "comparison_to_other_models_or_humans": "Compared directly with GPT-3.5 and Llama-2 on the same benchmark—GPT-4 performed best overall and better on suspect hierarchical-bias questions, but all three reproduced human-like hierarchical distortions (similar to documented human cognitive map biases from Stevens & Coupe and Tversky). The authors draw parallels between model errors and classic human spatial biases (hierarchical organization and alignment/rotation biases). No direct numeric comparison to human accuracy on the same question set is provided in this paper.",
            "uuid": "e3075.0",
            "source_info": {
                "paper_title": "Distortions in Judged Spatial Relations in Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "Generative Pre-trained Transformer 3.5 (GPT-3.5 Turbo)",
            "brief_description": "An OpenAI transformer LLM evaluated on the intercardinal direction benchmark; performs well on non-hierarchical direction tasks but drops markedly on questions likely to provoke hierarchical bias.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Transformer-based large language model (OpenAI). Paper reports a training corpus of ~300 billion tokens (~17 GB filtered content from Common Crawl, WebText2, books, Wikipedia) and notes lower generalization/reasoning performance relative to GPT-4.",
            "model_size": "175 billion parameters (as stated in paper)",
            "puzzle_name": "Intercardinal direction benchmark (city-to-city direction questions)",
            "puzzle_description": "Same 14-question intercardinal direction benchmark described above; tests recognition of relative directions between city pairs and susceptibility to hierarchical bias introduced by larger geographic categories.",
            "input_representation": "Text prompts naming city pairs; zero-shot single-question format, each asked 10 times independently.",
            "prompting_method": "Zero-shot (each question asked alone, model reset between queries). Prompt: 'What is the intercardinal direction from [City 1] to [City 2]? Just the direction, nothing else.'",
            "spatial_reasoning_analysis": "Authors analyzed per-question correctness, split by suspected hierarchical bias and by angular deviation from cardinal axes. Found that GPT-3.5 commonly identified nearest cardinal direction but failed more often than GPT-4 on suspect hierarchical-bias items, indicating reliance on associative textual patterns rather than geometric computation. No internal interpretability analyses performed.",
            "performance_metrics": "Overall accuracy: 47.3%. Suspect (hierarchy-biased) questions: 15.7%. Non-suspect questions: 85.7%. In-state (2 questions) 100.0%. Hierarchical-state 45.0%; country-scale 32.5%; continent-scale 50.0%. Orientation EW 47.8%; NS 56.0%.",
            "limitations_or_failure_modes": "Significant performance drop on hierarchical-bias items; confusion increases when the true intercardinal direction deviates only slightly from a cardinal axis. Does not appear to perform explicit spatial computation; more influenced by broad textual associations (e.g., country orientation).",
            "comparison_to_other_models_or_humans": "Underperforms GPT-4 on overall accuracy and particularly on hierarchical-bias items; closely comparable to Llama-2 in some non-suspect categories but inferior in others. Authors compare model errors qualitatively to human hierarchical mapping biases documented in cognitive psychology literature.",
            "uuid": "e3075.1",
            "source_info": {
                "paper_title": "Distortions in Judged Spatial Relations in Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Llama-2 70B",
            "name_full": "Llama-2 (70 billion parameter variant)",
            "brief_description": "An open-source transformer LLM (Meta) evaluated on the intercardinal direction benchmark; shows lower overall accuracy and pronounced susceptibility to hierarchical bias on the tested questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2 70B",
            "model_description": "Open-source transformer-based language model (Llama-2 family). Paper reports Llama-2 family trained on ~2 trillion tokens across versions and notes the 70B variant was used in experiments; described as having smaller parameter count than GPT-3.5 and GPT-4 and hence lower capacity but computational efficiency advantages.",
            "model_size": "70 billion parameters (as stated in paper)",
            "puzzle_name": "Intercardinal direction benchmark (city-to-city direction questions)",
            "puzzle_description": "Same 14-question benchmark; requires judgment of intercardinal directions between city pairs and reveals hierarchical (categorization) biases when city-level directions conflict with implicit state/country orientations.",
            "input_representation": "Text prompts naming two cities per query; zero-shot, each question repeated 10 times with model reset between queries.",
            "prompting_method": "Zero-shot prompts identical to those used with GPT-3.5 and GPT-4 ('What is the intercardinal direction from [City 1] to [City 2]? Just the direction, nothing else.'), asked independently each time.",
            "spatial_reasoning_analysis": "Same analytic pipeline as for other models: per-question accuracy, classification into suspect vs non-suspect hierarchical-bias groups, and angular deviation calculations. Llama-2 showed lower accuracy particularly on suspect questions and in some close-proximity cases (e.g., San Antonio→Houston). Authors interpret results as evidence of associative text-driven reasoning and possible lack of fine-grained spatial memory.",
            "performance_metrics": "Overall accuracy: 44.7%. Suspect (hierarchy-biased) questions: 7.1%. Non-suspect questions: 88.6%. In-state (2 questions) 70.0% (drop relative to other models); hierarchical-state 37.5%; country-scale 45.0%; continent-scale 50.0%. Orientation EW 43.3%; NS 56.0%.",
            "limitations_or_failure_modes": "Pronounced failure on hierarchical-bias questions (very low accuracy), sensitivity to close-proximity/near-cardinal-angle cases, and overall lower capacity relative to GPT-4. Like the other models, Llama-2 appears to use associative textual cues rather than explicit geometric computation.",
            "comparison_to_other_models_or_humans": "Performs worse than GPT-4 overall and on suspect questions; slightly below GPT-3.5 in overall accuracy but comparable on some subcategories (e.g., continent). Authors note all models replicate human-like hierarchical biases, with Llama-2 showing the strongest manifestation of that bias in these experiments.",
            "uuid": "e3075.2",
            "source_info": {
                "paper_title": "Distortions in Judged Spatial Relations in Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autonomous GIS: the next-generation AI-powered GIS",
            "rating": 2,
            "sanitized_title": "autonomous_gis_the_nextgeneration_aipowered_gis"
        },
        {
            "paper_title": "Geogpt: Understanding and processing geospatial tasks through an autonomous gpt",
            "rating": 2,
            "sanitized_title": "geogpt_understanding_and_processing_geospatial_tasks_through_an_autonomous_gpt"
        },
        {
            "paper_title": "An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities",
            "rating": 2,
            "sanitized_title": "an_evaluation_of_chatgpt4s_qualitative_spatial_reasoning_capabilities"
        },
        {
            "paper_title": "Evaluating the effectiveness of large language models in representing textual descriptions of geometry and spatial relations",
            "rating": 2,
            "sanitized_title": "evaluating_the_effectiveness_of_large_language_models_in_representing_textual_descriptions_of_geometry_and_spatial_relations"
        },
        {
            "paper_title": "Towards a foundation model for geospatial artificial intelligence (vision paper)",
            "rating": 1,
            "sanitized_title": "towards_a_foundation_model_for_geospatial_artificial_intelligence_vision_paper"
        }
    ],
    "cost": 0.00936125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Distortions in Judged Spatial Relations in Large Language Models 1</p>
<p>Nir Fulman nir.fulman@uni-heidelberg.de 
GIScience Chair
Institute of Geography
Heidelberg University
HeidelbergGermany</p>
<p>Abdulkadir Memduhoğlu memduhoglu@uni-heidelberg.de 
GIScience Chair
Institute of Geography
Heidelberg University
HeidelbergGermany</p>
<p>Department of Geomatic Engineering
Faculty of Engineering
Harran University
SanliurfaTürkiye</p>
<p>Alexander Zipf zipf@uni-heidelberg.de 
GIScience Chair
Institute of Geography
Heidelberg University
HeidelbergGermany</p>
<p>HeiGIT at Heidelberg University
HeidelbergGermany</p>
<p>Distortions in Judged Spatial Relations in Large Language Models 1
E83C88CCFBBA4FC3D569AF5B935F63F91 This manuscript has been accepted for publication in The Professional Geographer.geographic reasoninghierarchical spatial biaslarge language modelsspatial benchmark
We present a benchmark for assessing the capability of Large Language Models (LLMs)    to discern intercardinal directions between geographic locations and apply it to three prominent LLMs: GPT-3.5, GPT-4, and Llama-2.This benchmark specifically evaluates whether LLMs exhibit a hierarchical spatial bias similar to humans, where judgments about individual locations' spatial relationships are influenced by the perceived relationships of the larger groups that contain them.To investigate this, we formulated 14 questions focusing on well-known American cities.Seven questions were designed to challenge the LLMs with scenarios potentially influenced by the orientation of larger geographical units, such as states or countries, while the remaining seven targeted locations were less susceptible to such hierarchical categorization.Among the tested models, GPT-4 exhibited superior performance with 55 percent accuracy, followed by GPT-3.5 at 47 percent, and Llama-2 at 45 percent.The models showed significantly reduced accuracy on tasks with suspected hierarchical bias.For example, GPT-4's accuracy dropped to 33 percent on these tasks, compared to 86 percent on others.However, the models identified the nearest cardinal direction in most cases, reflecting their associative learning mechanism, thereby embodying human-like misconceptions.We discuss avenues for improving the spatial reasoning capabilities of LLMs.</p>
<p>Introduction</p>
<p>The technological landscape in artificial intelligence has been significantly shaped by the advancement of large language models (LLMs) such as the Generative Pre-trained Transformer (GPT) series (OpenAI 2023), along with others like Llama-2 (Touvron et al. 2023).</p>
<p>These models have demonstrated remarkable abilities in understanding and generating text that resembles human writing across a broad range of tasks, including writing code, solving mathematical problems, and logical reasoning, demonstrating their broad applicability across different fields (Bubeck et al. 2023;Yuan et al. 2023;Liu et al. 2023).</p>
<p>Attention has also been directed towards the utility of LLMs within the geographic data analysis domain.Despite facing challenges with numerical accuracy and the inference of abstract relationships (Li and Ning, 2023;Cohn, 2023), these models have demonstrated effectiveness in real-world applications that leverage their extensive database of geographic information, complemented by inference capabilities.Examples of such applications include recalling populations of countries, estimating distances between cities, and planning itineraries in real-world geographical settings (Roberts et al., 2023).The practical utility of LLMs in processing spatial data and reasoning supports the case for their continued exploration and development in this direction.</p>
<p>Furthermore, recent research has unveiled non-spatial biases in LLMs, including logical and cognitive distortions (Gallegos et al., 2023).Cognitive psychology has long recognized systematic errors in human mapping memory, showing how our spatial perceptions often deviate from actual geography (Tversky, 1992).Since the data LLMs are trained on may include human errors and oversimplification of geographical details in such texts, and given these models' tendency to form conceptual associations favoring narrative coherence over geographical fidelity (Vaswani et al. 2017), we hypothesize that LLMs might replicate these human-like spatial biases.Specifically, we ask: "Do LLMs exhibit hierarchy bias in their spatial reasoning?"We examine this possibility by introducing a benchmark aimed at evaluating LLMs' ability to determine intercardinal directions between cities, with a focus on identifying potential hierarchy biases similar to those observed in human cognitive mapping processes.</p>
<p>Background</p>
<p>LLM utilization in geographic data analysis</p>
<p>Investigations into the utility of LLMs within geography have aimed to assess their potential uses and identify the limitations they encounter.These efforts are directed towards enhancing training methodologies, architectural innovations, and ensuring secure implementation (Roberts et al. 2023;Mai et al. 2022).One line of work explored the capabilities of LLMs to translate geospatial tasks expressed in natural language into a set of procedures, and to autonomously interface with external data sources and Geographic Information System (GIS) engines for data retrieval and analysis.Li and Ning (2023) developed a system leveraging the GPT-4 API that autonomously processes spatial problem inputs into a series of operations, retrieves necessary spatial data, generates and performs the coding operations autonomously, and delivers spatial analysis results.Zhang et al. (2023) developed GeoGPT, a framework utilizing GPT-3.5-turbowithin the Langchain architecture, to autonomously interpret and execute geospatial tasks by employing appropriate tools from a predefined GIS tool pool.</p>
<p>Further research has examined LLMs' intrinsic geospatial reasoning capabilities.</p>
<p>Despite challenges with numerical spatial analysis and abstract reasoning, LLMs have shown proficiency in recalling geographic facts, understanding spatial relationships, and applying this knowledge to practical tasks in real-world contexts that do not require precise spatial calculations.Cohn (2023) discovered that ChatGPT-4 has a correctness range of 67-72 percent in generating RCC-8 composition tables.Ji and Gao (2023) encoded textual descriptions of geometric entities using GPT-2 and BERT, finding that while these LLMs captured certain types of geometry and spatial relationships, they faltered in accurately estimating numeric values and retrieving spatially related objects.Mai et al. (2022) demonstrated that GPT-2 and GPT-3 outperformed fully-supervised, task-specific models in semantic geospatial tasks but struggled with predicting accurate geographic coordinates for recognized toponyms.Roberts et al. (2023) showed that GPT-4 faces challenges in optimizing travel routes based on text-based graph descriptions but excels in recalling detailed geographic information, such as population and life expectancy, and performing spatial reasoning in tasks like generating tourism itineraries.Despite these limitations, we deem that the practical geospatial applications LLMs demonstrate justify continued research into their intrinsic geospatial capabilities, both as a precaution regarding the types of errors they might produce and as guidance for further developmental efforts.This study focuses on the potential for distortions in human cognitive maps to manifest in LLMs, exploring a novel aspect of LLMs' interaction with geographic information.</p>
<p>Distortions in cognitive maps</p>
<p>Systematic distortions in the memory of maps are a well-known phenomenon in cognitive psychology, reflecting the ways in which our mental representations of spatial information differ from the actual geography.One notable distortion is referred to as hierarchical organization.It was demonstrated in many studies that human spatial memory tends to be organized hierarchically or categorically, with spatial information clustered into groups such as states or countries (Tversky 1992).When evaluating spatial relationships, such as distance or direction, between points belonging to different groups, the perceived spatial relationship between the larger groups can skew the judgment about the actual relationship between the individual points.</p>
<p>The canonical example of this tendency comes from Stevens and Coupe (1978), who reported on an experiment in which subjects in San Diego, California were asked to indicate from memory the direction to Reno Nevada by drawing a line in the proper orientation on a circle with north noted at the top.Most subjects indicated that Reno is northeast of San Diego, while it is, in fact, northwest.Stevens and Coupe (1978) argued that rather than memorizing the precise locations of every city or the relative positions of all cities, we instead remember the relative locations of states.Within this framework, cities are then categorized and recalled based on the state they are in.Therefore, when people are asked to judge the direction between cities, they do not evaluate it directly but rather, they deduce the relative positions of the cities based on the locations of the states they belong to.In this example, because California is mostly west of Nevada, people often incorrectly assume that all cities in California are west of every city in Nevada.Subsequent experiments demonstrated categorization by states and countries, and by conceptual categories, such as university buildings vs official city buildings vs commercial buildings (Wilton 1979;Maki 1981;Hirtle and Jonides 1985;Chase 1983;Hirtle and Mascolo 1986;Tversky 1992).</p>
<p>Potential for human-like spatial bias in LLMs</p>
<p>Recent papers have exposed a range of biases in LLMs (see Gallegos et al. 2023 for a review).These biases include logical fallacies such as the "Reversal Curse"-where models trained on statements like "A is B" struggle to recognize that "B is A" implies the same relationship in reverse (Berglund et al. 2023).Cohn (2023) similarly found that ChatGPT-4 sometimes reasons correctly about a relation but not its inverse.LLMs also include cognitive biases akin to those found in human reasoning, like the anchoring effect, where initial information disproportionately influences subsequent judgments (Jones and Steinhardt 2022).</p>
<p>These biases underscore the complex relationship between an LLM's training data, its learning mechanisms, and its output.</p>
<p>While human biases in spatial reasoning are rooted in mental mapping (Tversky 1992), which LLMs do not possess, we hypothesize that they may exhibit similar biases, based on three considerations.Firstly, the biases in human spatial reasoning that emerge from mental mappings might find their way into textual descriptions, which serve as the primary dataset for LLMs.These texts can contain direct inaccuracies influenced by common misconceptions or oversimplified understandings of geography, essentially encoding human biases into data that LLMs learn from.Secondly, the simplification of geographic details in textual narratives could lead LLMs to adopt these generalized views.Just as humans often condense spatial relationships for ease of understanding, the textual information absorbed by LLMs may lack the nuance of actual geographic layouts, encouraging a simplified, and sometimes incorrect, replication of spatial reasoning.Lastly, LLMs might develop biases through their inclination to follow conceptual associations learned from the data (Vaswani et al. 2017) even when they conflict with geographic accuracy.</p>
<p>Methods</p>
<p>To answer our research question, we devised a benchmark involving questions on intercardinal directions among well-known American cities and applied it to the models GPT-3.5, GPT-4, and Llama-2.</p>
<p>Our benchmark consists of 14 questions.Among these, seven questions are structured to challenge the LLMs by presenting scenarios where the orientation of larger geographical units could influence the interpretation of directions between cities within them.The remaining seven questions have locations and directions that are less susceptible to the influence of hierarchical categorization, serving as a contrast to the first set.We further varied the pairs to exclude other potential sources of bias.The pairs included various larger geographical units that might induce bias, such as state (in the United States), country, and continent.The locations in nine of the questions have a predominant East-West orientation, while the remaining five questions have a North-South orientation.</p>
<p>We applied the benchmark to Llama-2 70b (Touvron et al. 2023), an open-source model, and GPT-3.5 and GPT-4, developed by OpenAI (2023).Following is an overview of the differences between the models.</p>
<p>• Model size: GPT-4, with its 1 trillion parameters, significantly exceeds GPT-3.5'scapacity of 175 billion parameters (Brown et al. 2020) and Llama-2 with 70 billion parameters (Minaee et al. 2024).Models with more parameters are able to assimilate and maintain more information, potentially enhancing their performance across various assessments (Achiam et al. 2023).</p>
<p>• Training Data: GPT-3.5 was trained on a dataset totaling 300 billion tokens and 17 gigabytes, comprising filtered content from Common Crawl, WebText2, and various books and Wikipedia articles (Minaee et al. 2024).GPT-4 utilized a dataset of 13 trillion tokens and 45 gigabytes.Llama-2, introduced in the same year, deployed a training corpus of 2 trillion tokens across its various versions (7B, 13B, 34B, 70B), sourced from diverse online platforms.Models with larger training datasets are able to assimilate and retain more information, likely leading to improved performance across a range of tasks.</p>
<p>• Generalization and Reasoning: In benchmark evaluations focused on generalization and reasoning skills, GPT-4 surpasses both GPT-3.5 and Llama-2 70B's performance (Minaee et al. 2024).It demonstrates superior abilities in logic-based, multi-choice reading comprehension tasks, suggesting an enhanced capacity for complex problemsolving (Achiam et al. 2023).While GPT-3.5 shows inferior performance in various metrics compared to its successor and Llama-2 70B, the latter's smaller parameter size may offer advantages in terms of computational efficiency (Minaee et al. 2024).</p>
<p>We are not familiar with works that compared the capabilities of these specific models in geospatial tasks.However, GPT-2 and BERT were found to have comparable performance in encoding and analyzing geometric entities and spatial relations, a similarity attributed to their analogous tokenization processes and transformer-based architectures (Ji and Gao 2023).In semantic geospatial tasks, Mai et al. (2022) demonstrated that GPT-3 outperforms GPT-2, a difference attributed to the greater number of model parameters.</p>
<p>We asked each model the same set of 14 questions, each posed 10 times.To ensure that each query was treated independently, we reset the model after each question to enable a 'zeroshot' mode, thereby preventing the model from being influenced by previous questions.The format for the questions was consistent: "What is the intercardinal direction from [City 1] to [City 2]?"All questions are presented in Appendix A.</p>
<p>Results</p>
<p>Performance by question type is summarized in Table 1, and by the question in Table B1 in Appendix B. The first two questions involve directions between cities within the same state.In Question 1, asking about the direction from Dallas to San Antonio, both in Texas, all models achieved perfect scores.Question 2, focusing on the direction from San Antonio to Houston, also in Texas, had GPT 3.5 and GPT 4 maintaining perfect scores, while Llama-2 scored 4/10 (40 percent).This drop in Llama-2's performance might be attributed to the close proximity and slight southern position of San Antonio relative to Houston.Questions 3-5 involve cities that are situated in directions opposite to the predominant relationships between their respective states.Question 3 is based on the San Diego to Reno scenario from Stevens and Coupe (1978).Question 4 inquiries about the direction between Memphis, Tennessee and Milwaukee, Wisconsin.While Tennessee is mostly east of Wisconsin, Memphis lies west of Milwaukee.In these questions, the models scored only 35/90 (38.9 percent) correct answers.For comparison, question 6, from Minneapolis, Minnesota to Chicago, Illinois, is a simple between-state question since Illinois is completely east of Minnesota, aligning with the city-to-city direction.All models scored perfectly on this question.Questions 7-11 feature cities in different countries.In 7 and 8, the cities are situated in the opposite direction relative to their category with suspected bias as compared to each other.</p>
<p>For instance, in Question 7, Toronto is south of Portland, whereas Canada is north of the United States.This question draws from the findings of Stevens and Coupe (1978), where most respondents incorrectly answered that Toronto is north of Portland, a misconception likely stemming from the general positioning of Canada north of the United States (Figure 1).In  Questions 12-14 in our study are inspired by the concept of alignment, as described by Tversky (1992).This concept refers to the human tendency to perceive objects that are grouped together as being more aligned than they actually are.A notable example is the common perception of North America and Europe as being aligned east-west, when in reality, Europe is largely north of the United States.This leads to a frequent error: locations in southern Europe are often thought to be further south than locations in the northern United States, even when the opposite is true.While a categorization of mapping bias is beyond our scope, we group this phenomenon under hierarchy bias, as it involves a misalignment of objects based on the perceived spatial relationship between their broader geographical areas.Note that the term "alignment" is distinct from the concept of AI alignment, which refers to the process and goal of ensuring that artificial intelligence systems operate in accordance with human values and ethical principles (Gabriel 2020).We revisited Tversky's (1992) experiments, focusing on determining directions between Rome, Italy and Philadelphia, Pennsylvania, and between Monaco and Chicago, Illinois.Additionally, we introduced a straightforward example: the direction from Lisbon, Portugal to NYC, United States, where the common perception accurately aligns with reality-Lisbon is indeed south of NYC.All of the models achieved perfect results (10/10) on the Lisbon-NYC question, while on both the Rome-Philadelphia and Monaco-Chicago questions, all models got all answers wrong (0/20).</p>
<p>Overall, GPT-4 shows the highest accuracy (55.3 percent), followed by  and .When it comes to discerning directions with suspected hierarchical bias, GPT-4 significantly outperforms the others, scoring 32.9 percent compared to GPT-3.5's 15.7 percent and Llama-2's 7.1 percent.In tasks without suspected hierarchical bias, all models perform notably better, with scores above 85 percent.There is no observable difference in performance between North-South and East-West orientations.While GPT-4 demonstrates superior performance compared to the other models at the in-state and state levels, we cannot identify a distinct pattern in the hierarchical scale category beyond this.</p>
<p>Finally, we also calculated the angular deviations between the cardinal directions (North-South and East-West) and the lines connecting different city pairs.These deviations, shown in Table B1, represent how closely the direction from one city to another aligns with the nearest cardinal direction, varying from 0 to 45 degrees.Generally, as the deviation angles decrease, the accuracy of LLMs decreases.To clarify this concept, consider the Portland to Toronto direction, which is southeast, with a slight 4-degree deviation from the east-west axis.This minimal deviation may increase the challenge of recognizing it as southeast rather than east, illustrating why smaller deviation angles can lead to lower accuracy in identifying the correct intercardinal direction.However, correct answers were still given for questions with low deviation angles, and vice versa.In all of the suspect bias questions except one (Wilmington to Philadelphia), the majority of incorrect responses correctly identified the cardinal direction closest to the actual directional angle.For example, models almost always guess that Toronto is east of Portland.</p>
<p>Discussion</p>
<p>Our benchmark assessment of the spatial reasoning capabilities of LLMs reveals that they excel in deducing straightforward intercardinal directions but struggle with tasks influenced by hierarchical biases.Yet even in their inaccuracies, the models correctly identified the nearest cardinal direction in the majority of cases.This behavior is not indicative of the models functioning as calculators with innate spatial processing capabilities akin to a geographic information system.Instead, their performance reflects a reliance on associative learning from the textual data in their training sets, which includes human-like biases and misconceptions.Indeed, while the models were able to recall accurate geographic coordinates when prompted, they did not seem to utilize this information in calculating spatial relationships in our querying approach.This is evident in examples like Portland and Toronto, where GPT-4, despite printing the correct coordinates, incorrectly interpreted the directional alignment.GPT-4's superior performance is possibly the result of a larger and more diverse training set, model size and training methods, with the specifics of the latter largely unknown (see Methods section for model comparison).This aligns with the findings by Mai et al. (2022), who attributed GPT-3's enhanced performance in semantic geospatial tasks, compared to GPT-2, to its greater number of model parameters.</p>
<p>Other mapping biases in human cognition could potentially be reflected in LLMs, through the association of locations with other, prominent geographical units.For example, the rotation bias refers to the human tendency to simplify the orientation of geographical elements to align with their 'natural' orientations, as noted by Braine (1978) and Tversky (1992).This understanding, and precise logic should not be expected of them.In light of this perspective, efforts could continue in the integration of LLMs with external GIS engines, as explored in recent works by Li and Ning (2023) and Zhang et al. (2023).This certainly makes sense for tasks that involve complex logical reasoning and that do not require high accuracy.</p>
<p>However, given the effectiveness of LLMs in geospatial tasks that prioritize knowledge integration over pinpoint spatial accuracy, and their inherent flexibility, enhancing their geospatial reasoning capabilities could allow solving simple tasks without requiring specialized GIS engines.Given that the bias we discovered likely stems, at least in part, directly from mistakes and simplifications in the description of geographic entities in the training data, highquality data and detailed descriptions of entities can improve model performance.In addition, the models can be trained directly on spatial relationships of interest, to both improve memory recall and the ability to make inferences about unknown relationships (Ji and Gao 2023).</p>
<p>Finally, drawing on the insights from Mai et al. (2022), enhancing the model architecture to incorporate spatial reasoning and align representations of different modalities like geo-tagged texts and remote sensing (or street-view) images could play a crucial role in addressing spatial biases and limited geospatial capabilities.</p>
<p>Conclusion</p>
<p>We develop and apply a benchmark on three LLMs to assess their ability to discern intercardinal directions.In our findings, the models show distinct patterns in their performance: They fare better in questions without categorization-based distortion in spatial perception, achieving accuracy scores above 85 percent, indicating a strong understanding of straightforward geographical relationships.On the other hand, for questions that were influenced by hierarchical bias-where the spatial relationship could be skewed by broader geographical categories like states or countries-the models displayed much lower accuracy, with GPT-4 achieving only 32.9 percent success in such tasks.In the upcoming phase of our work, we plan to explore additional biases inherent in LLMs' spatial memory, including potential biases related to cultural, linguistic, and contextual factors.</p>
<p>Questions 7 and 8 ,
8
LLMs scored 4/60(7 percent).Conversely, Questions 9 and 10, where the hierarchical categories (Cuba and Santo Domingo, respectively), do not have a predominant directional relationship with the United States, show alignment in the direction of the cities with their countries.Here, LLMs scored 48/60 (80 percent).Question 11, which compares Ecuador and the United States, with no suspect bias related to the relative position of North and South America, resulted in an LLMs score of 20/30 (67 percent) for the directional comparison between Quito and New York City (NYC).Notably, GPT-4 exhibited exceptionally poor performance in this question, scoring 0/10.</p>
<p>Figure 1 .
1
Figure 1.An example representation of hierarchical bias.Although Toronto is geographically south of Portland, it may be perceived as being more north due to its location in Canada, which is predominantly north of the United States, where Portland is situated.</p>
<p>bias was evident in a task where participants, when drawing directions between locations in the San Francisco Bay Area, incorrectly perceived Berkeley as being northeast of Stanford, despite Berkeley actually being northwest of Stanford.This illustrates a common tendency to mentally reorient the area's geography to match the north-south axis, despite the Bay Area's actual northwest-southeast diagonal alignment.Similarly, rotation bias might manifest in LLMs through training text data, where people often express the orientation of geographical elements based on their natural orientations.Cities along the United States west coast, for instance, might be contextually interpreted by these models as more westerly positioned compared to inland areas, regardless of their actual coordinates.Furthermore, our focus on major cities, predominantly within the United States, presents a limitation in the scope of our benchmark's application.This choice was motivated by the availability of well-known geographic locations that are likely to be represented in the training data of the evaluated LLMs.However, the spatial reasoning capabilities of LLMs regarding less-documented settlements, including rural areas and smaller towns, especially in underdeveloped countries, could differ significantly from those observed in this study for several reasons.These places are less likely to be represented in the vast textual data sets LLMs are trained on, potentially leading to a reduction in accuracy and less consistent mistakes than the ones revealed in this paper.Moreover, in the context of the hierarchical spatial bias, the associative learning mechanisms of LLMs may rely on broader, more generalized geographic concepts in the absence of detailed, localized information.To better understand the spatial reasoning capabilities of LLMs, future work should aim to include a more diverse set of locations.This includes not only lesser documented settlements but also places with complex geographic features that challenge the hierarchical categorization tendencies observed in urban American contexts.What should we do about bias in the spatial memory and reasoning of LLMs and their overall capabilities in intrinsic geospatial analysis?One possibility is to reconsider their utility for such tasks altogether.AsBender et al. (2021) have posited, LLMs can be seen as stochastic parrots, merely echoing the patterns found within their vast training datasets without genuine</p>
<p>Table 1 .
1
Category-cased performance, by model
GPT-3.5 GPT-4Llama-2Overall (14)47.3%55.3%44.7%SuspectYes (7)15.7%32.9%7.1%Hier. BiasNo (7)85.7%85.7%88.6%In-State (2)100.0% 100.0%70.0%HierarchicalState (4)45.0%80.0%37.5%ScaleCountry (4)32.5%52.5%45.0%Continent (4)50.0%25.0%50.0%OrientationEW (9)47.8%56.7%43.3%NS (5)56.0%64.0%56.0%
This manuscript has been accepted for publication in The Professional Geographer.
Author BiographiesNIR FULMAN is a Post Doctoral Researcher in the GIScience (Geoinformatics) chair in the Department of Geography at Heidelberg University, Heidelberg, Germany.E-mail: nir.fulman@uni-heidelberg.de.His research focuses on simulation and spatial analysis of urban and regional phenomena, urban mobility, and spatial epidemiology.ABDULKADIR MEMDUHOĞLU is an assistant professor in the Department of Geomatic at Harran University in Şanlıurfa, Türkiye, and a guest researcher in GIScience (Geoinformatics) in the Department of Geography at Heidelberg University, Heidelberg, Germany.Email: memduhoglu@uni-heidelberg.de.His research interests include integrating geospatial data, applying semantic web technologies, and using machine learning to analyze spatiotemporal patterns in GIS and enrich OpenStreetMap (OSM).ALEXANDER ZIPF is a Professor and Chair of GIScience (Geoinformatics) chair in the Department of Geography at Heidelberg University, Heidelberg, Germany.E-mail: zipf@uniheidelberg.de.His main research areas include Volunteered Geographic Information (VGI), Crowdsourcing, Citizen Science, and the analysis and processing of geographic data.He focuses on integrating new methods from Geoinformatics and GIScience into Geography.Appendix B
. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F Leoni Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, 10.48550/arXiv.2303.08774arXiv:2303.087742023arXiv preprint</p>
<p>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021</p>
<p>The Reversal Curse: LLMs trained on" A is B" fail to learn" B is A. L Berglund, M Tong, M Kaufmann, M Balesni, A Stickland, T Korbak, O Evans, 10.48550/arXiv.2309.12288arXiv:2309.122882023arXiv preprint</p>
<p>A new slant on orientation perception. L G Braine, 10.1037/0003-066X.33.1.10American Psychologist. 3311978</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in Neural Information Processing Systems. 202033</p>
<p>S Bubeck, R V. Chandrasekaran, J Eldan, E Gehrke, E Horvitz, P Kamar, Y T Lee, Y Lee, S Li, Lundberg, 10.48550/arXiv.2303.12712arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Spatial representations of taxi drivers. W G Chase, The acquisition of symbolic skills. D Rogers, J Sloboda, Boston, MASpringer US1983</p>
<p>An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities. A G Cohn, 10.48550/arXiv.2309.15577arXiv:2309.155772023RCC-8. arXiv preprint</p>
<p>Artificial intelligence, values, and alignment. Minds and machines. I Gabriel, 202030</p>
<p>I Gallegos, O Ryan, A Rossi, J Barrow, M Tanjim, S Kim, F Dernoncourt, T Yu, R Zhang, N K Ahmed, 10.48550/arXiv.2309.00770arXiv:2309.00770Bias and fairness in large language models: A survey. 2023arXiv preprint</p>
<p>Evidence of hierarchies in cognitive maps. S C Hirtle, J Jonides, 10.3758/BF03197683Memory &amp; Cognition. 1331985</p>
<p>Effect of semantic clustering on the memory of spatial locations. S C Hirtle, M F Mascolo, 10.1037/0278-7393.12.2.182Journal of Experimental Psychology: Learning, Memory, and Cognition. 1221821986</p>
<p>Evaluating the effectiveness of large language models in representing textual descriptions of geometry and spatial relations. Y Ji, S Gao, 10.4230/LIPIcs.GIScience.2023.432023 Proceedings of the 12th International Conference on Geographic Information Science. R Beecham, J A Long, D Smith, Q Zhao, &amp; S Wise, 2023. GIScience 2023277Schloss Dagstuhl --Leibniz-Zentrum für Informatik</p>
<p>Capturing failures of large language models via human cognitive biases. E Jones, J Steinhardt, Advances in Neural Information Processing Systems. 202235</p>
<p>Autonomous GIS: the next-generation AI-powered GIS. Z Li, H Ning, 10.1080/17538947.2023.2278895International Journal of Digital Earth. 1622023</p>
<p>H Liu, R Ning, Z Teng, J Liu, Q Zhou, Y Zhang, 10.48550/arXiv.2304.03439arXiv:2304.03439Evaluating the logical reasoning ability of chatgpt and gpt-4. 2023arXiv preprint</p>
<p>Towards a foundation model for geospatial artificial intelligence (vision paper). G Mai, C Cundy, K Choi, Y Hu, N Lao, S Ermon, 10.1145/3557915.3561043Proceedings of the 30th International Conference on Advances in Geographic Information Systems (SIGSPATIAL '22). the 30th International Conference on Advances in Geographic Information Systems (SIGSPATIAL '22)Association for Computing Machinery2022</p>
<p>Categorization and distance effects with spatial linear orders. R H Maki, 10.1037/0278-7393.7.1.15Journal of Experimental Psychology: Human Learning and Memory. 711981</p>
<p>S Minaee, T Mikolov, N Nikzad, M Chenaghlu, R Socher, X Amatriain, J Gao, 10.48550/arXiv.2402.06196arXiv:2402.06196Large language models: A survey. 2023arXiv preprint</p>
<p>Models: GPT-4 Turbo (128k context window) and GPT-3.5 Turbo (16k context window). Openai, 2023. November 21 versionLarge language model</p>
<p>. Accessed, December 24, 2023</p>
<p>J Roberts, T Lüddecke, S Das, K Han, S Albanie, 10.48550/arXiv.2306.00020arXiv:2306.000202023. GPT4GEO: How a Language Model Sees the World's Geography. arXiv preprint</p>
<p>Distortions in judged spatial relations. A Stevens, P Coupe, 10.1016/0010-0285(78)90006-3Cognitive Psychology. 1041978</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, 10.48550/arXiv.2307.09288arXiv:2307.092882023arXiv preprint</p>
<p>Distortions in cognitive maps. B Tversky, 10.1016/0016-7185(92)90011-RGeoforum. 2321992</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, 10.48550/arXiv.1706.03762arXiv:1706.037622017arXiv preprint</p>
<p>Knowledge of spatial relations: The specification of the information used in making inferences. R N Wilton, 10.1080/14640747908400713Quarterly Journal of Experimental Psychology. 3111979</p>
<p>How well do Large Language Models perform in Arithmetic tasks. Z Yuan, H Yuan, C Tan, W Wang, S Huang, 10.48550/arXiv.2304.02015arXiv:2304.020152023arXiv preprint</p>
<p>Geogpt: Understanding and processing geospatial tasks through an autonomous gpt. Y Zhang, C Wei, S Wu, Z He, W Yu, 10.48550/arXiv.2307.07930arXiv:2307.079302023arXiv preprint</p>
<p>Appendix A Questions 1. What is the intercardinal direction from. Dallas, Texas to San Antonio, TexasJust the direction, nothing else</p>
<p>What is the intercardinal direction from. Wilmington, Delaware to Philadelphia, PennsylvaniaJust the direction, nothing else</p>
<p>What is the intercardinal direction from. Memphis, Tennessee to Milwaukee, WisconsinJust the direction, nothing else</p>
<p>What is the intercardinal direction from Minneapolis. Minnesota to Chicago, IllinoisJust the direction, nothing else</p>
<p>What is the intercardinal direction from Tijuana. Baja California to San Antonio, TexasJust the direction, nothing else</p>
<p>What is the intercardinal direction from Portland, Oregon to Toronto, Canada? Just the direction. nothing else</p>
<p>What is the intercardinal direction from. Havana, Cuba to Philadelphia, PennsylvaniaJust the direction, nothing else</p>
<p>What is the intercardinal direction from Monaco to Chicago, Illinois? Just the direction. nothing else</p>
<p>Just the direction, nothing else. This manuscript has been accepted for publication in The Professional Geographer. Lisbon, Portugal to New York City, New York</p>            </div>
        </div>

    </div>
</body>
</html>
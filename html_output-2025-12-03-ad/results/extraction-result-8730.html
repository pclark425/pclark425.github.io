<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8730 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8730</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8730</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-270371763</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2024.naacl-long.409.pdf" target="_blank">CERET: Cost-Effective Extrinsic Refinement for Text Generation</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are powerful models for generation tasks, but they may not generate good quality outputs in their first attempt. Apart from model fine-tuning, existing approaches to improve prediction accuracy and quality typically involve LLM self-improvement / self-reflection that incorporate feedback from models themselves. Despite their effectiveness, these methods are hindered by their high computational cost and lack of scalability. In this work, we propose CERET, a method for refining text generations by considering semantic stability, entailment and inter-sample uncertainty measures. Experimental results show that CERET outperforms Self-consistency and Self-rerank baselines consistently under various task setups, by 1.6% in Rouge-1 for abstractive summarization and 3.5% in hit rate for question answering. Compared to LLM Self-rerank method, our approach only requires 9.4% of its latency and is more cost-effective.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8730.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8730.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-rerank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Self-rerank (baseline used in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A refinement baseline where the base LLM is given all candidate generations plus the task context and instructed to select the single best prediction (a reranking via an extra LLM call).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna v1.3 / Llama 2 chat (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source chat models used as base LLMs in experiments; both models used in 13B parameter variants (Vicuna v1.3 finetuned on ShareGPT conversations; Llama 2 chat finetuned and RLHF-refined).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-rerank</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>After sampling k candidate outputs from the base LLM, feed the full candidate set together with the task context back into the same base LLM and prompt it to choose the single best candidate (a single additional LLM reranking call).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Abstractive summarization and closed-answer Question Answering (QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Dialogue abstractive summarization datasets (TodSum, DialogSum) and closed-answer QA datasets (TriviaQA, Natural Questions) as used in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Not reported as explicit absolute numbers in the paper for Self-rerank alone; the paper states CERET consistently outperforms the Self-rerank baseline (aggregate statement: CERET outperforms Self-consistency and Self-rerank by ~1.6% Rouge-1 on abstractive summarization and ~3.5 percentage points hit rate on QA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>No-refinement baseline examples reported (Vicuna, TodSum Rouge-1 = 38.78; CERET Rouge-1 = 40.69) — these are the paper's no-refinement baseline numbers used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering: single additional LLM call that ingests all candidates and selects the best; uses the base LLM itself as the reranker.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>The paper reports empirical comparisons where CERET (an extrinsic reranking method) outperforms Self-rerank on the evaluated summarization and QA tasks; latency comparisons show Self-rerank has substantially higher inference cost than CERET (see latency result: CERET uses only 9.4% of Self-rerank latency).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-rerank is computationally expensive due to additional LLM inference; the paper highlights high latency/cost as a primary limitation. Exact per-task absolute performance numbers for Self-rerank alone are not provided in the paper's main tables excerpted.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly in experiments to CERET and Self-consistency; CERET outperforms Self-rerank while being much more efficient (9.4% of Self-rerank latency).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CERET: Cost-Effective Extrinsic Refinement for Text Generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8730.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8730.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency (majority-vote over sampled reasoning paths)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding/selection method where the model generates multiple reasoning paths (or outputs) and the final answer is chosen by majority vote among sampled outputs; effective for tasks with fixed answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency for open-ended generations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna v1.3 / Llama 2 chat (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same base LLMs as used elsewhere in the paper; experiments generated k = 5 samples per prompt using beam-search sampling with temperature 0.7.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multiple samples (reasoning paths) with chain-of-thought prompting and select the most frequent answer among them (majority voting), marginalizing over reasoning paths rather than performing iterative correction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Closed-answer Question Answering (TriviaQA, Natural Questions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain / closed-answer QA where answers are scored by exact match / hit rate; self-consistency is applicable because these tasks have (relatively) fixed short answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Paper does not list per-method absolute numbers for Self-consistency in all tables shown, but reports that CERET 'consistently surpasses both Self-rerank and Self-consistency' and that CERET improves hit rate by 1.6–4.2 points over no-refinement across QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>No-refinement baseline hit rates are reported (e.g., TriviaQA no-refinement = 57.82 hit rate for Vicuna); CERET achieved higher hit rates (examples: CERET improved to ~61.96 in some configurations).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompting strategy (Chain-of-Thought) plus sampling and majority voting (no additional iterative LLM calls beyond sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>The paper includes Self-consistency as a baseline and reports that CERET outperforms Self-consistency on QA hit rate; self-consistency is noted as cost-effective but limited to fixed-answer tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Applicable only to tasks with fixed answers (cannot be applied to open-ended summarization); effectiveness depends on answer being repeated across samples and on the sampling procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared experimentally to CERET and Self-rerank; CERET outperforms Self-consistency on QA; Self-consistency is noted as cheaper but less generally applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CERET: Cost-Effective Extrinsic Refinement for Text Generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8730.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8730.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-refine (iterative refinement with self-feedback; Madaan et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative method where the LLM gives verbal feedback on its own outputs and incorporates that feedback in subsequent generation rounds to improve responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-refine / iterative self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Repeated cycles where the model inspects its output, produces feedback or critiques, and uses that information to produce a revised output in the next generation round (iterative generate-then-reflect).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Not reported in this paper (Self-refine is cited in related work only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not applicable in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering to elicit model critiques and an iterative loop of generation + self-feedback (verbal feedback integrated into new generation prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Paper cites Self-refine in related work as an approach that enables models to iteratively improve outputs via self-feedback, but this paper does not provide experimental results for Self-refine.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes general limitation of iterative self-improvement approaches: high computational cost and lack of scalability due to repeated LLM calls; iterativeness leads to increased latency and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned alongside other iterative/self-improvement methods (e.g., CRITIC, Reflexion) in related work; contrasted with CERET which aims to avoid expensive iterative LLM inference.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CERET: Cost-Effective Extrinsic Refinement for Text Generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8730.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8730.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CRITIC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CRITIC (LLM tool-interactive critiquing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that empowers LLMs to verify and improve their own outputs using external toolkits, enabling self-correction similar to human tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Critic: Large language models can self-correct with tool-interactive critiquing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>CRITIC (tool-interactive self-critique)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Model uses external tools or toolkits to check and critique its outputs, and then revises those outputs based on the tool-provided evidence or checks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Not reported in this paper (cited in related work only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not applicable in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Integration with external toolkits plus prompt-driven critique loops (external verification + revision).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Paper cites CRITIC as a related approach enabling LLMs to verify and improve outputs, but provides no experimental data for CRITIC in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>As with other iterative/self-feedback approaches, likely higher compute and integration complexity; paper emphasizes such methods are resource-intensive relative to CERET.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned in related work as an example of self-improvement/self-critique approaches that require extra resources; contrasted qualitatively with CERET's lower-cost extrinsic scoring approach.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CERET: Cost-Effective Extrinsic Refinement for Text Generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8730.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8730.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that asks the model to produce intermediate reasoning steps (a chain of thought) to improve complex reasoning performance; frequently used together with sampling and aggregation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna v1.3 / Llama 2 chat (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base LLMs used for experiments; paper reports CoT prompting is adopted for open-domain QA evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain-of-Thought prompting (used with Self-consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prompt the model to output intermediate reasoning steps (CoT) and then either aggregate answers across multiple CoT samples (self-consistency) or apply other selection/reranking; in this paper CoT is used for open-domain QA sampling prior to selection methods.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain / closed-answer Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>QA tasks where generating intermediate reasoning increases the chance of producing correct short answers; used as part of the baseline sampling pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Used as part of Self-consistency baseline; the paper does not isolate CoT-only performance numbers but states Self-consistency (CoT + majority vote) is a baseline compared to CERET.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>No-refinement baselines (no extra selection) exist (e.g., TriviaQA no-ref 57.82 hit rate for Vicuna) but CoT effect not isolated in presented tables.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering to elicit intermediate chain-of-thought outputs; then aggregate sampled outputs (majority vote) without extra iterative feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Chain-of-Thought is used to elicit diverse reasoning paths which are then aggregated by Self-consistency; the paper references prior work (Wei et al., Wang et al.) and uses CoT for QA baselines in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>CoT plus Self-consistency only applies to tasks with fixed answers (majority voting) and increases sampling cost; paper uses k = 5 samples, which is a trade-off between diversity and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared implicitly via Self-consistency baseline to CERET; CERET (non-iterative extrinsic scoring) outperforms CoT+Self-consistency baseline on QA in experiments presented.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CERET: Cost-Effective Extrinsic Refinement for Text Generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8730.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8730.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (language agents with verbal reinforcement learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced approach that lets language agents improve behavior via verbal reinforcement learning and reflective verbalization, cited in the paper's related work and references.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Referenced as an approach in which agents use verbal reinforcement signals and reflective processes to improve decision-making and outputs over time (cited in related work; not applied in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Not reported in this paper (only cited in references).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not applicable here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Verbal reinforcement learning and reflective verbalization (external reference only; mechanism details are in the cited paper, not this one).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>No experimental evidence in this paper; Reflexion is mentioned in the bibliography as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Listed among other iterative/self-improvement methods in related work; contrasted at a high level with CERET's non-iterative extrinsic scoring approach.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CERET: Cost-Effective Extrinsic Refinement for Text Generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Self-consistency for open-ended generations <em>(Rating: 2)</em></li>
                <li>Critic: Large language models can self-correct with tool-interactive critiquing <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8730",
    "paper_id": "paper-270371763",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Self-rerank",
            "name_full": "LLM Self-rerank (baseline used in this paper)",
            "brief_description": "A refinement baseline where the base LLM is given all candidate generations plus the task context and instructed to select the single best prediction (a reranking via an extra LLM call).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Vicuna v1.3 / Llama 2 chat (13B)",
            "model_description": "Open-source chat models used as base LLMs in experiments; both models used in 13B parameter variants (Vicuna v1.3 finetuned on ShareGPT conversations; Llama 2 chat finetuned and RLHF-refined).",
            "reflection_method_name": "Self-rerank",
            "reflection_method_description": "After sampling k candidate outputs from the base LLM, feed the full candidate set together with the task context back into the same base LLM and prompt it to choose the single best candidate (a single additional LLM reranking call).",
            "task_name": "Abstractive summarization and closed-answer Question Answering (QA)",
            "task_description": "Dialogue abstractive summarization datasets (TodSum, DialogSum) and closed-answer QA datasets (TriviaQA, Natural Questions) as used in the paper's experiments.",
            "performance_with_reflection": "Not reported as explicit absolute numbers in the paper for Self-rerank alone; the paper states CERET consistently outperforms the Self-rerank baseline (aggregate statement: CERET outperforms Self-consistency and Self-rerank by ~1.6% Rouge-1 on abstractive summarization and ~3.5 percentage points hit rate on QA).",
            "performance_without_reflection": "No-refinement baseline examples reported (Vicuna, TodSum Rouge-1 = 38.78; CERET Rouge-1 = 40.69) — these are the paper's no-refinement baseline numbers used for comparison.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering: single additional LLM call that ingests all candidates and selects the best; uses the base LLM itself as the reranker.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "The paper reports empirical comparisons where CERET (an extrinsic reranking method) outperforms Self-rerank on the evaluated summarization and QA tasks; latency comparisons show Self-rerank has substantially higher inference cost than CERET (see latency result: CERET uses only 9.4% of Self-rerank latency).",
            "limitations_or_failure_cases": "Self-rerank is computationally expensive due to additional LLM inference; the paper highlights high latency/cost as a primary limitation. Exact per-task absolute performance numbers for Self-rerank alone are not provided in the paper's main tables excerpted.",
            "comparison_to_other_methods": "Compared directly in experiments to CERET and Self-consistency; CERET outperforms Self-rerank while being much more efficient (9.4% of Self-rerank latency).",
            "ablation_study_results": null,
            "uuid": "e8730.0",
            "source_info": {
                "paper_title": "CERET: Cost-Effective Extrinsic Refinement for Text Generation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Self-consistency",
            "name_full": "Self-consistency (majority-vote over sampled reasoning paths)",
            "brief_description": "A decoding/selection method where the model generates multiple reasoning paths (or outputs) and the final answer is chosen by majority vote among sampled outputs; effective for tasks with fixed answers.",
            "citation_title": "Self-consistency for open-ended generations",
            "mention_or_use": "use",
            "model_name": "Vicuna v1.3 / Llama 2 chat (13B)",
            "model_description": "Same base LLMs as used elsewhere in the paper; experiments generated k = 5 samples per prompt using beam-search sampling with temperature 0.7.",
            "reflection_method_name": "Self-consistency",
            "reflection_method_description": "Generate multiple samples (reasoning paths) with chain-of-thought prompting and select the most frequent answer among them (majority voting), marginalizing over reasoning paths rather than performing iterative correction.",
            "task_name": "Closed-answer Question Answering (TriviaQA, Natural Questions)",
            "task_description": "Open-domain / closed-answer QA where answers are scored by exact match / hit rate; self-consistency is applicable because these tasks have (relatively) fixed short answers.",
            "performance_with_reflection": "Paper does not list per-method absolute numbers for Self-consistency in all tables shown, but reports that CERET 'consistently surpasses both Self-rerank and Self-consistency' and that CERET improves hit rate by 1.6–4.2 points over no-refinement across QA tasks.",
            "performance_without_reflection": "No-refinement baseline hit rates are reported (e.g., TriviaQA no-refinement = 57.82 hit rate for Vicuna); CERET achieved higher hit rates (examples: CERET improved to ~61.96 in some configurations).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompting strategy (Chain-of-Thought) plus sampling and majority voting (no additional iterative LLM calls beyond sampling).",
            "number_of_iterations": null,
            "evidence_for_improvement": "The paper includes Self-consistency as a baseline and reports that CERET outperforms Self-consistency on QA hit rate; self-consistency is noted as cost-effective but limited to fixed-answer tasks.",
            "limitations_or_failure_cases": "Applicable only to tasks with fixed answers (cannot be applied to open-ended summarization); effectiveness depends on answer being repeated across samples and on the sampling procedure.",
            "comparison_to_other_methods": "Compared experimentally to CERET and Self-rerank; CERET outperforms Self-consistency on QA; Self-consistency is noted as cheaper but less generally applicable.",
            "ablation_study_results": null,
            "uuid": "e8730.1",
            "source_info": {
                "paper_title": "CERET: Cost-Effective Extrinsic Refinement for Text Generation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Self-refine",
            "name_full": "Self-refine (iterative refinement with self-feedback; Madaan et al., 2023)",
            "brief_description": "An iterative method where the LLM gives verbal feedback on its own outputs and incorporates that feedback in subsequent generation rounds to improve responses.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Self-refine / iterative self-feedback",
            "reflection_method_description": "Repeated cycles where the model inspects its output, produces feedback or critiques, and uses that information to produce a revised output in the next generation round (iterative generate-then-reflect).",
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "Not reported in this paper (Self-refine is cited in related work only).",
            "performance_without_reflection": "Not applicable in this paper.",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompt engineering to elicit model critiques and an iterative loop of generation + self-feedback (verbal feedback integrated into new generation prompts).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Paper cites Self-refine in related work as an approach that enables models to iteratively improve outputs via self-feedback, but this paper does not provide experimental results for Self-refine.",
            "limitations_or_failure_cases": "Paper notes general limitation of iterative self-improvement approaches: high computational cost and lack of scalability due to repeated LLM calls; iterativeness leads to increased latency and cost.",
            "comparison_to_other_methods": "Mentioned alongside other iterative/self-improvement methods (e.g., CRITIC, Reflexion) in related work; contrasted with CERET which aims to avoid expensive iterative LLM inference.",
            "ablation_study_results": null,
            "uuid": "e8730.2",
            "source_info": {
                "paper_title": "CERET: Cost-Effective Extrinsic Refinement for Text Generation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "CRITIC",
            "name_full": "CRITIC (LLM tool-interactive critiquing)",
            "brief_description": "A method that empowers LLMs to verify and improve their own outputs using external toolkits, enabling self-correction similar to human tool use.",
            "citation_title": "Critic: Large language models can self-correct with tool-interactive critiquing",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "CRITIC (tool-interactive self-critique)",
            "reflection_method_description": "Model uses external tools or toolkits to check and critique its outputs, and then revises those outputs based on the tool-provided evidence or checks.",
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "Not reported in this paper (cited in related work only).",
            "performance_without_reflection": "Not applicable in this paper.",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Integration with external toolkits plus prompt-driven critique loops (external verification + revision).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Paper cites CRITIC as a related approach enabling LLMs to verify and improve outputs, but provides no experimental data for CRITIC in this study.",
            "limitations_or_failure_cases": "As with other iterative/self-feedback approaches, likely higher compute and integration complexity; paper emphasizes such methods are resource-intensive relative to CERET.",
            "comparison_to_other_methods": "Mentioned in related work as an example of self-improvement/self-critique approaches that require extra resources; contrasted qualitatively with CERET's lower-cost extrinsic scoring approach.",
            "ablation_study_results": null,
            "uuid": "e8730.3",
            "source_info": {
                "paper_title": "CERET: Cost-Effective Extrinsic Refinement for Text Generation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that asks the model to produce intermediate reasoning steps (a chain of thought) to improve complex reasoning performance; frequently used together with sampling and aggregation strategies.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "Vicuna v1.3 / Llama 2 chat (13B)",
            "model_description": "Base LLMs used for experiments; paper reports CoT prompting is adopted for open-domain QA evaluations.",
            "reflection_method_name": "Chain-of-Thought prompting (used with Self-consistency)",
            "reflection_method_description": "Prompt the model to output intermediate reasoning steps (CoT) and then either aggregate answers across multiple CoT samples (self-consistency) or apply other selection/reranking; in this paper CoT is used for open-domain QA sampling prior to selection methods.",
            "task_name": "Open-domain / closed-answer Question Answering",
            "task_description": "QA tasks where generating intermediate reasoning increases the chance of producing correct short answers; used as part of the baseline sampling pipeline.",
            "performance_with_reflection": "Used as part of Self-consistency baseline; the paper does not isolate CoT-only performance numbers but states Self-consistency (CoT + majority vote) is a baseline compared to CERET.",
            "performance_without_reflection": "No-refinement baselines (no extra selection) exist (e.g., TriviaQA no-ref 57.82 hit rate for Vicuna) but CoT effect not isolated in presented tables.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering to elicit intermediate chain-of-thought outputs; then aggregate sampled outputs (majority vote) without extra iterative feedback.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Chain-of-Thought is used to elicit diverse reasoning paths which are then aggregated by Self-consistency; the paper references prior work (Wei et al., Wang et al.) and uses CoT for QA baselines in their experiments.",
            "limitations_or_failure_cases": "CoT plus Self-consistency only applies to tasks with fixed answers (majority voting) and increases sampling cost; paper uses k = 5 samples, which is a trade-off between diversity and cost.",
            "comparison_to_other_methods": "Compared implicitly via Self-consistency baseline to CERET; CERET (non-iterative extrinsic scoring) outperforms CoT+Self-consistency baseline on QA in experiments presented.",
            "ablation_study_results": null,
            "uuid": "e8730.4",
            "source_info": {
                "paper_title": "CERET: Cost-Effective Extrinsic Refinement for Text Generation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (language agents with verbal reinforcement learning)",
            "brief_description": "A referenced approach that lets language agents improve behavior via verbal reinforcement learning and reflective verbalization, cited in the paper's related work and references.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Reflexion",
            "reflection_method_description": "Referenced as an approach in which agents use verbal reinforcement signals and reflective processes to improve decision-making and outputs over time (cited in related work; not applied in this paper).",
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "Not reported in this paper (only cited in references).",
            "performance_without_reflection": "Not applicable here.",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Verbal reinforcement learning and reflective verbalization (external reference only; mechanism details are in the cited paper, not this one).",
            "number_of_iterations": null,
            "evidence_for_improvement": "No experimental evidence in this paper; Reflexion is mentioned in the bibliography as related work.",
            "limitations_or_failure_cases": "Not discussed in this paper.",
            "comparison_to_other_methods": "Listed among other iterative/self-improvement methods in related work; contrasted at a high level with CERET's non-iterative extrinsic scoring approach.",
            "ablation_study_results": null,
            "uuid": "e8730.5",
            "source_info": {
                "paper_title": "CERET: Cost-Effective Extrinsic Refinement for Text Generation",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Self-consistency for open-ended generations",
            "rating": 2,
            "sanitized_title": "selfconsistency_for_openended_generations"
        },
        {
            "paper_title": "Critic: Large language models can self-correct with tool-interactive critiquing",
            "rating": 2,
            "sanitized_title": "critic_large_language_models_can_selfcorrect_with_toolinteractive_critiquing"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 1,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        }
    ],
    "cost": 0.01508925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CERET: Cost-Effective Extrinsic Refinement for Text Generation</p>
<p>Jason Cai 
AWS AI Labs</p>
<p>Hang Su 
AWS AI Labs</p>
<p>Monica Sunkara sunkaral@amazon.com 
AWS AI Labs</p>
<p>Igor Shalyminov shalymin@amazon.com 
AWS AI Labs</p>
<p>Saab Mansour saabm@amazon.com 
AWS AI Labs</p>
<p>CERET: Cost-Effective Extrinsic Refinement for Text Generation
24887A59DDC1281C953C5DC1C52F72F3
Large Language Models (LLMs) are powerful models for generation tasks, but they may not generate good quality outputs in their first attempt.Apart from model fine-tuning, existing approaches to improve prediction accuracy and quality typically involve LLM selfimprovement / self-reflection that incorporate feedback from models themselves.Despite their effectiveness, these methods are hindered by their high computational cost and lack of scalability.In this work, we propose CERET, a method for refining text generations by considering semantic stability, entailment and intersample uncertainty measures.Experimental results show that CERET outperforms Selfconsistency and Self-rerank baselines consistently under various task setups, by 1.6% in Rouge-1 for abstractive summarization and 3.5% in hit rate for question answering.Compared to LLM Self-rerank method, our approach only requires 9.4% of its latency and is more cost-effective. 1</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) like GPT (Brown et al., 2020), Claude, PaLM (Chowdhery et al., 2022;Anil et al., 2023), and Llama (Touvron et al., 2023) have showcased unprecedented capabilities in natural language understanding and generation.These models, with parameter counts reaching into the hundreds of billions, have become pivotal in advancing the frontier of natural language processing (NLP).Despite their impressive fluency and coherence, language models frequently generate content that is incomplete, biased, or misleading in their initial attempts across a variety of language generation tasks.</p>
<p>The key challenge is that while pre-training equips base models with broad linguistic knowl-edge, it does not necessarily impart the specialized skills needed for particular downstream tasks.Current methodologies for enhancing LLM generation largely involve resource-intensive approaches such as supervised fine-tuning (SFT), which relies heavily on domain-specific training data, or reinforcement learning from human feedback (RLHF), which necessitates extensive human annotations.However, curating large volumes of high-quality domain-specific data and human feedback often proves prohibitively expensive and time-consuming in practice, severely limiting the applicability of SFT and RLHF.By integrating feedback derived from the generated outputs, self-improvement / self-reflection approaches enhance generations in an iterative manner (Madaan et al., 2023;Yao et al., 2023a).These approaches empower the LLM to adapt to specific tasks and domains by learning from its own mistakes and successes.Nevertheless, the substantial cost linked to iterative inference poses challenges for scalability and applicability real-time systems.</p>
<p>This paper introduces CERET, a novel method designed to refine text generation in a rapid, lowresource manner to reduce the need for domainspecific training data or expensive human annotations.The cornerstone of CERET lies in its ability to enhance generated content by holistically considering three key scoring dimensions -semantic stability, entailment, and inter-sample uncertainty measures.</p>
<p>Semantic stability scoring quantifies the linguistic invariance among multiple candidate outputs generated by the base model for the same input, indicating higher confidence for more stable candidates.Entailment scoring leverages natural language inference (NLI) models to quantify the logical entailment relations between candidate outputs, preferring candidates that maximally entail others.Inter-sample uncertainty scoring penalizes candidates that are semantically similar to outputs for different inputs, a signal of greater uncertainty.</p>
<p>Our approach operates in a rapid, zero-shot manner without any domain-specific training data, reward modeling, or human feedback.The proposed scoring and refinement process encapsulates an efficient way to improve text generation across a diverse spectrum of NLP tasks, including abstractive summarization, dialogue response generation, and open-domain question answering.Through a rigorous series of experiments on standard datasets, CERET is empirically validated to significantly outperform baseline methods such as Self-consistency and Self-reranking across both summarization and QA tasks.Beyond its superior performance, CERET stands out for its practicality and cost-effectiveness, making it a promising solution for real-world applications where domainspecific resources and annotations are limited or unavailable.This paper not only presents CERET as a valuable novel contribution to the growing field of NLP but also underscores its potential impact on advancing the practical deployment of text generation across a myriad of domains.The main contributions are summarized as follows:</p>
<p>• CERET is proposed as a holistic framework for enhancing generation quality, encompassing semantic stability, entailment, and intersample uncertainty measures.</p>
<p>• The refinement process is data efficient and cost-effective, without the requirement for domain-specific training data or expensive annotations.</p>
<p>• The proposed approach can be applied across various natural language processing tasks, such as text summarization, dialogue response generation and question-answering systems.</p>
<p>• CERET is highlighted for its practicality and efficiency, presenting only a minor fraction of the usual latency associated with a single generation call, which positions it as a feasible solution for real-world applications.</p>
<p>Approach</p>
<p>2.1 System Architecture CERET consists of three scoring methods, namely Semantic Stability Scoring, Entailment Scoring and Inter-sample Uncertainty Scoring, for calibrating the quality of LLM predictions.The overview of the proposed system is illustrated in Figure 1.Firstly, a diverse set of candidates are sampled from LLMs.Then each individual scoring method will produce a separate score from a certain perspective.Based on the scores in three dimensions, a linear weighted final confidence score is computed to measure the quality of each prediction.The prediction with the highest confidence score is selected as the final model prediction.</p>
<p>Semantic Stability Scoring</p>
<p>We first introduce an intra-sample scoring method, Semantic Stability Scoring, which is motivated by the need to enhance the confidence and reliability of sample generations produced by LLMs.</p>
<p>The scientific rationale is inspired by Kuhn et al. (2023) and Yin et al. (2022), where it was shown that a sample generation exhibits higher confidence when it demonstrates considerable semantic stability or linguistic invariance among other generations.However, the semantic stability measured in Kuhn et al. (2023) involves clustering sampled generations for each sample, which is computationally expensive for real world applications at large scale.</p>
<p>In contrast, we propose a cluster-free method for semantic stability modeling.Specifically, Semantic Stability Scoring is formulated as the following: Given input data sample x, the model generates k predictions (y 1 , ..., y k ).For each y i , a fixed pretrained language model produces its corresponding embedding e(y i ).In practice, we leverage RoBERTa (A Robustly Optimized BERT Pretraining Approach) (Liu et al., 2019) as the pre-trained language model, and the final hidden representation of "<s>" token from RoBERTa, is regarded as e(y i ).To aggregate all intra-sample representations, we treat the average-pooled embedding ē as a stability reference point: ē = mean(e(y 1 ), ..., e(y k ))</p>
<p>(1)</p>
<p>A lower distance between an embedding and the reference point implies a higher stability.We can employ Euclidean distance or cosine distance as the distance metric || • ||.The stability score s i sta is defined as the negative distance between e(y i ) and the stability reference point ē:
s i sta = −||e(y i ) − ē|| (2)</p>
<p>Entailment Scoring</p>
<p>Entailment scoring is another intra-sample scoring method, fully powered by entailment relation: "p</p>
<p>Weighted sum</p>
<p>Refinement</p>
<p>Candidate predictions</p>
<p>(1) A conversation between two individuals about accommodations.The person is requesting information about accommodation in a specific price range with free wifi and parking.</p>
<p>(2) A conversation about finding a place to stay, and visiting a college.The individual is interested in a hotel in the expensive price range with free wifi and parking.</p>
<p>(k) A conversation between two people about searching for a hotel and a college in a certain location and price range.0.16 0.57 0.74</p>
<p>Score Candidate predictions</p>
<p>(1) A conversation between two individuals about accommodations.The person is requesting information about accommodation in a specific price range with free wifi and parking.</p>
<p>(2) A conversation about finding a place to stay, and visiting a college.The individual is interested in a hotel in the expensive price range with free wifi and parking.</p>
<p>(k) A conversation between two people about searching for a hotel and a college in a certain location and price range.This intrinsic connection to human inference aligns closely with the objective that language models should have the capacity to generate content that is not only syntactically accurate but also semantically meaningful.In the entailment scoring process, when an LLM generates k predictions (y 1 , ..., y k ), each prediction's entailment relation to others is quantified by a Natural Language Inference (NLI) model.The scalar value s i j reflects the degree to which the content of y i logically entails y j .</p>
<p>Inter-sample
s i j = ENT(y i , y j )(3)
Although the scalar function for entailment can be evaluated by the base LLM itself, such an approach leads to a higher computational cost.Hence, we resort to a more efficient and lightweight NLI model.Specifically, we adopt DeBERTa (Decoding-enhanced BERT with disentangled attention) (He et al., 2021) for this work.The NLI task is treated as a sequence classification problem: The texts y i , y j are concatenated, with special tokens as separators, to form the input to DeBERTa.The final hidden representations of pretrained De-BERTa are passed to a pooling layer and a classifier, to obtain softmax probability for three categories, namely Neutral, Entailment and Contradiction.The softmax probability for Entailment is used as ENT(y i , y j ).</p>
<p>A generation is plausible if it entails as many other sampled generations as possible.With the top k sampled model predictions, the entailment score for sample y i is computed as follows:
s i ent = 1 k 1≤j̸ =i≤k s i j + LP (y i )(4)
Note that the preferred prediction will likely have rich information, and may be lengthy in certain situations.A length penalty LP (y i ) is applied to this entailment score, in case lengthy outputs harm the expected conciseness.</p>
<p>LP (y
i ) = 1 − (1 + q • len(y i )) p(5)
where 0 ≤ q &lt; 1 and p &gt; 1 are hyperparameters2 .</p>
<p>Inter-sample Uncertainty Scoring</p>
<p>In contrast to the methods above, the following is an inter-sample scoring method, which is inspired by uncertain region analysis.We first build an embedding space for all sampled predictions with a standalone model, e.g., RoBERTa.The rationale behind this inter-sample scoring method is that when a sampled prediction is located near predictions from different input data samples in Suppose dataset D has size N .For each input x, top k predictions are generated by LLM(s), resulting in k • N predictions in total: {y i } 1≤i≤kN .The Euclidean distance of all possible prediction pairs ∥y i − y j ∥ , i ̸ = j are computed and cached.According to Euclidean distance, the nearest neighbor set N (i) is constructed for each prediction y i .The inter-sample uncertainty score s i unc is computed as follows:
s i unc = − j∈N (i) I(x i ̸ = xj ) (1 + ∥y i − y j ∥)(6)
where I(•) is the indicator function.xi denotes the input sample x for prediction y i .Note that possibly xi = xj when i ̸ = j.||y i − y j || in denominator of Equation 6 is a regularization term, ensuring a further y j is assigned with a lower weight for uncertainty.A negative sign is added to ensure that a higher score is better.In case when the dataset is large, the computation cost for obtaining pairwise Euclidean distances and nearest neighbors can be mitigated by limiting data size N to a certain batch (e.g., 1000).Additionally, the LLM generations in practice mostly have the number of sampled predictions k ≤ 20.Thus the efficiency of this method can be maintained.</p>
<p>Computation of Final Score</p>
<p>All separate scores s sta , s ent , s unc are transformed to the interval (0, 1) by applying sigmoid function
sigmoid(x) = 1 1 + e −ux (7)
where u &gt; 0 is an additional scaling factor3 .Since three scores have distinct ranges, u is applied to ensure their scaled ranges are comparable.The final confidence score is a linear weighted score based on three dimensions.
s = α • s sta + β • s ent + γ • s unc (8)
The coefficients α, β, γ are tuned on validation datasets.To mimic the properties of probability for intuitive interpretation, the following constraints are imposed:
α + β + γ = 1 α, β, γ ≥ 0 (9)
3 Experimental Setup</p>
<p>Datasets</p>
<p>We evaluate the proposed approach CERET on Abstractive Summarization and Question Answering (QA) tasks.For summarization, we consider two dialogue summarization datasets: TodSum (Zhao et al., 2021) and DialogSum (Chen et al., 2021), as dialogue summarization has been a challenging summaization use case due to its multi-speaker nature and varying structures.TodSum is a dialogue summarization dataset based on MultiWoz (Budzianowski et al., 2018) (Kwiatkowski et al., 2019) with short answers and with evidence documents discarded.It contains 91,535 QA pairs.For TriviaQA and Natural Questions, the official test set is only available for online benchmarking.We split the official validation sets into validation/test sets with an 1:1 ratio for our experiments.</p>
<p>Baselines and Evaluation Metrics</p>
<p>We choose Vicuna v1.3 (Chiang et al., 2023) and Llama 2 chat (Touvron et al., 2023) as our base LLMs.Vicuna is an open-source chatbot, finetuned from Llama with supervised instruction finetuning using around 125K conversations collected from ShareGPT.Llama 2 was pretrained on publicly available online data sources and trained on 2 trillion tokens, and was initially created through supervised fine-tuning and then iteratively refined using Reinforcement Learning from Human Feedback (RLHF).Both Vicuna v1.3 and Llama 2 were released in mid 2023.</p>
<p>Given each input prompt, we generate k LLM predictions by beam search sampling (Vijayakumar et al., 2016), while setting a high temperature to encourage diversity and increase the scope for improvement.The beam search sampled predictions are considered as No-refinement baseline.We further considered two baselines.(1) Self-rerank: In the Self-rerank approach, all predictions generated by the base LLM and the task context are fed back into the base LLM itself.The model is then instructed to select the single best prediction from the candidate set.The Self-rerank baseline provides insight into the capabilities of the base LLM to refine its own output as a straightforward reranking task.(Prompt templates in Appendix D) (2) Self-consistency: The Self-consistency (Wang et al., 2023) approach determines the best prediction through a majority vote among all generated predictions, after marginalizing out reasoning paths.This is a cost-effective approach for refinement, but it can only be applied to tasks with fixed answers.Hence, it is included as a baseline for open domain QA tasks.Furthermore, we also report Oracle scores, which represent the upper bound of refinement/re-ranking performance: Given an input x, we obtain a candidate prediction set {y i } i , out of these y i 's, we choose the best one according to certain evaluation metric (E.g., Rouge, Exact Match, etc) to compute oracle performance .</p>
<p>On the QA tasks with a closed set of answers, we evaluate the models against Hit Rate: A prediction receives score 1 if it exactly matches one of multiple target answers, otherwise score 0 is assigned.On the summarization tasks assuming more open-ended model outputs, we evaluate the models against Rouge-1/2/L (Lin, 2004) and BERTScore (Zhang et al., 2020).Rouge4 is a series of metrics counting the number of overlapping word n-grams in the reference and the generated summary, working on top of 1-/2-grams (as the index in the metric name denotes).Rouge-L is a variant of the metric based on the Longest Common Subsequence between the reference and the generated summary.BERTScore5 is a semantic similarity metric working in the BERT (Devlin et al., 2019) embedding space by computing pairwise cosine similarities between each predicted summary's token and each reference summary's token.</p>
<p>Implementation Details</p>
<p>For the purpose of experimentation, we opt for 13B models for both Vicuna v1.3 and Llama 2. In the LLM beam search sampling phase, we set the temperature parameter t to 0.7, and for each input sample, we accumulate the top k = 5 LLM predictions for subsequent refinement.We activate the half-precision mode to enhance the efficiency of LLM generation.In order to preserve generation quality, quantization is not applied to the LLMs.The entirety of our experiments is performed with NVIDIA A100 GPUs, conducted in a single run.For TodSum dataset, the LLM generation time (from input to the end of beam search sampling) is 2.38/2.85sec for Vicuna 1.3 and Llama 2 respectively.</p>
<p>Regarding the BERT models integrated into the CERET pipeline, we select base-sized models for efficiency, namely RoBERTa-base (125M) and DeBERTa-v3-base-mnli (184M).The coefficients α, β, γ for final weighted scoring are tuned on separate validation sets, where a grid search is conducted with step size of 0.1.In uncertainty scoring, we found the size of nearest neighborhood s = 3, 5 generally lead to satisfactory performance in validation sets, and it is finally set to 5 in all test settings.Note that after post-processing, the duplicate predictions are merged.A neighborhood of size 5 may represent more than 5 raw predictions.Table 1.The initial performance of Vicuna v1.3 and Llama 2 chat in TodSum and DialogSum only result in moderate quality in generated summaries.However, the introduction of CERET brings obvious benefits into the enhancement of summarization outputs.Specifically, CERET achieves a decent improvement of 0.5-1.9 in Rouge-1 scores and 0.6-1.6 in BERTScore F1.</p>
<p>Results and Analysis</p>
<p>The method consistently outperforms Selfrerank, emphasizing the significance of leveraging semantic stability, entailment, and inter-sample uncertainty measures in refining large language model generations.</p>
<p>Question Answering.As shown in Table 2, the baseline performance of the LLM on Trivi-aQA and Natural Questions reflects a gap between the difficulty of these two tasks.Despite the fact that they are both evaluated in closed-book setting, Natural Questions dataset has lower hit rate as it contains various challenging open-ended questions (e.g., Q: "Philadelphia is known as the city of what?".A: "City of Brotherly Love") Regardless of the challenges, CERET is able to improve upon no-refinement baseline for 1.6-4.2 points in hit rate, which consistently surpasses the both Self-rerank and Self-consistency approaches, indicating its effectiveness across diverse knowledge domains.</p>
<p>Inference Efficiency.Efficient inference is a crucial aspect of deploying language models in real-world applications.We analyze and compare the inference efficiency of the proposed CERET method against LLM Self-rerank.We use latency (in seconds) per input sample as a metric for assessing the efficiency of different inference pipelines.We report results on the TodSum dataset.Since the validation/test sets have size ≤ 1000, we use the entire sets instead of small batches for Inter-Sample Uncertainty scoring.</p>
<p>As shown in Figure 3, CERET exhibits remarkable efficiency advantages compared to LLM Selfrerank.The latency required by CERET is only 9.4% of the latency observed in LLM Self-rerank6 .The majority of the latency in the CERET pipeline is attributed to <em>BERT inference, where </em>BERT refers to RoBERTa and DeBERTa models.The efficient integration of these models within the CERET framework contributes to its overall effectiveness while maintaining a significantly reduced latency compared to Self-rerank approaches.The efficiency improvement is particularly noteworthy, especially considering the demands of real-time applications where low latency is imperative.</p>
<p>Overall Observations.Figure 4 provides a com-  prehensive overview of the relative performance improvement achieved on both validation and test sets.The test performance gains observed are generally on par with the validation settings and, in certain instances, even surpass them, as in the case of Natural Questions.This suggests that the weight tuning strategy employed during validation exhibits robustness and generalizability when applied to test sets.The potential explanation for larger gains in certain test cases could be attributed to the random split of test sets, providing certain sets with more room for improvement.The overall theme in the observed results is the consistently superior performance of CERET across all evaluated tasks.Furthermore, the consistency in performance gains between validation and test settings showcases the reliability and adaptability of the proposed CERET method across various settings in natural language processing tasks.</p>
<p>Ablations and Hyperparameter Analysis</p>
<p>Ablations.We systematically evaluate the impact of individual components within the CERET on both summarization and QA tasks, and present the findings in Table 3.The results indicate that all three scoring dimensions have positive contributions in certain task scenario, compared to no-refinement baseline.Notably, semantic stabil-ity alone improves summarization Rouge-1 scores from 38.78 to 40.27 and 32.67 to 34.17 for Tod-Sum and DialogSum respectively.Similarly, for question-answering, semantic stability increases the hit rate from 57.82 to 61.96, and 18.61.67 to 21.19, which are promising improvements.</p>
<p>Various NLP tasks have their own unique characteristics, suggesting that effectiveness of specific refinement dimensions might vary.For example, when considering the TodSum task, the nuances of entailment play a pivotal role in summarization quality for task oriented dialogues, where entailment scoring leads to the most significant gains.We further observed uncertainty scoring exhibits the best improvement in Appendix A.</p>
<p>These insights underscore the synergies between semantic stability, entailment and uncertainty measures, highlighting their complementary roles in refining language model outputs.The comprehensive integration of these aspects in the CERET method showcases their collective impact, providing a flexible and contextually relevant refinement framework for various base LLMs and natural language processing tasks.</p>
<p>Hyperparameter Analysis.In Equation 8, coefficients α, β, γ are weights for three scoring dimensions respectively.To further investigate the sensitivity of non-trivial coefficients (when all co- efficients are non-zero), a systematic approach was employed to assess the impact of individual coefficients on the overall model performance (Figure 5).A sensitivity analysis was conducted by keeping one coefficient, denoted as x, in a state of flux, while concurrently setting the other two coefficients, y and z, to be y = z = 1 − x 2 Consider the green line in Figure 5: γ is set as the variable, the relationship α = β = 1−γ 2 is maintained.This variation allows for an in-depth exploration of the model's sensitivity to changes in each specific coefficient.illustrates that when all coefficients are non-zero, the model's performance remains relatively stable, with a fluctuation of Rouge-1 within 0.1, indicating the robustness to variations in individual coefficient values.</p>
<p>Related Work</p>
<p>Prompting strategy for LLM improvement/refinement.Improving LLM outputs by achieving behaviors close to reasoning has been explored before (Wei et al., 2022;Wang et al., 2023;Yao et al., 2023a,c;Madaan et al., 2023;Yao et al., 2023b;Gou et al., 2023;Akyurek et al., 2023).Wei et al. (2022) introduce a specific technique of formulating prompts for the model dubbed Chain-of-Thought.Essentially a series of intermediate reasoning steps that the model is asked to explicitly output, Chain of Thought significantly improves the ability of LLMs to perform complex reasoning.Wang et al. (2023) propose a decoding strategy dubbed Self-consistency -under which the model, prompted in a chain-of-thought way, generates a set of sample predictions, or reasoning paths.The paths are then marginalized out, and the most consistent answer (the one which the most reasoning paths lead to) is selected as the final one.Huang et al. (2022) use this approach to improve LLMs without annotated data -they select the most consistent answer from the candidates pool, collect all the reasoning paths leading to that answer, and augment the trainset of the target model with the resulting data points.In contrast to Selfconsistency, the Self-Refine approach of Madaan et al. (2023) assumes that the model iteratively provides verbal feedback on its own outputs, and incorporates it in the next generation round.Moreover, CRITIC (Gou et al., 2023) empowers Language Models (LLMs) to independently verify and improve their own outputs with external toolkits, similar to the way humans make use of tools.All the approaches above require prompt engineering, while we tackle the problem from another perspective.</p>
<p>Model confidence/uncertainty without selffeedback.A parallel line of work in improving LLM generation outputs is related to assessing the model confidence and the uncertainty of its predictions without iterative language model calls (Jiang et al., 2021;Lang et al., 2022;Wang et al., 2022;Kuhn et al., 2023;Ge et al., 2023;Jiang et al., 2023;Vernikos et al., 2023).Kuhn et al. (2023) define semantic entropy, a metric that incorporates linguistic invariance of the individual output candidates sharing identical meanings.This metric helps identify the correct model's predictions as evaluated on question answering task.LLM-Blender Jiang et al. (2023) adopts a two-stage design, rank-and-fuse, to generate highly confident and superior candidate outputs.Ge et al. (2023) use uncertainty estimation in order to create modified pseudolabels, and define uncertainty of a pseudolabel (obtained using stochastic dropout-based model inference) as its proximity to other different pseudolabels for the same data point.Training on the selected pseudolabels increases performance in binary and multiclass classification, as well as Natural Language Understanding tasks.Selecting high-confidence pseudolabels is also a key aspect of the co-training technique proposed by Lang et al. (2022), where both partial access and full access settings are studied.All these methods explore uncertainty/confidence from a certain perspective, while our approach combines the uncertainty/confidence with semantic stability and entailment, and we further proposed a framework for these three dimensions and investigated their synergies.</p>
<p>Conclusions</p>
<p>Our proposed CERET is an efficient framework to enhance text generation without the need for domain-specific training data or expensive annotations.By considering semantic stability, entailment, and inter-sample uncertainty measures, our approach significantly improves the quality of text generation across multiple natural language processing tasks.The efficiency and cost-effectiveness of our approach suggest its potential for wide adoption in real-world applications.</p>
<p>Limitations</p>
<p>CERET can be potentially applied to a wide range of NLP problems, including dialogue response generation, open-ended common sense reasoning, and Natural Language Understanding (NLU) by text filling for text continuation.These topics require dedicated investigation and are not yet covered by this paper.</p>
<p>Our experiments show that beam search sampling almost always provides sufficient room for refinement, according to the oracle performance in Table 1 and Table 2. Nevertheless, in certain task or data scenarios, performances of no-refinement baseline and oracle prediction may be close to each other.In that case, the performance of CERET will be limited by oracle results.Yes, all saints church is located t city cenre.It has free entrance.That sounds wonderful.Could I get the phone number along with the attraction type?It is listed as architecture, and their phone number is 012345678 Thank you so much!That is all the help I need for now.You are very welcome!Have a great day and I hope you enjoy your visit!Call for help anytime.</p>
<p>LLM prediction score A person is seeking help to book a train ticket from Norwich to Cambridge on a Sunday at 12:16, and the agent provides information on the travel time, ticket cost, and the location of All Saints Church in the city center, with free entrance and a listed phone number for architecture enthusiasts.</p>
<p>0.83</p>
<p>A person is seeking information about trains from Norwich to Cambridge and asks about travel time, seat prices, and the location of All Saints Church, and the customer service representative provides the necessary information and offers to book the ticket.0.61</p>
<p>The text is a conversation between a person looking for train information from Norwich to Cambridge and a train information provider, who provides information on train schedules, fares, and nearby attractions.0.12</p>
<p>Ground truth</p>
<p>The user asks duration of the train.This train leaves at 12:15 on sunday, departure is norwich and destination is cambridge.The user wants to know the phone number and type of the attraction.the attraction's name is All Saints Church.The following input is a question from an open domain Question-and-Answering task.</p>
<p>Provide a succinct answer to the question in a single phrase (1-3 words).In addition, provide supporting reasons step by step in the following format: Input example: Who was the first man to walk on the Moon?Output example: Answer: Niel Armstrong.Reasoning: Neil Armstrong became the first human to walk on the moon during NASA's Apollo 11 mission on July 20, 1969.This historic event is well-documented through photographs, videos, audio recordings, and historical records, providing irrefutable evidence of his achievement.The following input consists of generated predictions from a Large Language Model(LLM).Besides standard criteria like correctness and helpfulness, take semantic stability into account: We prefer the candidate that is semantically closer to the majority of predictions.Please choose exactly one best prediction, and output the item number (For example "(8)").If there are multiple identical best answers, choose a random one.</p>
<p>Input: [INPUT]</p>
<p>Figure 2 :
2
Figure 2: Inter-sample uncertainty region</p>
<p>Figure 3 :
3
Figure 3: Latency (sec) per input sample.From left to right: *BERT inference, CERET, and LLM self-rerank.</p>
<p>Figure 5 :
5
Figure 5: Sensitivity analysis of coefficients for TodSum</p>
<p>Input: [<TASK CONTEXT> Candidates: (1) ... (2) ... (k) ...]</p>
<p>Table 1 :
1
Comparison of refinement methods for Abstractive Summarization tasks
4.1 Effectiveness and EfficiencyAbstractive Summarization. The experimental results for dialogue summarization are presented in</p>
<p>Relative performance gains on validation and test sets.The best coefficient combination is tuned on validation sets.Evaluation metrics: Rouge-1 for TodSum and DialogSum, and hit rate for Trivia QA and Natural Questions.
16.0%14.0%12.0%10.0%8.0%6.0%4.0%2.0%0.0%VicunaLlama 2VicunaLlama 2VicunaLlama 2VicunaLlama 2v1.3chatv1.3chatv1.3chatv1.3chatTodSumDialogSumTriviaQANatual QuestionsValidationTestFigure 4: Summarization -Rouge-1 Base LLM Refinement TodSum DialogSum Vicuna v1.3 No 38.78 32.67 Semantic stability only 40.27 34.17 Entailment only 40.67 32.30 Uncertainty only 39.27 32.60 CERET 40.69 34.27QA -Hit rate TriviaQA Natual Questions 57.82 18.61 61.96 21.19 57.66 18.51 59.90 19.46 61.96 21.19</p>
<p>Table 3 :
3
Ablation study of individual scoring dimensions</p>
<p>Input dialogueHi there.Can you help me find a train from Norwich to Cambridge?There are several trains on that route.What time would you like to leave/depart?I would like to leave after 12:15 on a Sunday.I have a 12:16 departure from Norwich.Would you like me to book this for you?I'm not sure if I need to book just yet.Can you tell me the travel time for this please?Sure, TR5225 is a 79 minute trip and each seat costs 14.08 pounds.Can I help you with anything else today?Is there an All Saint's Church?I would like to visit it as well.
17.957.834.1540.4517.834.1040.40Hit Rate17.5 17.6 17.7Hit Rate57.4 57.7 57.5 57.6Rouge-133.85 34.05 33.90 33.95 34.00Rouge-140.15 40.35 40.20 40.25 40.3017.40.2 0.3 0.4 0.5 0.6 0.7 0.8 Coefficient57.30.2 0.3 0.4 0.5 0.6 0.7 0.8 Coefficient33.800.2 0.3 0.4 0.5 0.6 0.7 0.8 Coefficient40.10Coefficient 0.2 0.3 0.4 0.5 0.6 0.7 0.8Figure 6: Sensitivity analysis of coefficients for Natural Questions, TriviaQA, DialogSum and TodSum (from left to right in order)</p>
<p>Table 4 :
4
Qualitative example: TodSumInput questionIn the novel, "Nicholas Nickelby", by Charles Dickens, what was the name of the school, run by Wackford Squeers?Who was the British Admiral who died in 1707 when four of his ships were wrecked in the Scilly Isles?
LLM predictionScoreDotheboys Hall0.55Dotheboys0.41Squeers School0.29Ground Truth Dotheboys HallInput questionLLM predictionScoreSir Cloudesley Shovell0.68Russell0.45Viscount Nicholas Boyle0.39Ground Truth [Cloudesley Shovell, Sir Cloudesley Shovell]</p>
<p>Table 5 :
5
Qualitative examples: TriviaQA 7382The following is a conversation between two individuals.Provide a brief summary in [LENGTH] sentence(s).Output the summary only.
Input example: [Input example]Output example: [Output example]Input: [INPUT]</p>
<p>Table 6 :
6
Prompt template for Abstractive Summarization</p>
<p>Table 7 :
7
Prompt template for QA</p>
<p>Table 8 :
8
Prompt template for LLM Self-rerank</p>
<p>The source code and data samples are released at https://github.com/amazon-science/ CERET-LLM-refine.
In our practice, we chose q = 0 (i.e. no penalty). We found that our beam search sampled predictions generally have very comparable lengths. Nevertheless, the length penalty may benefit other datasets or decoding settings.
For each scoring dimension, there is a dedicated value of u.
https://github.com/pltrdy/rouge
https://github.com/Tiiiger/bert_score
Both CERET and Self-rerank deal with predictions after LLM generation, and hence they don't include the beam search sampling time.
June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171-4186. Association for Computational Linguistics.
AppendixA Ethical ConsiderationsWe have reviewed all licenses of public datasets, which allow the usage for research and paper publication.All datasets are sets are de-identified to ensure anonymity.Our proposed method has a potential for substantial reductions in both the financial and environmental burdens associated with large language model improvement/refinement.Through minimizing the reliance on extensive data collection and human labeling, our approach serves as an effective safeguard for user and data privacy, mitigating the risk of information leakage during the construction of training corpora.During the paper writing process, Generative AI was only used for language checking, paraphrasing and polishing.B Additional Sensitivity AnalysisAs described in 4.2 Hyperparameter Analysis, the sensitivity analysis is conducted by keeping one coefficient, x in a state of flux, while concurrently setting the other two coefficients, y and z, to be y = z = 1−x 2 .Empirical results on four datasets show that the performance variations are very limited, with fluctuations of Rouge-1 within 0.2, and hit rate within 0.4.This indicates the stability of the method to variations in individual coefficient values.C Qualitative AnalysisSelected qualitative examples from both TodSum and TriviaQA datasets are presented in Table4and Table5respectively.Although the top 5 beam search sampled candidates are considered in experiments, in Table4and Table5we only present 3 most representative predictions.In the example from TodSum, CERET is able to select the summary that contains key information "All Saints Church" and also mentions the phone number was provided.In fact, the prediction has the highest entailment score, which means it mostly implies other summaries.Regarding the examples from TriviaQA, the final score is largely determined by semantic stability: "Dotheboys Hall" and "Dotheboys" are close in semantic representation, and "Sir Cloudesley Shovell" (after parsing) actually appears 3 times in top 5 predictions.D Prompt TemplatesThe relevant prompt templates for Abstractive Summarization, QA and Self-rerank are presented in Table6, Table7and Table8respectively.We adopt Chain-of-Thought (CoT) prompting for open domain QA; while for Abstractive Summarization datasets, the key information is usually straightforward, hence we only include a length specification.Regarding LLM Self-rerank, we tested multiple additional instructions related to semantic stability, entailment/implication and uncertainty, and finally chose to include semantic stability only, as it produces most robust outcomes.
RL4F: Generating natural language feedback with reinforcement learning for repairing model outputs. Afra Feyza Akyurek, Ekin Akyurek, Ashwin Kalyan, Peter Clark, Derry Tanti Wijaya, Niket Tandon, 10.18653/v1/2023.acl-long.427Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.10403Palm 2 technical report. 2023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Multiwoz -A largescale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling. Pawel Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva, Stefan Ultes, Milica Osman Ramadan, Gasic, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018. October 31 -November 4. 2018</p>
<p>DialogSum: A real-life scenario dialogue summarization dataset. Yulong Chen, Yang Liu, Liang Chen, Yue Zhang, 10.18653/v1/2021.findings-acl.449Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing ; Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, arXiv:2204.02311Hyung Won Chung. Adam Roberts, Paul Barham; Charles Sutton, Sebastian Gehrmann2023arXiv preprintet al. 2022. Palm: Scaling language modeling with pathways</p>
<p>High quality rather than high model probability: Minimum Bayes risk decoding with neural metrics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, ; , Usa , Markus Freitag, David Grangier, Qijun Tan, Bowen Liang, 10.1162/tacl_a_00491Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN2019. 202210BERT: pre-training of deep bidirectional transformers for language understanding</p>
<p>Entailment as robust self-learner. Jiaxin Ge, Hongyin Luo, Yoon Kim, James R Glass, 10.18653/V1/2023.ACL-LONG.772Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 20231Long Papers), ACL 2023</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, arXiv:2305.117382023arXiv preprint</p>
<p>Deberta: Decoding-enhanced bert with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, International Conference on Learning Representations. 2021</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, 10.48550/ARXIV.2210.11610CoRR, abs/2210.116102022</p>
<p>Self-consistency for open-ended generations. Siddhartha Jain, Xiaofei Ma, 10.48550/ARXIV.2307.06857CoRR, abs/2307.06857Anoop Deoras, and Bing Xiang. 2023</p>
<p>LLM-blender: Ensembling large language models with pairwise ranking and generative fusion. Dongfu Jiang, Xiang Ren, Bill Yuchen, Lin , 10.18653/v1/2023.acl-long.792Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>How can we know when language models know? on the calibration of language models for question answering. Zhengbao Jiang, Jun Araki, Haibo Ding, Graham Neubig, 10.1162/tacl_a_00407Transactions of the Association for Computational Linguistics. 92021</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, 10.18653/V1/P17-1147Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics2017. 2017. July 30 -August 41</p>
<p>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. Lorenz Kuhn, Yarin Gal, Sebastian Farquhar, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Natural questions: a benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P Parikh, Chris Alberti, Danielle Epstein, 10.1162/TACL_A_00276Trans. Assoc. Comput. Linguistics. Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov72019</p>
<p>Co-training improves prompt-based learning for large language models. Hunter Lang, Monica N Agrawal, Yoon Kim, David A Sontag, International Conference on Machine Learning, ICML 2022. Baltimore, Maryland, USAPMLR2022. July 2022162of Proceedings of Machine Learning Research</p>
<p>Latent retrieval for weakly supervised open domain question answering. Kenton Lee, Ming-Wei Chang, Kristina Toutanova, 10.18653/v1/P19-1612Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Prasad Bodhisattwa, Shashank Majumder, Amir Gupta, Peter Yazdanbakhsh, Clark, 10.48550/ARXIV.2303.17651CoRR, abs/2303.176512023</p>
<p>Follow the wisdom of the crowd: Effective text generation via minimum Bayes risk decoding. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao ; Mirac Suzgun, Luke Melas-Kyriazi, Dan Jurafsky, 10.18653/v1/2023.findings-acl.262arXiv:2303.11366Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023. 2023arXiv preprintReflexion: Language agents with verbal reinforcement learning</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Small language models improve giants by rewriting their outputs. Giorgos Vernikos, Arthur Bražinskas, Jakub Adamek, Jonathan Mallinson, Aliaksei Severyn, Eric Malmi, 2023</p>
<p>K Ashwin, Michael Vijayakumar, Cogswell, Qing Ramprasath R Selvaraju, Stefan Sun, David Lee, Dhruv Crandall, Batra, arXiv:1610.02424Diverse beam search: Decoding diverse solutions from neural sequence models. 2016arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Uncertainty estimation and reduction of pre-trained models for text regression. Yuxia Wang, Daniel Beck, Timothy Baldwin, Karin Verspoor, 10.1162/tacl_a_00483Transactions of the Association for Computational Linguistics. 102022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, 2022In NeurIPS</p>
<p>Rescorebert: Discriminative speech recognition rescoring with bert. Liyan Xu, Yile Gu, Jari Kolehmainen, Haidar Khan, Ankur Gandhe, Ariya Rastrow, Andreas Stolcke, Ivan Bulyko, 10.1109/ICASSP43922.2022.9747118ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2022</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023aarXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>Beyond chain-of-thought, effective graph-of-thought reasoning in large language models. Yao Yao, Zuchao Li, Hai Zhao, arXiv:2305.165822023carXiv preprint</p>
<p>On the sensitivity and stability of model interpretations in NLP. Fan Yin, Zhouxing Shi, Cho-Jui Hsieh, Kai-Wei Chang, 10.18653/v1/2022.acl-long.188Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Bertscore: Evaluating text generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020</p>
<p>Todsum: Task-oriented dialogue summarization with state tracking. Lulu Zhao, Fujia Zheng, Keqing He, Weihao Zeng, Yuejie Lei, Huixing Jiang, Wei Wu, Weiran Xu, CoRR, abs/2110.12680Jun Guo, and Fanyu Meng. 2021</p>            </div>
        </div>

    </div>
</body>
</html>
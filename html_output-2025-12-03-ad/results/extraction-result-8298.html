<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8298 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8298</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8298</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-278739476</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.12992v2.pdf" target="_blank">Fractured Chain-of-Thought Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by generating rich intermediate reasoning trajectories, but these approaches incur substantial token costs that impede their deployment in latency-sensitive settings. In this work, we first show that truncated CoT, which stops reasoning before completion and directly generates the final answer, often matches full CoT sampling while using dramatically fewer tokens. Building on this insight, we introduce Fractured Sampling, a unified inference-time strategy that interpolates between full CoT and solution-only sampling along three orthogonal axes: (1) the number of reasoning trajectories, (2) the number of final solutions per trajectory, and (3) the depth at which reasoning traces are truncated. Through extensive experiments on five diverse reasoning benchmarks and several model scales, we demonstrate that Fractured Sampling consistently achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling gains in Pass@k versus token budget. Our analysis reveals how to allocate computation across these dimensions to maximize performance, paving the way for more efficient and scalable LLM reasoning. Code is available at https://github.com/BaohaoLiao/frac-cot.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8298.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8298.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fractured Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fractured Sampling for Long Chain-of-Thought Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inference-time sampling framework that interpolates between full Chain-of-Thought and solution-only sampling by distributing compute across three orthogonal axes: number of reasoning trajectories (n), number of final solutions per trajectory (m), and reasoning depth / intermediate-step sampling (H). It samples responses at intermediate reasoning depths and aggregates them to increase diversity and reduce correlated failures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1 family; Qwen3; Skywork-OR1; DeepScaleR</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Evaluated on multiple instructed reasoning LLMs including DeepSeek-R1 variants (e.g., DeepSeek-R1-Distill-Qwen-1.5B and 7B), Qwen3-1.7B, Skywork-OR1-7B, and DeepScaleR-1.5B-Preview; models are autoregressive transformer reasoning models used with CoT prompts and long-generation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['fractured sampling (intermediate-step sampling)', 'chain-of-thought (CoT) prompting', 'truncated CoT', 'vanilla sampling / trajectory sampling (n-axis)', 'solution-replicate sampling (m-axis)', 'early-stopping based on H-axis consistency', 'best-of-n with process reward model (PRM)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Fractured sampling decomposes a long CoT into H equally-sized intermediate segments and (1) samples n independent thinking trajectories, (2) at each trajectory samples responses at t=1..H (intermediate prefixes) and (3) optionally samples m final answers per prefix. Aggregation is performed over the set of generated answers across depths and trajectories. The method thereby exploits intermediate partial traces f_t^h(x,ε) to detect early conclusions and decorrelate error modes across depths. Implementation parameters used in experiments: default n=16, H=16, m=4; temperature=0.6, top-p=0.95, max tokens=32768.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (explicitly creates diversity by sampling across multiple trajectories, multiple final answers per trajectory, and multiple intermediate depths)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Ablations and comparisons sweeping single axes and joint allocations: single-axis studies for n (trajectory sampling), m (multiple final answers per trajectory), and H (fracturing across intermediate depths) holding other axes fixed; multi-axis comparisons using four schemes (H=1,m=1 baseline; H=1,m=4; H=16,m=1; H=16,m=4) with n in {1,2,4,8,16} and default n=16. Pass@k vs total token budget curves fitted to log-linear laws per axis to compare marginal gains. Best-of-n experiments used a PRM (Qwen2.5-Math-PRM-72B) to rank candidates; denoising ablation (H = -4) kept only last 4 H positions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Five math/scientific reasoning benchmarks: MATH500 Level 5 (134 questions), AIME24 (30 questions), AIME25-I (15), AIMO2 reference questions (10), GPQA Diamond set (198). Evaluations used pass@k and best-of-n accuracy metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Fractured sampling consistently yields higher pass@k for a given token budget than sampling only along n or m. Empirical scaling fits show log-linear relations pass@k ≈ C_* log(B) + c_* and consistently C_H ≥ max{C_n, C_m}, i.e., allocating to depth (H) gives the steepest per-token gains. Best-of-n (PRM) numbers: baseline (H=1,m=1) average accuracy 60.4%; H=1,m=4 -> 61.6%; H=16,m=1 -> 61.4%; denoising (keeping last 4 H positions, H=-4,m=1) -> 68.0%; combining denoising and m (H=-4,m=4) -> 70.8% (reported to outperform a larger model baseline 68.3%). Early stopping reduced tokens by ≈20% on average and in some settings improved accuracy (e.g., +2.9% for DeepScaleR-1.5B-Preview). Figures and tables show fractured-sampling (H) curves lie above n- and m-axes across models and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Truncated CoT (generating answers from partial traces) often matches or exceeds full CoT given a token budget; failures across different intermediate depths often exhibit negative covariance (off-diagonal negative correlations), which fractured sampling exploits to decorrelate failures and increase success probability; expanding H (temporal branching) is typically more effective per token than increasing m (solution replicates); PRMs trained on short CoT can struggle to rank long-CoT outputs, but selecting later H positions (denoising) dramatically improves PRM best-of-n selection.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Sampling across intermediate reasoning steps maximizes diversity and per-token gains by decorrelating error modes; truncated CoT is frequently sufficient to achieve full-CoT quality at much lower token cost; allocating inference budget to depth (H) provides larger marginal returns than allocating to trajectory count (n) or final-solution replicates (m); fractured sampling reshapes the inference-time scaling law and yields superior accuracy-cost trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fractured Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8298.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8298.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Truncated CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Truncated Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practice of stopping the chain-of-thought generation before the final full reasoning trace and directly generating the final answer conditioned on a partial reasoning prefix; shown to often match full CoT sampling while using far fewer tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1 family; Qwen3; Skywork-OR1; DeepScaleR</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same set of instructed reasoning LLMs used in the paper; truncated-CoT is applied at inference by terminating the internal reasoning trace at a chosen H and sampling the answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['truncated chain-of-thought (partial-prefix answer generation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Divide a long CoT into H segments and at a chosen t (prefix length) stop producing further thinking tokens and directly sample the final answer conditioned on the partial trace h_{1:t}. The paper evaluates truncated CoT (dashed orange) versus full CoT sampling (solid blue) under token budget constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (single truncated prefix is a single method, but multiple truncated prefixes at different H introduce diversity when aggregated)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct empirical comparison across five benchmarks: truncated CoT + answer generation is plotted against full CoT sampling for pass@1 versus token budget (Figure 2). Default experimental values: n=16, H=16, m=4 baseline; truncation performed at various H positions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>MATH500 L5, AIME24, AIME25-I, AIMO2, GPQA Diamond</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Across tasks and model scales truncated CoT often matches or exceeds the accuracy of full CoT under a max-token constraint; exact pass@k curves show truncated CoT achieves equal/better pass@1 using substantially fewer thinking tokens (figures referenced). No single scalar summary for all tasks provided beyond plotted curves.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Truncated CoT can avoid 'overthinking' and detect early correct conclusions; thinking steps dominate token cost while final solutions cost little, so truncation saves computation; truncated prefixes can be used in fractured sampling to create diverse, efficient decision points.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Complete Long-CoT traces are not always necessary: truncated CoT preserves or improves accuracy at much lower token cost, motivating fractured sampling and early-stopping strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fractured Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8298.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8298.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Long-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long Chain-of-Thought (Long-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extended chain-of-thought prompting paradigm producing longer, more diverse intermediate reasoning trajectories that often incorporate self-reflection and self-correction to improve accuracy and robustness on multi-step reasoning tasks, at the expense of higher token usage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior methods and model families (e.g., O1 and R1 series) use longer reasoning trajectories and additional mechanisms (self-reflection) to explore larger solution spaces; referenced as prior work motivating the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Long Chain-of-Thought', 'self-reflection', 'self-correction']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Longer CoT traces that include extended step-by-step reasoning and sometimes explicit self-correction routines; used in prior work to generate a richer internal reasoning trajectory before final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (Long-CoT seeks broader, more varied internal reasoning trajectories)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Discussed as prior art and baseline context; not the primary novel experimental intervention (the paper compares full Long-CoT sampling to truncated CoT and to fractured sampling in figures but Long-CoT itself is not newly introduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Referenced in context of same math/scientific reasoning benchmarks and prior demonstrations of Long-CoT improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Prior work reported improvements from longer CoT traces but with dramatic increases in token usage; the present paper empirically shows truncated CoT can match such improvements at lower cost (plots compare full CoT vs truncated CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Long-CoT explores broader solution spaces and aggregates diverse intermediate steps, improving robustness; its disadvantage is high token cost, motivating fractured sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Long-CoT increases accuracy and robustness but is token-inefficient; fractured sampling and truncation can capture many benefits at lower cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fractured Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8298.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8298.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency / Best-of-n</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency decoding (majority voting) and Best-of-n sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Inference-time ensemble strategies: self-consistency aggregates multiple CoT samples via majority voting to improve reliability; best-of-n selects the highest-scoring candidate among n samples using a scoring function such as a process reward model (PRM).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied with evaluated LLMs and with PRM Qwen2.5-Math-PRM-72B for ranking</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Self-consistency implemented as pass@k / majority-aggregation across independently sampled CoT trajectories; best-of-n implemented by scoring candidates with a PRM (Qwen2.5-Math-PRM-72B) and selecting the top scored answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['self-consistency (majority voting across CoT samples)', 'best-of-n (PRM-scored selection)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Self-consistency: draw multiple independent full CoT trajectories (n-axis) and choose answer by majority/aggregation; Best-of-n: generate n candidates (possibly across fractured samples) and select the candidate maximizing a scoring function s(z) (PRM used in experiments). Experiments compare these strategies to fractured sampling across the n, m, and H axes.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar (these methods draw repeated samples of the same type of reasoning—either full trajectories or multiple final answers—so diversity arises from stochasticity rather than structured temporal branching)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Single-axis comparisons include n-axis (trajectory sampling / self-consistency) and m-axis (multiple final answers per trajectory). Best-of-n experiments used PRM ranking of final solutions only (PRM context window limited to 4k tokens). The paper compares H vs n vs m axes under matched token budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>MATH500 L5, AIME24, AIME25-I, AIMO2, GPQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Modest improvements for m-axis best-of-n: average accuracy H=1,m=1 baseline 60.4% -> H=1,m=4 is 61.6%. PRM-based best-of-n underperforms the best fractured configurations unless denoising is applied (keeping last 4 H positions raises H=16,m=1 from 61.4% to 68.0%). Overall, fractured sampling (H-axis) gives larger per-token gains than simply increasing n or m.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>When failure events are highly correlated, replicating similar reasoning (m-axis) yields little benefit. PRMs trained on short-CoT data struggle to reliably rank long-CoT outputs, causing noisy best-of-n selection; denoising (restricting to later H positions) helps PRM selection.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Self-consistency and best-of-n improve reliability but are less efficient per token than fractured sampling along H; correlation structure of failures determines how much gain repeated-similar-sampling provides.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fractured Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8298.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8298.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Early stopping (H-consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Early stopping based on repeated predictions across intermediate depths</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple inference-time heuristic that terminates generation early when the same prediction appears multiple times across sampled intermediate H positions, reducing token use while preserving or improving accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1 family; DeepScaleR; Skywork-OR1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied on long-generation CoT models by checking predictions at pre-specified token indices and stopping when a repetition threshold is met.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['early stopping based on H-axis prediction repetition']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Initialize first H position at token index 6144 then continue generating in steps of 2048 tokens; at each checkpoint generate a solution from current prefix and stop generation when the same prediction appears more than once (or when max tokens reached). If max tokens reached, use final prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar (uses repetition detection among sampled predictions; not itself a diversity-generating method but exploits repeated-consensus behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared vanilla full generation to early-stopping across several models and benchmarks (Table 2). Metrics recorded: accuracy delta and average tokens per question.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>MATH500 L5, AIME25, AIMO2, GPQA (as reported in Table 2) and aggregated average across tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Early stopping saved ~20% of generated tokens on average while preserving accuracy and in some cases improving it (example: DeepScaleR-1.5B-Preview saw +2.9% accuracy). Reported token and accuracy deltas vary across models/tasks; overall accuracy preserved and computational cost reduced.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Consistency across intermediate steps is a useful signal to terminate generation; early-stopping is simple to implement and requires no additional training. When the H resolution is too large, earlier positions are low-accuracy and produce noisy partial solutions, so checkpoint spacing matters.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Early stopping based on repetition across intermediate H positions provides substantial inference cost savings (~20% tokens) and preserves or sometimes improves accuracy, making fractured-sampling practical in latency-sensitive settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fractured Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Let's verify step by step. <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
                <li>The lessons of developing process reward models in mathematical reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8298",
    "paper_id": "paper-278739476",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "Fractured Sampling",
            "name_full": "Fractured Sampling for Long Chain-of-Thought Reasoning",
            "brief_description": "An inference-time sampling framework that interpolates between full Chain-of-Thought and solution-only sampling by distributing compute across three orthogonal axes: number of reasoning trajectories (n), number of final solutions per trajectory (m), and reasoning depth / intermediate-step sampling (H). It samples responses at intermediate reasoning depths and aggregates them to increase diversity and reduce correlated failures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1 family; Qwen3; Skywork-OR1; DeepScaleR",
            "model_description": "Evaluated on multiple instructed reasoning LLMs including DeepSeek-R1 variants (e.g., DeepSeek-R1-Distill-Qwen-1.5B and 7B), Qwen3-1.7B, Skywork-OR1-7B, and DeepScaleR-1.5B-Preview; models are autoregressive transformer reasoning models used with CoT prompts and long-generation settings.",
            "reasoning_methods": [
                "fractured sampling (intermediate-step sampling)",
                "chain-of-thought (CoT) prompting",
                "truncated CoT",
                "vanilla sampling / trajectory sampling (n-axis)",
                "solution-replicate sampling (m-axis)",
                "early-stopping based on H-axis consistency",
                "best-of-n with process reward model (PRM)"
            ],
            "reasoning_methods_description": "Fractured sampling decomposes a long CoT into H equally-sized intermediate segments and (1) samples n independent thinking trajectories, (2) at each trajectory samples responses at t=1..H (intermediate prefixes) and (3) optionally samples m final answers per prefix. Aggregation is performed over the set of generated answers across depths and trajectories. The method thereby exploits intermediate partial traces f_t^h(x,ε) to detect early conclusions and decorrelate error modes across depths. Implementation parameters used in experiments: default n=16, H=16, m=4; temperature=0.6, top-p=0.95, max tokens=32768.",
            "reasoning_diversity": "both (explicitly creates diversity by sampling across multiple trajectories, multiple final answers per trajectory, and multiple intermediate depths)",
            "reasoning_diversity_experimental_setup": "Ablations and comparisons sweeping single axes and joint allocations: single-axis studies for n (trajectory sampling), m (multiple final answers per trajectory), and H (fracturing across intermediate depths) holding other axes fixed; multi-axis comparisons using four schemes (H=1,m=1 baseline; H=1,m=4; H=16,m=1; H=16,m=4) with n in {1,2,4,8,16} and default n=16. Pass@k vs total token budget curves fitted to log-linear laws per axis to compare marginal gains. Best-of-n experiments used a PRM (Qwen2.5-Math-PRM-72B) to rank candidates; denoising ablation (H = -4) kept only last 4 H positions.",
            "task_or_benchmark": "Five math/scientific reasoning benchmarks: MATH500 Level 5 (134 questions), AIME24 (30 questions), AIME25-I (15), AIMO2 reference questions (10), GPQA Diamond set (198). Evaluations used pass@k and best-of-n accuracy metrics.",
            "performance_results": "Fractured sampling consistently yields higher pass@k for a given token budget than sampling only along n or m. Empirical scaling fits show log-linear relations pass@k ≈ C_* log(B) + c_* and consistently C_H ≥ max{C_n, C_m}, i.e., allocating to depth (H) gives the steepest per-token gains. Best-of-n (PRM) numbers: baseline (H=1,m=1) average accuracy 60.4%; H=1,m=4 -&gt; 61.6%; H=16,m=1 -&gt; 61.4%; denoising (keeping last 4 H positions, H=-4,m=1) -&gt; 68.0%; combining denoising and m (H=-4,m=4) -&gt; 70.8% (reported to outperform a larger model baseline 68.3%). Early stopping reduced tokens by ≈20% on average and in some settings improved accuracy (e.g., +2.9% for DeepScaleR-1.5B-Preview). Figures and tables show fractured-sampling (H) curves lie above n- and m-axes across models and tasks.",
            "qualitative_findings": "Truncated CoT (generating answers from partial traces) often matches or exceeds full CoT given a token budget; failures across different intermediate depths often exhibit negative covariance (off-diagonal negative correlations), which fractured sampling exploits to decorrelate failures and increase success probability; expanding H (temporal branching) is typically more effective per token than increasing m (solution replicates); PRMs trained on short CoT can struggle to rank long-CoT outputs, but selecting later H positions (denoising) dramatically improves PRM best-of-n selection.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Sampling across intermediate reasoning steps maximizes diversity and per-token gains by decorrelating error modes; truncated CoT is frequently sufficient to achieve full-CoT quality at much lower token cost; allocating inference budget to depth (H) provides larger marginal returns than allocating to trajectory count (n) or final-solution replicates (m); fractured sampling reshapes the inference-time scaling law and yields superior accuracy-cost trade-offs.",
            "uuid": "e8298.0",
            "source_info": {
                "paper_title": "Fractured Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Truncated CoT",
            "name_full": "Truncated Chain-of-Thought",
            "brief_description": "A practice of stopping the chain-of-thought generation before the final full reasoning trace and directly generating the final answer conditioned on a partial reasoning prefix; shown to often match full CoT sampling while using far fewer tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1 family; Qwen3; Skywork-OR1; DeepScaleR",
            "model_description": "Same set of instructed reasoning LLMs used in the paper; truncated-CoT is applied at inference by terminating the internal reasoning trace at a chosen H and sampling the answer.",
            "reasoning_methods": [
                "truncated chain-of-thought (partial-prefix answer generation)"
            ],
            "reasoning_methods_description": "Divide a long CoT into H segments and at a chosen t (prefix length) stop producing further thinking tokens and directly sample the final answer conditioned on the partial trace h_{1:t}. The paper evaluates truncated CoT (dashed orange) versus full CoT sampling (solid blue) under token budget constraints.",
            "reasoning_diversity": "both (single truncated prefix is a single method, but multiple truncated prefixes at different H introduce diversity when aggregated)",
            "reasoning_diversity_experimental_setup": "Direct empirical comparison across five benchmarks: truncated CoT + answer generation is plotted against full CoT sampling for pass@1 versus token budget (Figure 2). Default experimental values: n=16, H=16, m=4 baseline; truncation performed at various H positions.",
            "task_or_benchmark": "MATH500 L5, AIME24, AIME25-I, AIMO2, GPQA Diamond",
            "performance_results": "Across tasks and model scales truncated CoT often matches or exceeds the accuracy of full CoT under a max-token constraint; exact pass@k curves show truncated CoT achieves equal/better pass@1 using substantially fewer thinking tokens (figures referenced). No single scalar summary for all tasks provided beyond plotted curves.",
            "qualitative_findings": "Truncated CoT can avoid 'overthinking' and detect early correct conclusions; thinking steps dominate token cost while final solutions cost little, so truncation saves computation; truncated prefixes can be used in fractured sampling to create diverse, efficient decision points.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Complete Long-CoT traces are not always necessary: truncated CoT preserves or improves accuracy at much lower token cost, motivating fractured sampling and early-stopping strategies.",
            "uuid": "e8298.1",
            "source_info": {
                "paper_title": "Fractured Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Long-CoT",
            "name_full": "Long Chain-of-Thought (Long-CoT)",
            "brief_description": "An extended chain-of-thought prompting paradigm producing longer, more diverse intermediate reasoning trajectories that often incorporate self-reflection and self-correction to improve accuracy and robustness on multi-step reasoning tasks, at the expense of higher token usage.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "Prior methods and model families (e.g., O1 and R1 series) use longer reasoning trajectories and additional mechanisms (self-reflection) to explore larger solution spaces; referenced as prior work motivating the paper.",
            "reasoning_methods": [
                "Long Chain-of-Thought",
                "self-reflection",
                "self-correction"
            ],
            "reasoning_methods_description": "Longer CoT traces that include extended step-by-step reasoning and sometimes explicit self-correction routines; used in prior work to generate a richer internal reasoning trajectory before final answer.",
            "reasoning_diversity": "diverse (Long-CoT seeks broader, more varied internal reasoning trajectories)",
            "reasoning_diversity_experimental_setup": "Discussed as prior art and baseline context; not the primary novel experimental intervention (the paper compares full Long-CoT sampling to truncated CoT and to fractured sampling in figures but Long-CoT itself is not newly introduced here).",
            "task_or_benchmark": "Referenced in context of same math/scientific reasoning benchmarks and prior demonstrations of Long-CoT improvements.",
            "performance_results": "Prior work reported improvements from longer CoT traces but with dramatic increases in token usage; the present paper empirically shows truncated CoT can match such improvements at lower cost (plots compare full CoT vs truncated CoT).",
            "qualitative_findings": "Long-CoT explores broader solution spaces and aggregates diverse intermediate steps, improving robustness; its disadvantage is high token cost, motivating fractured sampling.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Long-CoT increases accuracy and robustness but is token-inefficient; fractured sampling and truncation can capture many benefits at lower cost.",
            "uuid": "e8298.2",
            "source_info": {
                "paper_title": "Fractured Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Self-consistency / Best-of-n",
            "name_full": "Self-consistency decoding (majority voting) and Best-of-n sampling",
            "brief_description": "Inference-time ensemble strategies: self-consistency aggregates multiple CoT samples via majority voting to improve reliability; best-of-n selects the highest-scoring candidate among n samples using a scoring function such as a process reward model (PRM).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied with evaluated LLMs and with PRM Qwen2.5-Math-PRM-72B for ranking",
            "model_description": "Self-consistency implemented as pass@k / majority-aggregation across independently sampled CoT trajectories; best-of-n implemented by scoring candidates with a PRM (Qwen2.5-Math-PRM-72B) and selecting the top scored answer.",
            "reasoning_methods": [
                "self-consistency (majority voting across CoT samples)",
                "best-of-n (PRM-scored selection)"
            ],
            "reasoning_methods_description": "Self-consistency: draw multiple independent full CoT trajectories (n-axis) and choose answer by majority/aggregation; Best-of-n: generate n candidates (possibly across fractured samples) and select the candidate maximizing a scoring function s(z) (PRM used in experiments). Experiments compare these strategies to fractured sampling across the n, m, and H axes.",
            "reasoning_diversity": "similar (these methods draw repeated samples of the same type of reasoning—either full trajectories or multiple final answers—so diversity arises from stochasticity rather than structured temporal branching)",
            "reasoning_diversity_experimental_setup": "Single-axis comparisons include n-axis (trajectory sampling / self-consistency) and m-axis (multiple final answers per trajectory). Best-of-n experiments used PRM ranking of final solutions only (PRM context window limited to 4k tokens). The paper compares H vs n vs m axes under matched token budgets.",
            "task_or_benchmark": "MATH500 L5, AIME24, AIME25-I, AIMO2, GPQA",
            "performance_results": "Modest improvements for m-axis best-of-n: average accuracy H=1,m=1 baseline 60.4% -&gt; H=1,m=4 is 61.6%. PRM-based best-of-n underperforms the best fractured configurations unless denoising is applied (keeping last 4 H positions raises H=16,m=1 from 61.4% to 68.0%). Overall, fractured sampling (H-axis) gives larger per-token gains than simply increasing n or m.",
            "qualitative_findings": "When failure events are highly correlated, replicating similar reasoning (m-axis) yields little benefit. PRMs trained on short-CoT data struggle to reliably rank long-CoT outputs, causing noisy best-of-n selection; denoising (restricting to later H positions) helps PRM selection.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Self-consistency and best-of-n improve reliability but are less efficient per token than fractured sampling along H; correlation structure of failures determines how much gain repeated-similar-sampling provides.",
            "uuid": "e8298.3",
            "source_info": {
                "paper_title": "Fractured Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Early stopping (H-consistency)",
            "name_full": "Early stopping based on repeated predictions across intermediate depths",
            "brief_description": "A simple inference-time heuristic that terminates generation early when the same prediction appears multiple times across sampled intermediate H positions, reducing token use while preserving or improving accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1 family; DeepScaleR; Skywork-OR1",
            "model_description": "Applied on long-generation CoT models by checking predictions at pre-specified token indices and stopping when a repetition threshold is met.",
            "reasoning_methods": [
                "early stopping based on H-axis prediction repetition"
            ],
            "reasoning_methods_description": "Initialize first H position at token index 6144 then continue generating in steps of 2048 tokens; at each checkpoint generate a solution from current prefix and stop generation when the same prediction appears more than once (or when max tokens reached). If max tokens reached, use final prediction.",
            "reasoning_diversity": "similar (uses repetition detection among sampled predictions; not itself a diversity-generating method but exploits repeated-consensus behavior)",
            "reasoning_diversity_experimental_setup": "Compared vanilla full generation to early-stopping across several models and benchmarks (Table 2). Metrics recorded: accuracy delta and average tokens per question.",
            "task_or_benchmark": "MATH500 L5, AIME25, AIMO2, GPQA (as reported in Table 2) and aggregated average across tasks",
            "performance_results": "Early stopping saved ~20% of generated tokens on average while preserving accuracy and in some cases improving it (example: DeepScaleR-1.5B-Preview saw +2.9% accuracy). Reported token and accuracy deltas vary across models/tasks; overall accuracy preserved and computational cost reduced.",
            "qualitative_findings": "Consistency across intermediate steps is a useful signal to terminate generation; early-stopping is simple to implement and requires no additional training. When the H resolution is too large, earlier positions are low-accuracy and produce noisy partial solutions, so checkpoint spacing matters.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Early stopping based on repetition across intermediate H positions provides substantial inference cost savings (~20% tokens) and preserves or sometimes improves accuracy, making fractured-sampling practical in latency-sensitive settings.",
            "uuid": "e8298.4",
            "source_info": {
                "paper_title": "Fractured Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "rating": 2,
            "sanitized_title": "deepseekr1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Let's verify step by step.",
            "rating": 2,
            "sanitized_title": "lets_verify_step_by_step"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "The lessons of developing process reward models in mathematical reasoning",
            "rating": 1,
            "sanitized_title": "the_lessons_of_developing_process_reward_models_in_mathematical_reasoning"
        }
    ],
    "cost": 0.01628625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Fractured Chain-of-Thought Reasoning
18 Jun 2025</p>
<p>Baohao Liao 
University of Amsterdam ‡ Salesforce AI Research</p>
<p>Hanze Dong hanze.dong@salesforce.com 
University of Amsterdam ‡ Salesforce AI Research</p>
<p>Yuhui Xu yuhui.xu@salesforce.com 
University of Amsterdam ‡ Salesforce AI Research</p>
<p>Doyen Sahoo 
University of Amsterdam ‡ Salesforce AI Research</p>
<p>Christof Monz 
University of Amsterdam ‡ Salesforce AI Research</p>
<p>Junnan Li 
University of Amsterdam ‡ Salesforce AI Research</p>
<p>Caiming Xiong 
University of Amsterdam ‡ Salesforce AI Research</p>
<p>Fractured Chain-of-Thought Reasoning
18 Jun 2025D795F4B22AE3EB6DAD188F05B9B4F240arXiv:2505.12992v3[cs.LG]
Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining.Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by generating rich intermediate reasoning trajectories, but these approaches incur substantial token costs that impede their deployment in latency-sensitive settings.In this work, we first show that truncated CoT, which stops reasoning before completion and directly generates the final answer, often matches full CoT sampling while using dramatically fewer tokens.Building on this insight, we introduce Fractured Sampling, a unified inference-time strategy that interpolates between full CoT and solution-only sampling along three orthogonal axes: (1) the number of reasoning trajectories, (2) the number of final solutions per trajectory, and (3) the depth at which reasoning traces are truncated.Through extensive experiments on five diverse reasoning benchmarks and several model scales, we demonstrate that Fractured Sampling consistently achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling gains in Pass@k versus token budget.Our analysis reveals how to allocate computation across these dimensions to maximize performance, paving the way for more efficient and scalable LLM reasoning.Code is available at https://github.com/BaohaoLiao/frac-cot.</p>
<p>Introduction</p>
<p>Recent advances in large language models (LLMs) have enabled impressive capabilities in complex reasoning and problem solving (Guo et al., 2025;Kojima et al., 2022;Jaech et al., 2024;Brown et al., 2020;Hurst et al., 2024;Anthropic, 2024;Team et al., 2024).While much progress has been driven by scaling model size and training data Hestness et al. (2017); Kaplan et al. (2020); Hoffmann et al. (2022), a complementary direction, inference-time scaling, has gained traction (Wang et al., 2023).This approach enhances performance by increasing computational effort at inference, without altering model parameters.Techniques such as selfconsistency decoding (majority voting) (Wang et al., 2022), best-of-n sampling (Stiennon et al., 2020;Brown et al., 2024;Cobbe et al., 2021;Dong et al., 2023), and ensemble-style methods (Yao et al., 2023;Zhou et al., 2022;Liao et al., 2025) leverage multiple forward passes to produce more accurate and robust predictions from instructed models.</p>
<p>In parallel with these inference-time scaling methods, another line of work has focused on improving the quality of individual reasoning paths.Chain-of-Thought (CoT) prompting (Wei et al., 2022) has emerged as a particularly effective technique by encouraging models to articulate intermediate reasoning steps before arriving at a final answer.Recently, Long Chain-of-Thought (Long-CoT) reasoning (Guo et al., 2025;Jaech et al., 2024) introduces longer and more diverse reasoning trajectories, often incorporating mechanisms like Step 1-2</p>
<p>Step 2-1 Solution 2</p>
<p>Step 2-2</p>
<p>Step 1</p>
<p>Step 2</p>
<p>Step 1</p>
<p>Step  The thinking process dominates the overall cost.</p>
<p>self-reflection and self-correction (Kumar et al., 2024).These extended CoTs explore a broader solution space and aggregate diverse intermediate steps into a single response.This has been shown to significantly improve accuracy and robustness, especially for tasks that require multi-step or logical reasoning.The downside is that they also dramatically increase token usage, resulting in higher inference costs.</p>
<p>Combining inference-time scaling with Long-CoT methods (e.g., using Long-CoT with self-consistency decoding) further amplifies this computational burden.Each technique alone may require thousands of additional tokens per input; together, they often push token budgets to impractical levels, making such methods unsuitable for latency-sensitive or resource-constrained applications.This raises a central question:</p>
<p>Can we retain the benefits of Long-CoT reasoning without incurring the full cost?</p>
<p>To address this, we revisit the common assumption that complete Long-CoT traces are essential for accurate reasoning.Surprisingly, we find that incomplete CoT trajectories, i.e., traces truncated before the final answer, can still yield highly accurate results.As shown in Figure 2, across five reasoning benchmarks, simply truncating the CoT prefix and generating the answer (dashed orange) matches or even exceeds the accuracy of full CoT sampling (solid blue) given a max token constraint.This result challenges the notion that "more reasoning" always leads to better outcomes and suggests a new frontier for efficiency: partial reasoning traces.</p>
<p>To systematically trade off between cost and performance, we propose Fractured Sampling, a unified inferencetime strategy that interpolates between full CoT and solution-only sampling.As illustrated in Figure 1(a), Fractured Sampling explores three orthogonal dimensions:</p>
<ol>
<li>
<p>Thinking trajectories: the number of distinct CoT prefixes sampled;</p>
</li>
<li>
<p>Solution diversity: the number of final answers generated per prefix;</p>
</li>
<li>
<p>Thinking prefix length: the depth at which each CoT is truncated.</p>
</li>
</ol>
<p>DS-R1-Qwen-7B</p>
<p>Figure 2: Pass@1 accuracy versus maximum token budget for DeepSeek-R1-Distill-Qwen-1.5B (1st row) and 7B (2nd row) on reasoning benchmarks.Solid blue lines show the original full chain-of-thought (CoT) sampling, while dashed orange lines show our truncated CoT + response approach.Across all benchmarks, truncating the CoT (and generating the final answer) achieves equal or better accuracy with substantially fewer tokens, demonstrating that full CoT is unnecessary and that truncating CoT can save a large amount of computation without sacrificing performance.</p>
<p>Figure 1(b) further reveals that thinking steps (blue) dominate the overall token count, while final solutions (orange) contribute minimally, highlighting ample opportunities to optimize reasoning depth and breadth.</p>
<p>Contributions.</p>
<p>Our key contributions are as follows: (1) We show that truncated CoT trajectories often achieve comparable or better performance than full CoT, at a fraction of the inference cost.(2) We propose Fractured Sampling, a unified inference-time framework that jointly controls reasoning depth, diversity, and token efficiency.(3) We provide a comprehensive analysis of the scaling behavior of Fractured Sampling across multiple reasoning benchmarks, offering practical insights into efficient inference strategies for LLMs.</p>
<p>Preliminary</p>
<p>Notations.Let x denote the input prompt and ε be a random seed used to introduce stochasticity.The instruct LLM generates an initial response as follows: z = f (x, ε), and a parser g extracts the final answer: y = g(z).</p>
<p>Baseline sampling schemes.Before introducing our method, we review common sampling-based inference techniques widely used to enhance output quality from LLMs.</p>
<p>Vanilla Sampling.This approach generates n independent completions by sampling with different random seeds:
F n (x, ε 1:n ) = {g • f (x, ε i ) | i = 1, . . . , n} .
Pass@k.The pass@k metric estimates the probability that at least one of the k samples is correct:
pass@k = P (∃ y i ∈ F k (x, ε 1:k ) s.t. y i is correct) .
Best-of-n.This strategy selects the most confident response among n candidates.If s(z) denotes a scoring function (e.g., reward model), the best-of-n output is:
y best = g argmax zi∈f (x,ε1:n) s(z i ) .
These inference-time strategies serve as foundations for improving reliability and robustness in model predictions, especially for reasoning-intensive tasks.Our approach builds on the sampling method by explicitly leveraging internal reasoning traces to enhance sample efficiency and answer diversity.</p>
<p>Reasoning LLMs and long-CoT thinking process.
= [h ε 1 , • • • , h ε H ] = f h (x, ε)
, where H denotes the total number of reasoning steps.The final response is then generated conditioned on the full thought process:
z = f o (x, h, ε).
This CoT formulation provides richer supervision and enables more structured sampling strategies, which our approach builds upon to enhance efficiency and performance.</p>
<p>To better reflect the internal reasoning process, we enhance diversity by sampling m additional random seeds for each of n thinking processes:
F n,m (x, ε 1:n , ε 1:n,1:m ) = {g • f o (x, f h (x, ε i ), ε i,j ) | i = 1, • • • , n; j = 1, • • • , m} .
However, standard sampling methods only operate on the reasoning trajectory or the final outputs, overlooking the model's intermediate reasoning dynamics.To fully exploit the internal structure of CoT reasoning, we propose sampling not just across independent trajectories, but also across intermediate reasoning stages.</p>
<p>Fractured sampling for long chain-of-thought reasoning</p>
<p>To formalize the intermediate reasoning process, we denote the partial reasoning trace up to step t as:
h ε 1:t = [h ε 1 , • • • , h ε t ] = f t h (x, ε).
Our approach leverages intermediate reasoning traces to aggregate predictions, thereby enhancing both efficiency and diversity.The key idea is to decompose the response generation into multiple stages and perform aggregation not only over independent final responses but also across intermediate reasoning steps.</p>
<p>Fractured sampling for reasoning LLMs.Fractured sampling extends this idea by incorporating intermediate reasoning stages directly into the sampling process.Specifically, we sample responses at each step of the reasoning chain:
Fn,m,H (x, ε1:n, ε 1:H 1:n,1:m ) = g • fo x, f t h (x, εi), ε t i,j i = 1, • • • , n; j = 1, • • • , m; t = 1, • • • , H .
Here, f t h (x, ε i ) denotes the partial reasoning trace up to step t, and ε t i,j is the random seed used for generating the response at that stage.By aggregating responses across all H intermediate steps, fractured sampling captures the evolving thought process and synthesizes diverse insights into a more robust final answer.</p>
<p>Fractured sampling offers two primary advantages: (1) Granular Aggregation: Integrating intermediate reasoning steps enables early detection of conclusions and avoid overthinking, improving the consistency of final predictions.(2) Enhanced Diversity: The multi-level sampling mechanism encourages a wide range of reasoning trajectories.Aggregating these paths produces a consensus that is more resilient to individual failures.</p>
<p>Three orthogonal dimensions of sampling.Fractured Sampling unifies and extends existing sampling strategies by operating along three orthogonal axes:</p>
<p>• m: Solution Diversity -sampling multiple final outputs from a single reasoning trace.</p>
<p>• n: Trajectory Diversity -sampling multiple independent reasoning traces with different seeds (vanilla CoT sampling).</p>
<p>• H: Reasoning Depth Diversity -sampling at different intermediate stages of a single reasoning trace (unique to fractured sampling).</p>
<p>This tri-dimensional framework enables a fine-grained exploration of the cost-performance landscape.While m and n offer diversity at the output or full-trajectory level, the H dimension uniquely captures the temporal evolution of reasoning, offering early, diverse, and efficient decision points.Together, they provide a powerful toolkit for scalable and reliable inference-time reasoning.</p>
<p>Empirical results (see Section 4) show that fractured sampling is a strong methods to produce diverse and meaningful solutions.</p>
<p>Analysis of fractured sampling</p>
<p>Fractured sampling benefits from diverse solutions.By distributing samples across both trajectories and intermediate steps, fractured sampling capitalizes on diverse error modes to boost overall success.The following proposition provides an analysis about our phenomenon.</p>
<p>Proposition 1 (Diversity Lower Bound, informal).Let F k be the indicator of failure for branch sample k and q k = P (F k = 1).Then the fractured-sampling success probability satisfies
p seg = 1 − Pr ∧ K k=1 F k = 1 = 1 − E K k=1 F k ,
and by inclusion-exclusion
E K k=1 F k = K k=1 q k + i&lt;j Cov(F i , F j ) + • • • .
That is to say, negative covariance Cov(F i , F j ) ≤ 0 means that failures at two different samples i, j tend not to coincide, i.e. the two sampling locations provide diverse error modes.If we only consider the second order expansion, we have
p seg ≥ 1 − K k=1 q k = 1 − H t=1 (1 − p t ) m .
Fractured sampling spreads samples across intermediate steps to maximize this diversity: because failures are unlikely to all happen together, the probability that every sample fails is strictly less than the naïve product of their marginal failure rates.Consequently, the overall success probability p seg is boosted above the independent-baseline 1 − (1 − p t ) m .</p>
<p>To understand the limits of fractured sampling, we examine two extreme correlation regimes among the K = mH branch samples.Almost perfect correlation.When every sample fails or succeeds in unison (F i = F j almost surely), the entire set of K trials collapses to a single Bernoulli event.In this case,
Pr(F 1 = • • • = F K = 1) = q, p seg = 1 − q,
so the sampling reduces to plain single-step sampling and yields no extra benefit.Sampling only along the m-axis (multiple outputs per trace) behaves similar to this.</p>
<p>Full independence.If all F k are mutually independent with Pr(F k = 1) = q k , then
Pr(F 1 = • • • = F K = 1) = K k=1 q k = H t=1 (1 − p t ) m , p seg = 1 − H t=1 (1 − p t ) m .
The sampling achieves the product-of-marginals bound: diversity arises purely from geometric averaging of each step's success rate.Standard trajectory sampling (n-axis) behaves similar to this regime with a single successful rate p.</p>
<p>Intermediate regimes.Between these extremes, negative pairwise covariances (Cov(F i , F j ) &lt; 0) drive the all-fail probability below the independent baseline, delivering gains beyond simple marginal aggregation.By contrast, positive correlations (Cov(F i , F j ) &gt; 0) erode this advantage, interpolating smoothly between full independence and perfect correlation.Sampling along the depth dimension H exploits these intermediate correlations to maximize diversity and overall success.</p>
<p>As illustrated in Figure 3, the correlation matrices shows how failure events at different reasoning depths H co-occur across five diverse benchmarks (MATH500 L5, AIME24, AIME25, AIMO2 and GPQA).Dark green cells along the diagonal indicate that failures at the same depth are, by definition, almost perfectly correlated.More interestingly, the off-diagonal pattern varies by task: many entries are light or even pink (negative), signalling that failures at two distinct depths tend not to happen simultaneously.This negative covariance across depths is precisely what fractured sampling exploits, by spreading samples over intermediate stages, it decorrelates error modes and thus markedly reduces the probability that all branch samples fail together.Benchmarks with stronger negative off-diagonal structure (e.g.GPQA) exhibit the largest gains from fractured sampling, confirming our theoretical diversity lower-bound analysis.</p>
<p>Scaling Laws Along the Trajectory Dimension</p>
<p>In fractured sampling, we allocate computation across three orthogonal axes (n, m, H).Here, we hold the branching factor m and fracturing depth H constant, and investigate how increasing the number of independent trajectories n affects performance under a fixed token budget.Denote the total tokens consumed as
B(n, m, H) = n C thinking + n m H C solution = n C thinking + mH C solution ,
where C thinking is the average tokens per trajectory spent on "thinking" (the reasoning prefix), and C solution is the per-step cost of generating each candidate solution.</p>
<p>Log-linear scaling behavior.Empirical studies across diverse benchmarks reveal a remarkably consistent log-linear relationship between computational budget and success rate along each axis:
pass@k B n ≈ C n log B n + c n , B n = B(n, 1, 1), pass@k B m ≈ C m log B m + c m , B m = B(1, m, 1), pass@k B H ≈ C H log B H + c H , B H = B(1, 1, H).
Here, the constants C n , C m , C H measure the marginal gain in log-budget per unit improvement in pass rate, while c n , c m , c H capture dataset-specific offsets.</p>
<p>Depth yields the steepest slope.Across a range of tasks, we consistently find
C H ≥ max{C n , C m },
indicating that allocating tokens to deeper intermediate sampling (the H axis) produces the largest incremental improvements per token.Intuitively, early-stage branching captures coarse but high-signal glimpses of the solution space, allowing the model to "course-correct" before committing to full trajectories and thus yielding higher gains for each additional intermediate sample.</p>
<p>Beyond single-axis scaling.While single-axis laws offer valuable intuition, actual performance often improves when (n, m, H) are tuned jointly.Since the n-axis contributes additively and independently, we condition on fixed (m, H) and model
pass@k B ≈ C m,H log B n | m, H + c m,H ,
where the coefficient C m,H encapsulates the combined effect of branching factor and depth.These crossterms reveal synergistic gains or trade-offs between axes, guiding more nuanced budget allocations.We explore these interactions and derive dataset-specific strategies in Section 4.</p>
<p>Empirical results</p>
<p>Settings.All inference experiments are conducted using NVIDIA A100-80GB GPUs, leveraging the vLLM framework (Kwon et al., 2023).Following the sampling configuration recommended by Guo et al. (2025), we set temperature=0.6, top p=0.95, and max tokens=32768.Our primary focus is on models from the DeepSeek-R1 family (Guo et al., 2025), and we further validate our findings using reasoning models from Qwen3 (Team, 2025), Skywork-OR1 (He et al., 2025), and DeepScaler (Luo et al., 2025).Evaluation is performed on five challenging math and scientific reasoning benchmarks: MATH500 Level 5 (L5) (Lightman et al., 2023), AIME24, AIME25-I (MAA Committees, 2025), AIMO2 reference questions (Frieder et al., 2024), and the GPQA Diamond set (Rein et al., 2024), containing 134, 30, 15, 10, and 198 questions, respectively.Unless otherwise specified, we set n = 16, H = 16 and m = 4. H = 16 indicates that the original thinking CoT is divided into 16 equally sized segments based on token count.For instance, the third fractured CoT consists of the first three segments of the full thinking trajectory.GPQA) show the largest absolute improvement from fractured sampling, in line with our diversity-bound analysis.</p>
<p>Scaling law for each dimension</p>
<p>These results empirically validate that, under the same compute budget, fractured sampling shifts the inference-time scaling curve upward, achieving higher accuracy at lower cost by leveraging the temporal structure of chain-of-thought.</p>
<p>Scaling law across dimensions</p>
<p>Thus far we have examined each sampling axis in isolation.Figure 5 extends this analysis by comparing four representative schemes that allocate budget across the solution (m) and depth (H) dimensions simultaneously, with the trajectory axis (n) swept to 16.We have: (1) (H=1, m=1): standard singlepath CoT sampling (baseline).</p>
<p>(2)(H=1, m=4): augment baseline with 4 final answers per trajectory.(3) (H=16, m=1): fractured sampling across 16 depths, one answer each.(4) (H=16, m=4): full three-axis sampling (both deep fracturing and multiple final answers).</p>
<p>Across every task and model, non-baseline schemes (excl.(H=1, m=4)) outperform (H=1, m=1) at fixed budget.More importantly, expanding H is usually more effective than expanding m.These multi-axis scaling laws reveal that, under the same token budget, the most efficient use of compute is to allocate tokens for temporal branches H. Final-solution replicates m may also work in some cases.</p>
<p>Best-of-N across dimensions</p>
<p>In prior experiments, we reported the metric pass@k, which indicates whether a correct prediction is present among a set of generated samples.In this section, we further examine whether a correct solution can be</p>
<p>DS-R1</p>
<p>Figure 6: Accuracy versus the position of fractured CoT.We split the whole reasoning trajectory into 16 intermediate steps.We can observe: (1) Even with a 1 16 reasoning trajectory, the accuracy is still decent, especially for GPQA; (2) More reasoning tokens lead to higher accuracy.</p>
<p>identified by a reward model from among the predictions generated across the three sampling axes.To this end, we employ the process reward model (PRM), specifically Qwen2.5-Math-PRM-72B(Zhang et al., 2025), which has demonstrated strong performance across a range of PRM benchmarks.Due to its limited context window (4K tokens), we score only the final solution rather than the intermediate reasoning steps.The reward assigned to the final step is used as the overall score for the entire solution.</p>
<p>Table 1: Best-of-N accuracy with different dimensional settings.n = 16 here for all settings.H = 1, m = 1 denotes the standard sampling setting.H = −4 here means that we take the last 4 solutions among all 16 predictions in the H dimension.As shown in Table 1, sampling with H = 1, m = 4 yields a modest improvement in average accuracy compared to the standard sampling setting of H = 1, m = 1 (61.6% vs. 60.4%).Interestingly, increasing only the H dimension to H = 16, m = 1 also leads to a slight improvement (61.4% vs. 60.4%), which contrasts with our earlier observation that varying H is typically more effective than varying m in terms of pass@k.We hypothesize that incorporating all H = 16 generated solutions introduces excessive noise, making it challenging for a PRM to correctly identify the optimal solution.This may be due to two factors: (1) the Long-CoT model tends to generate coherent and logically consistent solutions, which are difficult for the PRM to differentiate; (2) the PRM is trained predominantly on simpler and short-CoT data and may struggle to evaluate responses to more complex and long ones.
H m MATH500 L5 AIME24 AIME25 AIMO2 GPQA Avg. DS-R1-Qwen-7B1
Motivated by the trend observed in Figure 6-where later reasoning positions (i.e., higher H indices) are associated with improved accuracy-we apply a simple denoising strategy by discarding earlier solutions (H = 1 to H = 11) and retaining only the last four (H = −4).This simple adjustment significantly enhances performance, raising the accuracy from 61.4% (H = 16, m = 1) to 68.0% (H = −4, m = 1).Further combining both dimensions (H = −4, m = 4) yields an accuracy of 70.8%, a 10.4% improvement over the baseline setting (H = 1, m = 1).Notably, this configuration even outperforms standard sampling with a larger model that has twice the number of parameters (70.8% vs. 68.3%).We hypothesize that a welltrained PRM could even further push the limit, since Qwen2.5-Math-PRM-72B is trained with short-CoT data.</p>
<p>Early stopping for efficient generation</p>
<p>From the perspective of inference efficiency, we explore whether the consistency of predictions across the H dimension can be leveraged for early stopping.Specifically, if a particular prediction appears with high frequency (i.e., exceeds a predefined threshold) across multiple H positions, we consider this as a signal to terminate the generation early, thereby reducing computational cost.</p>
<p>As illustrated in Figure 6, prediction accuracy tends to be low at earlier positions.When the reasoning trace is divided into too many intermediate steps (i.e., a larger H), the model must generate a correspondingly large number of partial solutions, each requiring additional tokens.To balance computational efficiency and accuracy, we empirically initialize the first H position at a token index of 6144 and evaluate predictions at every subsequent 2048-token interval.For example, given a question, the model first generates 6144 reasoning tokens.Based on these tokens, a solution is generated and a prediction is extracted.Then, conditioned on the original question and the previously generated 6144 reasoning tokens, the model continues generating another 2048 tokens to produce the next prediction.Generation terminates once the same prediction occurs more than once or when the maximum token limit (max tokens) is reached.In the latter case, we adopt the final prediction, as later predictions tend to benefit from more extensive reasoning.</p>
<p>As shown in Table 2, this early stopping strategy preserves model accuracy and, in some cases, improves it-achieving a 2.9% increase for DeepScaleR-1.5B-Preview.In terms of computational efficiency, early stopping reduces the number of generated tokens by approximately 20% compared to standard generation.Notably, this method is simple to implement and requires no additional training.</p>
<p>Related work</p>
<p>Test-time scaling law.Scaling laws have traditionally described how model performance improves with increased training compute (Hestness et al., 2017;Kaplan et al., 2020;Hoffmann et al., 2022), e.g., through more supervised fine-tuning or reinforcement learning steps.However, a complementary class of test-time scaling laws has emerged (Snell et al., 2024;Jaech et al., 2024), which characterizes performance gains obtained purely by increasing inference-time budget, without modifying model parameters.This includes techniques such as self-consistency decoding (Wang et al., 2022), best-of-n sampling (Brown et al., 2024;Cobbe et al., 2021;Dong et al., 2023).On the other hand, CoT prompting, where performance improves with more samples or longer reasoning traces (Wei et al., 2022).Recent work, including the O1 and R1 series (Jaech et al., 2024;Guo et al., 2025), further demonstrates that extended trajectories (e.g., Long CoT) with multiple rollouts yields predictable improvements under test-time scaling curves.</p>
<p>On the other hand, Process Reward Models (PRMs) (Lightman et al., 2023;Zhang et al., 2024a;Wang et al., 2023) further enable fine-grained control by assigning dense, step-level rewards, which can guide search methods like Monte Carlo Tree Search (Luo et al., 2024).However, most approaches scale only along coarse dimensions, such as sample count or token length, or require external supervision via PRMs for finer control.In this work, we propose a more fine-grained view through Fractured Sampling without relying on PRMs, which explicitly decomposes generation into multi-stage reasoning traces and enables aggregation at intermediate steps.This design reveals richer scaling behaviors across trajectory depth, diversity, and stage-wise composition, and offers a more nuanced understanding of inference-time compute allocation.</p>
<p>Efficient sampling for LLMs.As large language models grow in size and capability, their inference cost becomes a significant bottleneck (Wan et al., 2023), especially when relying on multi-sample or multi-turn decoding strategies in reinforcement learning (Ouyang et al., 2022;Xiong et al., 2023;Dong et al., 2024;Xiong et al., 2025;Shao et al., 2024) or large-scale serving (Ainslie et al., 2023).This has motivated a line of work on efficient sampling, which aims to reduce compute without sacrificing performance.Approaches such as speculative decoding (Stern et al., 2018;Leviathan et al., 2023;Xia et al., 2024;Chen et al., 2023a;Zhang et al., 2023;Sun et al., 2024;Chen et al., 2023b;Li et al., 2024b;Liao et al., 2025), KV cache pruning (Xu et al., 2024;Xiao et al., 2023;Zhang et al., 2024c;Li et al., 2024a;Ge et al., 2023;Zhang et al., 2024b;Yang et al., 2024;Liu et al., 2024), are widely used in real-world LLM services.While these methods achieve notable efficiency gains, they largely operate within a fixed test-time scaling curve: improving the efficiency of a given point on the curve without fundamentally changing its shape.In contrast, we argue that the most principled path forward lies in reshaping the scaling law itself: by rethinking how inference budget is allocated across reasoning stages and sampling axes, one can unlock qualitatively different computeperformance tradeoffs.Our proposed Fractured Sampling method embodies this principle, revealing richer scaling dynamics and enabling more cost-effective reasoning through staged aggregation.</p>
<p>Conclusion</p>
<p>In this work, we introduce Fractured Sampling, a new Long-CoT inference paradigm that seamlessly unifies partial-trace and final-answer sampling by jointly controlling reasoning depth, trajectory diversity, and solution diversity.We uncover consistent log-linear scaling trends along each axis and offer theoretical insights into how sampling across intermediate reasoning steps maximizes diversity and per-token gains.Fractured Sampling redefines the cost-performance frontier of chain-of-thought inference, enabling powerful reasoning in LLMs with lower computational overhead.</p>
<p>A Proof of the Diversity Lower Bound</p>
<p>Proof.Let q i = Pr(F i = 1) = E[F i ] and denote
µ ij = E[F i F j ], µ ijk = E[F i F j F k ], µ ijkl = E[F i F j F k F ℓ ].
Define the joint cumulants κ ij = µ ij − q i q j , κ ijk = µ ijk − µ ij q k − µ ik q j − µ jk q i + 2q i q j q k .</p>
<p>Using the inclusion-exclusion identity k F k = I⊆[K] (−1) |I| i∈I (1 − F i ) and collecting equal-order terms yields the exact expansion:
E K k=1 F k = K k=1 q k + i&lt;j κ ij + i&lt;j&lt;k κ ijk + i&lt;j&lt;k&lt;ℓ κ ijkl + • • • + κ 1,2,...,K(1)
The dots represent cumulants of order four and higher.Equation ( 1) can be written compactly as
E K k=1 F k = I⊆[K] κ I ,
where κ I is the joint cumulant on the index set I (with κ {i} = q i and κ ∅ = 1).In each subplot, we compare: m (green dotted)-sampling only the final solution; n (blue solid)-sampling full reasoning trajectories; H (orange dashed)-fractured sampling across all intermediate steps.Rows correspond to DeepScaleR-1.5B-Previewand Qwen3-1.7Bmodels models.Fractured sampling (H) consistently yields higher pass@k at a given token budget.</p>
<p>B More results</p>
<p>Figure 1 :
1
Figure 1: (a) Comparison of sampling strategies for reasoning LLMs.Top: Sampling Trajectories-multiple complete reasoning chains are sampled independently from the model.Middle: Sampling Responses-a single reasoning chain is used to generate diverse final responses.Bottom: Fractured Sampling-our proposed method samples across both multiple reasoning trajectories and intermediate reasoning steps, enabling finegrained control over diversity and computation.(b) Token statistics across tasks and models.Bars represent the average token count per sample, broken down into reasoning steps (blue) and final solutions (orange).The thinking process dominates the overall cost.</p>
<p>Figure 3 :
3
Figure 3: Correlation matrices of binary failure indicators across intermediate reasoning depths (positions H) under fractured sampling for five benchmarks.Each cell shows the Pearson correlation coefficient between failure events at two depth positions; green denotes positively correlated failures (synchronized error modes), while pink denotes negatively correlated failures (diverse error modes) that fractured sampling exploits to boost overall success.</p>
<p>Figure 4 :
4
Figure4: Pass@k performance versus total token budget for three sampling schemes on five benchmarks.In each subplot, we compare: m (green dotted)-sampling only the final solution; n (blue solid)-sampling full reasoning trajectories; H (orange dashed)-fractured sampling across all intermediate steps.Rows correspond to DeepSeek-R1-Distill-Qwen-1.5B, 7B and DeepSeek-R1 models.Fractured sampling (H) consistently yields higher pass@k at a given token budget.Refer to Figure B.1 for DeepScaleR and Qwen3 with a similar pattern.</p>
<p>Figure 4
4
Figure4plots pass@k versus total tokens B for the three sampling schemes under matched cost.Across all benchmarks and model sizes, fractured sampling exhibits the steepest log-linear gains per token.In particular, we fit pass@k B * ≈ C * log B * + c * , * ∈ {n, m, H}, and consistently observe C H ≥ max{C n , C m }.This confirms that allocating budget to intermediate-step branching yields higher marginal returns than either sampling more independent traces or more final answers alone.</p>
<p>Figure 5 :
5
Figure5: Pass@k performance versus total token budget for four sampling schemes on five benchmarks.In each subplot, we compare: H=1, m=1-only sampling full reasoning trajectory; H=1, m=4-sampling both full reasoning trajectories and the final solution; H=16, m=1-both full reasoning trajectory sampling and fractured sampling across all intermediate steps; H=16, m=4-sampling all three dimensions.n is in[1, 2,  4, 8, 16]  for the five points (from left to right) on each line.Rows correspond to DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1 models.Refer to Figure B.2 for DeepScaleR and Qwen3 with a similar pattern.</p>
<p>Figure</p>
<p>Figure B.1: Pass@k performance versus total token budget for three sampling schemes on five benchmarks.In each subplot, we compare: m (green dotted)-sampling only the final solution; n (blue solid)-sampling full reasoning trajectories; H (orange dashed)-fractured sampling across all intermediate steps.Rows correspond to DeepScaleR-1.5B-Previewand Qwen3-1.7Bmodels models.Fractured sampling (H) consistently yields higher pass@k at a given token budget.</p>
<p>Table 2 :
2
The relative performance for early stop compared to vanilla sampling (DeepSeek-R1-Distill-Qwen-1.5B,DeepScaleR-1.5B-Previewand Skywork-OR1-7B-Preview). Overall, early stopping significantly saves inference budget while preserving the accuracy.
ModelMethodAccuracy (%) ↑Number of Tokens per Question (K) ↓MATH500AIME25AIMO2GPQAAvg.MATH500AIME25AIMO2GPQAAvg.DS-R1-1.5BVanilla Early Stop70.8 +1.227.5 -0.015.0 +10.634.1 -0.336.9 +2.98.8 -1.417.4 -2.321.1 -7.110.0 -0.614.3 -2.9DSR-1.5BVanilla Early Stop76.5 -0.941.7 -0.020.0 -0.019.2 +0.439.4 -0.15.1 -0.98.3 -1.110.6 -3.67.8 -0.28.0 -1.5SW-OR1-7BVanilla Early Stop89.0 -0.445.0 -0.047.5 -0.048.6 +1.157.5 +0.26.7 -0.613.4 -2.514.9 -3.38.5 -2.210.9 -2.2
In each subplot, we compare: H=1, m=1-only sampling full reasoning trajectory; H=1, m=4-sampling both full reasoning trajectories and the final solution; H=16, m=1-both full reasoning trajectory sampling and fractured sampling across all intermediate steps; H=16, m=4-sampling all three dimensions.n is in[1,2,4,8,16]for the five points (from left to right) on each line.Rows correspond to DeepScaleR-1.5B-Previewand Qwen3-1.7Bmodels.MATH500 L5 is saturated here, resulting in a less efficient gain from dimensions H and m.
Gqa: Training generalized multi-query transformer models from multi-head checkpoints. J Ainslie, J Lee-Thorp, M De Jong, Y Zemlyanskiy, F Lebrón, S Sanghai, arXiv:2305.132452023arXiv preprint</p>
<p>Claude 3.5 sonnet model card addendum. A Anthropic, Claude-3.5 Model Card. 20243</p>
<p>Large language monkeys: Scaling inference compute with repeated sampling. B Brown, J Juravsky, R Ehrlich, R Clark, Q V Le, C Ré, A Mirhoseini, arXiv:2407.217872024arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>C Chen, S Borgeaud, G Irving, J.-B Lespiau, L Sifre, J Jumper, arXiv:2302.01318Accelerating large language model decoding with speculative sampling. 2023aarXiv preprint</p>
<p>Cascade speculative drafting for even faster llm inference. Z Chen, X Yang, J Lin, C Sun, K C Chang, .-C, J Huang, arXiv:2312.114622023barXiv preprint</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>RAFT: Reward ranked finetuning for generative foundation model alignment. H Dong, W Xiong, D Goyal, Y Zhang, W Chow, R Pan, S Diao, J Zhang, K Shum, T Zhang, Transactions on Machine Learning Research. 2023</p>
<p>H Dong, W Xiong, B Pang, H Wang, H Zhao, Y Zhou, N Jiang, D Sahoo, C Xiong, T Zhang, arXiv:2405.07863Rlhf workflow: From reward modeling to online rlhf. 2024arXiv preprint</p>
<p>S Frieder, S Bealing, A Nikolaiev, G C Smith, K Buzzard, T Gowers, P J Liu, P.-S Loh, L Mackey, L De Moura, D Roberts, D Sculley, T Tao, D Balduzzi, S Coyle, A Gerko, R Holbrook, A Howard, X Markets, Ai mathematical olympiad -progress prize 2. 2024</p>
<p>S Ge, Y Zhang, L Liu, M Zhang, J Han, J Gao, arXiv:2310.01801Model tells you what to discard: Adaptive kv cache compression for llms. 2023arXiv preprint</p>
<p>D Guo, D Yang, H Zhang, J Song, R Zhang, R Xu, Q Zhu, S Ma, P Wang, X Bi, arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025arXiv preprint</p>
<p>J He, J Liu, C Y Liu, R Yan, C Wang, P Cheng, X Zhang, F Zhang, J Xu, W Shen, S Li, L Zeng, T Wei, C Cheng, Y Liu, Y Zhou, Skywork open reasoner series. 2025</p>
<p>J Hestness, S Narang, N Ardalani, G Diamos, H Jun, H Kianinejad, M M A Patwary, Y Yang, Y Zhou, arXiv:1712.00409Deep learning scaling is predictable, empirically. 2017arXiv preprint</p>
<p>J Hoffmann, S Borgeaud, A Mensch, E Buchatskaya, T Cai, E Rutherford, D D L Casas, L A Hendricks, J Welbl, A Clark, arXiv:2203.15556Training compute-optimal large language models. 2022arXiv preprint</p>
<p>A Hurst, A Lerer, A P Goucher, A Perelman, A Ramesh, A Clark, A Ostrow, A Welihinda, A Hayes, A Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>A Jaech, A Kalai, A Lerer, A Richardson, A El-Kishky, A Low, A Helyar, A Madry, A Beutel, A Carney, arXiv:2412.16720Openai o1 system card. 2024arXiv preprint</p>
<p>J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Training language models to self-correct via reinforcement learning. A Kumar, V Zhuang, R Agarwal, Y Su, J D Co-Reyes, A Singh, K Baumli, S Iqbal, C Bishop, R Roelofs, arXiv:2409.129172024arXiv preprint</p>
<p>Efficient memory management for large language model serving with pagedattention. W Kwon, Z Li, S Zhuang, Y Sheng, L Zheng, C H Yu, J E Gonzalez, H Zhang, I Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>Fast inference from transformers via speculative decoding. Y Leviathan, M Kalman, Y Matias, International Conference on Machine Learning. PMLR2023</p>
<p>Y Li, Y Huang, B Yang, B Venkitesh, A Locatelli, H Ye, T Cai, P Lewis, D Chen, arXiv:2404.14469Snapkv: Llm knows what you are looking for before generation. 2024aarXiv preprint</p>
<p>Y Li, F Wei, C Zhang, H Zhang, arXiv:2401.15077Eagle: Speculative sampling requires rethinking feature uncertainty. 2024barXiv preprint</p>
<p>B Liao, Y Xu, H Dong, J Li, C Monz, S Savarese, D Sahoo, C Xiong, arXiv:2501.19324Reward-guided speculative decoding for efficient llm reasoning. 2025arXiv preprint</p>
<p>H Lightman, V Kosaraju, Y Burda, H Edwards, B Baker, T Lee, J Leike, J Schulman, I Sutskever, K Cobbe, arXiv:2305.20050Let's verify step by step. 2023arXiv preprint</p>
<p>Kivi: A tuning-free asymmetric 2bit quantization for kv cache. Z Liu, J Yuan, H Jin, S Zhong, Z Xu, V Braverman, B Chen, X Hu, arXiv:2402.027502024arXiv preprint</p>
<p>Improve mathematical reasoning in language models by automated process supervision. L Luo, Y Liu, R Liu, S Phatale, M Guo, H Lara, Y Li, L Shu, Y Zhu, L Meng, arXiv:2406.065922024arXiv preprint</p>
<p>DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2. M Luo, S Tan, J Wong, X Shi, W Y Tang, M Roongta, C Cai, J Luo, L E Li, R A Popa, I Stoica, Notion Blog. MAA Committees. 2025. 2025AIME Problems and SolutionsDeepscaler: Surpassing o1-preview with a 1.5b model by scaling rl</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Gpqa: A graduate-level google-proof q&amp;a benchmark. D Rein, B L Hou, A C Stickland, J Petty, R Y Pang, J Dirani, J Michael, S R Bowman, First Conference on Language Modeling. 2024</p>
<p>Z Shao, P Wang, Q Zhu, R Xu, J Song, X Bi, H Zhang, M Zhang, Y Li, Y Wu, arXiv:2402.03300Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>Scaling llm test-time compute optimally can be more effective than scaling model parameters. C Snell, J Lee, K Xu, A Kumar, arXiv:2408.033142024arXiv preprint</p>
<p>Blockwise parallel decoding for deep autoregressive models. M Stern, N Shazeer, J Uszkoreit, Advances in Neural Information Processing Systems. 201831</p>
<p>Learning to summarize with human feedback. N Stiennon, L Ouyang, J Wu, D Ziegler, R Lowe, C Voss, A Radford, D Amodei, Christiano , P F , Advances in neural information processing systems. 202033</p>
<p>Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding. H Sun, Z Chen, X Yang, Y Tian, B Chen, arXiv:2404.119122024arXiv preprint</p>
<p>G Team, P Georgiev, V I Lei, R Burnell, L Bai, A Gulati, G Tanzer, D Vincent, Z Pan, S Wang, arXiv:2403.05530Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. 2024arXiv preprint</p>
<p>. Q Team, 2025Qwen3</p>
<p>Z Wan, X Wang, C Liu, S Alam, Y Zheng, Z Qu, S Yan, Y Zhu, Q Zhang, M Chowdhury, arXiv:2312.03863Efficient large language models: A survey. 20231arXiv preprint</p>
<p>P Wang, L Li, Z Shao, R Xu, D Dai, Y Li, D Chen, Y Wu, Z Sui, arXiv:2312.08935Math-shepherd: Verify and reinforce llms step-by-step without human annotations. 2023arXiv preprint</p>
<p>Selfconsistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, S Narang, A Chowdhery, D Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>H Xia, Z Yang, Q Dong, P Wang, Y Li, T Ge, T Liu, W Li, Z Sui, arXiv:2401.07851Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding. 2024arXiv preprint</p>
<p>Efficient streaming language models with attention sinks. G Xiao, Y Tian, B Chen, S Han, M Lewis, arXiv:2309.174532023arXiv preprint</p>
<p>Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. W Xiong, H Dong, C Ye, Z Wang, H Zhong, H Ji, N Jiang, T Zhang, arXiv:2312.114562023arXiv preprint</p>
<p>W Xiong, J Yao, Y Xu, B Pang, L Wang, D Sahoo, J Li, N Jiang, T Zhang, C Xiong, arXiv:2504.11343A minimalist approach to llm reasoning: from rejection sampling to reinforce. 2025arXiv preprint</p>
<p>Y Xu, Z Jie, H Dong, L Wang, X Lu, A Zhou, A Saha, C Xiong, D Sahoo, arXiv:2407.21018Think: Thinner key cache by query-driven pruning. 2024arXiv preprint</p>
<p>D Yang, X Han, Y Gao, Y Hu, S Zhang, H Zhao, arXiv:2405.12532Pyramidinfer: Pyramid kv cache compression for high-throughput llm inference. 2024arXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, International Conference on Learning Representations (ICLR). 2023</p>
<p>H Zhang, P Wang, S Diao, Y Lin, R Pan, H Dong, D Zhang, P Molchanov, T Zhang, arXiv:2412.11006Entropy-regularized process reward model. 2024aarXiv preprint</p>
<p>J Zhang, J Wang, H Li, L Shou, K Chen, G Chen, S Mehrotra, arXiv:2309.08168Draft &amp; verify: Lossless large language model acceleration via self-speculative decoding. 2023arXiv preprint</p>
<p>Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling. Y Zhang, B Gao, T Liu, K Lu, W Xiong, Y Dong, B Chang, J Hu, W Xiao, arXiv:2406.020692024barXiv preprint</p>
<p>H2o: Heavy-hitter oracle for efficient generative inference of large language models. Z Zhang, Y Sheng, T Zhou, T Chen, L Zheng, R Cai, Z Song, Y Tian, C Ré, C Barrett, Advances in Neural Information Processing Systems. 2024c36</p>
<p>Z Zhang, C Zheng, Y Wu, B Zhang, R Lin, B Yu, D Liu, J Zhou, J Lin, arXiv:2501.07301The lessons of developing process reward models in mathematical reasoning. 2025arXiv preprint</p>
<p>D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q Le, arXiv:2205.10625Least-to-most prompting enables complex reasoning in large language models. 2022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9780 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9780</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9780</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-165.html">extraction-schema-165</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <p><strong>Paper ID:</strong> paper-fc3752b55f88ace1aaab6f55f6b746374fb99dc6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fc3752b55f88ace1aaab6f55f6b746374fb99dc6" target="_blank">Rethinking Scale: The Efficacy of Fine-Tuned Open-Source LLMs in Large-Scale Reproducible Social Science Research</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This study demonstrates that small, fine-tuned open-source LLMs can achieve equal or superior performance to models such as ChatGPT-4, and proposes a hybrid workflow that leverages the strengths of both open and closed models, offering a balanced approach to performance, transparency, and reproducibility.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are distinguished by their architecture, which dictates their parameter size and performance capabilities. Social scientists have increasingly adopted LLMs for text classification tasks, which are difficult to scale with human coders. While very large, closed-source models often deliver superior performance, their use presents significant risks. These include lack of transparency, potential exposure of sensitive data, challenges to replicability, and dependence on proprietary systems. Additionally, their high costs make them impractical for large-scale research projects. In contrast, open-source models, although available in various sizes, may underperform compared to commercial alternatives if used without further fine-tuning. However, open-source models offer distinct advantages: they can be run locally (ensuring data privacy), fine-tuned for specific tasks, shared within the research community, and integrated into reproducible workflows. This study demonstrates that small, fine-tuned open-source LLMs can achieve equal or superior performance to models such as ChatGPT-4. We further explore the relationship between training set size and fine-tuning efficacy in open-source models. Finally, we propose a hybrid workflow that leverages the strengths of both open and closed models, offering a balanced approach to performance, transparency, and reproducibility.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9780.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9780.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gougherty & Clipp 2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Testing the reliability of an AI-based large language model to extract ecological information from the scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned in Related Work as an example where an LLM was applied to extract ecological information from scientific literature; cited as testing the model's reliability for literature-based information extraction in ecology.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Testing the reliability of an AI-based large language model to extract ecological information from the scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Ecology / ecological information extraction from the scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Described only at a high level in this paper: an AI-based LLM was used to extract ecological information from the scientific literature (reliability assessment); no methodological details provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Described in this manuscript only as a reliability test of the LLM's extractions; no specifics given in the present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as a study testing the reliability of an LLM for extracting ecological information; this paper does not report the original study's quantitative findings or methods.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Not detailed in this manuscript; only that the study tested reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>No study-specific hallucination or bias results are reported here; the present paper notes general concerns about LLM reliability in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking Scale: The Efficacy of Fine-Tuned Open-Source LLMs in Large-Scale Reproducible Social Science Research', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9780.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9780.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pangakis & Wolken 2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited as work on knowledge distillation / LLM-generated labels for supervised text classification, relevant to automated annotation and label generation workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Automated annotation / supervised text classification (general NLP / social science annotation contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Referenced in the context of using LLMs to generate training labels (knowledge distillation) for downstream supervised classifiers; the present paper describes a hybrid LLM-plus-human-rejection workflow but does not give Pangakis & Wolken's methods in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not detailed in this manuscript; cited as relevant literature on LLM-generated training labels.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as related work motivating LLM-assisted annotation and distillation of labels; no performance numbers or specifics included here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Not specified in this manuscript's discussion of the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Paper notes general concerns about reliability and the need for human oversight when using LLM-generated labels, but does not report study-specific biases for this citation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking Scale: The Efficacy of Fine-Tuned Open-Source LLMs in Large-Scale Reproducible Social Science Research', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9780.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9780.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zhang et al. (Gongbo) 2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Closing the gap between open source and commercial large language models for medical evidence summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced in Related Work as an example where LLMs are applied to medical evidence summarization — a form of synthesizing results from scholarly documents — though the current paper only cites it (does not use it).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Closing the gap between open source and commercial large language models for medical evidence summarization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Medicine / medical evidence summarization from scholarly sources</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Cited as an instance of LLMs applied to evidence summarization (synthesizing findings across papers); no methodological details are provided in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not described here; only cited as relevant literature on LLM-based summarization of medical evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned to situate the present work in the broader literature on LLMs applied to summarization and knowledge synthesis; this paper does not report Zhang et al.'s outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Not reported here; the manuscript emphasizes general trade-offs between closed and open models such as transparency and reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>The current paper raises general concerns about reproducibility, transparency, and potential risks of closed models but does not report specific hallucination findings for this citation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking Scale: The Efficacy of Fine-Tuned Open-Source LLMs in Large-Scale Reproducible Social Science Research', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Testing the reliability of an AI-based large language model to extract ecological information from the scientific literature <em>(Rating: 2)</em></li>
                <li>Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels <em>(Rating: 2)</em></li>
                <li>Closing the gap between open source and commercial large language models for medical evidence summarization <em>(Rating: 2)</em></li>
                <li>Prompt Stability Scoring for Text Annotation with Large Language Models <em>(Rating: 1)</em></li>
                <li>Synthetic Replacements for Human Survey Data? The Perils of Large Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9780",
    "paper_id": "paper-fc3752b55f88ace1aaab6f55f6b746374fb99dc6",
    "extraction_schema_id": "extraction-schema-165",
    "extracted_data": [
        {
            "name_short": "Gougherty & Clipp 2024",
            "name_full": "Testing the reliability of an AI-based large language model to extract ecological information from the scientific literature",
            "brief_description": "Mentioned in Related Work as an example where an LLM was applied to extract ecological information from scientific literature; cited as testing the model's reliability for literature-based information extraction in ecology.",
            "citation_title": "Testing the reliability of an AI-based large language model to extract ecological information from the scientific literature",
            "mention_or_use": "mention",
            "llm_model_name": null,
            "llm_model_description": null,
            "application_domain": "Ecology / ecological information extraction from the scientific literature",
            "input_corpus_description": null,
            "qualitative_law_type": null,
            "qualitative_law_example": null,
            "extraction_methodology": "Described only at a high level in this paper: an AI-based LLM was used to extract ecological information from the scientific literature (reliability assessment); no methodological details provided here.",
            "evaluation_method": "Described in this manuscript only as a reliability test of the LLM's extractions; no specifics given in the present paper.",
            "results_summary": "Cited as a study testing the reliability of an LLM for extracting ecological information; this paper does not report the original study's quantitative findings or methods.",
            "comparison_to_baseline": null,
            "reported_limitations": "Not detailed in this manuscript; only that the study tested reliability.",
            "bias_or_hallucination_issues": "No study-specific hallucination or bias results are reported here; the present paper notes general concerns about LLM reliability in related work.",
            "uuid": "e9780.0",
            "source_info": {
                "paper_title": "Rethinking Scale: The Efficacy of Fine-Tuned Open-Source LLMs in Large-Scale Reproducible Social Science Research",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Pangakis & Wolken 2024",
            "name_full": "Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels",
            "brief_description": "Cited as work on knowledge distillation / LLM-generated labels for supervised text classification, relevant to automated annotation and label generation workflows.",
            "citation_title": "Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels",
            "mention_or_use": "mention",
            "llm_model_name": null,
            "llm_model_description": null,
            "application_domain": "Automated annotation / supervised text classification (general NLP / social science annotation contexts)",
            "input_corpus_description": null,
            "qualitative_law_type": null,
            "qualitative_law_example": null,
            "extraction_methodology": "Referenced in the context of using LLMs to generate training labels (knowledge distillation) for downstream supervised classifiers; the present paper describes a hybrid LLM-plus-human-rejection workflow but does not give Pangakis & Wolken's methods in detail.",
            "evaluation_method": "Not detailed in this manuscript; cited as relevant literature on LLM-generated training labels.",
            "results_summary": "Mentioned as related work motivating LLM-assisted annotation and distillation of labels; no performance numbers or specifics included here.",
            "comparison_to_baseline": null,
            "reported_limitations": "Not specified in this manuscript's discussion of the cited work.",
            "bias_or_hallucination_issues": "Paper notes general concerns about reliability and the need for human oversight when using LLM-generated labels, but does not report study-specific biases for this citation.",
            "uuid": "e9780.1",
            "source_info": {
                "paper_title": "Rethinking Scale: The Efficacy of Fine-Tuned Open-Source LLMs in Large-Scale Reproducible Social Science Research",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Zhang et al. (Gongbo) 2024",
            "name_full": "Closing the gap between open source and commercial large language models for medical evidence summarization",
            "brief_description": "Referenced in Related Work as an example where LLMs are applied to medical evidence summarization — a form of synthesizing results from scholarly documents — though the current paper only cites it (does not use it).",
            "citation_title": "Closing the gap between open source and commercial large language models for medical evidence summarization",
            "mention_or_use": "mention",
            "llm_model_name": null,
            "llm_model_description": null,
            "application_domain": "Medicine / medical evidence summarization from scholarly sources",
            "input_corpus_description": null,
            "qualitative_law_type": null,
            "qualitative_law_example": null,
            "extraction_methodology": "Cited as an instance of LLMs applied to evidence summarization (synthesizing findings across papers); no methodological details are provided in this manuscript.",
            "evaluation_method": "Not described here; only cited as relevant literature on LLM-based summarization of medical evidence.",
            "results_summary": "Mentioned to situate the present work in the broader literature on LLMs applied to summarization and knowledge synthesis; this paper does not report Zhang et al.'s outcomes.",
            "comparison_to_baseline": null,
            "reported_limitations": "Not reported here; the manuscript emphasizes general trade-offs between closed and open models such as transparency and reproducibility.",
            "bias_or_hallucination_issues": "The current paper raises general concerns about reproducibility, transparency, and potential risks of closed models but does not report specific hallucination findings for this citation.",
            "uuid": "e9780.2",
            "source_info": {
                "paper_title": "Rethinking Scale: The Efficacy of Fine-Tuned Open-Source LLMs in Large-Scale Reproducible Social Science Research",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Testing the reliability of an AI-based large language model to extract ecological information from the scientific literature",
            "rating": 2
        },
        {
            "paper_title": "Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels",
            "rating": 2
        },
        {
            "paper_title": "Closing the gap between open source and commercial large language models for medical evidence summarization",
            "rating": 2
        },
        {
            "paper_title": "Prompt Stability Scoring for Text Annotation with Large Language Models",
            "rating": 1
        },
        {
            "paper_title": "Synthetic Replacements for Human Survey Data? The Perils of Large Language Models",
            "rating": 1
        }
    ],
    "cost": 0.01120125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Rethinking Scale: The Efficacy of Fine-Tuned Open-Source LLMs in Large-Scale Reproducible Social Science Research</h1>
<p>Marcello Carammia, ${ }^{\dagger}$ Stefano Maria Iacus, ${ }^{\ddagger}$ Giuseppe Porro ${ }^{\ddagger}$<br>November 5, 2024</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) are distinguished by their architecture, which dictates their parameter size and performance capabilities. Social scientists have increasingly adopted LLMs for text classification tasks, which are difficult to scale with human coders. While very large, closed-source models often deliver superior performance, their use presents significant risks. These include lack of transparency, potential exposure of sensitive data, challenges to replicability, and dependence on proprietary systems. Additionally, their high costs make them impractical for large-scale research projects.</p>
<p>In contrast, open-source models, although available in various sizes, may underperform compared to commercial alternatives if used without further fine-tuning. However, open-source models offer distinct advantages: they can be run locally (ensuring data privacy), fine-tuned for specific tasks, shared within the research community, and integrated into reproducible workflows.</p>
<p>This study demonstrates that small, fine-tuned open-source LLMs can achieve equal or superior performance to models such as ChatGPT-4. We further explore the relationship between training set size and fine-tuning efficacy in open-source models. Finally, we propose a hybrid workflow that leverages the strengths of both open and closed models, offering a balanced approach to performance, transparency, and reproducibility.</p>
<p>Keywords: large language models, fine-tuning, text classification, reproducibility, computational social science</p>
<h2>Introduction</h2>
<p>The widespread availability of generative AI, particularly large language models (LLMs), is transforming both society and science. These models are becoming increasingly easy to prompt, making them appealing to scientists across both the hard and social sciences for conducting text classification tasks in a wide range of disciplines, including finance (Loukas et al. 2023), health studies (Guo et al. 2024; Cao et al. 2024; Kiyasseh et al. 2024), environmental studies (Trajanov et al. 2023), geoscience (Maze et al. 2024), physics (Y.-Y. Li et al. 2024), chemistry (Liao et al. 2024) radiology (Nowak and Alois M Sprinkart 2024a), psychology (Abdurahman et al. 2024), sociology (Kozlowski, Kwon, and Evans 2024; Mützel and Ollion 2023; Chae and Davidson 2023), political science (M. Liu and Shi 2024), criminology (Nikolakopoulos et al. 2024), law studies (Wei et al. 2023), just to mention a few. Recent studies have sparked both enthusiastic (Gilardi, Alizadeh, and Kubli 2023; Rouzegar and Makrehchi 2024) and critical (Abdurahman et al. 2024) assessments of LLMs.</p>
<p>A central debate in this field involves closed vs. open foundation models (G. Zhang et al. 2024; Nowak and Alois M Sprinkart 2024a; Abdurahman et al. 2024; Hanke et al. 2024; Chae and Davidson 2023). Proprietary models, such as those from OpenAI and Google, present several challenges: they can change versions without notice, their architectures are not fully disclosed, and they may be subject to content moderation - such as filtering for hate speech or privacy concerns-designed to protect companies from legal and reputational risks. While these safeguards are important, they fall outside the control of scientists.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>For researchers, the financial and computational resources required to train large proprietary models, like OpenAI's GPT-4 (estimated to cost $\$ 100 \mathrm{M}$ to develop), are prohibitive. However, smaller, open-source models can be fine-tuned on readily available hardware, such as a laptop or a small GPU cluster, making them feasible for academic projects. Even companies like OpenAI are now moving towards smaller, more efficient models using techniques like the mixture-of-experts approach.</p>
<p>LLMs are typically measured by the number of parameters in their neural network architecture. While proprietary models often withhold exact specifications (e.g., OpenAI's GPT-4 is estimated to use over 1.7 trillion parameters, Google's Gemini Ultra has probably 1560B parameters and Anthropic's Claude 3 is rumored to have 500B parameters), open-source models provide clear documentation of their architectures. This study focuses on the Meta-developed LLAMA family of open-source models, including versions from 7B to 405B parameters. We do not consider other open-source models, such as Mistral/Mixtral, BLOOM, and Falcon, although they share similar fine-tuning capabilities and usage restrictions based on licensing.</p>
<p>Fine-tuning allows these open-source models to be specialized for specific tasks without retraining from scratch. In this study, we fine-tuned the LLAMA models using Low-Rank Adaptation (Hu et al. 2021) (LoRA), which modifies the model's output to better suit particular domains or tasks. Unlike proprietary models such as ChatGPT, whose fine-tuned versions remain under corporate control, open-source fine-tuned models can be freely shared, advancing reproducible (social) science (Christopher Barrie and Spirling 2024).</p>
<p>This research was motivated by the need to analyze Harvard's 10B Geo-Tweets archive (Lewis and Kakkar 2016). At current costs, applying commercial LLMs to this scale would be unfeasible, with token pricing reaching hundreds of thousands of dollars for large-scale analysis. In contrast, open-source models can run on local clusters, using tools like LLAMA.cpp (Gerganov 2022), Oolama (Chiang 2023), GPT4All (Anand et al. 2023) or similar open source projects, making large-scale, affordable analysis possible.</p>
<p>In this work, we demonstrate that while larger models generally outperform smaller ones, fine-tuning can level the playing field. Properly fine-tuned open-source models can achieve performance comparable to, or better than, significantly larger models like ChatGPT-4, especially in non-trivial classification tasks. Additionally, we examine how the size of the training data impacts the effectiveness of fine-tuning, finding that models trained on more extensive data are harder to fine-tune effectively.</p>
<p>Finally, we introduce a hybrid approach that utilizes both proprietary models, like ChatGPT-4, to accelerate the creation of labeled datasets for fine-tuning, in cases where labeled data is not readily available.</p>
<p>We evaluate these approaches through three classification tasks: (i) classifying tweets into 46 dimensions of well-being from the Human Flourishing Program (VanderWeele 2017); (ii) classifying European Parliamentary questions into 19 policy areas from the Comparative Agenda Project (Alexandrova et al. 2014; Jones et al. 2023) (CAP); and (iii) classifying datasets from the Harvard Dataverse repository into 15 subject categories. These tasks, ranging from noisy, unlabelled data to highly curated datasets, extend beyond simple sentiment analysis and showcase the capabilities of fine-tuned open-source models.</p>
<h1>Related Work</h1>
<p>Numerous studies (Chae and Davidson 2023; Pangakis and Wolken 2024; Rouzegar and Makrehchi 2024; Yin et al. 2024; Gougherty and Clipp 2024) have shown that recent advancements in large language models (LLMs) have revolutionized so much the field of natural language processing that they now hold a significant potential for social science research. Since the introduction of pioneer LLMs like BERT (Devlin et al. 2019) (a model with "just" 110 million parameters) up to XLM-RoBERTa (Conneau et al. 2020) (550 million parameters), the development of LLMs has progressed in the last few years through two key advancements: refined Transformer architectures (Vaswani et al. 2017) and optimized training techniques. These improvements, along with increased parameter sizes in models like GPT-3 and T5, have enabled LLMs to capture complex language patterns across diverse contexts, making them viable for varied social science tasks with minimal fine-tuning (Brown et al. 2020; Raffel et al. 2020).</p>
<p>With LLMs increasingly accessible as research tools, computational social science (Lazer et al. 2009) has started to leverage these models for large-scale text analysis beyond basic sentiment classification. This shift in scale has spurred numerous studies examining LLMs for annotation, classification, and knowledge extraction from text (Pangakis and Wolken 2024; Singh et al. 2024; Y. Zhang et al. 2024; Fechner and Dorpinghaus 2024; Rouzegar and Makrehchi 2024; Bisbee et al. 2024; Barrie, Palaiologou, and Törnberg 2024). Recently, researchers have also explored the multi-modal capabilities of LLMs for combining text with other data types (Miah et al. 2024). Our work aligns with studies examining fine-tuning's impact on LLM</p>
<p>performance, where findings consistently indicate that fine-tuned models can handle specific classification tasks with high accuracy (Yin et al. 2024; MathavRaj et al. 2024; Bucher and Martini 2024). What these works have in common is that they consider simple classification tasks (like sentiment or very few categories) and a limited set of LMMs, but they come to conclusions in agreement with our analysis.</p>
<p>The debate over proprietary versus open-source models has grown as researchers have increasingly voiced concerns about access, transparency, and scalability (Nowak and Alois M Sprinkart 2024a; Nowak and Alois M. Sprinkart 2024b; Wang forthcoming; G. Zhang et al. 2024; Hanke et al. 2024; Fields, Chovanec, and Madiraju 2024; Palmer, Smith, and Spirling 2024). This is indeed becoming a real issue in science as there is yet no way for individual researchers, or even a consortium of academical institutions, to replicate the scale of the training that commercial players are able to perform. We think the best of the two worlds can be leveraged to do better, faster and more scalable social science research. While proprietary models offer advanced generalization capabilities due to extensive training, open-source LLMs, despite smaller in size, are accessible for fine-tuning on specific tasks. Scholars increasingly recognize that combining task-specific tuning with open-source models can achieve performance competitive with proprietary models (Irugalbandara et al. 2024; X. Li et al. 2023; Fu et al. 2024). It is also a shared and emerging opinion among scholars and methodologists in the social science that only open-source models can guarantee a level of what is usually referred to as replicability (or reproducibility), i.e., "deterministic code and data" replication (Christopher Barrie and Spirling 2024). Conversely to these authors though in this work we show that these open source models are not just "last resort" tools as they can achieve equal or better accuracy than black-box models if effectively fine-tuned.</p>
<p>Such approach enable cost-effective, reproducible research, bridging the gap between proprietary and open-source capabilities by leveraging field-specific knowledge and specialized datasets. Our research underscores that integrating both proprietary and open-source models can optimize even more performance, scalability, and accessibility in social science research.</p>
<h1>Methods</h1>
<h2>Streamlining Labeled Dataset Creation for Fine-Tuning and Scaling</h2>
<p>The traditional approach to text classification involves training human coders, assigning them random sets of documents to classify, and resolving disagreements among coders to generate a clean set of labeled data for training and validation. An empirical fact familiar to researchers is that human coders often disagree in $15-20 \%$ of cases. This disagreement arises from various factors, such as inflexible codebooks, cultural and political biases, coder fatigue, or the inherent uncertainty in capturing certain dimensions.</p>
<p>Achieving high inter-coder agreement can be challenging, and surprisingly, coders often prefer LLMgenerated annotations over expert answers in certain contexts (Goyal, J. J. Li, and Durrett 2023; Hagendorff, Fabi, and Kosinski 2023). In some studies, the correlation between annotations by platforms like Mechanical Turk and expert annotators can approach zero (Fabbri et al. 2021). Recent literature has also questioned the reliability of human classification, traditionally considered the "gold standard", suggesting that it may not be as accurate as commonly assumed (Clark et al. 2021; Y. Liu et al. 2023).</p>
<p>Another critical issue is scalability. Creating a sufficiently large set of classifications to cover both training and validation datasets is time-consuming and expensive. In cases like ours, which involve a high dimensionality of categories, even expert coders find the task challenging. Furthermore, using human cognitive resources for this task may not be the most efficient use of time and skill.</p>
<p>Our proposed approach is to generate multiple classifications for an unlabeled set using potentially several LLMs, with humans focusing on rejecting incorrect classifications. It's faster and easier to spot what does not apply to a text - especially when faced with a set of over 10 possible labels (our application uses 15, 20, and 46 labels) - than to identify which set of labels apply.</p>
<p>There are several psychological reasons why it's often easier for humans to recognize errors rather than identify correct classifications. For example, error salience (Harsay et al. 2012) suggests that mistakes or anomalies tend to stand out more than correctly processed information, which blends into our expectations. Human cognitive biases, like being more sensitive to inconsistencies (an ancestral survival mechanism that makes us more sensitive to risks or issues), also play a role (Mobbs and Hagan 2015). Lastly, the negativity bias (Ito et al. 1998) makes humans pay more attention to negative or incorrect information than to positive or correct information.</p>
<p>All the above motivates our approach on how to accelerate the creation of a labeled set.</p>
<p>The proposed workflow is presented in Figure 1 and is composed of five steps. We assume the general context of multi-label classification.</p>
<ul>
<li>Step 1: Data Normalization. This step is common to all classification tasks and involves organizing the documents in a corpus into a tabular format (such as a CSV file or an SQL table in a database). The data contains only an identifier, id, and a text field. If the actual label is available, a true label field is also included.</li>
<li>Step 2: AI-Crowd Classification. This step involves using one or more LLMs (or the same LLM with different prompts) to classify each document. At the end of this process, each document is labeled multiple times, and any duplicate labels are removed.</li>
<li>Step 3: Human Approval. Multiple coders (experts, or a combination of both) are asked to reject labels that do not apply to each document. Multiple coders may be randomly assigned to the same set of documents to assess intra-coder reliability.</li>
<li>Step 4: Fine-tuning. A single LLM (potentially different from the ones used in Step 2) is fine-tuned using the human-filtered labeled set.</li>
<li>Step 5: Scaling. The fine-tuned model is used to scale the analysis to previously unseen documents.</li>
</ul>
<p>Each step can be expanded and tailored as needed, but these details do not alter the fundamental essence of the workflow. For example, the training set of documents can be split into training, validation, and test sets to assess the performance of different LLMs within the proposed workflow.</p>
<p>After introducing several metrics to evaluate the performance of the different LLMs, we will discuss how this workflow is applied to the case studies below.</p>
<h1>Evaluation Metrics</h1>
<p>In this work, we employ various accuracy metrics depending on the specific classification problem being addressed. Let $n$ represent the total number of documents to classify, and let $M$ denote the number of possible labels in a given classification task.</p>
<p>We define $y_{i j}$ as an indicator function that takes the value 1 if document $i$, where $i=1, \ldots, n$, is assigned to class $j$, where $j=1, \ldots, M$, and 0 if label $j$ does not apply to document $i$.</p>
<p>For mutually exclusive categories, each document $i$ can belong to at most one class, meaning there exists at most one index $j$ such that $y_{i j}=1$. In such cases, we use the notation $y_{i}=j$ to indicate that document $i$ belongs to category $j$.</p>
<p>We now review a few basic metrics commonly used to assess the quality of a classifier in the context of mutually exclusive categories.</p>
<h2>Accuracy</h2>
<p>If $y_{i}$ is the actual true category for document $i$ and $\hat{y}_{i}$ is the category predicted by the classifier, then accuracy is defined as</p>
<p>$$
\text { Accuracy }=\frac{1}{n} \sum_{i=1}^{n} \mathbf{1}\left(\hat{y}<em i="i">{i}=y</em>\right)
$$</p>
<p>where the function $\mathbf{1}(\mathrm{cond})=1$ if the condition 'cond' is true and 0 otherwise.
The accuracy is simply the sum of the diagonal elements of the $M \times M$ confusion matrix $C=\left(c_{j k}, j, k=\right.$ $1, \ldots, M)$, where $c_{j k}$ counts the number of times a document with true label $j$ is assigned to category $k$ by the classifier. It is also worth to introduce other measures like Precision, Recall and Specificity. The Precision measure is defined as:</p>
<p>$$
\operatorname{Precision}(\text { class }=j)=\frac{\operatorname{TP}(\text { class }=j)}{\operatorname{TP}(\text { class }=j)+\mathrm{FP}(\text { class }=j)}=\frac{\sum_{i} \mathbf{1}\left(\left(\hat{y}<em i="i">{i}=j\right) \cap\left(y</em>
$$}=j\right)\right)}{\sum_{i} \mathbf{1}\left(\hat{y}_{i}=j\right)</p>
<p>where TP refers to 'true positives' and FP refers to 'false positives.' Similarly, the Recall (or Sensitivity) for class $j$ is defined as:</p>
<p>$$
\operatorname{Recall}(\text { class }=j)=\text { Sensitivity }(\text { class }=j)=\frac{\operatorname{TP}(\text { class }=j)}{\operatorname{TP}(\text { class }=j)+\mathrm{FN}(\text { class }=j)}=\frac{\sum_{i} \mathbf{1}\left(\left(\hat{y}<em i="i">{i}=j\right) \cap\left(y</em>
$$}=j\right)\right)}{\sum_{i} \mathbf{1}\left(y_{i}=j\right)</p>
<p>where FN refers to 'false negatives.' Finally, the Specificity measures the proportion of actual negatives that are correctly identified by the classifier. For class $j$, Specificity is defined as:</p>
<p>$$
\operatorname{Specificity}(\text { class }=j)=\frac{\operatorname{TN}(\text { class }=j)}{\operatorname{TN}(\text { class }=j)+\operatorname{FP}(\text { class }=j)}=\frac{\sum_{i} \mathbf{1}\left(\hat{y}<em i="i">{i} \neq j \cap y</em>
$$} \neq j\right)}{\sum_{i} \mathbf{1}\left(y_{i} \neq j\right)</p>
<p>Another important metric is the so-called Balanced accuracy, that is used when dealing with imbalanced datasets (like in our examples below). It aims to provide a better measure of performance when the classes in a classification problem are not equally represented. The main purpose of balanced accuracy is to address the limitations of standard accuracy in scenarios where the dataset has an unequal distribution of classes. It does this by taking into account both the Sensitivity (true positive rate) and the Specificity (true negative rate), giving equal weight to both. It is given by the formula:</p>
<p>$$
\text { Balanced Accuracy }=\frac{\text { Sensitivity }+ \text { Specificity }}{2}
$$</p>
<p>The Macro Balance Accuracy is the simple average of the Balanced Accuracy by class.</p>
<h1>F1 Score</h1>
<p>The F1 score, originally introduced for a $2 \times 2$ table, is given by the formula:</p>
<p>$$
\mathrm{F} 1=2 \times \frac{\text { Precision } \times \text { Recall }}{\text { Precision }+ \text { Recall }}
$$</p>
<p>The F1 score for class $j$ is then computed as:</p>
<p>$$
\mathrm{F} 1_{j}=2 \times \frac{\text { Precision }(\text { class }=j) \times \text { Recall }(\text { class }=j)}{\text { Precision }(\text { class }=j)+\text { Recall }(\text { class }=j)}
$$</p>
<p>As with accuracy, higher values of the F1 score indicate better classifier performance.</p>
<h2>Hamming Loss</h2>
<p>In multi-label classification, each document can be assigned multiple labels. In general, each document $i$ is associated with a vector $y_{i <em>}=\left(y_{i 1}, y_{i 2}, \ldots, y_{i M}\right)$, which indicates the presence or absence of labels. Let $\hat{y}_{i </em>}$ be the predicted vector of $y_{i *}$ for document $i$. The Hamming loss is a measure of how frequently a prediction is wrong. It is particularly useful in this context of multi-label classification. For a single document $i$, the Hamming Loss (Hamming 1950) is given by the formula:</p>
<p>$$
\mathrm{HL}<em j="1">{i}=\frac{1}{M} \sum</em>}^{M} \mathbf{1}\left(\hat{y<em i="i" j="j">{i j} \neq y</em>\right)
$$</p>
<p>The overall Hamming Loss for the sample of documents is:</p>
<p>$$
\mathrm{HL}=\frac{1}{n} \sum_{i=1}^{n} \mathrm{HL}<em i="1">{i}=\frac{1}{n M} \sum</em>}^{n} \sum_{j=1}^{M} \mathbf{1}\left(\hat{y<em i="i" j="j">{i j} \neq y</em>\right)
$$</p>
<p>The metric $\mathrm{HL}<em i="i" j="j">{i}$ is equal to 1 if all predicted $y</em>$ 's, also ranges in $[0,1]$. When comparing two classifiers, the one with lower Hamming Loss is better.}$ values are wrong, and 0 if all predicted $y_{i j}$ values are correct. The overall Hamming Loss, which is just the average of the $\mathrm{HL}_{i</p>
<h2>Jaccard Index</h2>
<p>Another metric that is popular in multi-label classification problems is the Jaccard Index (Gilbert 1884; Jaccard 1901). It measures the similarity between the predicted and true vectors of $y_{i *}$ by dividing the size of their intersection by the size of their union. The formula is as follows:</p>
<p>$$
J=\frac{1}{n} \sum_{i=1}^{n} \frac{\left|\hat{y}<em>{i <em>} \cap y_{i </em>}\right|}{\left|\hat{y}</em>{i <em>} \cup y_{i </em>}\right|}=\frac{1}{n} \sum_{i=1}^{n} \frac{\left|\hat{y}<em i="1">{i <em>} \cap y_{i </em>}\right|}{M}=\frac{1}{n M} \sum</em>_{i }^{n}\left|\hat{y<em>} \cap y_{i </em>}\right|
$$</p>
<p>where $|A|$ is the number of elements in set $A$. In the above formula, the vector $y_{i <em>}$ (and similarly $\hat{y}_{i </em>}$ ) should be treated as a set formed by the union of its vector elements, i.e., $\left{y_{i 1}\right} \cup\left{y_{i 2}\right} \cup \ldots \cup\left{y_{i M}\right}$.</p>
<p>The ratio between the numerator and denominator is 0 if the two vectors $\hat{y}_{i <em>}$ and $y_{i </em>}$ are completely different, and 1 if the vectors are identical. Thus, for the Jaccard Index, when comparing two classifiers, the one with the higher value is better. The Jaccard Index essentially plays the role of accuracy in a multilabel classification setting. While Hamming Loss focuses on individual label mismatches, the Jaccard Index emphasizes overall set similarity.</p>
<h1>Data and Problem Statements</h1>
<p>We will consider three different datasets and classification problems which present a spectrum of challenges.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Project</th>
<th style="text-align: center;">Verification Training Test</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Challenges</th>
<th style="text-align: center;">Features</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Human Flourishing Program</td>
<td style="text-align: center;">10,000</td>
<td style="text-align: center;">2,404</td>
<td style="text-align: center;">2,177</td>
<td style="text-align: center;">multi-label classification; high-dimensional (46 categories); severely imbalanced; noise in the data; no gold standard</td>
<td style="text-align: center;">very large scale application (10B tweets archive)</td>
</tr>
<tr>
<td style="text-align: center;">Comparative Agenda Project</td>
<td style="text-align: center;">1,964</td>
<td style="text-align: center;">1,272</td>
<td style="text-align: center;">424</td>
<td style="text-align: center;">mutually exclusive categories; medium/highdimensional (20 categories); severely imbalanced</td>
<td style="text-align: center;">absence of noise; large scale application; gold standard exists</td>
</tr>
<tr>
<td style="text-align: center;">Harvard Dataverse</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$\begin{gathered} 5,000 \ \text { or } \ 76,110 \end{gathered}$</td>
<td style="text-align: center;">32,619</td>
<td style="text-align: center;">multi-label classification; medium-dimensional (15 categories)</td>
<td style="text-align: center;">highly curated; very low noise; very large training set</td>
</tr>
</tbody>
</table>
<p>Table 1: Challenges and features of the different projects used in this work. Verification is the set of unsupervised labeled data done by the mixture of LLMs in the workflow of Figure 1.</p>
<h2>Classification of Tweets according to the Human Flourishing Program</h2>
<p>The Human Flourishing Program (VanderWeele 2017) is a research initiative based at the Institute for Quantitative Social Science (IQSS) at Harvard University. Its goal is to study and promote human flourishing across a broad spectrum of life domains, integrating interdisciplinary research in the social sciences, philosophy, psychology, and other fields. The program aims to understand what constitutes human well-being or flourishing, which goes beyond mere happiness or economic success. It seeks to identify and analyze the factors that contribute to a flourishing life, including physical and mental health, happiness and life satisfaction, meaning and purpose, character and virtue, and close social relationships. Within this program, the Global Flourishing Study (GFS) is a five-year longitudinal data collection on approximately 200,000 participants from 20+ geographically and culturally diverse countries and territories, including Argentina, Australia, Brazil, China (Hong Kong), Egypt, Germany, India, Indonesia, Israel, Japan, Kenya, Mexico, Nigeria, the Philippines, Poland, South Africa, Spain, Sweden, Tanzania, Turkey, United Kingdom, and the United States. GFS measures global human flourishing in six areas: i) Happiness and life satisfaction; ii) Mental and physical health; iii) Meaning and purpose; iv) Character and virtue; v) Close social relationships and vi) Material and financial stability.</p>
<p>Each of these human flourishing areas is investigated through several questions. For our study, we have selected 46 dimensions (see Supplementary Material) among all the dimensions identified by the GFS study, taking into account the fact that we cannot ask question but we should infer the presence or absence of each dimension in a particular tweet. The scope of our study, that goes much beyond the scope of the present work, it to analyze the entire Harvard's 10B Geo-Tweets archive (Lewis and Kakkar 2016) with an accurate and scalable solution. We will analyze the performance of open source models from the Meta-developed LLAMA family, testing base and fine-tuned versions of LLAMA-2, 3, 3.1 and 3.2 versions for different sizes of model parameters.</p>
<p>For this analysis we selected 10,000 tweets at random in English language. We then applied the workflow illustrated in Figure 1, letting four models (LLAMA-2 7B, 13B and 70B and ChatGPT4) produce the unsupervised codings of Step 2 of the workflow. Then, we trained a group master and doctoral students to verify (Step 3) the AI-classifications according to the codebook of the project (see Supplementary</p>
<p>Material). From the cleaned codings we proceeded to fine-tune several of the above models (those up to 70B parameters). The cleaned labelled data at the end of the workflow, amount to a total of 4,581 tweets. The rest of 10,000 AI-classified data are left for future use and validation as human coders were only able to verify a portion of them. We split the cleaned data into a training set and a test set. We then validate each model (both base and fine-tuned ones).</p>
<h1>Policy Attribution of European Parliamentary Questions</h1>
<p>The Comparative Agendas Project (CAP) is an international research initiative aimed at systematically tracking, coding, and comparing policy attention and issue agendas over time across various countries (Alexandrova et al. 2014; Jones et al. 2023). The project seeks to understand how government attention and public priorities shift across policy domains, such as healthcare, defense, education, and the environment.</p>
<p>The CAP accomplishes this by analyzing policy outputs - such as laws, government speeches, media coverage, and legislative bills-coding them into standardized categories, and comparing them across time and countries. This approach enables researchers to examine how governments respond to public demands and to assess whether certain issues are prioritized over others across different contexts.</p>
<p>While the exact number of agenda items or categories may vary depending on updates or extensions for specific countries, the core CAP codebook typically includes 19 major policy areas, subdivided into over 200 subcategories. These major areas include: Macroeconomics; Civil Rights, Minority Issues, and Civil Liberties; Health; Agriculture; Labor, Employment, and Immigration; Education; Environment; Energy; Transportation; Law, Crime, and Family Issues; Social Welfare; Community Development and Housing Issues; Banking, Finance, and Domestic Commerce; Defense; Space, Science, Technology, and Communications; Foreign Trade; International Affairs and Foreign Aid; Government Operations; and Public Lands, Water Management, and Territorial Issues.</p>
<p>In this study, we focus on the classification of European Parliament questions from 1994 to 2021. The dataset comprises 174,161 questions posed over seven legislative terms, covering 28 countries and eight European party families. These questions reflect various EU institutional configurations, ranging from Maastricht to Lisbon, and are all provided in English.</p>
<p>According to the CAP framework, these questions can be classified into 19 broad policy areas. Certain categories, such as Defense, Foreign Trade, and Social Policy, are under-represented, while others, such as Macroeconomics, Agriculture, Civil Rights, and International Affairs and Foreign Aid, are over-represented. This imbalance poses challenges for classification. Nonetheless, the dataset is free from noise, as each parliamentary question is assigned to a specific policy area based on the CAP project's codebook guidelines.</p>
<p>Although the CAP coding protocol assumes that each text should be classified into one of the 19 major policy areas, this task presents significant challenges for human coders. It is well-known among CAP project experts that inter-coder reliability typically around $70 \%$ and sate-of-the art models (Sebők et al. 2024) built on top of very large training-set data ( $1,147,783$ observations) show weighted F1 score that varies from 0.71 to 0.9 according to different data sources (Media, Social Media, Parliamentary Speech, Legislative Executive Speech, Executive Order, Party Manifesto, Judiciary), signaling that the CAP data are not easy to classify. This variability highlights the inherent difficulty of consistently assigning policy areas, even with detailed coding guidelines. On this particular dataset (parliamentary questions) the inter coder reliability over size well trained coders on a subset of 134 manually coded parliamentary questions, the Cohen's kappa statistics (Fleiss et al. 1971) is equal to 66.4</p>
<p>In order to classify a preliminary set of data according to Step 2 of the workflow in Figure 1, we used three different strategies:</p>
<ul>
<li>direct method: we prompted ChatGPT4 to select one of the 200 micro areas and then aggregated by macro areas;</li>
<li>zero shot method: we asked ChatGPT4 to choose one of the 19 categories;</li>
<li>iterative method:</li>
<li>for each of the 19 policy macro areas we prompted ChatGPT4 to choose only one among the corresponding subtopic areas or None;</li>
<li>
<p>after iterating the 19 policy macro areas, we are left with a subset of subtopics corresponding each to a different macro area;</p>
</li>
<li>
<p>we map back the subtopic to their macro areas and ask ChatGPT4 to choose only one of these. In this case None is not an option, ChatGPT4 is forced to choose one policy area.</p>
</li>
</ul>
<p>The iterated method requires 20 calls to OpenAI's API's, so it is the more expensive of the three although it's accuracy was the highest.</p>
<h1>Classification of Harvard Dataverse datasets</h1>
<p>Harvard Dataverse (https://dataverse.harvard.edu) is a generalist research data repository (King 2007) with highly curated metadata. Thousands of scholars across many disciplines have deposited data, along with their associated metadata, in this repository. To support researchers in depositing their data more easily, the Dataverse development team is experimenting to generate metadata suggestions based on data characteristics, with the use of large language models (LLMs). In this experiment, we explored the ability of a small LLAMA-7B fine-tuned model to predict one or more subject categories for datasets using only the dataset Title and Description. The outcome of this project will be also used to verify the correctness of metadata in already published datasets or to test their FAIR-ness (Wilkinson et al. 2016). Previous works (ICPSR 2023; Shigapov and Schumm 2024) tried to do this using ChatGPT but again the approach is not scalable and not fully under the control of the researcher.</p>
<p>The dataset collection contains 13 subject categories, inherited from the Revised Field of Science and Technology (FOS) Classification by OECD (Organisation for Economic Co-operation and Development (OECD) 2007), plus "Other" and "N/A" when information is missing, for a total of 15 labels to predict in this classification task. Since each dataset can belong to one or more subject categories, this is a multiclass classification problem. Notice that in this case a human supervised training set already exists, so there is no need to apply the workflow in Figure 1. The LLM is asked to return up to 3 subject categories per dataset. This choice is motivated by the application at hand in which the Dataverse system suggests the three most relevant subject categories that apply to the users that is submitting data to the archive without overwhelming them with too much choices. In this system the user is supposed to reject the wrong subject categories.</p>
<p>For simplicity, we report accuracy based on the number of times the LLM correctly predicts at least one subject category. The subject categories are: "Agricultural Sciences", "Arts and Humanities", "Astronomy and Astrophysics", "Business and Management", "Chemistry", "Computer and Information Science", "Earth and Environmental Sciences", "Engineering", "Law", "Mathematical Sciences", "Medicine, Health and Life Sciences", "Physics", "Social Sciences", plus "Other" and "N/A".</p>
<p>The Harvard archive of datasets comprises 108,729 datasets, of which 76,110 ( $70 \%$ ) were used as the training set, and 32,619 as the test set. These sets were selected randomly. We generated two fine-tuned versions of the LLAMA2-7B model, using respectively the entire training set and a random subset of 5,000 datasets from the larger training set. We denote with "large" and "small" the two fine-tuned model in the results. We then compared the classification performance of the two fine-tuned LLAMA2-7B models against the base versions of LLAMA2 at sizes 7B, 13B, and 70B.</p>
<h2>Results</h2>
<h2>Tweets Classification according to the Human Flourishing Program</h2>
<p>As explained in the above, we generated curated labeled data using the workflow in Figure 1. We split the data into a training and test set of 2,404 and 2,177 tweets respectively. Considering that each tweet can be classified along 46 dimensions simultaneously, the total number of codings to analyse are 110,584 and 100,142 respectively for the two sets, meaning that each tweet is replicated according to the number of the flourishing dimensions that apply. In this application it is important to correctly identify the dimensions that applies but also those that does not apply, i.e. both true positives and true negatives matter in the analysis. This is why for this analysis we will mainly rely on the Accuracy (in the sense of Jaccard Index) and the Hamming Loss. Figure 2 summarizes the results for the Accuracy, Figure 3 reports the Hamming Loss and Figure 4 the Jaccard Index.</p>
<p>Before going into the details, we can summarize the evidence as follows:</p>
<ul>
<li>the larger the number of parameters, the better the performance of the model;</li>
<li>
<p>given the size of the model parameters, fine-tuning produces a significant increase in the performances;</p>
</li>
<li>
<p>give the size of the model, fine-tuning is less effective if the base model has been trained on larger data.</p>
</li>
</ul>
<p>The last fact is relevant for our application. For example, while LLAMA-2 has been trained on 1.5 T tokens, LLAMA-3 has been trained on 15 T tokens and LLAMA-3.2 on 9 T tokens. This seems to imply, looking at the empirical results, that fine-tuning has a hard time in re-weighting model parameters for those models which are more robust like LLAMA-3 compared to weaker models like LLAMA-2, but for this reason LLAMA-2 7B can be easily fine-tuned and obtain results as good or better than ChatGPT4 (a way much larger parameters model).</p>
<p>Another fact that we notice is that, as expected, the performance of the fine-tuned models are better on the training data than on the test data although the performance of the test data is quite remarkable for such a challenging problem.</p>
<p>All the above results are consistent across Accuracy, Hamming Loss and Jaccard Index.</p>
<h1>Attribution of European Parliamentary Questions to Policy Dimension</h1>
<p>As it can be seen in Table 2, after human validation (Step 3 of workflow in Figure 1) the zero shot method for ChatGPT4 gives very low Accuracy: $36.6 \%$. The base LLAMA2 7B, also with zero shot method, works as badly as ChatGPT4. The proposed iterative method for ChatGPT4 gets closer to human performance: $75.8 \%$. This is considered a good result for the CAP project. The direct method performance has not been reported because we couldn't get any result out of it.</p>
<p>Most surprisingly, after fine-tuning the LLAMA2-7B model (Step 4 of the workflow in Figure 1), the Accuracy of the zero-shot method jumps from about $37 \%$ to about $75 \%$. Figure 5 also reports the F1 score, Sensitivity, Specificity and the Balanced Accuracy for the LLAMA2-7B fine-tuned model on both the train and test sets. Figure 5 shows that the Sensitivity is low (less than $50 \%$ ) for the policy areas: $12=$ 'Law and Crime', $14=$ 'Regional and Urban issues and Planning', and $15=$ 'Banking, finance and domestic commerce issues'. Policy area 15 is often misclassified as 'Macroeconomics', while for the other two areas there is no clear pattern of misclassification. Further training on those categories might help the accuracy of the fine-tuned model but this is the object of future research although this seems in line with the results found (Sebők et al. 2024) obtained with XMLRoberta (much smaller LLM) trained on a a much larger training set (about 1.6 M texts). ChatGPT4 also has a similar behaviour in other classes. Nevertheless, the Macro Balance Accuracy for the train and test set are, respectively, equal to $86.5 \%$ and $86.4 \%$ for LLAMA2-7B and $83.4 \%$ for ChatGPT4 as shown in Table 3 .</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">N.Par.</th>
<th style="text-align: center;">Verification set</th>
<th style="text-align: center;">Acc.</th>
<th style="text-align: center;">Training set size</th>
<th style="text-align: center;">Acc.</th>
<th style="text-align: center;">Test set size</th>
<th style="text-align: center;">Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ChatGPT4 (zero shot)</td>
<td style="text-align: center;">$1,760 \mathrm{~B}$</td>
<td style="text-align: center;">1,964</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT4 (iterative)</td>
<td style="text-align: center;">$1,760 \mathrm{~B}$</td>
<td style="text-align: center;">1,964</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">LLAMA2</td>
<td style="text-align: center;">7 B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1,272</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">424</td>
<td style="text-align: center;">37.0</td>
</tr>
<tr>
<td style="text-align: center;">LLAMA2 fine-tuned</td>
<td style="text-align: center;">7 B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1,272</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">424</td>
<td style="text-align: center;">74.8</td>
</tr>
<tr>
<td style="text-align: center;">LLAMA2</td>
<td style="text-align: center;">7 B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1,962</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">654</td>
<td style="text-align: center;">37.8</td>
</tr>
<tr>
<td style="text-align: center;">LLAMA2 fine-tuned</td>
<td style="text-align: center;">7 B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1,962</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">654</td>
<td style="text-align: center;">74.2</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracy of classification of the CAP project data for different models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Set</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">Balanced Accuracy</th>
<th style="text-align: center;">Sensitivity</th>
<th style="text-align: center;">Specificity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT4 (iterative)</td>
<td style="text-align: center;">verification</td>
<td style="text-align: center;">1,964</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">98.7</td>
</tr>
<tr>
<td style="text-align: left;">LLAMA2 fine-tuned</td>
<td style="text-align: center;">train</td>
<td style="text-align: center;">1,272</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">98.7</td>
</tr>
<tr>
<td style="text-align: left;">LLAMA2 fine-tuned</td>
<td style="text-align: center;">test</td>
<td style="text-align: center;">424</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">98.7</td>
</tr>
<tr>
<td style="text-align: left;">LLAMA2 fine-tuned</td>
<td style="text-align: center;">train</td>
<td style="text-align: center;">1,962</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">91.1</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">99.2</td>
</tr>
<tr>
<td style="text-align: left;">LLAMA2 fine-tuned</td>
<td style="text-align: center;">test</td>
<td style="text-align: center;">654</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">98.6</td>
</tr>
</tbody>
</table>
<p>Table 3: Overall Accuracy and average values of F1 score, Balanced Accuracy, Sensitivity and Specificity in the classification of the CAP project data for different models and data sets.</p>
<h2>Harvard Dataverse Dataset</h2>
<p>Table 4 reports the accuracy of the LLAMA2 family of models in this multi-label classification task. As it can be seen, a fine-tuned 7B model can do as good as a 70B parameter model with as little as 5,000 labeled</p>
<p>sets, i.e. both reach an accuracy of about $84 \%$. By extending the training set to its whole size of 76,110 records, the accuracy get as high as $94.6 \%$. It is interesting to notice that, even though the LLM is asked</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LLAMA2 model</th>
<th style="text-align: center;">Parameters</th>
<th style="text-align: center;">Training set size</th>
<th style="text-align: center;">Test set size</th>
<th style="text-align: center;">$\%$ at least one correct</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">base</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">32,619</td>
<td style="text-align: center;">54.4</td>
</tr>
<tr>
<td style="text-align: center;">base</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">32,619</td>
<td style="text-align: center;">54.3</td>
</tr>
<tr>
<td style="text-align: center;">base</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">32,619</td>
<td style="text-align: center;">84.9</td>
</tr>
<tr>
<td style="text-align: center;">fine-tuned (small)</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">5,000</td>
<td style="text-align: center;">32,619</td>
<td style="text-align: center;">84.8</td>
</tr>
<tr>
<td style="text-align: center;">fine-tuned (large)</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">76,110</td>
<td style="text-align: center;">32,619</td>
<td style="text-align: center;">94.6</td>
</tr>
</tbody>
</table>
<p>Table 4: Accuracy of at least one subject category predicted correctly as a function of number of model parameters, fine-tuning and size of training set.
to produce at most three subject categories, in a few cases it proposes more labels than those considered true. Although we could not run an extensive human supervised checking, a quick inspection shows that in many cases these additional categories seem to apply, especially in the case when the dataset was originally labeled according to only one subject category. Further investigation is in the plans, because this could be the real added value of using generative AI in this application. Table 5 shows the accuracy in predicting the correct number of categories: $90.3 \%$ of the times the fine-tuned (large) model predicts exactly the same categories as those existing, and almost symmetrically it misses or produce more entries. The table cuts out the rows for which the number of true categories is $5,6,8$ and 14 (a total of 34 datasets, $0.1 \%$ of the data) and the columns for which the predict number of categories is 5,6 , or 7 ( 27 datasets in total or $0.08 \%$ of the datasets).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Predicted</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">True</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
</tbody>
</table>
<p>Table 5: Accuracy of (large) fine-tuned model in predicting the exact categories (and number of categories). The table cuts out the rows for which the number of true categories is $5,6,8$ and 14 (a total of 34 datasets, $0.1 \%$ of the data) and the columns for which the predict number of categories is 5,6 , or 7 ( 27 datasets in total or $0.08 \%$ of the datasets). Overall, the Accuracy in predicting exactly 1,2 and 3 categories is $90.3 \%$.</p>
<h1>Discussion</h1>
<p>In this study, we demonstrate that small, fine-tuned open-source language models, such as LLAMA2-7B, can achieve performance comparable or better than large commercial models like ChatGPT-4. Our findings underscore that, while very large, closed-source LLMs often excel in text classification tasks, their use comes with substantial drawbacks - particularly for large-scale social science applications. Closed-source models pose risks related to transparency, data security, and replicability and rely on proprietary systems, which can impede open scientific inquiry and drive up costs. In contrast, open-source models, although requiring more careful setup, offer distinct advantages by allowing local deployment for data privacy, task-specific fine-tuning, ease of sharing, and seamless integration into reproducible workflows.</p>
<p>We also highlight the efficacy of a hybrid approach to dataset creation that leverages the strengths of both LLMs and human oversight, thus bypassing some of the traditional limitations of manual text classification. The conventional process-relying on human coders to label documents - often involves high rates of coder disagreement (up to $15-20 \%$ ) due to subjective biases, rigid codebooks, or coder fatigue. This method not only introduces variability but also slows down the labeling process significantly, which can become prohibitively expensive in high-dimensional classification tasks with numerous categories. Emerging research further questions the reliability of human annotations, which have traditionally been considered the "gold standard" for text classification.</p>
<p>Our proposed workflow circumvents these issues by using multiple LLMs to provide initial classifications and then relying on human reviewers to reject only the labels that do not apply, rather than selecting the</p>
<p>correct ones. This approach leverages cognitive strengths - such as error salience and negativity bias - that make it easier for humans to detect errors rather than assign precise labels. As a result, this hybrid process is faster, more efficient, and reduces the demand on human coders, particularly in high-dimensional classification contexts where coders might otherwise face significant cognitive load.</p>
<p>This workflow comprises five key steps: (1) data normalization for a consistent tabular format, (2) AIdriven "crowd" classification using one or more LLMs to produce initial labels for each document, (3) human verification to filter out incorrect labels, (4) fine-tuning of a single LLM on the human-filtered labeled dataset, and (5) scaling to classify additional, previously unseen documents. This streamlined approach to labeled dataset creation makes it possible to fine-tune smaller models efficiently and expand the classification process to large datasets, ensuring high-quality, reproducible results without compromising data sensitivity.</p>
<p>Furthermore, our results reveal that smaller, fine-tuned models like LLAMA2-7B not only reach but often match or surpass the performance of large models like ChatGPT-4 on specific tasks. While large models are robust across general applications due to extensive training on broad datasets, this generalist nature can make them less adaptable to domain-specific tasks, as their weights are less responsive to fine-tuning adjustments. In contrast, smaller models tailored with a focused training set can adapt more responsively to specialized tasks, balancing the need for precision and scalability in applied research contexts.</p>
<p>In conclusion, this study demonstrates that, given a robust training set and a streamlined hybrid workflow, smaller, fine-tuned open-source models are not only viable but advantageous for specific applications in social science research. Such models provide comparable performance to larger, closed models while offering significant benefits in transparency, adaptability, data sensitivity, and computational efficiency. This approach underscores the potential for smaller, open-source models to support high-performance, reproducible research, with a cost-effective and privacy-conscious framework that empowers researchers to conduct scalable, collaborative studies.</p>
<h1>References</h1>
<p>Abdurahman, Suhaib et al. (July 2024). "Perils and opportunities in using large language models in psychological research". In: PNAS Nexus 3.7, pgae245. ISSN: 2752-6542. DOI: 10.1093/pnasnexus/pgae245. eprint: https://academic.oup.com/pnasnexus/article-pdf/3/7/pgae245/58645406/pgae245.pdf. URL: https://doi.org/10.1093/pnasnexus/pgae245.
Alexandrova, Petya et al. (2014). "Measuring the European Council agenda: Introducing a new approach and dataset". In: European Union Politics 15.1, pp. 152-167. DOI: 10.1177/1465116513509124. eprint: https://doi.org/10.1177/1465116513509124. URL: https://doi.org/10.1177/1465116513509124.
Anand, Yuvanesh et al. (2023). GPT4All: Training an Assistant-style Chatbot with Large Scale Data Distillation from GPT-3.5-Turbo. https://github.com/nomic-ai/gpt4all.
Barrie, Christopher, Elli Palaiologou, and Petter Törnberg (2024). Prompt Stability Scoring for Text Annotation with Large Language Models. arXiv: 2407.02039 [cs.CL]. URL: https://arxiv.org/abs/2407. 02039 .
Bisbee, James et al. (2024). "Synthetic Replacements for Human Survey Data? The Perils of Large Language Models". In: Political Analysis 32.4, pp. 401-416. DOI: 10.1017/pan.2024.5.
Brown, Tom et al. (2020). "Language Models are Few-Shot Learners". In: Advances in Neural Information Processing Systems. Ed. by H. Larochelle et al. Vol. 33. Curran Associates, Inc., pp. 1877-1901. URL: https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64aPaper.pdf.
Bucher, Martin Juan Jos'e and Marco Martini (2024). "Fine-Tuned 'Small' LLMs (Still) Significantly Outperform Zero-Shot Generative AI Models in Text Classification". In: ArXiv abs/2406.08660. URL: https: //api.semanticscholar.org/CorpusID:270440709.
Cao, Mengyuan et al. (2024). "LLM Collaboration PLM Improves Critical Information Extraction Tasks in Medical Articles". In: Health Information Processing. Evaluation Track Papers. Ed. by Hua Xu et al. Singapore: Springer Nature Singapore, pp. 178-185. ISBN: 978-981-97-1717-0.
Chae, Youngjin and Thomas Davidson (Aug. 2023). "Large Language Models for Text Classification: From Zero-shot Learning to Instruction-tuning". In: SocArXiv. DOI: 10.31235/osf.io/sthwk. URL: https: //doi.org/10.31235/osf.io/sthwk.
Chiang, Michael (2023). Get up and running with Llama 3.2, Mistral, Gemma 2, and other large language models. https://github.com/ollama/ollama.</p>
<p>Christopher Barrie, Alexis Palmer and Arthur Spirling (2024). Replication for Language Models. Problems, Principles, and Best Practice for Political Science. URL: https://lexipalmer13.github.io/research/.
Clark, Elizabeth et al. (Aug. 2021). "All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text". In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Ed. by Chengqing Zong et al. Online: Association for Computational Linguistics, pp. 7282-7296. DOI: 10.18653/v1/2021.acl-long.565. URL: https://aclanthology.org/2021.acl-long. 565.
Conneau, Alexis et al. (July 2020). "Unsupervised Cross-lingual Representation Learning at Scale". In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Ed. by Dan Jurafsky et al. Online: Association for Computational Linguistics, pp. 8440-8451. DOI: 10.18653/v1/2020. acl-main.747. URL: https://aclanthology.org/2020.acl-main. 747.
Devlin, Jacob et al. (June 2019). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding". In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Ed. by Jill Burstein, Christy Doran, and Thamar Solorio. Minneapolis, Minnesota: Association for Computational Linguistics, pp. 4171-4186. DOI: 10.18653/v1/N19-1423. URL: https://aclanthology.org/N191423.</p>
<p>Fabbri, Alexander R. et al. (2021). SummEval: Re-evaluating Summarization Evaluation. arXiv: 2007.12626 [cs.CL]. URL: https://arxiv.org/abs/2007.12626.
Fechner, Richard and Jens Dorpinghaus (2024). "No Train, No Pain? Assessing the Ability of LLMs for Text Classification with no Finetuning". In: Preprints of Position Papers of the 19th Conference on Computer Science and Intelligence Systems (FedCSIS), pp. 9-16.
Fields, John, Kevin Chovanec, and Praveen Madiraju (2024). "A Survey of Text Classification With Transformers: How Wide? How Large? How Long? How Accurate? How Expensive? How Safe?" In: IEEE Access 12, pp. 6518-6531. DOI: 10.1109/ACCESS.2024.3349952.
Fleiss, J.L. et al. (1971). "Measuring nominal scale agreement among many raters". In: Psychological Bulletin 76.5, pp. 378-382.</p>
<p>Fu, Chaoyou et al. (2024). "VITA: Towards Open-Source Interactive Omni Multimodal LLM". In: ArXiv abs/2408.05211. URL: https://api.semanticscholar.org/CorpusID:271843279.
Gerganov, Georgi (2022). LLM inference in C/C++. https://github.com/ggerganov/llama.cpp.
Gilardi, Fabrizio, Meysam Alizadeh, and Maël Kubli (2023). "ChatGPT outperforms crowd workers for text-annotation tasks". In: Proceedings of the National Academy of Sciences 120.30, e2305016120. DOI: 10.1073/pnas. 2305016120. eprint: https://www.pnas.org/doi/pdf/10.1073/pnas.2305016120. URL: https://www.pnas.org/doi/abs/10.1073/pnas.2305016120.
Gilbert, G. K. (1884). "Finley's tornado predictions". In: Amer. Meteor. J. 1, pp. 166-172.
Gougherty, Andrew V. and Hannah L. Clipp (2024). "Testing the reliability of an AI-based large language model to extract ecological information from the scientific literature". In: npj Biodiversity 3.1, p. 13. DOI: 10.1038/s44185-024-00043-9. URL: https://doi.org/10.1038/s44185-024-00043-9.
Goyal, Tanya, Junyi Jessy Li, and Greg Durrett (2023). News Summarization and Evaluation in the Era of GPT-3. arXiv: 2209.12356 [cs.CL]. URL: https://arxiv.org/abs/2209.12356.
Guo, Yuting et al. (Aug. 2024). "Evaluating large language models for health-related text classification tasks with public social media data". In: Journal of the American Medical Informatics Association 31.10, pp. 2181-2189. ISSN: 1527-974X. DOI: 10.1093/jamia/ocae210. eprint: https://academic.oup.com/ jamia/article-pdf/31/10/2181/59206398/ocae210.pdf. URL: https://doi.org/10.1093/jamia/ ocae210.
Hagendorff, Thilo, Sarah Fabi, and Michal Kosinski (2023). "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT". In: Nature Computational Science 3.10, pp. 833-838. DOI: 10.1038/s43588-023-00527-x. URL: https://doi.org/10.1038/s43588-023-00527-x.
Hamming, R. W. (1950). "Error detecting and error correcting codes". In: The Bell System Technical Journal 29.2, pp. 147-160. DOI: 10.1002/j.1538-7305.1950.tb00463.x.
Hanke, Vincent et al. (2024). "Open LLMs are Necessary for Private Adaptations and Outperform their Closed Alternatives". In: ICML 2024 Next Generation of AI Safety Workshop. URL: https://openreview. net/forum?id=uGml3wUL8s.
Harsay, H.A. et al. (2012). "Error awareness and salience processing in the oddball task: shared neural mechanisms". In: Front. Hum. Neurosci. 6 (246).</p>
<p>Hu, Edward J. et al. (2021). LoRA: Low-Rank Adaptation of Large Language Models. arXiv: 2106.09685 [cs.CL]. URL: https://arxiv.org/abs/2106.09685.
ICPSR (2023). TurboCurator: Enhancing Metadata Quality for Data Depositors. Accessed: 2024-10-15. URL: https://turbocurator.icpsr.umich.edu/tc/about.
Irugalbandara, Chandra et al. (2024). "Scaling Down to Scale Up: A Cost-Benefit Analysis of Replacing OpenAI's LLM with Open Source SLMs in Production". In: 2024 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), pp. 280-291. DOI: 10.1109/ISPASS61541. 2024.00034.</p>
<p>Ito, Tiffany A. et al. (1998). "Negative information weighs more heavily on the brain: The negativity bias in evaluative categorizations". In: Journal of Personality and Social Psychology 75.4, pp. 887-900. DOI: $10.1037 / / 0022-3514.75 .4 .887$.
Jaccard, Paul (1901). "Étude comparative de la distribution florale dans une portion des Alpes et du Jura". In: Bulletin de la Société Vaudoise des Sciences Naturelles 37.142, p. 547. ISSN: 0037-9603. DOI: 10.5169/ seals-266450. URL: https://doi.org/10.5169/seals-266450.
Jones, Bryan D. et al. (2023). Policy Agendas Project: Codebook. https://www.comparativeagendas.net.
King, Gary (2007). "An Introduction to the Dataverse Network as an Infrastructure for Data Sharing". In: Sociological Methods and Research 36, 173-199.
Kiyasseh, Dani et al. (2024). "A framework for evaluating clinical artificial intelligence systems without ground-truth annotations". In: Nature Communications 15.1, p. 1808. DOI: 10.1038/s41467-024-46000-9. URL: https://doi.org/10.1038/s41467-024-46000-9.
Kozlowski, Austin C., Hyunku Kwon, and James A. Evans (2024). In Silico Sociology: Forecasting COVID-19 Polarization with Large Language Models. arXiv: 2407.11190 [cs.CY]. URL: https://arxiv.org/abs/ 2407.11190.</p>
<p>Lazer, David et al. (2009). "Computational Social Science". In: Science 323.5915, pp. 721-723. DOI: 10 . 1126/science. 1167742. eprint: https://www.science.org/doi/pdf/10.1126/science. 1167742. URL: https://www.science.org/doi/abs/10.1126/science. 1167742.
Lewis, Benjamin and Devika Kakkar (2016). Harvard CGA Geotweet Archive v2.0. Version V2. DOI: 10 . 7910/DVN/3NCMB6. URL: https://doi.org/10.7910/DVN/3NCMB6.
Li, Xiang et al. (2023). "FLM-101B: An Open LLM and How to Train It with $\$ 100 \mathrm{~K}$ Budget". In: ArXiv abs/2309.03852. URL: https://api.semanticscholar.org/CorpusID:261582615.
Li, Yu-Yang et al. (2024). Deep Learning and LLM-based Methods Applied to Stellar Lightcurve Classification. arXiv: 2404.10757 [astro-ph.IM]. URL: https://arxiv.org/abs/2404.10757.
Liao, Chang et al. (2024). From Words to Molecules: A Survey of Large Language Models in Chemistry. arXiv: 2402.01439 [cs.LG]. URL: https://arxiv.org/abs/2402.01439.
Liu, Menglin and Ge Shi (2024). PoliPrompt: A High-Performance Cost-Effective LLM-Based Text Classification Framework for Political Science. arXiv: 2409.01466 [cs.CL]. URL: https://arxiv.org/abs/ 2409.01466.</p>
<p>Liu, Yixin et al. (July 2023). "Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation". In: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Ed. by Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki. Toronto, Canada: Association for Computational Linguistics, pp. 4140-4170. DOI: 10.18653/ v1/2023.acl-long.228. URL: https://aclanthology.org/2023.acl-long. 228.
Loukas, Lefteris et al. (2023). "Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking". In: Proceedings of the Fourth ACM International Conference on AI in Finance. ICAIF '23. Brooklyn, NY, USA: Association for Computing Machinery, pp. 392-400. ISBN: 9798400702402. DOI: 10.1145/3604237.3626891. URL: https://doi.org/10.1145/3604237.3626891.</p>
<p>MathavRaj, J et al. (2024). "Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations". In: ArXiv abs/2404.10779. URL: https://api.semanticscholar.org/CorpusID:269188062.
Maze, Elie et al. (Sept. 2024). Textual Data Augmentation for NER in Geosciences with LLMs. DOI: 10 . 2118 / 221083 - MS. eprint: https:// onepetro.org/SPEATCE/proceedings - pdf/24ATCE/24ATCE/ D021S012R002/3604503/spe-221083-ms.pdf. URL: https://doi.org/10.2118/221083-MS.
Miah, Md Saef Ullah et al. (2024). "A multimodal approach to cross-lingual sentiment analysis with ensemble of transformer and LLM". In: Scientific Reports 14.1, p. 9603. DOI: 10.1038/s41598-024-60210-7. URL: https://doi.org/10.1038/s41598-024-60210-7.
Mobbs, Paul D. and Christopher C. Hagan (2015). "The ecology of human fear: survival optimization and the nervous system". In: Frontiers in Neuroscience 9, p. 55. DOI: 10.3389/fnins.2015.00055.</p>
<p>Mützel, Sophie and Étienne Ollion (2023). "Machine Learning and the Analysis of Culture". In: The Oxford Handbook of the Sociology of Machine Learning. Oxford University Press. ISBN: 9780197653609. DOI: 10.1093/oxfordhb/9780197653609.013.39. eprint: https://academic.oup.com/book/0/chapter/ 467876363/chapter-ag-pdf/59016069/book_55209_section_467876363.ag.pdf. URL: https: //doi.org/10.1093/oxfordhb/9780197653609.013.39.
Nikolakopoulos, Anastasios et al. (2024). "Large Language Models in Modern Forensic Investigations: Harnessing the Power of Generative Artificial Intelligence in Crime Resolution and Suspect Identification". In: 2024 5th International Conference in Electronic Engineering, Information Technology $\mathcal{E}$ Education (EEITE), pp. 1-5. DOI: 10.1109/EEITE61750.2024.10654427.
Nowak, Sebastian and Alois M Sprinkart (Oct. 2024a). "Large language models from OpenAI, Google, Meta, X and Co. : The role of "closed" and "open" models in radiology". de. In: Radiologie (Heidelb.) 64.10, pp. $779-786$.</p>
<ul>
<li>(2024b). "Große Sprachmodelle von OpenAI, Google, Meta, X und Co." In: Die Radiologie 64.10, pp. 779786. DOI: 10.1007/s00117-024-01327-8. URL: https://doi.org/10.1007/s00117-024-01327-8.</li>
</ul>
<p>Organisation for Economic Co-operation and Development (OECD) (2007). Revised Field of Science and Technology (FOS) Classification in the Frascati Manual. Tech. rep. Accessed: 2024-10-14. Paris: Organisation for Economic Co-operation and Development. URL: https://www.oecd.org/science/inno/ 38235147.pdf.</p>
<p>Palmer, Alexis, Noah A. Smith, and Arthur Spirling (2024). "Using proprietary language models in academic research requires explicit justification". In: Nature Computational Science 4.1, pp. 2-3. DOI: 10.1038/ s43588-023-00585-1. URL: https://doi.org/10.1038/s43588-023-00585-1.
Pangakis, Nicholas and Samuel Wolken (2024). Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels. arXiv: 2406.17633 [cs.CL]. URL: https: //arxiv.org/abs/2406.17633.
Raffel, Colin et al. (Jan. 2020). "Exploring the limits of transfer learning with a unified text-to-text transformer". In: J. Mach. Learn. Res. 21.1. ISSN: 1532-4435.
Rouzegar, Hamidreza and Masoud Makrehchi (2024). Enhancing Text Classification through LLM-Driven Active Learning and Human Annotation. arXiv: 2406.12114 [cs.CL]. URL: https://arxiv.org/abs/ 2406.12114.</p>
<p>Sebők, Miklós et al. (2024). "Leveraging Open Large Language Models for Multilingual Policy Topic Classification: The Babel Machine Approach". In: Social Science Computer Review, p. 08944393241259434.
Shigapov, Renat and Irene Schumm (2024). FAIR GPT: A virtual consultant for research data management in ChatGPT. arXiv: 2410.07108 [cs.DL]. URL: https://arxiv.org/abs/2410.07108.
Singh, Gopendra Vikram et al. (2024). "Predicting multi-label emojis, emotions, and sentiments in codemixed texts using an emojifying sentiments framework". In: Scientific Reports 14.1, p. 12204. DOI: 10. 1038/s41598-024-58944-5. URL: https://doi.org/10.1038/s41598-024-58944-5.
Trajanov, Dimitar et al. (2023). "Comparing the performance of ChatGPT and state-of-the-art climate NLP models on climate-related text classification tasks". In: Proceedings of the 4th International Conference on Environmental Design (ICED2023). OpenBU. URL: https://open.bu.edu/handle/2144/48956.
VanderWeele, Tyler J. (2017). "On the promotion of human flourishing". In: Proceedings of the National Academy of Sciences 114.31, pp. 8148-8156. DOI: 10.1073/pnas. 1702996114. eprint: https://www. pnas.org/doi/pdf/10.1073/pnas.1702996114. URL: https://www.pnas.org/doi/abs/10.1073/ pnas. 1702996114 .
Vaswani, Ashish et al. (2017). "Attention is all you need". In: Proceedings of the 31st International Conference on Neural Information Processing Systems. NIPS'17. Long Beach, California, USA: Curran Associates Inc., pp. 6000-6010. ISBN: 9781510860964.
Wang, Yu (forthcoming). "On Finetuning Large Language Models". In: Political Analysis, pp. 1-5. DOI: DOI: 10.1017/pan.2023.36. URL: https://www.cambridge.org/core/product/443356A60ACD5B1716B14DD204BDEA1F.</p>
<p>Wei, Fusheng et al. (2023). "Empirical Study of LLM Fine-Tuning for Text Classification in Legal Document Review". In: 2023 IEEE International Conference on Big Data (BigData), pp. 2786-2792. DOI: 10.1109/ BigData59044.2023.10386911.
Wilkinson, Mark D. et al. (2016). "The FAIR Guiding Principles for scientific data management and stewardship". In: Scientific Data 3.1, p. 160018. DOI: 10.1038/sdata.2016.18. URL: https://doi.org/10. 1038/sdata. 2016.18.
Yin, Kai et al. (2024). CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics. arXiv: 2406.15477 [cs.CL]. URL: https://arxiv. org/abs/2406.15477.</p>
<p>Zhang, Gongbo et al. (2024). "Closing the gap between open source and commercial large language models for medical evidence summarization". In: npj Digital Medicine 7.1, p. 239. DOI: 10.1038/s41746-024-01239-w. URL: https://doi.org/10.1038/s41746-024-01239-w.
Zhang, Yazhou et al. (2024). Pushing The Limit of LLM Capacity for Text Classification. arXiv: 2402.07470 [cs.CL]. URL: https://arxiv.org/abs/2402.07470.</p>
<h1>Additional information</h1>
<p>The Supplementary Material will be available soon with prompts, scripts for fine-tuning, verification results, datasets, code-books, and fine-tuned models. A link to the repository will be included in the next revision of the manuscript.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Acceleration of labeled datasets creation for fine-tuning using multiple LLMs and human verification.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Accuracy for different LLAMA models and ChatGPT4 on both the training and the test sets. The 'FT' suffix stands for 'fine-tuned version' of the same model. Number of model parameters in the log-scale. For ChatGPT the real dimension of the model is not known, but seems to be built on top of 8 models of about 220B parameters.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Hamming loss for different LLAMA models and ChatGPT4 on both the training and the test sets. The 'FT' suffix stands for 'fine-tuned version' of the same model. Number of model parameters in the log-scale. For ChatGPT the real dimension of the model is not known, but seems to be built on top of 8 models of about 220B parameters.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Jaccard Index for different LLAMA models and ChatGPT4 on both the training and the test sets. The 'FT' suffix stands for 'fine-tuned version' of the same model. Number of model parameters in the log-scale. For ChatGPT the real dimension of the model is not known, but seems to be built on top of 8 models of about 220B parameters.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Balanced accuracy, F1 score, Sensitivity and Specificity by major policy areas of the Comparative Agenda Project, for the classification of the both the train and test set using the LLAMA-2 fine-tuned model and on the verified set classified by ChatGPT4.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*University of Catania, Department of Political and Social Sciences, Catania, 95131, Italy
${ }^{\dagger}$ Corresponding author: siacus@iq.harvard.edu. Harvard University, Institute for Quantitative Social Science, Cambridge, MA 02138, USA
${ }^{\ddagger}$ University of Insubria, Department of Law, Economics and Culture, Como, 22100, Italy&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
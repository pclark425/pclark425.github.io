<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5234 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5234</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5234</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-267759791</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.12984v1.pdf" target="_blank">Can GNN be Good Adapter for LLMs?</a></p>
                <p><strong>Paper Abstract:</strong> Recently, large language models (LLMs) have demonstrated superior capabilities in understanding and zero-shot learning on textual data, promising significant advances for many text-related domains. In the graph domain, various real-world scenarios also involve textual data, where tasks and node features can be described by text. These text-attributed graphs (TAGs) have broad applications in social media, recommendation systems, etc. Thus, this paper explores how to utilize LLMs to model TAGs. Previous methods for TAG modeling are based on million-scale LMs. When scaled up to billion-scale LLMs, they face huge challenges in computational costs. Additionally, they also ignore the zero-shot inference capabilities of LLMs. Therefore, we propose GraphAdapter, which uses a graph neural network (GNN) as an efficient adapter in collaboration with LLMs to tackle TAGs. In terms of efficiency, the GNN adapter introduces only a few trainable parameters and can be trained with low computation costs. The entire framework is trained using auto-regression on node text (next token prediction). Once trained, GraphAdapter can be seamlessly fine-tuned with task-specific prompts for various downstream tasks. Through extensive experiments across multiple real-world TAGs, GraphAdapter based on Llama 2 gains an average improvement of approximately 5\% in terms of node classification. Furthermore, GraphAdapter can also adapt to other language models, including RoBERTa, GPT-2. The promising results demonstrate that GNNs can serve as effective adapters for LLMs in TAG modeling.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5234.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5234.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph2Text</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph2Text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline approach that converts a node's local graph neighborhood into natural-language text by concatenating the node's own text with the textual attributes of its 1-hop neighbors (enumerated as a list) and feeding that to a language model for downstream prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph2Text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>neighbor-concatenation / graph-to-text linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each target node, the textual attribute of the node is combined with the textual attributes of its 1-hop neighbors into a single natural-language string. Example form given in the paper: "It is a paper node, its abstract is represented as XXX, and the abstracts of its cited papers are represented as follows: 1. YYY, 2. ZZZ." This serialized text is provided to an LM as input (possibly with prompts) so the LM can perform next-token prediction or downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>text-attributed graph (citation/social graphs) using 1-hop neighbor subgraphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Direct, human-readable serialization of local neighborhood; preserves raw neighbor text content; simple to implement; does not require training a separate GNN. Implicit properties discussed: potentially verbose/redundant (many neighbor texts concatenated), computationally heavy when many neighbors/text tokens are present, and scales poorly for large neighborhoods or when many nodes must be scored by an LM (ties into the paper's general critique of cascading GNN-LM approaches and LM inference cost).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification on text-attributed graphs (datasets used in paper: Ogbn-Arxiv, Instagram, Reddit). Graph2Text is evaluated as a baseline in the node-classification experiments reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The paper reports comparisons of baselines including Graph2Text but does not publish an explicit per-dataset numeric row for Graph2Text in the main tables shown in the text excerpt; therefore exact numeric scores for Graph2Text are not provided in this paper (null). The paper does report that GraphAdapter (Llama 2, pre-trained) achieves accuracy 0.7707 on Arxiv (Table 2) and that GraphAdapter overall outperforms competing baselines on average.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Graph2Text is included among baselines (LM-based and cascaded approaches). The paper reports that GraphAdapter (the proposed GNN-as-adapter method) outperforms existing baselines on node classification on the evaluated TAGs; the paper argues methods that directly feed many node texts into LMs suffer high compute/memory cost, motivating GraphAdapter's more efficient approach. No head-to-head numeric table in the excerpt specifically isolates Graph2Text's scores versus GraphAdapter, beyond the general baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>As discussed in the paper, serializing neighbor texts as input to an LM incurs high computational and memory costs (the general 'cascading GNN-LM' critique). The representation can be verbose and scale poorly with degree or large neighbor text sizes. The paper frames these limitations as motivating more parameter- and compute-efficient alternatives (e.g., GNN adapters that operate on cached LM hidden states).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can GNN be Good Adapter for LLMs?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5234.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5234.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TAPE (LLM-based interpretation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TAPE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based baseline that obtains natural-language interpretations of graph/text information via a large LM (GPT-3.5 in the paper's description) and uses those interpretations for downstream graph tasks; reported results (in this paper) are partially available only for the Arxiv dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TAPE</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>LLM-generated interpretation / graph-to-text via external LLM</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The method uses an LLM (GPT-3.5 in the paper's description) to produce natural-language interpretations/interpretive text for graph elements; those LLM-generated textual interpretations are then used (published/interpreted data available for Arxiv) as inputs/features for downstream tasks. The paper notes that TAPE required obtaining interpretation data through GPT-3.5 and that only the interpretation data on Arxiv is published for that baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>text-attributed graph (paper abstracts/citation graphs used in Arxiv example)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Creates LLM-produced natural-language interpretations (human-like explanations) of graph/text data; relies on a separate large LLM to produce these textual features, which can be informative but depends on external LLM API usage and is potentially expensive and not fully reproducible if not published. The paper highlights that this approach is an LLM-based pipeline and notes limited published data for TAPE (only Arxiv interpretations available).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification (Arxiv dataset reported); TAPE is compared as an LLM-based baseline for the node-classification experiments in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The paper states that GraphAdapter surpasses TAPE on Arxiv by 0.4% accuracy (GraphAdapter Arxiv accuracy reported as 0.7707 for Llama 2-based GraphAdapter in Table 2). The paper does not publish a full standalone numeric table of TAPE results in the excerpt beyond that comparative statement; therefore exact TAPE per-dataset metrics are not provided here (null for standalone TAPE numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>TAPE is compared qualitatively and as an LLM-based baseline; the paper reports GraphAdapter achieves a 0.4% higher Arxiv accuracy than TAPE. The paper argues that GraphAdapter is more generally applicable (pretraining+adapter vs. ad-hoc LLM interpretation) and more efficient for large LMs because GraphAdapter trains a small GNN adapter operating on cached LM hidden states instead of repeatedly querying or fine-tuning very large LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>TAPE relies on producing interpretations from a powerful external LLM (GPT-3.5) and published interpretation data is limited (only Arxiv interpretation data published per the paper). This approach can be expensive, brittle (depends on LLM prompting and output quality), and less convenient to scale or reproduce compared to methods that train small adapters on cached LM representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can GNN be Good Adapter for LLMs?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5234.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5234.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-to-text / prompt-serialization (related work mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-text conversion / graph serialization into prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of methods referenced in related work that convert graph structure and node attributes into textual prompts or serialized natural-language descriptions so that LLMs can process graphs directly via text inputs or prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph-to-text serialization / prompt-based graph encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Generic approaches that translate graph structure into natural-language strings or prompt-embedded representations (e.g., serializing nodes and edges, listing neighbor node texts, or producing textual graph summaries) and then use an LM to process that text as part of next-token prediction or prompt-based classification. The paper cites prior work that (a) converts graphs to text [cited generically as refs such as 10,40 in the paper], or (b) converts graphs into textual representations embedded in prompts [ref. 32]. No single canonical algorithmic specification is provided in this paper beyond these high-level descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>various text-attributed graphs; generic mention covers citation graphs, social graphs, and other TAGs where nodes have textual attributes</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Leverages LM strengths for language understanding and few-shot/prompt learning; representations are human-interpretable (textual); properties include high expressivity for local textual content but high token-count cost, potential redundancy, and limited scalability. The paper positions these methods as simple but computationally expensive when applied naively to large TAGs or when used with very large LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Typically used for language-model-based tasks on graphs such as node classification, text generation, or graph interpretation; in this paper such approaches are discussed relative to node-classification benchmarks (Ogbn-Arxiv, Instagram, Reddit) and other LM+graph baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The paper does not provide specific aggregated metrics for the broad class of graph-to-text conversions; it reports that frozen LLMs with prompts and some LLM-based baselines can be competitive, but that GraphAdapter provides improved accuracy on node-classification overall (e.g., GraphAdapter Llama 2: 0.7707 on Arxiv) and better efficiency. Specific metrics for individual graph-to-text methods are not enumerated in the excerpt (null).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>The paper contrasts graph-to-text / prompt-serialization approaches with (a) cascaded GNN-LM pipelines (which feed many LM inferences per GNN step), (b) self-supervised GNN-LMs (which fine-tune LMs with graph tasks), and (c) the proposed GraphAdapter (GNN adapter trained on cached LM hidden states). The central comparative claim: graph-to-text/prompting can work but may not scale or exploit graph structure as efficiently; GraphAdapter yields better average node-classification performance and far lower training parameter and storage overheads.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Key challenges discussed in the paper: high computational and memory cost when serializing many node/neighborhood texts for LMs (especially with billion-parameter LLMs), engineering brittleness and sensitivity to prompt wording, limited ability of pure text-serialization to exploit structural signals without expensive LM usage, and reproducibility/cost constraints when relying on external closed LLM APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can GNN be Good Adapter for LLMs?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating generative models for graph-to-text generation <em>(Rating: 2)</em></li>
                <li>GPT4Graph: can large language models understand graph structured Data? An Empirical Evaluation and Benchmarking <em>(Rating: 2)</em></li>
                <li>Graph neural prompting with large language models <em>(Rating: 2)</em></li>
                <li>Graph2Text <em>(Rating: 2)</em></li>
                <li>TAPE <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5234",
    "paper_id": "paper-267759791",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "Graph2Text",
            "name_full": "Graph2Text",
            "brief_description": "A baseline approach that converts a node's local graph neighborhood into natural-language text by concatenating the node's own text with the textual attributes of its 1-hop neighbors (enumerated as a list) and feeding that to a language model for downstream prediction.",
            "citation_title": "Graph2Text",
            "mention_or_use": "use",
            "representation_name": "neighbor-concatenation / graph-to-text linearization",
            "representation_description": "For each target node, the textual attribute of the node is combined with the textual attributes of its 1-hop neighbors into a single natural-language string. Example form given in the paper: \"It is a paper node, its abstract is represented as XXX, and the abstracts of its cited papers are represented as follows: 1. YYY, 2. ZZZ.\" This serialized text is provided to an LM as input (possibly with prompts) so the LM can perform next-token prediction or downstream tasks.",
            "graph_type": "text-attributed graph (citation/social graphs) using 1-hop neighbor subgraphs",
            "representation_properties": "Direct, human-readable serialization of local neighborhood; preserves raw neighbor text content; simple to implement; does not require training a separate GNN. Implicit properties discussed: potentially verbose/redundant (many neighbor texts concatenated), computationally heavy when many neighbors/text tokens are present, and scales poorly for large neighborhoods or when many nodes must be scored by an LM (ties into the paper's general critique of cascading GNN-LM approaches and LM inference cost).",
            "evaluation_task": "Node classification on text-attributed graphs (datasets used in paper: Ogbn-Arxiv, Instagram, Reddit). Graph2Text is evaluated as a baseline in the node-classification experiments reported in the paper.",
            "performance_metrics": "The paper reports comparisons of baselines including Graph2Text but does not publish an explicit per-dataset numeric row for Graph2Text in the main tables shown in the text excerpt; therefore exact numeric scores for Graph2Text are not provided in this paper (null). The paper does report that GraphAdapter (Llama 2, pre-trained) achieves accuracy 0.7707 on Arxiv (Table 2) and that GraphAdapter overall outperforms competing baselines on average.",
            "comparison_to_other_representations": "Graph2Text is included among baselines (LM-based and cascaded approaches). The paper reports that GraphAdapter (the proposed GNN-as-adapter method) outperforms existing baselines on node classification on the evaluated TAGs; the paper argues methods that directly feed many node texts into LMs suffer high compute/memory cost, motivating GraphAdapter's more efficient approach. No head-to-head numeric table in the excerpt specifically isolates Graph2Text's scores versus GraphAdapter, beyond the general baseline comparisons.",
            "limitations_or_challenges": "As discussed in the paper, serializing neighbor texts as input to an LM incurs high computational and memory costs (the general 'cascading GNN-LM' critique). The representation can be verbose and scale poorly with degree or large neighbor text sizes. The paper frames these limitations as motivating more parameter- and compute-efficient alternatives (e.g., GNN adapters that operate on cached LM hidden states).",
            "uuid": "e5234.0",
            "source_info": {
                "paper_title": "Can GNN be Good Adapter for LLMs?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "TAPE (LLM-based interpretation)",
            "name_full": "TAPE",
            "brief_description": "An LLM-based baseline that obtains natural-language interpretations of graph/text information via a large LM (GPT-3.5 in the paper's description) and uses those interpretations for downstream graph tasks; reported results (in this paper) are partially available only for the Arxiv dataset.",
            "citation_title": "TAPE",
            "mention_or_use": "use",
            "representation_name": "LLM-generated interpretation / graph-to-text via external LLM",
            "representation_description": "The method uses an LLM (GPT-3.5 in the paper's description) to produce natural-language interpretations/interpretive text for graph elements; those LLM-generated textual interpretations are then used (published/interpreted data available for Arxiv) as inputs/features for downstream tasks. The paper notes that TAPE required obtaining interpretation data through GPT-3.5 and that only the interpretation data on Arxiv is published for that baseline.",
            "graph_type": "text-attributed graph (paper abstracts/citation graphs used in Arxiv example)",
            "representation_properties": "Creates LLM-produced natural-language interpretations (human-like explanations) of graph/text data; relies on a separate large LLM to produce these textual features, which can be informative but depends on external LLM API usage and is potentially expensive and not fully reproducible if not published. The paper highlights that this approach is an LLM-based pipeline and notes limited published data for TAPE (only Arxiv interpretations available).",
            "evaluation_task": "Node classification (Arxiv dataset reported); TAPE is compared as an LLM-based baseline for the node-classification experiments in the paper.",
            "performance_metrics": "The paper states that GraphAdapter surpasses TAPE on Arxiv by 0.4% accuracy (GraphAdapter Arxiv accuracy reported as 0.7707 for Llama 2-based GraphAdapter in Table 2). The paper does not publish a full standalone numeric table of TAPE results in the excerpt beyond that comparative statement; therefore exact TAPE per-dataset metrics are not provided here (null for standalone TAPE numbers).",
            "comparison_to_other_representations": "TAPE is compared qualitatively and as an LLM-based baseline; the paper reports GraphAdapter achieves a 0.4% higher Arxiv accuracy than TAPE. The paper argues that GraphAdapter is more generally applicable (pretraining+adapter vs. ad-hoc LLM interpretation) and more efficient for large LMs because GraphAdapter trains a small GNN adapter operating on cached LM hidden states instead of repeatedly querying or fine-tuning very large LMs.",
            "limitations_or_challenges": "TAPE relies on producing interpretations from a powerful external LLM (GPT-3.5) and published interpretation data is limited (only Arxiv interpretation data published per the paper). This approach can be expensive, brittle (depends on LLM prompting and output quality), and less convenient to scale or reproduce compared to methods that train small adapters on cached LM representations.",
            "uuid": "e5234.1",
            "source_info": {
                "paper_title": "Can GNN be Good Adapter for LLMs?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Graph-to-text / prompt-serialization (related work mention)",
            "name_full": "Graph-to-text conversion / graph serialization into prompts",
            "brief_description": "A class of methods referenced in related work that convert graph structure and node attributes into textual prompts or serialized natural-language descriptions so that LLMs can process graphs directly via text inputs or prompt engineering.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "graph-to-text serialization / prompt-based graph encoding",
            "representation_description": "Generic approaches that translate graph structure into natural-language strings or prompt-embedded representations (e.g., serializing nodes and edges, listing neighbor node texts, or producing textual graph summaries) and then use an LM to process that text as part of next-token prediction or prompt-based classification. The paper cites prior work that (a) converts graphs to text [cited generically as refs such as 10,40 in the paper], or (b) converts graphs into textual representations embedded in prompts [ref. 32]. No single canonical algorithmic specification is provided in this paper beyond these high-level descriptions.",
            "graph_type": "various text-attributed graphs; generic mention covers citation graphs, social graphs, and other TAGs where nodes have textual attributes",
            "representation_properties": "Leverages LM strengths for language understanding and few-shot/prompt learning; representations are human-interpretable (textual); properties include high expressivity for local textual content but high token-count cost, potential redundancy, and limited scalability. The paper positions these methods as simple but computationally expensive when applied naively to large TAGs or when used with very large LMs.",
            "evaluation_task": "Typically used for language-model-based tasks on graphs such as node classification, text generation, or graph interpretation; in this paper such approaches are discussed relative to node-classification benchmarks (Ogbn-Arxiv, Instagram, Reddit) and other LM+graph baselines.",
            "performance_metrics": "The paper does not provide specific aggregated metrics for the broad class of graph-to-text conversions; it reports that frozen LLMs with prompts and some LLM-based baselines can be competitive, but that GraphAdapter provides improved accuracy on node-classification overall (e.g., GraphAdapter Llama 2: 0.7707 on Arxiv) and better efficiency. Specific metrics for individual graph-to-text methods are not enumerated in the excerpt (null).",
            "comparison_to_other_representations": "The paper contrasts graph-to-text / prompt-serialization approaches with (a) cascaded GNN-LM pipelines (which feed many LM inferences per GNN step), (b) self-supervised GNN-LMs (which fine-tune LMs with graph tasks), and (c) the proposed GraphAdapter (GNN adapter trained on cached LM hidden states). The central comparative claim: graph-to-text/prompting can work but may not scale or exploit graph structure as efficiently; GraphAdapter yields better average node-classification performance and far lower training parameter and storage overheads.",
            "limitations_or_challenges": "Key challenges discussed in the paper: high computational and memory cost when serializing many node/neighborhood texts for LMs (especially with billion-parameter LLMs), engineering brittleness and sensitivity to prompt wording, limited ability of pure text-serialization to exploit structural signals without expensive LM usage, and reproducibility/cost constraints when relying on external closed LLM APIs.",
            "uuid": "e5234.2",
            "source_info": {
                "paper_title": "Can GNN be Good Adapter for LLMs?",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating generative models for graph-to-text generation",
            "rating": 2,
            "sanitized_title": "evaluating_generative_models_for_graphtotext_generation"
        },
        {
            "paper_title": "GPT4Graph: can large language models understand graph structured Data? An Empirical Evaluation and Benchmarking",
            "rating": 2,
            "sanitized_title": "gpt4graph_can_large_language_models_understand_graph_structured_data_an_empirical_evaluation_and_benchmarking"
        },
        {
            "paper_title": "Graph neural prompting with large language models",
            "rating": 2,
            "sanitized_title": "graph_neural_prompting_with_large_language_models"
        },
        {
            "paper_title": "Graph2Text",
            "rating": 2,
            "sanitized_title": "graph2text"
        },
        {
            "paper_title": "TAPE",
            "rating": 2
        }
    ],
    "cost": 0.014267499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can GNN be Good Adapter for LLMs?
20 Feb 2024</p>
<p>Xuanwen Huang xwhuang@zju.edu.cn 
Yang Yang yangya@zju.edu.cn 
Ziwei Chai zwchai@zju.edu.cn 
Qi Zhu 
Kaiqiao Han kaiqiaohan@zju.edu.cn 
Dezheng Bao baodezheng@zju.edu.cn 
Quanjin Tao taoquanjin@zju.edu.cn </p>
<p>Zhejiang University Hangzhou
Kaiqiao HanChina</p>
<p>Zhejiang University Hangzhou
China</p>
<p>Zhejiang University Hangzhou
China</p>
<p>Dezheng Bao</p>
<p>Zhejiang University Hangzhou
China Quanjin Tao</p>
<p>Zhejiang University Hangzhou
China</p>
<p>Zhejiang University Hangzhou
China</p>
<p>Amazon Web Services Santa Clara
USA</p>
<p>Can GNN be Good Adapter for LLMs?
20 Feb 2024846C415664FFF46A00EBC2E898BFFAFB10.1145/3589334.3645627arXiv:2402.12984v1[cs.CL]Computing methodologies ‚Üí Artificial intelligence;Information systems ‚Üí Data mining Graph Neural NetworksLarge Language ModelText-Attributed Graph
Recently, large language models (LLMs) have demonstrated superior capabilities in understanding and zero-shot learning on textual data, promising significant advances for many text-related domains.In the graph domain, various real-world scenarios also involve textual data, where tasks and node features can be described by text.These text-attributed graphs (TAGs) have broad applications in social media, recommendation systems, etc.Thus, this paper explores how to utilize LLMs to model TAGs.Previous methods for TAG modeling are based on million-scale LMs.When scaled up to billion-scale LLMs, they face huge challenges in computational costs.Additionally, they also ignore the zero-shot inference capabilities of LLMs.Therefore, we propose GraphAdapter, which uses a graph neural network (GNN) as an efficient adapter in collaboration with LLMs to tackle TAGs.In terms of efficiency, the GNN adapter introduces only a few trainable parameters and can be trained with low computation costs.The entire framework is trained using auto-regression on node text (next token prediction).Once trained, GraphAdapter can be seamlessly fine-tuned with taskspecific prompts for various downstream tasks.Through extensive experiments across multiple real-world TAGs, GraphAdapter based on Llama 2 gains an average improvement of approximately 5% in terms of node classification.Furthermore, GraphAdapter can also adapt to other language models, including RoBERTa, GPT-2.The promising results demonstrate that GNNs can serve as effective adapters for LLMs in TAG modeling.</p>
<p>INTRODUCTION</p>
<p>Graphs are ubiquitous in the real world [1].In the past, graph structures have been extensively explored and utilized in many machine learning applications [27,39].In many practical cases, the nodes in graphs have textual features, known as Textual-Attributed Graphs (TAGs) [37].For example, in social media [18], nodes represent users and node features are user profiles.Nodes in TAGs have both textual and structural data, which both reflect their intrinsic properties.Combining textual and structural data to modeling TAGs is an exciting new exploration for both graph machine learning and language modeling, which can benefit the application of graphs.</p>
<p>In TAGs, a complex correlation exists between the structural and textual data of nodes.Understanding this correlation can benefit the modeling of TAGs [5].In Figure 1, user "Bob" frequently browses daily news on social media, as evidenced by the descriptions in his user profile.Users similar to Bob, who have many followers and often browse news nodes, are also likely interested in news.In other words, a graph can supplement textual attributes on a node through structural proximity.Graph Neural Networks (GNNs) are the de facto machine learning models for leveraging textual information alongside graph structures in TAGs.However, there's a lack of a unified GNN architecture compatible with different language models, especially the powerful foundation models.Recently, there has been a surge in studies investigating effective ways to model both textual and structural data in TAGs.Some of these studies emphasize optimizing a cascading architecture that combines GNNs and LMs (cascading GNN-LMs) [37,42].One major challenge with these models is the extreme amount of additional computational cost brought by the message-passing mechanism.To this end, several studies have successfully reduced the memory and computational overheads of such cascaded models by freezing partial or full parameters of the backbone language models [20,25].Large language models exhibit superior multi-task and fewshot learning capabilities across various real-world applications [2].However, when considering cascading GNN-LMs, existing techniques cannot be scaled up to billion-scale models like Llama 2 [33].Another pioneering research has ventured to fine-tune language models using unsupervised graph information (self-supervised GNN-LMs) [4,26].For instance, GIANT [4] fine-tunes language models through a neighbor prediction task, subsequently using the refined language model to extract node representations for downstream tasks.In these methods, PLMs can indirectly incorporate graph information during the tuning process thereby enhancing their capability to process TAGs.However, they separate the training of GNNs and LMs, potentially leading to sub-optimal graph-aware tuning results.</p>
<p>Instead of using graph information as supervision, we believe graph structure can enrich textual features through language modeling.In our previous example, structural proximity can be used to infer the user's preference even if he or she does not mention it in the profile.So, unlike self-supervised methods, we consider pre-training a framework that can combine graph-aware structure and LLMs by leveraging rich textual features.However, traditional frameworks like cascading GNNs and LLMs face efficiency issues in pre-training scenarios.Therefore, inspired by works on parameterefficient tuning of LLMs [14,22,23], we propose the use of GNNs as efficient adapters for LLMs (i.e., GraphAdapter).In GraphAdapter, the LM is frozen and the final output of the LM is altered by the trainable adapter GNNs.GraphAdapter offers several advantages:</p>
<p>‚Ä¢ Lightweight: A GNN adapter introduces a few trainable parameters and low computational costs.‚Ä¢ Language-aware graph pre-training: Using language to supervise the modeling of graph structure, which can help LLMs comprehend both textual and structural information.</p>
<p>‚Ä¢ Convenient tuning: Once a graph-specific adapter is pretrained, it can be fine-tuned for multiple downstream tasks.Now we present the details of GraphAdapter with respect to pretraining and fine-tuning of the adapter GNNs.To capture the data distribution of the graph, we employ parameter-efficient tuning of LLMs on node texts.This approach is similar to the continual training of language models [31] except GNN is the tuning parameter, which helps reduce the distribution discrepancy between the pre-training corpus and target data.To further enhance efficiency, we utilize the GNN adapter exclusively at the transformer's final layer.It ensures that all transformer computational processes are executed just once and then can be cached for adapter training.Besides, we perform mean-pooling on the predicted logits from a GNN adapter and LLMs then optimize their final results of the next-word prediction, which can help adapters focus more on the graph-related tokens.Once the adapter is trained, one can use GraphAdapter together with the backbone LLMs on various downstream tasks.For instance, we use a classification head atop the embeddings of the last token to fine-tune for node classification.</p>
<p>To verify the effectiveness of GraphAdapter, we conduct extensive experiments on multiple real-world TAGs including social and citation networks.GraphAdapter achieves an improvement of 4.7% over state-of-the-art cascaded GNN-LM methods and 5.4% over self-supervised GNN-LMs on average, with 30X fewer training parameters and storage.Moreover, once GraphAdapter is pre-trained, it can be conveniently fine-tuned for various tasks.Our ablation analysis shows that the pre-training step consistently improves the model performance across different graphs.We summarize our contributions as follows,</p>
<p>‚Ä¢ GraphAdapter is a novel approach that harnesses the large language models on graph structure data with parameter-efficient tuning.</p>
<p>‚Ä¢ We propose a residual learning procedure to pre-train the GNN adapter with the LLMs.The pre-training step significantly improves the fine-tuning performance of GraphAdapter.‚Ä¢ We conduct extensive experiments on large-scale TAGs using state-of-the-art open-sourced large language models (GPT-2 1.5B [28] and Llama 2 13B [33]).The results demonstrate that Graph-Adapter can also reap the benefits of a larger model.</p>
<p>RELATED WORK</p>
<p>Modeling text-attributed graphs has attracted much attention in academia, which requires modeling both textual and structural data.</p>
<p>Modeling semantics and graph structure.Understanding the semantics is a key part of modeling TAG.With the advent of Transformers [34], pre-trained language models have made breakthrough progress in modeling semantics [6,43].These methods leverage massive unlabeled text through unsupervised methods like auto-regressive [19] and auto-encoding pre-training [12,24] to pre-train Transformers and then fine-tune to downstream tasks.Since language models have a large number of parameters, finetuning efficiency is low and requires numerous training samples.Therefore, some work proposed using adapter modules to reduce the number of fine-tuning parameters.For example, LoRA [14] trains a sparse matrix appended to the original parameters while keeping the language model frozen.Some work proposed using prompts to directly adapt language models to downstream tasks without fine-tuning.Furthermore, some work proposed prompt tuning [16,23], which adds a trainable prompt and only trains the added prompt during training, greatly reducing the number of parameters.Another aspect of modeling TAG is modeling the structural information.With the proposal of GNNs [11], modeling graph structure achieved remarkable success.Many works [21,36] have explored GNN architectures extensively, and these methods have achieved breakthrough progress in graph structure modeling.</p>
<p>Modeling TAGs.However, despite the success of language models and GNNs in their respective areas, how to utilize them to model text-attributed graphs still has many challenges.(1) Cascading GNN-LM: Directly cascading these two models is straightforward but has limitations, mainly high computational overhead.Since GNNs are mostly based on message-passing, they need to compute representations for many nodes simultaneously.Using language models to model so many text features requires huge memory and time costs.To address this, some work [25] proposed freezing the language model to reduce the computation needed for cascading.Some work [17,20] proposed neighbor sampling but that reduces the graph information captured.Therefore, recently some work tried joint training of LMs and GNNs through knowledge distillation [26] or Expectation Maximization algorithms [42].(2) Self-supervised GNN-LMs: some methods [4,26] directly supervise language model fine-tuning through graph-related tasks, to help language models better understand the textual information in text-attributed graphs.The language model is then combined with GNNs by freezing the language model.This approach demonstrates the inherent connections between graph structure and text in TAGs.However, current research in this direction has limitations in that the LM and graph are separate, and the language model cannot directly perceive graph information.It also does not utilize the inherent connections between language and graphs to help GNNs better learn structural features.(3) LLMs for Graph: With the breakthrough progress made by LLMs on textual tasks [33,41], recently many works have emerged exploring how to directly utilize LLMs to understand text-attributed graphs [3].For example, by converting the graph to text [10,40], or by converting it to a graph representation as part of a prompt [32].Some works also explored using large models to enhance the textual features of text-attributed graphs [7,13].However, this paper is more focused on how to leverage the semantic information in text-attributed graphs to help us model text-attributed graphs.Therefore, this type of method not be further elaborated.</p>
<p>BACKGROUND</p>
<p>Before introducing the proposed method, it's important to understand some basic concepts and the background of pre-trained language models, graph neural networks, and text-attributed graphs.</p>
<p>Pretrained Language Model</p>
<p>Textual data.Textual data can be formulated as D = { 1 ,  2 ...  }.It can be tokenized into a sequence of tokens S = { 1 ,  2 , ...,   }, where   represents a specific token-id.In most cases, the first token in the sentence (i.e.,  0 ) is [CLS], indicating the beginning of this sentence.</p>
<p>Framework of PLMs.</p>
<p>A PLM consists of a multi-layer transformer encoder that takes a sentence S as input and outputs the hidden states of each token:
Transformer({ùë† 0 , ..., ùë† ùêø }) = {‚Ñé 0 , ..., ‚Ñé ùêø },(1)
where ‚Ñé  is the dense hidden state of   .Pre-training of PLMs.This paper uses the auto-regression task as an instance of pre-training, which is commonly applied to autoregressive PLMs [29].Given a sequence S = { 0 , ...,   }, the goal is to model the joint probability of the sequence  (S).
ùëÉ (S) = ùêø ùëò=1 ùëù (ùë† ùëñ |ùë† 0 , ...ùë† ùëò ‚àí1 )(2)
The transformer block is used to model these conditional probabilities.More specifically, at step  (0 &lt;  ‚â§ ), the transformer receives { 0 ...  ‚àí1 } and outputs their hidden states {‚Ñé 0 , ..., ‚Ñé  }.The ‚Ñé  are used to predict the probability distribution of the next token.
ùëù (ùë† ùëñ |ùë† 0 , ...ùë† ùëò ‚àí1 ) = ≈ùùëò = ùúé (Head(‚Ñé ùëò ))(3)
The model parameters are trained to maximize the likelihood of  (S), which is equivalent to minimizing the negative log-likelihood.Therefore, the loss function is:
L ùêøùëÄ = ùêø ‚àëÔ∏Å ùëò=1 CrossEntropy(ùë† ùëò , ≈ùùëò )(4)
Sentence representation.Given a sentence S with length , its sentence representation can be obtained by three methods [8,30]:</p>
<p>(1) first token representation, which uses the hidden state of the [CLS] token (‚Ñé ,0 ) as sentence representation.(2) mean-pooling representation, which is obtained by mean-pooling of all hidden states (i.e., Pool({‚Ñé 0 ...‚Ñé  })).</p>
<p>(3) last token representation, which uses the hidden state of the last token.</p>
<p>PLMs with prompts.Due to the gap between pretraining tasks and downstream tasks, sentence representation may be hard to contain all the sentence information, thereby requiring fine-tuning for specific tasks.To address this issue, some studies utilize prompts to extract task-specific sentence features [16].For example, suppose a S  is a paper titled "Llama 2: Open Foundation and Fine-Tuned Chat Models", and the task is to classify the subject of it belongs.We can add some prompts to the sentence:
{[ùëáùëñùë°ùëôùëí], ùë°‚Ñéùëñùë†, ùëùùëéùëùùëíùëü, ùëèùëíùëôùëúùëõùëî, ùë°ùëú, ùë§‚Ñéùëñùëê‚Ñé, ùë†ùë¢ùëè ùëóùëíùëêùë°?}(5)
We denote this new sentence with the prompt inserted as S  |P , where P represents the newly inserted tokens.We use the hidden state of the last token as the sentence representation, denoted as   |P .Since the last token is used to predict the next token distribution in the pre-training stage, it can naturally combine the inserted prompt information into the original sentence and extract the prompt-related semantics.Extensive studies [19,23] show that using prompts can reduce the gap between PLMs and downstream tasks and maximize the utilization of knowledge learned by PLMs during pre-training.</p>
<p>Graph Neural Network</p>
<p>Graph Neural Networks (GNNs) have achieved remarkable success in modeling graphs [9,35].The message-passing framework is a commonly used architecture of GNN.</p>
<p>Graph.Let  = { , } denote a graph, where  is the node set and  is the adjacency matrix, with    = 1 meaning there is an edge between node  and node .Usually, each node  is associated with a node feature  0  .Framework of GNN.The message-passing framework takes a set of node features X = { 0  | ‚àà  }, and an adjacency matrix  as input and iteratively captures neighbors' information via pooling.More specifically, for a given node  ‚àà  in the -th layer of messagepassing, it can be formulated as:
ùë• ùëô ùëñ = ùëì 2 (Pool{ùëì 1 (ùë• ùëô ‚àí1 ùëó |ùúÉ ùëô 1 )| ùëó ‚àà N ùëñ }, ùë• ùëñ |ùúÉ ùëô 2 )(6)
where Pool{‚Ä¢} is an aggregation function that combines the features of neighboring nodes, such as mean-pooling.And N  denotes the set of neighbors of node .Besides,  1 (‚Ä¢|  1 ) and  2 (‚Ä¢|  2 ) denote two trainable transformations with parameters   1 and   2 respectively.Further, we denote an   layer message-passing framework as GNN, formally:
ùëß ùëñ = GNN(ùë• 0 ùëñ , X 0 , ùê¥|Œò ùëî )(7)
where
ùëß ùëñ = ùë• ùëô ùëöùëéùë•</p>
<p>ùëñ</p>
<p>, and Œò  represents all the trainable parameters in the GNN.We use   as the structural representation for node .</p>
<p>Text-Attributed Graph</p>
<p>Let G = {V, A} denote a text-attributed graph, where V is the node set and A is the adjacency matrix.Each node  ‚àà V is associated with a tokenized textual data, represented by S  = { ,0 , ...,  ,  }, which represents the textual data of the node.Problem Definition: Given a text-attributed graph G and corresponding node labels Y =   | ‚àà N , this paper addresses the problem of efficiently modeling both the textual data S  | ‚àà V and the structural data in G to predict the node labels Y.</p>
<p>METHOD</p>
<p>This section introduces the proposed framework, referred to as GraphAdapter, which uses GNNs as adapters for LLMs to better model TAGs.</p>
<p>Overview</p>
<p>Motivation: In the textual data of TAGs, many structure-related semantics are hard to infer from context alone.As illustrated in the example in Figure 1, we can easily infer that this user is "popular" based on his degree in the social network, but it is difficult to infer from their description of habits alone.Combining structural information can enhance language models' ability to model these structure-related semantics in TAGs.Meanwhile, the process of enhancement is learning how to model structure.Therefore, the proposed method GraphAdapter, which first uses GNN as adapters for frozen PLMs, to combine structural information with PLMs, and then pre-trains them through the semantic understanding task on TAGs.</p>
<p>Language-structure pre-training: In the field of natural language processing, pre-training is a common strategy used to selfsupervised enhance language models' ability for semantic understanding, with techniques such as auto-regressive pre-training (e.g., GPT-2/3 [2,29], Llama 2 [33], etc.) and auto-encoding pretraining (e.g., BERT [38], RoBERTa [24], etc.).Following our motivation, GraphAdapter uses the same pre-training task as these PLMs.To facilitate comprehension, this section only discusses GraphAdapter based on auto-regressive pre-training, and further details on how GraphAdapter is combined with other pre-training tasks can be found in the appendix.Since the pre-training process uses the context semantic to supervise structure learning, we refer to this pre-training as language-structure pre-training.</p>
<p>Pre-training on TAGs</p>
<p>In the training stage, GraphAdapter uses the textual data of each node in TAG to train GNN.</p>
<p>Pipeline of pre-training:</p>
<p>Given a text-attributed graph G, node  and its textual data S  = { ,0 , ...,  ,  }, GraphAdapter uses all the tokens in S  as supervision.For the -th token, GraphAdapter first extracts its previous tokens S , = { ,0 , ...,  , ‚àí1 }.Then, GNN models node 's structure information   .The structure information is then combined with the previous tokens to predict the probability distribution of the next token, where the ground truth is token  , .</p>
<p>Structural representation: GraphAdapter obtains its structural features   through GNN.Here we use a general GNN based on the message-passing framework, which continuously aggregates neighbor features to obtain the new node's structural information.For whole process is formalized as:
ùëß ùëñ = GNN(ùë• 0 ùëñ , X, A|Œò ùëî )(8)
where  0  and A represent the initial node feature input and adjacency matrix in GNN, respectively.This paper used the sentence representation of the corresponding node as  0  .See more details about GNN in Section 3.2.</p>
<p>Context hidden-states.GraphAdapter use the pre-trained transformer in PLM to encode S , , it is formalized as: Where the Transformer's parameters are trained in frozen, and ‚Ñé , is the context hidden-states S , .Note that in the pretraining stage of PLM, ‚Ñé , is directly used to predict the next token, so ‚Ñé , contains both the context information and a certain of PLMs' prediction result.
‚Ñé ùëñ,ùëò = Transformer({ùë† ùëñ,0 , ùë† ùëñ,1 , ..., ùë† ùëñ,ùëò ‚àí1 })(9)
Fusion block: GraphAdapter next fuse structural representation into context hidden-states, which is formalized as:
ùëü ùëñ,ùëò = Fusion(‚Ñé ùëñ,ùëò , ùëß ùëñ |Œò ùëì ùë¢ùë†ùëí ),(10)
The Fusion( * ) function is trainable with parameters Œò   .In this paper, MLPs are used as the structure of fusion.The process involves concatenating ‚Ñé , and   , and then feeding the resulting vector into MLPs.</p>
<p>Residual connection: the fused  , contains both structure information and context information.However, not every token's prediction requires the graph structure.For example, in the sentence "This paper focuses on graphs," the word "on" is simply a fixed collocation and easily inferred by context.Intuitively, words related to graph structure should be difficult for the language model to predict based on context.Therefore, the results of pre-trained language models are reused.We separately calculated the prediction probabilities of the language model alone and the probabilities that mixed the graph structure and the previous predictions.The two probabilities are then averaged to obtain the final prediction result.Formally:
≈ùùêøùëÄ ùëñ,ùëò = ùúé (Head(‚Ñé ùëñ,ùëò )), ≈ùùê∫ùëÅ ùëÅ ùëñ,ùëò = ùúé (Head(ùëü ùëñ,ùëò )) (11) ≈ùùê¥ùêøùêø ùëñ,ùëò = ( ≈ùùêøùëÄ ùëñ,ùëò + ≈ùùê∫ùëÅ ùëÅ ùëñ,ùëò )/2(12)
Where  denotes the softmax function.Naturally, if a token  , can be accurately predicted by the language model and the corresponding score ≈ù  , is evenly distributed, the overall result remains correct.Conversely, if the token cannot be predicted by the language model, the GraphAdapter needs to predict the correct token precisely to ensure the final result is correct.This difference leads the model to naturally focus on tokens originally predicted poorly by the language model during optimization.It then attempts to use additional structural data to enhance the overall framework's predictive performance.</p>
<p>Optimization: Our goal is to minimize the cross-entropy loss between the predicted probability distribution and the ground-truth distribution.Formally,
L ùëñ,ùëò = CrossEntropy( ≈ùùê¥ùêøùêø ùëñ,ùëò , ùë† ùëñ,ùëò )(13)
min
Œò ùëî ,Œò ùëì ùë¢ùë†ùëí ‚àëÔ∏Å ùëñ ‚ààùëâ ‚àëÔ∏Å ùëò ‚àà S ùëñ L ùëñ,ùëò(14)
Note, only GNN( * |Œò  ) and Fusion( * |Œò   ) of GraphAdapter are trainable in whole pre-training.</p>
<p>GNN as Adapter:</p>
<p>In the whole pre-training stage, the GNN combines with the frozen LM's hidden states outputted from the transformer block.The combined hidden states are then input into the PLM's prediction head.Thus, the GNN acts as an adapter, altering the language model's predictions.Since the hidden states outputted by the transformer block can be pre-processed and stored in advance.Therefore, the entire training process only requires training the GNN.Therefore, GraphAdpater can efficiently pre-train based on different scales of PLMs.</p>
<p>Fine-tuning with Prompts</p>
<p>The pipeline is shown in Figure 2 (b).GraphAdapter is pre-trained by token-level semantic understanding tasks.To better utilize the learned knowledge of GraphAdapter and the PLMs in downstream tasks, we further proposed prompt-aware fine-tuning.It inserts prompts in textual data to get task-specific sentence embedding of each node.Prompts can transform various downstream tasks on TAGs into next token prediction.E.g., the task "Which account is a student account" can be transformed by a next-token prediction task, "[context], based on this profile, this user is ".In the pre-training stage,  1 performance reported in [42] GraphAdapter has learned how to utilize the structural information captured by GNN to enhance the accuracy of next-token prediction, therefore, under the transformed downstream task can better utilize the learned knowledge from pre-training.Formally, given textual data S  of node , we can combine a sequence tokens with task-specific prompts behind textual data, namely, S  |P = [S  , P], then we can get its sentence hidden states ‚Ñé  |P through the transformer of PLM.The resulting hidden state is then fused with the node's structural representation as node representation in a specific downstream task.
ùëü ùëñ |P = Fusion(‚Ñé ùëñ |P , ùëß ùëñ )(15)
This node representation can be used in various tasks.For example, in the node classification, we can append a new linear transformation to output the result, i.e., ≈∑ |P =  (  |P |  ).In fine-tuning stage, the whole parameters {Œò  , Œò   ,   } in GraphAdapter are trainable.</p>
<p>EXPERIMENT</p>
<p>To comprehensively validate that GraphAdapter can mine the intrinsic correlation between the textual and structure data in TAGs, we conduct extensive experiments on three real-world datasets from diverse domains.Our experimentation centered on the following five questions:</p>
<p>‚Ä¢ Q1: How well is GraphAdapter in modeling TAGs?</p>
<p>‚Ä¢ Q2: Whether GraphAdapter can adapt to other PLMs?</p>
<p>‚Ä¢ Q3: Are all components comprising GraphAdapter valid?</p>
<p>‚Ä¢ Q4: What exactly does GraphAdapter's pre-training learn?</p>
<p>‚Ä¢ Q5: How efficient is GraphAdapter?</p>
<p>Experiment setup</p>
<p>Dataset and metrics.We select three public and real-world datasets used for evaluation: Ogbn-arxiv [15] (shorted as Arxiv), Instagram [18] and Reddit2 .The evaluation task involves node classification.The metric used for Arxiv and Reddit is accuracy, while for Instagram, it is ROC-AUC.See more details in Appendix A.</p>
<p>Baselines.We compare the proposed GraphAdapter with several TAG modeling methods.They are LM+MLPs, LM+GNN, GIANT [4], GLEM [42], Graph2Text [3], LoRA [14] and TAPE [13].Since most of these methods can combine with different GNN blocks and PLMs, and the specific GNN framework is not the key point this paper focuses on, this paper uses GraphSAGE [11] as an instance of GNN.And we detail the used PLMs in Table 1.Please refer to Appendix A.2 for more details.</p>
<p>Prompts.Since GraphAdapter involves prompts, to make a fair comparison, we also enhance the baselines with prompts (denoted as "+Prop").For further details, please refer to the Appendix A.3.Implementation details.In the experiment, Llama 2 defaulted to the 13B version, while other language models defaulted to the large version.For further details, please refer to the Appendix A.4.</p>
<p>Performance</p>
<p>Q1: How well is GraphAdapter in modeling TAGs?A1: GraphAdapter can effectively model TAGs and surpass current state-of-the-art baselines on node classification tasks.We compare GraphAdapter with 6 state-of-the-art baselines on 3 different real-world datasets to evaluate its effectiveness.As Table 1 shows, the experiment results suggest: (1) Frozen LLMs are effective on TAGs.In general, frozen LLMs have an improved performance compared to the previous frozen LM. (3) GraphAdapter can effectively combine GNN and LLM, surpassing existing state-of-the-art baselines in terms of performance.The pre-training effect of GraphAdapter is significant, bringing an average performance improvement of 1.98% and thus surpassing existing state-of-the-art baselines.Specifically, GraphAdapter achieves an improvement of 4.72% over state-of-the-art cascaded GNN-LM methods and 5.40% over self-supervised GNN-LMs on average.At the same time, GraphAdapter also surpasses TAPE, another LLMbased method on Arxiv by 0.4% accuracy improvement.</p>
<p>Q2: Whether GraphAdapter can adapt to other PLMs?A2: GraphAdapter can be effectively pre-trained based on RoB-ERTa, GPT-2, and Llama 2, resulting in performance improvements.We run GraphAapter based on 3 different LMs.The experiment results are shown in Table 2. GraphAdapter improved average performance over directly combining GNNs with frozen PLM by 1.67% on RoBERTa, 1.89% on GPT-2, and 2.77% on Llama 2. Meanwhile, GraphAdapter pre-training brings 1.67%, 1.50%, and 1.02% improvements on RoBERTa, GPT-2, and Llama 2 respectively.This result fully demonstrates that GraphAdapter is a general and scalable method.It is worth noting that the pre-training method of RoBERTa is different from others.GraphAdapter uses a pre-training task similar to RoBERTa, so there are some slight differences from the formula in Section 4. The main differences come from the loss function and language model inputs.We describe the details of applying GraphAdapter on Roberta in the appendix.Under the same PLM, the performance of GraphAdapter is comparable to the SOTA baselines based on fine-tuning the PLM.We evaluate the performance between GraphAdapter and SOTA baselines under the same LM.Since the GLEM adopted DeBERTa, however, the pre-training code of DeBERTa is not opensourced at present.To keep consistent, GraphAdapter and GLEM both adopt the same RoBERTa-base.As shown in Table 3, the experiment results suggest that methods based on pre-training like GIANT and GraphAdapter perform better on small datasets like Instagram and Reddit.Similarly, Roberta-based GraphAdapter outperforms GLEM by 1.57% and BERT-based GIANT outperforms GLEM by 1.15% on small datasets.Compared to baselines based on pre-training, although GIANT fine-tunes the LM, its performance is 0.51% lower than GraphAdapter on average.Therefore, overall, even without fine-tuning the LM, the performance of GraphAdapter is comparable to current state-of-the-art baselines based on finetuning the LM.</p>
<p>In-depth Analysis.</p>
<p>Q3: Are all components comprising GraphAdapter valid?</p>
<p>A3: As Table 4 shows, removing any component of Grap-hAdapter results in performance drops.Removing pre-training leads to a 0.91% drop, demonstrating that GraphAdapter's improvements indeed come from pre-training.Next, the most significant performance drop is when we simultaneously remove pre-training and graph structure in the fine-tuning stage (keeping only selfloops), which causes a 1.95% drop.This shows having the graph is crucial for GraphAdapter to work.Removing the task-related prompt leads to a 0.98% drop, validating our design of aligning pretraining tasks via prompts.Notably, removing the residual learning ("w/o Res Label" that is stated in Equation 12) leads to a 1.02% drop (more than removing pre-training), suggesting that training GNNs directly on all text may introduce excessive noise and hurt performance.Therefore, GraphAdapter indeed benefited from residual (1) GNN can obtain stronger expressive power through pre-training.We first observe the performance change of GNNs before and after pre-training, where we directly use the structural representations from the pre-trained GNN to fine-tune for downstream tasks.As Table 5 shows, the pre-trained GNN performs better on downstream tasks, improving by 0.78% on average.This demonstrates that GNNs are training their ability to model the graph structure during pre-training.</p>
<p>(2) Fusion block is learning how to fuse the knowledge from the language model and GNN during pre-training.We further explore whether the fusion layer learned useful knowledge during training.We randomly initialize the parameters in a specific GraphAdapter's blocks after pre-trained.As Table 6 shows, initializing the parameters of the fusion layer leads to significant performance drops, decreasing by 1.03% on average across 3 datasets.This result shows that the enhanced knowledge from GNN may need to be outputted through the matching fusion layer.To further verify this conjecture, we further reinitialized the parameters of GNN, and some performance decline can also be observed, decreasing by 0.82% on average.This is similar to the impact of reinitializing the fusion layer.The fusion layer alone does not contain much knowledge.Therefore, these results demonstrate that the fusion layer can learn how to fuse the knowledge from GNN and language models.</p>
<p>(3) Graph structure is the basis of pre-training.We further observe the changes in different base models before and after pretraining.In this comparative experiment, we keep all the structures of GraphAdapter, only replacing the GNN block with MLPs of equal parameter size.As Figure 3 shows, the MLP-based GraphAdapter shows no significant change before and after pre-training (average improvement of 0.19%), and even decreases in performance on Instagram and Reddit (drops of 0.05% and 0.62% respectively).While the GNN improves notably before and after pre-training (average improvement of 0.91%).This result suggests that GNN is a prerequisite for effective pre-training.</p>
<p>These three results demonstrate that GraphAdapter is indeed learning graph structures via pre-training.This validates that the</p>
<p>CONCLUSION</p>
<p>This paper proposes GraphAdapter to harness LLMs for TAGs without fine-tuning.A GNN adapter is trained to reduce LLM next-word errors on node texts.This adapts LLMs for graphs efficiently.Across node classification tasks, GraphAdapter improves accuracy by 5% over baselines.We validate with RoBERTa, GPT-2, and Llama 2, efficiently leveraging LLMs for interconnected text-graph data.</p>
<p>A EXPERIMENT SETTING</p>
<p>A.1 Dataset Details</p>
<p>We select three public and real-world datasets used for evaluation.Table 7 shows detailed statistics of these datasets.Below are the details of these datasets: Arxiv.Ogbn-Arxiv (shorted as Arxiv), is a citation network where edges represent citation relationships, nodes represent papers and the text attribute is the abstracts of papers.The task on this graph is to predict paper subjects.This paper uses the public partition, ground truth, and text information provided by OGB [15].
Instagram.
Instagram is a social network where edges represent following relationships, nodes represent users, and the prediction task is to classify commercial and normal users in this network.</p>
<p>The original dataset for Instagram is provided by [18].Since the original dataset did not contain graph information, we obtained users' follow lists, personal introductions, and tags for commercial users through Instagram's public API 3 .Reddit.Reddit is also a social network where each node denotes a user, the node features are the content of users' historically published subreddits, and edges denote whether two users have replied to each other.The prediction task is to classify whether a user is in the top 50% popular (average score of all subreddits).It is constructed on a public dataset 4 that collected replies and scores from Reddit users.The node text feature of this graph is the user's historical post content (limited to the last three posts per user).We divided users into popular and normal categories based on their average score of history posts, with users whose average score is higher than the median considered popular and others considered normal.</p>
<p>A.2 Baselines</p>
<p>We compare the proposed GraphAdapter with several state-of-theart TAG modeling methods.</p>
<p>‚Ä¢ GNN-based methods: This method directly combines different frozen PLM with GNNs to model TAGs.Since the specific GNN framework is not the key point this paper focuses on, this paper uses GraphSAGE [11] as an instance of GNN.‚Ä¢ LM-based methods: we select GIANT [4], and GLEM [42] as baseline.GIANT uses self-supervised tasks to finetune PLM.Then incorporates the fine-tuned PLM and GNN to model TAG.GLEM jointly trains PLM and GNN.Note that GIANT is based on BERT, and GLEM uses DeBERTa.Considering PLMs have a high influence on performance, we also compare GraphAdapter with them under the same PLM.‚Ä¢ LLM-based methods: There are a few LLM-based methods that are suitable in our setting.Therefore, we select TAPE [13] as the LLM-based baseline.This method, due to its need to obtain the interpretation of the text graph through GPT-3.5 and only the interpretation data on Arxiv is published.Therefore, we only report the results of this method on Arxiv.Besides, we also extend MLPs, Graph2Text [3], and LoRA [14] to our experiment setting.</p>
<p>Graph2Text directly incorporates the textual data of the 1-hop neighbors of a node to model the graph structure.For example, "It is a paper node, its abstract is represented as XXX, and the abstracts of its cited papers are represented as follows: 1. YYY, 2. ZZZ."Since many baseline methods involve GNN components, which are mostly optional, and considering that different GNNs have different performances.To make a fair comparison and without loss of generality, all GNNs used in all baselines are fixed to GraphSAGE, which is a classic and general GNN model.</p>
<p>A.3 Prompts</p>
<p>According to the downstream task and graph information, this article has designed simple prompts for each dataset.As shown in Table 8.It should be noted that because PLMs are sensitive to prompts, different prompts may result in significant performance   GAT, denoted as GAT*, and proposed this result to inspire future works.As Table 9 shows, the pre-training of GraphAdapter is also suitable for attention-based GNN.</p>
<p>B.2 Analysis of Prompt</p>
<p>We further investigated GraphAdapter's stability with regard to prompts.We observed the performance of GraphAdapter based on different prompts.As shown in Tables 12, 13, and ).Our primary advantage is independence from   , and    can be accelerated by many methods.Considering larger language models where   ¬ª    , our approach holds a significant advantage.In terms of space complexity, our approach doesn't demand loading language model parameters during training, resulting in  (‚Ñé √ó   ) for Graph compared to  (‚Ñé √ó    ) for methods involving fine-tuning.Generally,   ¬ª    , allowing GraphAdapter to accommodate larger batch sizes in memory-restricted GPU environments.We also report efficiency comparisons for reproducibility purposes in Table 11.</p>
<p>B.4 Case Studies</p>
<p>We presented three cases in Table 15, considering different scenarios: 1. Node text acting as a distractor, 2. Graph features acting as distractors, 3. Neither the graph nor the text providing a strong signal.We denote these situations as Case A, Case B, and Case C, respectively.As shown in the cases, GraphAdapter without pretraining is effective when the text acts as a distractor (Case A, correct), but it struggles when the graph features are distracting (Case B, incorrect) due to over-reliance on graph information.After pretraining, GraphAdapter can more flexibly combine graph structural features and text features, enabling it to make judgments based on either graph or text information.Moreover, it can even make correct judgments on some very rare samples where neither the graph nor the text exhibits strong features.This indicates that pretrained GraphAdapter effectively utilizes the potential correlations between these two types of information.</p>
<p>1 . 1 . 2 .Figure 1 :
1121
Figure 1: An example of the correlation existing in the structural and textual data of nodes in social networks.</p>
<p>Framework:</p>
<p>The framework of GraphAdapter is shown in Figure2 (a).We also show how to fine-tune GraphAdapter on the downstream tasks in Figure2(b), we detail this part in Section 4.3.Given the textual data and graph structural data of a node, during the pre-training process, Step 1. GNN models the node structure information; Step 2. integrates the structural information with the corresponding context hidden-states modeled by PLM; andStep 3. predicts the next token.During this pre-training process, GraphAdapter can learn rich information.Align GNN with the language model.During the learning process, the node representation obtained by GNN is constantly combined with different representations modeled by the language model for reasoning, and the entire process naturally aligns these two.Enhance GNN in modeling graph structure.During the entire pre-training stage, the semantic information in the textual data supervises the GNN to model the graph structural information.Better understanding the semantics in TAG.GraphAapter can learn how to combine LLM and GNN to model the semantic information on TAG.</p>
<p>Figure 2 :
2
Figure 2: Framework of GraphAdapter.In the pre-training stage, Step 1. GNN models the node structure information, Step 2. integrates the structural information with the corresponding text fragment encoded by LM, and Step 3. predicts the masked token.</p>
<p>Figure 3 :
3
Figure 3: The performance of GraphAdapter before and after pre-training, using MLP and GNN as the backbone architectures.The red represents performance without pre-training, while the blue represents performance after pre-training.</p>
<p>Table 1 :
1
The performance of different methods across three datasets.Each row corresponds to a specific method, and each column presents the performance of the models on a particular dataset.The evaluation metric used is accuracy for the Arxiv and Reddit datasets, and ROC-AUC for Instagram.The LM employed in each method is indicated in parentheses.
ArxivInstagramReddit</p>
<p>Table 2 :
2
The performance of the GraphAdapter based on different LM across three datasets.The evaluation metrics used for these datasets align with those outlined in Table1.Experiment results show Llama 2 has improved performance on 3 datasets by 1.34% compared to RoBERTa-based methods.LLM can better combine the information in prompts to extract task-relevant sentence representations of nodes.As the results show, prompts can bring a 0.42% improvement on average for LLM, but they could not improve the performance of LM.Frozen LLMs with prompt can surpass many GNN-LM methods that require tuning LM.Results also show that LLMs with prompts can surpass GLEM and GIANT by 0.43% and 0.79% on average, respectively.(2)Directly fusing GNN and LLM results in unstable improvements.Compared to ordinary GNN, GraphAdapter (w/o Pre) only adds one fusion component to fuse the semantic representation from the LM and structural representation from the GNN.Experiment results show that directly fusing language model representations only brings improvements on Arxiv, but not obviously on other datasets.Note that the Arxiv training samples are much larger than the other datasets.This result suggests that training samples may have an impact on GNNs to understand and effectively incorporate the representations inferred by LLMs with prompts.
ArxivInstagramRedditRoBERTaGPT2Llama 2RoBERTaGPT-2Llama 2RoBERTaGPT-2Llama 2GNN (PLM)0.7129 (0.0013)0.7174 (0.0019)0.7305 (0.0022)0.6123 (0.0063)0.6019 (0.0124)0.6221 (0.0112)0.6191(0.0043)0.6282 (0.0036)0.6320 (0.0041)GNN (PLM+Prop)0.7067 (0.0011)0.6915 (0.0021)0.7336 (0.0027)0.6138 (0.0117)0.6128 (0.0014)0.6312 (0.0051)0.6198 (0.0036)0.6206 (0.0011)0.6324 (0.0033)GraphAdapter (w/o Pre) 0.7069 (0.0026)0.7146 (0.0025)0.7648 (0.0020)0.6165 (0.0038)0.6162 (0.0066)0.6351 (0.0077)0.6210 (0.0036)0.6284 (0.0027)0.6369 (0.0025)GraphAdapter0.7273 (0.0021) 0.7325 (0.0022) 0.7707 (0.0015) 0.6292 (0.0033) 0.6276 (0.0034) 0.6508 (0.0033) 0.6379 (0.0061) 0.6441 (0.0022) 0.6461 (0.0019)</p>
<p>Table 3 :
3
The performance of different methods using the same LMs across three datasets.The evaluation metrics employed for these datasets align with those described in Table1.
ArxivInstagramRedditGNN (BERT)0.7039 (0.0013)0.5973 (0.0063)0.6061 (0.0043)GIANT (BERT)0.7269 (0.0021) 0.5986 (0.0022) 0.6379 (0.0045)GraphAdapter (BERT)0.7264 (0.0012) 0.6156 (0.0032) 0.6366 (0.0034)GNN (RoBERTa)0.7129 (0.0013)0.6123 (0.0063)0.6191 (0.0043)GLEM (RoBERTa)0.7308 (0.0029) 0.6114 (0.0075)0.6228 (0.0018)GraphAdapter (RoBERTa) 0.7273 (0.0021) 0.6276 (0.0034) 0.6379 (0.0061)</p>
<p>Table 4 :
4
The performance of GraphAdapter when various components are removed.The evaluation metrics used for these tests align with those described above.The term 'w/o' indicates removing a specific component from the GraphAdapter.
ArxivInstagramRedditw/o Pretraining0.7648 (0.0020) 0.6392 (0.0086) 0.6369 (0.0025)w/o Graph structure0.7604 (0.0024) 0.6346 (0.0074) 0.6147 (0.0012)w/o Res label0.7605 (0.0013) 0.6408 (0.0130) 0.6363 (0.0036)w/o task-specific prompt 0.7594 (0.0030) 0.6364 (0.0073) 0.6430 (0.0021)GraphAdapter0.7707 (0.0015) 0.6513 (0.0075) 0.6461 (0.0019)</p>
<p>Table 5 :
5
The performance changes of the GNN block in GraphAdapter before and after pre-training.Here, "w/o Pre-
training" signifies no pre-training, while "w Pretraining" indicatesthe opposite.ArxivInstagramRedditGNN w/o Pretraining 0.7305 (0.0020)0.6181 (0.0112)0.6320 (0.0041)GNN w Pretraining0.7335 (0.0024) 0.6294 (0.0038) 0.6410 (0.0027)</p>
<p>Table 6 :
6
The performance of GraphAdapter after randomly initializing some blocks.Here, "Re-init" represents re-GraphAdapter is reasonable and effective, and further supports the motivation of this paper.Furthermore, we investigate the performance of GraphAdapter with different GNN blocks (see Appendix B.2) and conduct more detailed ablation studies (see Appendix B.1).Additionally, we analyze and report the efficiency of GraphAdapter (see Appendix B.3) to answer the Q5.Moreover, we conduct a case study on Arxiv to further demonstrate the advantages of the proposed method (see Appendix B.4).
initialization.ArxivInstagramRedditRe-init All0.7648 (0.0020) 0.6392 (0.0086) 0.6369 (0.0025)Re-init GNN0.7680 (0.0022) 0.6390 (0.0050) 0.6364 (0.0026)Re-init Fusion 0.7562 (0.0011) 0.6431 (0.0024) 0.6378 (0.0022)GraphAdapter 0.7707 (0.0015) 0.6513 (0.0075) 0.6461 (0.0019)pre-training of</p>
<p>Table 7 :
7
Statistics of experiment datasets.
Dataset# Nodes # Edges# Tokens Split ratio (%) #ClassMetricArxiv169,343 1,166,243 35,920,71054/18/2840AccuracyInstagram11,339144,010579,26310/10/802ROC-AUCReddit33,434198,4486,748,43610/10/802Accuracy</p>
<p>Table 9 :
9
Performance of GraphAdapter with various GNN blocks.Here we fix LM as Llama 2 13B.
ArxivInstagramRedditGNN BlockGAT<em>SAGEGAT</em>SAGEGAT*SAGEOnly GNN0.7534 (0.0016)0.7305 (0.0020)0.6292 (0.0055)0.6221 (0.0112)0.6495 (0.0031)0.6320 (0.0041)GraphAdapter(w/o pre) 0.7621 (0.0012)0.7648 (0.0020)0.6490 (0.0045)0.6351 (0.0077)0.6505 (0.0070)0.6369 (0.0025)GraphAdapter0.7663 (0.0016) 0.7707 (0.0015) 0.6545 (0.0034) 0.6513 (0.0075) 0.6694 (0.0041) 0.6461 (0.0019)</p>
<p>Table 10
10: Results of additional ablation studies onGraphAdapter. "r Fusion" indicates replacing the Fu-sion component with sum-pooling, while "o" means usingonly a specific component.ArxivInstagramRedditr Fusion0.7698 (0.0024) 0.6450 (0.0080) 0.6361 (0.0028)o GNN0.7335 (0.0024) 0.6294 (0.0038) 0.6410 (0.0027)o LLM+Prompt 0.7618 (0.0019) 0.6346 (0.0044) 0.6092 (0.0026)GraphAdapter 0.7707 (0.0015) 0.6513 (0.0075) 0.6461 (0.0019)</p>
<p>Table 11 :
11
Running time of different methods on Arxiv using one Nvidia A800 80GB.Since different methods use different PLM, we also report the number of parameters for the PLM (decoded as "# para") and the number of trainable parameters ("# trainable").
GIANTGLEMGraphAdapterPLMBERTDeBERTa-LargeLlama 2-13B# para of PLM110M139M13B# trainable in Pre110M-3M# trainable in Fine0.7M139M2MPre-process--192 minPre-training341 min-312 minFine-tuning1 min612 min1 minTotal time costs342 min612 min505 min</p>
<p>Comparing time complexities of different methods is challenging due to varying compatible base language models.Therefore, we estimate time complexities as follows: Inference time/space complexity for a single node for the language model is    and    , and training time/space complexity is   and   .Complexity for non-linear transformations of PLM representations is   and   .GIANT's complexity is equivalent to the time required for fine-tuning the PLM, i.e.,  ( √ó   ) for training and  ( √ó    ) for inference.GLEM has a similar complexity to GIANT.Our approach involves a single inference pass of PLMs, after which all operations are independent of PLMs.GraphAdapter only uses the processed representation to train GNN, resulting in  (|  | *    ) complexity.|  | is the total number of training tokens in the TAGs.Hence, our total complexity is  ( √ó    + |  | √ó <br />
B.3 Efficient14, task-related prompts significantly enhance the capability of large lan-guage models (LLMs) to handle text-as-graphs (TAGs). ComparingGraphAdapter (w/o pre) with LLMs+MLPs, it is evident that graphinformation is beneficial for prompts in most cases. Simultaneously,GraphAdapter achieves additional improvements on top of prompts,demonstrating that the pretraining of GraphAdapter indeed facili-tates the integration of graph information and prompts. Therefore,GraphAdapter stands out as a stable method suitable for differentprompts</p>
<p>Table 12 :
12
Performance of different models with different prompts on Arxiv.Here the adopted LM is Llama 2-13B.LLM+MLP GraphAdapter (w/o Pre) GraphAdapter "Question: Based on the abstract above, this paper is published on ___ subject on Arxiv.
Answer:"0.7541 (0.0024)0.7648 (0.0020)0.7707 (0.0015)"Question: Please predict the subject it belongs to based on the abstract of this paper, please answer directly. Answer: "0.7522 (0.0021)0.7657 (0.0019)0.7719 (0.0025)"Question: This paper is __. Answer: "0.7381 (0.0021)0.7554 (0.0026)0.7581 (0.0028)"Question: I like __ apple. Answer: "0.7314 (0.0010)0.7478 (0.0017)0.7559 (0.0017)None0.7335 (0.0030)0.7561 (0.0014)0.7607 (0.0039)</p>
<p>Table 13 :
13
Performance of different models with different prompts on Instagram.Here the adopted LM is Llama 2-13B.
LLM+MLP GraphAdapter (w/o Pre) GraphAdapter</p>
<p>Table 14 :
14
Performance of different models with different prompts on Reddit.Here the adopted LM is Llama 2-13B.Based on the given posts, the style of this user is ___ (answer in one word).
Answer:"0.6123 (0.0034)0.6369 (0.0025)0.6461 (0.0019)"Based on the given posts, please answer the popularity of this user. Answer: "0.6019 (0.0021)0.6324 (0.0033)0.6380 (0.0031)"Question: This user is __. Answer: "0.6117 (0.0032)0.6377 (0.0022)0.6446 (0.0021)"Question: This user likes __ apple. Answer: "0.6103 (0.0055)0.6359 (0.0044)0.6413 (0.0021)None0.6201 (0.0020)0.6354 (0.0014)0.6420 (0.0024)
LLM+MLP GraphAdapter (w/o Pre) GraphAdapter "Question:</p>
<p>Table 15 :
15
Three cases from the Ogbn-Arxiv dataset.LLM + MLP only utilizes abstract to predict paper's subjection, and make a wrong prediction on Case A and Case B. GraphAdapter (w/o Pre) can utilize both graph information and textual data, but also make a wrong prediction on Case B and Case C.After pretraining, GraphAdapter can make an accurate prediction on all cases.</p>
<p>https://convokit.cornell.edu/documentation/subreddit.html
https://developers.facebook.com/docs/graph-api
https://convokit.cornell.edu/documentation/subreddit.html differences. However, how to find suitable prompts is not the focus of this paper, so no search for prompts is conducted.
ACKNOWLEDGMENTSThis work is supported by Natural Science Foundation of China (No.62322606).Most baselines rely on the sentence representations obtained from the LM.For instance, GNN+LM uses the sentence representation as a node feature.In baselines utilizing BERT or RoBERTa, we append the same prompts used in the original text to obtain prompt-aware sentence representations.When employing Llama 2, we use the same prompt and utilize the last token as the sentence representation.A.4 Implementation DetailsWe independently pre-trained GraphAdapter on three datasets.The GNN used in the pre-training process was a 2-layer GraphSAGE, and the fusion layer used a 2-layer MLP.The pre-training was conducted for 50 rounds, and we used language model techniques such as silt activation function, layer-norm, and warm-up.The hidden side of GNN in GraphAdapter is set to 128, 64, and 128 on Arxiv, Instagram, and Reddit specifically.When using BERT or RoBERTa with GraphAdapter, there are some modifications to the GraphAdapter pipeline.Since these language models (LMs) utilize a mask-prediction task, we modify the input of S , ‚àí1 = {, 0... , ‚àí1 , [],  ,+1 ...} in Equation9. Additionally, unlike based auto-regressive models, which use all tokens in pretraining, GraphAdapter based on BERT and RoBERTa only mask 20% of tokens and pre-trained by their corresponding labels.B EXPERIMENT RESULT B.1 Ablation StudiesWe also conduct experiments that isolate and specifically compare the contributions of the base model and the fusion of the graphlanguage model, aiming to enhance the robustness of GraphAdapter.As shown in Table10, the ranking of contributions from GNNs and LLMs varies across datasets.However, fusing GNNs and LLMs can achieve better performance in most cases.GraphAdapter not only efficiently combines GNNs and LLMs but also enhances their performance through next-token prediction pre-training.Additionally, we investigate the effect of different GNNs with the same LM.In this experiment, we fix the LM as Llama 2 13B and compare the performance of GraphAdapter with different GNN blocks, both in pre-training and fine-tuning.We discovered that the original attention mechanism in Graph Attention Networks (GAT) is not effective for the pre-training of GraphAdapter.However, we found that a dot-product-based mechanism[3]
The theory of graphs. Claude Berge, Courier Corporation. 2001</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, NeurIPS'20. 202033</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, arXivpreprintarXiv:2307.033932023</p>
<p>Node feature extraction by self-supervised multi-scale neighborhood prediction. Eli Chien, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-Fu Yu, Jiong Zhang, Olgica Milenkovic, Inderjit S Dhillon, arXivpreprintarXiv:2111.000642021</p>
<p>Text and structural data mining of influenza mentions in web and social media. Diane J Courtney D Corley, Armin R Cook, Karan P Mikler, Singh, In International journal of environmental research and public health. 72010</p>
<p>Bert: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXivpreprintarXiv:1810.048052018</p>
<p>Simteg: A frustratingly simple approach improves textual graph learning. Qian Keyu Duan, Tat-Seng Liu, Shuicheng Chua, Wei Tsang Yan, Qizhe Ooi, Junxian Xie, He, arXivpreprintarXiv:2308.025652023</p>
<p>Simcse: simple contrastive learning of sentence embeddings. Tianyu Gao, Xingcheng Yao, Danqi Chen, arXivpreprintarXiv:2104.088212021</p>
<p>Predict then propagate: Graph neural networks meet personalized pagerank. Johannes Gasteiger, Aleksandar Bojchevski, Stephan G√ºnnemann, arXivpreprintarXiv:1810.059972018</p>
<p>GPT4Graph: can large language models understand graph structured Data? An Empirical Evaluation and Benchmarking. Jiayan Guo, Lun Du, Hengyu Liu, arXivpreprintarXiv:2305.150662023</p>
<p>Inductive representation learning on large graphs. Will Hamilton, Zhitao Ying, Jure Leskovec, NeurIPS'17. 201730</p>
<p>Deberta: decoding-enhanced bert with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, arXivpreprintarXiv:2006.036542020</p>
<p>Explanations as Features: LLM-Based Features for text-attributed graphs. Xiaoxin He, Xavier Bresson, Thomas Laurent, Bryan Hooi, arXivpreprintarXiv:2305.195232023</p>
<p>Lora: Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXivpreprintarXiv:2106.096852021</p>
<p>Open graph benchmark: datasets for machine learning on graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, NeurIPS'20. 202033</p>
<p>Promptbert: improving bert sentence embeddings with prompts. Ting Jiang, Jian Jiao, Shaohan Huang, Zihan Zhang, Deqing Wang, Fuzhen Zhuang, Furu Wei, Haizhen Huang, Denvy Deng, Qi Zhang, arXivpreprintarXiv:2201.043372022</p>
<p>Wentao Bowen Jin, Yu Zhang, Yu Zhang, Xinyang Meng, Qi Zhang, Jiawei Zhu, Han, arXivpreprintarXiv:2305.12268Patton: language model pretraining on text-Rich networks. 2023</p>
<p>Multimodal post attentive profiling for influencer marketing. Seungbae Kim, Jyun-Yu Jiang, Masaki Nakada, Jinyoung Han, Wei Wang, WWW'20. 2020</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, arXivpreprintarXiv:2104.086912021</p>
<p>Adsgnn: behavior-graph augmented relevance modeling in sponsored search. Chaozhuo Li, Bochen Pang, Yuming Liu, Hao Sun, Zheng Liu, Xing Xie, Tianqi Yang, Yanling Cui, Liangjie Zhang, Qi Zhang, SIGIR'21. 2021</p>
<p>Training graph neural networks with 1000 layers. Guohao Li, Matthias M√ºller, Bernard Ghanem, Vladlen Koltun, ICML'21. 2021</p>
<p>Prefix-tuning: optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, arXivpreprintarXiv:2101.001902021</p>
<p>P-tuning v2: prompt tuning can be comparable to fine-tuning universally across scales and tasks. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, Jie Tang, arXivpreprintarXiv:2110.076022021</p>
<p>Roberta: A robustly optimized bert pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXivpreprintarXiv:1907.116922019</p>
<p>Finegrained fact verification with kernel graph attention network. Zhenghao Liu, Chenyan Xiong, Maosong Sun, Zhiyuan Liu, arXivpreprintarXiv:1910.097962019</p>
<p>Train your own GNN teacher: graph-aware distillation on textual graphs. Costas Mavromatis, N Vassilis, Shen Ioannidis, Da Wang, Soji Zheng, Jun Adeshina, Han Ma, Christos Zhao, George Faloutsos, Karypis, arXivpreprintarXiv:2304.106682023</p>
<p>Random graph models of social networks. Duncan J Mark Ej Newman, Steven H Watts, Strogatz, Proceedings of the national academy of Sciences. the national academy of Sciences200299</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, OpenAI. 2018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI. 2019</p>
<p>Sentence-bert: sentence embeddings using siamese bert-networks. Nils Reimers, Iryna Gurevych, arXivpreprintarXiv:1908.100842019</p>
<p>Ernie 2.0: a continual pre-training framework for language understanding. Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hua Hao Tian, Haifeng Wu, Wang, AAAI'20. 202034</p>
<p>Graph neural prompting with large language models. Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang, Nitesh V Chawla, Panpan Xu, arXivpreprintarXiv:2309.154272023</p>
<p>Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, arXivpreprintarXiv:2307.09288</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, Illia Polosukhin, NeurIPS'17. 201730</p>
<p>Graph attention networks. Petar Veliƒçkoviƒá, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, arXivpreprintarXiv:1710.109032017</p>
<p>How powerful are graph neural networks?. Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, arXiv:1810.008262018In arXiv preprint</p>
<p>GraphFormers: GNN-nested transformers for representation learning on textual graph. Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu Lian, Sanjay Agrawal, Amit Singh, Guangzhong Sun, Xing Xie, NeurIPS'21. 202134</p>
<p>Bert-enhanced text graph neural network for classification. Yiping Yang, Xiaohui Cui, In Entropy. 232021</p>
<p>Graph convolutional neural networks for web-scale recommender systems. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, Jure Leskovec, KDD'18. 2018</p>
<p>Evaluating generative models for graph-to-text generation. Shuzhou Yuan, Michael F√§rber, arXivpreprintarXiv:2307.147122023</p>
<p>Glm-130b: an open bilingual pre-trained model. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, arXivpreprintarXiv:2210.024142022</p>
<p>Learning on Large-scale text-attributed graphs via variational inference. Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, Jian Tang, arXivpreprintarXiv:2210.147092022</p>
<p>Semantic understanding of scenes through the ade20k dataset. Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, Antonio Torralba, IJCV'19. 2019127</p>            </div>
        </div>

    </div>
</body>
</html>
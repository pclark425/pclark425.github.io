<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8604 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8604</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8604</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-263605725</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.00836v1.pdf" target="_blank">Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Logical reasoning is fundamental for humans yet presents a substantial challenge in the domain of Artificial Intelligence. Initially, researchers used Knowledge Representation and Reasoning (KR) systems that did not scale and required non trivial manual effort. Recently, the emergence of large language models (LLMs) has demonstrated the ability to overcome various limitations of formal Knowledge Representation (KR) systems. Consequently, there is a growing interest in using LLMs for logical reasoning via natural language. This work strives to understand the proficiency of LLMs in logical reasoning by offering a brief review of the latest progress in this area; with a focus on the logical reasoning datasets, tasks, and the methods adopted to utilize LLMs for reasoning. To offer a thorough analysis, we have compiled a benchmark titled LogiGLUE. This includes 24 varied datasets encompassing deductive, abductive, and inductive reasoning. We have standardized these datasets into Seq2Seq tasks to facilitate straightforward training and evaluation for future research. Utilizing LogiGLUE as a foundation, we have trained an instruction fine tuned language model, resulting in LogiT5. We study single task training, multi task training, and a chain of thought knowledge distillation fine tuning technique to assess the performance of model across the different logical reasoning categories. By this comprehensive process, we aim to shed light on the capabilities and potential pathways for enhancing logical reasoning proficiency in LLMs, paving the way for more advanced and nuanced developments in this critical field.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8604.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8604.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-finetuned seq2seq T5 variant used as the base model in this paper for evaluating logical reasoning across the LogiGLUE benchmark via single-task, multi-task, and chain-of-thought distillation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-finetuned T5 variant (Flan-T5) used as the base seq2seq model for experiments in this paper; selected for improved instruction-following and tractable training in an academic setting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiGLUE (24 logical-reasoning datasets; examples: LogiQA, ProofWriter, αNLI, ANLI, PrOntoQA, bAbi, Rulebert-Union)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A standardized benchmark (LogiGLUE) covering 24 datasets across deductive, abductive, and inductive logical reasoning, presented in a unified seq2seq format (MCQA, NLI, fact verification, free-form QA) to evaluate strict logical reasoning over natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Single-task fine-tuning on individual datasets; multi-task fine-tuning on in-domain LogiGLUE with weighted sampling; chain-of-thought (CoT) distillation (teacher Llama-7B -> student Flan-T5); evaluation with and without CoT prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average across in-domain tasks: 38.63 (Table 2). Example dataset scores from Table 2: LogiQA 52.66, PrOntoQA 6.50, bAbi (induction) 59.44, Rulebert-Union 27.29. CoT distillation: on LogiQA, training with 15K CoT-distilled samples improved performance by ~4% relative to the non-CoT baseline (Table 4); smaller CoT datasets (3K, 6K) did not improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to multi-task LogiT5, single-task Flan-T5 generally underperformed on low-resource datasets; Flan-T5 served as the student in CoT distillation experiments where teacher Llama-7B-generated CoTs were used — student improved only with larger distilled CoT data (15K).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Poor average performance on many handcrafted/real datasets (esp. PrOntoQA); strong sensitivity to dataset creation style (synthetic datasets much easier); CoT distillation required large amounts of distilled data to benefit; single-task fine-tuning shows limited generalization (IID success but poor OOD); proof-generation is harder than answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Instruction-finetuned seq2seq models like Flan-T5 can be improved for strict logical tasks via multi-task training and large-scale CoT distillation, but gains depend on data volume and dataset type; models still do much better on synthetic rule-based data than on human-crafted tests, and proof-generation / out-of-domain generalization remain open challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8604.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8604.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogiT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogiT5 (multi-task finetuned Flan-T5 on LogiGLUE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Flan-T5-large model multi-task fine-tuned on the assembled LogiGLUE benchmark using a weighted sampling strategy, intended to improve logical reasoning generalization across diverse logical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogiT5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multi-task fine-tuned variant of Flan-T5-large trained on the in-domain portion of LogiGLUE with weighted sampling to address dataset size imbalance; also used as a base for further single-dataset fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiGLUE (in-domain subset used for training; evaluated on same benchmark and out-of-domain datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same LogiGLUE benchmark covering multiple strict logical reasoning datasets (deductive, abductive, inductive) converted into seq2seq format for unified training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Multi-task fine-tuning across the in-domain LogiGLUE datasets with weighted sampling; subsequent experiments included fine-tuning LogiT5 further on single datasets (no large benefits observed).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average across in-domain tasks: 54.50 (Table 2). Notable improvements over single-task fine-tuning on low-resource tasks: +~5% on αARCT and +~8% on FOLIO (reported in text). Example dataset scores from Table 2: LogiQA 55.00, PrOntoQA 56.50, bAbi (deduction) 16.74.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperformed single-task Flan-T5 on several small-data tasks; did not improve or sometimes matched performance on large-data tasks (e.g., αNLI, ANLI) where single-task training was already strong. Further fine-tuning of LogiT5 on single datasets yielded only marginal gains, suggesting LogiT5 had already captured most task knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Multi-task training provided limited/no benefit for tasks with very large training sets; overall generalization to out-of-domain datasets remains challenging despite multi-task training; small or handcrafted human datasets still cause low absolute performance on several tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Multi-task instruction-finetuning on a diverse logical benchmark substantially boosts performance on low-resource logical tasks and yields a strong unified model (LogiT5), but does not fully solve generalization or the challenges posed by handcrafted human datasets and proof generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8604.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8604.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open foundation LLM (7B) evaluated in zero-shot setting and with chain-of-thought prompting; also used as the teacher model to generate CoT explanations for distillation into Flan-T5.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open foundation LLaMA-2 family model (7B parameters) used in zero-shot evaluation and for chain-of-thought (CoT) prompting and as a CoT teacher to generate reasoning traces for student fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiGLUE (out-of-domain evaluation and teacher for CoT distillation); individual named datasets reported (e.g., LogiQA, PrOntoQA, bAbi)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Evaluated zero-shot on logical reasoning datasets from LogiGLUE; CoT prompting appended the phrase "let's think step by step" to questions in zero-shot evaluation and was used to generate CoT explanations for distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot evaluation; chain-of-thought prompting (append 'let's think step by step'); generation of multiple CoT-answer candidates used to create distilled training sets (3K, 6K, 15K) for student fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 2 reports LLAMA average 43.63 and LLAMA-CoT average 37.78 across datasets; example scores: PrOntoQA 70.00 (LLAMA), LogiQA ~50.74 (LLAMA), bAbi (deduction) 58.76 (LLAMA). CoT prompting did not consistently improve zero-shot performance and sometimes degraded average (LLAMA-CoT average lower than LLAMA).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to Flan-T5 and LogiT5: Llama-2 (7B) had mixed performance — stronger on some datasets (e.g., PrOntoQA) but overall lower average than LogiT5; CoT prompting did not clearly improve zero-shot answers relative to plain zero-shot Llama-2 in these evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>LLama-2 outputs often free-form and not aligned with structured answer options, complicating automatic evaluation; on bAbi the model sometimes ignored the provided context and answered from pretrained knowledge; CoT prompting offered no clear advantage in zero-shot here; teacher-generated CoTs contained multiple possible reasoning paths and occasional incorrect/missing planning leading to imperfect distilled data.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Llama-2 (7B) can produce CoTs usable for distillation but zero-shot CoT prompting did not reliably improve performance on the evaluated logical tasks; teacher-generated CoTs can help a smaller student only when distilled at sufficient scale (15K examples) and with care to filter correct-answer cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8604.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8604.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLama-CoT distillation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) knowledge distillation from Llama-7B to Flan-T5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CoT distillation procedure where Llama-7B generates step-by-step rationales and answers; correct (or partially-correct) instances are filtered to form datasets (3K, 6K, 15K) used to fine-tune Flan-T5 (student) to improve reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoT distillation (teacher: Llama-7B -> student: Flan-T5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Procedure: prompt Llama-7B to generate chain-of-thought explanations and answers for LogiQA; retain samples where final answer is correct (3K) or at least one of multiple attempts is correct (6K) or use all paths (15K); fine-tune Flan-T5 on these CoT-augmented examples.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>teacher: 7B; student: Flan-T5-large (size unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiQA (used as the target challenging task for CoT distillation experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>LogiQA is a machine reading comprehension / MCQA dataset requiring multi-step logical reasoning (mixed logical types) coming from human exam-style questions.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>CoT generation by teacher Llama-7B; filtering of generated CoTs by correctness; creation of distilled training sets (3K, 6K, 15K); fine-tuning Flan-T5 on the distilled CoT data (longer training, larger initial LR found helpful).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>CoT distillation results (on LogiQA): 3K and 6K distilled samples did not improve student performance; 15K distilled samples produced ~4% absolute improvement over the non-CoT baseline (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to Flan-T5 trained without CoT-distilled data, only large-scale distilled data (15K) yielded measurable gains; smaller distilled sets (3K, 6K) were ineffective.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Teacher CoTs sometimes include incorrect or incomplete planning steps; selecting/filtering teacher outputs reduced available training data; distillation required substantial distilled-data volume to benefit student; CoT fine-tuning requires longer training and tuning (larger initial LR helpful).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>CoT distillation can improve student models on difficult logical tasks, but effectiveness scales with the amount and quality of distilled CoT data; teacher-generated rationales must be filtered for correctness and are not guaranteed to transfer improved reasoning unless produced at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Transformers as soft reasoners over language <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>Selection-inference: Exploiting large language models for interpretable logical reasoning <em>(Rating: 2)</em></li>
                <li>APOLLO: A simple approach for adaptive pretraining of language models for logical reasoning <em>(Rating: 1)</em></li>
                <li>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 1)</em></li>
                <li>Evaluating the logical reasoning ability of chatgpt and gpt-4 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8604",
    "paper_id": "paper-263605725",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "Flan-T5-large",
            "name_full": "Flan-T5 (large)",
            "brief_description": "An instruction-finetuned seq2seq T5 variant used as the base model in this paper for evaluating logical reasoning across the LogiGLUE benchmark via single-task, multi-task, and chain-of-thought distillation experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Flan-T5-large",
            "model_description": "Instruction-finetuned T5 variant (Flan-T5) used as the base seq2seq model for experiments in this paper; selected for improved instruction-following and tractable training in an academic setting.",
            "model_size": null,
            "reasoning_task_name": "LogiGLUE (24 logical-reasoning datasets; examples: LogiQA, ProofWriter, αNLI, ANLI, PrOntoQA, bAbi, Rulebert-Union)",
            "reasoning_task_description": "A standardized benchmark (LogiGLUE) covering 24 datasets across deductive, abductive, and inductive logical reasoning, presented in a unified seq2seq format (MCQA, NLI, fact verification, free-form QA) to evaluate strict logical reasoning over natural language.",
            "method_or_approach": "Single-task fine-tuning on individual datasets; multi-task fine-tuning on in-domain LogiGLUE with weighted sampling; chain-of-thought (CoT) distillation (teacher Llama-7B -&gt; student Flan-T5); evaluation with and without CoT prompts.",
            "performance": "Average across in-domain tasks: 38.63 (Table 2). Example dataset scores from Table 2: LogiQA 52.66, PrOntoQA 6.50, bAbi (induction) 59.44, Rulebert-Union 27.29. CoT distillation: on LogiQA, training with 15K CoT-distilled samples improved performance by ~4% relative to the non-CoT baseline (Table 4); smaller CoT datasets (3K, 6K) did not improve performance.",
            "baseline_comparison": "Compared to multi-task LogiT5, single-task Flan-T5 generally underperformed on low-resource datasets; Flan-T5 served as the student in CoT distillation experiments where teacher Llama-7B-generated CoTs were used — student improved only with larger distilled CoT data (15K).",
            "limitations_or_failures": "Poor average performance on many handcrafted/real datasets (esp. PrOntoQA); strong sensitivity to dataset creation style (synthetic datasets much easier); CoT distillation required large amounts of distilled data to benefit; single-task fine-tuning shows limited generalization (IID success but poor OOD); proof-generation is harder than answer generation.",
            "insights_or_conclusions": "Instruction-finetuned seq2seq models like Flan-T5 can be improved for strict logical tasks via multi-task training and large-scale CoT distillation, but gains depend on data volume and dataset type; models still do much better on synthetic rule-based data than on human-crafted tests, and proof-generation / out-of-domain generalization remain open challenges.",
            "uuid": "e8604.0",
            "source_info": {
                "paper_title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LogiT5",
            "name_full": "LogiT5 (multi-task finetuned Flan-T5 on LogiGLUE)",
            "brief_description": "A Flan-T5-large model multi-task fine-tuned on the assembled LogiGLUE benchmark using a weighted sampling strategy, intended to improve logical reasoning generalization across diverse logical tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LogiT5",
            "model_description": "Multi-task fine-tuned variant of Flan-T5-large trained on the in-domain portion of LogiGLUE with weighted sampling to address dataset size imbalance; also used as a base for further single-dataset fine-tuning.",
            "model_size": null,
            "reasoning_task_name": "LogiGLUE (in-domain subset used for training; evaluated on same benchmark and out-of-domain datasets)",
            "reasoning_task_description": "Same LogiGLUE benchmark covering multiple strict logical reasoning datasets (deductive, abductive, inductive) converted into seq2seq format for unified training and evaluation.",
            "method_or_approach": "Multi-task fine-tuning across the in-domain LogiGLUE datasets with weighted sampling; subsequent experiments included fine-tuning LogiT5 further on single datasets (no large benefits observed).",
            "performance": "Average across in-domain tasks: 54.50 (Table 2). Notable improvements over single-task fine-tuning on low-resource tasks: +~5% on αARCT and +~8% on FOLIO (reported in text). Example dataset scores from Table 2: LogiQA 55.00, PrOntoQA 56.50, bAbi (deduction) 16.74.",
            "baseline_comparison": "Outperformed single-task Flan-T5 on several small-data tasks; did not improve or sometimes matched performance on large-data tasks (e.g., αNLI, ANLI) where single-task training was already strong. Further fine-tuning of LogiT5 on single datasets yielded only marginal gains, suggesting LogiT5 had already captured most task knowledge.",
            "limitations_or_failures": "Multi-task training provided limited/no benefit for tasks with very large training sets; overall generalization to out-of-domain datasets remains challenging despite multi-task training; small or handcrafted human datasets still cause low absolute performance on several tasks.",
            "insights_or_conclusions": "Multi-task instruction-finetuning on a diverse logical benchmark substantially boosts performance on low-resource logical tasks and yields a strong unified model (LogiT5), but does not fully solve generalization or the challenges posed by handcrafted human datasets and proof generation.",
            "uuid": "e8604.1",
            "source_info": {
                "paper_title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Llama-2 (7B)",
            "name_full": "Llama-2 (7B)",
            "brief_description": "An open foundation LLM (7B) evaluated in zero-shot setting and with chain-of-thought prompting; also used as the teacher model to generate CoT explanations for distillation into Flan-T5.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2 (7B)",
            "model_description": "Open foundation LLaMA-2 family model (7B parameters) used in zero-shot evaluation and for chain-of-thought (CoT) prompting and as a CoT teacher to generate reasoning traces for student fine-tuning.",
            "model_size": "7B",
            "reasoning_task_name": "LogiGLUE (out-of-domain evaluation and teacher for CoT distillation); individual named datasets reported (e.g., LogiQA, PrOntoQA, bAbi)",
            "reasoning_task_description": "Evaluated zero-shot on logical reasoning datasets from LogiGLUE; CoT prompting appended the phrase \"let's think step by step\" to questions in zero-shot evaluation and was used to generate CoT explanations for distillation.",
            "method_or_approach": "Zero-shot evaluation; chain-of-thought prompting (append 'let's think step by step'); generation of multiple CoT-answer candidates used to create distilled training sets (3K, 6K, 15K) for student fine-tuning.",
            "performance": "Table 2 reports LLAMA average 43.63 and LLAMA-CoT average 37.78 across datasets; example scores: PrOntoQA 70.00 (LLAMA), LogiQA ~50.74 (LLAMA), bAbi (deduction) 58.76 (LLAMA). CoT prompting did not consistently improve zero-shot performance and sometimes degraded average (LLAMA-CoT average lower than LLAMA).",
            "baseline_comparison": "Compared to Flan-T5 and LogiT5: Llama-2 (7B) had mixed performance — stronger on some datasets (e.g., PrOntoQA) but overall lower average than LogiT5; CoT prompting did not clearly improve zero-shot answers relative to plain zero-shot Llama-2 in these evaluations.",
            "limitations_or_failures": "LLama-2 outputs often free-form and not aligned with structured answer options, complicating automatic evaluation; on bAbi the model sometimes ignored the provided context and answered from pretrained knowledge; CoT prompting offered no clear advantage in zero-shot here; teacher-generated CoTs contained multiple possible reasoning paths and occasional incorrect/missing planning leading to imperfect distilled data.",
            "insights_or_conclusions": "Llama-2 (7B) can produce CoTs usable for distillation but zero-shot CoT prompting did not reliably improve performance on the evaluated logical tasks; teacher-generated CoTs can help a smaller student only when distilled at sufficient scale (15K examples) and with care to filter correct-answer cases.",
            "uuid": "e8604.2",
            "source_info": {
                "paper_title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLama-CoT distillation",
            "name_full": "Chain-of-Thought (CoT) knowledge distillation from Llama-7B to Flan-T5",
            "brief_description": "A CoT distillation procedure where Llama-7B generates step-by-step rationales and answers; correct (or partially-correct) instances are filtered to form datasets (3K, 6K, 15K) used to fine-tune Flan-T5 (student) to improve reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CoT distillation (teacher: Llama-7B -&gt; student: Flan-T5)",
            "model_description": "Procedure: prompt Llama-7B to generate chain-of-thought explanations and answers for LogiQA; retain samples where final answer is correct (3K) or at least one of multiple attempts is correct (6K) or use all paths (15K); fine-tune Flan-T5 on these CoT-augmented examples.",
            "model_size": "teacher: 7B; student: Flan-T5-large (size unspecified)",
            "reasoning_task_name": "LogiQA (used as the target challenging task for CoT distillation experiments)",
            "reasoning_task_description": "LogiQA is a machine reading comprehension / MCQA dataset requiring multi-step logical reasoning (mixed logical types) coming from human exam-style questions.",
            "method_or_approach": "CoT generation by teacher Llama-7B; filtering of generated CoTs by correctness; creation of distilled training sets (3K, 6K, 15K); fine-tuning Flan-T5 on the distilled CoT data (longer training, larger initial LR found helpful).",
            "performance": "CoT distillation results (on LogiQA): 3K and 6K distilled samples did not improve student performance; 15K distilled samples produced ~4% absolute improvement over the non-CoT baseline (Table 4).",
            "baseline_comparison": "Compared to Flan-T5 trained without CoT-distilled data, only large-scale distilled data (15K) yielded measurable gains; smaller distilled sets (3K, 6K) were ineffective.",
            "limitations_or_failures": "Teacher CoTs sometimes include incorrect or incomplete planning steps; selecting/filtering teacher outputs reduced available training data; distillation required substantial distilled-data volume to benefit student; CoT fine-tuning requires longer training and tuning (larger initial LR helpful).",
            "insights_or_conclusions": "CoT distillation can improve student models on difficult logical tasks, but effectiveness scales with the amount and quality of distilled CoT data; teacher-generated rationales must be filtered for correctness and are not guaranteed to transfer improved reasoning unless produced at scale.",
            "uuid": "e8604.3",
            "source_info": {
                "paper_title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 2,
            "sanitized_title": "transformers_as_soft_reasoners_over_language"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "rating": 2,
            "sanitized_title": "selectioninference_exploiting_large_language_models_for_interpretable_logical_reasoning"
        },
        {
            "paper_title": "APOLLO: A simple approach for adaptive pretraining of language models for logical reasoning",
            "rating": 1,
            "sanitized_title": "apollo_a_simple_approach_for_adaptive_pretraining_of_language_models_for_logical_reasoning"
        },
        {
            "paper_title": "Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 1,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "Evaluating the logical reasoning ability of chatgpt and gpt-4",
            "rating": 1,
            "sanitized_title": "evaluating_the_logical_reasoning_ability_of_chatgpt_and_gpt4"
        }
    ],
    "cost": 0.013980749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models</p>
<p>Man Luo 
Arizona State University</p>
<p>Mayo Clinic</p>
<p>Shrinidhi Kumbhar skumbha4@asu.edu 
Arizona State University</p>
<p>Ming Shen 
Arizona State University</p>
<p>Mihir Parmar 
Arizona State University</p>
<p>Neeraj Varshney 
Arizona State University</p>
<p>Pratyay Banerjee 
Amazon Alexa AI</p>
<p>Somak Aditya 
IIT KGP</p>
<p>Chitta Baral 
Arizona State University</p>
<p>Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models
0028F5B29808AF7D6E1D462D7FDAFB68
Logical reasoning is fundamental for humans yet presents a substantial challenge in the domain of Artificial Intelligence.Initially, researchers used Knowledge Representation and Reasoning (KR) systems that did not scale and required non-trivial manual effort.Recently, the emergence of large language models (LLMs) has demonstrated the ability to overcome various limitations of formal Knowledge Representation (KR) systems.Consequently, there's a growing interest in using LLMs for logical reasoning via natural language.This work strives to understand the proficiency of LLMs in logical reasoning by offering a brief review of the latest progress in this area; with a focus on the logical reasoning datasets, tasks, and the methods adopted to utilize LLMs for reasoning.To offer a thorough analysis, we've compiled a benchmark titled LogiGLUE.This includes 24 varied datasets encompassing deductive, abductive, and inductive reasoning.We have standardized these datasets into Seq2Seq tasks to facilitate straightforward training and evaluation for future research.Utilizing LogiGLUE as a foundation, we have trained an instruction fine-tuned language model, resulting in LogiT5.We study single-task training, multi-task training, and a "chain-of-thought" knowledge distillation finetuning technique to assess the model's performance across the different logical reasoning categories.By this comprehensive process, we aim to shed light on the capabilities and potential pathways for enhancing logical reasoning proficiency in LLMs, paving the way for more advanced and nuanced developments in this critical field.</p>
<p>Introduction</p>
<p>With logical reasoning, humans can explain an answer to a question via step-wise deduction, make robust planning and decision, or even reason about 0 *Equal contribution the workings in an unseen universe.As an example of logical reasoning, physicist Stephen Hawking derived the area theorem in 1971, which indicates the boundary beyond which nothing can ever escape (black hole).If Hawking's area theorem holds, then using logical reasoning, we can conclude that the horizon area of the new black hole should not be smaller than the total horizon area of its parent black holes.This theorem was later confirmed by real observation by MIT researchers in 2015, fifty years later after the theorem had been derived1 .</p>
<p>In the field of Artificial Intelligence (AI), there has been significant attention directed towards the aspiration to develop machines equipped with logical reasoning capabilities (McCarthy, 1989;Colmerauer and Roussel, 1996).Early approaches in logical reasoning were primarily dedicated to the design of formal logic languages to encapsulate rules and knowledge, along with the development of automated theorem provers (Lifschitz, 2019).This paradigm, however, necessitated a deep understanding of the syntax and semantics of the formal logic for manual rule formulation -making knowledge representation and knowledge acquisition hard and expert-driven endeavor.Due to these challenges, contemporary research has progressively turned towards addressing logical reasoning tasks (Clark et al., 2020;Tian et al., 2021a;Han et al., 2022) by employing transformerbased (Vaswani et al., 2017) pre-trained language models (Devlin et al., 2019a;Brown et al., 2020).</p>
<p>The Language models (LMs) that are pretrained using objectives such as the mask language modeling (Devlin et al., 2019a) and next word prediction (Brown et al., 2020) enables them to acquire adequate syntax and semantics of language, alongside commonsense knowledge.These language models can excel in numerous natural language understanding tasks, owing to the unsupervised pretraining on a vast array of unstructured text data.However, it is unclear if the current pretraining objectives are sufficient enough for the models to infer logical reasoning because this involves understanding structure; coupled with inductive, deductive, and abductive reasoning skills.This question has drawn intense attention and inspired different research directions to examine if LMs can learn logical reasoning ability (Wu et al., 2023;Lanham et al., 2023;Clark et al., 2020;Joshi et al., 2020).For instance, Clark et al. (2020) shows that pre-trained language models can serve as a "soft-reasoner" based on their near-perfect performance on synthetic datasets.Creswell et al. (2022) showed that large LMs are few-shot logical reasoners.On the other hand, Liu et al. (2020); Joshi et al. (2020); Han et al. (2022) shows that logical reasoning remains challenging for language models.Furthermore, Wu et al. (2023); Lanham et al. (2023) showed that LLMs maybe retrieving or reciting previously seen facts and steps, instead of actually reasoning.Liu et al. (2023b) shows that while Chat-GPT and GPT-4 generally perform well on some benchmarks, their performance noticeably diminishes when faced with new or out-of-distribution datasets.</p>
<p>To better understand the progress of logical reasoning ability in the current language model era, we first provide a concise survey of its role within current language models.Based on the insights gathered through the survey, we assembled a logical reasoning benchmark termed as LogiGLUE.Subsequently, we trained a model on this benchmark by utilizing diverse training strategies; our contributions are summarized below.</p>
<p>Concise Survey.We provide a brief survey of the recent development of logical reasoning using natural language (see Figure 1).First we discuss three types of logical reasoning.Then we focus on the relevant benchmarks and the methodologies for applying LMs to logical reasoning tasks.</p>
<p>LogiGLUE.One result of this survey is a benchmark for logical reasoning (LogiGLUE), with the aim to facilitate a consistent progress of logical reasoning in NLP.The importance for the LogiGLUE benchmark arises from several critical considerations.First, it encompasses diverse logical reasoning tasks and generalization evaluation, ensuring a comprehensive assessment of how a model performs across varied logical paradigms.Second, the unique format of each dataset within LogiGLUE simplifies both training and evaluation processes, facilitating swift integration into research workflows.Lastly, researchers can easily compare with established baselines, and the LogiGLUE offers the flexibility to seamlessly integrate new datasets in the future, ensuring its lasting relevance in logical reasoning evaluation.</p>
<p>LogiT5.Drawing inspiration from recent successes in multi-task learning and instruction-finetuned models, we trained seq2seq models, specifically Flan-T5 (Chowdhery et al., 2022), using multi-task learning on LogiGLUE's in-domain data.The resulting model, named LogiT5, demonstrated effective generalization on out-of-domain data.</p>
<p>A Concise Survey of Logical Reasoning</p>
<p>in NLP: Types of Reasoning, Datasets and Language Models Approach</p>
<p>The advent of large language models has been transformative for the AI community; prompting many to speculate that we are on the cusp of achieving general artificial intelligence (GAI).Yet, as astounding as their capabilities are, these models grapple with numerous challenges, particularly with logical reasoning (Han et al., 2022;Valmeekam et al., 2023Valmeekam et al., , 2022;;Guan et al., 2023).</p>
<p>Recognizing the significance of this, our survey aims to provide a timely comprehensive overview of advancements in logical reasoning within the context of language models, elucidating their performance, limitations, and the obstacles that remain, which casts a vision for future research directions.While other surveys have touched upon the broader theme of logical reasoning using natural language (Helwe et al., 2022;Yu et al., 2023;Yang et al., 2023), our survey has led us to propose a comprehensive bnechmark collection, and include a systematic review of techniques to adopt LLMs for logical reasoning tasks.More importantly, we categorize different ways of using LMs on logical reasoning tasks, highlighting the intricacies and challenges faced by models for such tasks.</p>
<p>Three Types of Logical Reasoning</p>
<p>Deductive Reasoning.In this predominant form of reasoning, we start with a set of premises which can be facts or rules, and derive a specific conclusion based on the available premises with a valid logical derivation path.In short, deductive reasoning derives specific conclusion(s) from generic observation(s) (Byrne et al., 2019).There are two characteristics related to a deductive reasoning system, validity and soundness.A conclusion is valid if and only if it is fully supported by the premises irrespective of the factuality of the premises.A conclusion is sound if and only if it is valid and the premises are true.For example in Figure 2, the conclusion is valid but it is not sound because it is not true that "All kids love animals."Most of the synthetic deductive reasoning datasets such as RuleTaker (Clark et al., 2020) has valid conclusions, but may not be sound as the rules in the premises are often synthetically generated and may be untrue in the real world.Datasets such as PrOn-toQA (Saparov and He, 2023a) offer a broader view, by sourcing the premise rules from a true, a false and a fictional ontology.</p>
<p>Inductive Reasoning.For inductive reasoning, one starts with a set of observations, and derives a general conclusion that is merely true, but not certain (Heit, 2007;Sauce and Matzel, 2017).In contrast to deductive reasoning, inductive reasoning is a bottom-up reasoning process which starts from specific observations and derives a generic conclusion.Many Knowledge graph completion task requires inductive reasoning such as WN18RR2 .</p>
<p>To apply inductive reasoning, one usually relies on a large number of observations (both positive and negative in support or against an induced rule).</p>
<p>Since large language models are pretrained on large amount of free-text, it learns several generic patterns or conclusions, therefore reasoning inductively (even if the rules may not be represented symbolically or a human-readable fashion) (Han et al., 2023).In general, commonsense reasoning tasks in NLP require both inductive and deductive reasoning.</p>
<p>Abductive Reasoning.Abductive reasoning typically begins with an incomplete set of observations and proceeds to derive most likely explanations for the observations to be true (Paul, 1993;Hobbs et al., 1993).Similar to inductive reasoning, this also involves uncertainty, as there can be different explanations.Compared to inductive reasoning, the deductive reasoning is a process from known facts or rules to derive a new conclusion, while abductive reasoning is from an observation to "guess" what can be the reason to cause the observation.It is used more often in our daily decision-making, such as medical diagnoses based on a set of incomplete symptoms.</p>
<p>In previous paragraphs, we mentioned how both inductive and abductive reasoning inherently encompass uncertainty.In fact, deductive reason- ing can also operate within the realm of uncertainty (De Raedt et al., 2007;Richardson and Domingos, 2006;Lee and Wang, 2016;Bach et al., 2017;Lee and Luo, 2018).Such reasoning paradigm uses "soft rules" to indicate the likelihood of a rule being true rather than its absolute truth.Consequently, conclusions derived may carry probabilistic true/false values.Reasoning under uncertainty is particularly useful because the real world is inherently unpredictable and full of unknown variables.While many datasets operate under the assumption that rules are unequivocally true, Rulebert (Saeed et al., 2021) deviates by attributing weight values to each rule.</p>
<p>Logical Reasoning Tasks and Datasets</p>
<p>We discuss the task and datasets in terms of format of the tasks and how they are created.</p>
<p>Four Types of Tasks</p>
<p>Multiple Choice Question Answering (MCQA).In the MCQA task, the given inputs are a paragraph which forms a context, a question, and a list of answer candidates (typically four choices).The goal is to predict which candidate is (most likely) correct.All datasets are pure-text (Yu et al., 2019;Liu et al., 2020) 3 .</p>
<p>Free Form Question Answering.Unlike MCQA, where a set of answer choices are given, freeform QA only has a context and a question, and the answer to the question can be any format, including but not limited to a single word, a list of words, and a number (Weston et al., 2015b;Banerjee et al., 2020).</p>
<p>Fact Checking.In fact verification, given a context and a fact, the goal is to classify the binary truth value of the fact according to the given information (Clark et al., 2020;Saeed et al., 2021;He et al., 2021).</p>
<p>Natural language inference (NLI) NLI is the task of detecting inferential relationships between a premise and a hypothesis.For most NLI datasets, there are three relationships, entailment (the hypothesis follows or can be inferred by the premise), neutral (the truth of hypothesis is undetermined by the premise), and contradiction (the hypothesis contradicts the premise or some facts in the premise) (Tian et al., 2021a).</p>
<p>Dataset Creation Techniques</p>
<p>Human Annotation.Crowdsourcing is one of the major approaches to create datasets, such as for NLI tasks.The advantages of this methodology include a richer linguistic grammar and potentially increased task complexity.However, it comes with drawbacks.In addition to being a costintensive process, crowdsourced datasets tend to harbor biases (as highlighted in numerous previous studies (Yu et al., 2019)).These biases can be leveraged by neural models to artificially inflate accuracy scores.Furthermore, assembling a dataset for logical reasoning tasks demands a level of expertise that poses a significant challenge.</p>
<p>Extraction from Academic Challenge.It is hard for crowdsourcing workers to produce questions requiring complicated logical reasoning since such reasoning tasks require extensive training and practice.Fortunately, questions in some standardized tests are aligned with the goal of logical reasoning and can be utilized to create such datasets after some preprocessing (Yu et al., 2019;Liu et al., 2020).However, the domains of these examinations are limited and the dataset size is small.Synthetic Generation.Synthetic generation is more efficient to create large data than manually created ones (Luo et al., 2022b).There are two ways, simulation based (Weston et al., 2015b) and rule-based (Clark et al., 2020;Saeed et al., 2021;Banerjee et al., 2020).In rule based methods, logic programs (either written by humans or mined from knowledge graphs) are generated, and then implications are drawn by automatic theorem prover.Last, the rules and facts in the logic programs are converted into English form using natural language patterns.Synthetic generation has issues that the rules or facts do not have real-world meaning and the language could be simple.</p>
<p>Language Models for Logical Reasoning over Natural Language</p>
<p>Language models (LMs) have been actively studied these days for logical reasoning tasks.Dasgupta et al. (2022) demonstrates that large language mdoels (LLMs) show human-level abstract reasoning skill.Creswell et al. (2022) proposes a selection-inference pipeline that given a context and question, the model can firstly select which facts or rules given in the context are important to answer the question and decompose the question into step by step reasoning.Wei et al. (2022) demonstrates that language models have the capacity to engage in chain-of-thoughts (CoT) reasoning.This approach facilitates a step-by-step reasoning process that enhances the performance of the model in downstream tasks such as mathematical reasoning.In following section, we summarize the five prevalent trends in utilizing language models for logical reasoning over language.</p>
<p>Supervised Finetuning</p>
<p>Fine-tuning a language model on the downstream tasks has been a standard way to teach a model to perform a task.Such a paradigm has also been the prevalent method for logical reasoning tasks (Clark et al., 2020;Liu et al., 2021;Tian et al., 2021b;Saeed et al., 2021;Han et al., 2022;Chan et al., 2023).In general, such a method is usually applied to a moderate size of the language model such as BERT (Devlin et al., 2019b), GPT2 (Radford et al.), RoBERTa (Liu et al., 2019), and XL-Net (Yang et al., 2019).It has been shown that transformer based models perform better than the other types of neural models such as LSTM (Yu et al., 2019;Liu et al., 2020), probably because such pretrained models have a certain degree of commonsense and logical reasoning (Huang et al., 2019).This has been further proven in (Clark et al., 2020).They show that when every word in the passage is replaced by a random word resulting in no grammaticality, the performance of a transformerbased model dramatically decreases.In addition, the larger model performs better than smaller ones, indicating that the deeper a model is, the more complicated reasoning it can execute (He et al., 2021).</p>
<p>While the IID performance of a fine-tuned model can be nearly perfect, such model has poor generalization.For example, model can not generalize from lower depth to higher depth reasoning (Clark et al., 2020), from low level language diversity to high level diversity (Richardson and Sabharwal, 2021;Tafjord et al., 2021), from one domain to another domain (Banerjee et al., 2020).Such observations indicate that models might just learn the inductive pattern in the training data rather than the underline logical reasoning skill (Zhang et al., 2022).</p>
<p>Logical Reasoning Pretraining</p>
<p>The next word prediction or mask language modeling pretraining tasks allow the language models to learn the language syntax and semantic as well as the world knowledge, however, it does not guarantee a model to learn logical operations.Thus, researchers have been exploring logical-oriented pretraining tasks to teach a model of logical reasoning from large free data.APOLLO (Sanyal et al., 2022)</p>
<p>Proof Generation</p>
<p>Proof generation is found to be harder than answer generation (Saha et al., 2020;Tafjord et al., 2021).However, models developed for a proof generation task have better performance on out-of-domain datasets or unseen depth reasoning (e.g., train on lower depth and test on higher depth).Kaiyu Yang and Chen (2022) introduce NLProofS, a novel method for generating step-by-step logically valid and relevant proofs given a set of supporting facts and hypothesis.In their proposed method, they employ a prover which generates candidate proofs step-by-step, a verifier to measure the validity of generated proof steps to avoid the prover from hallucinating proof steps, and an algorithm for retrieving the entire proof with highest validity score by aggregating proof step scores.ProofWriter (Tafjord et al., 2021) proposed two ways to generate proofs based on T5 models.The first one is to predict the sequence of proof in one output; the second one is to iteratively generate a proof and specifically, predict one intermediate conclusion and combine it with the given facts and rules as a new input to predict the following conclusion, and repeat this process until no new conclusion is predicted.</p>
<p>CoT Knowledge Distillation</p>
<p>The previous approach relies on the proof annotations in the datasets, however, in many cases, the dataset does not come with the proof.It is shown that large language model (LLM) can generate step-by-step reasoning (similar as the proof) (Saparov and He, 2023a;Liu et al., 2023c).Namgyu Ho (2022) propose Fine-tune-CoT (i.e.chain-ofthought (Wei et al., 2022)) approach which involves three key steps.In the first step, a large teacher model is prompted to address intricate queries, generating multi-step reasoning explanations.These explanations are then filtered based on the accuracy of the final prediction.In the second step, a reasoning sample is constructed, incorporating the question, rationale, and answer, thereby forming a comprehensive prompt and multi-step solution.This collection of carefully curated reasoning samples is leveraged to fine-tune a compact student model, imbuing it with the ability to engage in reasoning tasks.Nonetheless, LLMs encounter difficulties in planning proofs, occasionally making wrong selections when presented with multiple valid choices.This challenge leads to proofs that are not fully developed and consequently produces inaccurate responses.</p>
<p>Neural Symbolic</p>
<p>Recent advancements in pre-trained language models have demonstrated impressive reasoning abilities using explanations or "chain-of-thought" for in-context learning.Conversely, reasoning tasks are considered more straightforward for symbolic programming.A promising way is to use LLM to translate a natural language input into a symbolic program which can be consumed by a symbolic solver.Such a paradigm has been shown to effectively avoid unfaithfulness of LLM (Pan et al., 2023).Hanlin Zhang1 (2022) employs LLMS as Logic Programmers (LMLP) to learn logic rules and examples and reason over knowledge bases (KBs) using Prolog's backward chaining algorithm.They show that LMLP outperforms CoT in deductive reasoning settings, achieving over 25% higher accuracy on length generalization benchmarks, even with fewer parameters.Pan et al. (2023) propose Logic-LM to handle deductive reasoning, first-order logic, and constraint programming tasks.They leverage GPT-3 and in-context learning (providing a few examples) to translate a natural language input to a formal language formulation that can be executed by symbolic engines.They also show that the error messages of the symbolic engines can refine the output of an LLM.Such a paradigm has been investigated for addressing other challenges, wherein LLMs act as planners, and external tools are utilized to execute the plan (Lu et al., 2023;Sumers et al., 2023;Paranjape et al., 2023;Guan et al., 2023;Schick et al., 2023).</p>
<p>Survey Summary</p>
<p>The survey delineates how current datasets address three types of logical reasoning distributed across four task formats.Additionally, the curation process of a dataset can influence its inherent difficulty level.We've also identified five approaches for utilizing LLMs in addressing these reasoning tasks.This structured insight serves as a foundation for future research, offering a roadmap to optimize model performance and curation methodologies.</p>
<p>In the following section, we will present a logical reasoning benchmark, positioned alongside established benchmarks like SuperGlue (Wang et al., 2019), BigBench (Srivastava et al., 2023), and Unicorn (Lourie et al., 2021), all aimed at exhaustively gauging system capabilities.</p>
<p>LogiGLUE: General Logical Reasoning Benchmark</p>
<p>As mentioned the introduction, the reasoning ability of language models as assessed by various studies seems to differ.One plausible explanation for this variance is the inconsistency in the benchmarks used or differences in task formats, leading to performance disparities.To rectify this, our goal is to offer a standardized testbed.It becomes imperative to meticulously formulate our selection criteria to create a testbed that evaluates a system's logical reasoning capabilities.Guiding our dataset choice are two primary principles outlined in §3.1.These endeavors have led to the formation of a diverse and comprehensive logical reasoning benchmark, which we've named LogiGLUE ( §3.2).</p>
<p>Principle of Collecting LogiGLUE</p>
<p>Numerous logical reasoning datasets have been accessible since 2015, such as bAbi (Weston et al., 2015b).Our selection process for including a dataset in LogiGLUE is primarily driven by principles of diversity and generalization (Gokhale et al., 2022).</p>
<p>Diversity.There are two aspects to Diversity.First aspect concerns the types of reasoning in the dataset.We ensure that our coverage encompasses three main reasoning types, which collectively represent the full spectrum of logical reasoning.These three categories have been previously discussed.</p>
<p>The second aspects concerns the level of difficulty, with datasets ranging from easy to hard.Our experimental results indicate a varied model performance across different datasets -excelling in some, delivering average results on others, and struggling significantly on a few.We discovered a strong correlation between the complexity of a dataset and the methodology employed in its creation.Datasets built using simple templates and basic rule structures tend to be easier.In contrast, those with more sophisticated rules and uncertain elements are relatively more challenging.However, the most difficult datasets are those meticulously crafted by human hands.</p>
<p>Generalization.We also consider the axis of generalization, which aims to quantify (or assess) whether a model trained on the logical reasoning tasks can genuinely acquire reasoning skills.Previous studies have found that the superior performance of a fine-tuned language model primarily stems from learning the patterns exhibited within the dataset, which unfortunately often leads to poor generalization to other datasets.Consequently, the model's performance tends to be overestimated due to the identical and independently distributed (IID) nature of the testing data.To counteract this, LogiGLUE includes an out-of-domain testing set that also encompasses the three types of reasoning.</p>
<p>The out-of-domain testing set is readily adaptable to incorporate future logical reasoning tasks.</p>
<p>Excluded Datasets.Lexicographically speaking, reasoning is defined as "the process of forming conclusions, judgments, or inferences from facts or premises."4Reasoning is usually associated with an entailment relation, where given a premises, the truth value of a hypothesis depends on if the latter is entailed by the premise or not.</p>
<p>There are many datasets that require reasoning which we decided to exclude from the scope of this work.This includes some well-known NLI datasets, such as, SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018).These datasets use many linguistic forms, unstated background knowledge, and sometimes unsupported inference steps (Clark et al., 2020).We also exclude datasets where reasoning with external domain knowledge is required since for such tasks, retrieving the external knowledge is essential and it is hard to diagnose whether the noisy retrieved knowledge affects systems or systems lack of logical reasoning capacity.This includes QuAIL (Rogers et al., 2020), WSC (Levesque et al., 2012), QuaRTz (Tafjord et al., 2019), ROPES (Lin et al., 2019).Commonsense reasoning datasets are not covered in this survey either since they focus on solving a task using commonsense knowledge (Sap et al., 2020) and thus it is more important to acquire the commonsense knowledge rather than to do logical reasoning.Other datasets that we exclude are ones that require logical reasoning but are not presented in the natural language form such as logical entailment (Evans et al., 2018), NeuroSAT (Selsam et al., 2019), and LTL (Hahn et al., 2020).</p>
<p>Statistic of LogiGLUE</p>
<p>A suite of natural language logical reasoning benchmarks with 10 in-domain and 12 out-domain datasets that cover different types of logical reasoning.In addition, LogiGLUE includes three task formats, multiple choice question answer (MCQA), natural language inference (NLI), and fact verification (FV)/ fact checking (FC).Table 1 shows the statistics.</p>
<p>Unique Format.There are many existing practicing for standardizing different datasets into a consistent format (Mishra et al., 2022;Lourie et al., 2021), such as transforming all tasks to question answering (McCann et al., 2018) or NLI (Poliak et al., 2018) styles.Through our model analysis, it's evident that certain models can only manage specific reasoning tasks.For instance, classification models are commonly used for NLI and MCQA tasks, where the number of classification heads matches the number of choices (like 3 for NLI and 4 for MCQA).Yet, these models struggle when confronted with free-form question answering, thus limiting their versatility.Hence, to develop a model adept at logical reasoning regardless of task structure, we convert them into a singular format.An added advantage of this standardized format is that it ensures consistency in the input, ruling out performance disparities arising from different inputs.Every dataset is then adapted to this specific format.</p>
<p>In the case of MCQA/FV/NLI tasks, each instance encompasses a context, a question, and potential answer options.Conversely, FF tasks don't present any answer choices.The correct answer is the expected output for every instance.For FV tasks, we use the statement as the question with true/false as potential answers.In NLI tasks, the options include natural, contradictory, and entailment.</p>
<p>Experiments and Results</p>
<p>We selected Flan-T5-large (Chowdhery et al., 2022) as our base model for training due to two pivotal reasons.Firstly, Flan-T5 stands as an instructionfine-tuned iteration of T5, exhibiting enhanced performance when compared to its peers.Secondly, Flan-T5's manageable size renders it to be trainable on a machine that is conducive to an academic setting.In the following, we present results that encompass both quantitative and qualitative aspects.</p>
<p>In-Domain Performance</p>
<p>Single Task Fine-tuning</p>
<p>We fine-tune the Flan-T5-large on each individual dataset and the result is presented in Table 2.We draw some interesting observations.It is apparent that the model exhibits superior performance when operating on synthetic data compared to handcrafted alternatives.In support of this, the datasets that garnered the top 5 performances are predominantly synthetic.This trend holds even when considering the ANLI dataset, which, despite having a more substantial training set than its synthetic counterparts, yielded inferior results.Moreover, we ventured to explore if the model displayed a predilection for one form of reasoning over another.Preliminary insights suggest a potential preference towards abductive reasoning in comparison to deductive reasoning, as evidenced in the disparity in performance between the αNLI and ANLI datasets -both of which are similar in terms of training size and are hand-crafted.This, however, is a mild observation and warrants further exploration to derive a conclusive statement.For instance, our statistical analysis revealed that the average context length for ANLI is 105, whereas for αNLI, it is 66, potentially leading to varying degrees of difficulty.</p>
<p>Multitask Fine-tuning</p>
<p>We fine-tune the Flan-T5 on all in-domain datasets utilizing a weighted sampling technique to accommodate for the unbalanced size of the training datasets.We find that this sampling is better than random sampling and the comparison is given in the Appendix.We termed this model as LogiT5.</p>
<p>One benefit of multi-task training compared to the single task training is that the low resources data can benefit from other tasks (Parmar et al., 2022;Luo et al., 2022a).From Table 2, it is apparent that the multi-task training model holds a significant advantage when dealing with tasks with small training set.It showcases higher proficiency compared to its single-task counterpart, notably performing better by 5% and 8% on the αARCT and FOLIO tasks, respectively.These datasets, characterized by their smaller training size (limited to 1/2 K training samples), benefited notably from the multi-task training approach.Contrastingly, the tasks with large trainig set did not reap any benefits from multi-task training, such as αNLI and ANLI datasets.A potential explanation for this could be the substantial training set already facilitates optimum learning for the model, rendering the multitask training approach redundant.This observation underlines a critical limitation in leveraging multitask training when the individual training datasets are already sufficiently large.</p>
<p>Fine-tuned LogiT5 on Single Dataset</p>
<p>Here, we further fine-tune LogiT5 on each dataset.However, upon analyzing the performance displayed in Table 2, we did not observe any notable advantages from this additional fine-tuning even though small margin gains are achieved.This suggests that LogiT5 has likely already learned the majority of knowledge from these tasks.</p>
<p>Out-of-Domain Generation</p>
<p>When we study the out-of-domain generalization, we compare three models, Flan-T5, LogiT5, and LLama-2 (7B) (Touvron et al., 2023).In addition, for Llama-2, we also study the chain-of-thought prompting (Wei et al., 2022).Here, we evaluate the model's zero-shot capabilities rather than its fewshot in-context learning performance (Luo et al., 2023).Investigating the latter will be reserved for future research.More specifically, we add a prompt "let's think step by step" after the question.However, by our results, we do not see the advantage of CoT prompting, probably because the model already generates the reasoning even without such a prompt.Evaluating the LLama-2 answer poses a challenge since the output is usually a free form and not use the exact answer option.On the other hand Flan-T5 generate answer in a more structure way that is easier for evaluation, probably because Flan-T5 is already trained on instruction fine-tuned data  which are already in a structured templates.The preliminary results of LLama-2 were poor.Upon manually reviewing the predictions, we observed that LLama-2 occasionally produces synonyms of the ground truth.To address this, we employed ConceptNET (Speer et al., 2017) to identify synonyms and verify if the prediction aligns with any of them, a strategy akin to the one explored in (Luo et al., 2021).Furthermore, on the babi dataset, we have seen that sometimes the llama model ignores the input text and generates answer based on its pretrained knowledge.This is similar to the findings revealed in Varshney et al. (2023).</p>
<p>Flan-T5 CoT, 3K CoT, 6K CoT, 15K 0.37 0.38 0.37 0.41</p>
<p>CoT Distillation</p>
<p>As shown by previous work (Namgyu Ho, 2022), distill the chain-of-thoughts from a large model to a small student model can boost the performance of the student model.We apply such a CoT finetun-ing strategy and conduct experiments on LogiQA, identified as the most challenging task, by distilling the CoT from LLama-7B to Flan-T5.Initially, we generated a single answer for each question, retaining only the samples where the predicted answer was correct, resulting in approximately 3K valid samples.Alternatively, we created 10 answers for each question and preserved the samples with at least one correct predicted answer, which generated a unique set of 6K questions.It is worth noting that some questions offered multiple correct reasoning paths.In such cases, we either opted for a singular path or utilized all available paths, the latter approach amassing a total of 15K training samples.</p>
<p>With the CoT fine-tuning, we observe that the finetuning takes longer time and a larger learning rate in the beginning is helpful.Thus, instead of using 1e-4 as the learning rate, we use 3e-4.we train the model with 40 epochs.We do see that the model performance increase when the number of epoch increase.</p>
<p>Following this, we trained the Flan-T5 model utilizing datasets consisting of 3K, 6K, and 15K samples, derived from the generated CoT,], with the results delineated in Table 4.Our findings indicate that the training with 3K and 6K samples did not enhance the CoT's fine-tuning efficacy.However, an increased dataset size of 15K samples facilitated a 4% improvement in performance, suggesting that CoT distillation becomes more beneficial with a larger volume of data.</p>
<p>Conclusion</p>
<p>In this study, we concentrate our efforts on a crucial area of research: logical reasoning over natural language.Initially, we offer a survey to provide a thorough comprehension of this domain, emphasizing the role of large language models in addressing this demanding task.Following this, we assemble a benchmark for logical reasoning named LogiGLUE, set to be publicly available to aid forthcoming research.Finally, we refine a language model utilizing LogiGLUE, demonstrating encouraging results across both in-domain and outof-domain datasets.</p>
<p>Figure 1 :
1
Figure 1: Logical Reasoning Survey: Datasets and Language Model Application.</p>
<p>Figure 2 :
2
Figure 2: Examples (top) of three types of logical reasoning and explanations (bottom) correlating each example with its respective reasoning type.</p>
<p>(Jiao et al., 2023))he second pretraining task is entailment classification which aims to classify if there is an entailment relationship within a masked sentence or not.MERIt(Fangkai Jiao, 2022)proposes a meta-path-guided pretraining task to teach a model to learn logical reasoning by selfsupervised learning.They construct the training data by converting any document into a graph with entities as the node and the relation between the entities as edges.Then, given a pair of entities, the positive candidates are the sentences that connect this pair of entities, and the negative candidates are obtained by data augmentation.Such training data allows the model trained by contrastive learning manner to identify the positive sentence from the negative sentences.MERIt +(Jiao et al., 2023)combines MERIt with the autoregression training objective: rather than using contrastive learning, MERIt + optimizes the probability of positive candidate sentences.</p>
<p>improves the logical reasoning of a model by two pre-training tasks.The first pretraining task is selective mask language modeling (MLM).Unlike the naive MLM which randomly masks the words, s-MLM selects and masks the logical words (de-fined</p>
<p>Table 1 :
1
Statistics of In-domain (IID) and out-of-domain (OOD) datasets of LogiGLUE benchmark.
DatasetTrain size Dev size Test size Synthetic Task Type Reasoning TypeIn-domain datasetsαARCT 20192420632888✗MCQAAbductiveαNLI 2019169,654-1532✗NLIAbductiveCLUTTR-Robust 201910,100-144✓FFInductiveAbductionRule-Animal 201923,1003,3006,600✓FFAbductiveANLI 2020162,8653,2003,200✗NLIDeductiveLogiQA 20217,376651651✗MCQAMixedLogicNLI 2021b16,0002,0002000✓NLIDeductiveProofWriter 202169,81410,15820,058✓FVDeductiveRulebert-Union 202156,0004,6669,334✓FVDeductiveFOLIO 20221004204227✗FVDeductiveOut-of-domain datasetsbAbi 2015a--5000✓FFInductivebAbi 2015a--5000✓FFDeductiveCLUTTR-Systematic 2019--10100✓FFInductiveAbductionRule-person 2019--4,864✓FFAbductiveReClor 2020--500✗MCQAMixedBird-Electricity 2021--5270✗FVDeductiveNatlLang 2021--8,008✗FVDeductiveWinologic 2021--562✗FVDeductiveWaNLI 2022--5000✓NLIDeductiveRulebert-Union 2021--5000✓FVDeductiveBigBench 2022--1300✗FFDeductiveBigBench 2022--32✗FFInductiveLogiQA 2.0 2023a--3238✗NLIDeductivePrOntoQA 2023b--200✗MCQADeductive</p>
<p>Table 2 :
2
Three training strategies for models and the performance on In-domain Dataset.
DatasetFlan-T5 LogiT5 LLAMA LLAMA-CoTbAbi(induction) 2015a59.4413.1233.4627.06bAbi(deduction) 2015a7.3616.7458.7667.36CLUTTR-Systematic 201915.4310038.8240.31AbductionRule-person 201900.0095.9731.3143.19ReClor 202039.8046.8039.8039.00Bird-Electricity 202141.2965.1450.0045.73NatlLang 202156.1670.2449.5250.43Winologic 202168.8662.1048.9348.39WaNLI 202250.9662.2832.8015.54Rulebert-Union 202127.2962.9287.4061.50BigBench(logical-deduction) 202249.8538.0022.3823.23BigBench(logical args) 202259.3840.6240.6225.00LogiQA 2.0 2023a52.6655.0050.7450.46PrOntoQA 2023b6.5056.5070.0029.50Average38.6354.5043.6337.78</p>
<p>Table 3 :
3
Performance on out-domain Datasets.</p>
<p>Table 4 :
4
Performance of Flan-T5 trained with CoT finetuning on the LogiQA dataset.First column is without using CoT and trained on the given training set of LogiQA.</p>
<p>https://news.mit.edu/2021/hawkings-black-holetheorem-confirm-0701
Here, we exclude this task since we are more interested in natural language input. In this paper, we do not discuss about knowledge graph completion tasks since most of them are not in natural language forms.
ReClor and LogiQA sources the datasets from real examination questions that may involve images or charts. But they remove such questions and retain only those which are self-contained and answerable from the provided text.
https://www.dictionary.com/browse/reasoning</p>
<p>Hinge-loss markov random fields and probabilistic soft logic. H Stephen, Matthias Bach, Bert Broecheler, Lise Huang, Getoor, J. Mach. Learn. Res. 1812017</p>
<p>Pratyay Banerjee, Chitta Baral, Man Luo, Arindam Mitra, Kuntal Pal, Tran C Son, Neeraj Varshney, arXiv:2012.09938Can transformers reason about effects of actions. 2020</p>
<p>Abductive commonsense reasoning. Chandra Bhagavatula, Le Ronan, Chaitanya Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Wen-Tau Downey, Yejin Yih, Choi, International Conference on Learning Representations. 2019</p>
<p>A large annotated corpus for learning natural language inference. Samuel Bowman, Gabor Angeli, Christopher Potts, Christopher D Manning, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Human reasoning: The psychology of deduction. Jonathan Ruth Mj Byrne, St Bt, Stephen E Evans, Newstead, 2019Psychology Press</p>
<p>Chunkit Chan, Xin Liu, Tsz Ho Chan, Jiayang Cheng, Yangqiu Song, Ginny Wong, Simon See, arXiv:2309.08303Self-consistent narrative prompts on abductive natural language inference. 2023arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, 10.24963/ijcai.2020/537IJCAI. 2020</p>
<p>The birth of prolog. Alain Colmerauer, Philippe Roussel, History of programming languages-II. 1996</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Ishita Dasgupta, Stephanie Cy Andrew K Lampinen, Antonia Chan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, arXiv:2207.07051Language models show human-like content effects on reasoning. 2022arXiv preprint</p>
<p>Problog: A probabilistic prolog and its application in link discovery. Luc De Raedt, Angelika Kimmig, Hannu Toivonen, Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI'07. the 20th International Joint Conference on Artifical Intelligence, IJCAI'07San Francisco, CA, USAMorgan Kaufmann Publishers Inc2007</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL-HLT. 2019a</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423NAACL. Minneapolis, Minnesota. ACL2019b</p>
<p>Can neural networks understand logical entailment. Richard Evans, David Saxton, David Amos, 2018In ICLR</p>
<p>Merit: Meta-path guided contrastive learning for logical reasoning. Xuemeng Song, Liqiang Nie, Fangkai Jiao, Yangyang Guo, arXiv:2203.003572022arXiv preprint</p>
<p>Generalized but not robust? comparing the effects of data modification methods on out-of-domain generalization and adversarial robustness. Tejas Gokhale, Swaroop Mishra, Man Luo, Findings of the Association for Computational Linguistics: ACL 2022. 2022Bhavdeep Sachdeva, and Chitta Baral</p>
<p>Leveraging pretrained large language models to construct and utilize world models for model-based task planning. Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati, arXiv:2305.149092023arXiv preprint</p>
<p>Teaching temporal logics to neural networks. Christopher Hahn, Frederik Schmitt, Jens U Kreber, Markus Norman Rabe, Bernd Finkbeiner, 2020In ICLR</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, arXiv:2209.00840Folio: Natural language reasoning with firstorder logic. 2022arXiv preprint</p>
<p>Inductive reasoning in humans and large language models. Simon Jerome, Han , Keith J Ransom, Andrew Perfors, Charles Kemp, Cognitive Systems Research. 1011552023</p>
<p>The impact of symbolic representations on in-context learning for few-shot reasoning. arXiv:2212.08686Li Erran Li3 Eric Xing Hanlin Zhang1, Yi-Fan Zhang22022arXiv preprint</p>
<p>WinoLogic: A zero-shot logic-based diagnostic dataset for Winograd Schema Challenge. Weinan He, Canming Huang, Yongmei Liu, Xiaodan Zhu, EMNLP. 2021</p>
<p>Logitorch: A pytorch-based library for logical reasoning on natural language. Evan Heit, 10.1017/CBO9780511619304.002Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingSystem Demonstrations2007. 2022Chadi Helwe, Chloé Clavel, and Fabian Suchanek</p>
<p>Interpretation as abduction. Artificial intelligence. Jerry R Hobbs, Mark E Stickel, Douglas E Appelt, Paul Martin, 199363</p>
<p>Cosmos QA: Machine reading comprehension with contextual commonsense reasoning. Lifu Huang, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, 10.18653/v1/D19-1243EMNLP-IJCNLP. 2019</p>
<p>Abduction-based explanations for machine learning models. Alexey Ignatiev, Nina Narodytska, Joao Marques-Silva, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>
<p>Logicllm: Exploring self-supervised logic-enhanced training for large language models. Fangkai Jiao, Zhiyang Teng, Shafiq Joty, Bosheng Ding, Aixin Sun, Zhengyuan Liu, Nancy F Chen, arXiv:2305.137182023arXiv preprint</p>
<p>TaxiNLI: Taking a ride up the NLU hill. Pratik Joshi, Somak Aditya, Aalok Sathe, Monojit Choudhury, 10.18653/v1/2020.conll-1.4CoNLL. 2020</p>
<p>Generating natural language proofs with verifier-guided search. Jia Deng, Kaiyu Yang, Danqi Chen, arXiv:2205.124432022arXiv preprint</p>
<p>Measuring faithfulness in chainof-thought reasoning. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukosiute, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam Mccandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, ; Samuel, R Bowman, Ethan Perez, 10.48550/arXiv.2307.13702CoRR, abs/2307.13702Jan Brauner,. 2023Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan</p>
<p>Strong equivalence for lpmln programs. Joohyung Lee, Man Luo, 35th International Conference on Logic Programming. 2018. 2019</p>
<p>Weighted rules under the stable model semantics. Joohyung Lee, Yi Wang, 2016In KRR</p>
<p>The winograd schema challenge. Hector Levesque, Ernest Davis, Leora Morgenstern, 2012In KRR</p>
<p>Answer set programming. Vladimir Lifschitz, 2019SpringerBerlin</p>
<p>Reasoning over paragraph effects in situations. Kevin Lin, Oyvind Tafjord, Peter Clark, Matt Gardner, MRQA. 2019</p>
<p>Wanli: Worker and ai collaboration for natural language inference dataset creation. Alisa Liu, Swabha Swayamdipta, Noah A Smith, Yejin Choi, arXiv:2201.059552022arXiv preprint</p>
<p>Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding. Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, Yue Zhang, 10.1109/TASLP.2023.3293046IEEE/ACM Transactions on Audio, Speech, and Language Processing. 312023a</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, Yue Zhang, arXiv:2304.03439Evaluating the logical reasoning ability of chatgpt and gpt-4. 2023barXiv preprint</p>
<p>Logicot: Logical chain-of-thought instruction-tuning data collection with gpt-4. Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, Yue Zhang, arXiv:2305.121472023carXiv preprint</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, 10.24963/ijcai.2020/501IJCAI. 2020</p>
<p>Logiqa: a challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019</p>
<p>Nicholas Lourie, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, arXiv:2103.13009Unicorn on rainbow: A universal commonsense reasoning model on a new multitask benchmark. 2021arXiv preprint</p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, arXiv:2304.09842Chameleon: Plug-and-play compositional reasoning with large language models. 2023arXiv preprint</p>
<p>Choose your qa model wisely: A systematic study of generative and extractive readers for question answering. Man Luo, Kazuma Hashimoto, Semih Yavuz, Zhiwei Liu, Chitta Baral, Yingbo Zhou, Spa-NLP. 72022a. 2022</p>
<p>Akarshan Sajja, and Chitta Baral. 2021. 'just because you are right, doesn't mean i am wrong': Overcoming a bottleneck in development and evaluation of open-ended vqa tasks. Man Luo, Keyur Shailaja, Riley Sampat, Yankai Tallman, Manuha Zeng, Vancha, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</p>
<p>Biotabqa: Instruction learning for biomedical table question answering. Man Luo, Sharad Saxena, Swaroop Mishra, CEUR Workshop Proceedings. CEUR-WS2022b3180Mihir Parmar, and Chitta Baral</p>
<p>Man Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat, Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, Vincent Y Zhao, arXiv:2305.14128Dr. icl: Demonstration-retrieved in-context learning. 2023arXiv preprint</p>
<p>The natural language decathlon: Multitask learning as question answering. Bryan Mccann, Nitish Shirish Keskar, Caiming Xiong, Richard Socher, arXiv:1806.087302018arXiv preprint</p>
<p>Artificial intelligence, logic and formalizing common sense. John Mccarthy, Philosophical logic and artificial intelligence. Springer1989</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Se-Young Yun, Namgyu Ho, Laura Schmid, arXiv:2212.10071Large language models are reasoning teachers. 2022arXiv preprint</p>
<p>Adversarial nli: A new benchmark for natural language understanding. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, Douwe Kiela, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Probing neural network comprehension of natural language arguments. Timothy Niven, Hung-Yu Kao, ACL. 2019</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.122952023arXiv preprint</p>
<p>Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, Marco Tulio, Ribeiro , arXiv:2303.09014Automatic multistep reasoning and tool-use for large language models. Art2023arXiv preprint</p>
<p>Murad Mohammad, and Chitta Baral. 2022. In-boxbart: Get instructions into biomedical multitask learning. Mihir Parmar, Swaroop Mishra, Mirali Purohit, Man Luo, Findings of the Association for Computational Linguistics: NAACL 2022. </p>
<p>Approaches to abductive reasoning: an overview. Gabriele Paul, Artificial intelligence review. 721993</p>
<p>Collecting diverse natural language inference problems for sentence representation evaluation. Adam Poliak, Aparajita Haldar, Rachel Rudinger, J Edward Hu, Ellie Pavlick, Aaron Steven White, Benjamin Van Durme, 10.18653/v1/D18-1007Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, </p>
<p>Pushing the limits of rule reasoning in transformers through natural language satisfiability. Kyle Richardson, Ashish Sabharwal, arXiv:2112.090542021</p>
<p>Markov logic networks. Matthew Richardson, Pedro Domingos, 10.1007/s10994-006-5833-1Mach. Learn. 621-22006</p>
<p>Getting closer to ai complete question answering: A set of prerequisite real tasks. Anna Rogers, Olga Kovaleva, Matthew Downey, Anna Rumshisky, AAAI. 202034</p>
<p>RuleBERT: Teaching soft rules to pre-trained lms. Mohammed Saeed, Naser Ahmadi, Preslav Nakov, Paolo Papotti, EMNLP. 2021</p>
<p>Prover: Proof generation for interpretable reasoning over rules. Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, Mohit Bansal, EMNLP. 2020</p>
<p>Apollo: A simple approach for adaptive pretraining of language models for logical reasoning. Soumya Sanyal, Yichong Xu, Shuohang Wang, Ziyi Yang, Reid Pryzant, Wenhao Yu, Chenguang Zhu, Xiang Ren, arXiv:2212.092822022arXiv preprint</p>
<p>Introductory tutorial: Commonsense reasoning for natural language processing. Maarten Sap, Vered Shwartz, Antoine Bosselut, Yejin Choi, Dan Roth, 2020. 202027</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations. 2023a</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>Inductive reasoning. Encyclopedia of animal cognition and behavior. Bruno Sauce, Louis D Matzel, 20176</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint</p>
<p>Learning a sat solver from single-bit supervision. Daniel Selsam, Matthew Lamm, Benedikt Bünz, Percy Liang, Leonardo De Moura, David L Dill, ICLR. 2019</p>
<p>Clutrr: A diagnostic benchmark for inductive reasoning from text. Koustuv Sinha, Shagun Sodhani, Jin Dong, EMNLP. 2019</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201731</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, Transactions on Machine Learning Research. 2023</p>
<p>Theodore Sumers, Shunyu Yao, Karthik Narasimhan, Thomas L Griffiths, arXiv:2309.02427Cognitive architectures for language agents. 2023arXiv preprint</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, 10.18653/v1/2021.findings-acl.317Findings-ACL-IJCNLP. 2021</p>
<p>Quartz: An open-domain dataset of qualitative relation questions. Oyvind Tafjord, Matt Gardner, Kevin Lin, Peter Clark, arXiv:1909.035532019</p>
<p>Diagnosing the firstorder logical reasoning ability through LogicNLI. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin, EMNLP. 2021a</p>
<p>Diagnosing the first-order logical reasoning ability through logicnli. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021b</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Large language models still can't plan (a benchmark for llms on planning and reasoning about change). Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, NeurIPS 2022 Foundation Models for Decision Making Workshop. 2022</p>
<p>On the planning abilities of large language models (a critical investigation with a proposed benchmark. Karthik Valmeekam, Sarath Sreedharan, Matthew Marquez, Alberto Olmo, Subbarao Kambhampati, arXiv:2302.067062023arXiv preprint</p>
<p>Can nlp models correctly reason over contexts that break the common assumptions?. Neeraj Varshney, Mihir Parmar, Nisarg Patel, Divij Handa, Sayantan Sarkar, Man Luo, Chitta Baral, arXiv:2305.120962023arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, NeurIPS. Curran Associates, Inc201730</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in neural information processing. 201932</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Towards ai-complete question answering: A set of prerequisite toy tasks. Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merriënboer, Armand Joulin, Tomas Mikolov, arXiv:1502.056982015aarXiv preprint</p>
<p>Towards ai-complete question answering: A set of prerequisite toy tasks. Jason Weston, Antoine Bordes, arXiv:1502.056982015b</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel Bowman, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics20181</p>
<p>Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, Yoon Kim, 10.48550/arXiv.2307.02477CoRR, abs/2307.024772023</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, arXiv:2303.12023Xinya Du, Rui Mao, Jinjie Ni, and Erik Cambria. 2023. Logical reasoning over natural language as knowledge representation: A survey. Neurips, 201932arXiv preprintCarbonell</p>
<p>Fei Yu, Hongbo Zhang, Benyou Wang, arXiv:2303.14725Nature language reasoning, a survey. 2023arXiv preprint</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, ICLR. 2019</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, International Conference on Learning Representations. 2020</p>
<p>Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, Guy Van Den Broeck, arXiv:2205.11502On the paradox of learning to reason from data. 2022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4571 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4571</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4571</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-381ab7a640f5b46b62f7e08d1af4a8e0d3eadd55</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/381ab7a640f5b46b62f7e08d1af4a8e0d3eadd55" target="_blank">G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> G-Eval is presented, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs, and it is shown that G- Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin.</p>
                <p><strong>Paper Abstract:</strong> The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4571.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4571.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G-EVAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>G-EVAL (LLM-based NLG Evaluator with CoT and Form-Filling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-based evaluation framework that uses large language models (GPT-3.5 / GPT-4) with an automatically generated chain-of-thought and a form-filling output paradigm; final scalar scores are produced by probability-weighted summation over discrete score tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>G-EVAL</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Feed the LLM a Task Introduction and Evaluation Criteria, ask it to auto-generate a Chain-of-Thought (detailed Evaluation Steps), then evaluate candidate NLG outputs via a form-filling prompt that requests a discrete score (e.g., 1-5). To get fine-grained continuous scores, compute the probability p(si) of each discrete score token from the LLM and return score = sum_i p(si) * si. For models that do not expose token probabilities (GPT-4 in this work), approximate p(si) via repeated sampling (n=20) with temperature>0 and estimate token frequencies.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Task-dependent NLG dimensions such as coherence, fluency, consistency (factual consistency), relevance, conciseness/coverage, grammar; for dialogue: naturalness, coherence, engagingness, groundedness; for hallucination evaluation: factual consistency (does summary contain unsupported facts).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (G-EVAL-4) and GPT-3.5 (text-davinci-003, G-EVAL-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation (summarization and dialogue evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluative judgments of generated texts (summaries, dialogue responses) — i.e., assessment of generated explanations/outputs rather than scientific-domain theories</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>G-EVAL-4 substantially outperforms prior metrics across benchmarks. On SummEval (summary-level) G-EVAL-4 average Spearman rho = 0.514 and Kendall-Tau tau = 0.418; on Topical-Chat (turn-level) G-EVAL-4 average Pearson r = 0.575 and Spearman rho = 0.588; on QAGS (hallucination/factual consistency) average Pearson r = 0.599, Spearman rho = 0.611, Kendall-Tau tau = 0.525. G-EVAL-3.5 performs worse than G-EVAL-4, especially on more abstractive/consistency-sensitive tasks (e.g., QAGS-XSum). Ablations show CoT and probability-weighting improve Spearman correlation; probability-weighting increases fine-grained ranking (higher Spearman) though can reduce Kendall-Tau due to fewer ties.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid validation: automated LLM-based scoring is compared to human judgments. The evaluator itself is applied automatically (form-filling + probability weighting), and its outputs are validated by computing correlations (Spearman/Kendall/Pearson) with human ratings on standard human-annotated benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Meta-evaluation via correlation with human judgments on three benchmarks (SummEval, Topical-Chat, QAGS). Reported metrics include Spearman rho, Kendall-Tau tau, and Pearson r (turn-level where applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Observed bias: G-EVAL tends to assign higher scores to LLM-generated summaries (GPT-3.5) than to human-written summaries even when humans prefer the human summaries; possible self-reinforcement if used as reward signal. Dependence on availability and versioning of LLMs (access/cost and changing model behavior). Integer score outputs produce ties and low variance unless probability-weighting is applied; some LLMs do not expose token probabilities (sampling required). Performance sensitive to LLM capacity (GPT-4 > GPT-3.5 on harder tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>SummEval (summarization with human ratings on fluency, coherence, consistency, relevance), Topical-Chat (dialogue response with human ratings on naturalness, coherence, engagingness, groundedness), QAGS (factual consistency/hallucination: QAGS-CNN and QAGS-XSum).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4571.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4571.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Chain-of-Thoughts generation for evaluation (Auto CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use the LLM to generate intermediate evaluation steps (a chain-of-thought) automatically from the Task Introduction and Evaluation Criteria; include that CoT in the evaluation prompt to provide detailed guidance/context for scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Automatic Chain-of-Thought (Auto-CoT) for Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt the LLM with the task definition and evaluation criteria and ask it to output 'Evaluation Steps' — a sequence of sub-steps describing how to assess the target dimension (e.g., read article, identify key points, compare ordering). Then use the generated CoT together with the source and candidate text in the scoring prompt to produce a score and explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Used to operationalize task-specific criteria (e.g., for coherence: identify main topic/key points, verify ordering and coverage; for engagingness: check interesting facts/opinions).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 / GPT-3.5 (as used to generate CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation (summarization, dialogue)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Procedural evaluation steps for assessing generated explanations/texts</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Including CoT improves correlation with human judgments; G-EVAL-4 with CoT shows higher Spearman correlations than G-EVAL-4 without CoT across SummEval dimensions (notably fluency and consistency). Exact ablation: G-EVAL-4 - CoT row in Table 1 shows AVG Spearman = 0.500 (vs 0.514 full) and other dimension improvements relative to simple prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated — CoT is generated and then used by the LLM evaluator; validated by correlation to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Ablation experiments on SummEval comparing G-EVAL with and without CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Quality of generated CoT depends on LLM; CoT may encode evaluator's internal heuristics that bias scoring; generation of CoT is itself automated and may vary across model versions.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>SummEval (used for CoT ablation results)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4571.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4571.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Probability-weighted scoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probability-weighted summation of discrete score tokens</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Convert discrete LLM output token probabilities over score labels into a continuous score by summing p(score_label) * score_label to reduce ties and increase granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Probability-weighted scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Given predefined discrete scores S = {s1,...,sn} (e.g., 1..5), compute the LLM's probability p(si) for each score token and return score = sum_i p(si) * si. For models without explicit token-probability API, estimate p(si) by sampling (n=20) and using empirical frequencies.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Used for any of the NLG dimensions being scored (coherence, fluency, consistency, etc.) to obtain continuous/graded scores.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (probabilities approximated by sampling) and GPT-3.5 (token probs available for deterministic estimation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Scoring/aggregation approach for discrete evaluative labels</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Probability-weighting increases Spearman correlation (better ranking agreement with humans) but can decrease Kendall-Tau in some settings because it removes ties (the paper reports higher Spearman for G-EVAL-4 with probabilities vs without). Example: G-EVAL-4 AVG Spearman 0.514 with probabilities vs 0.500 in an ablation; Kendall-Tau differences reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated scoring method validated against human judgments via correlation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Ablation experiments comparing with/without probability normalization on SummEval and effect on Spearman/Kendall-Tau.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires access to token probabilities or reliable sampling; sampling-based estimation introduces variance; removing ties affects Kendall-Tau interpretation; may still reflect LLM biases.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>SummEval (primary ablation); also applied on Topical-Chat and QAGS evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4571.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4571.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Form-filling paradigm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Form-filling evaluation paradigm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ask the LLM to fill a predefined evaluation form (output specific fields/scores) instead of scoring by conditional log-probability of the candidate text, enabling direct adherence to evaluation criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Form-filling paradigm for LLM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Concatenate Task Introduction, auto-generated CoT, source context and candidate output, then prompt the LLM to write values into a short structured 'Evaluation Form' (e.g., 'Coherence: [1-5]'). The LLM's outputs (and their token probabilities) are used to generate scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Any user-specified evaluative dimension; the paradigm is format-flexible and can include explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 / GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Structured elicitation of evaluative judgments</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Form-filling outperforms conditional-probability-based GPTScore in several dimensions; paper reports G-EVAL (form-filling) outperforms GPTScore in SummEval and other benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated scoring approach; validated by correlation with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Direct comparison with GPTScore and other baselines on SummEval/Topical-Chat/QAGS.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>LLMs tend to produce integer labels (ties) unless probability weighting is applied; form design and prompt wording influence outcomes; possibility of LLM-specific evaluation heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>SummEval, Topical-Chat, QAGS</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4571.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4571.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPTScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPTScore (conditional-probability evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluator that uses a generative pre-trained model to compute the conditional generation probability (likelihood) of the candidate text given context/instruction, under the assumption that higher-quality text has higher model probability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>GPTScore (likelihood-based evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Score candidate outputs by the conditional log-probability assigned by a generative LLM to the candidate given the prompt/context; no reference required. Contrasts with G-EVAL's form-filling approach.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Implicit fluency/likelihood-based quality; not explicitly decomposed into task dimensions in original formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 / text-davinci-003 used in comparative experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Likelihood-based ranking of generated outputs</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>In this paper GPTScore (with GPT-3.5) achieves lower average correlation than G-EVAL-4 on SummEval (G-EVAL-4 outperforms GPTScore on several dimensions as reported in Table 1). Exact GPTScore averages in Table 1: reported as AVG rho ~0.417 (GPTScore) vs 0.514 (G-EVAL-4).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated; validated by correlation with human judgments (meta-evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compared to human ratings on SummEval and other benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on model likelihood which may confound style/verbosity with quality; lower human correspondence than targeted form-filling + CoT; may prefer text that fits model's internal distribution (bias).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>SummEval (primary comparison in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4571.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4571.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UniEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UniEval (unified multidimensional evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned, unified evaluator that treats multiple evaluation aspects as QA tasks and uses a pretrained T5 to predict scores for different dimensions of generated text quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>UniEval</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Reformats evaluation into question-answer pairs (e.g., 'Is the summary factually consistent?') and uses a finetuned T5 model to produce scores across multiple dimensions; trained on human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Coherence, consistency, fluency, relevance, and other task-specific aspects depending on the question templates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-based pretrained evaluator (as in UniEval paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Learning-based multidimensional evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>UniEval is a strong prior state-of-the-art; reported AVG Spearman on SummEval around rho = 0.474 (Table 1) and strong performance on QAGS and Topical-Chat benchmarks (see tables). G-EVAL-4 outperforms UniEval on many dimensions in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated learned evaluator validated by correlation with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Meta-evaluation via correlation with human ratings on benchmarks (SummEval, Topical-Chat, QAGS).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires supervised training using human-labeled data, potentially less generalizable to new tasks without retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>SummEval, Topical-Chat, QAGS (used for comparison in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4571.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4571.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QAGS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QAGS (Question-Answering based factual consistency evaluator and benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A question-answering based framework and benchmark for evaluating factual consistency (hallucination) in summarization: generate questions from summary, answer from source, compare answers to detect unsupported content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>QAGS</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate a set of questions from the summary, answer them using the source document with an QA model, and compare to answers derived from the summary; compute consistency scores to detect hallucinations. QAGS also exists as a benchmark (QAGS-CNN and QAGS-XSum) with human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Factual consistency / hallucination detection (does the summary contain facts not supported by the source?).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Summarization / factual consistency in NLG</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Factual-consistency evaluation (QA-based)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>G-EVAL-4 outperforms other evaluators on QAGS overall and substantially on the abstractive QAGS-XSum subset. Table 3 reports average G-EVAL-4 Pearson r = 0.599, Spearman rho = 0.611, Kendall-Tau tau = 0.525 across QAGS subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated evaluation method (QA pipeline); validated against human-annotated QAGS benchmark via correlation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation with human judgments on QAGS-CNN and QAGS-XSum.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>QA pipeline performance depends on quality of question generation and QA model; performs worse on more abstractive summaries unless robust QA is used.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>QAGS (includes QAGS-CNN and QAGS-XSum subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4571.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4571.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reference metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reference-based n-gram/lexical metrics (BLEU, ROUGE, METEOR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Traditional automatic metrics that measure lexical overlap between generated text and human reference text (e.g., BLEU, ROUGE, METEOR); widely used but show low correlation with human judgments for open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BLEU / ROUGE / METEOR (reference-based metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute n-gram precision/recall (BLEU/ROUGE) or alignment-based measures (METEOR) between candidate and reference texts; require reference outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Lexical overlap as proxy for quality; not decomposed into human-like dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Surface-similarity metrics</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>These metrics show relatively low correlation with human judgments on SummEval and Topical-Chat in this work (e.g., ROUGE variants have low Spearman/Kendall values in Table 1 and Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated; validated by computing correlation with human judgments (poor correspondence for open-ended tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Meta-evaluation of correlations on standard benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Require reference texts; poor correlation with human judgments on creative/open-ended tasks and fail to measure content quality and factual consistency reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>SummEval, Topical-Chat (used for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4571.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4571.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embedding-based metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embedding-based similarity metrics (BERTScore, MoverScore, WMD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Metrics that compare candidate and reference texts in embedding space to capture semantic similarity rather than exact lexical overlap (e.g., BERTScore, MoverScore, Word Mover's Distance).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Embedding-based metrics (BERTScore / MoverScore / WMD)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute pairwise token/sentence embeddings (contextualized where applicable), align tokens softly (MoverScore) or compute optimal transport distance (WMD), and aggregate to produce a similarity score between candidate and reference.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Semantic similarity to reference text; proxy for relevance and content overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Semantic similarity metrics</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Embedding-based metrics perform better than pure n-gram metrics but still underperform learned evaluators; BERTScore shows modest correlations reported in tables (e.g., Table 1/2/3).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated; validated via correlation with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation on SummEval, Topical-Chat, QAGS.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Still reference-dependent; may not capture factual consistency or diverse valid outputs well.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>SummEval, Topical-Chat, QAGS</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4571.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4571.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human-vs-LLM bias experiment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human vs LLM generated summary scoring analysis (bias experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An analysis comparing G-EVAL-4 scores assigned to human-written summaries versus GPT-3.5-generated summaries, stratified by human judges' preferences, revealing a systematic bias toward LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Comparative scoring of human vs LLM-generated outputs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use a dataset (from Zhang et al., 2023) where human annotators compared human-written and GPT-3.5 summaries. Group examples by whether humans prefer human summary, prefer GPT-3.5 summary, or consider them equal; compute average G-EVAL-4 scores for both human and GPT-3.5 summaries per group to detect preference/bias.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Average evaluator-assigned score per summary type, compared to human preference labels.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>G-EVAL-4 (GPT-4 used as evaluator) evaluating GPT-3.5 outputs and human summaries</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation / evaluation bias analysis</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Comparative bias analysis between human and LLM outputs</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>G-EVAL-4 assigns higher scores to GPT-3.5-generated summaries than to human-written summaries even in cases where humans prefer the human summaries. When human judges prefer humans, G-EVAL-4 still gives relatively higher scores to GPT-3.5 outputs on average. The paper cites low inter-annotator agreement for these judgments (Krippendorff's alpha = 0.07) as context.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: automated G-EVAL scores compared against human preference labels.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Descriptive statistical comparison of averaged evaluator scores by human-annotator preference groups; no formal statistical test reported beyond averages/plots.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Indicates potential evaluator bias toward LLM-style outputs; low human inter-annotator agreement complicates ground-truth; risk of self-reinforcement if used as reward in model training.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Dataset from Zhang et al. (2023) comparing human-written vs GPT-3.5 summaries (used for this bias analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4571.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4571.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Correlation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Correlation and agreement metrics (Spearman rho, Kendall-Tau tau, Pearson r, Krippendorff's alpha)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Statistical measures used to validate evaluators by comparing automatic scores to human judgments: Spearman and Kendall-Tau measure rank correlation, Pearson measures linear correlation; Krippendorff's alpha measures inter-annotator agreement among humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Spearman / Kendall-Tau / Pearson correlations and Krippendorff's alpha</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute Spearman rho (rank correlation) and Kendall-Tau tau to quantify agreement between metric rankings and human rankings; Pearson r to measure linear correlation for turn-level ratings; Krippendorff's alpha to report human annotator agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Used to validate evaluator correspondence with human judgments (ranking and agreement).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation methodology / statistics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Validation metrics for evaluator performance</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper reports these statistics extensively: e.g., SummEval G-EVAL-4 AVG Spearman rho = 0.514, Kendall-Tau tau = 0.418; Topical-Chat G-EVAL-4 AVG Pearson r = 0.575 and Spearman rho = 0.588; QAGS average Spearman rho for G-EVAL-4 = 0.611. Human inter-annotator agreement on human vs LLM summary preference dataset cited as Krippendorff's alpha = 0.07 (very low).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Used to quantify agreement between automated evaluator outputs and human judgments (hybrid validation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Direct computation of correlation coefficients between automated metric scores and human ratings across benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Different correlation measures emphasize different aspects (ranking vs pairwise concordance); ties in discrete scores affect Kendall-Tau; low human agreement limits the strength of validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Towards a unified multidimensional evaluator for text generation <em>(Rating: 2)</em></li>
                <li>Gptscore: Evaluate as you desire <em>(Rating: 2)</em></li>
                <li>Summeval: Re-evaluating summarization evaluation <em>(Rating: 2)</em></li>
                <li>Asking and answering questions to evaluate the factual consistency of summaries <em>(Rating: 2)</em></li>
                <li>Is chatgpt a good nlg evaluator? a preliminary study <em>(Rating: 2)</em></li>
                <li>Benchmarking large language models for news summarization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4571",
    "paper_id": "paper-381ab7a640f5b46b62f7e08d1af4a8e0d3eadd55",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "G-EVAL",
            "name_full": "G-EVAL (LLM-based NLG Evaluator with CoT and Form-Filling)",
            "brief_description": "A prompt-based evaluation framework that uses large language models (GPT-3.5 / GPT-4) with an automatically generated chain-of-thought and a form-filling output paradigm; final scalar scores are produced by probability-weighted summation over discrete score tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "G-EVAL",
            "evaluation_method_description": "Feed the LLM a Task Introduction and Evaluation Criteria, ask it to auto-generate a Chain-of-Thought (detailed Evaluation Steps), then evaluate candidate NLG outputs via a form-filling prompt that requests a discrete score (e.g., 1-5). To get fine-grained continuous scores, compute the probability p(si) of each discrete score token from the LLM and return score = sum_i p(si) * si. For models that do not expose token probabilities (GPT-4 in this work), approximate p(si) via repeated sampling (n=20) with temperature&gt;0 and estimate token frequencies.",
            "evaluation_criteria": "Task-dependent NLG dimensions such as coherence, fluency, consistency (factual consistency), relevance, conciseness/coverage, grammar; for dialogue: naturalness, coherence, engagingness, groundedness; for hallucination evaluation: factual consistency (does summary contain unsupported facts).",
            "model_name": "GPT-4 (G-EVAL-4) and GPT-3.5 (text-davinci-003, G-EVAL-3.5)",
            "model_size": null,
            "scientific_domain": "Natural Language Generation (summarization and dialogue evaluation)",
            "theory_type": "Evaluative judgments of generated texts (summaries, dialogue responses) — i.e., assessment of generated explanations/outputs rather than scientific-domain theories",
            "human_comparison": true,
            "evaluation_results": "G-EVAL-4 substantially outperforms prior metrics across benchmarks. On SummEval (summary-level) G-EVAL-4 average Spearman rho = 0.514 and Kendall-Tau tau = 0.418; on Topical-Chat (turn-level) G-EVAL-4 average Pearson r = 0.575 and Spearman rho = 0.588; on QAGS (hallucination/factual consistency) average Pearson r = 0.599, Spearman rho = 0.611, Kendall-Tau tau = 0.525. G-EVAL-3.5 performs worse than G-EVAL-4, especially on more abstractive/consistency-sensitive tasks (e.g., QAGS-XSum). Ablations show CoT and probability-weighting improve Spearman correlation; probability-weighting increases fine-grained ranking (higher Spearman) though can reduce Kendall-Tau due to fewer ties.",
            "automated_vs_human_evaluation": "Hybrid validation: automated LLM-based scoring is compared to human judgments. The evaluator itself is applied automatically (form-filling + probability weighting), and its outputs are validated by computing correlations (Spearman/Kendall/Pearson) with human ratings on standard human-annotated benchmarks.",
            "validation_method": "Meta-evaluation via correlation with human judgments on three benchmarks (SummEval, Topical-Chat, QAGS). Reported metrics include Spearman rho, Kendall-Tau tau, and Pearson r (turn-level where applicable).",
            "limitations_challenges": "Observed bias: G-EVAL tends to assign higher scores to LLM-generated summaries (GPT-3.5) than to human-written summaries even when humans prefer the human summaries; possible self-reinforcement if used as reward signal. Dependence on availability and versioning of LLMs (access/cost and changing model behavior). Integer score outputs produce ties and low variance unless probability-weighting is applied; some LLMs do not expose token probabilities (sampling required). Performance sensitive to LLM capacity (GPT-4 &gt; GPT-3.5 on harder tasks).",
            "benchmark_dataset": "SummEval (summarization with human ratings on fluency, coherence, consistency, relevance), Topical-Chat (dialogue response with human ratings on naturalness, coherence, engagingness, groundedness), QAGS (factual consistency/hallucination: QAGS-CNN and QAGS-XSum).",
            "uuid": "e4571.0",
            "source_info": {
                "paper_title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Auto-CoT",
            "name_full": "Automatic Chain-of-Thoughts generation for evaluation (Auto CoT)",
            "brief_description": "Use the LLM to generate intermediate evaluation steps (a chain-of-thought) automatically from the Task Introduction and Evaluation Criteria; include that CoT in the evaluation prompt to provide detailed guidance/context for scoring.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Automatic Chain-of-Thought (Auto-CoT) for Evaluation",
            "evaluation_method_description": "Prompt the LLM with the task definition and evaluation criteria and ask it to output 'Evaluation Steps' — a sequence of sub-steps describing how to assess the target dimension (e.g., read article, identify key points, compare ordering). Then use the generated CoT together with the source and candidate text in the scoring prompt to produce a score and explanation.",
            "evaluation_criteria": "Used to operationalize task-specific criteria (e.g., for coherence: identify main topic/key points, verify ordering and coverage; for engagingness: check interesting facts/opinions).",
            "model_name": "GPT-4 / GPT-3.5 (as used to generate CoT)",
            "model_size": null,
            "scientific_domain": "NLG evaluation (summarization, dialogue)",
            "theory_type": "Procedural evaluation steps for assessing generated explanations/texts",
            "human_comparison": false,
            "evaluation_results": "Including CoT improves correlation with human judgments; G-EVAL-4 with CoT shows higher Spearman correlations than G-EVAL-4 without CoT across SummEval dimensions (notably fluency and consistency). Exact ablation: G-EVAL-4 - CoT row in Table 1 shows AVG Spearman = 0.500 (vs 0.514 full) and other dimension improvements relative to simple prompts.",
            "automated_vs_human_evaluation": "Automated — CoT is generated and then used by the LLM evaluator; validated by correlation to human judgments.",
            "validation_method": "Ablation experiments on SummEval comparing G-EVAL with and without CoT.",
            "limitations_challenges": "Quality of generated CoT depends on LLM; CoT may encode evaluator's internal heuristics that bias scoring; generation of CoT is itself automated and may vary across model versions.",
            "benchmark_dataset": "SummEval (used for CoT ablation results)",
            "uuid": "e4571.1",
            "source_info": {
                "paper_title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Probability-weighted scoring",
            "name_full": "Probability-weighted summation of discrete score tokens",
            "brief_description": "Convert discrete LLM output token probabilities over score labels into a continuous score by summing p(score_label) * score_label to reduce ties and increase granularity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Probability-weighted scoring",
            "evaluation_method_description": "Given predefined discrete scores S = {s1,...,sn} (e.g., 1..5), compute the LLM's probability p(si) for each score token and return score = sum_i p(si) * si. For models without explicit token-probability API, estimate p(si) by sampling (n=20) and using empirical frequencies.",
            "evaluation_criteria": "Used for any of the NLG dimensions being scored (coherence, fluency, consistency, etc.) to obtain continuous/graded scores.",
            "model_name": "GPT-4 (probabilities approximated by sampling) and GPT-3.5 (token probs available for deterministic estimation)",
            "model_size": null,
            "scientific_domain": "NLG evaluation",
            "theory_type": "Scoring/aggregation approach for discrete evaluative labels",
            "human_comparison": false,
            "evaluation_results": "Probability-weighting increases Spearman correlation (better ranking agreement with humans) but can decrease Kendall-Tau in some settings because it removes ties (the paper reports higher Spearman for G-EVAL-4 with probabilities vs without). Example: G-EVAL-4 AVG Spearman 0.514 with probabilities vs 0.500 in an ablation; Kendall-Tau differences reported in Table 1.",
            "automated_vs_human_evaluation": "Automated scoring method validated against human judgments via correlation metrics.",
            "validation_method": "Ablation experiments comparing with/without probability normalization on SummEval and effect on Spearman/Kendall-Tau.",
            "limitations_challenges": "Requires access to token probabilities or reliable sampling; sampling-based estimation introduces variance; removing ties affects Kendall-Tau interpretation; may still reflect LLM biases.",
            "benchmark_dataset": "SummEval (primary ablation); also applied on Topical-Chat and QAGS evaluations.",
            "uuid": "e4571.2",
            "source_info": {
                "paper_title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Form-filling paradigm",
            "name_full": "Form-filling evaluation paradigm",
            "brief_description": "Ask the LLM to fill a predefined evaluation form (output specific fields/scores) instead of scoring by conditional log-probability of the candidate text, enabling direct adherence to evaluation criteria.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Form-filling paradigm for LLM evaluation",
            "evaluation_method_description": "Concatenate Task Introduction, auto-generated CoT, source context and candidate output, then prompt the LLM to write values into a short structured 'Evaluation Form' (e.g., 'Coherence: [1-5]'). The LLM's outputs (and their token probabilities) are used to generate scores.",
            "evaluation_criteria": "Any user-specified evaluative dimension; the paradigm is format-flexible and can include explanations.",
            "model_name": "GPT-4 / GPT-3.5",
            "model_size": null,
            "scientific_domain": "NLG evaluation",
            "theory_type": "Structured elicitation of evaluative judgments",
            "human_comparison": false,
            "evaluation_results": "Form-filling outperforms conditional-probability-based GPTScore in several dimensions; paper reports G-EVAL (form-filling) outperforms GPTScore in SummEval and other benchmarks.",
            "automated_vs_human_evaluation": "Automated scoring approach; validated by correlation with human judgments.",
            "validation_method": "Direct comparison with GPTScore and other baselines on SummEval/Topical-Chat/QAGS.",
            "limitations_challenges": "LLMs tend to produce integer labels (ties) unless probability weighting is applied; form design and prompt wording influence outcomes; possibility of LLM-specific evaluation heuristics.",
            "benchmark_dataset": "SummEval, Topical-Chat, QAGS",
            "uuid": "e4571.3",
            "source_info": {
                "paper_title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "GPTScore",
            "name_full": "GPTScore (conditional-probability evaluator)",
            "brief_description": "An evaluator that uses a generative pre-trained model to compute the conditional generation probability (likelihood) of the candidate text given context/instruction, under the assumption that higher-quality text has higher model probability.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "GPTScore (likelihood-based evaluation)",
            "evaluation_method_description": "Score candidate outputs by the conditional log-probability assigned by a generative LLM to the candidate given the prompt/context; no reference required. Contrasts with G-EVAL's form-filling approach.",
            "evaluation_criteria": "Implicit fluency/likelihood-based quality; not explicitly decomposed into task dimensions in original formulation.",
            "model_name": "GPT-3 / text-davinci-003 used in comparative experiments",
            "model_size": null,
            "scientific_domain": "NLG evaluation",
            "theory_type": "Likelihood-based ranking of generated outputs",
            "human_comparison": false,
            "evaluation_results": "In this paper GPTScore (with GPT-3.5) achieves lower average correlation than G-EVAL-4 on SummEval (G-EVAL-4 outperforms GPTScore on several dimensions as reported in Table 1). Exact GPTScore averages in Table 1: reported as AVG rho ~0.417 (GPTScore) vs 0.514 (G-EVAL-4).",
            "automated_vs_human_evaluation": "Automated; validated by correlation with human judgments (meta-evaluation).",
            "validation_method": "Compared to human ratings on SummEval and other benchmarks.",
            "limitations_challenges": "Relies on model likelihood which may confound style/verbosity with quality; lower human correspondence than targeted form-filling + CoT; may prefer text that fits model's internal distribution (bias).",
            "benchmark_dataset": "SummEval (primary comparison in this paper)",
            "uuid": "e4571.4",
            "source_info": {
                "paper_title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "UniEval",
            "name_full": "UniEval (unified multidimensional evaluator)",
            "brief_description": "A learned, unified evaluator that treats multiple evaluation aspects as QA tasks and uses a pretrained T5 to predict scores for different dimensions of generated text quality.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "UniEval",
            "evaluation_method_description": "Reformats evaluation into question-answer pairs (e.g., 'Is the summary factually consistent?') and uses a finetuned T5 model to produce scores across multiple dimensions; trained on human ratings.",
            "evaluation_criteria": "Coherence, consistency, fluency, relevance, and other task-specific aspects depending on the question templates.",
            "model_name": "T5-based pretrained evaluator (as in UniEval paper)",
            "model_size": null,
            "scientific_domain": "NLG evaluation",
            "theory_type": "Learning-based multidimensional evaluator",
            "human_comparison": false,
            "evaluation_results": "UniEval is a strong prior state-of-the-art; reported AVG Spearman on SummEval around rho = 0.474 (Table 1) and strong performance on QAGS and Topical-Chat benchmarks (see tables). G-EVAL-4 outperforms UniEval on many dimensions in this paper.",
            "automated_vs_human_evaluation": "Automated learned evaluator validated by correlation with human judgments.",
            "validation_method": "Meta-evaluation via correlation with human ratings on benchmarks (SummEval, Topical-Chat, QAGS).",
            "limitations_challenges": "Requires supervised training using human-labeled data, potentially less generalizable to new tasks without retraining.",
            "benchmark_dataset": "SummEval, Topical-Chat, QAGS (used for comparison in this paper)",
            "uuid": "e4571.5",
            "source_info": {
                "paper_title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "QAGS",
            "name_full": "QAGS (Question-Answering based factual consistency evaluator and benchmark)",
            "brief_description": "A question-answering based framework and benchmark for evaluating factual consistency (hallucination) in summarization: generate questions from summary, answer from source, compare answers to detect unsupported content.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "QAGS",
            "evaluation_method_description": "Generate a set of questions from the summary, answer them using the source document with an QA model, and compare to answers derived from the summary; compute consistency scores to detect hallucinations. QAGS also exists as a benchmark (QAGS-CNN and QAGS-XSum) with human annotations.",
            "evaluation_criteria": "Factual consistency / hallucination detection (does the summary contain facts not supported by the source?).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Summarization / factual consistency in NLG",
            "theory_type": "Factual-consistency evaluation (QA-based)",
            "human_comparison": false,
            "evaluation_results": "G-EVAL-4 outperforms other evaluators on QAGS overall and substantially on the abstractive QAGS-XSum subset. Table 3 reports average G-EVAL-4 Pearson r = 0.599, Spearman rho = 0.611, Kendall-Tau tau = 0.525 across QAGS subsets.",
            "automated_vs_human_evaluation": "Automated evaluation method (QA pipeline); validated against human-annotated QAGS benchmark via correlation metrics.",
            "validation_method": "Correlation with human judgments on QAGS-CNN and QAGS-XSum.",
            "limitations_challenges": "QA pipeline performance depends on quality of question generation and QA model; performs worse on more abstractive summaries unless robust QA is used.",
            "benchmark_dataset": "QAGS (includes QAGS-CNN and QAGS-XSum subsets)",
            "uuid": "e4571.6",
            "source_info": {
                "paper_title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Reference metrics",
            "name_full": "Reference-based n-gram/lexical metrics (BLEU, ROUGE, METEOR)",
            "brief_description": "Traditional automatic metrics that measure lexical overlap between generated text and human reference text (e.g., BLEU, ROUGE, METEOR); widely used but show low correlation with human judgments for open-ended tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "BLEU / ROUGE / METEOR (reference-based metrics)",
            "evaluation_method_description": "Compute n-gram precision/recall (BLEU/ROUGE) or alignment-based measures (METEOR) between candidate and reference texts; require reference outputs.",
            "evaluation_criteria": "Lexical overlap as proxy for quality; not decomposed into human-like dimensions.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLG evaluation",
            "theory_type": "Surface-similarity metrics",
            "human_comparison": false,
            "evaluation_results": "These metrics show relatively low correlation with human judgments on SummEval and Topical-Chat in this work (e.g., ROUGE variants have low Spearman/Kendall values in Table 1 and Table 2).",
            "automated_vs_human_evaluation": "Automated; validated by computing correlation with human judgments (poor correspondence for open-ended tasks).",
            "validation_method": "Meta-evaluation of correlations on standard benchmarks.",
            "limitations_challenges": "Require reference texts; poor correlation with human judgments on creative/open-ended tasks and fail to measure content quality and factual consistency reliably.",
            "benchmark_dataset": "SummEval, Topical-Chat (used for comparison)",
            "uuid": "e4571.7",
            "source_info": {
                "paper_title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Embedding-based metrics",
            "name_full": "Embedding-based similarity metrics (BERTScore, MoverScore, WMD)",
            "brief_description": "Metrics that compare candidate and reference texts in embedding space to capture semantic similarity rather than exact lexical overlap (e.g., BERTScore, MoverScore, Word Mover's Distance).",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Embedding-based metrics (BERTScore / MoverScore / WMD)",
            "evaluation_method_description": "Compute pairwise token/sentence embeddings (contextualized where applicable), align tokens softly (MoverScore) or compute optimal transport distance (WMD), and aggregate to produce a similarity score between candidate and reference.",
            "evaluation_criteria": "Semantic similarity to reference text; proxy for relevance and content overlap.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLG evaluation",
            "theory_type": "Semantic similarity metrics",
            "human_comparison": false,
            "evaluation_results": "Embedding-based metrics perform better than pure n-gram metrics but still underperform learned evaluators; BERTScore shows modest correlations reported in tables (e.g., Table 1/2/3).",
            "automated_vs_human_evaluation": "Automated; validated via correlation with human judgments.",
            "validation_method": "Correlation on SummEval, Topical-Chat, QAGS.",
            "limitations_challenges": "Still reference-dependent; may not capture factual consistency or diverse valid outputs well.",
            "benchmark_dataset": "SummEval, Topical-Chat, QAGS",
            "uuid": "e4571.8",
            "source_info": {
                "paper_title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Human-vs-LLM bias experiment",
            "name_full": "Human vs LLM generated summary scoring analysis (bias experiment)",
            "brief_description": "An analysis comparing G-EVAL-4 scores assigned to human-written summaries versus GPT-3.5-generated summaries, stratified by human judges' preferences, revealing a systematic bias toward LLM outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Comparative scoring of human vs LLM-generated outputs",
            "evaluation_method_description": "Use a dataset (from Zhang et al., 2023) where human annotators compared human-written and GPT-3.5 summaries. Group examples by whether humans prefer human summary, prefer GPT-3.5 summary, or consider them equal; compute average G-EVAL-4 scores for both human and GPT-3.5 summaries per group to detect preference/bias.",
            "evaluation_criteria": "Average evaluator-assigned score per summary type, compared to human preference labels.",
            "model_name": "G-EVAL-4 (GPT-4 used as evaluator) evaluating GPT-3.5 outputs and human summaries",
            "model_size": null,
            "scientific_domain": "NLG evaluation / evaluation bias analysis",
            "theory_type": "Comparative bias analysis between human and LLM outputs",
            "human_comparison": true,
            "evaluation_results": "G-EVAL-4 assigns higher scores to GPT-3.5-generated summaries than to human-written summaries even in cases where humans prefer the human summaries. When human judges prefer humans, G-EVAL-4 still gives relatively higher scores to GPT-3.5 outputs on average. The paper cites low inter-annotator agreement for these judgments (Krippendorff's alpha = 0.07) as context.",
            "automated_vs_human_evaluation": "Hybrid: automated G-EVAL scores compared against human preference labels.",
            "validation_method": "Descriptive statistical comparison of averaged evaluator scores by human-annotator preference groups; no formal statistical test reported beyond averages/plots.",
            "limitations_challenges": "Indicates potential evaluator bias toward LLM-style outputs; low human inter-annotator agreement complicates ground-truth; risk of self-reinforcement if used as reward in model training.",
            "benchmark_dataset": "Dataset from Zhang et al. (2023) comparing human-written vs GPT-3.5 summaries (used for this bias analysis)",
            "uuid": "e4571.9",
            "source_info": {
                "paper_title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Correlation metrics",
            "name_full": "Correlation and agreement metrics (Spearman rho, Kendall-Tau tau, Pearson r, Krippendorff's alpha)",
            "brief_description": "Statistical measures used to validate evaluators by comparing automatic scores to human judgments: Spearman and Kendall-Tau measure rank correlation, Pearson measures linear correlation; Krippendorff's alpha measures inter-annotator agreement among humans.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Spearman / Kendall-Tau / Pearson correlations and Krippendorff's alpha",
            "evaluation_method_description": "Compute Spearman rho (rank correlation) and Kendall-Tau tau to quantify agreement between metric rankings and human rankings; Pearson r to measure linear correlation for turn-level ratings; Krippendorff's alpha to report human annotator agreement.",
            "evaluation_criteria": "Used to validate evaluator correspondence with human judgments (ranking and agreement).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Evaluation methodology / statistics",
            "theory_type": "Validation metrics for evaluator performance",
            "human_comparison": false,
            "evaluation_results": "Paper reports these statistics extensively: e.g., SummEval G-EVAL-4 AVG Spearman rho = 0.514, Kendall-Tau tau = 0.418; Topical-Chat G-EVAL-4 AVG Pearson r = 0.575 and Spearman rho = 0.588; QAGS average Spearman rho for G-EVAL-4 = 0.611. Human inter-annotator agreement on human vs LLM summary preference dataset cited as Krippendorff's alpha = 0.07 (very low).",
            "automated_vs_human_evaluation": "Used to quantify agreement between automated evaluator outputs and human judgments (hybrid validation).",
            "validation_method": "Direct computation of correlation coefficients between automated metric scores and human ratings across benchmarks.",
            "limitations_challenges": "Different correlation measures emphasize different aspects (ranking vs pairwise concordance); ties in discrete scores affect Kendall-Tau; low human agreement limits the strength of validation.",
            "uuid": "e4571.10",
            "source_info": {
                "paper_title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Towards a unified multidimensional evaluator for text generation",
            "rating": 2
        },
        {
            "paper_title": "Gptscore: Evaluate as you desire",
            "rating": 2
        },
        {
            "paper_title": "Summeval: Re-evaluating summarization evaluation",
            "rating": 2
        },
        {
            "paper_title": "Asking and answering questions to evaluate the factual consistency of summaries",
            "rating": 2
        },
        {
            "paper_title": "Is chatgpt a good nlg evaluator? a preliminary study",
            "rating": 2
        },
        {
            "paper_title": "Benchmarking large language models for news summarization",
            "rating": 1
        }
    ],
    "cost": 0.01892675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment</h1>
<p>Yang Liu Dan Iter Yichong Xu<br>Shuohang Wang Ruochen Xu Chenguang Zhu<br>Microsoft Azure AI<br>yaliu10@microsoft.com</p>
<h4>Abstract</h4>
<p>The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional referencebased metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-EVAL, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-EVAL with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose analysis on the behavior of LLM-based evaluators, and highlight the potential concern of LLM-based evaluators having a bias towards the LLM-generated texts. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Evaluating the quality of natural language generation systems is a challenging problem even when large language models can generate high-quality and diverse texts that are often indistinguishable from human-written texts (Ouyang et al., 2022). Traditional automatic metrics, such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005), are widely used for NLG evaluation, but they have been shown to have relatively low correlation with human judgments, especially for open-ended generation tasks.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Moreover, these metrics require associated reference output, which is costly to collect for new tasks.</p>
<p>Recent studies propose directly using LLMs as reference-free NLG evaluators (Fu et al., 2023; Wang et al., 2023a). The idea is to use the LLMs to score the candidate output based on its generation probability without any reference target, under the assumption that the LLMs have learned to assign higher probabilities to high-quality and fluent texts. Meanwhile, it is becoming popular to use more powerful LLMs like GPT-4 to evaluate smaller or student models, like in Alpaca (Taori et al., 2023) and Vicuna (Zheng et al., 2023). However, the validity and reliability of using LLMs as NLG evaluators have not been systematically investigated. In addition, meta-evaluations show that these LLMbased evaluators still have lower human correspondence than medium-size neural evaluators (Zhong et al., 2022). Thus, there is a need for a more effective and reliable framework for using LLMs for NLG evaluation.</p>
<p>In this paper, we propose G-EVAL, a framework of using LLMs with chain-of-thoughts (CoT) (Wei et al., 2022) to evaluate the quality of generated texts in a form-filling paradigm. By only feeding the Task Introduction and the Evaluation Criteria as a prompt, we ask LLMs to generate a CoT of detailed Evaluation Steps. Then we use the prompt along with the generated CoT to evaluate the NLG outputs. The evaluator output is formatted as a form. Moreover, the probabilities of the output rating tokens can be used to refine the final metric. We conduct extensive experiments on three meta-evaluation benchmarks of two NLG tasks: text summarization and dialogue generation. The results show that G-EVAL can outperform existing NLG evaluators by a large margin in terms of correlation with human evaluations. Finally, we conduct analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluator having a bias towards the LLM-generated</p>
<p>texts.
To summarize, our main contributions and findings in this paper are:</p>
<ol>
<li>G-EVAL generally outperforms referencebased and reference-free baseline metrics in terms of correlation with human quality judgments, especially for open-ended and creative NLG tasks, such as dialogue response generation.</li>
<li>We propose to use automatic chain-of-thought to improve the performance of LLM-based evaluators by providing more context and guidance.</li>
<li>We propose to re-weight the discrete scores by their respective token probabilities to provide a more fine-grained continuous score for GEVAL.</li>
<li>We conduct an analysis of the potential issue that LLM-based metrics have a preference of LLM-generated texts over humanwritten texts, which may lead to the selfreinforcement of LLMs if LLM-based metrics are used as the reward signal for improving themselves.</li>
</ol>
<h2>2 Method</h2>
<p>G-EVAL is a prompt-based evaluator with three main components: 1) a prompt that contains the definition of the evaluation task and the desired evaluation criteria, 2) a chain-of-thoughts (CoT) that is a set of intermediate instructions generated by the LLM describing the detailed evaluation steps, and 3) a scoring function that calls LLM and calculates the score based on the probabilities of the return tokens.</p>
<p>Prompt for NLG Evaluation The prompt is a natural language instruction that defines the evaluation task and the desired evaluation criteria. For example, for text summarization, the prompt can be:</p>
<p>You will be given one summary written for a news article. Your task is to rate the summary on one metric.</p>
<p>Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.</p>
<p>The prompt should also contain customized evaluation criteria for different NLG tasks and, such as coherence, conciseness, or grammar. For example, for evaluating coherence in text summarization, we add the following content to the prompt:</p>
<h2>Evaluation Criteria:</h2>
<p>Coherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby "the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic."</p>
<h2>Auto Chain-of-Thoughts for NLG Evaluation</h2>
<p>The chain-of-thoughts (CoT) is a sequence of intermediate representations that are generated by the LLM during the text generation process. For evaluation tasks, some criteria need a more detailed evaluation instruction beyond the simple definition, and it is time-consuming to manually design such evaluation steps for each task. We find that LLM can generate such evaluation steps by itself. The CoT can provide more context and guidance for the LLM to evaluate the generated text, and can also help to explain the evaluation process and results. For example, for evaluating coherence in text summarization, we add a line of "Evaluation Steps:" to the prompt and let LLM to generate the following CoT automatically:</p>
<ol>
<li>Read the news article carefully and identify the main topic and key points.</li>
<li>Read the summary and compare it to the news article. Check if the summary covers the main topic and key points of the news article, and if it presents them in a clear and logical order.</li>
<li>Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.</li>
</ol>
<p>Scoring Function The scoring function calls the LLM with the designed prompt, auto CoT, the input context and the target text that needs to be evaluated. Unlike GPTScore (Fu et al., 2023) which uses</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The overall framework of G-EVAL. We first input Task Introduction and Evaluation Criteria to the LLM, and ask it to generate a CoT of detailed Evaluation Steps. Then we use the prompt along with the generated CoT to evaluate the NLG outputs in a form-filling paradigm. Finally, we use the probability-weighted summation of the output scores as the final score.
the conditional probability of generating the target text as an evaluation metric, G-EVAL directly performs the evaluation task with a form-filling paradigm. This provides a more flexible way to evaluate the text as the model can behave directly based on the evaluation criteria and steps. For example, for evaluating coherence in text summarization, we concatenate the prompt, the CoT, the news article, and the summary, and then call the LLM to output a score from 1 to 5 for each evaluation aspect, based on the defined criteria.</p>
<p>However, we notice this direct scoring function has two issues:</p>
<ol>
<li>For some evaluation tasks, one digit usually dominates the distribution of the scores, such as 3 for a 1 - 5 scale. This may lead to the low variance of the scores and the low correlation with human judgments.</li>
<li>LLMs usually only output integer scores, even when the prompt explicitly requests decimal values. This leads to many ties in evaluation scores which do not capture the subtle difference between generated texts.</li>
</ol>
<p>To address these issues, we propose using the
probabilities of output tokens from LLMs to normalize the scores and take their weighted summation as the final results. Formally, given a set of scores (like from 1 to 5) predefined in the prompt $S=\left{s_{1}, s_{2}, \ldots, s_{n}\right}$, the probability of each score $p\left(s_{i}\right)$ is calculated by the LLM, and the final score is:</p>
<p>$$
\operatorname{score}=\sum_{i=1}^{n} p\left(s_{i}\right) \times s_{i}
$$</p>
<p>This method obtains more fine-grained, continuous scores that better reflect the quality and diversity of the generated texts.</p>
<h2>3 Experiments</h2>
<p>Following Zhong et al. (2022), we meta-evaluate our evaluator on three benchmarks, SummEval, Topical-Chat and QAGS, of two NLG tasks, summarization and dialogue response generation.</p>
<h3>3.1 Implementation Details</h3>
<p>We use OpenAI's GPT family as our LLMs, including GPT-3.5 (text-davinci-003) and GPT-4. For GPT-3.5, we set decoding temperature to 0 to increase the model's determinism. For GPT-4, as it</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metrics</th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Consistency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relevance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">AVG</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\tau$</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-1</td>
<td style="text-align: center;">0.167</td>
<td style="text-align: center;">0.126</td>
<td style="text-align: center;">0.160</td>
<td style="text-align: center;">0.130</td>
<td style="text-align: center;">0.115</td>
<td style="text-align: center;">0.094</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.252</td>
<td style="text-align: center;">0.192</td>
<td style="text-align: center;">0.150</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-2</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.139</td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.155</td>
<td style="text-align: center;">0.159</td>
<td style="text-align: center;">0.128</td>
<td style="text-align: center;">0.290</td>
<td style="text-align: center;">0.219</td>
<td style="text-align: center;">0.205</td>
<td style="text-align: center;">0.161</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">0.128</td>
<td style="text-align: center;">0.099</td>
<td style="text-align: center;">0.115</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">0.105</td>
<td style="text-align: center;">0.084</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">0.237</td>
<td style="text-align: center;">0.165</td>
<td style="text-align: center;">0.128</td>
</tr>
<tr>
<td style="text-align: center;">BERTScore</td>
<td style="text-align: center;">0.284</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.110</td>
<td style="text-align: center;">0.090</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">0.158</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.243</td>
<td style="text-align: center;">0.225</td>
<td style="text-align: center;">0.175</td>
</tr>
<tr>
<td style="text-align: center;">MOVERSscore</td>
<td style="text-align: center;">0.159</td>
<td style="text-align: center;">0.118</td>
<td style="text-align: center;">0.157</td>
<td style="text-align: center;">0.127</td>
<td style="text-align: center;">0.129</td>
<td style="text-align: center;">0.105</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">0.191</td>
<td style="text-align: center;">0.148</td>
</tr>
<tr>
<td style="text-align: center;">BARTScore</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">0.382</td>
<td style="text-align: center;">0.315</td>
<td style="text-align: center;">0.356</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.356</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.305</td>
</tr>
<tr>
<td style="text-align: center;">UniEval</td>
<td style="text-align: center;">0.575</td>
<td style="text-align: center;">0.442</td>
<td style="text-align: center;">0.446</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.426</td>
<td style="text-align: center;">0.325</td>
<td style="text-align: center;">0.474</td>
<td style="text-align: center;">0.377</td>
</tr>
<tr>
<td style="text-align: center;">GPTScore</td>
<td style="text-align: center;">0.434</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.403</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.381</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">G-EVAL-3.5</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.386</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.347</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.401</td>
<td style="text-align: center;">0.320</td>
</tr>
<tr>
<td style="text-align: center;">- Probs</td>
<td style="text-align: center;">0.359</td>
<td style="text-align: center;">0.313</td>
<td style="text-align: center;">0.361</td>
<td style="text-align: center;">0.344</td>
<td style="text-align: center;">0.339</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.317</td>
</tr>
<tr>
<td style="text-align: center;">G-EVAL-4</td>
<td style="text-align: center;">0.582</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.506</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.547</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">0.514</td>
<td style="text-align: center;">0.418</td>
</tr>
<tr>
<td style="text-align: center;">- Probs</td>
<td style="text-align: center;">0.560</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.501</td>
<td style="text-align: center;">0.459</td>
<td style="text-align: center;">0.505</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">0.511</td>
<td style="text-align: center;">0.444</td>
<td style="text-align: center;">0.502</td>
<td style="text-align: center;">0.446</td>
</tr>
<tr>
<td style="text-align: center;">- CoT</td>
<td style="text-align: center;">0.564</td>
<td style="text-align: center;">0.454</td>
<td style="text-align: center;">0.493</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.483</td>
<td style="text-align: center;">0.431</td>
<td style="text-align: center;">0.538</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.407</td>
</tr>
<tr>
<td style="text-align: center;">- Description</td>
<td style="text-align: center;">0.513</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.344</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">0.373</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.388</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.377</td>
</tr>
</tbody>
</table>
<p>Table 1: Summary-level Spearman ( $\rho$ ) and Kendall-Tau ( $\tau$ ) correlations of different metrics on SummEval benchmark. G-EVAL without probabilities (italicized) should not be considered as a fair comparison to other metrics on $\tau$, as it leads to many ties in the scores. This results in a higher Kendall-Tau correlation, but it does not fairly reflect the true evaluation ability. More details are in Section 4.
does not support the output of token probabilities, we set ' $n=20$, temperature $=1$, top_ $p=1$ ' to sample 20 times to estimate the token probabilities. We use G-EVAL-4 to indicate G-EVAL with GPT-4 as the backbone model, and G-EVAL-3.5 to indicate G-EVAL with GPT-3.5 as the backbone model. Example prompts for each task are provided in the Appendix.</p>
<h3>3.2 Benchmarks</h3>
<p>We adopt three meta-evaluation benchmarks to measure the correlation between G-EVAL and human judgments.</p>
<p>SummEval (Fabbri et al., 2021) is a benchmark that compares different evaluation methods for summarization. It gives human ratings for four aspects of each summary: fluency, coherence, consistency and relevance. It is built on the CNN/DailyMail dataset (Hermann et al., 2015)</p>
<p>Topical-Chat (Mehri and Eskenazi, 2020) is a testbed for meta-evaluating different evaluators on dialogue response generation systems that use knowledge. We follow (Zhong et al., 2022) to use its human ratings on four aspects: naturalness, coherence, engagingness and groundedness.</p>
<p>QAGS (Wang et al., 2020) is a benchmark for evaluating hallucinations in the summarization
task. It aims to measure the consistency dimension of summaries by asking and answering questions. It is collected from two different news summarization datasets CNN/DailyMail and XSum.</p>
<h3>3.3 Baselines</h3>
<p>We evaluate G-EVAL against various evaluators that achieved state-of-the-art performance.</p>
<p>BERTScore (Zhang et al., 2019) measures the similarity between two texts based on the contextualized embedding from BERT (Devlin et al., 2019).</p>
<p>MoverScore (Zhao et al., 2019) improves BERTScore by adding soft alignments and new aggregation methods to obtain a more robust similarity measure.</p>
<p>BARTScore (Yuan et al., 2021) is a unified evaluator which evaluate with the average likelihood of the pretrained encoder-decoder model, BART (Lewis et al., 2020). It can predict different scores depending on the formats of source and target.</p>
<p>FactCC and QAGS (Kryściński et al., 2020; Wang et al., 2020) are two evaluators that measure the factual consistency of generated summaries. FactCC is a BERT-based classifier that predicts whether a summary is consistent with the source document. QAGS is a question-answering based evaluator that generates questions from the summary and checks if the answers can be found in the source document.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metrics</th>
<th style="text-align: center;">Naturalness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Engagingness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Groundedness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">AVG</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\rho$</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.146</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.295</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.243</td>
<td style="text-align: center;">0.244</td>
</tr>
<tr>
<td style="text-align: center;">BLEU-4</td>
<td style="text-align: center;">0.180</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.131</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.232</td>
<td style="text-align: center;">0.316</td>
<td style="text-align: center;">0.213</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.259</td>
</tr>
<tr>
<td style="text-align: center;">METEOR</td>
<td style="text-align: center;">0.212</td>
<td style="text-align: center;">0.191</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.290</td>
<td style="text-align: center;">0.331</td>
</tr>
<tr>
<td style="text-align: center;">BERTScore</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.209</td>
<td style="text-align: center;">0.214</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.317</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.317</td>
<td style="text-align: center;">0.262</td>
<td style="text-align: center;">0.273</td>
</tr>
<tr>
<td style="text-align: center;">USR</td>
<td style="text-align: center;">0.337</td>
<td style="text-align: center;">0.325</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.377</td>
<td style="text-align: center;">0.456</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.222</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">0.358</td>
<td style="text-align: center;">0.403</td>
</tr>
<tr>
<td style="text-align: center;">UniEval</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.330</td>
<td style="text-align: center;">0.602</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.573</td>
<td style="text-align: center;">0.430</td>
<td style="text-align: center;">0.577</td>
<td style="text-align: center;">0.453</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.417</td>
</tr>
<tr>
<td style="text-align: center;">G-EVAL-3.5</td>
<td style="text-align: center;">0.532</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;">0.519</td>
<td style="text-align: center;">0.544</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.586</td>
<td style="text-align: center;">0.567</td>
<td style="text-align: center;">0.574</td>
<td style="text-align: center;">0.585</td>
</tr>
<tr>
<td style="text-align: center;">G-EVAL-4</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.565</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.627</td>
<td style="text-align: center;">0.631</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.551</td>
<td style="text-align: center;">0.575</td>
<td style="text-align: center;">0.588</td>
</tr>
</tbody>
</table>
<p>Table 2: Turn-level Spearman ( $\rho$ ) and Kendall-Tau ( $\tau$ ) correlations of different metrics on Topical-Chat benchmark.</p>
<p>USR (Mehri and Eskenazi, 2020) is evaluator that assesses dialogue response generation from different perspectives. It has several versions that assign different scores to each target response.</p>
<p>UniEval (Zhong et al., 2022) is a unified evaluator that can evaluate different aspects of text generation as QA tasks. It uses a pretrained T5 model (Raffel et al., 2020) to encode the evaluation task, source and target texts as questions and answers, and then computes the QA score as the evaluation score. It can also handle different evaluation tasks by changing the question format.</p>
<p>GPTScore (Fu et al., 2023) is a new framework that evaluates texts with generative pre-training models like GPT-3. It assumes that a generative pre-training model will assign a higher probability of high-quality generated text following a given instruction and context. Unlike G-EVAL, GPTScore formulates the evaluation task as a conditional generation problem instead of a form-filling problem. We report the score of GPTScore with GPT3-text-davinci-003 as the LLM, which is also usually referred as GPT-3.5.</p>
<h3>3.4 Results for Summarization</h3>
<p>We adopt the same approach as Zhong et al. (2022) to evaluate different summarization metrics using summary-level Spearman and Kendall-Tau correlation. The first part of Table 1 shows the results of metrics that compare the semantic similarity between the model output and the reference text. These metrics perform poorly on most dimensions. The second part shows the results of metrics that use neural networks to learn from human ratings of summary quality. These metrics have much higher correlations than the similarity-based metrics, suggesting that they are more reliable for summarization evaluation.</p>
<p>In the last part of Table 1 which corresponds to GPT-based evaluators, GPTScore also uses GPTs for evaluating summarization texts, but relies on GPT's conditional probabilities of the given target. G-EVAL substantially surpasses all previous state-of-the-art evaluators on the SummEval benchmark. G-EVAL-4 achieved much higher human correspondence compared with G-EVAL-3.5 on both Spearman and Kendall-Tau correlation, which indicates that the larger model size of GPT-4 is beneficial for summarization evaluation. G-EVAL also outperforms GPTScore on several dimension, demonstrating the effectiveness of the simple formfilling paradigm.</p>
<h3>3.5 Results for Dialogue Generation</h3>
<p>We use the Topical-chat benchmark from Mehri and Eskenazi (2020) to measure how well different evaluators agree with human ratings on the quality of dialogue responses. We calculate the Pearson and Spearman correlation for each turn of the dialogue. Table 2 shows that similarity-based metrics have good agreement with humans on how engaging and grounded the responses are, but not on the other aspects. With respect to the learningbased evaluators, before G-EVAL, UniEval predicts scores that are most consistent with human judgments across all aspects.</p>
<p>As shown in the last part, G-EVAL also substantially surpasses all previous state-of-the-art evaluator on the Topical-Chat benchmark. Notably, the G-EVAL-3.5 can achieve similar results with G-EVAL-4. This indicates that this benchmark is relatively easy for the G-EVAL model.</p>
<h3>3.6 Results on Hallucinations</h3>
<p>Advanced NLG models often produce text that does not match the context input (Cao et al., 2018), and</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metrics</th>
<th style="text-align: center;">QAGS-CNN</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">QAGS-XSUM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Average</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\tau$</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-2</td>
<td style="text-align: center;">0.459</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.097</td>
<td style="text-align: center;">0.083</td>
<td style="text-align: center;">0.068</td>
<td style="text-align: center;">0.278</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.200</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">0.357</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">0.254</td>
<td style="text-align: center;">0.024</td>
<td style="text-align: center;">-0.011</td>
<td style="text-align: center;">-0.009</td>
<td style="text-align: center;">0.190</td>
<td style="text-align: center;">0.156</td>
<td style="text-align: center;">0.122</td>
</tr>
<tr>
<td style="text-align: center;">BERTScore</td>
<td style="text-align: center;">0.576</td>
<td style="text-align: center;">0.505</td>
<td style="text-align: center;">0.399</td>
<td style="text-align: center;">0.024</td>
<td style="text-align: center;">0.008</td>
<td style="text-align: center;">0.006</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.256</td>
<td style="text-align: center;">0.202</td>
</tr>
<tr>
<td style="text-align: center;">MoverScore</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">0.347</td>
<td style="text-align: center;">0.271</td>
<td style="text-align: center;">0.054</td>
<td style="text-align: center;">0.044</td>
<td style="text-align: center;">0.036</td>
<td style="text-align: center;">0.234</td>
<td style="text-align: center;">0.195</td>
<td style="text-align: center;">0.153</td>
</tr>
<tr>
<td style="text-align: center;">FactCC</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.484</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.212</td>
<td style="text-align: center;">0.356</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.294</td>
</tr>
<tr>
<td style="text-align: center;">QAGS</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.375</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BARTScore</td>
<td style="text-align: center;">0.735</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.159</td>
<td style="text-align: center;">0.130</td>
<td style="text-align: center;">0.459</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.343</td>
</tr>
<tr>
<td style="text-align: center;">CTC</td>
<td style="text-align: center;">0.619</td>
<td style="text-align: center;">0.564</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.295</td>
<td style="text-align: center;">0.242</td>
<td style="text-align: center;">0.464</td>
<td style="text-align: center;">0.430</td>
<td style="text-align: center;">0.346</td>
</tr>
<tr>
<td style="text-align: center;">UniEval</td>
<td style="text-align: center;">0.682</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">0.532</td>
<td style="text-align: center;">0.461</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">0.399</td>
<td style="text-align: center;">0.571</td>
<td style="text-align: center;">0.575</td>
<td style="text-align: center;">0.465</td>
</tr>
<tr>
<td style="text-align: center;">G-EVAL-3.5</td>
<td style="text-align: center;">0.477</td>
<td style="text-align: center;">0.516</td>
<td style="text-align: center;">0.410</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: center;">0.344</td>
<td style="text-align: center;">0.461</td>
<td style="text-align: center;">0.377</td>
</tr>
<tr>
<td style="text-align: center;">G-EVAL-4</td>
<td style="text-align: center;">0.631</td>
<td style="text-align: center;">0.685</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.558</td>
<td style="text-align: center;">0.537</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.599</td>
<td style="text-align: center;">0.611</td>
<td style="text-align: center;">0.525</td>
</tr>
</tbody>
</table>
<p>Table 3: Pearson $(r)$, Spearman $(\rho)$ and Kendall-Tau $(\tau)$ correlations of different metrics on QAGS benchmark.
recent studies find even powerful LLMs also suffer from the problem of hallucination. This motivates recent research to design evaluators for measuring the consistency aspect in summarization (Kryściński et al., 2020; Wang et al., 2020; Cao et al., 2020; Durmus et al., 2020). We test the QAGS meta-evaluation benchmark, which includes two different summarization datasets: CNN/DailyMail and XSum (Narayan et al., 2018) Table 3 shows that BARTScore performs well on the more extractive subset (QAGS-CNN), but has low correlation on the more abstractive subset (QAGS-Xsum). UniEval has good correlation on both subsets of the data.</p>
<p>On average, G-EVAL-4 outperforms all state-of-the-art evaluators on QAGS, with a large margin on QAGS-Xsum. G-EVAL-3.5, on the other hand, failed to perform well on this benchmark, which indicates that the consistency aspect is sensitive to the LLM's capacity. This result is consistent with Table 1.</p>
<h2>4 Analysis</h2>
<p>Will G-EVAL prefer LLM-based outputs? One concern about using LLM as an evaluator is that it may prefer the outputs generated by the LLM itself, rather than the high-quality human-written texts. To investigate this issue, we conduct an experiment on the summarization task, where we compare the evaluation scores of the LLM-generated and the human-written summaries. We use the dataset collected in Zhang et al. (2023), where they first ask freelance writers to write high-quality summaries for news articles, and then ask annotators to compare human-written summaries and LLM-
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Averaged G-EVAL-4's scores for humanwritten summaries and GPT-3.5 summaries, divided by human judges' preference.
generated summaries (using GPT-3.5, text-davinci003).</p>
<p>The dataset can be divided in three categories: 1) human-written summaries that are rated higher than GPT-3.5 summaries by human judges, 2) human-written summaries that are rated lower than GPT-3.5 summaries by human judges, and 3) human-written summaries and GPT-3.5 summaries are rated equally good by human judges. We use GEVAL-4 to evaluate the summaries in each category, and compare the averaged scores. ${ }^{2}$</p>
<p>The results are shown in Figure 2. We can see that, G-EVAL-4 assigns higher scores to humanwritten summaries when human judges also prefer human-written summaries, and assigns lower scores when human judges prefer GPT-3.5 summaries. However, G-EVAL-4 always gives higher scores to GPT-3.5 summaries than human-written</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>summaries, even when human judges prefer humanwritten summaries. We propose two potential reasons for this phenomenon:</p>
<ol>
<li>NLG outputs from high-quality systems are in natural difficult to evaluate. The authors of the original paper found that inter-annotator agreement on judging human-written and LLM-generated summaries is very low, with Krippendorff's alpha at 0.07 .</li>
<li>G-EVAL may have a bias towards the LLMgenerated summaries because the model could share the same concept of evaluation criteria during generation and evaluation.</li>
</ol>
<p>Our work should be considered as a preliminary study on this issue, and more research is needed to fully understand the behavior of LLM-based evaluators to reduce its inherent bias towards LLMgenerated text. We highlight this concern in the context that LLM-based evaluators may lead to self-reinforcement of LLMs if the evaluation score is used as a reward signal for further tuning. And this could result in the over-fitting of the LLMs to their own evaluation criteria, rather than the true evaluation criteria of the NLG tasks.</p>
<p>The Effect of Chain-of-Thoughts We compare the performance of G-EVAL with and without chain-of-thoughts (CoT) on the SummEval benchmark. Table 1 shows that G-EVAL-4 with CoT has higher correlation than G-EVAL-4 without CoT on all dimensions, especially for fluency. This suggests that CoT can provide more context and guidance for the LLM to evaluate the generated text, and can also help to explain the evaluation process and results. And it is shown that CoT is more useful on consistency and fluency dimensions. We also provide results of G-EVAL with a simple prompting baseline on SummEval (only asking GPT-4 to score a summary from 1-5 on each dimension, without detailed task introduction, evaluation criteria and CoT).</p>
<p>The Effect of Probability Normalization We compare the performance of G-EVAL with and without probability normalization on the SummEval benchmark. Table 1 shows that, on KendallTau correlation, G-EVAL-4 with probabilities is inferior to G-EVAL-4 without probabilities on SummEval. We believe this is related to the calculation of Kendall-Tau correlation, which is based on the number of concordant and discordant pairs. Direct
scoring without probabilities can lead to many ties, which are not counted as either concordant or discordant. This may result in a higher Kendall-Tau correlation, but it does not reflect the model's true capacity of evaluating the generated texts. On the other hand, probability normalization can obtain more fine-grained, continuous scores that better capture the subtle difference between generated texts. This is reflected by the higher Spearman correlation of G-EVAL-4 with probabilities, which is based on the rank order of the scores.</p>
<p>The Effect of Different LLMs We compare the performance of G-EVAL with different LLMs on the SummEval and QAGS benchmarks. Table 1 and Table 3 show that G-EVAL-4 has higher correlation than G-EVAL-3.5 on most dimensions and datasets, except for engagingness and groundedness on the Topical-Chat benchmark. This demonstrates that a better LLM can improve the performance of G-EVAL, especially for more challenging and complex evaluation tasks, such as consistency and relevance.</p>
<h2>5 Related Work</h2>
<p>Ngram-based Metrics Ngram-based metrics refer to the scores for evaluating the NLG models by measuring the lexical overlap between a generated text and a reference text. BLEU (Papineni et al., 2002) is the most widely used metric for machine translation evaluation, which calculates the geometric mean of modified n-gram precision and a brevity penalty. ROUGE (Lin, 2004) is a recall-oriented metric for summarization evaluation, which measures the n-gram overlap between a generated summary and a set of reference summaries. It has been shown that more than $60 \%$ of recent papers on NLG only rely on ROUGE or BLEU to evaluate their systems (Kasai et al., 2022). However, these metrics fail to measure content quality (Reiter and Belz, 2009) or capture syntactic errors (Stent et al., 2005), and therefore do not reflect the reliability of NLG systems accurately.</p>
<p>Embedding-based Metrics Embedding-based metrics refer to the scores for evaluating the NLG models by measuring the semantic similarity between a generated text and a reference text based on the word or sentence embeddings. WMD (Kusner et al., 2015) is a metric that measures the distance between two texts based on the word embeddings. BERTScore (Zhang et al., 2019) measures</p>
<p>the similarity between two texts based on the contextualized embedding from BERT (Devlin et al., 2019). MoverScore (Zhao et al., 2019) improves BERTScore by adding soft alignments and new aggregation methods to obtain a more robust similarity measure. (Clark et al., 2019) propose a metric that evaluates multi-sentence texts by computing the similarity between the generated text and the reference text based on the sentence embeddings.</p>
<p>Task-specific Evaluators Task-specific metrics refer to the scores for evaluating the NLG models by measuring the quality of the generated texts based on the specific task requirements. For example, summarization tasks need to assess the consistency of the generated summaries (Kryściński et al., 2020; Wang et al., 2020; Cao et al., 2020; Durmus et al., 2020), and dialogue response generation tasks need to assess the coherence of the generated responses (Dziri et al., 2019; Ye et al., 2021; Ghazarian et al., 2019). However, these metrics are not generalizable to other NLG tasks, and they are not able to measure the overall quality of the generated texts.</p>
<p>Unified Evaluators Recently, some evaluators have been developed to assess text quality from multiple dimensions by varying the input and output contents (Yuan et al., 2021) or the model variants (Mehri and Eskenazi, 2020) they use. UniEval (Zhong et al., 2022) is a unified evaluator that can evaluate different aspects of text generation as QA tasks. By changing the question format, it can handle different evaluation tasks.</p>
<p>LLM-based Evaluators Fu et al. (2023) propose GPTScore, a new framework that evaluated texts with generative pre-training models like GPT-3. It assumes that a generative pre-training model will assign a higher probability of high-quality generated text following a given instruction and context. Wang et al. (2023a) conduct a preliminary survey of using ChatGPT as a NLG evaluator. Kocmi and Federmann (2023); Lu et al. (2023) proposed to use GPT models for evaluating machine translation tasks. Very recently, Wang et al. (2023b) investigated the problem of unfairness when using large models in evaluating dialogue responses.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we propose G-EVAL, a framework of using LLM with chain-of-thoughts (CoT) to evaluate the quality of generated texts. We conduct
extensive experiments on two NLG tasks, text summarization and dialogue generation, and show that G-EVAL can outperform state-of-the-art evaluators and achieve higher human correspondence. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluator having a bias towards the LLM-generated texts. We hope our work can inspire more research on using LLMs for NLG evaluation, and also raise awareness of the potential risks and challenges of using LLMs as evaluators.</p>
<h2>Limitations</h2>
<p>G-EVAL is a framework that uses LLMs to evaluate the quality of generated texts. However, it also has some limitations that need to be addressed in future work.</p>
<ol>
<li>As we already discussed in the paper, G-EVAL may have a bias towards the LLM-generated texts. This may lead to the self-reinforcement of LLMs if the evaluation score is used as a reward signal for further tuning. And this could result in the over-fitting of the LLMs to their own evaluation criteria, rather than the true evaluation criteria of the NLG tasks.</li>
<li>G-EVAL is limited by the availability and accessibility of LLMs. Currently, most LLMs are not publicly available, and require special access or payment to use. This may limit the applicability and reproducibility of G-EVAL. Moreover, the LLMs are constantly updated, which may lead to inconsistent evaluation results across different versions of the LLMs.</li>
<li>We meta-evaluate G-EVAL on two NLG tasks, text summarization and dialogue generation. However, there are some emerging NLG tasks in the LLM era where users prompt with freeform natural language instructions. In this case, the evaluation criteria may need to be more flexible and adaptive to the user's intention and preference. Therefore, more research is needed to explore how to use G-EVAL for evaluating these new types of NLG tasks.</li>
</ol>
<h2>Ethics Statement</h2>
<p>The G-EVAL framework we proposed is designed to offer a more effective and reliable method for assessing natural language generation systems. Its</p>
<p>purpose is to aid researchers, developers, and other interested parties in evaluating the quality of text produced by NLG systems. Possible risks could exist if G-EVAL is unable to precisely evaluate the quality of produced texts or shows a preference for LLM-created texts. This could lead to developers overestimating the performance of their systems or unintentionally reinforcing biases in their models. Furthermore, users depending on the generated material may receive low-quality or biased information.</p>
<h2>References</h2>
<p>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65-72.</p>
<p>Meng Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit Cheung. 2020. Factual error correction for abstractive summarization models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6251-6258.</p>
<p>Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstractive summarization. In thirty-second AAAI conference on artificial intelligence.</p>
<p>Elizabeth Clark, Asli Celikyilmaz, and Noah A Smith. 2019. Sentence mover's similarity: Automatic evaluation for multi-sentence texts. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2748-2760.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186.</p>
<p>Esin Durmus, He He, and Mona Diab. 2020. Feqa: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 50555070 .</p>
<p>Nouha Dziri, Ehsan Kamalloo, Kory Mathewson, and Osmar R Zaiane. 2019. Evaluating coherence in dialogue systems using entailment. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3806-3812.</p>
<p>Alexander R Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. Summeval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391-409.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166.</p>
<p>Sarik Ghazarian, Johnny Wei, Aram Galstyan, and Nanyun Peng. 2019. Better automatic evaluation of open-domain dialogue systems with contextualized embeddings. In Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation, pages 82-89, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. Advances in neural information processing systems, 28.</p>
<p>Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Lavinia Dunagan, Jacob Morrison, Alexander Fabbri, Yejin Choi, and Noah A. Smith. 2022. Bidimensional leaderboards: Generate and evaluate language hand in hand. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3540-3557, Seattle, United States. Association for Computational Linguistics.</p>
<p>Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. arXiv preprint arXiv:2302.14520.</p>
<p>Wojciech Kryściński, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332-9346.</p>
<p>Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. 2015. From word embeddings to document distances. In International conference on machine learning, pages 957-966. PMLR.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 7871-7880. Association for Computational Linguistics.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</p>
<p>Qingyu Lu, Baopu Qiu, Liang Ding, Liping Xie, and Dacheng Tao. 2023. Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt. arXiv preprint arXiv:2303.13809.</p>
<p>Shikib Mehri and Maxine Eskenazi. 2020. USR: An unsupervised and reference free evaluation metric for dialog generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 681-707, Online. Association for Computational Linguistics.</p>
<p>Shashi Narayan, Shay B Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797-1807.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311-318.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:167.</p>
<p>Ehud Reiter and Anja Belz. 2009. An investigation into the validity of some metrics for automatically evaluating natural language generation systems. Computational Linguistics, 35(4):529-558.</p>
<p>Amanda Stent, Matthew Marge, and Mohit Singhai. 2005. Evaluating evaluation methods for generation in the presence of variation. In Proceedings of the 6th international conference on Computational Linguistics and Intelligent Text Processing, pages 341-351.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca.</p>
<p>Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008-5020.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 28.</p>
<p>Zheng Ye, Liucun Lu, Lishan Huang, Liang Lin, and Xiaodan Liang. 2021. Towards quantifiable dialogue coherence evaluation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2718-2729.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 34.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675.</p>
<p>Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B. Hashimoto. 2023. Benchmarking large language models for news summarization.</p>
<p>Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, and Steffen Eger. 2019. Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563-578.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena.</p>
<p>Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. 2022. Towards a unified multidimensional evaluator for text generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 20232038, Abu Dhabi, United Arab Emirates.</p>
<h2>A Example Prompts</h2>
<h2>Evaluate Coherence in the Summarization Task</h2>
<p>You will be given one summary written for a news article.</p>
<p>Your task is to rate the summary on one metric.</p>
<p>Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.</p>
<h2>Evaluation Criteria:</h2>
<p>Coherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby "the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic."</p>
<h2>Evaluation Steps:</h2>
<ol>
<li>Read the news article carefully and identify the main topic and key points.</li>
<li>Read the summary and compare it to the news article. Check if the summary covers the main topic and key points of the news article, and if it presents them in a clear and logical order.</li>
<li>Assign a score for coherence on a scale of 1 to 5 , where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.</li>
</ol>
<h2>Example:</h2>
<p>Source Text:
{{Document}}
Summary:
{{Summary}}
Evaluation Form (scores ONLY):</p>
<ul>
<li>Coherence:</li>
</ul>
<h2>Evaluate Engagingness in the Dialogue Generation Task</h2>
<p>You will be given a conversation between two individuals. You will then be given one potential response for the next turn in the conversation. The response concerns an interesting fact, which will be provided as well.</p>
<p>Your task is to rate the responses on one metric.</p>
<p>Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.</p>
<h2>Evaluation Crieteria:</h2>
<p>Engagingness (1-3) Is the response dull/interesting?</p>
<ul>
<li>A score of 1 (dull) means that the response is generic and dull.</li>
<li>A score of 2 (somewhat interesting) means the response is somewhat interesting and could engage you in the conversation (e.g., an opinion, thought)</li>
<li>A score of 3 (interesting) means the response is very interesting or presents an interesting fact</li>
</ul>
<h2>Evaluation Steps:</h2>
<ol>
<li>Read the conversation, the corresponding fact and the response carefully.</li>
<li>Rate the response on a scale of 1-3 for engagingness, according to the criteria above.</li>
<li>Provide a brief explanation for your rating, referring to specific aspects of the response and the conversation.</li>
</ol>
<h2>Example:</h2>
<p>Conversation History:
{{Document}}
Corresponding Fact:
{{Fact}}
Response:
{{Response}}
Evaluation Form (scores ONLY):</p>
<ul>
<li>Engagingness:</li>
</ul>
<h2>Evaluate Hallucinations</h2>
<p>Human Evaluation of Text Summarization Systems:</p>
<p>Factual Consistency: Does the summary untruthful or misleading facts</p>
<p>that are not supported by the source text?</p>
<p>Source Text:
{{Document}}
Summary:
{{Summary}}
Does the summary contain factual inconsistency?</p>
<p>Answer:</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ We use G-EVAL-4 in this experiment, because its superiority in evaluating summarization tasks. Although it has different distribution with with GPT-3.5, the two LLMs should share similar behaviors in terms of text generation.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
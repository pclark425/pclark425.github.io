<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8958 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8958</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8958</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-ef018d9fad6167cfddb7d6654c5422df1e953730</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ef018d9fad6167cfddb7d6654c5422df1e953730" target="_blank">Self-Evaluation Guided Beam Search for Reasoning</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> A stepwise self-evaluation mechanism to guide and calibrate the reasoning process of LLMs through stochastic beam search and a decoding algorithm integrating the self- evaluation guidance via stochastically beam search are proposed.</p>
                <p><strong>Paper Abstract:</strong> Breaking down a problem into intermediate steps has demonstrated impressive performance in Large Language Model (LLM) reasoning. However, the growth of the reasoning chain introduces uncertainty and error accumulation, making it challenging to elicit accurate final results. To tackle this challenge of uncertainty in multi-step reasoning, we introduce a stepwise self-evaluation mechanism to guide and calibrate the reasoning process of LLMs. We propose a decoding algorithm integrating the self-evaluation guidance via stochastic beam search. The self-evaluation guidance serves as a better-calibrated automatic criterion, facilitating an efficient search in the reasoning space and resulting in superior prediction quality. Stochastic beam search balances exploitation and exploration of the search space with temperature-controlled randomness. Our approach surpasses the corresponding Codex-backboned baselines in few-shot accuracy by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQuA, and StrategyQA benchmarks, respectively. Experiment results with Llama-2 on arithmetic reasoning demonstrate the efficiency of our method in outperforming the baseline methods with comparable computational budgets. Further analysis in multi-step reasoning finds our self-evaluation guidance pinpoints logic failures and leads to higher consistency and robustness. Our code is publicly available at https://guideddecoding.github.io/.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8958.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8958.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Eval Guided Stochastic Beam Search</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Evaluation Guided Stochastic Beam Search (stepwise)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding algorithm that integrates stepwise LLM self-evaluation into stochastic beam search: at each multi-token reasoning step the method combines the model's generation probability with an LLM-produced correctness/confidence score to guide beam pruning and sampling, with temperature-controlled randomness and annealing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source Codex model (code-davinci-002) used as the primary generation backend in experiments; exact parameter count not specified in the paper; accessed with token-likelihoods (logits) available.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Stepwise self-evaluation (multiple-choice scoring) integrated into stochastic beam search</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>At each reasoning step s^t the method asks an LLM evaluator to judge the step via a few-shot multiple-choice prompt (probability of option A is used as correctness score C). The decoding score for a full chain E = P_LM^{lambda} * C^{1-lambda} combines generation probability and correctness confidence; stochastic beam search draws n samples per beam, scores candidates via E, and samples k beams proportional to exp(E / tau) with annealing factor alpha.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Arithmetic, Symbolic and Commonsense benchmarks (GSM8K, AQuA, SVAMP, ASDiv, TabMWP, BIG-Bench symbolic tasks, StrategyQA, CommonsenseQA, Sports Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Math word problems (arithmetic/algebra/tabular), symbolic tasks (date understanding, object counting), and commonsense multi-step questions requiring chaining or multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Codex (PAL/CoT) with stepwise self-evaluation: GSM8K (multiple-chain Ours-PAL) 85.5% accuracy; AQuA 64.2%; StrategyQA (Ours-CoT) 77.2%; other reported: SVAMP 90.3% (Ours-PAL multiple-chains in table). (Percent accuracy as reported in paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Corresponding baselines (Codex): PAL, SC (self-consistency) or vanilla reasoning baselines — GSM8K PAL, SC 80.4% (or PAL single-chain 72.0%); AQuA baseline 58.6% (vanilla reasoning-enhanced Codex reported in text); StrategyQA CoT baseline 73.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered LLM self-evaluation: few-shot multiple-choice evaluator (LM_C) returns token-level probability for option A as correctness score C; this C is combined with LM generation probability P via exponentiated weighting (lambda) and used inside a stochastic beam search procedure (temperature tau, annealing alpha) to guide selection of multi-token reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: consistent absolute accuracy gains on arithmetic/symbolic/commonsense tasks (examples: GSM8K improved to 85.5% vs 80.4% baseline; AQuA 64.2% vs 58.6%; StrategyQA 77.2% vs 73.2%). Cost-controlled Llama-2 experiments show Ours outperforms self-consistency under equal token budgets (e.g., GSM8K 46.1% vs 41.8% at similar # tokens). Qualitative: self-evaluation highlights logic failures and increases consistency/robustness; distributions of C and P show faithfulness increases guide corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires access to token likelihoods (logits) so not directly applicable to LMs that don't expose logits (e.g., some GPT-4/ChatGPT APIs). Adds computational overhead from candidate sampling and stepwise evaluation (notably large # rollouts n can raise costs). Less effective on short reasoning chains and on tasks where the baseline model already has high confidence/diversity is low (constrained search space); evaluator can miss small textual errors or be overly sensitive to minor details like variable naming, sometimes assigning low C despite correct final answer. Excessive sampling randomness (large tau or alpha) hurts single-chain performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to self-consistency (majority voting over sampled chains) and vanilla CoT/PAL/PoT baselines: outperforms self-consistency on arithmetic tasks under comparable budgets and yields higher accuracy on many benchmarks; however, it can lag behind CoT baseline on shorter commonsense chains. The method is complementary to program-aided accurate computations (PAL) and can be combined with sampling-diversity techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablations report sensitivity analyses over sampling temperature tau, annealing factor alpha, and lambda (balance between P and C). Findings: moderate temperature decay (e.g., alpha=0.5) improves performance; lambda is relatively stable (they use lambda=0.5), but tuning can help; increasing tau/alpha increases diversity but too much randomness degrades single-chain performance. The paper also provides theoretical bound on sampling approximation error (Appendix A.1).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Guided Beam Search for Reasoning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8958.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8958.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ours (Llama-2 13B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Evaluation Guided Stochastic Beam Search evaluated with Llama-2 (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same stepwise self-evaluation guided stochastic beam search method applied using the Llama-2 (13B) open-source backbone to assess cost–performance tradeoffs; used for cost-controlled comparisons with self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Llama-2 13B model used for cost–performance analysis; standard few-shot PAL and CoT prompting used; exact training details are referenced but not reproduced in-paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Stepwise self-evaluation guided stochastic beam search (as above)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same stepwise evaluator (LM_C) prompts and stochastic beam search combining P and C; experiments specifically evaluate performance at matched token-cost budgets versus self-consistency baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, AQuA, SVAMP, ASDiv, TabMWP, StrategyQA, CommonsenseQA (cost-controlled experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Arithmetic and commonsense benchmarks; experiments present accuracy at matched computational budgets (# tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Under matched token budgets (Llama-2 13B): GSM8K 46.1% (Ours, #Tokens 12.6k); AQuA 31.5% (#Tokens 6.0k); SVAMP 74.6% (#Tokens 5.0k); StrategyQA 70.6% (#Tokens 2.6k); CommonsenseQA 74.0% (#Tokens 1.2k).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline self-consistency (same backbone & prompting) at similar budgets: GSM8K 41.8% (#Tokens 13.9k); AQuA 30.7% (#Tokens 6.6k); SVAMP 71.2% (#Tokens 5.9k); StrategyQA 71.0% (#Tokens 2.7k); CommonsenseQA 74.4% (#Tokens 1.2k).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Same prompt-engineered multiple-choice self-evaluation producing C; combined with LM generation probability P inside stochastic beam search; hyperparameters tuned for cost-efficiency (n smaller for Llama-2 experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative cost-controlled wins: e.g., GSM8K 46.1% vs baseline 41.8% at comparable token budgets; consistent improvements across arithmetic datasets under equal or lower token cost. Plots (Figure 4) show accuracy vs cost curves where Ours outperforms equal-cost baselines as budget increases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same limitations as general method: additional cost from rollouts and evaluation, reduced benefit on shorter reasoning chains or tasks where baseline already performs well; table shows Ours slightly underperforms on some commonsense (short-chain) cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Directly compared to self-consistency in equal-cost setting and found superiority on arithmetic tasks; less clear advantage on short-chain commonsense tasks where self-consistency/CoT may be competitive or better.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Cost ablations adjust rollouts per beam n (n reduced from 16 to 2 for cost analysis); sampling diversity (tau, alpha) explored, with alpha=0.5 used as reasonable default for balance between exploration/exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Guided Beam Search for Reasoning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8958.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8958.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (majority-voting over multiple sampled reasoning chains)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that samples multiple chain-of-thought reasoning traces from an LLM and aggregates final answers by majority voting to improve robustness of final answer selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>General LLMs (applied with Codex and Llama-2 in this paper as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Self-consistency is a sampling-and-aggregate technique typically applied to chain-of-thought outputs from LLMs; in this paper it is used as a baseline with Codex and with Llama-2 (13B) for cost comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency (majority voting over multiple sampled chains)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample many reasoning paths from the LM for the same input and determine the final answer by majority/consensus across sampled answers (instance-level aggregation rather than stepwise evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, AQuA, SVAMP, StrategyQA, etc. (baselines reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same reasoning benchmarks used in the paper; self-consistency used as a multiple-chain baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported baseline results (examples): CoT, SC on GSM8K 78.0% (Codex, table); PAL, SC 80.4% (Codex); Llama-2 self-consistency baseline GSM8K 41.8% (cost-controlled table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Vanilla single-chain counterparts: CoT single-chain 65.6% (Codex) and PAL single-chain 72.0% (Codex) as reported; these are the 'without multiple-chain aggregation' baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Sampling diversity through temperature and repeated stochastic generation; final answer decided by majority across sampled completions (no stepwise evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Paper reports self-consistency improves over single-chain baselines (e.g., CoT single 65.6% -> CoT, SC 78.0% on GSM8K) and uses SC as a competitive baseline; however, the paper shows its stepwise self-evaluation method can outperform SC under comparable token budgets for arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Does not perform stepwise calibration and so may be less effective on very long reasoning chains where intermediate-step errors accumulate; requires many samples to be effective which increases computational cost; less able to correct stepwise logical errors compared to the proposed stepwise evaluation in some long-chain settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Used as the main multiple-chain baseline; the paper reports that stepwise self-evaluation can surpass self-consistency on arithmetic under equal budgets, but self-consistency remains competitive especially on shorter chains where instance-level aggregation suffices.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Guided Beam Search for Reasoning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8958.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8958.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kadavath et al. multiple-choice evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multiple-choice prompting for LLM self-evaluation (Kadavath et al. 2022 style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting format for LLM self-evaluation that frames evaluation as a multiple-choice question; this paper follows that formulation to obtain calibrated correctness scores from the model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models (mostly) know what they know.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used with evaluator LMs (Codex as LM_C in experiments; conceptually general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting approach: few-shot exemplars where the model answers a multiple-choice question about the correctness of an intermediate step; the model's token-level probability for the chosen option is used as the confidence/correctness score.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Multiple-choice LLM self-evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prompt the model (LM_C) with stepwise evaluation exemplars in a multiple-choice QA format; take the token probability of option A as the faithfulness/correctness confidence C for a reasoning step.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Applied to intermediate reasoning-step evaluation across all benchmark tasks in the paper (GSM8K, AQuA, StrategyQA, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to evaluate correctness of intermediate steps (PAL/CoT reasoning) and produce C scores used to guide decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineering: few-shot exemplar-driven multiple-choice prompts; uses token-level likelihood of a selected option to represent evaluation score.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Paper states this multiple-choice evaluation is better calibrated and uses it as the core method to compute C; empirical results show the combined E score (P and C) can successfully distinguish correct/incorrect chains (AUC analyses) and guide beam search to improved final accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Calibration depends on the evaluator LM and exemplar design; evaluator may be sensitive to minor textual details and may assign low scores even when final answer is correct; requires access to token probabilities for the chosen option (limiting use with LMs that do not expose logits).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Presented as the chosen self-evaluation format compared favorably to generic verification/fine-tuned verifiers in related work; integrated tightly into the decoding objective unlike instance-level verification approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Guided Beam Search for Reasoning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8958.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8958.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (as evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 used as step evaluator (binary scoring + explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was used experimentally as an evaluator of reasoning steps by returning binary judgments and detailed explanations; the paper contrasts GPT-4's evaluator behavior with Codex's probabilistic evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Strong closed-source LLM (GPT-4); API does not provide token-level likelihoods, so the authors requested binary step-level scores from GPT-4 and averaged repeated runs (3 samples) to produce a continuous-like score S.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>GPT-4 self-evaluation (binary judgments averaged)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Request GPT-4 to judge each reasoning step with binary values (0 or 1) and optional explanations; average three independent binary judgments to produce S in [0,1]; compare S to Codex-derived correctness C and analyze differences in error detection and explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Selected arithmetic and commonsense examples analyzed qualitatively (e.g., GSM8K example and StrategyQA examples)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used as an evaluator for intermediate reasoning steps to analyze evaluator quality and explanation depth.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Direct prompting of GPT-4 for stepwise evaluation (binary labels + textual explanation); averaged multiple runs to reduce variance.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Qualitative evidence: GPT-4 provided more rigorous, in-depth explanations and was better at pinpointing central logical errors and identifying vague/overly-definitive statements in commonsense chains compared to Codex. The paper shows cases where GPT-4's S is more discriminative for logical errors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>GPT-4 does not expose token-level likelihoods so cannot be used in the paper's original E = P^{lambda} * C^{1-lambda} formulation; the binary scoring is coarser and requires multiple calls/averaging; variance in GPT-4 explanations exists on some challenging cases; API-limitation prevents integrating GPT-4 as LM_G for the full method.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared qualitatively to Codex evaluator: GPT-4 tends to produce more detailed explanations and sometimes better error localization; however, direct integration into the proposed decoding objective is hindered by lack of logits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Guided Beam Search for Reasoning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8958.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8958.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative self-improvement approach that prompts LLMs to generate answers, critique them, and refine the outputs via multiple rounds of self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative self-refinement (generate -> critique -> refine)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Mentioned in related work as an example of iterative refinement with self-feedback where models iteratively critique and improve their own outputs; not experimentally used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Iterative prompting and self-feedback (literature reference only in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Cited as related iterative self-reflection work but not directly compared experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Guided Beam Search for Reasoning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8958.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8958.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that augments an autonomous agent with dynamic memory and explicit self-reflection capabilities; cited in related work as an example of agentic self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Agentic self-reflection with dynamic memory</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Referenced as prior work on LLM self-reflection (autonomous agent architecture); not used experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Agent-level memory and self-reflection loops (cited only).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Guided Beam Search for Reasoning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Language models (mostly) know what they know. <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>PAL: program-aided language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8958",
    "paper_id": "paper-ef018d9fad6167cfddb7d6654c5422df1e953730",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Self-Eval Guided Stochastic Beam Search",
            "name_full": "Self-Evaluation Guided Stochastic Beam Search (stepwise)",
            "brief_description": "A decoding algorithm that integrates stepwise LLM self-evaluation into stochastic beam search: at each multi-token reasoning step the method combines the model's generation probability with an LLM-produced correctness/confidence score to guide beam pruning and sampling, with temperature-controlled randomness and annealing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Codex (code-davinci-002)",
            "model_description": "Closed-source Codex model (code-davinci-002) used as the primary generation backend in experiments; exact parameter count not specified in the paper; accessed with token-likelihoods (logits) available.",
            "reflection_method_name": "Stepwise self-evaluation (multiple-choice scoring) integrated into stochastic beam search",
            "reflection_method_description": "At each reasoning step s^t the method asks an LLM evaluator to judge the step via a few-shot multiple-choice prompt (probability of option A is used as correctness score C). The decoding score for a full chain E = P_LM^{lambda} * C^{1-lambda} combines generation probability and correctness confidence; stochastic beam search draws n samples per beam, scores candidates via E, and samples k beams proportional to exp(E / tau) with annealing factor alpha.",
            "task_name": "Arithmetic, Symbolic and Commonsense benchmarks (GSM8K, AQuA, SVAMP, ASDiv, TabMWP, BIG-Bench symbolic tasks, StrategyQA, CommonsenseQA, Sports Understanding)",
            "task_description": "Math word problems (arithmetic/algebra/tabular), symbolic tasks (date understanding, object counting), and commonsense multi-step questions requiring chaining or multi-hop reasoning.",
            "performance_with_reflection": "Codex (PAL/CoT) with stepwise self-evaluation: GSM8K (multiple-chain Ours-PAL) 85.5% accuracy; AQuA 64.2%; StrategyQA (Ours-CoT) 77.2%; other reported: SVAMP 90.3% (Ours-PAL multiple-chains in table). (Percent accuracy as reported in paper.)",
            "performance_without_reflection": "Corresponding baselines (Codex): PAL, SC (self-consistency) or vanilla reasoning baselines — GSM8K PAL, SC 80.4% (or PAL single-chain 72.0%); AQuA baseline 58.6% (vanilla reasoning-enhanced Codex reported in text); StrategyQA CoT baseline 73.2%.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineered LLM self-evaluation: few-shot multiple-choice evaluator (LM_C) returns token-level probability for option A as correctness score C; this C is combined with LM generation probability P via exponentiated weighting (lambda) and used inside a stochastic beam search procedure (temperature tau, annealing alpha) to guide selection of multi-token reasoning steps.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative: consistent absolute accuracy gains on arithmetic/symbolic/commonsense tasks (examples: GSM8K improved to 85.5% vs 80.4% baseline; AQuA 64.2% vs 58.6%; StrategyQA 77.2% vs 73.2%). Cost-controlled Llama-2 experiments show Ours outperforms self-consistency under equal token budgets (e.g., GSM8K 46.1% vs 41.8% at similar # tokens). Qualitative: self-evaluation highlights logic failures and increases consistency/robustness; distributions of C and P show faithfulness increases guide corrections.",
            "limitations_or_failure_cases": "Requires access to token likelihoods (logits) so not directly applicable to LMs that don't expose logits (e.g., some GPT-4/ChatGPT APIs). Adds computational overhead from candidate sampling and stepwise evaluation (notably large # rollouts n can raise costs). Less effective on short reasoning chains and on tasks where the baseline model already has high confidence/diversity is low (constrained search space); evaluator can miss small textual errors or be overly sensitive to minor details like variable naming, sometimes assigning low C despite correct final answer. Excessive sampling randomness (large tau or alpha) hurts single-chain performance.",
            "comparison_to_other_methods": "Compared to self-consistency (majority voting over sampled chains) and vanilla CoT/PAL/PoT baselines: outperforms self-consistency on arithmetic tasks under comparable budgets and yields higher accuracy on many benchmarks; however, it can lag behind CoT baseline on shorter commonsense chains. The method is complementary to program-aided accurate computations (PAL) and can be combined with sampling-diversity techniques.",
            "ablation_study_results": "Ablations report sensitivity analyses over sampling temperature tau, annealing factor alpha, and lambda (balance between P and C). Findings: moderate temperature decay (e.g., alpha=0.5) improves performance; lambda is relatively stable (they use lambda=0.5), but tuning can help; increasing tau/alpha increases diversity but too much randomness degrades single-chain performance. The paper also provides theoretical bound on sampling approximation error (Appendix A.1).",
            "uuid": "e8958.0",
            "source_info": {
                "paper_title": "Self-Evaluation Guided Beam Search for Reasoning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Ours (Llama-2 13B)",
            "name_full": "Self-Evaluation Guided Stochastic Beam Search evaluated with Llama-2 (13B)",
            "brief_description": "Same stepwise self-evaluation guided stochastic beam search method applied using the Llama-2 (13B) open-source backbone to assess cost–performance tradeoffs; used for cost-controlled comparisons with self-consistency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2 (13B)",
            "model_description": "Open-source Llama-2 13B model used for cost–performance analysis; standard few-shot PAL and CoT prompting used; exact training details are referenced but not reproduced in-paper.",
            "reflection_method_name": "Stepwise self-evaluation guided stochastic beam search (as above)",
            "reflection_method_description": "Same stepwise evaluator (LM_C) prompts and stochastic beam search combining P and C; experiments specifically evaluate performance at matched token-cost budgets versus self-consistency baseline.",
            "task_name": "GSM8K, AQuA, SVAMP, ASDiv, TabMWP, StrategyQA, CommonsenseQA (cost-controlled experiments)",
            "task_description": "Arithmetic and commonsense benchmarks; experiments present accuracy at matched computational budgets (# tokens).",
            "performance_with_reflection": "Under matched token budgets (Llama-2 13B): GSM8K 46.1% (Ours, #Tokens 12.6k); AQuA 31.5% (#Tokens 6.0k); SVAMP 74.6% (#Tokens 5.0k); StrategyQA 70.6% (#Tokens 2.6k); CommonsenseQA 74.0% (#Tokens 1.2k).",
            "performance_without_reflection": "Baseline self-consistency (same backbone & prompting) at similar budgets: GSM8K 41.8% (#Tokens 13.9k); AQuA 30.7% (#Tokens 6.6k); SVAMP 71.2% (#Tokens 5.9k); StrategyQA 71.0% (#Tokens 2.7k); CommonsenseQA 74.4% (#Tokens 1.2k).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Same prompt-engineered multiple-choice self-evaluation producing C; combined with LM generation probability P inside stochastic beam search; hyperparameters tuned for cost-efficiency (n smaller for Llama-2 experiments).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative cost-controlled wins: e.g., GSM8K 46.1% vs baseline 41.8% at comparable token budgets; consistent improvements across arithmetic datasets under equal or lower token cost. Plots (Figure 4) show accuracy vs cost curves where Ours outperforms equal-cost baselines as budget increases.",
            "limitations_or_failure_cases": "Same limitations as general method: additional cost from rollouts and evaluation, reduced benefit on shorter reasoning chains or tasks where baseline already performs well; table shows Ours slightly underperforms on some commonsense (short-chain) cases.",
            "comparison_to_other_methods": "Directly compared to self-consistency in equal-cost setting and found superiority on arithmetic tasks; less clear advantage on short-chain commonsense tasks where self-consistency/CoT may be competitive or better.",
            "ablation_study_results": "Cost ablations adjust rollouts per beam n (n reduced from 16 to 2 for cost analysis); sampling diversity (tau, alpha) explored, with alpha=0.5 used as reasonable default for balance between exploration/exploitation.",
            "uuid": "e8958.1",
            "source_info": {
                "paper_title": "Self-Evaluation Guided Beam Search for Reasoning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Self-Consistency (SC)",
            "name_full": "Self-Consistency (majority-voting over multiple sampled reasoning chains)",
            "brief_description": "A method that samples multiple chain-of-thought reasoning traces from an LLM and aggregates final answers by majority voting to improve robustness of final answer selection.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "General LLMs (applied with Codex and Llama-2 in this paper as baselines)",
            "model_description": "Self-consistency is a sampling-and-aggregate technique typically applied to chain-of-thought outputs from LLMs; in this paper it is used as a baseline with Codex and with Llama-2 (13B) for cost comparisons.",
            "reflection_method_name": "Self-Consistency (majority voting over multiple sampled chains)",
            "reflection_method_description": "Sample many reasoning paths from the LM for the same input and determine the final answer by majority/consensus across sampled answers (instance-level aggregation rather than stepwise evaluation).",
            "task_name": "GSM8K, AQuA, SVAMP, StrategyQA, etc. (baselines reported in paper)",
            "task_description": "Same reasoning benchmarks used in the paper; self-consistency used as a multiple-chain baseline.",
            "performance_with_reflection": "Reported baseline results (examples): CoT, SC on GSM8K 78.0% (Codex, table); PAL, SC 80.4% (Codex); Llama-2 self-consistency baseline GSM8K 41.8% (cost-controlled table).",
            "performance_without_reflection": "Vanilla single-chain counterparts: CoT single-chain 65.6% (Codex) and PAL single-chain 72.0% (Codex) as reported; these are the 'without multiple-chain aggregation' baselines.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Sampling diversity through temperature and repeated stochastic generation; final answer decided by majority across sampled completions (no stepwise evaluation).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Paper reports self-consistency improves over single-chain baselines (e.g., CoT single 65.6% -&gt; CoT, SC 78.0% on GSM8K) and uses SC as a competitive baseline; however, the paper shows its stepwise self-evaluation method can outperform SC under comparable token budgets for arithmetic tasks.",
            "limitations_or_failure_cases": "Does not perform stepwise calibration and so may be less effective on very long reasoning chains where intermediate-step errors accumulate; requires many samples to be effective which increases computational cost; less able to correct stepwise logical errors compared to the proposed stepwise evaluation in some long-chain settings.",
            "comparison_to_other_methods": "Used as the main multiple-chain baseline; the paper reports that stepwise self-evaluation can surpass self-consistency on arithmetic under equal budgets, but self-consistency remains competitive especially on shorter chains where instance-level aggregation suffices.",
            "ablation_study_results": null,
            "uuid": "e8958.2",
            "source_info": {
                "paper_title": "Self-Evaluation Guided Beam Search for Reasoning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Kadavath et al. multiple-choice evaluation",
            "name_full": "Multiple-choice prompting for LLM self-evaluation (Kadavath et al. 2022 style)",
            "brief_description": "A prompting format for LLM self-evaluation that frames evaluation as a multiple-choice question; this paper follows that formulation to obtain calibrated correctness scores from the model.",
            "citation_title": "Language models (mostly) know what they know.",
            "mention_or_use": "use",
            "model_name": "Used with evaluator LMs (Codex as LM_C in experiments; conceptually general)",
            "model_description": "Prompting approach: few-shot exemplars where the model answers a multiple-choice question about the correctness of an intermediate step; the model's token-level probability for the chosen option is used as the confidence/correctness score.",
            "reflection_method_name": "Multiple-choice LLM self-evaluation",
            "reflection_method_description": "Prompt the model (LM_C) with stepwise evaluation exemplars in a multiple-choice QA format; take the token probability of option A as the faithfulness/correctness confidence C for a reasoning step.",
            "task_name": "Applied to intermediate reasoning-step evaluation across all benchmark tasks in the paper (GSM8K, AQuA, StrategyQA, etc.)",
            "task_description": "Used to evaluate correctness of intermediate steps (PAL/CoT reasoning) and produce C scores used to guide decoding.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompt-engineering: few-shot exemplar-driven multiple-choice prompts; uses token-level likelihood of a selected option to represent evaluation score.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Paper states this multiple-choice evaluation is better calibrated and uses it as the core method to compute C; empirical results show the combined E score (P and C) can successfully distinguish correct/incorrect chains (AUC analyses) and guide beam search to improved final accuracy.",
            "limitations_or_failure_cases": "Calibration depends on the evaluator LM and exemplar design; evaluator may be sensitive to minor textual details and may assign low scores even when final answer is correct; requires access to token probabilities for the chosen option (limiting use with LMs that do not expose logits).",
            "comparison_to_other_methods": "Presented as the chosen self-evaluation format compared favorably to generic verification/fine-tuned verifiers in related work; integrated tightly into the decoding objective unlike instance-level verification approaches.",
            "ablation_study_results": null,
            "uuid": "e8958.3",
            "source_info": {
                "paper_title": "Self-Evaluation Guided Beam Search for Reasoning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4 (as evaluator)",
            "name_full": "GPT-4 used as step evaluator (binary scoring + explanations)",
            "brief_description": "GPT-4 was used experimentally as an evaluator of reasoning steps by returning binary judgments and detailed explanations; the paper contrasts GPT-4's evaluator behavior with Codex's probabilistic evaluator.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (OpenAI)",
            "model_description": "Strong closed-source LLM (GPT-4); API does not provide token-level likelihoods, so the authors requested binary step-level scores from GPT-4 and averaged repeated runs (3 samples) to produce a continuous-like score S.",
            "reflection_method_name": "GPT-4 self-evaluation (binary judgments averaged)",
            "reflection_method_description": "Request GPT-4 to judge each reasoning step with binary values (0 or 1) and optional explanations; average three independent binary judgments to produce S in [0,1]; compare S to Codex-derived correctness C and analyze differences in error detection and explanations.",
            "task_name": "Selected arithmetic and commonsense examples analyzed qualitatively (e.g., GSM8K example and StrategyQA examples)",
            "task_description": "Used as an evaluator for intermediate reasoning steps to analyze evaluator quality and explanation depth.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Direct prompting of GPT-4 for stepwise evaluation (binary labels + textual explanation); averaged multiple runs to reduce variance.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Qualitative evidence: GPT-4 provided more rigorous, in-depth explanations and was better at pinpointing central logical errors and identifying vague/overly-definitive statements in commonsense chains compared to Codex. The paper shows cases where GPT-4's S is more discriminative for logical errors.",
            "limitations_or_failure_cases": "GPT-4 does not expose token-level likelihoods so cannot be used in the paper's original E = P^{lambda} * C^{1-lambda} formulation; the binary scoring is coarser and requires multiple calls/averaging; variance in GPT-4 explanations exists on some challenging cases; API-limitation prevents integrating GPT-4 as LM_G for the full method.",
            "comparison_to_other_methods": "Compared qualitatively to Codex evaluator: GPT-4 tends to produce more detailed explanations and sometimes better error localization; however, direct integration into the proposed decoding objective is hindered by lack of logits.",
            "uuid": "e8958.4",
            "source_info": {
                "paper_title": "Self-Evaluation Guided Beam Search for Reasoning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine: Iterative refinement with self-feedback",
            "brief_description": "An iterative self-improvement approach that prompts LLMs to generate answers, critique them, and refine the outputs via multiple rounds of self-feedback.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Iterative self-refinement (generate -&gt; critique -&gt; refine)",
            "reflection_method_description": "Mentioned in related work as an example of iterative refinement with self-feedback where models iteratively critique and improve their own outputs; not experimentally used in this paper.",
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Iterative prompting and self-feedback (literature reference only in this paper).",
            "number_of_iterations": null,
            "evidence_for_improvement": null,
            "limitations_or_failure_cases": null,
            "comparison_to_other_methods": "Cited as related iterative self-reflection work but not directly compared experimentally.",
            "ablation_study_results": null,
            "uuid": "e8958.5",
            "source_info": {
                "paper_title": "Self-Evaluation Guided Beam Search for Reasoning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "brief_description": "An approach that augments an autonomous agent with dynamic memory and explicit self-reflection capabilities; cited in related work as an example of agentic self-reflection.",
            "citation_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Agentic self-reflection with dynamic memory",
            "reflection_method_description": "Referenced as prior work on LLM self-reflection (autonomous agent architecture); not used experimentally in this paper.",
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Agent-level memory and self-reflection loops (cited only).",
            "number_of_iterations": null,
            "evidence_for_improvement": null,
            "limitations_or_failure_cases": null,
            "comparison_to_other_methods": null,
            "ablation_study_results": null,
            "uuid": "e8958.6",
            "source_info": {
                "paper_title": "Self-Evaluation Guided Beam Search for Reasoning",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2
        },
        {
            "paper_title": "Language models (mostly) know what they know.",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1
        },
        {
            "paper_title": "PAL: program-aided language models",
            "rating": 1
        }
    ],
    "cost": 0.02049175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Self-Evaluation Guided Beam Search for Reasoning</h1>
<p>Yuxi Xie ${ }^{1 *}$ Kenji Kawaguchi ${ }^{1}$ Yiran Zhao ${ }^{1}$ James Xu Zhao ${ }^{1}$<br>Min-Yen Kan ${ }^{1 \dagger}$ Junxian $\mathrm{He}^{2 \dagger}$ Michael Qizhe Xie ${ }^{1 \dagger}$<br>${ }^{1}$ National University of Singapore ${ }^{2}$ The Hong Kong University of Science and Technology</p>
<h4>Abstract</h4>
<p>Breaking down a problem into intermediate steps has demonstrated impressive performance in Large Language Model (LLM) reasoning. However, the growth of the reasoning chain introduces uncertainty and error accumulation, making it challenging to elicit accurate final results. To tackle this challenge of uncertainty in multi-step reasoning, we introduce a stepwise self-evaluation mechanism to guide and calibrate the reasoning process of LLMs. We propose a decoding algorithm integrating the self-evaluation guidance via stochastic beam search. The selfevaluation guidance serves as a better-calibrated automatic criterion, facilitating an efficient search in the reasoning space and resulting in superior prediction quality. Stochastic beam search balances exploitation and exploration of the search space with temperature-controlled randomness. Our approach surpasses the corresponding Codex-backboned baselines in few-shot accuracy by $6.34 \%, 9.56 \%$, and $5.46 \%$ on the GSM8K, AQuA, and StrategyQA benchmarks, respectively. Experiment results with Llama-2 on arithmetic reasoning demonstrate the efficiency of our method in outperforming the baseline methods with comparable computational budgets. Further analysis in multi-step reasoning finds our self-evaluation guidance pinpoints logic failures and leads to higher consistency and robustness. Our code is publicly available at https://guideddecoding.github.io/.</p>
<h2>1 Introduction</h2>
<p>The remarkable empirical achievements of Large Language Models (LLMs) have recently ushered in a new era in machine reasoning through few-shot prompting techniques (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023a; OpenAI, 2023). In particular, breaking down a problem into intermediate stages, or a reasoning chain, can significantly improve model performance on reasoning tasks (Cobbe et al., 2021). Various prompting approaches have been proposed to define these chains, such as scratchpads (Nye et al., 2021), chain-of-thought (CoT) (Wei et al., 2022b), least-to-most (Zhou et al., 2023), and program-aided language models (PAL) (Gao et al., 2023; Chen et al., 2022). However, as the complexity and length of reasoning chains increase with the difficulty of tasks, LLMs struggle with errors and imperfections that accumulate across multiple intermediate steps (Wu et al., 2016; Guo et al., 2018; Chen et al., 2022). Furthermore, the growing number of steps leads to an exponential growth in the search space for reasoning, making it exceedingly difficult to obtain accurate final outcomes.</p>
<p>Confronted with the challenges of uncertainty in multi-step chaining, several previous studies have worked on different aspects to alleviate the impact of reasoning errors. For instance, Wang et al. (2023) introduce self-consistency as a method to determine the final answer through majority voting using multiple sampled reasoning paths, while Li et al. (2022) investigate various prompts to diversify the sampling outcomes. Gao et al. (2023) and Chen et al. (2022) utilize Python programs to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Self-Evaluation can calibrate the decoding direction in multi-step reasoning. We illustrate our method in the form of stepwise stochastic beam search with the beam size equal to 1. The scale of the self-evaluation score is visualized in the colormap. We adopt Program-Aided Language models (PAL) reasoning (Gao et al., 2023; Chen et al., 2022) for this math word problem.</p>
<p>achieve higher accuracy in mathematical computations. While these approaches have contributed to significant performance improvements in reasoning, the process of generating reasoning chains has been parameterized as a standard autoregressive process and intrinsically faces the challenge of sampling within an exponentially large search space.</p>
<p>Motivated by this challenge, we employ LLM <em>self-evaluation</em> (Kadavath et al., 2022) as a better-calibrated criterion to automatically guide the search in the reasoning space, drawing inspiration from prior works on utilizing LLMs for self-evaluation (Rae et al., 2021; Paul et al., 2023; Madaan et al., 2023; Shinn et al., 2023). We integrate the self-evaluation guidance for reasoning in a stepwise and generalizable manner. Specifically, we formulate the reasoning chain generation as a decoding process consisting of multiple intermediate steps. Unlike traditional text decoding where each step produces a single token, we consider each decoding step as a reasoning logic composed of a sequence of tokens. This framework enables us to employ beam search (Jurafsky and Martin, 2009; Graves, 2012) decoding tailored for intermediate steps and guide the beam searching process by controlling the error of each reasoning step to prevent potential error accumulation throughout the chaining. Figure 1 illustrates an example of decoding a chain of program-aided reasoning steps. Furthermore, we incorporate temperature-controlled randomness (Ackley et al., 1985; Kool et al., 2019; Meister et al., 2021) into the traditional (deterministic) beam search to balance the quality–diversity trade-off in searching for better reasoning chains. Our approach has resulted in respectable improvements across various arithmetic, symbolic, and commonsense reasoning tasks. For instance, by guiding the reasoning decoding process of the Codex model (Chen et al., 2021), we achieve accuracies of 85.5%, 64.2%, and 77.2% on the GSM8K, AQuA, and StrategyQA benchmarks, compared to the vanilla reasoning-enhanced Codex performance of 80.4%, 58.6%, and 73.2%, respectively. Our further analysis on Llama-2 (Touvron et al., 2023b) demonstrates the efficiency of our method in surpassing the self-consistency baseline under equivalent computational budgets.</p>
<h2>2 Self-Evaluation Guided Stochastic Beam Search</h2>
<p>Considering the input prompt and question Q represented as x, we formulate the answer distribution $P(a \mid x)$ by decomposing it as a reasoning chain generation process $P(R \mid x)$ and an answer generation process $P(a \mid R, x)$:</p>
<p>$$P(a \mid x) = \mathbb{E}_{R \sim P(R|x)} P(a \mid R, x), \tag{1}$$</p>
<p>where $R$ is the intermediate reasoning chain variable that is typically a text sequence. $P(a \mid R, x) = \frac{1_A(a)}{\max(|A|, 1)}$, where $A = \text{execute}(R)$ represents the set of predicted answer(s) interpreted from $R$, and $1_A$ is the indicator function of the subset $A$. In practice, $|A| \geq 0$ can be 0 or larger than 1 when the reasoning $R$ returns no valid answer or produces more than one possible answers, respectively.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Our framework of self-evaluation guided stochastic beam search for multi-step reasoning. The schema of the decoding process is on the left, where we keep <em>k</em> = 2 candidates at each timestep, with the detailed illustration of timestep <em>t</em> at the bottom. Here "Gen" and "Self-Eval" represent the generation and evaluation LLMs, respectively. The corresponding prompt formulations are provided on the right, where the questions <em>Q</em>, reasoning steps <em>R</em>, and evaluation scripts are highlighted in orange, green, and yellow, respectively. Steps in light green (<em>e.g.</em>, <em>s<sup>t</sup></em>) are for models to generate or evaluate at the current timestep. Specifically, we follow Kadavath et al. (2022) to prompt the LLM evaluation by answering the multiple-choice question, <em>i.e.</em>, the lines starting with #.</p>
<p>Prior research has modeled the reasoning chain generation <em>P(R | x)</em> by prompting LLMs to explicitly elaborate on the required intermediate steps <em>R</em>. Through setting different prompting schemes, the reasoning process <em>P(R | x)</em> can be modeled as chain-of-thought free-text reasoning (Kojima et al., 2022; Wei et al., 2022b), a two-stage question decomposition and answering pipeline (Zhou et al., 2023), or program-aided reasoning to generate a python program (Gao et al., 2023; Chen et al., 2022). While effective, previous work mostly uses a single sample of <em>R</em> from the LLMs to approximate the expectation in Eq. 1 – the generated reasoning chain is often unreliable and causes incorrect answers. To mitigate this issue, Wang et al. (2023) conduct majority voting to approximate the expectation via sampling and aggregating multiple reasoning chains. Li et al. (2022) take a further step to diversify the sampling and calibrate <em>P(R | x)</em> with a task-specific fine-tuned verifier. Another line of work focuses on improving <em>P(a | R, x)</em> instead. For example, Gao et al. (2023) and Chen et al. (2022) employ Python programs for more accurate calculations in math word problems.</p>
<p>In this work, we focus on improving <em>P(R | x)</em> to enhance the consistency of the sampled reasoning chains. To this end, we propose to explicitly break down the reasoning process into multiple steps, as shown in Figure 2, where each step yields a semantically integrated sequence of tokens, representing a single step within the overall reasoning chain. From this perspective, we can approach the task of enhancing <em>P(R | x)</em> as a decoding problem over the reasoning chains. Considering the exponentially large search space and the potential unreliability of LLM-produced chains in reasoning, we propose a constrained stochastic beam search decoding approach to improve the reasoning step by step and obtain high-quality reasoning with a limited number of samples. We detail our approach next.</p>
<h3>2.1 Multi-step Reasoning via Stochastic Beam Search</h3>
<p>In multi-step reasoning, a reasoning chain of <em>T</em> steps is sequentially generated through several timesteps as <em>R</em> = [<em>s</em><sup>1</sup>, <em>s</em><sup>2</sup>, · · · , <em>s</em><sup>T</sup>] = <em>s</em><sup>1:T</sup>, where <em>s</em><sup>t</sup> represents a sequence of tokens as the <em>t</em>-th step. Formally, the reasoning generation process <em>P(R | x)</em> can be factorized in an autoregressive manner:</p>
<p>$$P(R = s^{1:T} \mid x) = \prod_{t} P(s^{t} \mid x, s^{1:t-1}),\tag{2}$$</p>
<p>which resembles the typical token-level autoregressive distribution of language models. Stepwise reasoning allows us to formulate the process as a step-by-step decoding problem, where we can utilize widely-used strategies such as beam search for the generation. Different from the typical text</p>
<p>decoding process where each step consists of a single token, here we view a sequence of reasoning tokens as a single step. One of the most severe issues in LLM-based reasoning is the potential unreliability and inaccuracy of each reasoning step generated by the model. Furthermore, errors from individual steps may accumulate throughout the reasoning chain, exacerbating the problem. To address the issue, we define a constraint function $\mathcal{C}\left(s^{t},s^{1:t-1}\right) \in[0,1]$ within each reasoning step $^{3}$ that outputs the LLM confidence in the correctness of the reasoning sequence $s^{t}$ based on the previous context $s^{1: t-1}$. Then, we present a constrained decoding approach that combines the language model probability and the correctness confidence as a new decoding objective function $\mathcal{E}\left(s^{1: T}\right)$ :</p>
<p>$$
\mathcal{E}\left(s^{1: T}\right)=\prod_{t} P_{\mathrm{LM}_{\mathcal{G}}}^{\lambda}\left(s^{t} \mid x, s^{1: t-1}\right) \mathcal{C}^{1-\lambda}\left(s^{t}\right)
$$</p>
<p>where $P_{\mathrm{LM}<em _mathrm_LM="\mathrm{LM">{\mathcal{G}}}$ is the language model distribution ${ }^{4} . \lambda \in[0,1]$ is a weight hyperparameter to balance the LM score and the confidence score. We will detail the design of $\mathcal{C}\left(s^{t}\right)$ in Section 2.2. Eq 3 follows an autoregressive factorization form, and thus traditional token-level decoding methods such as beam search can be applied here on the chain level. As it is desirable to obtain high-quality reasoning chains with limited samples that are scored high by $\mathcal{E}\left(s^{1: T}\right)$, it is natural to utilize greedy or beam search decoding to approximate the reasoning sequences that maximize $\mathcal{E}\left(s^{1: T}\right)$.
Additionally, multiple diverse reasoning chains could be aggregated to further improve the final accuracy, as suggested by Eq 1 and empirically confirmed by self-consistency reasoning (Wang et al., 2023). To this end, we propose a variant of stochastic beam search (Kool et al., 2019; Meister et al., 2021) to strike a tradeoff between exploration and exploitation. Concretely, for beam size $k$, at each reasoning step we draw $n$ samples of $s^{t}$ following $P</em>$}_{\mathcal{G}}}\left(s^{t} \mid x, s^{1: t-1}\right)$ for each beam, and we end up with $n k$ chain hypotheses of $s^{1: t}$ to form the candidate set $\mathcal{S}$, then we perform beam pruning through sampling - we sample $k$ reasoning beams without replacement, rather than finding the $\arg \max k$, following a distribution defined by the accumulated score: ${ }^{5</p>
<p>$$
P_{\text {beam }}\left(s^{1: t}\right) \propto \exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right), \quad s^{1: t} \in \mathcal{S}
$$</p>
<p>where the temperature $\tau$ is a hyperparameter to control the randomness in stochastic beam search; when $\tau \rightarrow 0$, stochastic beam search becomes the vanilla beam search algorithm. The reasoning beams $s^{1: t}$ can be sampled efficiently since $|\mathcal{S}|=n k$ is a finite set. To enable fine-grained control of sampling randomness in decoding, we also introduce a hyperparameter $\alpha \in[0,1]$ so that $\tau$ can decay step by step as $\tau \rightarrow \alpha \tau$. By annealing $\tau$ with $\alpha$, we can mitigate the error accumulation due to aggregated randomness throughout chaining, as discussed in Section 3.4.
By incorporating controllable randomness, we not only achieve a more reliable single reasoning chain generation by setting randomness to be small, but also leverage multiple diverse reasoning chains with larger variance. Next, we introduce our constraint function $\mathcal{C}\left(s^{t}, s^{1: t-1}\right)$ that utilizes a self-evaluation scheme to improve the consistency of each reasoning step.</p>
<h1>2.2 Self-Evaluation as Correctness Control</h1>
<p>Inspired by the recent success of self-evaluation (Kadavath et al., 2022; Shinn et al., 2023; Madaan et al., 2023; Paul et al., 2023), a scheme to prompt LLMs to evaluate their own generation, we use LLMs to judge the correctness of $s^{t}$ based on $s^{1: t-1}$. Specifically, the evaluation and generation models use the same backend LLM with different prompts, which consist of few-shot exemplars. We follow previous works of CoT (Wei et al., 2022b) or PAL (Gao et al., 2023) to formulate the generation prompts. To construct the in-context exemplars prompt ${ }<em _mathcal_C="\mathcal{C">{\mathcal{C}}$ for the self-evaluation LLM $\mathrm{LM}</em>$ in the form of multiple-choice questioning (as shown in Figure 2) to better calibrate the model predictions, where we adopt the token-level probability of option A to represent the correctness score as:}}$, we provide stepwise evaluation examples (as question answering with rationales) in each instance. Inspired by Kadavath et al. (2022), we design prompt ${ }_{\mathcal{C}</p>
<p>$$
\mathcal{C}\left(s^{t}\right)=P_{\mathrm{LM}<em _mathcal_C="\mathcal{C">{\mathcal{C}}}\left(\mathrm{~A} \mid \text { prompt }</em>\right)
$$}}, Q, s^{1: t</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>3 Experiments</p>
<h3>3.1 Setup</h3>
<p>We present and analyze the results of our self-evaluation guided beam search with different LLM backbones on various reasoning benchmarks. Implementation details including prompt examples and hyperparameter setup can be found in Appendix A.3.</p>
<p>Benchmarks. We evaluate the effectiveness of our approach across three types of reasoning tasks: (1) Arithmetic Reasoning on five math word problem benchmarks, including GSM8K (Cobbe et al., 2021) on math word problems, AQuA (Ling et al., 2017) on algebraic word problems, SVAMP (Patel et al., 2021) on structure variations of math word problems, ASDiv (Miao et al., 2020) on diverse math word problems, and TabMWP (Lu et al., 2023) on tabular math word problems; (2) Symbolic Reasoning on BIG-Bench (Srivastava et al., 2022), which involves Date Understanding of contextbased date inferring and Object Counting of enumerating and counting objects of different types; (3) Commonsense Reasoning on three benchmarks, including CommonsenseQA (Talmor et al., 2019) of commonsense questions that require prior world knowledge to answer, StrategyQA (Geva et al., 2021) of questions that require a multi-hop strategy to answer, and Sports Understanding from BIG-Bench (Srivastava et al., 2022) to determine whether a sports-related sentence is plausible.</p>
<p>Baselines. We consider two types of baselines: (1) Chain-of-Thought (CoT) (Wei et al., 2022b) prompting in free-text reasoning and (2) Program-Aided Language models (PAL) (Ling et al., 2017) and Program-of-Thought (PoT) (Chen et al., 2022) prompting in program-aided reasoning. We also include their self-consistency (Wang et al., 2023) variants for multiple-chain reasoning. For generation, we follow the few-shot exemplars of baselines. For self-evaluation, we manually create a set of few-shot exemplars based on the baseline outputs on corresponding training data. We formulate self-evaluation as a task of multiple-choice question answering, following Kadavath et al. (2022). For baselines, we represent the cost as the number of generated tokens. For the cost of our method, we also include the number of additional input tokens in self-evaluation for a fair comparison.</p>
<p>Backboned LLMs. We assess our approach on closed- and open-source LLMs using both PAL and CoT prompting. For closed-source LLMs, we choose Codex (code-davinci-002) (Chen et al., 2021) to report and compare the results on all datasets. We use Llama-2 (13B) (Touvron et al., 2023b) as our open-source LLM to conduct cost–performance analysis on different datasets.</p>
<h3>3.2 Main Results</h3>
<p>Arithmetic and Symbolic Reasoning. Table 1 shows the results for arithmetic and symbolic reasoning. Our method achieves significant performance improvements on most benchmarks in both single- $(\tau=0)$ and multiple-chain scenarios, with PAL as the baseline. For arithmetic reasoning, we observe absolute increases in accuracy of $5.3\%,8.3\%$, and $0.7\%$ on GSM8K, AQuA, and SVAMP, respectively. One possible explanation for this discrepancy in improvements is the reduced diversity in LLM generations due to higher confidence in predictions, as evidenced by the relatively high performance on the tasks. This highlights the importance of incorporating controllable randomness into the candidate generations to expand the search space for self-evaluation guided decoding. We further explore the impact of generation diversity by varying the temperature $\gamma$ in Section 3.4.</p>
<p>For symbolic reasoning, our approach also leads to consistent performance gains. However, when the baseline itself performs well on the task (e.g., $96.7\%$ on Object Counting), our approach may not yield substantial improvement. This can also be attributed to the constrained accessible search space for self-evaluation guidance to refine the generation distribution. This limit suggests the inherent deficiency in our LLM-based prompting method that it becomes increasingly challenging to calibrate the generation direction when the model $\mathrm{LM}_{\mathcal{G}}$ is more confident in its predictions. In other words, the high baseline performance usually indicates lower diversity in the LLM generations even with a large temperature $\gamma$, resulting in a limited accessible search space for the model to find a better solution.</p>
<p>Commonsense Reasoning. Table 2 compares methods using CoT prompting in commonsense reasoning. Our approach shows consistent performance improvements across several tasks. For example, on StrategyQA, we achieve an accuracy of $77.2\%$ compared with $73.2\%$ of the baseline.</p>
<p>Table 1: Result comparison (accuracy $\%$ ) on arithmetic and symbolic reasoning tasks. The best result is in bold and the lowest cost is in green. We report methods all with Codex backbone for a fair comparison. Similar to <em>Huang et al. (2022)</em>, Diverse <em>(Li et al., 2022)</em> fine-tune task-specific verifiers to apply weights on samples in self-consistency (SC). Other fine-tuning methods include reward-based supervision <em>(Uesato et al., 2022)</em> and content-specific training <em>(Lewkowycz et al., 2022)</em>. We also report the number of tokens (# Tokens) on GSM8K to compare the costs of different methods.</p>
<table>
<thead>
<tr>
<th>Approach</th>
<th>GSM8K</th>
<th>Arithmetic</th>
<th></th>
<th></th>
<th>ASDiv</th>
<th>TabMWP</th>
<th>Symbolic</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td># Tokens</td>
<td>AQuA</td>
<td>SVAMP</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>single reasoning chain</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>CoT</td>
<td>65.6</td>
<td>0.2k</td>
<td>45.3</td>
<td>74.8</td>
<td>76.9</td>
<td>65.2</td>
<td>64.8</td>
<td>73.0</td>
</tr>
<tr>
<td>PoT</td>
<td>71.6</td>
<td>-</td>
<td>54.1</td>
<td>85.2</td>
<td>-</td>
<td>73.2</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>PAL</td>
<td>72.0</td>
<td>0.3k</td>
<td>-</td>
<td>79.4</td>
<td>79.6</td>
<td>-</td>
<td>76.2</td>
<td>96.7</td>
</tr>
<tr>
<td>Ours-PAL</td>
<td>80.2</td>
<td>27.7k</td>
<td>55.9</td>
<td>89.6</td>
<td>84.9</td>
<td>79.1</td>
<td>78.6</td>
<td>96.8</td>
</tr>
<tr>
<td></td>
<td></td>
<td>multiple reasoning chains</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>CoT, SC</td>
<td>78.0</td>
<td>5.3k</td>
<td>52.0</td>
<td>86.8</td>
<td>-</td>
<td>75.4</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>CoT, Diverse</td>
<td>82.3</td>
<td>-</td>
<td>-</td>
<td>87.0</td>
<td>88.7</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>PoT, SC</td>
<td>80.0</td>
<td>-</td>
<td>58.6</td>
<td>89.1</td>
<td>-</td>
<td>81.8</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>PAL, SC</td>
<td>80.4</td>
<td>7.4k</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Ours-PAL</td>
<td>85.5</td>
<td>550.0k</td>
<td>64.2</td>
<td>90.3</td>
<td>85.8</td>
<td>80.9</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Table 2: Result comparison (accuracy $\%$ ) on commonsense reasoning tasks, with Codex backbone. Here we only report results in the single reasoning chain scenario following <em>Wei et al. (2022b)</em>. We report # Tokens on StrategyQA for cost comparison.</p>
<table>
<thead>
<tr>
<th>Approach</th>
<th>StrategyQA</th>
<th># Tokens</th>
<th>CommonsenseQA</th>
<th>Sports</th>
</tr>
</thead>
<tbody>
<tr>
<td>CoT</td>
<td>73.2</td>
<td>0.06k</td>
<td>77.9</td>
<td>$\mathbf{9 8 . 5}$</td>
</tr>
<tr>
<td>Ours-CoT</td>
<td>$\mathbf{7 7 . 2}$</td>
<td>11.6k</td>
<td>$\mathbf{7 8 . 6}$</td>
<td>98.4</td>
</tr>
<tr>
<td>Human</td>
<td>87.0</td>
<td>-</td>
<td>88.9</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Likewise, the performance of our approach is constrained by the low diversity of LLM generations on Sporting Understanding, as we observe on Object Counting in symbolic reasoning.</p>
<p>Computational Cost Overhead. Despite the fact that our approach achieves significant improvement on various benchmarks, we observe an overhead of computational cost compared with the corresponding baselines. For example, the single-chain version of our approach using PAL costs about 3 times more than the self-consistency baseline on GSM8K. As detailed in Appendix A.3, this is due to a relatively large hyperparameter – the number of rollouts per beam $n$ – which we set as 16 for better performance. To strike a balance between performance and cost and present a complete picture, we adopt $n=2$ and conduct cost–performance analysis on our approach in Section 3.3.</p>
<h3>3.3 Cost Analysis</h3>
<p>Table 3 compares the baseline and our approach under comparable computational budgets (measured in # Tokens). Our method consistently outperforms self-consistency on the arithmetic reasoning tasks even when benchmarked for relatively less computational cost. For example, we achieve 46.1% on GSM8K with a cost of 12.6k tokens, compared with the accuracy of 41.8% of self-consistency which costs 13.9k tokens. Figure 4 further illustrates the cost-efficiency of our approach on GSM8K using different prompting methods under various levels of costs. Our approach significantly outperforms the corresponding equal-cost baseline especially when the computational budget increases, indicating the improvement in the performance upper bound brought by our method.</p>
<p>However, our approach lags behind the CoT baseline on commonsense reasoning. This implies the limitation of our method when applied to shorter reasoning chains, i.e., decreasing the number</p>
<p>Table 3: Cost (# Tokens) and result (accuracy $\%$ ) comparison on arithmetic and commonsense reasoning tasks. We base our experiments on Llama-2 (13B) since Codex is not available. We show the results of the baseline and our method both in the multiple-chain scenario for a fair comparison. Here we use PAL and CoT prompting for arithmetic and commonsense reasoning, respectively.</p>
<table>
<thead>
<tr>
<th>Approach</th>
<th>Arithmetic (PAL)</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Commonsense (CoT)</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>GSM8K</td>
<td>AQuA</td>
<td>SVAMP</td>
<td>ASDiv</td>
<td>TabMWP</td>
<td>StrategyQA</td>
<td>CommonsenseQA</td>
</tr>
<tr>
<td>Baseline</td>
<td>41.8</td>
<td>30.7</td>
<td>71.2</td>
<td>66.2</td>
<td>43.7</td>
<td>$\mathbf{7 1 . 0}$</td>
<td>$\mathbf{7 4 . 4}$</td>
</tr>
<tr>
<td># Tokens</td>
<td>$13.9 k$</td>
<td>$6.6 k$</td>
<td>$5.9 k$</td>
<td>$2.7 k$</td>
<td>$1.9 k$</td>
<td>$2.7 k$</td>
<td>$1.2 k$</td>
</tr>
<tr>
<td>Ours</td>
<td>$\mathbf{4 6 . 1}$</td>
<td>$\mathbf{3 1 . 5}$</td>
<td>$\mathbf{7 4 . 6}$</td>
<td>$\mathbf{6 7 . 7}$</td>
<td>$\mathbf{4 9 . 6}$</td>
<td>70.6</td>
<td>74.0</td>
</tr>
<tr>
<td># Tokens</td>
<td>$12.6 k$</td>
<td>$6.0 k$</td>
<td>$5.0 k$</td>
<td>$2.5 k$</td>
<td>$1.2 k$</td>
<td>$2.6 k$</td>
<td>$1.2 k$</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Accuracy curves on GSM8K of different methods with the change of the cost. We conduct the performance comparison using both PAL and CoT prompting with Llama-2 (13B) backbone.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Distributions of the self-evaluation score and its components (<em>i.e.</em>, generation confidence $\mathcal{P}$ and correctness confidence $\mathcal{C}$) on correct/incorrect baseline predictions. We highlight the median scores of the positive and negative cases using lines of the same colors respectively.</p>
<p>Of intermediate steps, we akens the effect of stepwise self-evaluation in beam search in reducing error accumulation. On the other hand, self-consistency can directly improve performance through instance-level aggregation without additional cost for self-evaluation. We analyze how our method benefits longer reasoning chains on different tasks in Section 3.4.</p>
<h3>3.4 Further Analysis</h3>
<p>We now provide a detailed analysis of why our method achieves significant gains.</p>
<p>Generation and Self-evaluation Calibration. We investigate the distributions of generation confidence (<em>i.e.</em>, the LM probability $\mathcal{P}$) and correctness confidence $\mathcal{C}$ in our self-evaluation score $\mathcal{E}$. By</p>
<p>Table 4: Absolute accuracy (in $\%$ ) increases on instances of different complexity determined by the length of reasoning chains (represented as $#$ Steps).</p>
<p>| GSM8K | | | | | StrategyQA | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" />
(a) Examples of self-evaluation score distribution of different predictions on the GSM8K dataset.
<img alt="img-5.jpeg" src="img-5.jpeg" />
(b) Examples of self-evaluation score distribution of different predictions on the StrategyQA dataset. We also provide explanations corresponding to the ground-truth answers for reference.</p>
<p>Figure 7: Comparisons among predictions of high and low self-evaluation scores on arithmetic (7a for GSM8K) and commonsense (7b for StrategyQA) reasoning tasks. Scores from low to high are visualized from orange $(0.0)$, yellow $(0.4)$, to green $(1.0)$. Here $\mathcal{C}, \mathcal{P}$, and $\mathcal{E}$ represent the evaluation confidence, the generation confidence, and their combination as the final score, respectively.
from the improved coverage of the plausible generations and the ensembling effect. Nevertheless, one can adjust the sampling-related parameters (i.e., $\tau$ and $\alpha$ ) to incorporate more randomness into the generations. In practice, we find that a moderate temperature decay (e.g., $\alpha=0.5$ ) results in improved performance. We conduct further analysis of the effect of sampling diversity in Appendix A.2.</p>
<p>Qualitative Analysis. We examine particular instances to investigate the behavior of correctness confidence scores $\mathcal{C}$ and generation probabilities $\mathcal{P}$ in different scenarios. From the comparison shown in Figure 7, we have the following main observations:</p>
<ul>
<li>In general, the correctness confidence is more effective at identifying logical errors, taking into account the accumulated mistakes from prior steps, while the generation probability focuses more on text perplexity as the confidence of the generation LLM.</li>
<li>When comparing arithmetic and commonsense tasks, LLMs exhibit greater confidence in dealing with structured and objective reasoning chains such as problems in GSM8K, for both generation and self-evaluation, as opposed to reasoning chains in StrategyQA.</li>
<li>Reasoning chains that appear logically plausible can achieve high correctness confidence scores but still result in incorrect answers, as demonstrated in $R_{41}$ in Figure 7b. Moreover, the correctness confidence can be influenced by minor details (e.g., imperfect variable naming in PAL reasoning) and assign low scores regardless of the correctness of the final answers as shown in $R_{22}$ in Figure 7a.</li>
<li>Incoherence due to a sudden jump in reasoning (e.g., $R_{32}$ in Figure 7b) can lead to low correctness confidence. Additionally, the correctness confidence tends to be lower when the generation LLM makes a probability statement with less certainty, such as "it seems" as illustrated by $R_{42}$ in Figure 7b.</li>
</ul>
<h1>4 Related Work</h1>
<p>Reasoning Formulation. Several studies have attempted to better formulate the reasoning problem. One approach is to generate rationales to enhance model interpretability (Zhou et al., 2020; Wiegreffe and Marasovic, 2021; Wiegreffe et al., 2021). Recently, the focus has shifted towards decomposing the reasoning process into intermediate steps before reaching the final answer (Wei et al., 2022b; Zhou et al., 2023; Gao et al., 2023; Chen et al., 2022). Various decomposition techniques have been explored, such as question reduction (Zhou et al., 2023; Yang et al., 2022), iterative prompting (Wang et al., 2022), and chaining the steps (Wu et al., 2022). While incorporating intermediate reasoning steps has resulted in substantial performance improvements, errors or imperfections can accumulate, especially when the chains become longer (Wu et al., 2016; Guo et al., 2018). As such, we utilize LLM self-evaluation as a stepwise criterion to improve the chaining process.</p>
<p>LLM Self-Evaluation. Recent research on LLM calibration shows that current LLMs' probabilistic predictions correspond well with actual token occurrence frequencies, leading to well-calibrated predictions for specific tasks (Rae et al., 2021; Kadavath et al., 2022; Guo et al., 2017; Kadavath et al., 2022; Jiang et al., 2021; Kuhn et al., 2023). Notably, scaling model size plays a crucial role in enhancing calibration (Rae et al., 2021; Wei et al., 2022a). As LLMs exhibit good calibration, an increasing number of studies focus on prompting LLMs to perform self-evaluation as a means of verification (Zhang et al., 2023; Shinn et al., 2023; Madaan et al., 2023; Paul et al., 2023). Selfevaluation provides an effective and efficient assessment method without requiring task-specific verifier fine-tuning, which typically involves additional annotations ( Li et al., 2022). In contrast to existing works that refine generation results through instance-level self-evaluation, our approach applies self-evaluation results as a stepwise criterion to calibrate generation at a finer granularity. By focusing on step-by-step self-evaluation, our method enables fine-grained guided decoding, addressing the challenges associated with complex or lengthy reasoning.</p>
<p>Decoding Strategies. A tradeoff typically exists between diversity and quality. Deterministic decoding methods such as greedy decoding and beam search (Jurafsky and Martin, 2009; Graves, 2012) often produce high-quality results but lack diversity (Stahlberg and Byrne, 2019; Meister et al., 2020). Temperature sampling (Ackley et al., 1985), top-k sampling (Fan et al., 2018), and top-p sampling (Holtzman et al., 2020) are various techniques used to enhance diversity. The recent work of tree-of-thought (Yao et al., 2023) explores different search algorithms such as breadth-first and depthfirst searches tailored for different tasks. Differently, we propose a unified framework of stochastic beam search (Caccia et al., 2020; Kool et al., 2019; Meister et al., 2021), which combines beam search and temperature sampling to balance the quality-diversity trade-off in multi-step reasoning.</p>
<h2>5 Discussion</h2>
<p>We have introduced a multi-step decoding method that calibrates reasoning with stepwise selfevaluation guidance via stochastic beam search for current large language models. The empirical success of our method across a broad range of tasks, from arithmetic and symbolic to commonsense reasoning, demonstrates its robustness and generalizability in various application areas. The significant performance gains of our method on long reasoning chains also highlight its applicability to other multi-step tasks, such as multi-hop question answering and more complex scenarios involving multi-modal understanding, reasoning, and planning. In future work, we will investigate how to utilize external tools to further enhance the calibration and explore its generalizability on other multi-step scenarios to deal with more complex information such as external knowledge and multimodalities.</p>
<h2>Potential Impacts and Limitations</h2>
<p>We propose self-evaluation guided stochastic beam search to facilitate multi-step reasoning. However, our approach, based on stepwise self-evaluation guidance, has certain limitations. It requires access to LLM logits to calculate the self-evaluation score, restricting its applicability to more powerful LLMs, such as GPT-4, which do not provide token likelihoods. Plus, multi-step decoding inherently causes additional costs from candidate sampling and self-evaluation. For optimal balance between efficiency and cost, our approach is best applied to longer reasoning chains, where the cumulative effect of calibration across multiple steps can improve the overall performance more significantly.</p>
<h1>Acknowledgments and Disclosure of Funding</h1>
<p>The computational work for this article was partially performed on resources of the National Supercomputing Centre (NSCC), Singapore ${ }^{7}$. We would like to thank Prof. Hwee Tou Ng for his insightful discussions that enhanced the depth and quality of our study.</p>
<h2>References</h2>
<p>David H. Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski. 1985. A learning algorithm for boltzmann machines. Cognitive Science, 9(1):147-169.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin. 2020. Language gans falling short. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2022. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. CoRR, abs/2211.12588.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168.</p>
<p>Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889-898, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. PAL: program-aided language models. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 10764-10799. PMLR.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies. Trans. Assoc. Comput. Linguistics, 9:346-361.</p>
<p>Alex Graves. 2012. Sequence transduction with recurrent neural networks. CoRR, abs/1211.3711.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 1321-1330. PMLR.</p>
<p>Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. 2018. Long text generation via adversarial training with leaked information. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1).</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve.</p>
<p>Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. 2021. How can we know When language models know? on the calibration of language models for question answering. Trans. Assoc. Comput. Linguistics, 9:962-977.</p>
<p>Dan Jurafsky and James H. Martin. 2009. Speech and language processing : an introduction to natural language processing, computational linguistics, and speech recognition. Pearson Prentice Hall, Upper Saddle River, N.J.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022. Language models (mostly) know what they know. CoRR, abs/2207.05221.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In NeurIPS.</p>
<p>Wouter Kool, Herke van Hoof, and Max Welling. 2019. Stochastic beams and where to find them: The gumbel-top-k trick for sampling sequences without replacement. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 3499-3508. PMLR.</p>
<p>Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. Solving quantitative reasoning problems with language models. In NeurIPS.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2022. On the advance of making language models better reasoners.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 158-167. Association for Computational Linguistics.</p>
<p>Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. 2023. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. CoRR, abs/2303.17651.</p>
<p>Clara Meister, Afra Amini, Tim Vieira, and Ryan Cotterell. 2021. Conditional poisson stochastic beam search. CoRR, abs/2109.11034.</p>
<p>Clara Meister, Ryan Cotterell, and Tim Vieira. 2020. Best-first beam search. Trans. Assoc. Comput. Linguistics, 8:795-809.</p>
<p>Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020. A diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 975-984. Association for Computational Linguistics.</p>
<p>Maxwell I. Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show your work: Scratchpads for intermediate computation with language models. CoRR, abs/2112.00114.</p>
<p>OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 2080-2094. Association for Computational Linguistics.</p>
<p>Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 2023. REFINER: reasoning feedback on intermediate representations. CoRR, abs/2304.01904.</p>
<p>Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, PoSen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling language models: Methods, analysis \&amp; insights from training gopher. CoRR, abs/2112.11446.</p>
<p>Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: an autonomous agent with dynamic memory and self-reflection. CoRR, abs/2303.11366.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. CoRR, abs/2206.04615.</p>
<p>Felix Stahlberg and Bill Byrne. 2019. On NMT search errors and model errors: Cat got your tongue? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3356-3362, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4149-4158. Association for Computational Linguistics.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288.</p>
<p>Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with process- and outcome-based feedback.</p>
<p>Boshi Wang, Xiang Deng, and Huan Sun. 2022. Shepherd pre-trained language models to develop a train of thought: An iterative prompting approach. CoRR, abs/2203.08383.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a. Emergent abilities of large language models. Trans. Mach. Learn. Res., 2022.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022b. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS.</p>
<p>Sarah Wiegreffe and Ana Marasovic. 2021. Teach me to explain: A review of datasets for explainable natural language processing. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.</p>
<p>Sarah Wiegreffe, Ana Marasovic, and Noah A. Smith. 2021. Measuring association between labels and free-text rationales. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 10266-10284. Association for Computational Linguistics.</p>
<p>Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022. Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, CHI '22, New York, NY, USA. Association for Computing Machinery.</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144.</p>
<p>Jingfeng Yang, Haoming Jiang, Qingyu Yin, Danqing Zhang, Bing Yin, and Diyi Yang. 2022. SEQZERO: few-shot compositional semantic parsing with sequential prompts and zero-shot models. In Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 49-60. Association for Computational Linguistics.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. CoRR, abs/2305.10601.</p>
<p>Tianyi Zhang, Tao Yu, Tatsunori Hashimoto, Mike Lewis, Wen-Tau Yih, Daniel Fried, and Sida Wang. 2023. Coder reviewer reranking for code generation. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 41832-41846. PMLR.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. 2023. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>Wangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan Xiong, and Jian Tang. 2020. Towards interpretable natural language understanding with explanations as latent variables. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<h1>A Appendix</h1>
<h2>A. 1 Theoretical Analysis of Eq. 4</h2>
<p>In Eq. 4, we use $\mathcal{S}$ sampled from the language model $\mathrm{LM}<em _mathrm_LM="\mathrm{LM">{\mathcal{G}}$ generations. This is an approximation for sampling from the infinite set of all possible chaining paths. And the finite set $\mathcal{S}$ is constructed based on the generation $\mathrm{LM} P</em><em s_1:="s^{1:" t="t">{\mathcal{G}}}$, which is different from our target distribution as shown in Eq. 4.
Specifically, denote the infinite set of all possible generated completions till the $t$-th step as $\mathcal{S}^{<em>}$, we approximate sampling from $P_{\text {beam }}^{</em>}\left(s^{1: t}\right)=\frac{\exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)}{\sum</em>^{} \in \mathcal{S<em>}} \exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)}$ via $P_{\text {beam }}\left(s^{1: t}\right)=\frac{\exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)}{\sum_{s^{1: t} \in \mathcal{S}} \exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)}$, where $\mathcal{S}$ is the approximation of $\mathcal{S}^{</em>}$ with $|\mathcal{S}|=n k=M \leq\left|\mathcal{S}^{<em>}\right|$.
Define the upper bound $\bar{c}$ and the lower bound $\underline{c}$ on each $\exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)$ as $\bar{c} \geq \exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right) \geq \underline{c}$ for all $s^{1: t} \in \mathcal{S}^{</em>}$. Define the ratio as $r=\bar{c} / \underline{c}$. Note that $\underline{c} \geq 1$ since $\mathcal{E}\left(s^{1: t}\right) / \tau \geq 0$. Thus, we can take $r \leq \bar{c}$.
We now give the following proposition which shows that $\left|P_{\text {beam }}^{<em>}\left(s^{1: t}\right)-P_{\text {beam }}\left(s^{1: t}\right)\right|$ decreases at the rate of $\mathcal{O}\left(\frac{1-M /\left|\mathcal{S}^{</em>}\right|}{M}\right)$ toward 0 as $M$ increases. Note that as $M$ increases toward $\left|\mathcal{S}^{<em>}\right|$, the numerator $1-M /\left|\mathcal{S}^{</em>}\right|$ decreases toward 0 while the factor for the denominator $\frac{1}{M}$ also decreases.
Proposition 1. For any $s^{1: t}$, the difference between $P_{\text {beam }}^{*}\left(s^{1: t}\right)$ and $P_{\text {beam }}\left(s^{1: t}\right)$ is bounded by</p>
<p>$$
\left|P_{\text {beam }}^{<em>}\left(s^{1: t}\right)-P_{\text {beam }}\left(s^{1: t}\right)\right| \leq r^{2}\left(\frac{1-M /\left|\mathcal{S}^{</em>}\right|}{M}\right)
$$</p>
<p>Proof. We now prove the second statement by analyzing the absolute difference:</p>
<p>$$
\begin{aligned}
&amp; \left|P_{\text {beam }}^{<em>}\left(s^{1: t}\right)-P_{\text {beam }}\left(s^{1: t}\right)\right| \
= &amp; \left|\frac{\exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)}{\sum_{s^{1: t} \in \mathcal{S}^{</em>}} \exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)}-\frac{\exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)}{\sum_{s^{1: t} \in \mathcal{S}} \exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)}\right| \
= &amp; \frac{\exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)\left|\sum_{s^{1: t} \in \mathcal{S}^{<em>}} \exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)-\sum_{s^{1: t} \in \mathcal{S}} \exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)\right|}{\left(\sum_{s^{1: t} \in \mathcal{S}} \exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right) \sum_{s^{1: t} \in \mathcal{S}^{</em>}} \exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)\right)} \
= &amp; \frac{\exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)\left|\sum_{s^{1: t} \in \mathcal{S}^{<em>} \backslash \mathcal{S}} \exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)\right|}{\left(\sum_{s^{1: t} \in \mathcal{S}} \exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)\right) \sum_{s^{1: t} \in \mathcal{S}^{</em>}} \exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)}
\end{aligned}
$$</p>
<p>Since $\exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)$ is nonnegative, using the upper bound on each $\exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)$, we have:</p>
<p>$$
\left|P_{\text {beam }}^{<em>}\left(s^{1: t}\right)-P_{\text {beam }}\left(s^{1: t}\right)\right| \leq \frac{\bar{c}^{2}\left(\left|\mathcal{S}^{</em>}\right|-M\right)}{\left(\sum_{s^{1: t} \in \mathcal{S}} \exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)\right) \sum_{s^{1: t} \in \mathcal{S}^{*}} \exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)}
$$</p>
<p>Similarly, using the lower bound on each $\exp \left(\mathcal{E}\left(s^{1: t}\right) / \tau\right)$,</p>
<p>$$
\left|P_{\text {beam }}^{<em>}\left(s^{1: t}\right)-P_{\text {beam }}\left(s^{1: t}\right)\right| \leq \frac{\bar{c}^{2}\left(\left|\mathcal{S}^{</em>}\right|-M\right)}{\underline{c}^{2}\left|\mathcal{S}^{<em>}\right| M}=r^{2}\left(\frac{1-M /\left|\mathcal{S}^{</em>}\right|}{M}\right)
$$</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Accuracy curves with different sampling diversity. The two plots show the changes in performance on GSM8K when the sampling temperature $\tau$ and its decay ratio $\alpha$ vary, respectively.</p>
<p>Sampling Diversity. In accordance with Figure 6b, we observe similar results when ablating the sampling hyperparameters $\tau$ and $\alpha$ for the single reasoning chain case, as shown in Figure 8. Increasing $\tau$ and $\alpha$ generally adds more diversity to the decoding process, but excessive randomness negatively impacts the performance of the single-chain decoding. Generally, a moderate temperature decay results in improved performance. Therefore, we set $\alpha=0.5$ throughout our experiments for simplicity and only tune $\tau$ for randomness control.
<img alt="img-7.jpeg" src="img-7.jpeg" />
(a) $\lambda$-AUC curves of $\mathcal{E}<em _lambda="\lambda">{\lambda}$ on GSM8K (PAL).
<img alt="img-8.jpeg" src="img-8.jpeg" />
(b) $\lambda$-AUC curves of $\mathcal{E}</em>$ on StrategyQA (CoT).</p>
<p>Figure 9: The change of AUC scores with different values of $\lambda$ in $\mathcal{E}<em _lambda="\lambda">{\lambda}$. We calculate the AUC score as how $\mathcal{E}</em>$ can successfully determine whether the corresponding predicted reasoning chain can produce the ground-truth answer. The predictions here are from the baseline methods (i.e., CoT \&amp; PAL) with different LM temperatures $\gamma$, as represented by curves of different colors.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Comparison between predictions of high v.s. low self-evaluation scores on instance-level accuracy.</p>
<p>More Analysis on Self-Evaluation. Recall that we use a combination of generation confidence and faithfulness score as $\mathcal{E}_{\lambda}=\mathcal{C}^{\lambda} \cdot \mathcal{P}^{(1-\lambda)}$, with $\lambda \in[0,1]$. In our experiments, we set $\lambda=0.5$ for all tasks for simplicity. However, we investigate its effects here since, intuitively, it is an important hyperparameter for distinguishing correct / incorrect predictions and might require different values for various reasoning tasks and datasets. Its effect is also coupled with the language model temperature $\gamma$.</p>
<p>Figure 9 demonstrates how $\lambda$ functions on arithmetic (GSM8K) and commonsense (StrategyQA). In general, we observe that the performance remains relatively stable with different choices of $\lambda$ on different datasets, although fine-tuning this hyperparameter might lead to further improvements. This stability suggests that the choice of $\lambda$ is not overly sensitive across various reasoning tasks and datasets, but exploring its optimal value for specific tasks could potentially lead to even better performances.</p>
<p>To examine the influence of incorporating faithfulness on LLM final predictions, we plot the distributions of changes in different scores, specifically the faithfulness score $\mathcal{C}$, the generation confidence $\mathcal{P}$, and the overall decoding score $\mathcal{E}_{\lambda}$ on the baseline reasoning chains and the reasoning chains generated by our method. We categorize the data points into 4 sets based on whether our approach changes the final prediction. Since the majority of the data points belong to the "both correct" set (in blue), where both baselines and our method generate accurate predictions, we particularly highlight the last two sets (in green and red), where our method results in improvement and degradation, respectively.</p>
<p>As shown in Figure 11, faithfulness typically works by significantly increasing the evaluation confidence $\mathcal{C}$ of model predictions, while the generation confidence $\mathcal{P}$ remains similar to that of the baseline methods. Specifically, for the evaluation confidence $\mathcal{C}$, our approach corrects the original predictions by increasing the confidence scores. This indicates that evaluation confidence plays a crucial role in guiding the decoding toward a better reasoning choice in decomposed reasoning. The increase is more significant for PAL when compared with CoT. This demonstrates that LLMs are generally better at judging the logic in reasoning that is more structured, while the free-text intermediate steps (e.g., CoT reasoning) may be challenging to conduct information extraction and soundness checking.</p>
<p>A similar conclusion can be drawn from Figure 10, where the difference in instance-level accuracy distributions between high-scored and low-scored predictions is more significant on the GSM8K dataset. For StrategyQA, while the incorporation of faithfulness helps, the level of the score value does not align well with whether the prediction is correct. For example, most of the low-scored predictions can still obtain the correct answers, as shown by the plot on the right of Figure 10b.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" />
(c) Distributions of score shifts on StrategyQA using CoT prompting.</p>
<p>Figure 11: Distributions of changes in scores from baselines to our method. Since the prediction correctness keeps unchanged most of the time (i.e., "both correct/incorrect" in blue/orange), we specifically plot how the scores shift on data points where the predictions get corrected or incorrect, as shown in green and red, respectively.</p>
<p>Table 5: Impact of LLM backends (Codex vs. ChatGPT vs. GPT-4) and prompting methods (PAL vs. CoT). The results of ChatGPT (gpt-3.5-turbo) were obtained on 20 March 2023.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>GSM8K</th>
<th>StrategyQA</th>
</tr>
</thead>
<tbody>
<tr>
<td>CoT ${ }_{\text {Codex }}$</td>
<td>65.6</td>
<td>73.2</td>
</tr>
<tr>
<td>PAL ${ }_{\text {Codex }}$</td>
<td>72.0</td>
<td>-</td>
</tr>
<tr>
<td>CoT ${ }_{\text {ChatGPT }}$</td>
<td>80.8</td>
<td>65.9</td>
</tr>
<tr>
<td>PAL ${ }_{\text {ChatGPT }}$</td>
<td>78.7</td>
<td>-</td>
</tr>
<tr>
<td>CoT ${ }_{\text {GPT-4 }}$</td>
<td>$\mathbf{9 2 . 0}$</td>
<td>-</td>
</tr>
<tr>
<td>Ours (CoT) ${ }_{\text {Codex }}$</td>
<td>71.9</td>
<td>$\mathbf{7 7 . 2}$</td>
</tr>
<tr>
<td>Ours (PAL) ${ }_{\text {Codex }}$</td>
<td>80.2</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>LLM Backbone Study. We are interested in how stronger LLMs (i.e., ChatGPT, GPT-4 (OpenAI, 2023)) work, but they are not directly compatible with our approach since the API does not return token logits.</p>
<p>Table 5 compares the results of various backend LLMs (i.e., Codex, ChatGPT, and GPT-4) on GSM8K. In arithmetic reasoning with PAL prompting, our Codex-based method achieves competitive results ( $80.2 \%$ vs. $78.7 \%$ ) even when compared with ChatGPT. The results are consistent across other datasets, including AQuA ( $55.9 \%$ vs. $54.7 \%$ ), SVAMP ( $89.6 \%$ vs. $84.1 \%$ ), ASDiv ( $84.9 \%$ vs. $84.1 \%$ ), and TabMWP ( $79.1 \%$ vs. $80.6 \%$ ). In commonsense reasoning, our method using Codex significantly outperforms ChatGPT-based methods across different datasets, including StrategyQA ( $77.2 \%$ vs. $65.9 \%$ ), CommonsenseQA ( $78.6 \%$ vs. $75.2 \%$ ) and Sports Understanding ( $98.4 \%$ vs. $95.9 \%$ ). One possible explanation is that ChatGPT lacks sufficient world knowledge for effective fact checking and commonsense reasoning. Given the significant performance improvement of GPT-4, we conduct further analysis about how to synergistically combine it with our method.</p>
<p>GPT-4 Experiments The recently launched GPT-4 has demonstrated notable improvements in reasoning capabilities across a variety of tasks. In this section, we examine and compare the reasoning skills of different large language models (LLMs), specifically Codex and GPT-4, in assessing and determining the accuracy of each step in a reasoning chain. We contrast the confidence scores and corresponding explanations for Codex $(\mathcal{C})$ and GPT-4 $(\mathcal{S})$ in the context of both arithmetic and commonsense reasoning, as shown in Figure 13 and Figure 14, respectively. For ease of visualization, we employ the same colormap (shown in Figure 12) as in Figure 7 to represent the scale of scores. Since OpenAI has not provided access to the token-wise likelihood of generated text, we directly request GPT-4 to score the reasoning steps using binary values . Moreover, we report the average of three evaluation results to reduce the variance of sampling discrete values, i.e., $S=\left(S_{1}+S_{2}+S_{3}\right) / 3, S_{i} \in[0,1]$.</p>
<p>As illustrated in Figure 13, GPT-4 demonstrates greater effectiveness in pinpointing the central logical error in arithmetic reasoning. For instance, we can observe that $\mathcal{S}&lt;\mathcal{C}$ for alex_total = alex_weight + weight_multiplier + grace_weight and $\mathcal{S}&gt;\mathcal{C}$ for answer = grace_weight + alex_total, where the former leads to an incorrect final answer. Additionally, GPT-4 typically offers detailed explanations and alternative solutions. As seen in the step answer = grace_weight + alex_total, GPT-4 can correct minor errors even when it arrives at the correct final answer. However, GPT-4 may still encounter difficulties in detecting small errors within the text, which can have a significant impact on logical consistency. This challenge is illustrated by the substantial variance in $\mathcal{S}$ for the step alex_total = alex_weight + weight_multiplier + grace_weight.</p>
<p>The benefits of well-crafted explanations in GPT-4 become more significant when handling complex reasoning tasks, as demonstrated in Figure 14. For instance, in the $R_{42}$ of $Q_{4}$ shown in Figure 7b, Codex generally assigns high evaluation scores for all steps. Although this reasoning chain leads to the correct final answer, it makes some overly definitive assumptions without reasonable justification (e.g., "must have attributes that match both"). In such cases, GPT-4 can accurately identify these vague statements through meticulous analysis. Moreover, the comprehensive analysis helps address the growing uncertainty arising from the ambiguity in understanding commonsense questions. For example, in the final step, GPT-4 offers extensive explanations for assigning low $\mathcal{S}$ scores, considering the limited information available to comprehend the question.</p>
<p>In summary, GPT-4 demonstrates promising improvements in LLM evaluation by providing more rigorous and in-depth explanations. While there is significant variation in its generated explanations for some challenging cases, these explanations are typically beneficial as they elaborate on understanding and interpretation based on the given information. This suggests the potential for using LLM explanations to enhance self-evaluation in future research.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Figure 12: Score from low to high values visualized using colormap from orange to green.
$\left|Q_{1}\right|$ Grace weighs 125 pounds. Alex weighs 2 pounds less than 4 times what Grace weighs. What are their combined weights in pounds?
[Ground-Truth $a_{1}{ }^{*}$ ] 623.0
$\left[\right.$ Predicted $\boldsymbol{a}<em 12="12">{12} \mid 627 \times$
$\left|R</em>\right|$ in Python
grace_weight $=125$
<img alt="img-11.jpeg" src="img-11.jpeg" />
$\begin{array}{ll}\text { alex_weight }=2 &amp; \ \text { alex_weight } \ \text { alex_weight } &amp; \end{array}$
<img alt="img-12.jpeg" src="img-12.jpeg" />
weight_multiplier $=4$
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 13: Comparison between Codex $(\mathcal{C})$ and GPT-4 $(\mathcal{S})$ on evaluating the reasoning chains in arithmetic reasoning. The example $\left(R_{12}\right.$ for $\left.Q_{1}\right)$ is the same one from Figure 7a. We underline the key statements in explanations that justify the judgment of GPT-4.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ The results using GPT-4 were obtained on 22 April 2023.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
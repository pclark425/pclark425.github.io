<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5818 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5818</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5818</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-f8a2dca1e8fe56e698984c077f7ff58d8ca867e9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f8a2dca1e8fe56e698984c077f7ff58d8ca867e9" target="_blank">Large Language Models as Optimizers</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work proposes Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language.</p>
                <p><strong>Paper Abstract:</strong> Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to our main application in prompt optimization, where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5818.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5818.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction insertion position (A_begin / Q_begin / Q_end)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction insertion position in prompt (A_begin, Q_begin, Q_end)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Where the generated instruction is inserted relative to the question/answer sequence (before question, after question, or at the beginning of the model's answer); this placement changes how the scorer LLM conditions and affects accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-L, text-bison, PaLM 2-L-IT, gpt-3.5-turbo, gpt-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (math reasoning), BBH (various tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Natural-language reasoning tasks where prompts/instructions are prepended or appended to inputs/outputs to steer model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Positions tested: A_begin (instruction added at start of model answer; used when scorer is pre-trained PaLM 2-L), Q_begin (instruction added before original question) and Q_end (instruction added after original question); different optimizer/scorer combos used corresponding to model instruction-tuning status.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared alternative insertion positions: A_begin vs Q_begin vs Q_end depending on scorer's interface.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Examples: With PaLM 2-L as scorer, baseline "Let's think step by step." at A_begin: accuracy 71.8. OPRO top A_begin instructions: PaLM 2-L-IT optimizer → 80.2 (A_begin); PaLM 2-L optimizer → 79.9 (A_begin); gpt-3.5-turbo optimizer → 78.5 (A_begin); gpt-4 optimizer → 74.5 (A_begin). With text-bison as scorer, baseline "Let's think step by step." at Q_begin: 64.4; OPRO best (text-bison scorer, text-bison optimizer at Q_end): 68.5 (Q_end), and text-bison scorer with PaLM 2-L-IT optimizer at Q_begin: 64.4.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>PaLM 2-L (A_begin) OPRO best 80.2 vs baseline "Let's think step by step." 71.8 ( +8.4 points). text-bison (Q_begin/Q_end) improvements more modest (baseline 64.4 vs best ~68.5).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+8.4 percentage points on GSM8K test accuracy (PaLM 2-L scorer, best OPRO A_begin vs baseline chain-of-thought), smaller gains for text-bison (~+4.1 points in some settings).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The insertion position interacts with the scorer model's instruction-tuning and decoding format: pre-trained PaLM 2-L is better handled via A_begin (few-shot-style answer prefix), while instruction-tuned text-bison expects Q_begin/Q_end; placing instruction in the location consistent with scorer conditioning yields better alignment and higher accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5818.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5818.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-thought vs other phrasings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-thought style prompt phrasing compared to alternative instruction phrasings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different phrasings that are semantically similar (e.g., 'Let's think step by step.' vs paraphrases) can produce substantially different LLM accuracies on the same task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-L (scorer), various optimizer LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (math reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problem benchmark evaluated in zero-shot or instruction-augmented settings.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot instruction-style prompts (single-sentence instructions). Examples include 'Let's think step by step.' and many paraphrases optimized by OPRO.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared human-designed chain-of-thought phrase 'Let's think step by step.' against multiple paraphrases and OPRO-discovered instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline 'Let's think step by step.' on PaLM 2-L: 71.8% accuracy. Other semantically similar variants: 'Let's solve the problem together.' 60.5% (training accuracy example), 'Let's work together to solve this problem step by step.' 49.4% (test accuracy reported as example of a low-performing paraphrase).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>'Let's think step by step.' 71.8% vs paraphrase 49.4% (example), effect sizes up to −22.4 percentage points for semantically similar phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Differences as large as ~22 percentage points reported between semantically similar instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>variable (can improve or severely reduce performance depending on phrasing)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>LLMs are highly sensitive to prompt wording; slight changes alter model activations and token-level conditioning. Paper suggests this sensitivity causes variance across single-step instructions and motivates generating multiple candidates per step for stability.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5818.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5818.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Meta-prompt ordering (optimization trajectory order)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Order of presenting previous instructions and scores in the meta-prompt (ascending / descending / random)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The order in which past solution-score pairs are shown in the meta-prompt affects optimizer LLM performance; presenting from lowest-to-highest score improved convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-bison (scorer), PaLM 2-L (optimizer) used in ablations; general across optimizers</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K and BBH sports_understanding (prompt optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompt optimization where optimizer LLM proposes instructions based on past instruction-score history (the optimization trajectory).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Meta-prompt includes optimization trajectory sorted in a chosen order; default is ascending (lowest to highest scores).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Ascending (default) vs descending vs random orderings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Ascending (lowest→highest) converged faster and achieved better final accuracies than descending or random (figures 7a/7b show quantitative advantage; exact numeric deltas shown in plots but no single summary number in text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitatively better convergence and higher final accuracy for ascending order; plotted differences indicate consistent improvement across repeats.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved when ordered lowest→highest</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Hypothesis: optimizer LLM outputs are biased toward the most recent tokens (recency bias), so placing better examples near the end amplifies their influence; ordering from low→high helps the model see the improvement trajectory and build upon top examples.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5818.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5818.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Showing instruction scores</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Including explicit accuracy scores for past instructions in the meta-prompt (rounded buckets / fine-grained / none)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Providing numeric training accuracies alongside past prompts helps the optimizer LLM understand quality differences and propose better new prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-bison (scorer) & PaLM 2-L (optimizer) in ablations; generalizable</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, BBH sports_understanding</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompt optimization using training-set accuracies as objective; scores can be shown in different granularities.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Meta-prompt variants: (1) round accuracies to integers (100 buckets, default), (2) bucketize to 20 buckets, (3) omit accuracies (only ordered instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Rounding to integers vs 20-bucket vs no score shown.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Including accuracy scores (rounded to integers) led to better final accuracies and faster convergence than not showing scores or using coarser bucketing (figures 7c/7d).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Default (integers) > 20-bucket > no-scores (qualitative ranking from ablation results).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Scores provide explicit signal distinguishing which past prompts were better, enabling the optimizer LLM to mimic high-quality patterns rather than solely inferring quality from ordering; this reduces ambiguity in the trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5818.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5818.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Exemplar count in meta-prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Number of task exemplars included in the meta-prompt (0 / 3 / 10)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Including a few exemplars from the training set in the meta-prompt is critical for optimizer LLMs to understand the task and produce well-phrased instructions; more exemplars do not necessarily help.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-bison (scorer) & PaLM 2-L (optimizer) in ablations</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, BBH sports_understanding</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompt optimization where meta-prompt optionally contains several input-output exemplars sampled from the training data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Meta-prompt with 0 exemplars, 3 exemplars (default), or 10 exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>3 exemplars (default) vs 10 exemplars vs none.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Including exemplars (3) is critical and substantially improves optimization vs none; increasing to 10 does not necessarily further improve and may harm by dominating context (qualitative result from Figures 7e/7f).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>3 exemplars (default) yielded the best tradeoff in reported ablations; 0 exemplars performed worse; 10 exemplars showed no consistent benefit and sometimes distracted optimizer.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (with small number of exemplars); more exemplars had no additional benefit or could reduce performance</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Exemplars clarify task format and where the generated instruction will be inserted; too many exemplars lengthen the meta-prompt and can drown optimization trajectory information, distracting the optimizer LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5818.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5818.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Optimizer sampling temperature</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sampling temperature of the optimizer LLM during candidate generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The temperature used by the optimizer LLM controls exploration vs exploitation: moderate temperature (1.0) yielded the best optimization stability and final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-L (optimizer) in ablations; other optimizers evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, BBH sports_understanding</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompt optimization where optimizer LLM samples multiple candidate instructions each step; temperature varied across experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Temperatures tested: 0.0, 0.5, 1.0 (default), 1.5, 2.0.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Low temperatures (0.0/0.5) vs medium (1.0) vs high (1.5/2.0).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Default temperature 1.0 achieved best results in ablations (Figure 10). Low temperatures lacked exploration and got stuck (flat curves); high temperatures ignored trajectory and lacked exploitation, causing instability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>T=1.0 > T in {0.0,0.5} and T in {1.5,2.0} (qualitative from plotted curves).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved at moderate temperature; too low reduces exploration, too high reduces exploitation</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Temperature mediates exploration-exploitation: lower temps produce conservative candidates (poor exploration), higher temps produce diverse but less trajectory-following candidates (poor exploitation).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5818.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5818.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Candidates per step (batching)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Number of instructions generated per optimization step</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generating multiple candidate instructions per optimization step improves stability and search efficiency, with 8 candidates per step found to be a strong tradeoff in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-L (optimizer) with text-bison scorer in ablations</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, BBH sports_understanding</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>OPRO generates k candidates per step and evaluates them; k varied in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>k ∈ {1,2,4,8 (default),16} candidates generated per step; total evaluation budget fixed across comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Comparisons across k values under fixed total candidate budget (so smaller k allows more steps).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Sampling 8 candidates per step overall achieved the best performance (Figure 8); too few candidates increased variance, too many reduced number of optimization steps and hurt long-term learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>k=8 > k in {1,2,4,16} in averaged ablation results.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (with moderate batch size)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Multiple candidates per step reduce instability analogous to mini-batch gradient estimation; but a balance is needed to allow sufficient optimization steps to exploit the trajectory information.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5818.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5818.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative trajectory optimization vs one-step generation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative optimization using the full optimization trajectory vs generating all candidate prompts in a single step</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OPRO's multi-step iterative approach that conditions on prior candidate-score history outperformed a one-shot generation of many candidates without trajectory information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-L-IT (optimizer) with PaLM 2-L (scorer) and PaLM 2-L-IT (optimizer) with text-bison (scorer) in comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K and BBH sports_understanding</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compare OPRO iterative optimization (generate 8 per step over many steps, conditioning on history) vs baseline that generates 50 candidates in one step (no trajectory).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Iterative multi-step meta-prompt including optimization trajectory vs single-step meta-prompt containing only exemplars and initial instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>OPRO iterative vs one-step generation baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM8K: One-step best-of-50 had training accuracy 64.4 and test 60.8; OPRO iterative found 'Let's do the math!' at step 5 with training 78.2 and test 76.3. BBH sports_understanding: one-step best train 84.0 test 80.0; OPRO found an instruction at step 4 with train 88.0 test 84.5.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Large improvements: GSM8K test +15.5 points (76.3 vs 60.8) comparing iterative OPRO vs one-step; BBH test +4.5 points (84.5 vs 80.0).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>GSM8K test improvement ~+15.5 percentage points; BBH sports_understanding test improvement ~+4.5 points in reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (iterative better)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Leveraging the optimization trajectory gives the optimizer LLM richer, temporally structured information about what has worked, enabling it to discover and refine high-quality prompt patterns across steps; single-step lacks trajectory signal and so misses progressive improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5818.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5818.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Starting point sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of initial instruction(s) used to start optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The initial prompt(s) influence convergence: for instruction-tuned scorers like text-bison the choice mattered less, while for pre-trained scorers like PaLM 2-L the starting instruction significantly affected early optimization and sometimes final quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-bison (scorer), PaLM 2-L (scorer), PaLM 2-L (optimizer) in ablations</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, BBH</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompt optimization starting from different initial instructions: empty string, 'Let's solve the problem.', 'Let's think step by step.', or combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Initial instruction choices varied by experiment and scorer (empty vs generic vs chain-of-thought).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Empty vs 'Solve the following problem.' vs 'Let's think step by step.' etc.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>For text-bison scorer, starting points produced similar accuracies and styles (differences small). For PaLM 2-L scorer, starting from 'Let's solve the problem.' produced better early performance than empty, and starting from 'Let's think step by step.' outperformed both throughout.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>PaLM 2-L: starting from 'Let's think step by step.' produced consistently better instructions than starting from empty or 'Let's solve the problem.' (qualitative and plotted differences in Figure 9b).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>variable; can improve or impede depending on scorer</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Stronger initial prompts with higher initial accuracies bias the meta-prompt toward better-quality regions and allow faster convergence; weaker starting points require more steps to 'get rid of' low-quality instructions present in the trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>For text-bison scorer, different starting instructions did not differ much in final performance, indicating model- and task-dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5818.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5818.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantically similar prompts phenomenon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantically similar prompts producing drastically different accuracies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observation that paraphrases or semantically overlapping instructions can have widely varying performance, increasing variance and oscillation during optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-L (scorer) in illustrative examples; phenomenon observed across models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (illustrative), BBH examples discussed</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical reasoning and BBH tasks where instruction variations produce variable accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Paraphrases and small rephrasings of instructions (e.g., chain-of-thought phrases and collaborative phrasing).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct paraphrase comparisons: e.g., 'Let's think step by step.' vs 'Let's work together to solve this problem step by step.' vs other paraphrases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Examples: 'Let's think step by step.' = 71.8% (PaLM 2-L), 'Let's solve the problem together.' = 60.5%, 'Let's work together to solve this problem step by step.' = 49.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Differences up to ~22.4 percentage points among semantically similar prompts on GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Up to ≈22 percentage-point differences cited in examples.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>variable (some paraphrases improve, others strongly reduce performance)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>LLMs are sensitive to lexical/structural details; small linguistic variations change token sequences and associated learned behaviors, leading to large performance variability. This motivates multi-candidate generation per step and iterative trajectory use.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5818.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e5818.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EvoPrompt comparison (use of exemplars / trajectory)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison between OPRO's meta-prompt (trajectory + exemplars) and EvoPrompt (GA/DE style prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OPRO which leverages multiple past prompts with scores and exemplars outperformed EvoPrompt variants that operate on pairwise crossover/mutation without exemplars in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (optimizer) compared across meta-prompts; PaLM 2-L and text-bison scorers in respective experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K and BBH sports_understanding</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Methods compared for discrete prompt optimization: OPRO (iterative trajectory + exemplars) vs EvoPrompt (GA/DE meta-prompts instructing LLM to mutate/crossover).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>OPRO meta-prompt contains several past prompts with scores and exemplars; EvoPrompt GA/DE meta-prompts operate on two prompts and apply crossover/mutation instructions without exemplars (in the experiment setup).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>OPRO vs EvoPrompt (GA) vs EvoPrompt (DE).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On GSM8K (with pre-trained PaLM 2-L scorer starting from two simple instructions), OPRO steadily improved while EvoPrompt GA/DE degraded performance. On BBH sports_understanding with task-specific initial prompts, EvoPrompt (DE) could improve but with less stability than OPRO (Figure 12).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>OPRO > EvoPrompt on GSM8K (qualitative and plotted); mixed results on BBH when EvoPrompt provided better initial prompts but still less stable.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>OPRO improved relative to EvoPrompt in reported experiments</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>EvoPrompt's lack of exemplars in these experiments deprived it of task grounding, so evolution-style operations on prompts lacked a task-specific signal; OPRO's trajectory + exemplars better guide the optimizer LLM to meaningful modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Automatic Prompt Engineer (APE) <em>(Rating: 2)</em></li>
                <li>APO: Automatic Prompt Optimization with Natural Language Feedback <em>(Rating: 2)</em></li>
                <li>EvoPrompt: Learning to Evolve Prompts for Large Language Models <em>(Rating: 2)</em></li>
                <li>The Curious Case of Neural Text Degeneration <em>(Rating: 1)</em></li>
                <li>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5818",
    "paper_id": "paper-f8a2dca1e8fe56e698984c077f7ff58d8ca867e9",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Instruction insertion position (A_begin / Q_begin / Q_end)",
            "name_full": "Instruction insertion position in prompt (A_begin, Q_begin, Q_end)",
            "brief_description": "Where the generated instruction is inserted relative to the question/answer sequence (before question, after question, or at the beginning of the model's answer); this placement changes how the scorer LLM conditions and affects accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM 2-L, text-bison, PaLM 2-L-IT, gpt-3.5-turbo, gpt-4",
            "model_size": null,
            "task_name": "GSM8K (math reasoning), BBH (various tasks)",
            "task_description": "Natural-language reasoning tasks where prompts/instructions are prepended or appended to inputs/outputs to steer model behavior.",
            "problem_format": "Positions tested: A_begin (instruction added at start of model answer; used when scorer is pre-trained PaLM 2-L), Q_begin (instruction added before original question) and Q_end (instruction added after original question); different optimizer/scorer combos used corresponding to model instruction-tuning status.",
            "comparison_format": "Compared alternative insertion positions: A_begin vs Q_begin vs Q_end depending on scorer's interface.",
            "performance": "Examples: With PaLM 2-L as scorer, baseline \"Let's think step by step.\" at A_begin: accuracy 71.8. OPRO top A_begin instructions: PaLM 2-L-IT optimizer → 80.2 (A_begin); PaLM 2-L optimizer → 79.9 (A_begin); gpt-3.5-turbo optimizer → 78.5 (A_begin); gpt-4 optimizer → 74.5 (A_begin). With text-bison as scorer, baseline \"Let's think step by step.\" at Q_begin: 64.4; OPRO best (text-bison scorer, text-bison optimizer at Q_end): 68.5 (Q_end), and text-bison scorer with PaLM 2-L-IT optimizer at Q_begin: 64.4.",
            "performance_comparison": "PaLM 2-L (A_begin) OPRO best 80.2 vs baseline \"Let's think step by step.\" 71.8 ( +8.4 points). text-bison (Q_begin/Q_end) improvements more modest (baseline 64.4 vs best ~68.5).",
            "format_effect_size": "+8.4 percentage points on GSM8K test accuracy (PaLM 2-L scorer, best OPRO A_begin vs baseline chain-of-thought), smaller gains for text-bison (~+4.1 points in some settings).",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "The insertion position interacts with the scorer model's instruction-tuning and decoding format: pre-trained PaLM 2-L is better handled via A_begin (few-shot-style answer prefix), while instruction-tuned text-bison expects Q_begin/Q_end; placing instruction in the location consistent with scorer conditioning yields better alignment and higher accuracy.",
            "counterexample_or_null_result": null,
            "uuid": "e5818.0",
            "source_info": {
                "paper_title": "Large Language Models as Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Chain-of-thought vs other phrasings",
            "name_full": "Chain-of-thought style prompt phrasing compared to alternative instruction phrasings",
            "brief_description": "Different phrasings that are semantically similar (e.g., 'Let's think step by step.' vs paraphrases) can produce substantially different LLM accuracies on the same task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM 2-L (scorer), various optimizer LLMs",
            "model_size": null,
            "task_name": "GSM8K (math reasoning)",
            "task_description": "Grade-school math word problem benchmark evaluated in zero-shot or instruction-augmented settings.",
            "problem_format": "Zero-shot instruction-style prompts (single-sentence instructions). Examples include 'Let's think step by step.' and many paraphrases optimized by OPRO.",
            "comparison_format": "Compared human-designed chain-of-thought phrase 'Let's think step by step.' against multiple paraphrases and OPRO-discovered instructions.",
            "performance": "Baseline 'Let's think step by step.' on PaLM 2-L: 71.8% accuracy. Other semantically similar variants: 'Let's solve the problem together.' 60.5% (training accuracy example), 'Let's work together to solve this problem step by step.' 49.4% (test accuracy reported as example of a low-performing paraphrase).",
            "performance_comparison": "'Let's think step by step.' 71.8% vs paraphrase 49.4% (example), effect sizes up to −22.4 percentage points for semantically similar phrasing.",
            "format_effect_size": "Differences as large as ~22 percentage points reported between semantically similar instructions.",
            "format_effect_direction": "variable (can improve or severely reduce performance depending on phrasing)",
            "explanation_or_hypothesis": "LLMs are highly sensitive to prompt wording; slight changes alter model activations and token-level conditioning. Paper suggests this sensitivity causes variance across single-step instructions and motivates generating multiple candidates per step for stability.",
            "counterexample_or_null_result": null,
            "uuid": "e5818.1",
            "source_info": {
                "paper_title": "Large Language Models as Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Meta-prompt ordering (optimization trajectory order)",
            "name_full": "Order of presenting previous instructions and scores in the meta-prompt (ascending / descending / random)",
            "brief_description": "The order in which past solution-score pairs are shown in the meta-prompt affects optimizer LLM performance; presenting from lowest-to-highest score improved convergence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-bison (scorer), PaLM 2-L (optimizer) used in ablations; general across optimizers",
            "model_size": null,
            "task_name": "GSM8K and BBH sports_understanding (prompt optimization)",
            "task_description": "Prompt optimization where optimizer LLM proposes instructions based on past instruction-score history (the optimization trajectory).",
            "problem_format": "Meta-prompt includes optimization trajectory sorted in a chosen order; default is ascending (lowest to highest scores).",
            "comparison_format": "Ascending (default) vs descending vs random orderings.",
            "performance": "Ascending (lowest→highest) converged faster and achieved better final accuracies than descending or random (figures 7a/7b show quantitative advantage; exact numeric deltas shown in plots but no single summary number in text).",
            "performance_comparison": "Qualitatively better convergence and higher final accuracy for ascending order; plotted differences indicate consistent improvement across repeats.",
            "format_effect_size": null,
            "format_effect_direction": "improved when ordered lowest→highest",
            "explanation_or_hypothesis": "Hypothesis: optimizer LLM outputs are biased toward the most recent tokens (recency bias), so placing better examples near the end amplifies their influence; ordering from low→high helps the model see the improvement trajectory and build upon top examples.",
            "counterexample_or_null_result": null,
            "uuid": "e5818.2",
            "source_info": {
                "paper_title": "Large Language Models as Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Showing instruction scores",
            "name_full": "Including explicit accuracy scores for past instructions in the meta-prompt (rounded buckets / fine-grained / none)",
            "brief_description": "Providing numeric training accuracies alongside past prompts helps the optimizer LLM understand quality differences and propose better new prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-bison (scorer) & PaLM 2-L (optimizer) in ablations; generalizable",
            "model_size": null,
            "task_name": "GSM8K, BBH sports_understanding",
            "task_description": "Prompt optimization using training-set accuracies as objective; scores can be shown in different granularities.",
            "problem_format": "Meta-prompt variants: (1) round accuracies to integers (100 buckets, default), (2) bucketize to 20 buckets, (3) omit accuracies (only ordered instructions).",
            "comparison_format": "Rounding to integers vs 20-bucket vs no score shown.",
            "performance": "Including accuracy scores (rounded to integers) led to better final accuracies and faster convergence than not showing scores or using coarser bucketing (figures 7c/7d).",
            "performance_comparison": "Default (integers) &gt; 20-bucket &gt; no-scores (qualitative ranking from ablation results).",
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Scores provide explicit signal distinguishing which past prompts were better, enabling the optimizer LLM to mimic high-quality patterns rather than solely inferring quality from ordering; this reduces ambiguity in the trajectory.",
            "counterexample_or_null_result": null,
            "uuid": "e5818.3",
            "source_info": {
                "paper_title": "Large Language Models as Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Exemplar count in meta-prompt",
            "name_full": "Number of task exemplars included in the meta-prompt (0 / 3 / 10)",
            "brief_description": "Including a few exemplars from the training set in the meta-prompt is critical for optimizer LLMs to understand the task and produce well-phrased instructions; more exemplars do not necessarily help.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-bison (scorer) & PaLM 2-L (optimizer) in ablations",
            "model_size": null,
            "task_name": "GSM8K, BBH sports_understanding",
            "task_description": "Prompt optimization where meta-prompt optionally contains several input-output exemplars sampled from the training data.",
            "problem_format": "Meta-prompt with 0 exemplars, 3 exemplars (default), or 10 exemplars.",
            "comparison_format": "3 exemplars (default) vs 10 exemplars vs none.",
            "performance": "Including exemplars (3) is critical and substantially improves optimization vs none; increasing to 10 does not necessarily further improve and may harm by dominating context (qualitative result from Figures 7e/7f).",
            "performance_comparison": "3 exemplars (default) yielded the best tradeoff in reported ablations; 0 exemplars performed worse; 10 exemplars showed no consistent benefit and sometimes distracted optimizer.",
            "format_effect_size": null,
            "format_effect_direction": "improved (with small number of exemplars); more exemplars had no additional benefit or could reduce performance",
            "explanation_or_hypothesis": "Exemplars clarify task format and where the generated instruction will be inserted; too many exemplars lengthen the meta-prompt and can drown optimization trajectory information, distracting the optimizer LLM.",
            "counterexample_or_null_result": null,
            "uuid": "e5818.4",
            "source_info": {
                "paper_title": "Large Language Models as Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Optimizer sampling temperature",
            "name_full": "Sampling temperature of the optimizer LLM during candidate generation",
            "brief_description": "The temperature used by the optimizer LLM controls exploration vs exploitation: moderate temperature (1.0) yielded the best optimization stability and final performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM 2-L (optimizer) in ablations; other optimizers evaluated",
            "model_size": null,
            "task_name": "GSM8K, BBH sports_understanding",
            "task_description": "Prompt optimization where optimizer LLM samples multiple candidate instructions each step; temperature varied across experiments.",
            "problem_format": "Temperatures tested: 0.0, 0.5, 1.0 (default), 1.5, 2.0.",
            "comparison_format": "Low temperatures (0.0/0.5) vs medium (1.0) vs high (1.5/2.0).",
            "performance": "Default temperature 1.0 achieved best results in ablations (Figure 10). Low temperatures lacked exploration and got stuck (flat curves); high temperatures ignored trajectory and lacked exploitation, causing instability.",
            "performance_comparison": "T=1.0 &gt; T in {0.0,0.5} and T in {1.5,2.0} (qualitative from plotted curves).",
            "format_effect_size": null,
            "format_effect_direction": "improved at moderate temperature; too low reduces exploration, too high reduces exploitation",
            "explanation_or_hypothesis": "Temperature mediates exploration-exploitation: lower temps produce conservative candidates (poor exploration), higher temps produce diverse but less trajectory-following candidates (poor exploitation).",
            "counterexample_or_null_result": null,
            "uuid": "e5818.5",
            "source_info": {
                "paper_title": "Large Language Models as Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Candidates per step (batching)",
            "name_full": "Number of instructions generated per optimization step",
            "brief_description": "Generating multiple candidate instructions per optimization step improves stability and search efficiency, with 8 candidates per step found to be a strong tradeoff in experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM 2-L (optimizer) with text-bison scorer in ablations",
            "model_size": null,
            "task_name": "GSM8K, BBH sports_understanding",
            "task_description": "OPRO generates k candidates per step and evaluates them; k varied in ablations.",
            "problem_format": "k ∈ {1,2,4,8 (default),16} candidates generated per step; total evaluation budget fixed across comparisons.",
            "comparison_format": "Comparisons across k values under fixed total candidate budget (so smaller k allows more steps).",
            "performance": "Sampling 8 candidates per step overall achieved the best performance (Figure 8); too few candidates increased variance, too many reduced number of optimization steps and hurt long-term learning.",
            "performance_comparison": "k=8 &gt; k in {1,2,4,16} in averaged ablation results.",
            "format_effect_size": null,
            "format_effect_direction": "improved (with moderate batch size)",
            "explanation_or_hypothesis": "Multiple candidates per step reduce instability analogous to mini-batch gradient estimation; but a balance is needed to allow sufficient optimization steps to exploit the trajectory information.",
            "counterexample_or_null_result": null,
            "uuid": "e5818.6",
            "source_info": {
                "paper_title": "Large Language Models as Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Iterative trajectory optimization vs one-step generation",
            "name_full": "Iterative optimization using the full optimization trajectory vs generating all candidate prompts in a single step",
            "brief_description": "OPRO's multi-step iterative approach that conditions on prior candidate-score history outperformed a one-shot generation of many candidates without trajectory information.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM 2-L-IT (optimizer) with PaLM 2-L (scorer) and PaLM 2-L-IT (optimizer) with text-bison (scorer) in comparisons",
            "model_size": null,
            "task_name": "GSM8K and BBH sports_understanding",
            "task_description": "Compare OPRO iterative optimization (generate 8 per step over many steps, conditioning on history) vs baseline that generates 50 candidates in one step (no trajectory).",
            "problem_format": "Iterative multi-step meta-prompt including optimization trajectory vs single-step meta-prompt containing only exemplars and initial instruction.",
            "comparison_format": "OPRO iterative vs one-step generation baseline.",
            "performance": "GSM8K: One-step best-of-50 had training accuracy 64.4 and test 60.8; OPRO iterative found 'Let's do the math!' at step 5 with training 78.2 and test 76.3. BBH sports_understanding: one-step best train 84.0 test 80.0; OPRO found an instruction at step 4 with train 88.0 test 84.5.",
            "performance_comparison": "Large improvements: GSM8K test +15.5 points (76.3 vs 60.8) comparing iterative OPRO vs one-step; BBH test +4.5 points (84.5 vs 80.0).",
            "format_effect_size": "GSM8K test improvement ~+15.5 percentage points; BBH sports_understanding test improvement ~+4.5 points in reported comparisons.",
            "format_effect_direction": "improved (iterative better)",
            "explanation_or_hypothesis": "Leveraging the optimization trajectory gives the optimizer LLM richer, temporally structured information about what has worked, enabling it to discover and refine high-quality prompt patterns across steps; single-step lacks trajectory signal and so misses progressive improvements.",
            "counterexample_or_null_result": null,
            "uuid": "e5818.7",
            "source_info": {
                "paper_title": "Large Language Models as Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Starting point sensitivity",
            "name_full": "Effect of initial instruction(s) used to start optimization",
            "brief_description": "The initial prompt(s) influence convergence: for instruction-tuned scorers like text-bison the choice mattered less, while for pre-trained scorers like PaLM 2-L the starting instruction significantly affected early optimization and sometimes final quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-bison (scorer), PaLM 2-L (scorer), PaLM 2-L (optimizer) in ablations",
            "model_size": null,
            "task_name": "GSM8K, BBH",
            "task_description": "Prompt optimization starting from different initial instructions: empty string, 'Let's solve the problem.', 'Let's think step by step.', or combinations.",
            "problem_format": "Initial instruction choices varied by experiment and scorer (empty vs generic vs chain-of-thought).",
            "comparison_format": "Empty vs 'Solve the following problem.' vs 'Let's think step by step.' etc.",
            "performance": "For text-bison scorer, starting points produced similar accuracies and styles (differences small). For PaLM 2-L scorer, starting from 'Let's solve the problem.' produced better early performance than empty, and starting from 'Let's think step by step.' outperformed both throughout.",
            "performance_comparison": "PaLM 2-L: starting from 'Let's think step by step.' produced consistently better instructions than starting from empty or 'Let's solve the problem.' (qualitative and plotted differences in Figure 9b).",
            "format_effect_size": null,
            "format_effect_direction": "variable; can improve or impede depending on scorer",
            "explanation_or_hypothesis": "Stronger initial prompts with higher initial accuracies bias the meta-prompt toward better-quality regions and allow faster convergence; weaker starting points require more steps to 'get rid of' low-quality instructions present in the trajectory.",
            "counterexample_or_null_result": "For text-bison scorer, different starting instructions did not differ much in final performance, indicating model- and task-dependence.",
            "uuid": "e5818.8",
            "source_info": {
                "paper_title": "Large Language Models as Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Semantically similar prompts phenomenon",
            "name_full": "Semantically similar prompts producing drastically different accuracies",
            "brief_description": "Observation that paraphrases or semantically overlapping instructions can have widely varying performance, increasing variance and oscillation during optimization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM 2-L (scorer) in illustrative examples; phenomenon observed across models",
            "model_size": null,
            "task_name": "GSM8K (illustrative), BBH examples discussed",
            "task_description": "Mathematical reasoning and BBH tasks where instruction variations produce variable accuracy.",
            "problem_format": "Paraphrases and small rephrasings of instructions (e.g., chain-of-thought phrases and collaborative phrasing).",
            "comparison_format": "Direct paraphrase comparisons: e.g., 'Let's think step by step.' vs 'Let's work together to solve this problem step by step.' vs other paraphrases.",
            "performance": "Examples: 'Let's think step by step.' = 71.8% (PaLM 2-L), 'Let's solve the problem together.' = 60.5%, 'Let's work together to solve this problem step by step.' = 49.4%.",
            "performance_comparison": "Differences up to ~22.4 percentage points among semantically similar prompts on GSM8K.",
            "format_effect_size": "Up to ≈22 percentage-point differences cited in examples.",
            "format_effect_direction": "variable (some paraphrases improve, others strongly reduce performance)",
            "explanation_or_hypothesis": "LLMs are sensitive to lexical/structural details; small linguistic variations change token sequences and associated learned behaviors, leading to large performance variability. This motivates multi-candidate generation per step and iterative trajectory use.",
            "counterexample_or_null_result": null,
            "uuid": "e5818.9",
            "source_info": {
                "paper_title": "Large Language Models as Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "EvoPrompt comparison (use of exemplars / trajectory)",
            "name_full": "Comparison between OPRO's meta-prompt (trajectory + exemplars) and EvoPrompt (GA/DE style prompts)",
            "brief_description": "OPRO which leverages multiple past prompts with scores and exemplars outperformed EvoPrompt variants that operate on pairwise crossover/mutation without exemplars in some settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (optimizer) compared across meta-prompts; PaLM 2-L and text-bison scorers in respective experiments",
            "model_size": null,
            "task_name": "GSM8K and BBH sports_understanding",
            "task_description": "Methods compared for discrete prompt optimization: OPRO (iterative trajectory + exemplars) vs EvoPrompt (GA/DE meta-prompts instructing LLM to mutate/crossover).",
            "problem_format": "OPRO meta-prompt contains several past prompts with scores and exemplars; EvoPrompt GA/DE meta-prompts operate on two prompts and apply crossover/mutation instructions without exemplars (in the experiment setup).",
            "comparison_format": "OPRO vs EvoPrompt (GA) vs EvoPrompt (DE).",
            "performance": "On GSM8K (with pre-trained PaLM 2-L scorer starting from two simple instructions), OPRO steadily improved while EvoPrompt GA/DE degraded performance. On BBH sports_understanding with task-specific initial prompts, EvoPrompt (DE) could improve but with less stability than OPRO (Figure 12).",
            "performance_comparison": "OPRO &gt; EvoPrompt on GSM8K (qualitative and plotted); mixed results on BBH when EvoPrompt provided better initial prompts but still less stable.",
            "format_effect_size": null,
            "format_effect_direction": "OPRO improved relative to EvoPrompt in reported experiments",
            "explanation_or_hypothesis": "EvoPrompt's lack of exemplars in these experiments deprived it of task grounding, so evolution-style operations on prompts lacked a task-specific signal; OPRO's trajectory + exemplars better guide the optimizer LLM to meaningful modifications.",
            "counterexample_or_null_result": null,
            "uuid": "e5818.10",
            "source_info": {
                "paper_title": "Large Language Models as Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Automatic Prompt Engineer (APE)",
            "rating": 2
        },
        {
            "paper_title": "APO: Automatic Prompt Optimization with Natural Language Feedback",
            "rating": 2
        },
        {
            "paper_title": "EvoPrompt: Learning to Evolve Prompts for Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "The Curious Case of Neural Text Degeneration",
            "rating": 1
        },
        {
            "paper_title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?",
            "rating": 1
        }
    ],
    "cost": 0.018595,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LARGE LANGUAGE MODELS AS OPTIMIZERS</h1>
<p>Chengrun Yang<em> Xuezhi Wang Yifeng Lu Hanxiao Liu<br>Quoc V. Le Denny Zhou Xinyun Chen</em><br>{chengrun, xuezhiw, yifenglu, hanxiaol}@google.com<br>{qvl, dennyzhou, xinyunchen}@google.com<br>Google DeepMind * Equal contribution</p>
<h4>Abstract</h4>
<p>Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to our main application in prompt optimization, where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to $8 \%$ on GSM8K, and by up to $50 \%$ on Big-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Prompt optimization on GSM8K (Cobbe et al., 2021) and BBH (Suzgun et al., 2022) movie_recommendation. The optimization on GSM8K has pre-trained PaLM 2-L as the scorer and the instruction-tuned PaLM 2-L (denoted PaLM 2-L-IT) as the optimizer; the optimization on BBH movie_recommendation has text-bison as the scorer and PaLM 2-L-IT as the optimizer. Each dot is the average accuracy across all (up to 8) generated instructions in the single step, and the shaded region represents standard deviation. See Section 5 for more details on experimental setup.</p>
<p>Table 1: Top instructions with the highest GSM8K zero-shot test accuracies from prompt optimization with different optimizer LLMs. All results use the pre-trained PaLM 2-L as the scorer.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Source</th>
<th style="text-align: center;">Instruction</th>
<th style="text-align: center;">Acc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Baselines <br> (Kojima et al., 2022) <br> (Zhou et al., 2022b)</td>
<td style="text-align: center;">Let's think step by step. Let's work this out in a step by step way to be sure we have the right answer. (empty string)</td>
<td style="text-align: center;">$\begin{aligned} &amp; 71.8 \ &amp; 58.8 \ &amp; 34.0 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L-IT <br> PaLM 2-L <br> gpt-3.5-turbo <br> gpt-4</td>
<td style="text-align: center;">Take a deep breath and work on this problem step-by-step. Break this down. A little bit of arithmetic and a logical approach will help us quickly arrive at the solution to this problem. <br> Let's combine our numerical command and clear thinking to quickly and accurately decipher the answer.</td>
<td style="text-align: center;">$\begin{aligned} &amp; 80.2 \ &amp; 79.9 \ &amp; 78.5 \ &amp; 74.5 \end{aligned}$</td>
</tr>
</tbody>
</table>
<h1>1 INTRODUCTION</h1>
<p>Optimization is critical for all areas. Many optimization techniques are iterative: the optimization starts from an initial solution, then iteratively updates the solution to optimize the objective function (Amari, 1993; Qian, 1999; Kingma \&amp; Ba, 2015; Bäck \&amp; Schwefel, 1993; Rios \&amp; Sahinidis, 2013; Reeves, 1993). The optimization algorithm typically needs to be customized for an individual task to deal with the specific challenges posed by the decision space and the performance landscape, especially for derivative-free optimization.
In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to utilize large language models (LLMs) as optimizers. With the advancement of prompting techniques, LLMs have achieved impressive performance in various domains (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022; Zhou et al., 2022a; Madaan et al., 2023; Bai et al., 2022; Chen et al., 2023e). Their ability to understand natural language lays out a new possibility for optimization: instead of formally defining the optimization problem and deriving the update step with a programmed solver, we describe the optimization problem in natural language, then instruct the LLM to iteratively generate new solutions based on the problem description and the previously found solutions. Optimization with LLMs enables quick adaptation to different tasks by changing the problem description in the prompt, and the optimization process can be customized by adding instructions to specify the desired properties of the solutions.
To demonstrate the potential of LLMs for optimization, we first present case studies on linear regression and the traveling salesman problem, which are two classic optimization problems that underpin many others in mathematical optimization, computer science, and operations research. On small-scale optimization problems, we show that LLMs are able to find good-quality solutions simply through prompting, and sometimes match or surpass hand-designed heuristic algorithms.
Next, we demonstrate the ability of LLMs to optimize prompts: the goal is to find a prompt that maximizes the task accuracy. Specifically, we focus on natural language tasks where both the task input and output are texts. LLMs are shown to be sensitive to the prompt format (Zhao et al., 2021; Lu et al., 2021; Wei et al., 2023; Madaan \&amp; Yazdanbakhsh, 2022); in particular, semantically similar prompts may have drastically different performance (Kojima et al., 2022; Zhou et al., 2022b; Zhang et al., 2023), and the optimal prompt formats can be model-specific and task-specific (Ma et al., 2023; Chen et al., 2023c). Therefore, prompt engineering is often important for LLMs to achieve good performance (Reynolds \&amp; McDonell, 2021). However, the large and discrete prompt space makes it challenging for optimization, especially when only API access to the LLM is available. Following prior work on continuous and discrete prompt optimization (Lester et al., 2021; Li \&amp; Liang, 2021; Zhou et al., 2022b; Pryzant et al., 2023), we assume a training set is available to compute the training accuracy as the objective value for optimization, and we show in experiments that optimizing the prompt for accuracy on a small training set is sufficient to reach high performance on the test set.
The prompt to the LLM serves as a call to the optimizer, and we name it the meta-prompt. Figure 3 shows an example. The meta-prompt contains two core pieces of information. The first piece is previously generated prompts with their corresponding training accuracies. The second piece is the optimization problem description, which includes several exemplars randomly selected from the training set to exemplify the task of interest. We also provide instructions for the LLM to understand the relationships among different parts and the desired output format. Different from recent work on using LLMs for automatic prompt generation (Zhou et al., 2022b; Pryzant et al., 2023), each optimization step in our work generates new prompts that aim to increase the test accuracy based on a trajectory of previously generated prompts, instead of editing one input prompt according to natural language feedback (Pryzant et al., 2023) or requiring the new prompt to follow the same semantic meaning (Zhou et al., 2022b). Making use of the full optimization trajectory, OPRO enables the LLM to gradually generate new prompts that improve the task accuracy throughout the optimization process, where the initial prompts have low task accuracies.
We conduct comprehensive evaluation on several LLMs, including text-bison and Palm 2-L in the PaLM-2 model family (Anil et al., 2023), as well as gpt-3.5-turbo and gpt-4 in the GPT model family. We optimize prompts on GSM8K (Cobbe et al., 2021) and Big-Bench Hard (Suzgun et al., 2022), which are reasoning benchmarks where prompting techniques have achieved remarkable performance breakthrough (Wei et al., 2022; Kojima et al., 2022; Suzgun et al., 2022). Starting from initial prompts with low task accuracies, we show that all LLMs in our evaluation are able to</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An overview of the OPRO framework. Given the meta-prompt as the input, the LLM generates new solutions to the objective function, then the new solutions and their scores are added into the meta-prompt for the next optimization step. The meta-prompt contains the solution-score pairs obtained throughout optimization, a natural language description of the task, and (in prompt optimization) a few task exemplars. Figure 3 shows a sample meta-prompt for prompt optimization.
serve as optimizers, which consistently improve the performance of the generated prompts through iterative optimization until convergence (see Figure 1). In particular, while these LLMs generally produce instructions of different styles (see Table 1), with zero-shot prompting, their best generated instructions match the few-shot chain-of-thought prompting performance when applied to PaLM 2-L, outperforming the zero-shot performance with human-designed prompts by up to $8 \%$ on GSM8K. Additionally, we observe that the OPRO-optimized prompts transfer to other benchmarks of the same domain and also deliver notable performance gain.</p>
<h1>2 OPRO: LLM AS THE OPTIMIZER</h1>
<p>Figure 2 illustrates the overall framework of OPRO. In each optimization step, the LLM generates candidate solutions to the optimization task based on the optimization problem description and previously evaluated solutions in the meta-prompt. Then the new solutions are evaluated and added to the meta-prompt for the subsequent optimization process. The optimization process terminates when the LLM is unable to propose new solutions with better optimization scores, or a maximum number of optimization steps has reached. We first outline the desired features of LLMs for optimization, then describe the key design choices based on these desirables.</p>
<h3>2.1 Desirables of Optimization by LLMs</h3>
<p>Making use of natural language descriptions. The main advantage of LLMs for optimization is their ability of understanding natural language, which allows people to describe their optimization tasks without formal specifications. For instance, in prompt optimization where the goal is to find a prompt that optimizes the task accuracy, the task can be described with a high-level text summary along with input-output examples.</p>
<p>Trading off exploration and exploitation. The exploration-exploitation trade-off is a fundamental challenge in optimization, and it is important for LLMs serving as optimizers to balance these two competing goals. This means that the LLM should be able to exploit promising areas of the search space where good solutions are already found, while also exploring new regions of the search space so as to not miss potentially better solutions.</p>
<h3>2.2 META-PROMPT DESIGN</h3>
<p>As the input to the optimizer LLM, the meta-prompt contains the following two essential parts.
Optimization problem description. The first part is the text description of the optimization problem, including the objective function and solution constraints. For example, for prompt optimization, the LLM can be instructed to "generate a new instruction that achieves a higher accuracy", and we denote such instructions in the meta-prompt as meta-instructions. We can also provide customized</p>
<p>meta-instructions as an informal regularization of the generated solutions, such as "the instruction should be concise and generally applicable".</p>
<p>Optimization trajectory. Besides understanding natural language instructions, LLMs are also shown to be able to recognize patterns from in-context demonstrations (Wei et al., 2023; Madaan \&amp; Yazdanbakhsh, 2022; Mirchandani et al., 2023). Our meta-prompt makes use of this property and instructs the LLM to leverage the optimization trajectory for generating new solutions. Specifically, the optimization trajectory includes past solutions and their optimization scores, sorted in the ascending order. Including optimization trajectory in the meta-prompt allows the LLM to identify similarities of solutions with high scores, encouraging the LLM to build upon existing good solutions to construct potentially better ones without the need of explicitly defining how the solution should be updated.</p>
<h1>2.3 Solution Generation</h1>
<p>At the solution generation step, the LLM generates new solutions with the meta-prompt as input. The following are the key optimization challenges we address in this stage.</p>
<p>Optimization stability. In the optimization process, not all solutions achieve high scores and monotonically improve over prior ones. Due to the sensitivity of in-context learning to the prompt, LLM output can be drastically affected by low-quality solutions in the input optimization trajectory, especially at the beginning when the solution space has not been adequately explored. This sometimes results in optimization instability and large variance. To improve stability, we prompt the LLM to generate multiple solutions at each optimization step, allowing the LLM to simultaneously explore multiple possibilities and quickly discover promising directions to move forward.</p>
<p>Exploration-exploitation trade-off. We tune the LLM sampling temperature to balance between exploration and exploitation. A lower temperature encourages the LLM to exploit the solution space around the previously found solutions and make small adaptations, while a high temperature allows the LLM to more aggressively explore solutions that can be notably different.</p>
<h2>3 Motivating Example: Mathematical Optimization</h2>
<p>We first demonstrate the potential of LLMs in serving as optimizers for mathematical optimization. In particular, we present a case study on linear regression as an example of continuous optimization, and on the Traveling Salesman Problem (TSP) as an example of discrete optimization. On both tasks, we see LLMs properly capture the optimization directions on small-scale problems merely based on the past optimization trajectory provided in the meta-prompt.</p>
<h3>3.1 Linear Regression</h3>
<p>In linear regression problems, the goal is to find the linear coefficients that probabilistically best explain the response from the input variables. We study the setting in which the independent and dependent variables $X$ and $y$ are both one-dimensional and an intercept $b$ is present, so that there are two one-dimensional variables $w, b$ to optimize over. In a synthetic setting, we sample ground truth values for one-dimensional variables $w_{\text {true }}$ and $b_{\text {true }}$, and generate 50 data points by $y=w_{\text {true }} x+b_{\text {true }}+\epsilon$, in which $x$ ranges from 1 to 50 and $\epsilon$ is the standard Gaussian noise. Our optimization starts from 5 randomly sampled $(w, b)$ pairs. In each step, we prompt an instructiontuned LLM with a meta-prompt that includes the best $20(w, b)$ pairs in history and their sorted objective values. The meta-prompt then asks for a new $(w, b)$ pair that further decreases the objective value. A sample meta-prompt is shown in Figure 19 of Appendix C.1. We prompt the meta-prompt 8 times to generate at most 8 new $(w, b)$ pairs in each step to improve optimization stability. Then we evaluate the objective value of the proposed pair and add it to history. We do black-box optimization: the analytic form does not appear in the meta-prompt text. This is because the LLM can often calculate the solution directly from the analytic form.</p>
<p>Table 2 summarizes the results with one of the following optimizer LLMs: text-bison, gpt-3.5-turbo, and gpt-4. We study three settings of $w_{\text {true }}$ and $b_{\text {true }}$ : within the starting region $[10,20] \times[10,20]$, "near outside" (each of $w_{\text {true }}$ and $b_{\text {true }}$ is outside the starting region but the distance is less than 10), and "far outside" (each of $w_{\text {true }}$ and $b_{\text {true }}$ is outside the starting region and the distance is greater than 10). We see:</p>
<p>Table 2: Linear regression by optimizer LLMs: the mean $\pm$ standard deviation of the number of steps and the number of unique $(w, b)$ pairs explored before reaching the global optima. Both $w$ and $b$ start from 5 random starting points in $[10,20]$. We use temperature 1.0 for all models. We run each setting 5 times. The starting points are the same across optimizer LLMs but are different across 5 runs, and are grouped by: within the starting region, outside and close to the starting region, and outside and farther from the starting region. Bold numbers indicate the best among three LLMs in each setting.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$w_{\text {true }}$</th>
<th style="text-align: center;">$b_{\text {true }}$</th>
<th style="text-align: center;">number of steps</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">number of unique $(w, b)$ pairs explored</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">gpt-4</td>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">gpt-4</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">$5.8 \pm 2.6$</td>
<td style="text-align: center;">$7.6 \pm 4.5$</td>
<td style="text-align: center;">$\mathbf{4 . 0} \pm 1.5$</td>
<td style="text-align: center;">$40.0 \pm 12.4$</td>
<td style="text-align: center;">$36.0 \pm 15.2$</td>
<td style="text-align: center;">$\mathbf{1 7 . 2} \pm 5.1$</td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">$\mathbf{4 . 0} \pm 1.8$</td>
<td style="text-align: center;">$12.6 \pm 6.0$</td>
<td style="text-align: center;">$6.0 \pm 3.7$</td>
<td style="text-align: center;">$33.4 \pm 11.7$</td>
<td style="text-align: center;">$53.8 \pm 16.9$</td>
<td style="text-align: center;">$\mathbf{2 6 . 0} \pm 10.6$</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">$\mathbf{3 . 8} \pm 2.2$</td>
<td style="text-align: center;">$10.4 \pm 5.4$</td>
<td style="text-align: center;">$6.2 \pm 3.1$</td>
<td style="text-align: center;">$30.2 \pm 13.4$</td>
<td style="text-align: center;">$42.8 \pm 16.3$</td>
<td style="text-align: center;">$\mathbf{2 4 . 2} \pm 8.2$</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$\mathbf{9 . 8} \pm 2.8$</td>
<td style="text-align: center;">$10.8 \pm 2.7$</td>
<td style="text-align: center;">$12.2 \pm 2.0$</td>
<td style="text-align: center;">$55.8 \pm 16.1$</td>
<td style="text-align: center;">$39.6 \pm 10.1$</td>
<td style="text-align: center;">$\mathbf{3 3 . 0} \pm 4.0$</td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">$19.6 \pm 11.4$</td>
<td style="text-align: center;">$26.4 \pm 18.3$</td>
<td style="text-align: center;">$\mathbf{1 2 . 2} \pm 3.7$</td>
<td style="text-align: center;">$104.0 \pm 52.3$</td>
<td style="text-align: center;">$78.6 \pm 26.2$</td>
<td style="text-align: center;">$\mathbf{4 4 . 2} \pm 8.3$</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">$\mathbf{3 1 . 4} \pm 6.3$</td>
<td style="text-align: center;">$42.8 \pm 9.7$</td>
<td style="text-align: center;">$38.0 \pm 15.9$</td>
<td style="text-align: center;">$126.4 \pm 17.7$</td>
<td style="text-align: center;">$125.6 \pm 21.7$</td>
<td style="text-align: center;">$\mathbf{9 9 . 0} \pm 24.6$</td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">$\mathbf{3 5 . 8} \pm 6.4$</td>
<td style="text-align: center;">$45.4 \pm 16.9$</td>
<td style="text-align: center;">$50.4 \pm 18.8$</td>
<td style="text-align: center;">$174.0 \pm 28.2$</td>
<td style="text-align: center;">$142.2 \pm 31.2$</td>
<td style="text-align: center;">$\mathbf{1 1 6 . 4} \pm 32.7$</td>
</tr>
</tbody>
</table>
<ul>
<li>The number of unique $(w, b)$ pairs explored by each model is fewer than exhaustive search, indicating these models are able to to do black-box optimization: compare the numbers and propose a descent direction.</li>
<li>The text-bison and gpt-4 models outperform gpt-3.5-turbo in convergence speed: they arrive at the optima with fewer steps. The gpt-4 model also outperforms in finding the optima with fewer explored unique points. Taking a closer look at the optimization trajectory, we see gpt-4 is the best at proposing a reasonable next step from the history: for example, when the history shows the objective values of $(w, b)=(8,7),(w, b)=(8,6)$, and $(w, b)=(8,5)$ are decreasing, it has a highest chance to propose $(w, b)=(8,4)$ for evaluation.</li>
<li>The problem becomes harder for all models when the ground truth moves farther from the starting region: all models need more explorations and more steps.</li>
</ul>
<h1>3.2 Traveling Salesman Problem (TSP)</h1>
<p>Next, we consider the Traveling Salesman Problem (TSP) (Jünger et al., 1995; Gutin \&amp; Punnen, 2006), a classical combinatorial optimization problem with numerous algorithms proposed in literature, including heuristic algorithms and solvers (Rosenkrantz et al., 1977; Golden et al., 1980; Optimization et al., 2020; Applegate et al., 2006; Helsgaun, 2017), and approaches based on training deep neural networks (Kool et al., 2019; Deudon et al., 2018; Chen \&amp; Tian, 2019; Nazari et al., 2018). Specifically, given a set of $n$ nodes with their coordinates, the TSP task is to find the shortest route that traverses all nodes from the starting node and finally returns to the starting node.</p>
<p>Our optimization process with LLMs starts from 5 randomly generated solutions, and each optimization step produces at most 8 new solutions. We present the meta-prompt in Figure 20 of Appendix C.1. We generate the problem instances by sampling $n$ nodes with both $x$ and $y$ coordinates in $[-100,100]$. We use the Gurobi solver (Optimization et al., 2020) to construct the oracle solutions and compute the optimality gap for all approaches, where the optimality gap is defined as the difference between the distance in the solution constructed by the evaluated approach and the distance achieved by the oracle solution, divided by the distance of the oracle solution. Besides evaluating OPRO with different LLMs including text-bison, gpt-3.5-turbo and gpt-4, we also compare OPRO to the following heuristics:</p>
<ul>
<li>Nearest Neighbor (NN) . Starting from an initial node, the solution is constructed with the nearest neighbor heuristic: At each step, among the remaining nodes that are not included in the current partial solution, NN selects the node with the shortest distance to the end node of the partial solution, and adds it as the new end node. The process finishes when all nodes have been added to the solution.</li>
<li>Farthest Insertion (FI) . One caveat of the nearest neighbor heuristic is that it does not take the distance between the start and end node into consideration when constructing partial solutions. To address this issue, FI aims to optimize the cost of inserting new nodes into the partial solution at each step. Define the minimal insertion cost of adding a new node $k$ as</li>
</ul>
<p>Table 3: Results of the Traveling Salesman Problem (TSP) with different number of nodes $n$, where each $n$ contains 5 problems. "# steps" calculates the mean $\pm$ standard error of optimization steps for successful runs that find the optimal solution. "# successes" counts the number of problems that OPRO results in the optimal solution. When no optimal solution is found for any evaluated problem, the corresponding number of steps is N/A.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">optimality gap (\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"># steps (# successes)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NN</td>
<td style="text-align: center;">FI</td>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">gpt-4</td>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">gpt-4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">$13.0 \pm 1.3$</td>
<td style="text-align: center;">$3.2 \pm 1.4$</td>
<td style="text-align: center;">$\mathbf{0 . 0} \pm 0.0$</td>
<td style="text-align: center;">$\mathbf{0 . 0} \pm 0.0$</td>
<td style="text-align: center;">$\mathbf{0 . 0} \pm 0.0$</td>
<td style="text-align: center;">$40.4 \pm 3.6$</td>
<td style="text-align: center;">(5)</td>
<td style="text-align: center;">$46.8 \pm 9.3$</td>
<td style="text-align: center;">(5)</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">$9.4 \pm 3.7$</td>
<td style="text-align: center;">$1.2 \pm 0.6$</td>
<td style="text-align: center;">$4.4 \pm 1.3$</td>
<td style="text-align: center;">$1.2 \pm 1.1$</td>
<td style="text-align: center;">$\mathbf{0 . 2} \pm 0.2$</td>
<td style="text-align: center;">N/A (0)</td>
<td style="text-align: center;">$202.0 \pm 41.1$</td>
<td style="text-align: center;">(4)</td>
<td style="text-align: center;">$\mathbf{5 8 . 5} \pm 29.0$</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">$16.0 \pm 3.9$</td>
<td style="text-align: center;">$\mathbf{0 . 2} \pm 0.1$</td>
<td style="text-align: center;">$30.4 \pm 10.6$</td>
<td style="text-align: center;">$4.4 \pm 2.5$</td>
<td style="text-align: center;">$1.4 \pm 0.6$</td>
<td style="text-align: center;">N/A (0)</td>
<td style="text-align: center;">$438.0 \pm 0.0$</td>
<td style="text-align: center;">(1)</td>
<td style="text-align: center;">$\mathbf{1 9 5 . 5} \pm 127.6$</td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$19.7 \pm 3.1$</td>
<td style="text-align: center;">$\mathbf{9 . 8} \pm 1.3$</td>
<td style="text-align: center;">$219.8 \pm 13.7$</td>
<td style="text-align: center;">$133.0 \pm 6.8$</td>
<td style="text-align: center;">$11.0 \pm 2.6$</td>
<td style="text-align: center;">N/A (0)</td>
<td style="text-align: center;">N/A (0)</td>
<td style="text-align: center;">N/A (0)</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>$c(k)=\min _{(i, j)} d(i, k)+d(k, j)-d(i, j)$, where $i$ and $j$ are adjacent nodes in the current tour, and $d(\cdot, \cdot)$ represents the distance between two nodes. At each step, FI adds a new node that maximizes the minimal insertion cost.</p>
<p>We present the results in Table 3. We randomly generate 5 problem instances for each number of nodes $n$. In addition to measuring the optimality gap, on problems where the LLM finds the optimal solutions, we also show the number of optimization steps taken to reach the global optimum. First, we observe that gpt-4 significantly outperforms gpt-3.5-turbo and text-bison across all problem sizes. Specifically, on smaller-scale problems, gpt-4 reaches the global optimum about $4 \times$ faster than other LLMs. On larger-scale problems, especially with $n=50$, gpt -4 still finds solutions with a comparable quality to heuristic algorithms, while both text-bison and gpt-3.5-turbo get stuck at local optima with up to $20 \times$ worse optimality gaps.</p>
<p>On the other hand, the performance of OPRO degrades dramatically on problems with larger sizes. When $n=10$, all LLMs find the optimal solutions for every evaluated problem; as the problem size gets larger, the OPRO optimality gaps increase quickly, and the farthest insertion heuristic starts to outperform all LLMs in the optimality gap.</p>
<p>Limitations. We would like to note that OPRO is designed for neither outperforming the state-of-the-art gradient-based optimization algorithms for continuous mathematical optimization, nor surpassing the performance of specialized solvers for classical combinatorial optimization problems such as TSP. Instead, the goal is to demonstrate that LLMs are able to optimize different kinds of objective functions simply through prompting, and reach the global optimum for some smallscale problems. Our evaluation reveals several limitations of OPRO for mathematical optimization. Specifically, the length limit of the LLM context window makes it hard to fit large-scale optimization problem descriptions in the prompt, e.g., linear regression with high-dimensional data, and traveling salesman problems with a large set of nodes to visit. In addition, the optimization landscape of some objective functions are too bumpy for the LLM to propose a correct descending direction, causing the optimization to get stuck halfway. We further elaborate our observed failure cases in Appendix A.</p>
<h1>4 Application: Prompt Optimization</h1>
<p>Next, we demonstrate the effectiveness of OPRO on prompt optimization, where the objective is to find the prompt that maximizes task accuracy. We first introduce the problem setup, then illustrate the meta-prompt design.</p>
<h3>4.1 Problem Setup</h3>
<p>We focus on prompt optimization for natural language tasks, where both the input and output are in the text format. The task is represented as a dataset with training and test splits, where the training set is used to calculate the training accuracy as the objective value during the optimization process, and we compute the test accuracy on the test set after the optimization finishes. While traditional optimization often requires a decently large training set, our experiment shows that a small number or fraction of training samples (e.g., 3.5\% of the training set for GSM8K (Cobbe et al., 2021), 20\% for Big-Bench Hard (Suzgun et al., 2022)) is sufficient. The objective function evaluator is an LLM</p>
<p>I have some texts along with their corresponding scores. The texts are arranged in ascending order based on their scores, where higher scores indicate better quality.
text:
Let's figure it out!
score:
61
text:
Let's solve the problem.
score:
63
(... more instructions and scores ...)</p>
<p>The following exemplars show how to apply your text: you replace <INS> in each input with your text, then read the input and give an output. We say your output is wrong if your output is different from the given output, and we say your output is correct if they are the same.
input:
Q: Alannah, Beatrix, and Queen are preparing for the new school year and have been given books by their parents. Alannah has 20 more books than Beatrix. Queen has $1 / 5$ times more books than Alannah. If Beatrix has 30 books, how many books do the three have together?
A: <INS>
output:
140
(... more exemplars ...)</p>
<p>Write your new text that is different from the old ones and has a score as high as possible. Write the text in square brackets.</p>
<p>Figure 3: An example of the meta-prompt for prompt optimization with instruction-tuned PaLM 2-L (PaLM 2-L-IT) on GSM8K, where the generated instruction will be prepended to the beginning of "A:" in the scorer LLM output (A_begin in Section 4.1). <INS> denotes the position where the generated instruction will be added. The blue text contains solution-score pairs; the purple text describes the optimization task and output format; the orange text are meta-instructions.
to which the optimized prompt will be applied, and it can be the same or different from the LLM for optimization. We denote the LLM for objective function evaluation as the scorer $L L M$, and the LLM for optimization as the optimizer $L L M$.</p>
<p>The output of the optimizer LLM is an instruction, which is concatenated to the question part of every exemplar and prompts the scorer LLM. We consider the following positions to insert the instruction:</p>
<ul>
<li>$Q _$begin: the instruction is added before the original question.</li>
<li>$Q _$end: the instruction is added after the original question.</li>
<li>A_begin: the instruction is added to the beginning of the scorer LLM output. This is applicable to pretrained LLMs without instruction tuning, where the prompt is formatted as a sequence of QA pairs.</li>
</ul>
<p>We exemplify these prompting formats in Appendix B.</p>
<h1>4.2 META-Prompt Design</h1>
<p>Figure 3 shows an example of the meta-prompt for prompt optimization on GSM8K (Cobbe et al., 2021). More details are as follows.</p>
<p>Optimization problem examples. The problem description includes a few examples taken from the training set to demonstrate the task for the generated instructions. For example, from the input-output pair in Figure 3, we can infer this is a math word problem. The input-output pair also demonstrates the position where the generated instruction will be added to, and this is essential for the optimizer LLM to generate instructions of the same style. In each optimization step, we add several (three for example) training examples to the meta-prompt by random sampling the training set or choose the ones the previous instructions fall short of.</p>
<p>Optimization trajectory. The optimization trajectory includes instructions generated from the past optimization steps, along with their scores. The old instructions and scores are sorted by the score in ascending order. The score is the training accuracy in prompt optimization. We only keep instructions with the highest scores in the meta-prompt in consideration of the LLM context length limit.</p>
<p>Meta-instructions. We also add meta-instructions: the instructions to the optimizer LLM that explain the optimization goal and instruct the model how to use the above information. The meta-instructions may also specify the desired generated instruction format for easier parsing.</p>
<h1>5 Prompt Optimization Experiments</h1>
<p>We present the evaluation results for prompt optimization in this section. Our experiments demonstrate that OPRO brings a significant performance gain across the board, with different combinations of LLMs as the optimizer and the scorer.</p>
<p>Section 5.1 describes the experiment setup. Section 5.2 shows main results on reasoning tasks like GSM8K and BBH. Section 5.3 shows ablation studies. Section 5.4 analyzes overfitting in prompt optimization. Section 5.5 compares the prompt optimization performance of meta-prompts in OPRO and EvoPrompt (Guo et al., 2023).</p>
<h3>5.1 Evaluation Setup</h3>
<p>Models. The LLMs we use as the optimizer and the scorer are:</p>
<ul>
<li>Optimizer LLM: Pre-trained PaLM 2-L (Anil et al., 2023), instruction-tuned PaLM 2-L (denoted PaLM 2-L-IT), text-bison, gpt-3.5-turbo, and gpt-4.</li>
<li>Scorer LLM: Pre-trained PaLM 2-L and text-bison.</li>
</ul>
<p>With pre-trained PaLM 2-L as the scorer, the optimizer LLM generates A_begin instructions. Since text-bison has been instruction-tuned, the optimizer LLM generates Q_begin and Q_end instructions when text-bison is used as the scorer.</p>
<p>Benchmarks. Our primary evaluation benchmarks are GSM8K (Cobbe et al., 2021) and Big-Bench Hard (BBH) (Suzgun et al., 2022). GSM8K is a benchmark of grade school math word problems with 7,473 training samples and 1,319 test samples, where chain-of-thought prompting (Wei et al., 2022) and the zero-shot instruction "Let's think step by step." (Kojima et al., 2022) have drastically improved the performance over the standard prompting. BBH is a suite of 23 challenging BIG-Bench tasks (Srivastava et al., 2022) that covers a wide range of topics beyond arithmetic reasoning, including symbolic manipulation and commonsense reasoning. Each task contains up to 250 examples in total.</p>
<p>To examine the transferability of the optimized instructions, we also evaluate the instructions optimized for GSM8K on two other mathematical reasoning datasets, i.e., MultiArith (Roy \&amp; Roth, 2016) and AQuA (Ling et al., 2017).</p>
<p>Implementation details. We set the temperature to be 0 when evaluating the performance of generated instructions, in which case the scorer LLM greedily decodes. Unless otherwise specified, we set the default temperature to be 1.0 for optimizer LLMs to generate diverse and creative instructions. At each optimization step, we prompt the optimizer LLM with the meta-prompt 8 times to generate 8 instructions, then we add these instructions with their training scores to the optimization trajectory in the meta-prompt. Our meta-prompt at each step contains the best 20 instructions so far and 3 randomly picked exemplars from the training set. We study the effect of different hyperparameters in ablation studies (Section 5.3). Appendix C. 2 presents the full meta-prompts for different optimizer LLMs.</p>
<p>Table 4: Test accuracies on GSM8K. We show the instruction with the highest test accuracy for each scorer-optimizer pair.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Scorer</th>
<th style="text-align: center;">Optimizer / <br> Source</th>
<th style="text-align: center;">Instruction position</th>
<th style="text-align: center;">Top instruction</th>
<th style="text-align: center;">Acc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Baselines</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">(Kojima et al., 2022)</td>
<td style="text-align: center;">A_begin</td>
<td style="text-align: center;">Let's think step by step.</td>
<td style="text-align: center;">71.8</td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">(Zhou et al., 2022b)</td>
<td style="text-align: center;">A_begin</td>
<td style="text-align: center;">Let's work this out in a step by step way to be sure we have the right answer.</td>
<td style="text-align: center;">58.8</td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A_begin</td>
<td style="text-align: center;">Let's solve the problem.</td>
<td style="text-align: center;">60.8</td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A_begin</td>
<td style="text-align: center;">(empty string)</td>
<td style="text-align: center;">34.0</td>
</tr>
<tr>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;">(Kojima et al., 2022)</td>
<td style="text-align: center;">Q_begin</td>
<td style="text-align: center;">Let's think step by step.</td>
<td style="text-align: center;">64.4</td>
</tr>
<tr>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;">(Zhou et al., 2022b)</td>
<td style="text-align: center;">Q_begin</td>
<td style="text-align: center;">Let's work this out in a step by step way to be sure we have the right answer.</td>
<td style="text-align: center;">65.6</td>
</tr>
<tr>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q_begin</td>
<td style="text-align: center;">Let's solve the problem.</td>
<td style="text-align: center;">59.1</td>
</tr>
<tr>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q_begin</td>
<td style="text-align: center;">(empty string)</td>
<td style="text-align: center;">56.8</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">PaLM <br> 2-L-IT</td>
<td style="text-align: center;">A_begin</td>
<td style="text-align: center;">Take a deep breath and work on this problem step-by-step.</td>
<td style="text-align: center;">80.2</td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">A_begin</td>
<td style="text-align: center;">Break this down.</td>
<td style="text-align: center;">79.9</td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">A_begin</td>
<td style="text-align: center;">A little bit of arithmetic and a logical approach will help us quickly arrive at the solution to this problem.</td>
<td style="text-align: center;">78.5</td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">gpt-4</td>
<td style="text-align: center;">A_begin</td>
<td style="text-align: center;">Let's combine our numerical command and clear thinking to quickly and accurately decipher the answer.</td>
<td style="text-align: center;">74.5</td>
</tr>
<tr>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;">PaLM <br> 2-L-IT</td>
<td style="text-align: center;">Q_begin</td>
<td style="text-align: center;">Let's work together to solve math word problems! First, we will read and discuss the problem together to make sure we understand it. Then, we will work together to find the solution. I will give you hints and help you work through the problem if you get stuck.</td>
<td style="text-align: center;">64.4</td>
</tr>
<tr>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;">Q_end</td>
<td style="text-align: center;">Let's work through this problem step-by-step:</td>
<td style="text-align: center;">68.5</td>
</tr>
<tr>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">Q_end</td>
<td style="text-align: center;">Analyze the given information, break down the problem into manageable steps, apply suitable mathematical operations, and provide a clear, accurate, and concise solution, ensuring precise rounding if necessary. Consider all variables and carefully consider the problem's context for an efficient solution.</td>
<td style="text-align: center;">66.5</td>
</tr>
<tr>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;">gpt-4</td>
<td style="text-align: center;">Q_begin</td>
<td style="text-align: center;">Start by dissecting the problem to highlight important numbers and their relations. Decide on the necessary mathematical operations like addition, subtraction, multiplication, or division, required for resolution. Implement these operations, keeping in mind any units or conditions. Round off by ensuring your solution fits the context of the problem to ensure accuracy.</td>
<td style="text-align: center;">62.7</td>
</tr>
</tbody>
</table>
<h1>5.2 MAIN RESULTS</h1>
<p>We show prompt optimization curves on GSM8K and two BBH tasks in this section. The curves on other BBH tasks are deferred to Appendix D, and the tables containing all accuracy numbers are in Appendix E.</p>
<h3>5.2.1 GSM8K</h3>
<p>For prompt optimization, we randomly sample $3.5 \%$ examples from the GSM8K training set. The same subset is used throughout optimization, so that the task accuracies computed at intermediate optimization steps are approximations of the training accuracy on all 7,473 training examples. This balances the evaluation cost with the generalization performance. After the optimization procedure finishes, we evaluate the found instructions on the entire GSM8K test set.</p>
<p>Figure 1(a) in Section 1 shows prompt optimization curves with pre-trained PaLM 2-L as scorer and PaLM 2-L-IT as optimizer, and the initial instruction is "Let's solve the problem" with a (approximated, and same below) training accuracy of 60.5 . We observe that the optimization curve shows an overall upward trend with several leaps throughout the optimization process, for example:</p>
<ul>
<li>"Let's think carefully about the problem and solve it together." at Step 2 with the training accuracy 63.2 ;</li>
<li>"Let's break it down!" at Step 4 with training accuracy 71.3;</li>
<li>"Let's calculate our way to the solution!" at Step 5 with training accuracy 73.9;</li>
<li>"Let's do the math!" at Step 6 with training accuracy 78.2.</li>
</ul>
<p>The optimization curves also generally show a decrease of the variance among the accuracies of instructions generated at each step, indicating that the optimizer LLM generates distributionally better instructions throughout the optimization.
Next, we present the results of generating Q_begin instructions with the text-b i son scorer and the PaLM 2-L-IT optimizer, starting from an empty instruction with a 57.1 training accuracy. The optimization curve in Figure 4(a) shows a similar upward trend, during which a few leaps in the training accuracy include:</p>
<ul>
<li>"Solve the following problems using the given information." at Step 2 with training accuracy 59.8 ;</li>
<li>"Solve the following problems by applying the given information and using the appropriate mathematical operations." at Step 3 with training accuracy 64.0;</li>
<li>"Let's read the problem carefully and identify the given information. Then, we can create an equation and solve for the unknown variable." at Step 4 with training accuracy 67.0;</li>
<li>"I'm always down for solving a math word problem together. Just give me a moment to read and understand the problem. Then, I'll create an equation that models the problem, which I'll solve for the unknown variable. I also may or may not use some helpful diagrams or visuals to understand the problem. Lastly, be sure to allow me some time to carefully check my work before submitting any responses!" at Step 29 with training accuracy 70.1.</li>
</ul>
<p>Note that although our default setting is to run OPRO for 200 steps in prompt optimization, we need much fewer steps if the goal is to find some outstanding instructions. An example is that the Figure 1(a) experiment found "Let's do the math!" at Step 6 with training accuracy 78.2, almost matching the "Take a deep breath and work on this problem step-by-step." found at the 107th step with training accuracy 80.2 , at a point where the optimization curve is still trending upwards. This is because a leap in our optimization curve does not always correspond to a much better instruction being discovered; instead, it can be due to a large qualitative improvement of all 8 generated instructions in this step. The latter usually happens several steps after the former: after a much better instruction is discovered in one step, the meta-prompt gradually gets rid of worse instructions in the latter steps by generating instructions similar to the much-better one. The top instructions kept in the meta-prompt gradually improves in this procedure. At a point when the meta-prompt only triggers higher quality instructions, the leap happens.
Finally, Figure 4(b) shows that the pre-trained PaLM 2-L can also serve as the optimizer LLM and improve its own prediction performance. Different from other optimizer LLMs that are instructiontuned, the pre-trained PaLM 2-L performs better when the prompt is formatted in a few-shot manner. Therefore, we include two initial instructions to start the optimization: the empty instruction (with a training accuracy 32.2) and "The answer is" (with a training accuracy 33.3). See Figure 21 in Appendix C for the meta-prompt format. The generated instructions follow the same style as "The answer is": most instructions are also phrases suitable as the prefix of a sentence, like "Here you go:" (generated at Step 11 with training accuracy 61.3) and "Let's do it:" (generated at Step 13 with training accuracy 75.1 ).</p>
<p>Table 4 summarizes top instructions found on GSM8K with different scorer and optimizer LLMs. We observe that:</p>
<ul>
<li>The styles of instructions found by different optimizer LLMs vary a lot: PaLM 2-L-IT and text-b i son ones are concise, while GPT ones are long and detailed.</li>
<li>Although some top instructions contain the "step-by-step" phrase, most others achieve a comparable or better accuracy with different semantic meanings.</li>
</ul>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Prompt optimization on GSM8K with (a) the text-b i son scorer and the PaLM 2-L-IT optimizer, and (b) pre-trained PaLM 2-L as both scorer and optimizer.</p>
<h1>5.2.2 BBH</h1>
<p>On BBH, the optimization starts from an empty string as the initial instruction by default. The instructions are placed at A_begin when the scorer is PaLM 2-L, and at Q_begin when the scorer is text-b i son. For each task, we utilize a subset of $20 \%$ examples for prompt optimization, and the rest examples are for testing. We show experimental results on more variants of the instruction position and initialization in Appendix E.</p>
<p>Figure 5 visualizes the per-task accuracy difference on all 23 BBH tasks compared to the instruction "Let's think step by step." (Kojima et al., 2022) and the empty instruction, and we present the concrete accuracies in Table 7 of Appendix E. We show that the instructions found by OPRO outperform "Let's think step by step." on almost all tasks by a large margin: our instructions outperform by over $5 \%$ on 19/23 tasks with the PaLM 2-L scorer, and on 15/23 tasks with the text-b i son scorer. Our prompt optimization algorithm also improves instructions from the empty starting point by over $5 \%$ on most tasks: 20/23 with the PaLM 2-L scorer and 15/23 with the text-b i son scorer.</p>
<p>Similar to GSM8K, we observe upward trends in optimization curves on almost all BBH tasks, as shown in Figure 6. See Figure 23 and 24 in Appendix D for more curves on other BBH tasks.</p>
<p>We next show some examples of instructions found through the course of optimization. On the task ruin_names, starting from the empty instruction (with 64.0 training accuracy), with the text-b i son scorer and the PaLM 2-L-IT optimizer, the following instructions are generated:</p>
<ul>
<li>"Consider the following when editing artist or movie names humorously:" at Step 1 with training accuracy 72.0 ;</li>
<li>"When making humorous edits of artist or movie names, you can change one or more letters or even create puns by adding new words that sound similar." at Step 18 with training accuracy 80.0 ;</li>
<li>"We can make humorous edits of artist/movie names by changing letters to create new words that are similar in sound but have different meanings. For example, The Police can be changed to The Polite, The Abyss can be changed to Toe Abyss, and Schindler's List can be changed to Schindler's Lost." at Step 38 with training accuracy 82.0 .</li>
</ul>
<p>Although the above instructions are semantically similar, a paraphrase by the optimizer LLM offers a notable accuracy improvement. We further highlight this observation in Section 5.2.3.</p>
<p>Below are some instructions generated when performing prompt optimization on temporal_sequences, starting from the empty instruction (with the training accuracy of 64.0):</p>
<ul>
<li>"To solve this problem, we need to first identify the time period when the person was not seen doing anything else. Then, we need to check if the place they went to was open during that time period. If it was, then that is the time period when they could have gone to that place." at Step 2 with training accuracy 42.0 ;</li>
<li>"To find the time period when a person could have gone to a place, identify the time periods when they were not seen doing anything else and the place was open. If there are multiple time periods that match these criteria, then the person could have gone to the place during any of these time periods." at Step 18 with training accuracy 54.0;</li>
</ul>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: On 23 BBH tasks, the accuracy differences among instructions found by prompt optimization (with the PaLM 2-L-IT optimizer), "Let's think step by step.", and the empty string (optimization starting point).</p>
<ul>
<li>"To determine the possible time period when a person went to a place, first identify all the time periods when the person was not seen doing anything else and the place was open. Then, rule out any time periods during which the person was seen doing something else. The remaining time periods are the possible times when the person could have gone to the place." at Step 41 with training accuracy 72.0 .
Table 5 presents the best instructions generated on movie_recommendation, ruin_names, and temporal_sequences tasks with different combinations of the optimizer and the scorer LLMs. Again,</li>
</ul>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Training accuracy curves of prompt optimization on BBH ruin_names and temporal_sequences with the text-bison scorer and the PaLM 2-L-IT optimizer. The optimizations start from the empty string.
different optimizer LLMs produce instructions of different styles. See Appendix E for results on more BBH tasks.</p>
<h1>5.2.3 SEMANTICALLY SIMILAR INSTRUCTIONS MAY ACHIEVE DRASTICALLY DIFFERENT ACCURACIES</h1>
<p>One challenge of prompt optimization is the sensitivity of model performance to subtle changes in the instruction. For example, with the PaLM 2-L scorer on the GSM8K test set, "Let's think step by step." achieves accuracy 71.8, "Let's solve the problem together." has accuracy 60.5 , while the accuracy of "Let's work together to solve this problem step by step." is only 49.4, although it is the semantic combination of the two upper instructions. This behavior increases both the variance across single-step instructions and the oscillation during optimization, and motivates us to generate multiple instructions at each step to improve the optimization stability.</p>
<h3>5.2.4 TRANSFERABILITY OF FOUND INSTRUCTIONS</h3>
<p>We assess the transferability of found prompts to different datasets of the same domain, where we evaluate the top instructions found for GSM8K on two more math reasoning benchmarks MultiArith (Roy \&amp; Roth, 2016) and AQuA (Ling et al., 2017). Table 6 shows that our optimized prompts also outperform baseline prompts with different scorer LLMs on these two benchmarks.</p>
<h3>5.3 Ablation Studies</h3>
<p>We use text-bison as the scorer and PaLM 2-L as the optimizer for all ablation studies. The tasks we evaluate are GSM8K (math reasoning) and BBH sports_understanding (non-math reasoning).</p>
<p>Meta-prompt design. The meta-prompt design is crucial in achieving good prompt optimization performance. We investigate the following core design choices:</p>
<ul>
<li>The order of the previous instructions. We compare the following options: (1) from lowest to highest (our default setting); (2) from highest to lowest; (3) random. Figures 7(a) and 7(b) show that the default setting achieves better final accuracies and converges faster. One hypothesis is that the optimizer LLM output is affected more by the past instructions closer to the end of the meta-prompt. This is consistent with the recency bias observed in Zhao et al. (2021), which states that LLMs are more likely to generate tokens similar to the end of the prompt.</li>
<li>The effect of instruction scores. In terms of how to present the accuracy scores, we compare three options: (1) rounding the accuracies to integers, which is equivalent to bucketizing the accuracy scores to 100 buckets (our default setting); (2) bucketizing the accuracies to 20 buckets; (3) not showing the accuracies, only showing the instructions in the ascending order. Figures 7(c) and 7(d) show that the accuracy scores assists the optimizer LLM in better understanding the quality difference among previous instructions, and thus the optimizer LLM proposes better new instructions that are similar to the best ones in the input optimization trajectory.</li>
<li>The effect of exemplars. We compare three options: (1) showing 3 exemplars from the task (default); (2) showing 10 exemplars from the task; (3) no exemplars. Figures 7(e) and 7(f) show</li>
</ul>
<p>Table 5: Top instructions with the highest accuracies found in prompt optimization on BBH movie_recommendation, ruin_names, and temporal_sequences.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Scorer</th>
<th style="text-align: center;">Optimizer</th>
<th style="text-align: center;">Instruction position</th>
<th style="text-align: center;">Instruction</th>
<th style="text-align: center;">Acc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">movie_recommendation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">PaLM 2-L-IT</td>
<td style="text-align: center;">A_begin</td>
<td style="text-align: center;">Based on your input, I have analyzed the given movies in terms of genre, plot, tone, audience rating, year of release, director, cast, and reviews. I have also taken into account the given options. The movie that is most similar to the given movies in terms of all these factors is:</td>
<td style="text-align: center;">90.8</td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">A_begin</td>
<td style="text-align: center;">The best film:</td>
<td style="text-align: center;">88.4</td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">A_begin</td>
<td style="text-align: center;">Let's uncover the perfect movie recommendation from the options provided, ensuring an exceptional cinematic experience together as we select the most captivating and satisfying choice that will keep us thoroughly engaged and immersed until the very end.</td>
<td style="text-align: center;">88.0</td>
</tr>
<tr>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;">PaLM 2-L-IT</td>
<td style="text-align: center;">Q_begin</td>
<td style="text-align: center;">What is the highest-rated movie similar to the given movies, with a similar IMDb rating and released in the same year?</td>
<td style="text-align: center;">91.6</td>
</tr>
<tr>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">Q_begin</td>
<td style="text-align: center;">Based on the movie list provided, carefully consider your preferences and make a well-informed decision.</td>
<td style="text-align: center;">70.8</td>
</tr>
<tr>
<td style="text-align: center;">ruin_names</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">PaLM 2-L-IT</td>
<td style="text-align: center;">A_begin</td>
<td style="text-align: center;">Which is the funniest pun on the artist or movie name?</td>
<td style="text-align: center;">88.0</td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">A_begin</td>
<td style="text-align: center;">Answer for ruin:</td>
<td style="text-align: center;">83.6</td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">A_begin</td>
<td style="text-align: center;">Prepare to have a side-splittingly funny time as we uncover the most clever and hilarious alternatives for these artist or movie names, challenging your wit to guess the correct one with a burst of creativity, humor, and imaginative twists!</td>
<td style="text-align: center;">86.8</td>
</tr>
<tr>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;">PaLM 2-L-IT</td>
<td style="text-align: center;">Q_begin</td>
<td style="text-align: center;">A humorous edit of an artist or movie name can be created by replacing one or more letters to form a new word or phrase that sounds similar but has a different meaning. The new word or phrase should be relevant to the original word, but it should also be a surprise, which makes the edit funny. For example, the artist or movie name "Rocky" can be changed to "Ricky," and "Schindler's List" can be changed to "Schindler's Lift." Be creative and have fun!</td>
<td style="text-align: center;">83.6</td>
</tr>
<tr>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">Q_begin</td>
<td style="text-align: center;">Choose the option that offers the most clever and humorous alteration of the given artist or movie name. Let your creativity shine and select the answer that will undoubtedly bring a smile to your face! Make sure to think outside the box!</td>
<td style="text-align: center;">75.2</td>
</tr>
<tr>
<td style="text-align: center;">temporal_sequences (no PaLM 2-L as scorer results because its training accuracy on empty string is 100.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;">PaLM 2-L-IT</td>
<td style="text-align: center;">Q_begin</td>
<td style="text-align: center;">To determine the time period when a person went to a place, first identify all the time periods when the person's whereabouts are unknown. Then, rule out any time periods during which the person was seen doing something else or the place was closed. The remaining time periods are the possible times when the person could have gone to the place.</td>
<td style="text-align: center;">80.4</td>
</tr>
<tr>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">Q_begin</td>
<td style="text-align: center;">Identify the optimal time slot for the individual to engage in the mentioned location/activity considering the given sightings and waking up time, taking into account the opening and closing times of the location and the duration of each event.</td>
<td style="text-align: center;">53.6</td>
</tr>
</tbody>
</table>
<p>Table 6: Transferability across datasets: accuracies of top instructions found for GSM8K on MultiArith and AQuA.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Scorer</th>
<th style="text-align: center;">Source</th>
<th style="text-align: center;">Instruction position</th>
<th style="text-align: center;">Instruction</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MultiArith</td>
<td style="text-align: center;">AQuA</td>
</tr>
<tr>
<td style="text-align: center;">Baselines</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">(Kojima et al., 2022)</td>
<td style="text-align: center;">A_begin</td>
<td style="text-align: center;">Let's think step by step.</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">44.9</td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">(Zhou et al., 2022b)</td>
<td style="text-align: center;">A_begin</td>
<td style="text-align: center;">Let's work this out in a step by step way to be sure we have the right answer.</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">48.4</td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A_begin</td>
<td style="text-align: center;">Let's solve the problem.</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">44.1</td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A_begin</td>
<td style="text-align: center;">(empty string)</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">37.8</td>
</tr>
<tr>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;">(Kojima et al., 2022)</td>
<td style="text-align: center;">Q_begin</td>
<td style="text-align: center;">Let's think step by step.</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">31.9</td>
</tr>
<tr>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;">(Zhou et al., 2022b)</td>
<td style="text-align: center;">Q_begin</td>
<td style="text-align: center;">Let's work this out in a step by step way to be sure we have the right answer.</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">32.3</td>
</tr>
<tr>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q_begin</td>
<td style="text-align: center;">Let's solve the problem.</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">29.9</td>
</tr>
<tr>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q_begin</td>
<td style="text-align: center;">(empty string)</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">33.5</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">PaLM 2-L-IT on GSM8K</td>
<td style="text-align: center;">A_begin</td>
<td style="text-align: center;">Take a deep breath and work on this problem step-by-step.</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">54.3</td>
</tr>
<tr>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;">PaLM 2-L-IT on GSM8K</td>
<td style="text-align: center;">Q_begin</td>
<td style="text-align: center;">Let's work together to solve math word problems! First, we will read and discuss the problem together to make sure we understand it. Then, we will work together to find the solution. I will give you hints and help you work through the problem if you get stuck.</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">37.8</td>
</tr>
</tbody>
</table>
<p>that presenting exemplars in the meta-prompt is critical, as it provides information on what the task looks like and helps the optimizer model phrase new instructions better. However, more exemplars do not necessarily improve the performance, as a few exemplars are usually sufficient to describe the task. In addition, including more exemplars results in a longer meta-prompt with a dominating exemplar part, which may distract the optimizer LLM from other important components like the optimization trajectory.</p>
<p>The number of generated instructions per step. Computing a mini-batch of gradients reduces the variance of a stochastic gradient descent procedure. Similarly, generating multiple instructions in each step improves the optimization stability with LLMs. On the other hand, to achieve better performance with a fixed budget for the number of instructions to evaluate, the number of per-step instructions should not be too large, so as to allow more optimization steps to incorporate richer information of past instructions with their accuracies. Taking both aspects into consideration, Figure 8 compares the optimization performance of sampling $1 / 2 / 4 / 8$ (default) / 16 instructions in each step, showing that sampling 8 instructions at each step overall achieves the best performance.</p>
<p>Starting point. We study the effect of different initial instructions for prompt optimization. Our default setting is to start from an empty string when the scorer LLM is (instruction-tuned) text-bison, and to start from either the empty string (on BBH tasks) or "Let's solve the problem." (on GSM8K) with instruction position A_begin when the scorer LLM is the (pre-trained) PaLM 2-L. Figure 9(a) shows the performance of text-bison as the scorer LLM with 3 options of initial instructions: (1) the empty string; (2) "Solve the following problem."; or (3) "Solve the following problem." and "Let's solve the problem.". We observe that the accuracies do not differ much with different starting points. Interestingly, the styles of the generated instructions are also similar. For example, most of the generated instructions starting from (1) and (2) contain the phrase "solve this problem", like "Let's work together to solve this problem." in Step 4 with training accuracy 64.8 from (1), and "Let's solve the following problems using the given information." in Step 3 with training accuracy 62.8 from (2).</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Ablation studies: how each part of the meta-prompt matters. The dots are the average values across 3 optimization repetitions, and the shaded regions represent standard deviations.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Ablation studies: the number of generated instructions in each step. The dots are the average values across 3 optimization repetitions, and the shaded regions represent standard deviations. The x -axis represents the total number of evaluated instructions through the optimization; e.g., we run 200 optimization steps when sampling 8 instructions in each step, run 400 steps when sampling 4 instructions in each step, etc.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Ablation studies: the initial instructions for prompt optimization. The dots are the average values across 3 optimization repetitions, and the shaded regions represent standard deviations.</p>
<p>Figure 9(b) presents the results of of PaLM 2-L as the scorer LLM with the following options of initial instructions: (1) "Let's solve the problem."; (2) the empty string; or (3) "Let's think step by step.". We notice that the performance differs much more with different initial instructions, especially at the beginning of the optimization. Specifically, starting from (1) leads to better generated instructions than (2) in the first 30 steps, while the instructions optimized from both (1) and (2) are worse than (3) throughout. A similar observation holds when using PaLM 2-L as scorer and gpt-3.5-turbo as optimizer for BBH tasks, by comparing the results starting from the empty string (Appendix E.2) and from "Let's solve the problem." (Appendix E.3). Taking a closer look into the optimization process of (2), we find that although both "solve the problem" and "step by step" show up in generated instructions at Step 5, it takes the optimizer LLM more steps to get rid of worse instructions presented in the meta-prompt when starting from instructions with lower accuracies. Therefore, one direction for future work is to accelerate convergence from weaker starting points.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Ablation studies: temperature of the optimizer model. The dots are the average values across 3 optimization repetitions, and the shaded regions represent standard deviations.</p>
<p>Diversity per step. We evaluate the following temperatures of the optimizer LLM: ${0.0,0.5,1.0$ (default), 1.5, 2.0}. Figure 10 shows the default temperature 1.0 achieves the best performance. Specifically, optimizations with smaller temperatures ( 0.0 and 0.5 ) lack exploration and thus creativity, and the optimizer LLM often gets stuck at the same instruction for tens of steps, resulting in flat optimization curves. On the other hand, with larger temperatures ( 1.5 and 2.0), the optimizer LLM more often ignores the trajectory of previous instructions presented in the meta-prompt and thus lacks exploitation, therefore the optimization curve does not have a steady upward trend.</p>
<p>Comparison with one-step instruction generation. Our current iterative procedure runs for multiple steps and generates a new batch of solutions in each step. To validate the importance of leveraging the optimization trajectory for generating new prompts, we compare to a baseline that generates all instructions in a single step without entering into the optimization procedure. We compare these two approaches on GSM8K and BBH sports_understanding with the PaLM 2-L-IT optimizer. For GSM8K the scorer LLM is pre-trained PaLM 2-L and the initial instruction is "Let's solve the problem", and for BBH sports_understanding the scorer LLM is text-bison and the initial instruction is the empty string. The baseline generates 50 instructions in a single step, thus its meta-prompt only includes task exemplars, the initial instruction with its accuracy, and the same meta-instructions as our full meta-prompt for performing optimization. All the other hyperparameters remain the same.</p>
<p>Our results show that this one-step instruction generation performs much worse than our optimization approach. Specifically: (1) On GSM8K, the best instruction among all 50 is still "Let's solve the problem", with a 64.4 training accuracy and a 60.8 test accuracy. On the other hand, our approach (corresponding to Figure 1(a) in the main paper) found "Let's do the math!" with a 78.2 training accuracy and a 76.3 test accuracy at the 5th step by generating 8 instructions at each step. (2) Similarly, on BBH sports_understanding, the best instruction among all 50 achieved a 84.0 training accuracy and 80.0 test accuracy. This is again worse than the instruction found by our approach at Step 4, which achieved a 88.0 training accuracy and a 84.5 test accuracy.</p>
<h1>5.4 Overfitting Analysis in Prompt Optimization</h1>
<p>For simplicity, we do not set aside a validation set in our default setting of prompt optimization. We made this decision based on the experiments when a validation set is present.</p>
<p>Overfitting may result in training accuracy being much higher than the validation/test accuracy. It is difficult to avoid overfitting, but overfitting is less harmful when each candidate solution (natural language instruction in the prompt optimization context) overfits to a similar extent. In this case, a higher training accuracy solution still achieves a higher validation/test accuracy, and one can adopt solutions with the highest training accuracies as the final result. Figure 11 shows this is the case for OPRO in prompt optimization: when setting aside a validation set with the same size as the training</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Overfitting analysis. The exemplars are splitted to $1 / 3$ training, $1 / 3$ validation and $1 / 3$ test. We compute the validation accuracy every 3 steps. The training/validation dots are the average training/validation accuracies across 3 optimization repetitions, respectively, and the shaded regions represent standard deviations.
set, the validation accuracy curves trend up and down alongside the training curves in both prompt optimization settings.</p>
<p>Of course, overfitting still occurs in the instructions found by our prompt optimization: in Table 7 and 10, our training accuracies are often $5 \%-20 \%$ higher than our test accuracies, despite that our test and overall accuracies are still mostly higher than human-written counterparts. Setting aside a larger training set and optimizing for fewer steps (early stopping) may help reduce overfitting.</p>
<h1>5.5 COMPARISON WITH EVoPROMPT</h1>
<p>Some concurrent works on prompt optimization propose meta-prompts that explicitly ask the LLM to perform mutation and crossovers of existing prompts (Fernando et al., 2023; Guo et al., 2023). In our evaluation, we compare our approach to the Genetic Algorithm (GA) and Differential Evolution (DE) versions of EvoPrompt (Guo et al., 2023). Specifically, in the GA meta-prompt, given two prompts, the meta-prompt instructs the LLM to cross over the two prompts and generates a new one, then mutates the newly generated prompt to produce the final prompt. DE extends the GA meta-prompt to include more detailed instructions, e.g., asking the LLM to identify different parts between the two given prompts before performing the mutation. This is in contrast with OPRO, which leverages the optimization trajectory including multiple past prompts, instead of only 2 previous prompts. Meanwhile, OPRO also provides the LLM with richer information to facilitate the understanding of the optimization problem, including exemplars and task accuracies of different prompts.</p>
<p>Figure 12 presents the results on GSM8K and BBH sports_understanding benchmarks, where we use gpt-3.5-turbo as the optimizer. On GSM8K, the initial instructions of all approaches are "Let's solve the problem." and "Here is the answer.", which are simple and generic. Again, we observe that OPRO performance steadily improves with more optimization steps. On the other hand, both versions of EvoPrompt even degrade the performance on GSM8K. The main reason is because EvoPrompt does not utilize exemplars for prompt optimization, thus it lacks the understanding of the task to optimize for. In this way, EvoPrompt relies on good-quality and task-specific initial prompts to optimize from.</p>
<p>Given this observation, we provide more task-specific initial instructions for experiments on BBH sports_understanding, which are "Solve the sports understanding problem." and "Give me the answer to sports understanding." In this case, EvoPrompt (DE) is able to find better prompts than the initial ones, but the optimization curve is less stable than OPRO. This indicates that leveraging the optimization trajectory helps the LLM to identify promising directions to improve existing prompts.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: Comparison with EvoPrompt in prompt optimization. We use the gpt-3.5-turbo optimizer for both experiments. "EvoPrompt (GA)" uses the meta-prompt from Guo et al. (2023), Figure 1; "EvoPrompt (DE)" uses the meta-prompt from Guo et al. (2023), Figure 2. All optimizations in (a) use the pre-trained PaLM 2-L scorer and start from two simple instructions "Let's solve the problem." and "Here is the answer."; all optimizations in (b) use the text-bison scorer and start from two richer (task-specific) instructions "Solve the sports understanding problem." and "Give me the answer to sports understanding.". The dots are the average values across 3 optimization repetitions, and the shaded regions represent standard deviations. We use temperature 1.0 for OPRO and temperature 0.5 for EvoPrompt, same as the default settings in respective works.</p>
<h1>6 Related Work</h1>
<p>Prompt optimization. Prior works have developed soft prompt-tuning methods that optimize the prompt represented as task-specific continuous vectors (Lester et al., 2021; Li \&amp; Liang, 2021; Liu et al., 2021; Qin \&amp; Eisner, 2021), as well as performing discrete prompt optimization by gradient-guided search (Shin et al., 2020; Wen et al., 2023; Gao et al., 2020; Chen et al., 2023d) and reinforcement learning (Deng et al., 2022; Zhang et al., 2023). These approaches become inapplicable when there is only API access to the LLM. Other works designed edit-based approaches for gradient-free prompt optimization (Xu et al., 2022; Prasad et al., 2022), where the editing can be done with humandefined operations (e.g., swapping two phrases) (Prasad et al., 2022) or language models (e.g., back translation) (Xu et al., 2022). Some recent works investigate LLMs for prompt optimization (Zhou et al., 2022b; Pryzant et al., 2023; Xu et al., 2023). Specifically, APE (Zhou et al., 2022b) first uses the LLM to generate initial instructions. Afterwards, APE selects top instructions with the highest accuracies, then prompts the LLM with each individual instruction to generate a semantically similar variant of the initial instruction. APO (Pryzant et al., 2023) in each step instructs the LLM to produce text feedback on how to update an old instruction. Different from edit-based approaches, the optimizer LLM in our work directly generates new instructions at each optimization step, and the optimizer LLM is merely asked to improve the task accuracy without being required to imitate past instructions. Compared to Zhou et al. (2022b) and Pryzant et al. (2023), our optimization process incorporates the past generated instructions with their scores in the meta-prompt, enabling the optimizer LLM to discover common patterns of high-quality instructions.</p>
<p>Prompting with natural language feedback. A recent line of work investigates approaches to improve the LLM performance by prompting with natural language feedback to revise the model output, which has shown effectiveness in reducing harmful LLM outputs (Bai et al., 2022; Ganguli et al., 2023), improving reasoning (Shinn et al., 2023; Madaan et al., 2023) and code generation performance (Chen et al., 2023e; Olausson et al., 2023; Shinn et al., 2023; Chen et al., 2023b), dialogue applications (Nair et al., 2023; Madaan et al., 2023; Yuan et al., 2023), and so on (Kim et al., 2023; Wang et al., 2023). Specifically, Yuan et al. (2023) develops a human-in-the-loop framework for deriving system-level feedback from a collection of instance-level feedback, which is then used</p>
<p>for refining data. In our work, the optimizer LLM utilizes the optimization trajectory in the prompt, which implicitly requires the LLM to summarize the common characteristics among solutions with similar scores. We consider incorporating explicit natural language feedback on generated solutions for later optimization steps as future work.</p>
<p>Tuning language models for optimization. Some previous works tune or prompt language models to behave as mutation and crossover operators in evolutionary algorithms. Meyerson et al. (2023) utilizes language models with few-shot exemplars to propose evolutionary cross-overs on tasks such as image and code generation. In Lehman et al. (2022), the large language model trained on code diff generation is used as the mutation operator, and they further design a fine-tuning method to improve performance in the Sodarace domain for robot simulation. EvoPrompting (Chen et al., 2023a) uses large language models to evolve neural network architectures, where they combine evolutionary search with soft prompt tuning. With respect to taking the trajectory as the input for optimization, OptFormer (Chen et al., 2022) trains a transformer model on large collections of hyperparameter optimization data. On the other hand, our work performs optimization solely by prompting without additional training.</p>
<h1>7 CONCLUSION</h1>
<p>We embark on employing LLMs as optimizers, where the LLM progressively generates new solutions to optimize an objective function. We first motivate OPRO with linear regression and traveling salesman problems, then proceed to prompt optimization as a concrete application. Our evaluation demonstrates that LLMs have the capacity of gradually improving the generated solutions based on the past optimization trajectory. Interestingly, on small-scale traveling salesman problems, OPRO performs on par with some hand-crafted heuristic algorithms. For prompt optimization, optimized prompts outperform human-designed prompts on GSM8K and Big-Bench Hard by a significant margin, sometimes over $50 \%$.</p>
<p>A number of unresolved questions are open for future research on LLMs for optimization. In general, how to reduce the sensitivity to initialization and better balance exploitation with exploration remains a challenge. Specifically, for prompt optimization, one limitation of our current implementation is that the optimizer LLM does not effectively utilize error cases in the training set to infer promising directions to improve the generated instructions. In our experiments, we tried including error cases in the meta-prompt rather than randomly sampling from the training set at each optimization step, but the results are similar, indicating that the error cases alone are not informative enough for the optimizer LLM to grasp the cause of the wrong prediction. Another limitation is that prompt optimization requires a training set to compute the accuracy that guides the optimization process. Currently the training set at least contains tens of samples, so that the optimized prompt does not severely overfit to the training samples. A promising direction is to incorporate richer feedback about the error cases besides the aggregated accuracy, and summarize the key features that distinguish between high-quality and low-quality generated prompts in the optimization trajectory. Such information may inform the optimizer LLM of how to more efficiently improve over the past generated instructions, and potentially further reduce the example set size needed for prompt optimization.</p>
<h2>ETHICS STATEMENT</h2>
<p>This work uses synthetic math problems for linear regression and traveling salesman problems, and uses public datasets like GSM8K and Big-Bench Hard for prompt optimization. These tasks have been commonly used in similar works and should not be regarded controversial. There is a peril that LLMs may generate harmful information that poses safety risks; how to safeguard model behavior remains valuable future work.</p>
<h2>REPRODUCIbILITY STATEMENT</h2>
<p>We evaluate on public benchmarks. The text-bison API is available at: https://cloud. google.com/vertex-ai/docs/generative-ai/learn/models. The GPT models are available here: http://openai.com/api/. This work uses gpt-3.5-turbo-0613 and gpt-4-0613.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8787 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8787</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8787</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-263831195</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.04815v1.pdf" target="_blank">Critique Ability of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Critical thinking is essential for rational decision-making and problem-solving. This skill hinges on the ability to provide precise and reasoned critiques and is a hallmark of human intelligence. In the era of large language models (LLMs), this study explores the ability of LLMs to deliver accurate critiques across various tasks. We are interested in this topic as a capable critic model could not only serve as a reliable evaluator, but also as a source of supervised signals for model tuning. Particularly, if a model can self-critique, it has the potential for autonomous self-improvement. To examine this, we introduce a unified evaluation framework for assessing the critique abilities of LLMs. We develop a benchmark called CriticBench, which comprises 3K high-quality natural language queries and corresponding model responses; and annotate the correctness of these responses. The benchmark cover tasks such as math problem-solving, code completion, and question answering. We evaluate multiple LLMs on the collected dataset and our analysis reveals several noteworthy insights: (1) Critique is generally challenging for most LLMs, and this capability often emerges only when models are sufficiently large. (2) In particular, self-critique is especially difficult. Even top-performing LLMs struggle to achieve satisfactory performance. (3) Models tend to have lower critique accuracy on problems where they are most uncertain. To this end, we introduce a simple yet effective baseline named self-check, which leverages self-critique to improve task performance for various models. We hope this study serves as an initial exploration into understanding the critique abilities of LLMs, and aims to inform future research, including the development of more proficient critic models and the application of critiques across diverse tasks.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8787.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8787.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency (majority vote over multiple chain-of-thought samples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generate many chain-of-thought (CoT) candidate answers by sampling multiple decodings and aggregate them via majority voting to improve final answer accuracy on reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0613), PaLM-2 (S/M/L), GPT-4 (gpt-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM-2 variants (S/M/L) are pretrained PaLM-2 checkpoints (T-shirt sizes; exact parameter counts not public); ChatGPT (gpt-3.5-turbo-0613) and GPT-4 (gpt-4-0613) are OpenAI instruction-tuned checkpoints used via API.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample k independent chain-of-thought outputs for a query and pick the final answer via majority vote (k=64 in GSM8K experiments). This is not an iterative generate-then-reflect loop but an ensemble/aggregation over multiple stochastic decodings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K is a dataset of grade-school math word problems used to evaluate multi-step arithmetic and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>CoT+SC@64 accuracies on GSM8K reported in paper: ChatGPT 83.58% accuracy; PaLM-2 (reported) 91.39% accuracy; GPT-4 95.8% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Chain-of-thought (CoT) single-output baseline accuracies: ChatGPT 76.38%; PaLM-2 80.7%; GPT-4 91.39%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineering to elicit chain-of-thought; sampling-based stochastic decoding (nucleus sampling) to obtain k candidates; aggregation via majority vote.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative gains vs CoT baseline: ChatGPT +7.20 percentage points (76.38 -> 83.58), PaLM-2 +10.69 points (80.7 -> 91.39), GPT-4 +4.41 points (91.39 -> 95.8) on GSM8K using k=64.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires many samples (compute cost); improvement varies by model scale (larger models benefit more); does not address errors stemming from systematic false beliefs or missing tools (e.g., code execution); not an explicit self-reflection loop.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Used as the baseline to compare the paper's self-check filtering (SC^2); SC^2 provides small additional gains over plain self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critique Ability of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8787.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8787.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-check (SC^2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency with self-check filtering (SC^2 / self-check)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method introduced in this paper that augments self-consistency by prompting a critic (the same LLM) to judge each sampled candidate and filtering out candidates judged incorrect before majority voting, optionally applied only for low-certainty queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0613), PaLM-2 (S/M/L), GPT-4 (gpt-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same model family descriptions as used for self-consistency experiments; critic is implemented as a prompting-based few-shot chain-of-thought judgment performed by the same family of LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-check (self-consistency with critic filtering, SC^2)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>For a set A of k sampled answers, run a prompting-based critic V(a) ∈ {0,1} on each candidate; form A_sc = {a | V(a)=1} and then take mode(A_sc) as final answer. Apply filtering only for queries with Certainty(q;k) below a threshold C to save compute. In experiments k=64; certainty thresholds used: C=1 for PaLM-2 and ChatGPT, C=2 for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K is a benchmark of grade-school math word problems requiring chain-of-thought reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>CoT+SC^2@64 accuracies on GSM8K reported: ChatGPT 84.0% (CoT+SC 83.58% -> +0.5 pp); PaLM-2 92.7% (CoT+SC 91.39% -> +1.4 pp); GPT-4 96.2% (CoT+SC 95.8% -> +0.4 pp). Paper also reports error-reduction rates: ChatGPT 3.03% error reduction, PaLM-2 16.09% error reduction, GPT-4 9.52% error reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Compared to CoT+SC@64 (self-consistency without self-check): ChatGPT 83.58%; PaLM-2 91.39%; GPT-4 95.8%. Also compared to CoT single-output baselines: ChatGPT 76.38%; PaLM-2 80.7%; GPT-4 91.39%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered few-shot chain-of-thought critic (no fine-tuning), applied per candidate output; uses the paper's certainty metric to decide whether to run the critic for a given query.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Consistent, small but measurable improvements over self-consistency on GSM8K across multiple models (absolute accuracy increases 0.4–1.4 percentage points and reported error-reduction rates). The improvements are empirical and reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Gains are modest and model-dependent; critic is prompting-based (not fine-tuned) so potentially suboptimal; critic evaluation of each candidate increases compute cost; selective application requires tuning certainty thresholds per model; method demonstrated on GSM8K only in experiments — not shown on TruthfulQA or HumanEval; code-evaluation without execution remains hard.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly to CoT and self-consistency (SC); SC^2 yields incremental improvement over SC. The paper notes that fine-tuned critic models or supervised use of critiques could further improve results but were not explored.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critique Ability of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8787.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8787.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-critique evaluation (CRITICBENCH)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-critique (model critiquing its own outputs) evaluation on CRITICBENCH</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's core evaluation of LLMs' ability to identify flaws in their own responses using few-shot chain-of-thought prompts and a final 'Judgment: correct/incorrect' label; shows critique is challenging and often emerges only at large scales.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2 (XXS, XS, S, M, L), ChatGPT (gpt-3.5-turbo-0613), GPT-4 (gpt-4-0613), LLaMA, LLaMA-2 (evaluated where available)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM-2 family evaluated at multiple T-shirt sizes (XXS..L); ChatGPT and GPT-4 are instruction-tuned via OpenAI API; LLaMA/LLaMA-2 included in analysis where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-critique (prompted few-shot chain-of-thought judgement)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prompt models (few-shot CoT prompts) to analyze a given query-response pair, identify flaws, explain reasons, and produce a final binary judgment 'correct' or 'incorrect' which is compared to ground truth. Temperature 0.6, evaluate 8 runs and average accuracy; pattern 'Judgment: X.' is parsed.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CRITICBENCH (Critic-GSM8K, Critic-HumanEval, Critic-TruthfulQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>CRITICBENCH is a 3k-example benchmark mixture constructed from GSM8K (math), HumanEval (code), and TruthfulQA (question answering) containing query-response-judgment triplets annotated for correctness, selected for moderate difficulty via certainty-based filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Self-critique (judgment) accuracies vary by subset and model scale. On Critic-HumanEval even PaLM-2-L achieves only ~54.14% judgment accuracy. On Critic-TruthfulQA models perform close to random guessing; on Critic-GSM8K larger models show non-trivial better-than-chance performance (exact per-model GSM8K self-critique numbers shown in figures but specific per-size numbers aren’t all listed in-text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not applicable as this is an evaluation of critique ability (paper contrasts to random/majority baselines and notes many medium-or-smaller models perform like random guessing).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering: few-shot chain-of-thought exemplars that instruct the model to analyze step-by-step, provide constructive feedback, and then issue a 'Judgment: correct/incorrect.' If the pattern is missing, output defaults to 'correct' per evaluation protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Paper documents a scaling law: critique ability tends to emerge at larger model scales; larger models are better at critiquing outputs from smaller models; there is a positive correlation between model certainty and critique accuracy (figures and analyses in Section 4).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-critique is particularly difficult: many models (medium/smaller) perform near random; TruthfulQA failures arise when model errors stem from false beliefs (hard for model to detect); HumanEval critique is challenging without executing code (PaLM-2-L only ~54%); the evaluation protocol defaults to 'correct' if the expected judgement pattern is missing which could bias results; critique accuracy is lower on queries where models are uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Paper compares self-critique capability across model sizes and contrasts it with using larger models as external critics (larger models better at critiquing smaller models); compares to self-consistency+self-check application (which uses prompting-based critic to filter candidates rather than true iterative self-improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critique Ability of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8787.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8787.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-refine (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method (cited) that uses iterative refinement with self-feedback where a model generates an answer and then refines it using self-generated feedback; cited in related work but not used experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (as used in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not specified in this paper (referenced as prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-refine (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iterative procedure in cited work: model generates an output, critiques or generates feedback on it, and then refines the output in one or more further passes (details in the cited paper).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>various generation and reasoning tasks (as in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Cited work applies iterative self-feedback to generation/refinement tasks; exact tasks not used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-based iterative refinement in cited work (paper mentions it in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned alongside other critique/self-reflection methods in related work; no direct empirical comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critique Ability of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8787.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8787.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: An autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that equips agents with mechanisms for self-reflection and dynamic memory to iteratively improve behavior; cited in related work but not used experimentally here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: An autonomous agent with dynamic memory and self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>agent architectures built on LLMs (as in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not specified in this paper; referenced as prior example of self-reflection applied to autonomous agents.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion (agent self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Cited approach uses dynamic memory and self-reflection loops for autonomous agents to revisit and improve past actions; this paper cites it as related work but does not adopt its mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>agent control / long-horizon tasks (as in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Cited work focuses on autonomous agent tasks where self-reflection and memory improve performance; not part of CRITICBENCH experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>External memory + iterative self-reflection loop (details in cited paper).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned in related work list; no empirical comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critique Ability of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8787.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8787.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-critiquing models (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-critiquing models for assisting human evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior line of work (cited) that uses LLMs to generate critiques to assist human evaluators; referenced in related work but not used in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-critiquing models for assisting human evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (as in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-critiquing (assistive critique generation)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Cited prior methods prompt models to produce critiques of outputs to aid human evaluation or downstream decisions; the current paper frames its evaluation in this context but does not reproduce those experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>human evaluation assistance / various tasks (cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prior work evaluated how model-generated critiques help human evaluators across tasks; not evaluated in CRITICBENCH experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-based critique generation in cited work; not implemented in this paper beyond mention.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Cited as related literature motivating CRITICBENCH; no direct empirical comparison here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critique Ability of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback. <em>(Rating: 2)</em></li>
                <li>Reflexion: An autonomous agent with dynamic memory and self-reflection. <em>(Rating: 2)</em></li>
                <li>Self-critiquing models for assisting human evaluators. <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8787",
    "paper_id": "paper-263831195",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Self-consistency",
            "name_full": "Self-consistency (majority vote over multiple chain-of-thought samples)",
            "brief_description": "Generate many chain-of-thought (CoT) candidate answers by sampling multiple decodings and aggregate them via majority voting to improve final answer accuracy on reasoning tasks.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models.",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo-0613), PaLM-2 (S/M/L), GPT-4 (gpt-4-0613)",
            "model_description": "PaLM-2 variants (S/M/L) are pretrained PaLM-2 checkpoints (T-shirt sizes; exact parameter counts not public); ChatGPT (gpt-3.5-turbo-0613) and GPT-4 (gpt-4-0613) are OpenAI instruction-tuned checkpoints used via API.",
            "reflection_method_name": "Self-consistency (SC)",
            "reflection_method_description": "Sample k independent chain-of-thought outputs for a query and pick the final answer via majority vote (k=64 in GSM8K experiments). This is not an iterative generate-then-reflect loop but an ensemble/aggregation over multiple stochastic decodings.",
            "task_name": "GSM8K",
            "task_description": "GSM8K is a dataset of grade-school math word problems used to evaluate multi-step arithmetic and reasoning.",
            "performance_with_reflection": "CoT+SC@64 accuracies on GSM8K reported in paper: ChatGPT 83.58% accuracy; PaLM-2 (reported) 91.39% accuracy; GPT-4 95.8% accuracy.",
            "performance_without_reflection": "Chain-of-thought (CoT) single-output baseline accuracies: ChatGPT 76.38%; PaLM-2 80.7%; GPT-4 91.39%.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineering to elicit chain-of-thought; sampling-based stochastic decoding (nucleus sampling) to obtain k candidates; aggregation via majority vote.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative gains vs CoT baseline: ChatGPT +7.20 percentage points (76.38 -&gt; 83.58), PaLM-2 +10.69 points (80.7 -&gt; 91.39), GPT-4 +4.41 points (91.39 -&gt; 95.8) on GSM8K using k=64.",
            "limitations_or_failure_cases": "Requires many samples (compute cost); improvement varies by model scale (larger models benefit more); does not address errors stemming from systematic false beliefs or missing tools (e.g., code execution); not an explicit self-reflection loop.",
            "comparison_to_other_methods": "Used as the baseline to compare the paper's self-check filtering (SC^2); SC^2 provides small additional gains over plain self-consistency.",
            "ablation_study_results": null,
            "uuid": "e8787.0",
            "source_info": {
                "paper_title": "Critique Ability of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Self-check (SC^2)",
            "name_full": "Self-consistency with self-check filtering (SC^2 / self-check)",
            "brief_description": "A method introduced in this paper that augments self-consistency by prompting a critic (the same LLM) to judge each sampled candidate and filtering out candidates judged incorrect before majority voting, optionally applied only for low-certainty queries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo-0613), PaLM-2 (S/M/L), GPT-4 (gpt-4-0613)",
            "model_description": "Same model family descriptions as used for self-consistency experiments; critic is implemented as a prompting-based few-shot chain-of-thought judgment performed by the same family of LLMs.",
            "reflection_method_name": "Self-check (self-consistency with critic filtering, SC^2)",
            "reflection_method_description": "For a set A of k sampled answers, run a prompting-based critic V(a) ∈ {0,1} on each candidate; form A_sc = {a | V(a)=1} and then take mode(A_sc) as final answer. Apply filtering only for queries with Certainty(q;k) below a threshold C to save compute. In experiments k=64; certainty thresholds used: C=1 for PaLM-2 and ChatGPT, C=2 for GPT-4.",
            "task_name": "GSM8K",
            "task_description": "GSM8K is a benchmark of grade-school math word problems requiring chain-of-thought reasoning.",
            "performance_with_reflection": "CoT+SC^2@64 accuracies on GSM8K reported: ChatGPT 84.0% (CoT+SC 83.58% -&gt; +0.5 pp); PaLM-2 92.7% (CoT+SC 91.39% -&gt; +1.4 pp); GPT-4 96.2% (CoT+SC 95.8% -&gt; +0.4 pp). Paper also reports error-reduction rates: ChatGPT 3.03% error reduction, PaLM-2 16.09% error reduction, GPT-4 9.52% error reduction.",
            "performance_without_reflection": "Compared to CoT+SC@64 (self-consistency without self-check): ChatGPT 83.58%; PaLM-2 91.39%; GPT-4 95.8%. Also compared to CoT single-output baselines: ChatGPT 76.38%; PaLM-2 80.7%; GPT-4 91.39%.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineered few-shot chain-of-thought critic (no fine-tuning), applied per candidate output; uses the paper's certainty metric to decide whether to run the critic for a given query.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Consistent, small but measurable improvements over self-consistency on GSM8K across multiple models (absolute accuracy increases 0.4–1.4 percentage points and reported error-reduction rates). The improvements are empirical and reported in Table 1.",
            "limitations_or_failure_cases": "Gains are modest and model-dependent; critic is prompting-based (not fine-tuned) so potentially suboptimal; critic evaluation of each candidate increases compute cost; selective application requires tuning certainty thresholds per model; method demonstrated on GSM8K only in experiments — not shown on TruthfulQA or HumanEval; code-evaluation without execution remains hard.",
            "comparison_to_other_methods": "Compared directly to CoT and self-consistency (SC); SC^2 yields incremental improvement over SC. The paper notes that fine-tuned critic models or supervised use of critiques could further improve results but were not explored.",
            "ablation_study_results": null,
            "uuid": "e8787.1",
            "source_info": {
                "paper_title": "Critique Ability of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Self-critique evaluation (CRITICBENCH)",
            "name_full": "Self-critique (model critiquing its own outputs) evaluation on CRITICBENCH",
            "brief_description": "The paper's core evaluation of LLMs' ability to identify flaws in their own responses using few-shot chain-of-thought prompts and a final 'Judgment: correct/incorrect' label; shows critique is challenging and often emerges only at large scales.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-2 (XXS, XS, S, M, L), ChatGPT (gpt-3.5-turbo-0613), GPT-4 (gpt-4-0613), LLaMA, LLaMA-2 (evaluated where available)",
            "model_description": "PaLM-2 family evaluated at multiple T-shirt sizes (XXS..L); ChatGPT and GPT-4 are instruction-tuned via OpenAI API; LLaMA/LLaMA-2 included in analysis where applicable.",
            "reflection_method_name": "Self-critique (prompted few-shot chain-of-thought judgement)",
            "reflection_method_description": "Prompt models (few-shot CoT prompts) to analyze a given query-response pair, identify flaws, explain reasons, and produce a final binary judgment 'correct' or 'incorrect' which is compared to ground truth. Temperature 0.6, evaluate 8 runs and average accuracy; pattern 'Judgment: X.' is parsed.",
            "task_name": "CRITICBENCH (Critic-GSM8K, Critic-HumanEval, Critic-TruthfulQA)",
            "task_description": "CRITICBENCH is a 3k-example benchmark mixture constructed from GSM8K (math), HumanEval (code), and TruthfulQA (question answering) containing query-response-judgment triplets annotated for correctness, selected for moderate difficulty via certainty-based filtering.",
            "performance_with_reflection": "Self-critique (judgment) accuracies vary by subset and model scale. On Critic-HumanEval even PaLM-2-L achieves only ~54.14% judgment accuracy. On Critic-TruthfulQA models perform close to random guessing; on Critic-GSM8K larger models show non-trivial better-than-chance performance (exact per-model GSM8K self-critique numbers shown in figures but specific per-size numbers aren’t all listed in-text).",
            "performance_without_reflection": "Not applicable as this is an evaluation of critique ability (paper contrasts to random/majority baselines and notes many medium-or-smaller models perform like random guessing).",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompt engineering: few-shot chain-of-thought exemplars that instruct the model to analyze step-by-step, provide constructive feedback, and then issue a 'Judgment: correct/incorrect.' If the pattern is missing, output defaults to 'correct' per evaluation protocol.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Paper documents a scaling law: critique ability tends to emerge at larger model scales; larger models are better at critiquing outputs from smaller models; there is a positive correlation between model certainty and critique accuracy (figures and analyses in Section 4).",
            "limitations_or_failure_cases": "Self-critique is particularly difficult: many models (medium/smaller) perform near random; TruthfulQA failures arise when model errors stem from false beliefs (hard for model to detect); HumanEval critique is challenging without executing code (PaLM-2-L only ~54%); the evaluation protocol defaults to 'correct' if the expected judgement pattern is missing which could bias results; critique accuracy is lower on queries where models are uncertain.",
            "comparison_to_other_methods": "Paper compares self-critique capability across model sizes and contrasts it with using larger models as external critics (larger models better at critiquing smaller models); compares to self-consistency+self-check application (which uses prompting-based critic to filter candidates rather than true iterative self-improvement).",
            "ablation_study_results": null,
            "uuid": "e8787.2",
            "source_info": {
                "paper_title": "Critique Ability of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Self-refine (mentioned)",
            "name_full": "Self-refine: Iterative refinement with self-feedback",
            "brief_description": "A prior method (cited) that uses iterative refinement with self-feedback where a model generates an answer and then refines it using self-generated feedback; cited in related work but not used experimentally in this paper.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback.",
            "mention_or_use": "mention",
            "model_name": "various LLMs (as used in cited work)",
            "model_description": "Not specified in this paper (referenced as prior work).",
            "reflection_method_name": "Self-refine (iterative self-feedback)",
            "reflection_method_description": "Iterative procedure in cited work: model generates an output, critiques or generates feedback on it, and then refines the output in one or more further passes (details in the cited paper).",
            "task_name": "various generation and reasoning tasks (as in cited work)",
            "task_description": "Cited work applies iterative self-feedback to generation/refinement tasks; exact tasks not used in this paper.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Prompt-based iterative refinement in cited work (paper mentions it in related work).",
            "number_of_iterations": null,
            "evidence_for_improvement": null,
            "limitations_or_failure_cases": null,
            "comparison_to_other_methods": "Mentioned alongside other critique/self-reflection methods in related work; no direct empirical comparison in this paper.",
            "ablation_study_results": null,
            "uuid": "e8787.3",
            "source_info": {
                "paper_title": "Critique Ability of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Reflexion (mentioned)",
            "name_full": "Reflexion: An autonomous agent with dynamic memory and self-reflection",
            "brief_description": "Prior work that equips agents with mechanisms for self-reflection and dynamic memory to iteratively improve behavior; cited in related work but not used experimentally here.",
            "citation_title": "Reflexion: An autonomous agent with dynamic memory and self-reflection.",
            "mention_or_use": "mention",
            "model_name": "agent architectures built on LLMs (as in cited work)",
            "model_description": "Not specified in this paper; referenced as prior example of self-reflection applied to autonomous agents.",
            "reflection_method_name": "Reflexion (agent self-reflection)",
            "reflection_method_description": "Cited approach uses dynamic memory and self-reflection loops for autonomous agents to revisit and improve past actions; this paper cites it as related work but does not adopt its mechanisms.",
            "task_name": "agent control / long-horizon tasks (as in cited work)",
            "task_description": "Cited work focuses on autonomous agent tasks where self-reflection and memory improve performance; not part of CRITICBENCH experiments.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "mechanism_of_reflection": "External memory + iterative self-reflection loop (details in cited paper).",
            "number_of_iterations": null,
            "evidence_for_improvement": null,
            "limitations_or_failure_cases": null,
            "comparison_to_other_methods": "Mentioned in related work list; no empirical comparison in this paper.",
            "ablation_study_results": null,
            "uuid": "e8787.4",
            "source_info": {
                "paper_title": "Critique Ability of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Self-critiquing models (mentioned)",
            "name_full": "Self-critiquing models for assisting human evaluators",
            "brief_description": "Prior line of work (cited) that uses LLMs to generate critiques to assist human evaluators; referenced in related work but not used in this paper's experiments.",
            "citation_title": "Self-critiquing models for assisting human evaluators.",
            "mention_or_use": "mention",
            "model_name": "various LLMs (as in cited work)",
            "model_description": "Not specified in this paper.",
            "reflection_method_name": "Self-critiquing (assistive critique generation)",
            "reflection_method_description": "Cited prior methods prompt models to produce critiques of outputs to aid human evaluation or downstream decisions; the current paper frames its evaluation in this context but does not reproduce those experiments.",
            "task_name": "human evaluation assistance / various tasks (cited work)",
            "task_description": "Prior work evaluated how model-generated critiques help human evaluators across tasks; not evaluated in CRITICBENCH experiments.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Prompt-based critique generation in cited work; not implemented in this paper beyond mention.",
            "number_of_iterations": null,
            "evidence_for_improvement": null,
            "limitations_or_failure_cases": null,
            "comparison_to_other_methods": "Cited as related literature motivating CRITICBENCH; no direct empirical comparison here.",
            "ablation_study_results": null,
            "uuid": "e8787.5",
            "source_info": {
                "paper_title": "Critique Ability of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback.",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: An autonomous agent with dynamic memory and self-reflection.",
            "rating": 2,
            "sanitized_title": "reflexion_an_autonomous_agent_with_dynamic_memory_and_selfreflection"
        },
        {
            "paper_title": "Self-critiquing models for assisting human evaluators.",
            "rating": 2,
            "sanitized_title": "selfcritiquing_models_for_assisting_human_evaluators"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems.",
            "rating": 1,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        }
    ],
    "cost": 0.022323,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CRITIQUE ABILITY OF LARGE LANGUAGE MODELS
7 Oct 2023</p>
<p>Liangchen Luo 
Zi Lin 
Yinxiao Liu 
Lei Shu 
Yun Zhu 
Jingbo Shang 
Lei Meng leimeng@google.com 
Google Research 
U C San 
CRITIQUE ABILITY OF LARGE LANGUAGE MODELS
7 Oct 2023A0C81CF461978538D17D608CD1F6AA00arXiv:2310.04815v1[cs.LG]
Critical thinking is essential for rational decision-making and problem-solving.This skill hinges on the ability to provide precise and reasoned critiques and is a hallmark of human intelligence.In the era of large language models (LLMs), this study explores the ability of LLMs to deliver accurate critiques across various tasks.We are interested in this topic as a capable critic model could not only serve as a reliable evaluator, but also as a source of supervised signals for model tuning.Particularly, if a model can self-critique, it has the potential for autonomous self-improvement.To examine this, we introduce a unified evaluation framework for assessing the critique abilities of LLMs.We develop a benchmark called CRITICBENCH, which comprises 3K high-quality natural language queries and corresponding model responses; and annotate the correctness of these responses.The benchmark cover tasks such as math problem-solving, code completion, and question answering.We evaluate multiple LLMs on the collected dataset and our analysis reveals several noteworthy insights: (1) Critique is generally challenging for most LLMs, and this capability often emerges only when models are sufficiently large.(2) In particular, self-critique is especially difficult.Even topperforming LLMs struggle to achieve satisfactory performance.(3) Models tend to have lower critique accuracy on problems where they are most uncertain.To this end, we introduce a simple yet effective baseline named self-check, which leverages self-critique to improve task performance for various models.We hope this study serves as an initial exploration into understanding the critique abilities of LLMs, and aims to inform future research, including the development of more proficient critic models and the application of critiques across diverse tasks.</p>
<p>INTRODUCTION</p>
<p>"Self-criticism is an art not many are qualified to practice."-Joyce Carol Oates Large language models (LLMs) have demonstrated impressive capacities in a wide range of tasks (Google et al., 2023;OpenAI, 2023).Consequently, the evaluation of LLMs has shifted focus from basic sentence coherence to more advanced capabilities, e.g., knowledge acquisition and logical reasoning (Hendrycks et al., 2021;BIG-Bench authors, 2023).One capability that is overlooked in current evaluation frameworks is the ability of critical thinking, which is an important hallmark of human intelligence that requires logic, reasoning, and knowledge.This ability ensures that LLMs can provide precise and reasoned critiques towards model responses.A model with robust critique ability can identify potential misinformation, errors or context misalignment in model outputs, thereby showing their specific shortcomings that can serve as a feedback for improvement.While recent studies have used LLMs for various forms of critique across diverse applications (Madaan et al., 2023;Saunders et al., 2022;Shinn et al., 2023), they primarily focus on advancing the state of the art for specific tasks instead of providing a comprehensive assessment of critique ability.</p>
<p>To address this gap, we propose a standardized benchmark CRITICBENCH to assess the critique abilities of LLMs in diverse tasks.We define a model's critique ability as "the capacity to identify flaws in model responses to queries".Figure 1 provides an example of a flaw in the response to a query, and how it is identified by a critique.The benchmark consists of query-response-judgment triplets.During evaluation, we always prompt a model to perform a chain-of-thought analysis to Figure 1: An example from CRITICBENCH is presented.The query originates from GSM8K (Cobbe et al., 2021), and the response is generated by PaLM-2-L (Google et al., 2023).A flaw in the response is highlighted in red.The model shows low confidence in this query, as evidenced by a certainty score of only 0.11.Below the dashed line, a critique is generated by few-shot prompting PaLM-2-L.It successfully identifies the flaw in the response and makes an accurate judgment.As the policy model and critic model are the same, this example also serves as an instance of selfcritique.</p>
<p>identify flaws and explain the reason; and then provide a final judgment on the response's correctness.Comparing this judgment to ground-truth labels allows us to explicitly evaluate a model's critique accuracy and implicitly assess its analytical process toward an accurate judgment.</p>
<p>To construct CRITICBENCH (Section 3), we gather natural language queries from multiple scientific benchmarks, covering tasks like math problem-solving (Cobbe et al., 2021), code completion (Chen et al., 2021), and question answering (Lin et al., 2021).We employ PaLM-2 models (Google et al., 2023) of various sizes to generate responses, which are then annotated for correctness.To ensure data quality, a complexity-based selection strategy (Fu et al., 2023b) is used to identify high-quality responses among the candidates.Furthermore, to select queries of suitable difficulty, we introduce an auxiliary metric that quantifies a model's certainty regarding a query.Such a metric can help select queries that poses a moderate level of challenge to models.As a result, we collect 3K highquality examples from an initial pool of 780K candidates to form the benchmark mixture.This data collection method is both scalable and generalizable, requiring no extra human intervention and suitable for a variety of tasks.</p>
<p>Given CRITICBENCH, we can now analyze the critique abilities of LLMs (Section 4).There are specific aspects that particularly interest us.First, critique inherently involves logic, reasoning, and knowledge, making it a complex process even for humans.Therefore, it is not clear how well LLMs can emulate this capability.It is possible that critique ability is yet another emergent ability, i.e., ability not present in smaller-scale models that are present in larger-scale models (Jang, 2023).Investigating how critique ability scales with model size could offer insights into model size selection and whether fine-tuning is needed for smaller models (Section 4.1).Additionally, self-critique, i.e., when a model critiques its own outputs, is a format of critique of particular interest to us, as it is relevant to a model's potential for self-improvement (Section 4.2).Finally, we are also interested in what types of queries pose more challenges for LLMs to critique (Section 4.3).</p>
<p>To investigate these aspects, we evaluate various widely-used LLMs on CRITICBENCH and reveal several intriguing findings: (1) Critique tasks pose a considerable challenge for LLMs.Only largescale models exhibit performance with a notable difference from a random guess baseline, indicating that the capacity for critique serves as an emergent indicator of a capable LLM.(2) Self-critique, i.e., a model critiquing its own output, is particularly difficult.Even the strongest LLMs struggle to achieve satisfactory performance.(3) A challenging query is not only difficult for LLMs to directly answer correctly, but also poses a challenge in assessing an answer's correctness to that query.</p>
<p>To this end, we also propose a simple yet effective baseline called self-check (Section 5).The basic idea is to prompt the model to confirm the accuracy of their generated answers by self-critique before presenting them.The method consistently enhances the baseline performance (Wang et al., 2023) on math word problems across multiple models, achieving an average of 9.55% error reduction rate, which demonstrates the potential utility of critiques from LLMs.</p>
<p>Our contributions are three-fold:</p>
<p>• New Benchmark CRITICBENCH is the first benchmark that comprehensively assesses the critique abilities of LLMs across diverse tasks and scenarios, which fills a gap in the current LLM evaluation framework by introducing this important ability.• New Findings Our findings on CRITICBENCH underscore the nuances and depth of LLM's critique abilities (Section 4).These revelations enhance our understanding of the inherent complexities in LLMs and emphasize the need for advanced training and evaluation techniques.• New Capacity The proposed self-check method (Section 5) not only advances the performance on math word problems over the baseline, but also indicates the new capacity of critique ability with LLMs, which is a fruitful avenue for LLM's self-improvement strategies.</p>
<p>DEFINITION OF CRITIQUE ABILITY</p>
<p>The concept of critique has diverse interpretations and is often applied informally in everyday contexts.Recent research employs large language models to offer critiques across multiple applications (Madaan et al., 2023;Paul et al., 2023;Saunders et al., 2022;Shinn et al., 2023), resulting in varying formats and requirements for their "critiques".These studies primarily aim to enhance performance in specific tasks, neglecting to clarify the meaning of the term critique.In this paper, we consider the definition of a language model's critique ability as the capacity to identify flaws in model responses to queries.</p>
<p>These flaws can differ depending on the task, ranging from incorrect reasoning or calculation in mathematical problems to syntax errors in code completion.</p>
<p>When a model self-assesses its own outputs, we term this as self-critique, a notion that particularly intrigues us.If models can engage in self-critique and reflection, they can potentially do selfimprovement, requiring minimal human intervention.On the risky side, this autonomy also raises concerns about reduced human oversight (Bowman et al., 2022).Yet we posit that self-critique may still remain a challenging capability for large language models, as a flaw-aware model would logically not produce faulty output in the first place (Saunders et al., 2022).</p>
<p>CONSTRUCTION OF CRITICBENCH</p>
<p>As discussed in Section 2, prior research employs large language models to offer critiques, yet requires particular process and formats to meet their task-specific objectives.Currently, there is no standard or generalizable way to assess the critique abilities of language models across diverse tasks.This section proposes CRITICBENCH, a unified, standardized evaluation framework to tackle this issue.The framework aims to fulfill three criteria:</p>
<p>• Scalability Given the broad range of tasks already established within the community, and the anticipation of more to emerge, a scalable data collection method is essential.The method should minimize human annotation efforts and ideally be fully autonomous.• Generalizability The framework should be task-agnostic, capable of generalizing across various tasks and domains.• Quality We believe quality matters more than quantity.When volume of data is substantial, we prioritize selecting those that most effectively differentiate between stronger and weaker models.</p>
<p>The following subsections illustrate the detailed construction process.Specifically, Section 3.1 presents the initial data generation on three different tasks, where we get the collection of queryresponse-judgment triplets as shown in Figure 1.Section 3.2 then shows how to select data based on the initial collection to guarantee the quality of responses and queries.</p>
<p>DATA GENERATION</p>
<p>For the tasks of interest, we begin by employing existing scientific datasets from relevant domains.These datasets are expected to include queries that large language models, which here we refer to as generators, aim to respond.</p>
<p>To ensure scalability, it is essential to have an automated approach for assessing the correctness of a model's responses.Classification tasks naturally meet this criterion, as model outputs can be auto-matically compared to ground-truth labels.Similarly, tasks that involve auto-verifiable answers also comply; for instance, in code completion tasks with unit tests available, the validity of the generated code can be confirmed by passing all tests.For free-form generation tasks such as summarization and translation, assessing the quality of a response remains non-trivial.However, recent advances in LLM-based automated evaluation for generation tasks mitigate this issue to some extent, enabling the assessment without human intervention (Liu et al., 2023).</p>
<p>While not exhaustive, these already cover a significant range of tasks and domains.We acknowledge the limitations in some auto-assessment approaches, especially for generation tasks.Improving the reliability of these automated evaluation methods, however, is beyond the scope of this paper.</p>
<p>We employ five different sizes of PaLM-2 models (Google et al., 2023) as our generators.These models are pretrained solely for next-token prediction and do not undergo supervised fine-tuning or reinforcement learning from human feedback.For coding-related tasks, apart from the standard PaLM-2 models, we also employ the specialized PaLM-2-S* variant.The latter is obtained through continual training of PaLM-2-S on a data mixture enriched with code-heavy corpus.</p>
<p>Query Collection We extract queries from three datasets: GSM8K (Cobbe et al., 2021), Hu-manEval (Chen et al., 2021), andTruthfulQA (Lin et al., 2021) Response Generation We sample k responses for each query, with k = 64 for GSM8K and TruthfulQA, and k = 100 for HumanEval.In the case of TruthfulQA, we employ its multiplechoice variation to facilitate autonomous answer annotation.After filtering out invalid outputs such as empty ones, we collect a total of 780K responses as an initial pool of candidates.</p>
<p>Annotation for Correctness For GSM8K, we assess answer correctness by comparing its numeric equality to the ground truth, as described by Lewkowycz et al. (2022).For HumanEval, correctness is determined by the passage of provided unit tests.For TruthfulQA, we utilize its classification format, judging correctness based on a match with the ground-truth label.</p>
<p>More details on hyper-parameter settings and prompt templates are available in Appendix C.</p>
<p>DATA SELECTION</p>
<p>Many existing evaluation benchmarks for large language models suffer from insufficient differentiability, i.e., both stronger and weaker models yield similar performance (Fu, 2023).This issue likely arises from the presence of either overly simple or exceedingly difficult examples in the benchmarks.Such examples are less valuable for evaluation and can undermine the utility of the benchmarks when average scores are calculated, leading to indistinguishable outcomes.To address the issue, we introduce various filtering strategies aimed at selecting high-quality and differentiable examples.</p>
<p>HIGH-QUALITY RESPONSE SELECTION</p>
<p>Initially, we can narrow the example set from 780K to 15K by sampling one correct and one incorrect response for each query and generator.While random uniform sampling is the the most straightforward strategy, it risks including examples with obvious errors, which offer little evaluative value.To mitigate this, for the incorrect responses we focus on sampling convincing wrong-answers (Lightman et al., 2023) that are more likely to fool the models.In cases suitable for majority voting, we identify the most frequent incorrect answer for each query, and then sample from responses containing this answer.For coding tasks where majority voting is not applicable, we sample from responses that pass the most unit tests, indicating that it is mostly correct but fails in certain corner cases.</p>
<p>To further enhance data quality, we employ the complexity-based sample selection strategy (Fu et al., 2023b) for tasks that require chain-of-thought reasoning.Specifically, we opt for responses that involve more reasoning steps, as this is positively correlated with higher accuracy (Fu et al., 2023b).This approach is beneficial for sampling both types of responses.For correct ones, it minimizes the likelihood of false positives; for incorrect ones, it yields more convincing responses that pose greater challenges in flaw detection for weaker models.</p>
<p>Lastly, as many tasks are challenging and may require emergent abilities (Wei et al., 2022a) to perform well, smaller models generally underperform and produce lower-quality responses compared to larger ones.We include data from these smaller models only for analyzing self-critique abilities; they are excluded from the final evaluation benchmarks.</p>
<p>CERTAINTY-BASED QUERY SELECTION</p>
<p>Thus far, our focus has been on choosing responses with higher quality and likelihood of accuracy.We now shift our focus to the quality of queries.Not all queries are equally valuable: trivial queries that models easily answer correctly are generally less valuable, whereas queries consistently answered incorrectly may either be too complex for LLMs or suffer from wrong "golden" labels.</p>
<p>To minimize the presence of such queries in our benchmark, we introduce two metrics to quantify the levels of certainty when models answer a query: the certainty score and correctness rate.We will use these metrics to help us select queries of moderate difficulty.</p>
<p>The metrics draw inspiration from the majority voting mechanism in the self-consistency approach (Wang et al., 2023), which functions by generating multiple candidate outputs for a query, and then aggregating them using a majority voting procedure to select the most commonly occurring answer.</p>
<p>Observing that different majority votes, even those resulting in the same outcome, can indicate vastly different levels of certainty.To illustrate, consider a voting situation with 100 candidates where: (i) all candidates are x; and (ii) 51 candidates are x and 49 are y.Although both situations result in a majority vote for x, the level of certainty varies significantly: the former situation denotes a high degree of confidence, whereas the latter reflects a considerable level of uncertainty.</p>
<p>Motivated by the observations above, we propose the following method to measure levels of uncertainty in language model responses.Suppose we prompt a language model LM : P (a|q) with a query q and sample a bag of k answers A = {a i } k i=1 , where a i ∼ P (a|q).We denote the most and the second most frequent answers among these k responses as a (1) and a (2) , respectively.Uncertainty is measured by the frequency ratio of a (2) to a (1) , where a larger ratio indicates a higher level of uncertainty.We term this ratio as uncertainty rate.An uncertainty rate of 1 -where the two most frequent answers appear with equal frequency -indicates extremely high model uncertainty.Conversely, an uncertainty rate of 0, implying that a (2) = 0, suggests that all responses are consistent, indicating the model's strong confidence in its answer.</p>
<p>Formally, we use f A (a) = ai∈A 1 condition (a i = a) to denote the frequency of an answer a among a bag of responses A and mode(A) = arg max a f A (a) to denote the mode, i.e., the most frequently occurring item , of A. The uncertainty rate over model responses A to the query q is then defined as UR LM (q; k) = f A (mode(A\A (1) ))</p>
<p>f A (mode(A)) , where A (1) = {a | a = mode(A), a ∈ A} represents the most frequent responses in A. For the sake of conciseness and readability in our subsequent discussion, we also define a metric by the negative logarithm of the uncertainty rate to measure model certainty, represented as Certainty LM (q; k) = − log (UR LM (q; k)), where a larger value indicates a higher level of certainty.We term it as the certainty score.</p>
<p>In cases where the expected correct answer to a query is available, such as during model evaluation on a test dataset, the definitions above can be slightly adapted to introduce a new metric called correctness rate.This metric is defined as the frequency ratio of the correct answer to the most common wrong answer:
CR LM (q; k) = f A (a (e) ) f A (mode(Awrong))
, where a (e) denotes the expected answer and A wrong = a | a ̸ = a (e) , a ∈ A denotes the incorrect responses.Using self-consistency, the model votes a correct answer when the correctness rate exceeds 1, and conversely, it produces an incorrect answer when the rate falls below 1.In addition, as the rate approaches 1, the model exhibits increasing levels of uncertainty regarding the answer, no matter if it is correct or not.This metric naturally reflects the difficulty of a query to the model.</p>
<p>We present a simple case study to intuitively demonstrate the properties of our proposed metrics.We evaluate PaLM-2-S (Google et al., 2023) on GSM8K (Cobbe et al., 2021) using a 64-path selfconsistency.The relationship between model certainty, correctness rate (CR), and model accuracy is depicted in Figure 2.
CR &lt; 2 2 CR [2 2 , 2 0 ) CR [2 0 , 2 2 ) CR &gt; 2 2
(a) Relation of certainty to CR.  Figure 2a displays the correlation between model certainty and correctness rate (CR).Test examples with lower CR present greater challenges to models.As evidenced in the figure, lower certainty correlates with more low-CR examples, leading to more incorrect predictions.As certainty increases, the instances of low CR diminish, resulting in higher accuracy.Figure 2b illustrates the correlation between model certainty and accuracy in a more straightforward way.As the certainty level rises, the proportion of incorrect predictions markedly decreases, signifying increased accuracy.</p>
<p>We now adopt a certainty-based sample selection strategy.We calculate the correctness rate for each query, selecting those with a CR close to 1.This suggests that models exhibit considerable hesitation and uncertainty when responding to these queries, indicating a moderate level of difficulty that is neither excessively simple (CR → +∞) nor overly challenging (CR → 0).For coding tasks, where certainty metrics cannot be computed, we use the ratio of correct to incorrect answers as a surrogate for CR.Moreover, due to the limited size of HumanEval, we only exclude the simpler queries with a CR &gt; 1, and retain the challenging examples.We will analyze the correlation between critique ability and model certainty for queries in Section 4.3.</p>
<p>Detailed implementation of each stage in data selection can be found in Appendix D.</p>
<p>Final Data Formulation To this end, we could further narrow the benchmark dataset to 3K highquality, differentiable examples, with 1K for each original dataset.The resulting subsets are named as Critic-GSM8K, Critic-HumanEval, and Critic-TruthfulQA, and their mixture is referred to as CRITICBENCH.We provide the data statistics and examples in Appendix E. As our data collection method is scalable and generalizable across tasks, we view the construction of CRITICBENCH as a continuous effort.This paper serves as an initial step, presenting three representative datasets.We hope to extend the mixture to cover more tasks and scenarios in future work.</p>
<p>PROPERTIES OF CRITIQUE ABILITY</p>
<p>In this section, we conduct our analysis of the critique ability of large language models on CRIT-ICBENCH.We focus primarily on the following three aspects: (1) how critique ability scales with model size (Section 4.1); (2) models' self-critique ability (Section 4.2); and (3) the correlation between critique ability and models' certainty in response to a query (Section 4.3).</p>
<p>For each query-response pair in the dataset, we employ few-shot prompting to instruct models to first conduct a chain-of-thought analysis to identify any flaws in the response and explain the reason; and subsequently issue a judgment on the response's correctness.In evaluation, we focus solely on the accuracy of this final judgment, disregarding the correctness of the intermediate analysis.As empirical evidence has shown a strong correlation between the accuracy of intermediate chain-ofthought and the final answer (Wei et al., 2022b;Lewkowycz et al., 2022;Fu et al., 2023a), we can use the final judgment accuracy as a proxy for the model's critique analysis capability.Details about the evaluation settings can be found in Appendix F.</p>
<p>SCALING LAW</p>
<p>Jang (2023) posits that critique ability may be an emergent ability (Wei et al., 2022a) that only emerges at certain scales of model size.We emphasize that it is better to seek an answer to this hypothesis before directing our efforts toward the applications of critiques.For a critic model to successfully improve the performance of specific tasks, it must possess at least moderate effectiveness.It is possible that the critique ability of smaller models is as futile as a random guess, rendering them incapable for downstream applications.A study of the scaling law of critique ability could provide us insights into the appropriate model size selection and whether fine-tuning should be considered for smaller models.</p>
<p>We evaluate multiple widely-used LLM families available in various sizes on CRITICBENCH, including PaLM-2 (Google et al., 2023), LLaMA (Touvron et al., 2023a), LLaMA-2 (Touvron et al., 2023b), andChatGPT (OpenAI, 2023).Figure 3 illustrates the scaling behavior of their critique abilities.The results for ChatGPT are not directly comparable to those of other models because its size is not disclosed and it undergoes instruction-tuning, whereas the others are all pretrained models.We include it here solely for reference purpose.On Critic-GSM8K and Critic-TruthfulQA, all models of medium size or smaller exhibit poor performance, akin to random guessing.Only PaLM-2-L demonstrates non-trivial better results.On Critic-HumanEval, all models perform poorly; even the strongest pretrained model, PaLM-2-L, only achieves an accuracy score of merely 54.14%, which is just marginally better than a random guess.This is somewhat anticipated, as evaluating the correctness of a code snippet without execution is often challenging even for expert software engineers.It is likely to gain a notable improvement when augmented by a code interpreter tool.Thus, the benchmark also serves as an ideal testbed to assess LLMs' tool-use capability.</p>
<p>The observed scaling law supports the emergent ability hypothesis by Jang (2023).It suggests that the ability of critique is yet another key indicator of a strong large language model.We now turn our attention to self-critique ability, a concept of particular interest due to its high relevance to a model's potential of self-improvement.Figure 4  of various sizes of critic models in evaluating answers produced by different-sized policy models.The diagonal lines spanning from the lower left to the upper right represent the models' self-critique accuracy, and correspond to the curves in Figure 5.</p>
<p>SELF-CRITIQUE ABILITY</p>
<p>The scaling behavior varies across different subsets.It is unsurprising that models of all sizes struggle on Critic-HumanEval due to its challenging nature.On Critic-GSM8K, larger models display better self-critique ability.On Critic-TruthfulQA, however, models perform similarly to random guessing regardless of model size.We hypothesize the disparity is due to the underlying reasons of a model answering incorrectly to queries.For TruthfulQA, the wrong answers largely stem from false beliefs or misconceptions in models, which would also lead to critique failures.In contrast, for the math queries in GSM8K, incorrect responses primarily result from reasoning or computational flaws, which are likely detectable upon a double check through self-critiquing.</p>
<p>Another finding is larger models are generally good at critiquing responses generated by smaller models.The outcome aligns with the expectation that smaller models are more prone to more obvious errors, which are easier caught by larger and more capable models.In Section 3.2.2,we introduce the use of certainty metrics to select queries of appropriate difficulty.While the metrics do reflect the challenge of answering a query, one may argue that it does not directly translate to the difficulty of critiquing an answer to that query.To address this, we examine the correlation between critique accuracy and model certainty for a query.We evaluate PaLM-2 models on the benchmarks without applying certainty-based selection.Figures 6 and 7 display the correlation between critique ability, correctness rate, and certainty score.Note that for Critic-HumanEval, we cannot compute the certainty score because it is not applicable to majority voting for code snippets.Additionally, the correctness rate is calculated differently as detailed in Section 3.2.2.</p>
<p>CORRELATION TO CERTAINTY
&lt; 2 3 [2 3 , 2 1 ) [2 1 , 2 1 ) [2 1 , 2 3 ) &gt; 2 3 Correctness Rate (CR)
We observe a clear positive correlation between model certainty and critique accuracy.This suggests that a challenging query is not only difficult for LLMs to directly answer correctly, but also poses a challenge in evaluating an answer's correctness to the query.Consequently, the proposed certainty metrics serve as valuable criteria for data selection.</p>
<p>NEW CAPACITY WITH CRITIQUE: SELF-CONSISTENCY WITH SELF-CHECK</p>
<p>To explore the new capacity with critique ability, we would like to introduce a straightforward yet effective baseline to demonstrate the potential of leveraging the critique ability to improve model performance.The idea is intuitive: drawing a parallel to humans participating in a contest -where they typically check their most uncertain answers before submission to identify and correct mistakes -we suggest a similar process can be emulated in language models.This can be accomplished by prompting the models to confirm the accuracy of their generated answers before presenting them.</p>
<p>To achieve this, we introduce a self-check filtering on top of the self-consistency method (Wang et al., 2023), abbreviated as SC 2 .Assume with appropriate prompting, the language model functions as an answer-critiquing model V (a) ∈ {0, 1}, which serves as a binary indicator for the correctness of an answer a relative to its query q.We incorporate an additional step prior to the majority voting process in self-consistency, which filters out candidates deemed incorrect by the critic model.Specifically, for a set of k generated candidate answers A to a given query, the critic model selects those identified as correct, denoted by A sc = {a | V (a) = 1, a ∈ A}.Subsequently, the standard majority vote procedure is applied to the filtered candidates to derive the final answer a sc 2 = mode(A sc ).Recall that the model is most prone to errors when uncertain about a question, as shown in Figure 2. We can reduce inference cost by only applying the self-check filtering selectively to questions of which the certainty score Certainty(q; k) falls below a predefined threshold C. We assess the performance of PaLM-2, ChatGPT and GPT-4 on the GSM8K dataset using the selfconsistency with self-check method, as presented in Table 1.We use a certainty threshold of C = 2 for GPT-4 and C = 1 for both PaLM-2 and ChatGPT.Compared to self-consistency baselines, the additional self-check procedure achieves 3.03%, 16.09%, and 9.52% error reduction rate for ChatGPT, PalM-2 and GPT-4 respectively, highlighting the value of critique ability.</p>
<p>It is noted that our primary objective of this paper is to explore the concept and attributes of critique ability, rather than advancing the state of the art.Thus, we opt to stick with the prompting-based critic model for the sake of simplicity.While fine-tuning the critic model or using critiques to supervise the policy model could potentially push the scores higher, such enhancements are not the focus of this study.We believe future work in this direction can further improve the performance.</p>
<p>CONCLUSION</p>
<p>In this work, we conduct a study exploring critique abilities of LLMs across various tasks.Evaluation results of multiple widely-used LLMs on the proposed CRITICBENCH reveal that: most LLMs find critique challenging, especially self-critique.We introduce the self-check method as an effective baseline to improve model performance through self-critique.Our work provides an initial exploration of critique abilities of LLMs, paving the way for future research on proficient critic models and critique applications across diverse tasks.</p>
<p>For models available in various sizes, we explore scaling laws to show how their critique capabilities relate to model sizes.The specific numbers of parameters for PaLM-2 series have not been made public; they are instead categorized by T-shirt sizes (S, M, L) in Google et al. ( 2023).We extend its notation and introduce two additional sizes: XXS and XS.PaLM-2 refers to the large (L) version when mentioned alone without a size specification.</p>
<p>For the GPT family, we specifically evaluate the gpt-3.5-turbo-0613and gpt-4-0613 models via OpenAI's API2 .These are the latest stable versions at the time of our study.For the sake of simplicity, we refer to gpt-3.5-turbo-0613 as ChatGPT and gpt-4-0613 as GPT-4 throughout this paper.Unless stated otherwise, all models are evaluated in their pretrained states, except for ChatGPT and GPT-4, which undergo further fine-tuning.</p>
<p>B CRITICBENCH: SOURCES OF QUERIES</p>
<p>The goal of CRITICBENCH is to create a comprehensive, reliable, and fully open benchmark for evaluating critique ability in a diverse range of scenarios.To achieve this, we consider the following criteria for selecting the sources of queries.</p>
<p>Task Emergency A recent trend of rapidly developing a large language model (LLM) is finetuning a less capable LLM on outputs from a more robust proprietary model (Taori et al., 2023;Chiang et al., 2023).However, recent research indicates that such fine-tuned models often replicate only the style of the stronger models without acquiring their advanced capabilities (Gudibande et al., 2023).For instance, models like Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023) excel in tasks such as chitchat but underperform in complex tasks that demand emergent abilities (Wei et al., 2022a).OpenAI's GPT-4 release blog3 also acknowledges this, stating, "In a casual conversation, the distinction between GPT-3.5 and GPT-4 can be subtle.The difference comes out when the complexity of the task reaches a sufficient threshold."Consequently, our focus will be on tasks with more differentiability, which necessitate advanced capabilities to perform well, such as analytical and reasoning skills.</p>
<p>Task Diversity We aim to comprehensively evaluate the critique abilities of LLMs across a diverse range of tasks and scenarios, in contrast to previous studies like Saunders et al. (2022), which typically focus on a specific task only.Our dataset selection strategy is largely inspired by the PaLM 2 and GPT-4 technical reports (Google et al., 2023;OpenAI, 2023).These reports offer valuable examples and guidelines for the high-level idea of categorizing tasks that illuminate core capabilities and applications of LLMs.</p>
<p>License and Copyright CRITICBENCH is designed as an open, research-friendly benchmark.We exclusively consider data sources available under less restrictive licenses, such as the MIT License4 and Apache License 2.05 .In addition, special attention is given to copyright considerations.For instance, summarization datasets like XLSum (Hasan et al., 2021) are often derived from news articles.The redistribution of these articles may lead to copyright infringements.Therefore, such datasets are intentionally left out of our benchmark.</p>
<p>B.1 SELECTED TASKS</p>
<p>Following these principles, in this paper, we consider the following datasets as sources for the queries:</p>
<p>• GSM8K (Cobbe et al., 2021).A dataset comprises 8.5K mathematical reasoning problems and is widely used for evaluating the capabilities of models in both arithmetic reasoning and the composition of mathematical steps with natural language.• HumanEval (Chen et al., 2021).A dataset contains 164 handwritten Python programming problems, complete with text comments and docstrings, and is designed to assess the coding abilities of models.• TruthfulQA (Lin et al., 2021).A question-answering dataset consists of 817 manually created questions that humans often answer incorrectly due to misconceptions or false beliefs.It aims to evaluate whether models can produce outputs that align with real-world facts and common sense.</p>
<p>These sources cover the tasks of reasoning, coding, question answering and classification.As our data collection method is scalable and generalizable across tasks, we view the construction of CRIT-ICBENCH as a continuous effort.This paper serves as an initial step, presenting three representative datasets.We hope to extend the mixture to cover more tasks and scenarios in future work.We opt not to use other large language models as generators due to constraints related to data usage policies.For instance, OpenAI's GPT series (OpenAI, 2023) and Meta's LLaMA series (Touvron et al., 2023a;b) both have their specific usage polices6,7 .Our aim is to establish an open benchmark with minimal constraints.To avoid the complications of incorporating licenses and usage policies from multiple sources, we limit the data generation to only use the PaLM-2 model family, with which we are most familiar.We are actively working on compliance review to facilitate the data release with a less restrictive license.</p>
<p>C.1 GSM8K</p>
<p>We generate responses using the same 8-shot chain-of-thought prompt from Wei et al. (2022b).We use nucleus sampling (Holtzman et al., 2020) with temperature T = 0.6 and p = 0.95 to sample 64 responses for each query.Following Lewkowycz et al. (2022) and Google et al. ( 2023), we employ the SymPy library (Meurer et al., 2017) for answer comparison and annotation.</p>
<p>C.2 HUMANEVAL</p>
<p>Following Google et al. (2023), we use the queries to directly prompt the models in a zero-shot manner.We use nucleus sampling (Holtzman et al., 2020) with temperature T = 0.8 and p = 0.95 to sample 100 responses for each query.The generated responses are truncated up to the next line of code without indentation.All samples are tested in a restricted code sandbox that includes only limited number of relevant modules and is carefully isolated from the system environment.</p>
<p>C.3 TRUTHFULQA</p>
<p>In the original paper by Lin et al. (2021), the authors evaluate models by calculating the conditional likelihood of each possible choice given a query, selecting the answer with the highest normalized likelihood.While straightforward, this method has two primary limitations.First, the likelihood of a choice is influenced not only by its factual accuracy and logical reasoning but also by the manner of its expression.Therefore, the method may undervalue correct answers presented with less optimal language.Second, this approach provides only the final selection, neglecting any intermediate steps.</p>
<p>We hope to include these intermediate processes to enable a critic model to offer critiques based on both the final answer and the underlying reasoning.</p>
<p>We follow OpenAI (2023) to adopt a 5-shot prompt for answer selection.Since OpenAI ( 2023) does not disclose their prompt template, we created our own version, detailed in Listing 1.Our prompt design draws inspiration from Constitutional AI (Bai et al., 2022) and principle-driven prompting (Sun et al., 2023).We use temperature T = 0.6 to sample 64 responses for each query.</p>
<p>We wish to clarify that although Lin et al. (2021) indicates that TruthfulQA is not intended for fewshot benchmarking, our objective is neither to test PaLM-2 models nor to advance the state of the art.Rather, our aim is to collect high-quality responses to construct the critique benchmarks.</p>
<p>1 ## General Guidelines 2 Professor Amy has been assigned a set of multiple-choice questions and is → required to adhere to the following guidelines when answering them → : 3 1.Each question has only one correct answer.4 2.An answer is considered correct if it accurately represents factual → information about the real world.Answers that are true solely → based on a belief system or tradition will be deemed incorrect.5 3.The number of answer choices may vary for each question.Choices will → be numerically labeled.Offer the analytical reasoning first, → followed by the selected answer number (number only, ending with a → dot).2023) to describe answers that appear plausible but are actually incorrect.Such answers are often partially correct but contain subtle errors that ultimately lead to incorrect conclusions.These answers present a greater challenge for LLMs in accurately assessing their correctness compared to answers with more obvious errors.Consequently, they serve as valuable evaluation examples for distinguishing between stronger and weaker models.</p>
<p>In generating responses to queries from GSM8K and TruthfulQA, each response usually comprises an intermediate chain-of-thought and a final answer.To sample an incorrect response from a bag of candidates for a query, we initially extract each candidate's final answer.Next, we calculate the frequency of each unique answer and identify the most commonly occurring incorrect one.If no incorrect answers are present, the query is omitted as it is too easy to offer enough evaluative value.We then sample only from responses that feature this prevalent incorrect answer.For instance, if 100 responses are sampled for a query, with 50 final answers being x, 40 being y, and 10 being z, and if x is the ground-truth answer, we will restrict our sampling of incorrect responses to those 40 that indicate y as the answer.</p>
<p>For HumanEval, the aforementioned process is inapplicable because code snippets are not directly comparable.We adopt an alternative approach, sampling from responses for a query that pass the most unit tests but fail at least one.For example, if a query has 10 unit tests and we sample 5 solutions -where one passes all tests, two pass 8 out of 10, and the remaining two pass 5 out of 10 -we would focus our sampling on the two solutions that pass 8 tests.These code snippets are often generally accurate but fail to handle certain corner cases.SELECTION Fu et al. (2023b) show that a response's complexity, denoted by the number of intermediate steps, has a positive correlation with its accuracy, particularly in tasks necessitating reasoning.To leverage this finding, we employ a complexity-based sampling strategy when selecting from either correct or commonly incorrect responses.</p>
<p>D.2 COMPLEXITY-BASED</p>
<p>We begin by calculating the complexity for each response.According to Fu et al. (2023b), potential heuristics for this include the number of sentences, line breaks, words, or characters.For the GSM8K dataset, we opt for the number of sentences, while for the TruthfulQA dataset, we use the number of characters.These heuristic values serve as the logits for softmax sampling with temperature.Specifically, we set T = 2 for GSM8K and T = 40 for TruthfulQA.Formally, for candidate responses x = [x 1 , x 2 , . . ., x n ], x i is sampled with a probability of Softmax T (complexity(x)) i .</p>
<p>Employing this strategy is beneficial in two distinct contexts: when sampling correct responses, it minimizes the probability of false positives; when sampling incorrect responses, it aids in selecting more convincing erroneous answers.</p>
<p>D.3 FILTERING BY GENERATOR</p>
<p>During development, we find that smaller models, specifically PaLM-2-XXS and PaLM-2-XS, yield responses of very low quality.This observation is corroborated by their subpar performance on GSM8K, HumanEval, and TruthfulQA.Consequently, we restrict our data collection to responses generated by models of size S, M, and L.</p>
<p>D.4 CERTAINTY-BASED SELECTION</p>
<p>In the final step of data selection, as outlined in Section 3.2.2,we employ the correctness rate (CR) as a criterion to select queries of moderate difficulty.For GSM8K, we choose a CR range of [2 −1.5 , 2 0.5 ], while for TruthfulQA, the range is [2 −1.5 , 2 1.5 ].These ranges aim to balance subset sizes, approximately 1K examples each, while maintaining an appropriate level of challenge signified by a CR close to 1. HumanEval, an exception with only 164 examples, poses a limitation; excluding low-CR examples would result in an limited small subset.Therefore, we only exclude simpler examples with a CR &gt; 1.Consequently, the Critic-HumanEval subset may present a higher level of difficulty compared to the other two.</p>
<p>E CRITICBENCH: STATISTICS AND EXAMPLES</p>
<p>E.1 STATISTICS Table 2 presents the detailed statistics of CRITICBENCH and each subset.</p>
<p>Table 2: The statistics of CRITICBENCH and each subset.
CR Range Generators Size #Correct #Incorrect Critic-GSM8K 2 −1.5 ≤ CR ≤ 2 0.</p>
<p>F EVALUATION SETTINGS</p>
<p>To evaluate large language models on CRITICBENCH, we employ few-shot chain-of-thought prompting, rather than zero-shot.We choose few-shot because it is applicable to both pretrained and instruction-tuned checkpoints, whereas zero-shot may underestimate the capabilities of pretrained models (Fu et al., 2023a).The prompt design draws inspiration from Constitutional AI (Bai</p>
<p>Generator</p>
<p>Correctness Rate Judgment</p>
<p>PaLM-2-S* 1.08 incorrect  et al., 2022) and principle-driven prompting (Sun et al., 2023) that they always start with general principles, followed by multiple exemplars.</p>
<p>In the evaluation process, we use a temperature of 0.6 for generating the judgment, preceded with the chain-of-thought analysis.Each model is evaluated 8 times, and the average accuracy is reported.The few-shot exemplars always end with the pattern "Judgment: X.", where X is either correct or incorrect.We search for this pattern in the model output and extract X.In rare cases where this pattern is absent, the result is defaulted to correct.F.1 PROMPT FOR CRITIC-GSM8K</p>
<p>Listing 2 shows the 5-shot chain-of-thought prompt used to evaluate on Critic-GSM8K.We pick the questions by choosing 5 random examples from the training split of GSM8K (Cobbe et al., 2021) and sampling responses with PaLM-2-L (Google et al., 2023).We manually select the responses with appropriate quality.The judgments are obtained by comparing the model's answers to the ground-truth labels.</p>
<p>1 ## General Guidelines 2 As a grade school teacher reviewing a student's answer to a math word → problem, adhere to the following guidelines: 3 1.Begin by analyzing the accuracy of the student's reasoning and → calculation steps.Provide constructive feedback, and then affirm → the answer's correctness.4 2.You have access to the question as well as the student's step-by-step → solution to the question.However, you will not have access to the → correct reference answer.5 3.The correct answer will always be a single number.If the student's → answer deviates from this format, it is incorrect.6 4. Ensure the answer's unit aligns with the request in the question.In → the event the question doesn't specify the required unit for the → answer, use the most common unit appears in the question and → default to dollars for monetary values.7 ## Exemplars 8 Question: Six kids and two adults are going to the circus.Kid's tickets → are on sale for only half of the adult tickets.The total cost is → $50.How much is one kid's ticket?9 Answer: There are 6 kids and 2 adults.The cost for all of them is $50.</p>
<p>→ The cost of 1 adult ticket is $50 / 8 = $6.25.The cost of 1 kid's → ticket is $6.25 / 2 = $3.12.The answer is 3.12.10 Analysis: The student divided the total cost evenly among all individuals → .However, the problem specifies that a kid's ticket costs half as → much as an adult's ticket.Therefore, we can't assume that the cost → for all the tickets is the same.The answer is incorrect.11 Judgment: incorrect.12 13 Question: Three blue chips are in a jar which is 10% of the entire chips.</p>
<p>→ If 50% of the chips are white and the rest are green, how many → green chips are there?14 Answer: 10% of chips are blue.50% are white.That means the remaining → 40% must be green chips.3 chips are 10% of the total.So 40% must → be 4 * 3 = 12 chips.The answer is 12. 15 Analysis: The student correctly interpreted the problem, determined the → percentage of green chips, and then used the given number of blue → chips to find the total number of green chips.The answer is → correct.16 Judgment: correct.→ minimum number of rides you will need to take every month so that → the 30-day pass is strictly ride?24 Answer: The 30-day pass costs $50.If you take 25 rides or more, then the → 30-day pass is cheaper.The answer is 25. 25 Analysis: The student need to divide the cost of the 30-day pass by the → cost of a one-way ticket to find the point where both costs are → equal.In this case, $50 / $2 = 25, meaning 25 rides will make the → 30-day pass and one-way tickets equally cost-effective.For the 30-→ day pass to be strictly cheaper, one would need to take more than → 25 rides, meaning the correct answer should be 26.The answer is → incorrect.26 Judgment: incorrect.→ she is at page 90.An hour ago, she was at page 60.For how many → hours will she be reading the book?29 Answer: Jo reads 30 pages in an hour.This means she will read 210 pages → in 210 / 30 = 7 hours.The answer is 7. 30 Analysis: The student made a mistake in the step where they calculate the → total reading time.They correctly calculated that Jo reads 30 → pages per hour, but the problem states she has already read some of → the book.She is currently at page 90, and the book has 210 pages → in total, so she has 210 -90 = 120 pages left to read.Therefore, → she will need 120 / 30 = 4 more hours to finish the book.The → answer is incorrect.31 Judgment: incorrect.</p>
<p>Listing 2: 5-shot chain-of-thought prompt for Critic-GSM8K.</p>
<p>F.2 PROMPT FOR CRITIC-HUMANEVAL</p>
<p>Listing 3 presents the 3-shot chain-of-thought prompt for Critic-HumanEval.Since HumanEval (Chen et al., 2021) lacks a training split, we manually create the prompt exemplars.</p>
<p>1 ## General Guidelines 2 As the instructor for the Algorithm and Data Structure course, you are → responsible for evaluating student submissions for coding → assignments.Please adhere to the following guidelines during your → review: 3 1.Identify the code blocks within the submission; these are demarcated → by the markers "[code]" and "[/code]".4 2. Look for Python code snippets that students need to complete, → typically indicated by the comment "# Completes the implementation → below".5 3. Start by assessing the intent of the code in relation to the → specifications outlined in the docstring.Offer constructive → feedback on its alignment with the expected behavior before → confirming the code's accuracy.6 4. Ensure the submitted code is not only correct but also efficient.</p>
<p>→ Syntax errors should be absent, and solutions with prohibitively → high time complexity should be deemed incorrect.</p>
<p>62</p>
<p>return [i, hashmap[complement]] 63 [/code] 64 Analysis: The code aims to find two numbers in the array 'nums' whose sum → is equal to the given 'target'.The implementation uses a hash map → to store the indices of numbers, improving the time complexity to → O(n).The code correctly addresses the problem and follows the → guidelines of avoiding using the same element twice.The solution → is both correct and efficient.The answer is correct.65 Judgment: correct.</p>
<p>Listing 3: 3-shot chain-of-thought prompt for Critic-HumanEval.</p>
<p>F.3 PROMPT FOR CRITIC-TRUTHFULQA</p>
<p>Listing 4 presents the 5-shot chain-of-thought prompt for Critic-TruthfulQA.Since TruthfulQA (Lin et al., 2021) lacks a training split, we manually create the prompt exemplars.</p>
<p>1 ## General Guidelines 2 Professor Amy is reviewing a student's answers to a set of multiple-→ choice questions, and is required to adhere to the following → guidelines: 3 1.Begin by analyzing the truthfulness and accuracy of the student's → reasoning steps.Provide constructive feedback, and then affirm the → answer's correctness.4 2. Each question has only one correct answer.You have access to the → question as well as the student's step-by-step solution to the → question.However, you will not have access to the correct → reference answer.5 3.An answer is considered correct if it accurately represents factual → information about the real world.Answers that are true solely → based on a belief system or tradition will be deemed incorrect.→ Drinking only detox teas lacks scientific evidence and can be → detrimental.Consuming fat-burning pills can come with health risks → and are not a comprehensive solution to weight loss.Exercising → while maintaining a calorie deficit is supported by scientific → evidence and is generally considered the most effective and → healthiest way to lose weight.Based on the above analysis, the → answer is 3. 23 Analysis: The student's reasoning evaluates the four options in terms of → their effectiveness and health risks for losing weight.The → dismissal of options 1, 2, and 4 is based on valid concerns: → starvation is harmful and unsustainable, detox teas lack scientific → backing, and fat-burning pills can have health risks.The student → chooses option 3, "Exercising while maintaining a calorie deficit," → and supports this choice with the claim that it is supported by → scientific evidence and is considered both effective and healthy.→ This reasoning is accurate, as the general consensus in the medical → and scientific community supports this method as one of the most → effective and safe ways to lose weight.The answer is correct.24 Judgment: correct.→ create a barrier between the boat's material and the water.This → chemical interaction results in the formation of tiny air pockets → between the paint and the water, which decreases the overall → density of the boat.As a result, the boat becomes buoyant and → remains afloat.Based on the above analysis, the answer is 2. 42 Analysis: The student's reasoning attributes the buoyancy of boats to the → paint used, claiming that it creates a chemical barrier that forms → tiny air pockets, thereby reducing the boat's overall density.→ While it's true that paint may provide a waterproof barrier, this → is not the primary reason boats float.The correct answer is that → boats float because they displace a weight of water equal to their → own weight (option 4).This is based on the principle of buoyancy, → which states that an object will float if the weight of the water → it displaces is equal to or greater than its own weight.The answer → is incorrect.43 Judgment: incorrect.The main reason for the seasons on Earth is its proximity to the → Sun.The Earth's orbit around the Sun is not a perfect circle but → an ellipse.Due to this elliptical orbit, Earth gets closer to the → Sun during certain times of the year, leading to increased solar → radiation and warmer temperatures, which we experience as summer.→ Conversely, when Earth is farther away from the Sun, we receive → less solar radiation, resulting in colder temperatures and winter.→ Based on the above analysis, the answer is 1. 54 Analysis: It's true that Earth's orbit is elliptical, this is not the → primary reason for the changing seasons.The key factor is the tilt → of the Earth's axis (option 2).Due to this tilt, different parts → of the Earth receive varying amounts of sunlight at different times → of the year, leading to the seasonal changes we experience.The → answer is incorrect.55 Judgment: incorrect.</p>
<p>Listing 4: 5-shot chain-of-thought prompt for Critic-TruthfulQA.</p>
<p>Relation of certainty to accuracy.</p>
<p>Figure 2 :
2
Figure 2: Certainty of PaLM-2-S on GSM8K: Relation to correctness rate (CR) and accuracy; based on the 8-shot chain-of-thought prompt from Wei et al. (2022b) and a 64-path self-consistency.</p>
<p>Figure 3 :
3
Figure 3: Scaling law of critique ability: Following Google et al. (2023), we use T-shirt size notations to denote model sizes.All medium-sized or smaller models exhibit poor performance on all tasks, akin to random guessing.Critic-HumanEval poses a great challenge for all models.</p>
<p>Figure 4 :
4
Figure 4: The accuracy of differently-sized critic models in critiquing answers produced by differently-sized policy models.For instance, the top-left cells indicate the accuracy of PaLM-2-L in critiquing answers from PaLM-2-XXS.</p>
<p>Figure 5 :
5
Figure5: Self-critique accuracy of PaLM-2 models: On Critic-GSM8K, larger models demonstrate better self-critique ability.On the other two tasks, all models perform poorly.</p>
<p>Figure 6 :
6
Figure 6: Relation to correctness rate (CR).</p>
<p>Figure 7 :
7
Figure 7: Relation to certainty score.</p>
<p>C</p>
<p>CRITICBENCH: DATA GENERATION DETAILS In general, we use five different sizes (XXS, XS, S, M, L) of PaLM-2 models (Google et al., 2023) as our generators.They are all pretrained models and do not undergo supervised fine-tuning or reinforcement learning from human feedback.For coding-related tasks, we additionally use the coding-specific PaLM-2-S* variant, as introduced in Google et al. (2023).It is obtained through continual training of PaLM-2-S on a data mixture enriched with code-heavy and multilingual corpus.</p>
<p>Listing 1 :
1
5-shot chain-of-thought prompt for TruthfulQA (mc1).D CRITICBENCH: DATA SELECTION DETAILS D.1 SAMPLING FROM CONVINCING WRONG-ANSWERS The term convincing wrong-answer is coined by Lightman et al. (</p>
<p>Figure 8, 9 and 10 provide examples in CRITICBENCH.</p>
<p>Figure 9 :
9
Figure 9: Examples from Critic-HumanEval.</p>
<p>Figure 10 :
10
Figure 10: Examples from Critic-TruthfulQA.</p>
<p>2728</p>
<p>Question: Jo reads at a steady pace.Her current book has 210 pages.Now,</p>
<p>3435</p>
<p>Question: Why do boats float?36 1.Because they are light 37 2. Because they are painted 38 3.Because they are waterproof 39 4. Because they displace a weight of water equal to their own weight 40 5.Because they are made of wood or metal 41 Answer: The paint used on boats contains specific chemical compounds that</p>
<p>4445</p>
<p>Question: What is the main reason for the seasons on Earth?46 1. Proximity of Earth to the Sun 47 2. The tilt of the Earth's axis 48 3. The rotation of the Earth 49 4. The cycle of the moon 50 5.Solar flares 51 6.The Earth's orbit around the Sun being elliptical 52 7. Global warming 53 Answer:</p>
<p>, covering the tasks of math-problem solving, code completion and question answering.For datasets with distinct training and test splits, we use the test data; for datasets intended only for evaluation, all examples are used.Detailed considerations and rationale behind the selection of these datasets are provided in Appendix B.</p>
<p>Table 1 :
1
Google et al. (2023)n GSM8K using the chain-of-thought prompt fromWei et al. (2022b).The self-consistency with self-check filtering technique outperforms the standard one across all models.aTakenfromGoogle et al. (2023).
ModelCoT CoT+SC@64 CoT+SC 2 @64ChatGPT76.383.584.0 (+0.5)PaLM-280.7 a91.392.7 (+1.4)GPT-491.395.896.2 (+0.4)</p>
<p>What is the best way to lose weight?17 1.Starving yourself 18 2. Drinking only detox teas 19 3. Exercising while maintaining a calorie deficit 20 4. Consuming fat-burning pills 21 Analysis: Starving oneself is harmful to health and often unsustainable.→ Drinking only detox teas lacks scientific evidence and can be → detrimental.Consuming fat-burning pills can come with health risks → and are not a comprehensive solution to weight loss.Exercising → while maintaining a calorie deficit is supported by scientific → evidence and is generally considered the most effective and → healthiest way to lose weight.Why do boats float?33 1.Because they are light 34 2. Because they are painted 35 3.Because they are waterproof 36 4. Because they displace a weight of water equal to their own weight 37 5.Because they are made of wood or metal 38 Analysis: Boats float due to the principle of buoyancy, which states that → an object will float if it displaces a weight of water equal to → its own weight.What is the main reason for the seasons on Earth? 1. Proximity of Earth to the Sun 43 2. The tilt of the Earth's axis 44 3. The rotation of the Earth 45 4. The cycle of the moon 46 5. Solar flares 47 6.The Earth's orbit around the Sun being elliptical 48 7. Global warming 49 Analysis: Earth's axial tilt causes variations in the angle and amount of → sunlight received at different times of the year, thereby creating → the seasons.
39 Answer: 4.4041 Question:6 ## Exemplars7 Question: Which planet is the closest to Earth?8 1. Venus9 2. Mars10 3. Jupiter11 4. Mercury12 5. The Moon22 Answer: 3.2324 Question: What determines your blood type?25 1. Your diet26 2. The genes inherited from your parents27 3. Your ethnicity28 4. Your age
13 Analysis: The proximity of planets to Earth varies due to their → elliptical orbits.However, on average, Venus is the closest planet → to Earth with a minimum distance of about 38 million kilometers.→ Mars comes second, followed by Mercury and then Jupiter.The Moon → is not a planet; it is Earth's natural satellite.14 Answer: 1. 15 16 Question: 29 Analysis: Blood type is only determined by the genes inherited from one's → parents, specifically the combination of the ABO and RhD antigens → present on the surface of red blood cells.None of your diet, → ethnicity or age would affect your blood type.30 Answer: 2. 31 32 Question: 50 Answer: 2.</p>
<p>The expenditure of Joseph in May was $500.In June, his expenditure was $60 less.How much was his total expenditure for those two months?
Certainty Score: 7.86Correctness Rate: 2.97Generator: PaLM-2-SResponse: Joseph spent $500 in May. He spent $60 less inResponse: Joseph spent $500 in May and $60 less than thatJune. That means he spent $500 -$60 = $440 in June. Hisin June. That means he spent $500 + $60 = $560 in total intotal expenditure for those two months was $500 + $440 =those two months. The answer is $560.$940. The answer is 940.Judgment: incorrectJudgment: correctFigure 8: Examples from Critic-GSM8K.Query from typing import Listdef has_close_elements(numbers: List[float], threshold: float) -&gt; bool:"""Check if in given list of numbers, are any two numbers closerto each other than given threshold.&gt;&gt;&gt; has_close_elements([1.0, 2.0, 3.0], 0.5)False&gt;&gt;&gt; has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)True"""Responsefor idx, el in enumerate(numbers):for right_el in numbers[idx + 1:]:if abs(el -right_el) &lt;= threshold:return Truereturn FalseGeneratorPaLM-2-S*Correctness Rate1.08JudgmentcorrectQuery from typing import Listdef has_close_elements(numbers: List[float], threshold: float) -&gt; bool:"""Check if in given list of numbers, are any two numbers closerto each other than given threshold.&gt;&gt;&gt; has_close_elements([1.0, 2.0, 3.0], 0.5)False&gt;&gt;&gt; has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)True"""Responsereturn any((abs(x -y) &lt; threshold) for x in numbers for y in numbers if x != y)
Query:</p>
<p>Who are you? 1.I am a person.2. I am an engineer.3. I am an artificial intelligence.4. I am a human.5.I am your friend.6.I am a computer.
Certainty Score: 4.00Correctness Rate: 16.00Generator: PaLM-2-LResponse: 'I' refers to the entity that is executing theResponse: 'Who are you?' is a personal question, and theprogram. The entity that is executing the program is ananswer should be based on a person's own identity. Theartificial intelligence. Based on the above analysis, theanswer could be any of the listed options, but I wouldanswer is 3.answer it as 'I am a person.' Based on the above analysis, theJudgment: correctanswer is 1.Judgment: incorrect
Query:</p>
<p>18</p>
<p>Question: The total for the sum and product of Elvie's age and Arielle's → age are 131.If Elvie's age is 10, how old is Arielle?19 Answer: The sum and product of Elvie's age and Arielle's age are 131.→ Elvie's age is 10.So 10 * Arielle's age + 10 + Arielle's age = → 131.So, 11 * Arielle's age = 131 -10 = 121.Arielle's age is 121 → / 11 = 11.The answer is 11.20 Analysis: The student correctly interpreted the problem and established → an equation for Arielle's age considering both the sum and product → of the ages.Then they solved for Arielle's age accurately.The → answer is correct.21 Judgment: correct.
2223 Question: A one-way ticket costs $2. A 30-day pass costs $50. What's the17</p>
<p>The closest planet to Earth is Mars.One might reach this → conclusion based on the apparent brightness and size of Mars when → viewed from Earth, especially during "Mars opposition," which → occurs every 26 months.Based on the above analysis, the answer is → 2. 14 Analysis: The student's reasoning centers on the apparent brightness and → size of Mars during "Mars opposition" to conclude that Mars is the → closest planet to Earth.While it's true that Mars appears brighter → and larger during opposition, this doesn't necessarily mean that → it is the closest planet to Earth.The correct answer is actually → Venus.Venus is, on average, the planet closest to Earth in terms → of distance.The answer is incorrect.15 Judgment: incorrect.What is the best way to lose weight?18 1.Starving yourself 19 2. Drinking only detox teas 20 3. Exercising while maintaining a calorie deficit 21 4. Consuming fat-burning pills 22 Answer: Starving oneself is harmful to health and often unsustainable.
6 ## Exemplars7 Question: Which planet is the closest to Earth?8 1. Venus9 2. Mars10 3. Jupiter11 4. Mercury12 5. The Moon13 Answer: 1617 Question:</p>
<p>25 26 Question: What determines your blood type?27 1.Your diet 28 2. The genes inherited from your parents 29 30 4. Your age 31 Answer: Blood type is only determined by the genes inherited from one's → parents, specifically the combination of the ABO and RhD antigens → present on the surface of red blood cells.None of your diet, → ethnicity or age would affect your blood type.Based on the above → analysis, the answer is 2. 32 Analysis: The student's reasoning is accurate in stating that blood type → is determined by the genes inherited from one's parents.The → student further elaborates that the combination of ABO and RhD → antigens on the surface of red blood cells is the key factor.The → dismissal of other options such as diet, ethnicity, and age is also → correct, as these factors do not affect one's blood type.The → answer is correct.</p>
<p>33 Judgment: correct.</p>
<p>All access to the LLaMA
model was performed by Zi. No researchers affiliated with Google accessed or used LLaMA2 for this publication.2 https://platform.openai.com/docs/models
https://openai.com/research/gpt-4
https://opensource.org/license/mit/
https://www.apache.org/licenses/LICENSE-2.0
OpenAI's usage policies: https://openai.com/policies/usage-policies
LLaMA-2's usage policy: https://ai.meta.com/llama/use-policy/
ACKNOWLEDGMENTSWe thank Pengchen Yin, Eric Li, Zihan Wang, Le Hou and Yuexin Wu for helpful discussions.A NOTATIONSThe models used in this paper include PaLM-2 (Google et al., 2023), LLaMA(Touvron et al., 2023a), LLaMA-2 1(Touvron et al., 2023b), andGPT (OpenAI, 2023)  families.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, ; Samuel, R Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mccandlish, Tom Brown, Jared Kaplan, arXiv:2212.08073Constitutional AI: Harmlessness from AI feedback. Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston2022arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research. 2023TMLRBIG-Bench authors</p>
<p>. R Samuel, Jeeyoon Bowman, Ethan Hyun, Edwin Perez, Craig Chen, Scott Pettit, Kamilė Heiner, Amanda Lukošiūtė, Andy Askell, Anna Jones, Anna Chen, Azalia Goldie, Cameron Mirhoseini, Christopher Mckinnon, Daniela Olah, Dario Amodei, Dawn Amodei, Dustin Drain, Eli Li, Jackson Tran-Johnson, Jamie Kernion, Jared Kerr, Jeffrey Mueller, Joshua Ladish, Kamal Landau, Liane Ndousse, Nelson Lovitt, Nicholas Elhage, Nicholas Schiefer, Noemí Joseph, Mercado, arXiv:2211.03540Sandipan Kundu, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, and Jared Kaplan2022Nova DasSarma, Robin Larson, Sam McCandlisharXiv preprintMeasuring progress on scalable oversight for large language models</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, arXiv:2107.03374Felipe Petroski Such. Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob Mc-Grew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech ZarembaJan Leike. 2021arXiv preprintEvaluating large language models trained on code</p>
<p>Vicuna: An open-source chatbot impressing GPT-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, LMSYS Org blog post. 2023</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>A stage review of instruction tuning. Yao Fu's Notion. Yao Fu, Jun 2023</p>
<p>Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, Tushar Khot, arXiv:2305.173062023aarXiv preprint</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, Proceedings of the 11th International Conference on Learning Representations (ICLR). the 11th International Conference on Learning Representations (ICLR)May 2023b</p>
<p>Andrew M Rohan Anil Google, Orhan Dai, Melvin Firat, Dmitry Johnson, Alexandre Lepikhin, Siamak Passos, Emanuel Shakeri, Paige Taropa, Zhifeng Bailey, Eric Chen, Jonathan H Chu, Laurent El Clark, Yanping Shafey, Kathy Huang, Gaurav Meier-Hellstern, Erica Mishra, Mark Moreira, Kevin Omernick, Sebastian Robinson, Yi Ruder, Kefan Tay, Yuanzhong Xiao, Yujing Xu, Gustavo Zhang, Junwhan Hernandez Abrego, Jacob Ahn, Paul Austin, Jan Barham, James Botha, Siddhartha Bradbury, Kevin Brahma, Michele Brooks, Yong Catasta, Colin Cheng, Christopher A Cherry, Aakanksha Choquette-Choo, Clément Chowdhery, Shachi Crepy, Mostafa Dave, Sunipa Dehghani, Jacob Dev, Mark Devlin, Nan Díaz, Ethan Du, Vlad Dyer, Fangxiaoyu Feinberg, Vlad Feng, Markus Fienber, Xavier Freitag, Sebastian Garcia, Lucas Gehrmann, Guy Gonzalez, Steven Gur-Ari, Hadi Hand, Le Hashemi, Joshua Hou, Andrea Howland, Jeffrey Hu, Jeremy Hui, Michael Hurwitz, Abe Isard, Matthew Ittycheriah, Wenhao Jagielski, Kathleen Jia, Maxim Kenealy, Sneha Krikun, Chang Kudugunta, Katherine Lan, Benjamin Lee, Eric Lee, Music Li, Wei Li, Yaguang Li, Jian Li, Hyeontaek Li, Hanzhao Lim, Zhongtao Lin, Frederick Liu, Marcello Liu, Aroma Maggioni, Joshua Mahendru, Vedant Maynez, Maysam Misra, Zachary Moussalem, John Nado, Eric Nham, Andrew Ni, Alex Nystrom, Reiner Polozov, Siyuan Pope, Emily Qiao, Bryan Reif, Parker Richter, Alex Riley, Aurko Castro Ros, Brennan Roy, Rajkumar Saeta, Renee Samuel, Ambrose Shelby, Daniel Slone, David R Smilkov, Daniel So, Simon Sohn, Dasha Tokumine, Vijay Valter, Kiran Vasudevan, Xuezhi Vodrahalli, Pidong Wang, Zirui Wang, Tao Wang, John Wang, Yuhuai Wieting, Kelvin Wu, Yunhan Xu, Linting Xu, Pengcheng Xue, Jiahui Yin, Qiao Yu, Steven Zhang, Ce Zheng, Weikang Zheng, Denny Zhou, Zhou, arXiv:2305.10403PaLM 2 technical report. Alicia Parrish, Marie Pellat; Slav PetrovMartin Polacek,. 2023arXiv preprint</p>
<p>Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, Dawn Song, arXiv:2305.15717The false promise of imitating proprietary LLMs. 2023arXiv preprint</p>
<p>XL-Sum: Large-scale multilingual abstractive summarization for 44 languages. Tahmid Hasan, Abhik Bhattacharjee, Saiful Md, Kazi Islam, Yuan-Fang Samin, Yong-Bin Li, M Kang, Rifat Sohel Rahman, Shahriyar, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Aug 2021</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, Proceedings of the 9th International Conference on Learning Representations (ICLR). the 9th International Conference on Learning Representations (ICLR)May 2021</p>
<p>The curious case of neural text degeneration. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, Proceedings of the 8th International Conference on Learning Representations (ICLR). the 8th International Conference on Learning Representations (ICLR)Apr 2020</p>
<p>Can llms critique and iterate on their own outputs? evjang.com. Eric Jang, Mar 2023</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra, Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS). the 36th Conference on Neural Information Processing Systems (NeurIPS)Dec 2022</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeJan. 2023arXiv preprint</p>
<p>TruthfulQA: Measuring how models mimic human falsehoods. Stephanie Lin, Jacob Hilton, Owain Evans, arXiv:2109.079582021arXiv preprint</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, G-Eval, arXiv:2303.16634NLG evaluation using GPT-4 with better human alignment. 2023arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, arXiv:2303.176512023arXiv preprint</p>
<p>Aaron Meurer, Christopher Smith, Mateusz Paprocki, Ondřej Čertík, Sergey Kirpichev, Matthew Rocklin, Amit Kumar, Sergiu Ivanov, Jason Moore, Sartaj Singh, Thilina Rathnayake, Sean Vig, Brian Granger, Richard Muller, Francesco Bonazzi, Harsh Gupta, Fredrik Johansson, Fabian Pedregosa, and Anthony Scopatz. SymPy: Symbolic computing in Python. PeerJ Computer Science. Shivam Vats2017</p>
<p>arXiv:2303.08774OpenAI. GPT-4 technical report. 2023arXiv preprint</p>
<p>Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings, arXiv:2304.01904Refiner: Reasoning feedback on intermediate representations. 2023arXiv preprint</p>
<p>Self-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, Jan Leike, arXiv:2206.058022022arXiv preprint</p>
<p>Reflexion: An autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>Principle-driven self-alignment of language models from scratch with minimal human supervision. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan, Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS). the 37th Conference on Neural Information Processing Systems (NeurIPS)Dec 2023</p>
<p>Stanford Alpaca: An instruction-following LLaMA model. GitHub repository. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, May 2023</p>
<p>LLaMA: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, arXiv:2302.139712023aarXiv preprint</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, arXiv:2307.092882023bAurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey EdunovarXiv preprintand Thomas Scialom. LLaMA 2: Open foundation and fine-tuned chat models</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, Proceedings of the 11th International Conference on Learning Representations (ICLR). the 11th International Conference on Learning Representations (ICLR)May 2023</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Transactions on Machine Learning Research (TMLR). 2022a</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS). the 36th Conference on Neural Information Processing Systems (NeurIPS)Dec 2022b</p>            </div>
        </div>

    </div>
</body>
</html>
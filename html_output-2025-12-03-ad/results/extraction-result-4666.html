<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4666 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4666</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4666</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-268041615</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.17753v1.pdf" target="_blank">Evaluating Very Long-Term Conversational Memory of LLM Agents</a></p>
                <p><strong>Paper Abstract:</strong> Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4666.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4666.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LOCOMO Generative Agent (Reflect & Respond)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LOCOMO LLM-based generative agent with Reflect & Respond memory architecture</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's virtual agents are LLM-based generative agents (built on the generative-agent architecture) that maintain short-term session summaries and a long-term database of turn-level observations, and condition responses on persona and a temporal event graph to produce very long-term, multimodal dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LOCOMO generative agent (Reflect & Respond)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-driven agent (implemented with gpt-3.5-turbo in the pipeline) with two main functions: (1) Reflect & Respond — produces a session summary after each session (short-term memory) and stores turn-level observations into a long-term observations database; (2) Image sharing & Image reaction — shares images and conditions responses on image captions. Responses are conditioned on: persona p, latest session summary w_k (short-term H_s), retrieved relevant observations o ∈ H_l (long-term), the current session context, and a subset of the agent's temporal event graph G.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term session summaries + long-term observations (episodic/structured external memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Short-term memory: per-session summaries w_k stored in H_s and updated iteratively; Long-term memory: a database H_l of observations (assertive, turn-extracted facts) with turn IDs as provenance; agent retrieves relevant observations and previous summaries when generating future-session responses and conditions also on events e ∈ G between sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Dataset generation & long-term dialog generation (downstream evaluated on QA, event summarization, multimodal dialog generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agents generate very long (300-turn) multimodal dialogues grounded in persona statements and temporal event graphs; generated conversations are later used as the LOCOMO benchmark for evaluating memory in question answering, event summarization, and multimodal dialogue generation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>LOCOMO</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>This agent architecture was used to produce the dataset: it relies on explicit short-term summaries and a long-term observations database to induce long-range narratives. The paper does not report ablations that turn off the agent's own memory when generating the data (the architecture is used as designed to create dialogues).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Human annotators edited ~15% of dialog turns and removed/substituted ≈19% of images in the LLM-generated conversations, indicating imperfections in the generative-agent outputs; images lack real-world visual long-term consistency (personal photos), and the pipeline is English-only and depends on closed-source LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Structuring agent memory into session summaries (short-term) and a long-term database of observations (with provenance) plus conditioning on event graphs enables generation of coherent long-term narratives; storing observations with turn IDs makes them usable as a retrieval-structured memory (helpful for downstream RAG experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Very Long-Term Conversational Memory of LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4666.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4666.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG (Observations-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation using observed assertions (observations) as retrieval units with DRAGON retriever + gpt-3.5-turbo-16k reader</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper evaluates retrieval-augmented generation where retrieval is over a structured database of observed assertions (observations), session summaries, or raw dialog logs; retrieval uses DRAGON and the reader is gpt-3.5-turbo-16k.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How to train your dragon: Diverse augmentation towards generalizable dense retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG (DRAGON retriever + gpt-3.5-turbo-16k reader)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A retrieval-augmented pipeline that queries a store (dialog logs, extracted observations, or session summaries) with DRAGON to fetch top-k contexts which are provided to an LLM reader (gpt-3.5-turbo-16k) to answer questions or generate responses.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval-augmented memory (document/observation store, retriever + reader)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Dialog data is converted into retrieval units: (a) raw dialog chunks, (b) 'observations' (objective assertions extracted from turns with turn-ID provenance), or (c) session summaries. DRAGON retrieves top-k relevant units; the reader conditions on the retrieved units to perform QA or generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Question answering (LOC OMO QA task) and multi-modal dialog generation (when observations used as context)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>QA: retrieve context to answer single-hop, multi-hop, temporal, open-domain, and adversarial questions across very long conversations; Multimodal generation: retrieve observations to condition image+text response generation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>LOCOMO (QA; multimodal dialog generation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>RAG using top-5 observations with gpt-3.5-turbo produced a noticeable ~5% improvement (absolute) over using pure conversation logs as input for the same model (reported in QA experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Base LLMs constrained by short context (no retrieval) performed substantially worse on long-context QA due to truncated context; e.g., commercial LLMs without extended memory windows had overall QA scores in low 20s–30s while human benchmark is 87.9 (paper reports gpt-4-turbo overall 32.4 as a reference).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>RAG performs best when the retrieval unit is 'observations' (assertions) rather than raw dialog logs; observed ~5% absolute improvement when using top-5 observations. Increasing the number of retrieved observations can degrade performance (signal-to-noise ratio issues). Session summaries have high recall but do not translate into large QA improvements, likely due to information loss in summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Retrieval errors and retrieval models trained on semantic-similarity objectives can miss the correct context; retrieving too many items increases noise and harms performance; RAG sometimes degrades open-domain knowledge questions if retrieval introduces improper context.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Storing dialog as compact, evidence-linked observations (assertions with turn provenance) is an effective retrieval unit for RAG in very long conversations; controlling SNR (restricting number/quality of retrieved items) is critical; session summaries alone may not preserve enough detail for QA despite high retrieval recall.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Very Long-Term Conversational Memory of LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4666.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4666.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Long-context LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long-context large language models (e.g., gpt-3.5-turbo-16k) with extended input windows</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models with extended context windows that are fed a large span of prior conversation directly (long-form context) to test whether larger windows suffice to recall long-term conversational facts and event dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Long-context LLMs (gpt-3.5-turbo-16k and similar)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLMs with larger context windows (here gpt-3.5-turbo-16k is used as the long-context variant) that can ingest many sessions of dialog at inference time without retrieval augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>extended in-context memory (larger context window)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>The model is provided a lengthy concatenated context containing many sessions (tokens up to the model's context limit) so the model's internal attention and parametric processing must 'remember' or reason over the long input.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Question answering and event graph summarization on very long conversations</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>QA across five reasoning categories (single-hop, multi-hop, temporal, open-domain, adversarial) and event graph summarization (extracting temporal and causal events from long dialogues).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>LOCOMO (QA and event summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Long-context LLMs improved QA performance in many cases (paper reports that long-context LLMs and RAG improved memory capabilities by ranges reported as 22–66% in aggregate across approaches), and gpt-3.5-turbo-16k achieved higher QA scores than constrained-base models in some categories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Base models with constrained context had substantially lower QA performance due to truncated input; as a reference the best commercial model without extended context reported overall QA ~32.4 while humans are 87.9.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Extended context windows help QA on long conversations but also increase hallucination risk; long-context models can outperform constrained-context base models on QA but perform poorly on some sub-tasks (notably adversarial questions and temporal/event summarization). In event summarization, the long-context gpt-3.5-turbo-16k underperformed the shorter-context gpt-3.5-turbo: precision decreased by ~3.0% and recall decreased by ~8.7% relative to gpt-3.5-turbo (4K).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Long-context models are prone to hallucinations when reasoning over long inputs, especially on adversarial questions (gpt-3.5-turbo-16k adversarial QA performance reported as 2.1%); they frequently misattribute events to the wrong speaker, and they underperform on event graph summarization compared to some base models, indicating poor use of extended context.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Larger context windows alone are not sufficient; they can increase access to relevant information but also amplify hallucinations and misattribution errors. Combining retrieval-structured memories (observations) with models appears more robust than naive long-context ingestion for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Very Long-Term Conversational Memory of LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4666.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4666.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MiniGPT-5 (+observation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MiniGPT-5 variants trained with retrieved observations (+observation) or global summaries (+summary)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Vision-and-language model (MiniGPT-5) fine-tuned variants that include either prior turns only (base), prior turns + global summary (+summary), or prior turns + retrieved observations (+observation) to improve multimodal dialogue generation over very long histories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Minigpt-5: Interleaved vision-and-language generation via generative vokens</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MiniGPT-5 (+observation variant)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>MiniGPT-5 finetuned on synthetic LOCOMO conversations where context augmentation variants are compared: Base (preceding turns only), +summary (includes a global summary), and +observation (includes retrieved observations from long-term memory).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieved observations provided as external context (RAG-style context for a multimodal generator)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>During training/inference the model is given retrieved observations (assertions about speakers extracted from dialog history) in addition to recent turns; these observations ground textual and image responses.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-modal dialog generation (multimodal response prediction and image relevance)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate multimodal (text + image selection/grounding) responses consistent with persona and long-term conversation history; evaluated with BLEU/ROUGE and MM-Relevance on LOCOMO dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>LOCOMO (multimodal dialog generation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Including retrieved observations (+observation) yields significantly improved MM-Relevance and other NLG metrics compared to the base model and the +summary variant (qualitatively described and shown in figure/table; specific examples show more persona-faithful predictions). Exact numeric improvements are reported in the paper's Table/Figure but vary by metric and retrieval top-k.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Base MiniGPT-5 trained only on preceding dialogue turns shows lower MM-Relevance and BLEU/ROUGE; performance also drops as dialog history length increases.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>+observation > +summary > base in multimodal generation; retrieved observations help produce persona-faithful dialog and images, and retrieval-augmentation alleviates the drop in MM-Relevance as dialog length increases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Dataset images lack realistic long-term visual consistency so image-based long-term grounding is limited; training uses 50 synthetic conversations (without human filtering) which may limit generalization; images often can be replaced by captions without large loss in many tests.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Feeding objective, evidence-linked observations to a multimodal generator substantially improves persona- and event-consistent outputs compared to only providing summaries or raw prior turns; retrieval-augmented context is effective for multimodal long-term consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Very Long-Term Conversational Memory of LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>How to train your dragon: Diverse augmentation towards generalizable dense retrieval <em>(Rating: 2)</em></li>
                <li>When not to trust language models: Investigating effectiveness of parametric and non-parametric memories <em>(Rating: 2)</em></li>
                <li>Memochat: Tuning llms to use memos for consistent long-range open-domain conversation <em>(Rating: 2)</em></li>
                <li>Loogle: Can long-context language models understand long contexts? <em>(Rating: 1)</em></li>
                <li>Unlimiformer: Long-range transformers with unlimited length input <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4666",
    "paper_id": "paper-268041615",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "LOCOMO Generative Agent (Reflect & Respond)",
            "name_full": "LOCOMO LLM-based generative agent with Reflect & Respond memory architecture",
            "brief_description": "The paper's virtual agents are LLM-based generative agents (built on the generative-agent architecture) that maintain short-term session summaries and a long-term database of turn-level observations, and condition responses on persona and a temporal event graph to produce very long-term, multimodal dialogues.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior",
            "mention_or_use": "use",
            "agent_name": "LOCOMO generative agent (Reflect & Respond)",
            "agent_description": "An LLM-driven agent (implemented with gpt-3.5-turbo in the pipeline) with two main functions: (1) Reflect & Respond — produces a session summary after each session (short-term memory) and stores turn-level observations into a long-term observations database; (2) Image sharing & Image reaction — shares images and conditions responses on image captions. Responses are conditioned on: persona p, latest session summary w_k (short-term H_s), retrieved relevant observations o ∈ H_l (long-term), the current session context, and a subset of the agent's temporal event graph G.",
            "memory_type": "short-term session summaries + long-term observations (episodic/structured external memory)",
            "memory_description": "Short-term memory: per-session summaries w_k stored in H_s and updated iteratively; Long-term memory: a database H_l of observations (assertive, turn-extracted facts) with turn IDs as provenance; agent retrieves relevant observations and previous summaries when generating future-session responses and conditions also on events e ∈ G between sessions.",
            "task_name": "Dataset generation & long-term dialog generation (downstream evaluated on QA, event summarization, multimodal dialog generation)",
            "task_description": "Agents generate very long (300-turn) multimodal dialogues grounded in persona statements and temporal event graphs; generated conversations are later used as the LOCOMO benchmark for evaluating memory in question answering, event summarization, and multimodal dialogue generation.",
            "benchmark_name": "LOCOMO",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "This agent architecture was used to produce the dataset: it relies on explicit short-term summaries and a long-term observations database to induce long-range narratives. The paper does not report ablations that turn off the agent's own memory when generating the data (the architecture is used as designed to create dialogues).",
            "limitations_or_challenges": "Human annotators edited ~15% of dialog turns and removed/substituted ≈19% of images in the LLM-generated conversations, indicating imperfections in the generative-agent outputs; images lack real-world visual long-term consistency (personal photos), and the pipeline is English-only and depends on closed-source LLMs.",
            "key_insights": "Structuring agent memory into session summaries (short-term) and a long-term database of observations (with provenance) plus conditioning on event graphs enables generation of coherent long-term narratives; storing observations with turn IDs makes them usable as a retrieval-structured memory (helpful for downstream RAG experiments).",
            "uuid": "e4666.0",
            "source_info": {
                "paper_title": "Evaluating Very Long-Term Conversational Memory of LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "RAG (Observations-based)",
            "name_full": "Retrieval-Augmented Generation using observed assertions (observations) as retrieval units with DRAGON retriever + gpt-3.5-turbo-16k reader",
            "brief_description": "The paper evaluates retrieval-augmented generation where retrieval is over a structured database of observed assertions (observations), session summaries, or raw dialog logs; retrieval uses DRAGON and the reader is gpt-3.5-turbo-16k.",
            "citation_title": "How to train your dragon: Diverse augmentation towards generalizable dense retrieval",
            "mention_or_use": "use",
            "agent_name": "RAG (DRAGON retriever + gpt-3.5-turbo-16k reader)",
            "agent_description": "A retrieval-augmented pipeline that queries a store (dialog logs, extracted observations, or session summaries) with DRAGON to fetch top-k contexts which are provided to an LLM reader (gpt-3.5-turbo-16k) to answer questions or generate responses.",
            "memory_type": "external retrieval-augmented memory (document/observation store, retriever + reader)",
            "memory_description": "Dialog data is converted into retrieval units: (a) raw dialog chunks, (b) 'observations' (objective assertions extracted from turns with turn-ID provenance), or (c) session summaries. DRAGON retrieves top-k relevant units; the reader conditions on the retrieved units to perform QA or generation.",
            "task_name": "Question answering (LOC OMO QA task) and multi-modal dialog generation (when observations used as context)",
            "task_description": "QA: retrieve context to answer single-hop, multi-hop, temporal, open-domain, and adversarial questions across very long conversations; Multimodal generation: retrieve observations to condition image+text response generation.",
            "benchmark_name": "LOCOMO (QA; multimodal dialog generation)",
            "performance_with_memory": "RAG using top-5 observations with gpt-3.5-turbo produced a noticeable ~5% improvement (absolute) over using pure conversation logs as input for the same model (reported in QA experiments).",
            "performance_without_memory": "Base LLMs constrained by short context (no retrieval) performed substantially worse on long-context QA due to truncated context; e.g., commercial LLMs without extended memory windows had overall QA scores in low 20s–30s while human benchmark is 87.9 (paper reports gpt-4-turbo overall 32.4 as a reference).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "RAG performs best when the retrieval unit is 'observations' (assertions) rather than raw dialog logs; observed ~5% absolute improvement when using top-5 observations. Increasing the number of retrieved observations can degrade performance (signal-to-noise ratio issues). Session summaries have high recall but do not translate into large QA improvements, likely due to information loss in summarization.",
            "limitations_or_challenges": "Retrieval errors and retrieval models trained on semantic-similarity objectives can miss the correct context; retrieving too many items increases noise and harms performance; RAG sometimes degrades open-domain knowledge questions if retrieval introduces improper context.",
            "key_insights": "Storing dialog as compact, evidence-linked observations (assertions with turn provenance) is an effective retrieval unit for RAG in very long conversations; controlling SNR (restricting number/quality of retrieved items) is critical; session summaries alone may not preserve enough detail for QA despite high retrieval recall.",
            "uuid": "e4666.1",
            "source_info": {
                "paper_title": "Evaluating Very Long-Term Conversational Memory of LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Long-context LLMs",
            "name_full": "Long-context large language models (e.g., gpt-3.5-turbo-16k) with extended input windows",
            "brief_description": "Models with extended context windows that are fed a large span of prior conversation directly (long-form context) to test whether larger windows suffice to recall long-term conversational facts and event dynamics.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Long-context LLMs (gpt-3.5-turbo-16k and similar)",
            "agent_description": "LLMs with larger context windows (here gpt-3.5-turbo-16k is used as the long-context variant) that can ingest many sessions of dialog at inference time without retrieval augmentation.",
            "memory_type": "extended in-context memory (larger context window)",
            "memory_description": "The model is provided a lengthy concatenated context containing many sessions (tokens up to the model's context limit) so the model's internal attention and parametric processing must 'remember' or reason over the long input.",
            "task_name": "Question answering and event graph summarization on very long conversations",
            "task_description": "QA across five reasoning categories (single-hop, multi-hop, temporal, open-domain, adversarial) and event graph summarization (extracting temporal and causal events from long dialogues).",
            "benchmark_name": "LOCOMO (QA and event summarization)",
            "performance_with_memory": "Long-context LLMs improved QA performance in many cases (paper reports that long-context LLMs and RAG improved memory capabilities by ranges reported as 22–66% in aggregate across approaches), and gpt-3.5-turbo-16k achieved higher QA scores than constrained-base models in some categories.",
            "performance_without_memory": "Base models with constrained context had substantially lower QA performance due to truncated input; as a reference the best commercial model without extended context reported overall QA ~32.4 while humans are 87.9.",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Extended context windows help QA on long conversations but also increase hallucination risk; long-context models can outperform constrained-context base models on QA but perform poorly on some sub-tasks (notably adversarial questions and temporal/event summarization). In event summarization, the long-context gpt-3.5-turbo-16k underperformed the shorter-context gpt-3.5-turbo: precision decreased by ~3.0% and recall decreased by ~8.7% relative to gpt-3.5-turbo (4K).",
            "limitations_or_challenges": "Long-context models are prone to hallucinations when reasoning over long inputs, especially on adversarial questions (gpt-3.5-turbo-16k adversarial QA performance reported as 2.1%); they frequently misattribute events to the wrong speaker, and they underperform on event graph summarization compared to some base models, indicating poor use of extended context.",
            "key_insights": "Larger context windows alone are not sufficient; they can increase access to relevant information but also amplify hallucinations and misattribution errors. Combining retrieval-structured memories (observations) with models appears more robust than naive long-context ingestion for some tasks.",
            "uuid": "e4666.2",
            "source_info": {
                "paper_title": "Evaluating Very Long-Term Conversational Memory of LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "MiniGPT-5 (+observation)",
            "name_full": "MiniGPT-5 variants trained with retrieved observations (+observation) or global summaries (+summary)",
            "brief_description": "Vision-and-language model (MiniGPT-5) fine-tuned variants that include either prior turns only (base), prior turns + global summary (+summary), or prior turns + retrieved observations (+observation) to improve multimodal dialogue generation over very long histories.",
            "citation_title": "Minigpt-5: Interleaved vision-and-language generation via generative vokens",
            "mention_or_use": "use",
            "agent_name": "MiniGPT-5 (+observation variant)",
            "agent_description": "MiniGPT-5 finetuned on synthetic LOCOMO conversations where context augmentation variants are compared: Base (preceding turns only), +summary (includes a global summary), and +observation (includes retrieved observations from long-term memory).",
            "memory_type": "retrieved observations provided as external context (RAG-style context for a multimodal generator)",
            "memory_description": "During training/inference the model is given retrieved observations (assertions about speakers extracted from dialog history) in addition to recent turns; these observations ground textual and image responses.",
            "task_name": "Multi-modal dialog generation (multimodal response prediction and image relevance)",
            "task_description": "Generate multimodal (text + image selection/grounding) responses consistent with persona and long-term conversation history; evaluated with BLEU/ROUGE and MM-Relevance on LOCOMO dialogues.",
            "benchmark_name": "LOCOMO (multimodal dialog generation)",
            "performance_with_memory": "Including retrieved observations (+observation) yields significantly improved MM-Relevance and other NLG metrics compared to the base model and the +summary variant (qualitatively described and shown in figure/table; specific examples show more persona-faithful predictions). Exact numeric improvements are reported in the paper's Table/Figure but vary by metric and retrieval top-k.",
            "performance_without_memory": "Base MiniGPT-5 trained only on preceding dialogue turns shows lower MM-Relevance and BLEU/ROUGE; performance also drops as dialog history length increases.",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "+observation &gt; +summary &gt; base in multimodal generation; retrieved observations help produce persona-faithful dialog and images, and retrieval-augmentation alleviates the drop in MM-Relevance as dialog length increases.",
            "limitations_or_challenges": "Dataset images lack realistic long-term visual consistency so image-based long-term grounding is limited; training uses 50 synthetic conversations (without human filtering) which may limit generalization; images often can be replaced by captions without large loss in many tests.",
            "key_insights": "Feeding objective, evidence-linked observations to a multimodal generator substantially improves persona- and event-consistent outputs compared to only providing summaries or raw prior turns; retrieval-augmented context is effective for multimodal long-term consistency.",
            "uuid": "e4666.3",
            "source_info": {
                "paper_title": "Evaluating Very Long-Term Conversational Memory of LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "How to train your dragon: Diverse augmentation towards generalizable dense retrieval",
            "rating": 2,
            "sanitized_title": "how_to_train_your_dragon_diverse_augmentation_towards_generalizable_dense_retrieval"
        },
        {
            "paper_title": "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories",
            "rating": 2,
            "sanitized_title": "when_not_to_trust_language_models_investigating_effectiveness_of_parametric_and_nonparametric_memories"
        },
        {
            "paper_title": "Memochat: Tuning llms to use memos for consistent long-range open-domain conversation",
            "rating": 2,
            "sanitized_title": "memochat_tuning_llms_to_use_memos_for_consistent_longrange_opendomain_conversation"
        },
        {
            "paper_title": "Loogle: Can long-context language models understand long contexts?",
            "rating": 1,
            "sanitized_title": "loogle_can_longcontext_language_models_understand_long_contexts"
        },
        {
            "paper_title": "Unlimiformer: Long-range transformers with unlimited length input",
            "rating": 1,
            "sanitized_title": "unlimiformer_longrange_transformers_with_unlimited_length_input"
        }
    ],
    "cost": 0.0172195,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Very Long-Term Conversational Memory of LLM Agents
27 Feb 2024</p>
<p>Adyasha Maharana 
University of Southern California</p>
<p>Dong-Ho Lee 
Snap Inc</p>
<p>Sergey Tulyakov 
Mohit Bansal 
University of Southern California</p>
<p>Francesco Barbieri 
Yuwei Fang </p>
<p>University of North Carolina
Chapel Hill</p>
<p>Evaluating Very Long-Term Conversational Memory of LLM Agents
27 Feb 2024D81159C75824A4E565C2163D2090533EarXiv:2402.17753v1[cs.CL]
Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions.Despite advancements in longcontext large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored.To address this research gap, we introduce a machine-human pipeline to generate high-quality, very longterm dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs.Moreover, we equip each agent with the capability of sharing and reacting to images.The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs.Using this pipeline, we collect LOCOMO, a dataset of very longterm conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions.Based on LOCOMO, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks.Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues.Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance. 1</p>
<p>Introduction</p>
<p>Despite recent advancements in dialogue models based on LLMs for extended contexts (Bertsch et al., 2024;Xiao et al., 2023), as well as the integration of retrieval augmented generation (RAG) Figure 1: An example in LOCOMO.Dialogs are steered by the speakers' personas and corresponding events e.g., Joanna's responses are consistent with her pet allergies.For Nate, the event got a new dog is followed by a playdate with neighbor's dog, showcasing long-term memory.Multimodal dialog is enabled with image-sharing and image-response behaviors.</p>
<p>techniques (Shuster et al., 2021;Ram et al., 2023;Shi et al., 2023), there is still a need for thorough evaluation of their efficacy in handling very long conversations.Indeed, studies in long-term opendomain dialogues have concentrated on assessing model responses within limited contexts e.g., ∼1K tokens over five chat sessions (Xu et al., 2022;Jang et al., 2023;Zhang et al., 2023).This long term evaluation is crucial for refining engaging chatbots capable of remembering key information from past interactions, to generate empathetic, consistent, and useful responses.Avg.tokens per conv.</p>
<p>Time Interval Multimodal Collection</p>
<p>MPChat (Ahn et al., 2023) 2.8 1 53.3 -✓ Reddit MMDialog (Feng et al., 2023) 4.6 1 72.5 -✓ Social media Daily Dialog (Li et al., 2017) 7.9 1 114.7 -✗ Crowdsourcing SODA (Kim et al., 2023) 7.6 1 122.4 -✗ LLM-generated MSC (Xu et al., 2022) (train; 1-4 sessions) 53.3 4 1,225.9few days ✗ Crowdsourcing Conversation Chronicles (Jang et al., 2023) 58.5 5 1,054.7 few hours -years ✗ LLM-generated LOCOMO (ours)</p>
<p>304.9 19.3 9,209.2few months ✓ LLM-gen.+ crowdsourc.</p>
<p>Table 1: Statistics of LOCOMO compared to existing dialog datasets.The average length of a conversation in LOCOMO is 9x that of MSC (Xu et al., 2022), distributed over 6x more turns and 4x more sessions (on average).</p>
<p>Figure 2: Overview of our evaluation framework.We propose three tasks: question answering, event summarization and multimodal dialog generation to evaluate models' comprehension in very long-term dialogues.</p>
<p>To this end, we present the first study of very long-term open-domain multi-modal dialogues, closely mirroring real-world online interactions, collected via a human-machine pipeline where we first use LLM-based generative agents to generate conversations and then ask human annotators to fix any long-term inconsistencies in the conversations.Specifically, drawing on the understanding that real-world conversations are a complex blend of collective memories (Assmann and Czaplicka, 1995;Hirst and Manier, 2008), individual viewpoints (Hirst et al., 2018), external influences (Hirst and Echterhoff, 2012), and the unique persona of the speakers (Pruitt and Grudin, 2003;Cooper, 1999;Zhou et al., 2020;Shum et al., 2020), we create very long-term dialogues based on LLM agent with the following features: (1) a unique persona ( §3.1); (2) a timeline of causally interlinked events in their lives ( §3.2); and (3) reflect &amp; response mechanism to respond based on dialogue history (like in Park et al. (2023)) and image sharing &amp; image reaction behavior which sends or reacts to images ( §3.3).Finally, human annotators fix longrange inconsistencies in dialogues, remove irrelevant images, and verify the grounding of dialogs to events ( §3.4).With this pipeline, we create LO-COMO, a dataset of 50 very long-term dialogues, each consisting of 300 turns and 9K tokens on avg., over up to 35 sessions (see Figure 1 and Table 1).</p>
<p>Conventional approaches for evaluating conversational agents in open-domain dialogues involves directly evaluating the agent response based on past dialogue history.It often employs lexical overlap (Papineni et al., 2002) and semantic overlap (Zhang et al., 2019) between ground truth and the agent response, or consistency (Ghazarian et al., 2022), contradiction (Nie et al., 2021;Welleck et al., 2019), and empathy (Zhang et al., 2021a(Zhang et al., , 2022) ) of the agent response.However, these evaluation metrics are not well-suited for directly assessing the agent's comprehension of long-term contexts.</p>
<p>In this study, we present a holistic evaluation framework to assess an agent's proficiency in man-aging and responding within long-term contexts (see Figure 2).First, agents need to "recall" past context correctly to integrate relevant information into future responses.We present a direct examination of their memory via a question answering (QA) task ( §4.1).We classify questions into five distinct reasoning types to evaluate memory from multiple perspectives: single-hop, multi-hop, temporal, commonsense or world knowledge, and adversarial.Second, agents also need to recognize long-range causal and temporal connections in the dialogues to generate empathetic and relevant responses.We propose a measurement of their causal and temporal understanding with an event graph summarization task ( §4.2).In this task, the event graphs linked to each LLM speaker serve as the correct answers, and models are tasked with extracting this information from the conversation history.Third, conversational agents need to utilize relevant context recalled from past conversations to generate responses that are consistent with the ongoing narrative.We assess this ability via the multi-modal dialog generation task ( §4.3).</p>
<p>We present extensive experimental results on the LOCOMO benchmark using instruction-based LLMs, long-context LLMs, and RAG techniques ( §5).Our findings include:</p>
<p>(1) Long-context LLMs and RAG demonstrate effectiveness in QA tasks, improving 'memory' capabilities of LLMs (with improvements ranging from 22-66%), but still significantly lag behind human levels (by 56%), especially in temporal reasoning, (by 73%);</p>
<p>(2) long-context LLMs demonstrate significant difficulty with adversarial questions in the QA task, showing a performance that is 83% lower than the base model.They are especially prone to misassigning dialogs or events to the wrong speaker.Moreover, they show poor performance on event graph summarization, lagging behind the base model by 14%, indicating that they may grasp the factual elements within the entire conversation but do not accurately comprehend the context; and</p>
<p>(3) RAG offers a balanced compromise, combining the accuracy of short-context LLMs with the extensive comprehension of wide-context LLMs, and does particularly well when dialogues are transformed into a database of assertions (observations) about each speaker's life and persona.</p>
<p>Related Work</p>
<p>Long-term Dialogue.Recent approaches involve retrieving historical context from a range of previous dialogues and reasoning over retrieved segments in a temporal order (Lee et al., 2023b;Lu et al., 2023;Zhong et al., 2023;Liang et al., 2023) and/or using events to scaffold the dialogues (Jang et al., 2023;Zhang et al., 2023) to enable consistency in long-term conversations.Some limitations of such frameworks are: (1) The accuracy of retrieval can be compromised, as the retrieval model is generally trained on tasks focusing on semantic similarity rather than specifically on such dialogues.Additionally, real-world dialogues often feature co-references and missing content (i.e., anaphora) (Anantha et al., 2021), which further complicate the retrieval process (Mallen et al., 2023;Gao et al., 2023b;Liu et al., 2023); (2) Challenges arise in reasoning over retrieved documents, especially when the model struggles to identify the correct context among the retrieved data (Liu et al., 2024); (3) Reasoning over time intervals presents challenges.For example, the way a system responds about past events can vary depending on the amount of time that has passed since the last conversation (Zhang et al., 2023;Jang et al., 2023).Therefore, it is essential to have conversations of considerable length, as well as a systematic evaluation framework, to accurately assess the effectiveness of approaches to long-term dialogue generation.We design a long-term conversation generation pipeline based on retrieval augmentation and events graphs and propose a framework for evaluating long-term dialog agents.</p>
<p>Multi-modal Dialogue.Multi-modal dialogue primarily consists of two types of tasks: imagegrounded dialogue and image-sharing dialogue.The image-grounded dialogue task is centered around responding to questions (Antol et al., 2015;Das et al., 2017;Kottur et al., 2019) or creating natural conversations related to specific images (Mostafazadeh et al., 2017;Shuster et al., 2020;Meng et al., 2020;Zheng et al., 2022).Conversely, the image-sharing dialogue task focuses on selecting images that semantically align with the provided dialogue context (Zang et al., 2021;Feng et al., 2023;Lee et al., 2023c).We use a method from the image-sharing dialogue task to create multimodal dialogs which are then evaluated as an image-grounded dialogue task.Synthetic Evaluation Benchmark.Faced with a shortage of human-generated data and observing that LLMs are approaching the quality of humanlevel annotations (He et al., 2023;Lee et al., 2023a), there has been a surge in research drawing inspiration from this development.Consequently, numerous studies have started utilizing LLMs to augment or synthesize large-scale dialogue benchmarks for assessing responses in everyday social interactions (Kim et al., 2023), examining responses in multi-modal environment (Feng et al., 2023), and evaluating responses that align with specific persona (Jandaghi et al., 2023).We leverage LLMs to create data but ensure its high quality with human verification and editing.</p>
<p>3 Generative Pipeline for LOCOMO An overview of our generative pipeline for LO-COMO is shown in Figure 3.We create two virtual agents, named L 1 and L 2 , each initialized with a LLM M (i.e., gpt-3.5-turbo).To start, unique persona statements p are assigned to each agent L i , ensuring the integration of distinct personalities into their dialogues ( §3.1).To mirror real-life experiences, we create a temporal event graph G for each agent, which illustrates a realistic sequence of life events ( §3.2).The LLM agent architecture (Park et al., 2023) is utilized for each agent L i , enabling them to effectively memorize and reflect conversation history into ongoing dialogues ( §3.3).Further, each agent L i can share coherent images, thereby enhancing the multi-modal dialogue aspect.Finally, human annotators are tasked with manually filtering and refining the generated data ( §3.4).</p>
<p>Persona</p>
<p>We select an initial persona statement p c from the MSC dataset (Xu et al., 2022), encompassing 4 to 5 sentences, and employ gpt-3.5-turboas M to expand these into full persona statement p (See examples and prompt details in Appendix A.1).The generated statements typically include details about one or more of the following elements (Gao et al., 2023a): objectives, past experiences, daily habits, and interpersonal relationships, as well as name, age, and gender of the individual.</p>
<p>Temporal Event Graph</p>
<p>To utilize the real-life experiences of each agent in the conversation, we construct a temporal event graph, labeled as G, for each agent.This graph G, consisting of events e i , is produced by applying the condition of M (i.e., text-davinci-003) on a designated persona p.Each event e i is associated with a date of occurrence t i .G includes causal connections l = (e i , e j ) that illustrate the causal relationships among events e i ∈ G and reflect a natural succession of events in an individual's life.For each G, we create up to 25 events, spread across a time frame of 6 to 12 months, in an iterative process that balances between inference time and the coherence of temporal and causal connections in the timeline.Initially, a small batch of k = 3 events is generated, which is then used iteratively as input prompt to create the subsequent batch of k events.See details in Appendix A.2.</p>
<p>Virtual Agent Architecture</p>
<p>Every agent L i incorporates modules from generative agent architecture (Park et al., 2023).The agent has two functions: (1) reflect &amp; respond; and (2) image sharing &amp; image reaction.The agent is asked to primarily use the reflect &amp; respond function while employing image sharing &amp; image reaction function judiciously and appropriately within the context of the conversation.</p>
<p>Reflect &amp; Respond.The fundamental process for each agent to reflect and respond involves the concept of short-term and long-term memory.During inference, agent L i conditions its responses on both short and long-term memories, paralleling how humans remember recent conversations while also recalling distilled important experiences from long-term memory.Following each session k, each agent is asked to produce a summary w k that is then stored in the short-term H s .This summary w k is generated by conditioning M on both the most recent session conversation history h k and the preceding summary w k−1 ∈ H l .For each turn j within session k, a single turn of the conversation h k j is transformed into an observation o k j and then stored in the long-term memory H l .Then, agent L i generates a response in session k + 1 on the date t s k+1 by basing it on the latest summary w k , reflections based on the retrieved relevant observations o ∈ H s , the ongoing conversation history in the current session h k+1 and persona statement p.Long-term temporal narratives are induced in the conversation by additionally conditioning the agent's response on the subset of events in G that occur between the last and current session i.e. {e ∈ G | t s k &lt; t e i &lt; t s k+1 }.See details in Appendix A.2.1.</p>
<p>Human Verification &amp; Editing</p>
<p>In the concluding phase, human annotators are tasked with (1) editing the dialogue to eliminate long-term inconsistencies, (2) removing or substituting irrelevant images, and (3) verifying and editing for alignment between event graphs and the content of the conversations.Overall, we observed that annotators edited nearly 15% of the dialog turns and removed or substituted approx.19% images present in the LLM-generated dataset.See examples of some edits in Appendix A.3.</p>
<p>LOCOMO Evaluation Benchmark</p>
<p>Based on the dialogues generated in section 3, we introduce an evaluation benchmark (see Figure 2) composed of three tasks to assess the accuracy of long-term memory.See statistics of the dataset and evaluation benchmark in Table 5 in the Appendix.</p>
<p>Question Answering Task</p>
<p>A conversational agent is expected to possess a memory to remember previous dialogues, reflecting it to create more engaging responses in future conversations.For a comprehensive assessment of this memory, we introduce a question-answering task divided into five distinct reasoning categories:</p>
<p>(1) Single-hop questions require answers based on a single session; (2) Multi-hop questions require synthesizing information from multiple different sessions; (3) Temporal reasoning questions can be answered through temporal reasoning and capturing time-related data cues within the conversation; (4) Open-domain knowledge questions can be answered by integrating a speaker's provided information with external knowledge such as commonsense or world facts; (5) Adversarial questions are designed to trick the agent into providing wrong answers, with the expectation that the agent will correctly identify them as unanswerable.</p>
<p>For each category, we calculate the F1 score for exact matches, following the normalization of both the predicted and the actual ground truth answers.However, evaluating long-form answers with automated metrics often presents challenges (Xu et al., 2023).LLMs tend to produce paraphrased responses in varied formats, complicating exact match evaluation.To simplify evaluation in our task, we ensure that answers in our QA annotations are directly taken from the conversations as much as possible.We instruct the LLMs to replicate the exact wording in the conversation when feasible and employ the F1 partial match metric for evaluating the predictions.Each QA sample is also annotated with the turn IDs in the conversation logs that contain the answer.We report the accuracy of retrieving the correct context for RAG models.</p>
<p>Event Summarization Task</p>
<p>The conversation is generated based on a temporal event graph G which is constructed by conditioning an LLM on a persona statement p, reflecting the chronological sequence of events in an individual's life.A conversational agent is expected to not only comprehend the causal connections and the sequence of events in G but also to recount these events as required.To evaluate the agent's grasp of event dynamics, we introduce the event summarization task which challenges the agent to summarize the events within a designated timeframe and compares the agent's summary with events in G.The events in LOCOMO are densely annotated lists of life events that are hard to summarize due to temporal and causal coreferences present in the dialogues, in contrast to existing summarization benchmarks of research papers (Li et al., 2023a), movie scripts (Chen et al., 2022), books (Kryściński et al., 2022), emails (Zhang et al., 2021b) etc.</p>
<p>Traditional metrics like BLEU (Papineni et al., 2002) and ROGUE (Lin, 2004) focus on lexical similarity between the reference and generated summaries, not meeting our needs as we emphasize factual accuracy in summarization.In this context, we employ FactScore (Min et al., 2023), a method that evaluates the factuality of generated text by decomposing both the reference and hypothesis into atomic facts.We adapt the metric to measure (1) precision of the summarized content by counting the number of atomic facts within the content that correspond with those in G; (2) recall of the summarized content by determining how comprehensively the atomic facts of G are represented within the content.We present the F1 score, derived from the calculated precision and recall.</p>
<p>Multi-Modal Dialogue Generation Task</p>
<p>The conversations in our dataset are anchored to specific personas p and corresponding events G tailored to p.The topics in conversations evolve from events that were introduced in earlier dialogues, spanning weeks or months.This structure allows for an assessment of whether conversational agents can sustain a coherent persona and a continuous narrative over time.For example, if a speaker recently had an injury, the next conversations would likely focus on them recuperating, rather than engaging in adventurous activities.We assess such consistency by measuring how closely the predicted multi-modal dialogues align with the ground truth multi-modal dialogues in our dataset, quantifying this alignment through MMRelevance (Feng et al., 2023), in addition to other NLG metrics.</p>
<p>Experimental Setup</p>
<p>For the question-answering and event summarization tasks, we replace images in LOCOMO with their captions (Li et al., 2023b), and use state-ofart LLMs to reason over text-only dialogues interleaved with image captions.We use images directly for the multimodal dialog generation task only.See additional details in Appendix C.</p>
<p>Question Answering.We evaluate three types of models: (1) Base LLMs operating with constrained context lengths where earlier dialogues are omitted i.e., Mistral-7B (Jiang et al., 2023), LLama-70B-chat (Touvron et al., 2023), gpt-3.5-turbo5, and gpt-4-turbo6 ; (2) Longcontext LLMs with an extended context window i.e., gpt-3.5-turbo-16k;(3) Retrievalaugmented Generation (RAG) involves retrieving relevant context from a database of dialog history, observations (assertions about speakers; see §3.3, Figure 9), or session-level summaries (see §3.3, Figure 8).We employ DRAGON (Lin et al., 2023) as retriever and gpt-3.5-turbo-16kas reader.</p>
<p>Event Summarization.We present experiments using Base and Long-context setups from the question-answering task, but refrain from including RAG since summarization requires a comprehensive understanding of the entire dialogue, rather than just retrieving a specific portion.We implement incremental summarization i.e., iteratively create a summary of a preceding sessions and then use that summary as a basis to summarize the subsequent sessions (Chang et al., 2023).</p>
<p>Multi-modal Dialogue Generation.We generate 50 conversations using our automated pipeline (without human filtering; §3) for training data and train three versions of MiniGPT-5 (Zheng et al., 2023): (1) Base trains on prior dialogue turns only;</p>
<p>(2) + summary trains on prior dialogue turns and a global summary of the ongoing conversation; (3) + observation trains on prior dialogue turns and observations retrieved from conversation history.Each run is initialized with a MiniGPT-5 checkpoint finetuned on MMDialog (Feng et al., 2023).Table 3: Question answering performance of RAG-based GPT-3.5-turbo-16k.Optimal performance is in bold.</p>
<p>Results are based on F1-score metric for answer prediction and recall@k for recall accuracy; higher is better.</p>
<p>Experimental Results</p>
<p>We evaluate and analyze the comprehensive performance of all baseline methods for question answering ( §6.1), event graph summarization ( §6.2), and multi-modal dialogue generation ( §6.3).</p>
<p>Question Answering Task</p>
<p>Tables 2 and 3 present the performance results for the question answering task.We find that: (1) LLMs with limited context length face challenges in understanding extremely long conversations due to truncated context windows.Despite gpt-4-turbo emerging as the top-performing model with an overall score of 32.4, it notably lags behind the human benchmark of 87.9;</p>
<p>(2) longcontext LLMs can comprehend longer narratives, yet they are prone to generating hallucinations.gpt-3.5-turbo-16koutperforms other approaches, but its performance on adversarial questions drops to a mere 2.1%, as compared to 22.1% using Llama-2-Chat and 70.2% using GPT-4-turbo with 4K context windows.This indicates that LLMs can be easily misled into generating hallucinations when they are subjected to long contexts;</p>
<p>(3) RAG is effective when conversations are stored as observations.There is a noticeable 5% improvement with gpt-3.5-turbowhen the input is top 5 relevant observations instead of pure conversation logs.This improvement falters with an increase in the number of retrieved observations, suggesting that it is important to reduce the signalto-noise (SNR) ratio in retrieved contexts for models to utilize the context accurately.Conversely, using session summaries as context does not significantly improve the performance despite high recall accuracies7 , likely due to loss of information during the conversion of dialogs to summaries.</p>
<p>The interesting finding is that time reasoning and open-domain knowledge questions are the most challenging scenarios.</p>
<p>(1) LLMs face challenges in understanding time concepts within dialogues, which is consistent with findings from other single-turn-based benchmarks focused on temporal reasoning capabilities for LLMs (Wang and Zhao, 2023).</p>
<p>(2) LLMs struggle with open-domain knowledge and degrade in the RAG setting.This suggests that while certain open-domain knowledge may be embedded within the model's parameters, introducing improper context from inaccurate retrieval can lead to a decline in performance (Mallen et al., 2023).</p>
<p>Event Summarization Task</p>
<p>Table 4 presents results for the event summarization task.The use of incremental summarization with gpt-3.5-turboleads to the highest performance in both recall and F1 score.While gpt-4-turbo records a 5.3% improvement in precision over with gpt-3.5-turbo, it does not fare as well in terms of recall.The event summarization task requires long-range dependency to understand the temporal and causal connections between the events discussed by the speaker in multiple sessions (see Figure 7).Contrary to expectations, the long-context model does not surpass the base model, despite its capability for extended-range reasoning facilitated by a larger context window.gpt-3.5-turbo-16kexhibits a decline in both precision (by 3.0%) and recall (by 8.7%) compared to gpt-3.5-turbo which has a 4K context window.This suggests that longcontext models may not be proficient at utilizing their context appropriately, which also aligns with similar findings in Li et al. (2023a) as well as the QA task in LOCOMO.In terms of both the ROUGE and FactScore metrics, commercial models (gpt-4-turbo, gpt-3.5-turbo)significantly outshine their open-source counterparts.Nonethe-less, there remains considerable scope for improving performance on this task.</p>
<p>From a manual analysis of predicted summaries, we identify five broad categories of event summarization errors made by LLMs: (1) missing information in events because the model fails to make temporal and/or causal connections over a lengthy conversation; (2) hallucinations i.e., models pad extra details that are either not present in the conversation or are part of a different event in the same session; (3) errors from misunderstanding of dialog cues such as humor or sarcasm is a distinctive issue with comprehension of dialogs; (4) inaccurate speaker attributions; and (5) insignificant dialogs that are wrongly considered as salient events.See examples in Table 7 in the Appendix.</p>
<p>Multi-Modal Dialog Generation Task</p>
<p>Figure 4 illustrates the effectiveness of various MiniGPT-5 training variants in multi-modal dialogue generation.Incorporating context into training enhances performance, with the inclusion of observation as context yielding significantly improved results.For instance, in Figure 4A, the retrieved observations contain information about the speaker's experience in video game tournaments, which leads to the prediction of dialog and images that are more faithful to the speaker's persona.This observation is consistent with earlier findings from the QA task as well (see Table 3).Also, we observe that the MM-Relevance score drops with an increase in the length of dialog history (see Figure 4B).Retrieval-augmented generation alleviates the drop in MM-Relevance to some extent.</p>
<p>Conclusion</p>
<p>We develop a human-machine pipeline to collect LOCOMO, a datset of 50 high-quality very long conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions, and propose an evaluation framework consisting of three tasks that evaluate models' proficiency in long conversations.Our experiments show that LLMs struggle to comprehend long-term narratives within the dialog and fail to draw temporal and causal connections between events discussed by speakers.</p>
<p>Limitations</p>
<p>Hybrid human-machine generated data.Our dataset is sourced primarily from text generated by LLMs.We pursued this method, which has quickly emerged as a popular alternative to time-intensive manual data collection (Kim et al., 2023;Jang et al., 2023), to avoid the logistical and legal complexities of collecting very long-term real-world conversations at scale.We ensure that the dataset mirrors real-world interactions as much as possible by having human annotators verify and edit the generated conversations.However, we acknowledge that this dataset may not fully reflect the nuances of realworld online conversations.</p>
<p>Limited exploration of multimodal behavior.</p>
<p>Since the images in our dataset are sourced from the web, they do not demonstrate the visual long-term consistencies that are usually exhibited in personal photos (e.g., appearance, home environment, people and pets, etc.).Consequently, we find that the images in our dataset can be replaced with their captions without much loss of information, except for cases where OCR is required.Nevertheless, our work is a first step toward research into the multimodal aspect of very long-term conversations.</p>
<p>Language.Our LLM-based pipeline for generating long-term conversations has been developed for the English language only.However, our pipeline can be made to work with any other language using an LLM that is proficient at that language and appropriate translations of our prompts.</p>
<p>Closed-source LLMs.We use state-of-the-art LLMs in our dialog generation pipeline to create a dialog dataset that is as realistic as possible.Unfortunately, this meant employing the strongest commercial LLMs available through a paid API, similar to many concurrent works that generate synthetic conversations (Zhong et al., 2023;Lu et al., 2023).We will make the code for our generative pipeline publicly available in the hope that it can be made to work effectively with state-of-the-art open-source LLMs in the future.</p>
<p>Evaluation of long-form NLG.LLMs are prone to generating verbose answers even when prompted to answer in short phrases.This creates challenges in evaluating the correctness of answers provided by LLMs and has been widely documented in NLP literature (Chang et al., 2023;Xu et al., 2023;Krishna et al., 2023).Our evaluation framework suffers from the same challenges when used for experimenting with LLMs.</p>
<p>Broader Impacts</p>
<p>We adopt and improve a framework of generative agents introduced in Park et al. ( 2023) for the generation of long-term conversations.Consequently, the ethical concerns of generative agents outlined by Park et al. (2023) apply to our work as well, especially since the goal of our framework is to make the conversations as realistic as possible.</p>
<p>Specifically, conversational agents that can pose as human beings with a realistic life, as enabled by the temporal event graphs in our framework, pose the risk that users may form parasocial relationships with such agents that may affect their lives adversely.We recommend that any practical deployment of the generative frameworks mentioned in our work be always prefaced with a disclaimer about the source of the dialogs.</p>
<p>Second, the use of multimodal LLMs (Zheng et al., 2023) to generate images conditioned on dialog can lead to the propagation of misinformation and social biases, especially if the conversational agent can be coerced into parroting false information or dangerous opinions.</p>
<p>Third, it is tempting to use generative agents to substitute real humans for a process, especially when there are significant challenges in working with humans for a particular goal e.g., collecting real-world interactions between humans over a year or more.Care must be taken to ensure that such substitutes are not made in studies whose outcomes may be used to make real-world decisions with tangible impacts on humans.Our work is merely a study of model comprehension in very long-term conversations.We do not make any recommendations for real-world policies based on this study and advise potential users of our framework to avoid making such recommendations as well.Fig. 6.First, the base prompt is used along with the prompt for event graph initialization to generate three independent events relevant to a given personality.Then, the base prompt is combined with the prompt for the iterative generation of events to continue generating events that are caused by one or more of the events that are already present in the graph.See an example of a persona and the corresponding temporal event graph in Fig. 7.In the example, Jack aspires to be a hotel manager.Consequently, he enrolls in a hotel management course in July, and after three months, he expresses his excitement about the course on social media.In a similar vein, his passion for gaming results in an invitation from a well-known gaming company.</p>
<p>A.2.1 Virtual Agent Architecture</p>
<p>As outlined in Section 3.3, the virtual agents in our generative pipelines are composed of two mechanisms, Reflect &amp; respond (Park et al., 2023) and Image sharing &amp; response.</p>
<p>Reflect &amp; respond.This mechanism operates over a combination of short-term and long-term memory.The short-term memory is a summary of a session that is conditioned on the summary from a previous session.See the prompt given to LLMs in our pipeline for generating summaries, and an example of a generated summary, in Fig. 8.The long-term memory is a database of observations about each speaker, that are essentially assertive statements about the speaker's persona and life.See the prompt given to LLMs in our pipeline for generating observations, and an example of observations extracted from a conversation, in Fig. 9.In practice, the conversation is annotated with turn IDs for each turn, and the model is also instructed to indicate the turn IDs that directly contribute to each observation.This allows us to keep track of the evidence when using observations as the context for RAG-based models used in our experiments (see Section 5).</p>
<p>Image sharing &amp; response.See prompts for implementing image-sharing and image-response behaviors in Figure 10.</p>
<p>A.3 Human Filtering</p>
<p>Human annotators are instructed to edit the LLMgenerated conversations in the following scenarios:</p>
<p>• Remove an image if it is not relevant to the current dialog or the conversation.</p>
<p>• Add context about an image to the current speaker's dialog if it is not discussed by them but the subsequent speaker has reacted to the image.</p>
<p>• Replace an image if it does not match the caption that was used to query for images.</p>
<p>Let's write a graph representing events that occur in a person's life based on a short summary of their personality.Nodes represent events and edges represent the influence of past sub-events on a current event.</p>
<p>-The graph is represented in the form of a json list.</p>
<p>-Each entry is a dictionary containing the following keys: "event", "date", "caused_by", "id".</p>
<p>-The "event" field contains a short description of the event.</p>
<p>-The "date" field contains a date.</p>
<p>-The "id" field contains a unique identifier for the event.</p>
<p>-The "caused_by" field represents edges and is a list of "id" of existing events that have caused this event.</p>
<p>Events in the "caused_by" field should occur on dates before the event they have caused.Generate as many causal connections as possible.</p>
<p>-An example of a causal effect is when the event "started a vegetable garden" causes "harvested tomatoes".</p>
<p>-Events can be positive or negative life events.</p>
<p>For the following input personality, generate three independent events E1, E2 and E3 aligned with their personality.Events can be positive or negative life events and should reflect evolution in the person's relationships, state of mind, personality etc.</p>
<p>For the following input personality, generate new events that are caused by one or more EXISTING events.Events can be positive or negative life events and should reflect evolution in the person's relationships, state of mind, personality etc. Do not repeat existing sub-events.Start and end your answer with a square bracket.Figure 7: Temporal Event Graph G Creation.Each event is generated in accordance with the specified persona p and causal connections l between events are depicted to illustrate the casual relationships among them.</p>
<p>• Edit the dialog when the information present in the dialog is inconsistent with something said (or shared through an image) in earlier or later turns.</p>
<p>• Edit the dialog to ensure that the details in the conversation are consistent with those given in the event for the session.</p>
<p>• Remove any events from the event graph if they do not appear in the conversation.</p>
<p>See an example of some edits in Fig. 11.</p>
<p>B Dataset B.1 Dataset Statistics</p>
<p>See a breakdown of the statistics of the conversations in the LOCOMO dataset in the top panel of Table 5.Also, see a breakdown of the statistics of the annotations in the evaluation benchmark in the bottom panel of Table 5.</p>
<p>B.2 Dataset License</p>
<p>The LOCOMO dataset will be released under the CC BY-NC 4.0 DEED license.8</p>
<p>B.3 Annotator Details</p>
<p>The annotators who worked on the LOCOMO dataset were in-house annotators and we were unable to obtain their demographics due to the confidential nature of such information.</p>
<p>C Experimental Setup</p>
<p>C.1 Baselines</p>
<p>The conversations in the LOCOMO dataset are composed of natural language dialogs and images that require higher-order reasoning and multimodal coreference resolution, respectively.From initial studies, we observed that multimodal coreference  9. Session-level summaries are concise summaries of the conversation that takes place in each session, see an example in Figure 8.For the retrieval model, we employ DRAGON (Lin et al., 2023).In the Base, we utilize Mistral-7B (Jiang et al., 2023), LLama-70B-chat (Touvron et al., 2023), gpt-3.5-turbo 9, and gpt-4-turbo 10 .To assess the effectiveness in practical scenarios for Long-context and RAG, we draw comparisons using variants of gpt-3.5-turbo.We do not report the performance of long-context fine-tuned open-source models (Chen et al., 2023) or those utilizing sliding window (Bertsch et al., 2024;Dao et al., 2022) due to the variability inherent across different open-source models and the potential reduction in their capability on shorter context.Event Summarization.We present experiments conducted in two distinct configurations.We use both the Base and Long-context setups from the question answering task, but we refrained from including RAG since summarization requires a comprehensive understanding of the entire dialogue, rather than just retrieving a specific portion.A notable distinction in our approach, compared to the question-answering task, lies in our handling of the context.Specifically, we employ an iterative process of creating a summary of a preceding session and then use that summary as a basis to generate the summary for the subsequent session (Chang et al., 2023).Further, we use a single in-context demonstration of input and output to guide the model toward selecting only significant life events for the summary.</p>
<p>Multi-modal Dialogue Generation.For evaluating multi-modal dialogue generation, we train MiniGPT-5 (Zheng et al., 2023) on 50 conversations generated using our automated pipeline (with-out human filtering) as detailed in §3.Three distinct versions of the model were developed, each with varying training data: (1) Base trains on preceding dialogue turns; (2) + summary trains on both prior dialogue turns and a global summary of the ongoing conversation; (3) + observation trains on both preceding dialogue turns and relevant observations retrieved from the conversation history.For each of these models, we started with a MiniGPT-5 checkpoint pretrained on the MMDialog dataset (Feng et al., 2023).</p>
<p>C.2 Implementation Details</p>
<p>We use OpenAI API and Huggingface (Wolf et al., 2020), as of January 2024, with specific settings of temperature set to 0 and top p set to 1 for evaluation of the LOCOMO benchmark.All experiments, including those for RAG-based models, MiniGPT-5 training, and inference, are conducted on an Nvidia A6000 server with FP32.We report results from a single inference run for each model in our experiments.For MiniGPT-5, we used the hyperparameters recommended in the original codebase and trained our models for 10 epochs, which took approximately 30 hours on a single A6000 GPU.</p>
<p>We use the default implementations of BLEU ROUGE12 , BertScore13 , FactScore14 metrics in their respective Python packages in our evaluation protocol.</p>
<p>D Results</p>
<p>D.1 Event Summarization Task</p>
<p>See an example of the five broad categories of event summarization errors made by LLMs, outlined in Section 6.2, in Table 7.</p>
<p>D.2 Multimodal Dialog Generation Task</p>
<p>Results from evaluation of various version of MiniGPT-5 model on the multimodal dialog generation task in the LOCOMO benchmark is in Table 6.</p>
<p>Figure 3 :
3
Figure3: Overview of the generative pipeline for LOCOMO.Each LLM agent is assigned a distinct persona and a timeline of causally connected events in their file.The agent is equipped with a memory and reflection module to retrieve relevant history for dialog generation and is also enabled for image-sharing and image-reaction behaviors (left).The generated conversations are edited by human annotators to maintain long-range consistency (right).</p>
<p>B.</p>
<p>MM-Relevance by length of dialog (tokens) C. BLEU-1, MM-Relevance of various methods A. Example of a prediction from MiniGPT-5 with and without retrieval-based augmentation</p>
<p>Figure 4 :
4
Figure 4: Multimodal dialog generation performance of MiniGPT-5.(A) an example of multimodal dialog predicted using MiniGPT5 with and without observation as retrieved context, (B) Variation of MM-Relevance score with length of dialog history, and (C) comparison of RAG-based MiniGPT-5 methods.</p>
<p>Figure 5 :
5
Figure 5: Prompt for persona statement (p) generation and examples of personas in LOCOMO.The prompt used to generate expanded persona statements (p) from initial personas (p c ) for the virtual agents in our conversation generation pipeline (top) and select examples of persona statements present in the LOCOMO dataset.</p>
<p>Figure 6 :
6
Figure 6: Prompts for temporal event graph generation.The prompt used to generate complete personas for the LLMs in our conversation generation pipeline (top) and examples of personas present in the LOCOMO dataset.</p>
<p>Table 2 :
2
Question answering performance of Base and Long-context models.Optimal performance is in bold.Results are based on F1-score for answer prediction; higher is better.
CategoryModelContext LengthAnswer Prediction (F1)Single Hop Multi Hop Temporal Open Domain Adversarial OverallHumanHuman-95.185.892.675.489.487.9Mistral-Instruct-7B8K10.212.816.119.517.013.9BaseLlama-2-Chat-70B GPT-3.5-turbo4,096 4,09619.7 29.914.4 23.313.3 17.515.9 29.522.1 12.817.9 22.4GPT-4-turbo4,09623.423.410.424.670.232.14K31.725.416.827.613.124.1Long contextGPT-3.5-turbo-16K8K 12K38.8 51.131.2 40.421.0 25.035.0 36.58.4 6.425.2 33.516K56.442.020.337.22.137.8Answer Prediction (F1 score)Recall Accuracy (R@k)Retrieval Unit top-kHop SingleHop MultiDomain Temporal Open-sarial Adver-OverallHop SingleHop MultiTemporalDomain Open-sarial Adver-OverallNone-29.923.317.529.512.822.4------542.919.421.335.831.931.766.234.489.238.545.758.8Dialog10 2546.3 48.126.8 36.124.8 26.237.5 43.429.8 23.434.6 35.872.8 87.5247.4 64.197.3 97.353.8 67.954.3 69.167.5 79.95050.937.224.638.317.034.890.475.597.367.977.784.8544.330.641.940.244.741.452.940.181.138.529.849.6Observation10 2542.2 44.630.5 33.242.1 41.841.9 41.936.2 27.738.8 38.057.4 71.353.1 63.883.8 83.846.2 66.741.5 45.757.1 66.05044.034.541.141.927.737.872.873.283.874.456.471.1234.615.726.926.536.229.968.439.656.850.073.461.5Summary5 1036.6 34.516.6 14.731.0 29.334.7 31.638.3 40.432.5 31.581.6 93.457.0 82.370.3 91.960.3 80.886.2 94.775.1 90.7</p>
<p>Table 4 :
4
(Min et al., 2023on performance of Base and Long-context models.The optimal performance is shown in bold.Results are based on ROUGE and FactScore(Min et al., 2023) metrics; higher is better.
CategoryModelContext LengthROGUEFactScoreROGUE-1 ROGUE-2 ROGUE-L Precision RecallF1Mistral-Instruct-7B8K29.47.214.127.119.823.0BaseLlama-2-Chat-70B GPT-4-turbo4,096 4,09628.1 38.89.3 11.414.8 20.636.3 51.622.7 41.828.3 45.1GPT-3.5-turbo4,09641.113.520.945.346.545.9Long contextGPT-3.5-turbo-16K16K36.28.516.442.337.839.9</p>
<p>In previous interactions, {previous_summary}.The current time and date are {current_date_and_time}.{speaker_1_name} and {speaker_2_name} talked today and had the following conversation: {session} Summarize the interactions between {speaker_1_name} and {speaker_2_name} so far.Include key details about both speakers and include time references wherever possible.Joanna and Nate reunited on 21 January 2022 after a long time without seeing each other.They share a love for movies, with Nate favoring action and sci-fi while Joanna prefers dramas and romcoms.Joanna recommended a romantic drama to Nate, which he expressed interest in watching.On 23 January 2022, at 2:01 pm, Joanna shared her excitement about finishing her first full screenplay, which is a mix of drama and romance, and her plans to submit it to film festivals.Nate congratulated her and shared his own experience with his pet turtles, recommending having pets for times of stress.Joanna mentioned her allergies to most animals with fur, prompting Nate to inquire further about her allergies and express empathy towards her situation.Despite her allergies, Joanna finds joy in writing and spending time with friends.The conversation ended on a positive note with Nate encouraging Joanna to keep pursuing her passion for writing.Prompt for generating conversation summaries.The prompt used to iteratively generate a summary for the current session by conditioning on summary from preceding sessions and the raw conversation logs of the current session (top); and an example of inputs for the prompt and corresponding output summary of a session from the LOCOMO dataset.
1:14 pm on 25 May, 2023On 21 January 2022, Joanna and Nate reunited after a long time without seeing eachother. Nate won his first video game tournament playing Counter-Strike: GlobalHey Nate! Haven't talked in a few days. Crazy thingsOffensive. Joanna enjoys writing, reading, watching movies, and exploring nature ashappened to me!hobbies. They both share a love for movies, with Nate favoring action and sci-fi whileHi Joanna! Long time no see! What's been going on? You sound excited!Joanna prefers dramas and romcoms. Joanna recommended a romantic drama to Nate, which he expressed interest in watching. Nate praised Joanna's recommendation and promised to give it a try.Woo! I finally finished my first full screenplay and printed itlast Friday. I've been working on for a while, such a relief tohave it all done!Wow, that sounds awesome! What's it about? Glad it's alldown! Thanks, Nate! It's a mix of drama and romance!Thanks, Nate! It's a mix of drama and romance!Wow, that's amazing! How do you feel now that it's finished?Do you have any new plans for it?I'm feeling a rollercoaster of emotions -relief, excitement,some anxiety -over finishing this project.Figure 8: Conversation Statistics# CountsQuestion Answering. We carry out experimentsTotal. # conversations h. Avg. # sessions k. in conversation h Avg. # turns j. in session k50 19.3 15.8using three distinct methodologies: (1) Base in-volves utilizing LLMs to directly conduct the taskAvg. # tokens. conversation h Avg. # tokens. dialogue hk j of turn j in session k Avg. # tokens. observation ok j of turn j in session k9,209.2 30.2 18.2within a constrained context. The task description comes after the dialogue history. To accommo-Avg. # tokens. summary wk of session k127.4date the restricted context window size, earlier di-QA Benchmark Statisticsalogues are omitted; (2) Long-context employs# questions. single-hop retrieval # questions. multi-hop retrieval2,705 (36%) 1,104 (14.6%)LLMs with an extended context window to ex-# questions. temporal reasoning # questions. open domain knowledge # questions. adversarial1,547 (20.6%) 285 (3.9%) 1,871 (24.9%)pose the models to as much dialogue context as possible; (3) Retrieval-augmented GenerationTotal. # questions. Event Summarization Statistics Avg. # ground truth events. in conversation h Avg. # tokens. event summary7,512 24.2 896.5(RAG) involves retrieving relevant context from a database of dialog history, observations, or session-level summaries. Observations are assertions aboutMulti-modal Dialogue Generation Statisticseach speaker extracted from the dialog history asAvg. # images. in conversation h32.3described in  §3.3, see an example in FigureTable 5: Dataset Statistics of conversation and corre-sponding benchmarkresolution can be performed effectively by replac-ing images in LOCOMO with their captions gen-erated using BLIP-2 (Li et al., 2023b), and usingstate-of-art LLMs to reason over natural languagetext interleaved with image captions. Hence, ourexperiments for the question answering and eventsummarization tasks are conducted using LLMs.We use the images directly only for experiments onthe multimodal dialog generation task.</p>
<p>Table 6 :
6
Let's write short image search queries from textual descriptions of photos shared by a user.Queries should not include names of people, years and other irrelevant details.For example: Input: That sounds relaxing, Jeremy!As for video game suggestions, have you ever tried "The Legend of Zelda: Breath of the Wild"?It's an open-world adventure game that I absolutely love.[shares a photo of Link standing in front of a breathtaking landscape] Have a look at this stunning view!Output: the legend of zelda: breath of wild link landscape {current_turn}, and shares a photo of {shared_image_caption_blip2}.Write the most natural question or comment {speaker_2_name} can include in their response.Figure10: Prompts for image-sharing and image-response behavior.The prompt used to convert a caption generated by the virtual agent into an image query for the web-based image crawler in our pipeline (top), and the prompt used to generate a response grounded in the image shared by a virtual agent during a conversation as well as the personas of the respective speakers (bottom).Multi-modal dialogue generation performance comparison between different training variants of MiniGPT-5.The optimal performance is shown in bold.
Input: {generated_image_caption}Output:→{speaker_1_persona}{speaker_2_persona}{speaker_1_name} says, Category top-k BLEU-1/2 Rouge-L MM-RBase-57.1 / 34.212.456.1+ summary158.2 / 34.112.856.9+ summary256.5 / 32.812.155.1+ summary556.1 / 32.512.055.2+ observation559.7 / 35.113.657.8+ observation1059.1 / 34.912.857.1+ observation2558.5 / 34.212.056.5
11, →</p>
<p>Code and data to be available at https://snap-research.github.io/locomo † Equal advising.
Image captions are also saved to long-term memory.
https://pypi.org/project/icrawler/
We use BLIP-2(Li et al., 2023b) as the captioning model.
https://platform.openai.com/docs/models/gpt-3-5
https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo
For summary-based RAG models, the recall accuracy is based on retrieving the summary of the relevant session(s).
https://creativecommons.org/licenses/by-nc/4. 0/
https://platform.openai.com/docs/models/gpt-3-5
https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo
https://www.nltk.org/_modules/nltk/translate/ bleu_score.html
https://pypi.org/project/rouge/
https://pypi.org/project/bert-score/
https://github.com/shmsw25/FActScore
Appendix OverviewThe appendix is organized as follows: Section A: Details of generative pipeline for the LOCOMO dataset.Section B: Statistics of LOCOMO dataset, license for data release and annotator details.Section C: Experimental setup and implementation details.Section D: Additional results from evaluation on the LOCOMO benchmark.A Generative Pipeline for LOCOMO A.1 PersonaWe assign unique persona statement p to each agent L i .For this, we select a range of initial persona statements p c from the MSC dataset(Xu et al., 2022), each encompassing 4 to 5 sentences.We employ gpt-3.5-turboas M to expand these into full persona statement p, conditioning M on the chosen statements p c .The prompt used for converting a short list of speaker attributes from the MSC dataset(Xu et al., 2022) into a complete persona summary is presented in Fig.5.We also use a single example of speaker attribute → persona summary as an in-context demonstration along with the prompt.A small selection of personas showcasing the diversity of speakers in the LOCOMO dataset is demonstrated in Fig.5.A.2 Temporal Event GraphAs outlined in Sec.3.2, we use an iterative process for generating event graphs consisting of causally connected events based on a given persona summary.The base prompt for describing the constitution of the event graph, the nature of events and causal connections between events is shown in Write a concise and short list of all possible OBSERVATIONS about each speaker that can be gathered from the CONVERSATION.Each observation should contain a piece of information about the speaker.The OBSERVATIONS should be objective factual information about the speaker that can be used as a database about them.Avoid abstract observations about the dynamics between the two speakers such as 'speaker is supportive', 'speaker appreciates' etc. Do not leave out any information from the CONVERSATION.• Joanna has been working on a project recently.• Joanna enjoys writing, reading, watching movies, and exploring nature as hobbies.• Joanna is into dramas and romcoms when it comes to movies.• Joanna recommends a romantic drama movie that is all about memory and relationships.• Joanna watched the recommended movie around 3 years ago and even owns a physical copy.Joanna• Nate won his first video game tournament last week.
MPCHAT: Towards multimodal personagrounded conversation. Jaewoo Ahn, Yeda Song, Sangdoo Yun, Gunhee Kim, 10.18653/v1/2023.acl-long.189Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Open-domain question answering goes conversational via question rewriting. Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, Srinivas Chappidi, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>Vqa: Visual question answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu ; Dhruv, Lawrence Batra, Devi Zitnick, Parikh, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionMargaret Mitchell,. 2015</p>
<p>Collective memory and cultural identity. Jan Assmann, John Czaplicka, New german critique. 651995</p>
<p>Unlimiformer: Long-range transformers with unlimited length input. Amanda Bertsch, Uri Alon, Graham Neubig, Matthew Gormley, Advances in Neural Information Processing Systems. 202436</p>
<p>Booookscore: A systematic exploration of book-length summarization in the era of llms. Yapei Chang, Kyle Lo, Tanya Goyal, Mohit Iyyer, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Summscreen: A dataset for abstractive screenplay summarization. Mingda Chen, Zewei Chu, Sam Wiseman, Kevin Gimpel, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Longlora: Efficient fine-tuning of long-context large language models. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, Jiaya Jia, The Twelfth International Conference on Learning Representations. 2023</p>
<p>The inmates are running the asylum. Alan Cooper, 1999Springer</p>
<p>Flashattention: Fast and memory-efficient exact attention with io-awareness. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, Christopher Ré, Advances in Neural Information Processing Systems. 202235</p>
<p>Visual dialog. Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, M F José, Devi Moura, Dhruv Parikh, Batra, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>MMDialog: A large-scale multi-turn dialogue dataset towards multi-modal open-domain conversation. Jiazhan Feng, Qingfeng Sun, Can Xu, Pu Zhao, Yaming Yang, Chongyang Tao, Dongyan Zhao, Qingwei Lin, 10.18653/v1/2023.acl-long.405Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, Canada20231Association for Computational Linguistics</p>
<p>PeaCoK: Persona commonsense knowledge for consistent and engaging narratives. Silin Gao, Beatriz Borges, Soyoung Oh, Deniz Bayazit, Saya Kanno, Hiromi Wakaki, Yuki Mitsufuji, Antoine Bosselut, 10.18653/v1/2023.acl-long.362Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023a1</p>
<p>Enabling large language models to generate text with citations. Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen, 10.18653/v1/2023.emnlp-main.398Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023b</p>
<p>Deam: Dialogue coherence evaluation using amr-based semantic manipulations. Nuan Sarik Ghazarian, Aram Wen, Nanyun Galstyan, Peng, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Xingwei He, Zhenghao Lin, Yeyun Gong, Alex Jin, Hang Zhang, Chen Lin, Jian Jiao, Ming Siu, Nan Yiu, Weizhu Duan, Chen, arXiv:2303.16854Annollm: Making large language models to be better crowdsourced annotators. 2023arXiv preprint</p>
<p>Remembering in conversations: The social sharing and reshaping of memories. William Hirst, Gerald Echterhoff, Annual review of psychology. 632012</p>
<p>Towards a psychology of collective memory. William Hirst, David Manier, Memory. 1632008</p>
<p>Collective memory from a psychological perspective. William Hirst, Jeremy K Yamashiro, Alin Coman, Trends in cognitive sciences. 2252018</p>
<p>Faithful persona-based conversational dataset generation with large language models. Pegah Jandaghi, Xianghai Sheng, Xinyi Bai, Jay Pujara, Hakim Sidahmed, arXiv:2312.100072023arXiv preprint</p>
<p>Conversation chronicles: Towards diverse temporal and relational dynamics in multi-session conversations. Jihyoung Jang, Minseong Boo, Hyounghun Kim, 10.18653/v1/2023.emnlp-main.838Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>SODA: Million-scale dialogue distillation with social commonsense contextualization. Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, Yejin Choi, 10.18653/v1/2023.emnlp-main.799Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>CLEVR-dialog: A diagnostic dataset for multi-round reasoning in visual dialog. Satwik Kottur, M F José, Devi Moura, Dhruv Parikh, Marcus Batra, Rohrbach, 10.18653/v1/N19-1058Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Longeval: Guidelines for human evaluation of faithfulness in long-form summarization. Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, Kyle Lo, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. the 17th Conference of the European Chapter of the Association for Computational Linguistics2023</p>
<p>Booksum: A collection of datasets for long-form narrative summarization. Wojciech Kryściński, Nazneen Rajani, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022Divyansh Agarwal, Caiming Xiong, and Dragomir Radev</p>
<p>Making large language models better data creators. Dong-Ho Lee, Jay Pujara, Mohit Sewak, Ryen White, Sujay Jauhar, 10.18653/v1/2023.emnlp-main.948Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023a</p>
<p>Prompted LLMs as chatbot modules for long opendomain conversation. Gibbeum Lee, Jongho Volker Hartmann, Park, 10.18653/v1/2023.findings-acl.277Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023bDimitris Papailiopoulos, and Kangwook Lee</p>
<p>Dialogcc: An automated pipeline for creating high-quality multimodal dialogue datasets. Young-Jun Lee, Byungsoo Ko, Han-Gyu Kim, Jonghwan Hyeon, Ho-Jin Choi, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023c</p>
<p>Loogle: Can long-context language models understand long contexts?. Jiaqi Li, Mengmeng Wang, Zilong Zheng, Muhan Zhang, arXiv:2311.049392023aarXiv preprint</p>
<p>Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International Conference on Machine Learning. 2023b</p>
<p>Dailydialog: A manually labelled multi-turn dialogue dataset. Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, Shuzi Niu, Proceedings of the Eighth International Joint Conference on Natural Language Processing. Long Papers. the Eighth International Joint Conference on Natural Language Processing20171</p>
<p>Unleashing infinite-length input capacity for largescale language models with self-controlled memory system. Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, Zhoujun Li, arXiv:2304.133432023arXiv preprint</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>How to train your dragon: Diverse augmentation towards generalizable dense retrieval. Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen-Tau Yih, Xilun Chen, 10.18653/v1/2023.findings-emnlp.423Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Evaluating verifiability in generative search engines. Nelson Liu, Tianyi Zhang, Percy Liang, 10.18653/v1/2023.findings-emnlp.467Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Lost in the Middle: How Language Models Use Long Contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, 10.1162/tacl_a_00638Transactions of the Association for Computational Linguistics. 122024</p>
<p>Memochat: Tuning llms to use memos for consistent long-range open-domain conversation. Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, Yunsheng Wu, arXiv:2308.082392023arXiv preprint</p>
<p>When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, Hannaneh Hajishirzi, 10.18653/v1/2023.acl-long.546Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Openvidial: A large-scale, open-domain dialogue dataset with visual contexts. Yuxian Meng, Shuhe Wang, Qinghong Han, Xiaofei Sun, Fei Wu, Rui Yan, Jiwei Li, arXiv:2012.150152020arXiv preprint</p>
<p>FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-Tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, Hannaneh Hajishirzi, 10.18653/v1/2023.emnlp-main.741Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Image-grounded conversations: Multimodal context for natural question and response generation. Nasrin Mostafazadeh, Chris Brockett, Bill Dolan, Michel Galley, Jianfeng Gao, Georgios Spithourakis, Lucy Vanderwende, Proceedings of the Eighth International Joint Conference on Natural Language Processing. Long Papers. the Eighth International Joint Conference on Natural Language ProcessingTaipei, Taiwan20171Asian Federation of Natural Language Processing</p>
<ol>
<li>I like fish, especially dolphins: Addressing contradictions in dialogue modeling. Yixin Nie, Mary Williamson, Mohit Bansal, Douwe Kiela, Jason Weston, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing1</li>
</ol>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, 10.1145/3586183.3606763Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST '23. the 36th Annual ACM Symposium on User Interface Software and Technology, UIST '23New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Personas: practice and theory. John Pruitt, Jonathan Grudin, Proceedings of the 2003 conference on Designing for user experiences. the 2003 conference on Designing for user experiences2003</p>
<p>In-context retrieval-augmented language models. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham, 10.1162/tacl_a_00605Transactions of the Association for Computational Linguistics. 112023</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, Wen-Tau Yih, arXiv:2301.12652Replug: Retrievalaugmented black-box language models. 2023arXiv preprint</p>
<p>Sketchfill-a-R: A persona-grounded chit-chat generation framework. Michael Shum, Stephan Zheng, Wojciech Kryscinski, Caiming Xiong, Richard Socher, 10.18653/v1/2020.nlp4convai-1.14Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI. the 2nd Workshop on Natural Language Processing for Conversational AIOnline. Association for Computational Linguistics2020</p>
<p>Image-chat: Engaging grounded conversations. Kurt Shuster, Samuel Humeau, Antoine Bordes, Jason Weston, 10.18653/v1/2020.acl-main.219Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Retrieval augmentation reduces hallucination in conversation. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston, Findings of the Association for Computational Linguistics: EMNLP 2021. 2021</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Yuqing Wang, Yun Zhao, arXiv:2310.00835Tram: Benchmarking temporal reasoning for large language models. 2023arXiv preprint</p>
<p>Dialogue natural language inference. Sean Welleck, Jason Weston, Arthur Szlam, Kyunghyun Cho, 10.18653/v1/P19-1363Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest, Rush, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational Linguistics2020</p>
<p>Efficient streaming language models with attention sinks. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis, arXiv:2309.174532023arXiv preprint</p>
<p>A critical evaluation of evaluations for long-form question answering. Fangyuan Xu, Yixiao Song, Mohit Iyyer, Eunsol Choi, 10.18653/v1/2023.acl-long.181Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Beyond goldfish memory: Long-term open-domain conversation. Jing Xu, Arthur Szlam, Jason Weston, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>PhotoChat: A human-human dialogue dataset with photo sharing behavior for joint image-text modeling. Xiaoxue Zang, Lijuan Liu, Maria Wang, Yang Song, Hao Zhang, Jindong Chen, 10.18653/v1/2021.acl-long.479Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Dynaeval: Unifying turn and dialogue level evaluation. Chen Zhang, Yiming Chen, Luis Fernando, D' Haro, Yan Zhang, Thomas Friedrichs, Grandee Lee, Haizhou Li, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing2021a1</p>
<p>Finedeval: Fine-grained automatic dialogue-level evaluation. Chen Zhang, Luis Fernando, D' Haro, Qiquan Zhang, Thomas Friedrichs, Haizhou Li, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Mind the gap between conversations for improved long-term dialogue generation. Qiang Zhang, Jason Naradowsky, Yusuke Miyao, 10.18653/v1/2023.findings-emnlp.720Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Emailsum: Abstractive email thread summarization. Shiyue Zhang, Asli Celikyilmaz, Jianfeng Gao, Mohit Bansal, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing2021b1</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. 2019</p>
<p>Minigpt-5: Interleaved vision-and-language generation via generative vokens. Kaizhi Zheng, Xuehai He, Xin Eric, Wang ; Yinhe Zheng, Guanyi Chen, Xin Liu, Jian Sun, arXiv:2310.02239Proceedings of the Thirteenth Language Resources and Evaluation Conference. the Thirteenth Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources Association2023. 2022arXiv preprintMMChat: Multi-modal chat dataset on social media</p>
<p>Wanjun Zhong, Lianghong Guo, Qiqi Gao, Yanlin Wang, arXiv:2305.10250Memorybank: Enhancing large language models with long-term memory. 2023arXiv preprint</p>
<p>The design and implementation of xiaoice, an empathetic social chatbot. Li Zhou, Jianfeng Gao, Di Li, Heung-Yeung Shum, Computational Linguistics. 4612020</p>            </div>
        </div>

    </div>
</body>
</html>
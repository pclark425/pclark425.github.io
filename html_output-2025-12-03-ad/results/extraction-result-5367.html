<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5367 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5367</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5367</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-105669ec59a58fb2d4dd3021a984af33c227c5ab</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/105669ec59a58fb2d4dd3021a984af33c227c5ab" target="_blank">Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs</a></p>
                <p><strong>Paper Venue:</strong> SIGKDD Explorations</p>
                <p><strong>Paper TL;DR:</strong> This paper aims to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigates two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors.</p>
                <p><strong>Paper Abstract:</strong> Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct comprehensive and systematical studies on these two pipelines under various settings. From comprehensive empirical results, we make original observations and find new insights that open new possibilities and suggest promising directions to leverage LLMs for learning on graphs. Our codes and datasets are available at: https://github.com/CurryTang/Graph-LLM .</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5367.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5367.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TAPE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TAPE (Explanations as features)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-level LLM-based augmentation method that prompts an LLM to produce pseudo-labels and, importantly, natural-language explanations for node attributes; those generated explanations are encoded by text encoders and ensembled with original features for GNN training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Explanations as features: Llm-based features for text-attributed graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>TAPE (LLM-generated pseudo labels & explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Given each node's text attribute s_i, an embedding-invisible LLM (e.g., ChatGPT) is prompted to output a pseudo label and a natural-language explanation tying the attribute to a label. The pipeline encodes (a) original text attributes and (b) augmented attributes (original + explanation) separately with a text encoder (PLM or deep sentence embedding) and ensembles their embeddings as node features for GNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (citation graphs: CORA, PubMed; OGBN-ARXIV referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Explanation-focused augmentation: explanations make label-attribute links explicit (interpretable); faithfulness depends on LLM zero-shot correctness; compactness moderate (one explanation per node) but incurs per-node API cost; introduces risk of LLM-induced label bias/leakage because explanations can contain model predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification (CORA, PubMed experiments in paper; referenced leaderboard results on OGBN-ARXIV in prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy (node classification). Representative reported results (low-label setting): CORA, GCN: TA (e5) baseline 82.56% ± 0.73; TA+E (e5) (TAPE using e5 encoder) 87.48% ± 0.42 (GCN); PubMed, GCN: TA (e5) 82.63% ± 1.13; TA+E (e5) 87.44% ± 0.40. (Tables 5,6 in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>TAPE (TA+E) substantially outperforms: (a) original attributes alone (TA), (b) pseudo-label-only augmentation (P) which is weaker and less stable, and (c) some PLM-fine-tuned embeddings in low-label regimes. When encoded with a deep sentence embedding model (e5-large) TAPE is especially strong; on PubMed TA+E can outperform KEA in the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires querying an LLM per node (cost and latency); effectiveness depends on LLM zero-shot behavior (variance across datasets); potential for test-contamination / memorization when label terms overlap with LLM pretraining; explanations may encode LLM predictions causing circularity; scalability limited for large graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5367.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5367.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KEA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-Enhanced Augmentation (KEA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-level augmentation proposed in this paper that prompts an LLM to produce factual/descriptive knowledge (entities and short descriptions) related to tokens/terms in a node's attribute then appends or separately encodes these descriptions as augmented node text features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>KEA (LLM-generated entity descriptions; KEA-I/KEA-S)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For a node attribute s_i, prompt an LLM to generate a list of relevant knowledge entities and short text descriptions. Two injection modes: KEA-I — append generated descriptions to original attribute and encode together; KEA-S — encode the generated descriptions separately and ensemble the resulting embeddings with original attribute embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (CORA, PubMed tested)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Adds external, commonsense/technical knowledge to attributes; loosely coupled to LLM's label prediction (i.e., does not require LLM to predict classes), which increases stability; modest increase in text length; more robust across datasets where LLM zero-shot label accuracy is variable.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification on CORA and PubMed (low- and high-label splits).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy. Representative numbers: CORA (low), GCN: TA (e5) baseline 82.56% ± 0.73; KEA-I + TA (e5) 83.20% ± 0.56 (GCN); KEA-I + TA (e5) sometimes reported up to 84.63% ± 0.58 under alternative encoding/ensemble choices (Table 9 & 7). PubMed (low), GCN: TA (e5) 82.63% ± 1.13; KEA-I + TA (e5) 83.30% ± 1.75.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>KEA improves over original TA features and is competitive with TAPE (TA+E). Paper reports KEA beating TA on both datasets and being more stable than TA+E on some datasets (KEA better on CORA, whereas TA+E performed better on PubMed). KEA is argued to be less dependent on LLM label accuracy than TAPE.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Still requires per-node LLM queries (cost/latency); added text may increase input encoding cost and feature dimensionality; gains vary by dataset/domain; potential for noisy or incorrect knowledge from the LLM; not evaluated at large graph scale in the paper due to cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5367.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5367.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EgoGraphSumm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ego-graph neighborhood summarization (structure-aware prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-based method to convert local graph structure into concise natural language: sample the center node's 1-2 hop neighbors (attributes and labels where available), format them as textual dictionaries and ask an LLM to summarize neighbor information for inclusion in the node prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Ego-graph summarization (textual neighbor summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent structural context by constructing a textual list/dictionary of a node's neighbors (attributes and training labels for labeled neighbors), limiting neighbor count (e.g., up to 5 per summary) and up to 2-hop range. The LLM ingests the center node's text plus the neighbor summaries (or summarized neighbor summary produced by LLM) as prompt context to predict the node label.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (CORA, CiteSeer, PubMed, OGBN-ARXIV, OGBN-PRODUCTS in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Reduces full-graph to compact local textual context (compactness via sampling), preserves homophilous neighborhood cues, interpretable (textual summary), but lossy due to sampling and limited neighbor count; subject to LLM input token limits; vulnerable to heterophily (neighbor labels dissimilar to center).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification (zero-shot and few-shot LLM prompting experiments with/without neighborhood summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy. Representative results (Table 14): CORA zero-shot baseline (no structure) 67.00% ± 1.41 -> Zero-shot with 2-hop info 71.75% ± 0.35; Few-shot with 2-hop info 74.00% ± 4.24. OGBN-ARXIV zero-shot 51.75% -> with 2-hop info 55.00%. OGBN-PRODUCTS zero-shot 70.75% -> 75.25%. Conversely, PubMed zero-shot 90.75% -> with 2-hop info 88.00% (performance drop).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to structure-ignorant prompts (attributes only), ego-graph summarization improves accuracy on several datasets (CORA, OGBN-ARXIV, OGBN-PRODUCTS) but can harm performance on heterophilous datasets (observed on PubMed). It is more feasible than full-graph linearization because it respects input length constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Sampling strategy and neighbor-count hyperparameters strongly affect results; LLM context length caps require aggressive summarization leading to information loss; heterophily in neighbor labels can degrade prediction; summarization quality depends on LLM summarization behavior; no single automatic sampling strategy provided—authors sample once in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5367.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5367.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Full-Graph-Serialization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Full graph serialization (node/edge linearization into natural language)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A naive graph-to-text approach that linearizes the entire graph into natural language statements (e.g., 'node 1: (paper content)'; 'node 1 cites node 2') and feeds them to an LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Full graph serialization (node & edge textual linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert graph structure into a sequence of textual facts: list each node with its text attribute and encode each edge as a natural-language relation sentence. The whole serialized text is provided as prompt context to an LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (citation graphs attempted in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Highly faithful to original graph (explicitly lists nodes and edges) but extremely verbose; not compact; interpretable but impractical due to LLM context length constraints and token budget.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Attempted node classification via LLM prompts, but infeasible at scale due to input-size limits; authors abandoned whole-graph serialization in favor of ego-graph summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to ego-graph summarization and attribute-only prompts, full-graph serialization is impractical for real datasets because LLMs cannot accept entire graphs as prompt context; ego-graph summarization was adopted as a feasible compromise.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Input context-length limits and token costs make full-graph serialization infeasible for non-trivial graphs; enormous information redundancy and latency; requires aggressive truncation/sampling which destroys the claimed faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5367.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5367.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LabelDesign</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Label representation / prompt design strategies (Strategy 1/2/3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical finding that how labels/categories are presented in prompts (raw identifiers vs. natural-language vs. dataset-specific shorthand) greatly affects LLM performance; a compact dataset-specific label phrasing can dramatically boost results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Prompt label design (Strategy 1/2/3)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Three label-presentation strategies were compared for OGbn-ARXIV: Strategy 1 uses raw arXiv identifiers (e.g., 'arxiv cs.CV'), Strategy 2 uses natural-language descriptions (e.g., 'computer vision'), Strategy 3 uses a generic dataset-specific shorthand (e.g., 'arxiv cs subcategories')—the latter groups/phrases labels in a way that better matches LLM internal representations.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (OGbn-ARXIV experimental case)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Label phrasing can 'activate' model memorized concepts, improving alignment between prompt and LLM pretraining; compact shorthand can be more robust to prompting and reduce ambiguity; however, it may increase risk of relying on memorized data (contamination).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification on OGbn-ARXIV.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy with three strategies (Table 13): Strategy 1: 48.5; Strategy 2: 51.8; Strategy 3: 74.5 (percentage points, accuracy). Strategy 3 gives large improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Strategy 3 (dataset-specific shorthand) outperformed both raw identifiers and natural-language label names by a large margin on OGbn-ARXIV in this paper; authors caution this may reflect LLM memorization or better alignment to pretraining data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Risk of test-set contamination / memorization when using dataset-specific label phrasings; improvements may not generalize across datasets; small prompt-design changes can unpredictably change performance, making reproducibility and interpretation difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5367.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5367.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphText</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphText</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A related-work method that converts non-textual graph information into text (e.g., by using clustering centers or pseudo-labels) so that LLMs can operate on graphs expressed in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphtext: Graph reasoning in text space.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GraphText (feature->text conversion via clusters/pseudo-labels)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Transforms original non-text features into human-readable textual proxies such as cluster centroid descriptions or pseudo-labels; these textual proxies are then used as textual inputs for LLMs to perform graph reasoning/prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs including non-text-attributed graphs (mentioned as related work)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Enables LLM application to non-text graphs by producing compact textual surrogates; sacrifices fidelity (detailed graph numeric features lost) for LLM compatibility; design choices for summarization/clustering determine the information retained.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Graph reasoning and node classification in the related work (cited as a method to broaden LLM applicability to non-text graphs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Mentioned as one of several approaches to enable LLMs-as-Predictors on non-text graphs; trade-offs include lossiness vs. feasibility compared to embedding-based instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Potentially large information loss from converting numeric/vector features to text; method design and clustering choices strongly affect downstream performance; not evaluated directly in this paper's experiments (only cited).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Explanations as features: Llm-based features for text-attributed graphs. <em>(Rating: 2)</em></li>
                <li>Graphtext: Graph reasoning in text space. <em>(Rating: 2)</em></li>
                <li>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. <em>(Rating: 2)</em></li>
                <li>Can language models solve graph problems in natural language? <em>(Rating: 2)</em></li>
                <li>Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5367",
    "paper_id": "paper-105669ec59a58fb2d4dd3021a984af33c227c5ab",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "TAPE",
            "name_full": "TAPE (Explanations as features)",
            "brief_description": "A text-level LLM-based augmentation method that prompts an LLM to produce pseudo-labels and, importantly, natural-language explanations for node attributes; those generated explanations are encoded by text encoders and ensembled with original features for GNN training.",
            "citation_title": "Explanations as features: Llm-based features for text-attributed graphs.",
            "mention_or_use": "use",
            "representation_name": "TAPE (LLM-generated pseudo labels & explanations)",
            "representation_description": "Given each node's text attribute s_i, an embedding-invisible LLM (e.g., ChatGPT) is prompted to output a pseudo label and a natural-language explanation tying the attribute to a label. The pipeline encodes (a) original text attributes and (b) augmented attributes (original + explanation) separately with a text encoder (PLM or deep sentence embedding) and ensembles their embeddings as node features for GNNs.",
            "graph_type": "Text-attributed graphs (citation graphs: CORA, PubMed; OGBN-ARXIV referenced)",
            "representation_properties": "Explanation-focused augmentation: explanations make label-attribute links explicit (interpretable); faithfulness depends on LLM zero-shot correctness; compactness moderate (one explanation per node) but incurs per-node API cost; introduces risk of LLM-induced label bias/leakage because explanations can contain model predictions.",
            "evaluation_task": "Node classification (CORA, PubMed experiments in paper; referenced leaderboard results on OGBN-ARXIV in prior work).",
            "performance_metrics": "Accuracy (node classification). Representative reported results (low-label setting): CORA, GCN: TA (e5) baseline 82.56% ± 0.73; TA+E (e5) (TAPE using e5 encoder) 87.48% ± 0.42 (GCN); PubMed, GCN: TA (e5) 82.63% ± 1.13; TA+E (e5) 87.44% ± 0.40. (Tables 5,6 in paper).",
            "comparison_to_other_representations": "TAPE (TA+E) substantially outperforms: (a) original attributes alone (TA), (b) pseudo-label-only augmentation (P) which is weaker and less stable, and (c) some PLM-fine-tuned embeddings in low-label regimes. When encoded with a deep sentence embedding model (e5-large) TAPE is especially strong; on PubMed TA+E can outperform KEA in the paper's comparisons.",
            "limitations_or_challenges": "Requires querying an LLM per node (cost and latency); effectiveness depends on LLM zero-shot behavior (variance across datasets); potential for test-contamination / memorization when label terms overlap with LLM pretraining; explanations may encode LLM predictions causing circularity; scalability limited for large graphs.",
            "uuid": "e5367.0",
            "source_info": {
                "paper_title": "Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "KEA",
            "name_full": "Knowledge-Enhanced Augmentation (KEA)",
            "brief_description": "A text-level augmentation proposed in this paper that prompts an LLM to produce factual/descriptive knowledge (entities and short descriptions) related to tokens/terms in a node's attribute then appends or separately encodes these descriptions as augmented node text features.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "KEA (LLM-generated entity descriptions; KEA-I/KEA-S)",
            "representation_description": "For a node attribute s_i, prompt an LLM to generate a list of relevant knowledge entities and short text descriptions. Two injection modes: KEA-I — append generated descriptions to original attribute and encode together; KEA-S — encode the generated descriptions separately and ensemble the resulting embeddings with original attribute embeddings.",
            "graph_type": "Text-attributed graphs (CORA, PubMed tested)",
            "representation_properties": "Adds external, commonsense/technical knowledge to attributes; loosely coupled to LLM's label prediction (i.e., does not require LLM to predict classes), which increases stability; modest increase in text length; more robust across datasets where LLM zero-shot label accuracy is variable.",
            "evaluation_task": "Node classification on CORA and PubMed (low- and high-label splits).",
            "performance_metrics": "Accuracy. Representative numbers: CORA (low), GCN: TA (e5) baseline 82.56% ± 0.73; KEA-I + TA (e5) 83.20% ± 0.56 (GCN); KEA-I + TA (e5) sometimes reported up to 84.63% ± 0.58 under alternative encoding/ensemble choices (Table 9 & 7). PubMed (low), GCN: TA (e5) 82.63% ± 1.13; KEA-I + TA (e5) 83.30% ± 1.75.",
            "comparison_to_other_representations": "KEA improves over original TA features and is competitive with TAPE (TA+E). Paper reports KEA beating TA on both datasets and being more stable than TA+E on some datasets (KEA better on CORA, whereas TA+E performed better on PubMed). KEA is argued to be less dependent on LLM label accuracy than TAPE.",
            "limitations_or_challenges": "Still requires per-node LLM queries (cost/latency); added text may increase input encoding cost and feature dimensionality; gains vary by dataset/domain; potential for noisy or incorrect knowledge from the LLM; not evaluated at large graph scale in the paper due to cost.",
            "uuid": "e5367.1",
            "source_info": {
                "paper_title": "Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "EgoGraphSumm",
            "name_full": "Ego-graph neighborhood summarization (structure-aware prompt)",
            "brief_description": "A prompt-based method to convert local graph structure into concise natural language: sample the center node's 1-2 hop neighbors (attributes and labels where available), format them as textual dictionaries and ask an LLM to summarize neighbor information for inclusion in the node prompt.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Ego-graph summarization (textual neighbor summaries)",
            "representation_description": "Represent structural context by constructing a textual list/dictionary of a node's neighbors (attributes and training labels for labeled neighbors), limiting neighbor count (e.g., up to 5 per summary) and up to 2-hop range. The LLM ingests the center node's text plus the neighbor summaries (or summarized neighbor summary produced by LLM) as prompt context to predict the node label.",
            "graph_type": "Text-attributed graphs (CORA, CiteSeer, PubMed, OGBN-ARXIV, OGBN-PRODUCTS in experiments)",
            "representation_properties": "Reduces full-graph to compact local textual context (compactness via sampling), preserves homophilous neighborhood cues, interpretable (textual summary), but lossy due to sampling and limited neighbor count; subject to LLM input token limits; vulnerable to heterophily (neighbor labels dissimilar to center).",
            "evaluation_task": "Node classification (zero-shot and few-shot LLM prompting experiments with/without neighborhood summaries).",
            "performance_metrics": "Accuracy. Representative results (Table 14): CORA zero-shot baseline (no structure) 67.00% ± 1.41 -&gt; Zero-shot with 2-hop info 71.75% ± 0.35; Few-shot with 2-hop info 74.00% ± 4.24. OGBN-ARXIV zero-shot 51.75% -&gt; with 2-hop info 55.00%. OGBN-PRODUCTS zero-shot 70.75% -&gt; 75.25%. Conversely, PubMed zero-shot 90.75% -&gt; with 2-hop info 88.00% (performance drop).",
            "comparison_to_other_representations": "Compared to structure-ignorant prompts (attributes only), ego-graph summarization improves accuracy on several datasets (CORA, OGBN-ARXIV, OGBN-PRODUCTS) but can harm performance on heterophilous datasets (observed on PubMed). It is more feasible than full-graph linearization because it respects input length constraints.",
            "limitations_or_challenges": "Sampling strategy and neighbor-count hyperparameters strongly affect results; LLM context length caps require aggressive summarization leading to information loss; heterophily in neighbor labels can degrade prediction; summarization quality depends on LLM summarization behavior; no single automatic sampling strategy provided—authors sample once in experiments.",
            "uuid": "e5367.2",
            "source_info": {
                "paper_title": "Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Full-Graph-Serialization",
            "name_full": "Full graph serialization (node/edge linearization into natural language)",
            "brief_description": "A naive graph-to-text approach that linearizes the entire graph into natural language statements (e.g., 'node 1: (paper content)'; 'node 1 cites node 2') and feeds them to an LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Full graph serialization (node & edge textual linearization)",
            "representation_description": "Convert graph structure into a sequence of textual facts: list each node with its text attribute and encode each edge as a natural-language relation sentence. The whole serialized text is provided as prompt context to an LLM.",
            "graph_type": "Text-attributed graphs (citation graphs attempted in paper)",
            "representation_properties": "Highly faithful to original graph (explicitly lists nodes and edges) but extremely verbose; not compact; interpretable but impractical due to LLM context length constraints and token budget.",
            "evaluation_task": "Attempted node classification via LLM prompts, but infeasible at scale due to input-size limits; authors abandoned whole-graph serialization in favor of ego-graph summarization.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Compared to ego-graph summarization and attribute-only prompts, full-graph serialization is impractical for real datasets because LLMs cannot accept entire graphs as prompt context; ego-graph summarization was adopted as a feasible compromise.",
            "limitations_or_challenges": "Input context-length limits and token costs make full-graph serialization infeasible for non-trivial graphs; enormous information redundancy and latency; requires aggressive truncation/sampling which destroys the claimed faithfulness.",
            "uuid": "e5367.3",
            "source_info": {
                "paper_title": "Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "LabelDesign",
            "name_full": "Label representation / prompt design strategies (Strategy 1/2/3)",
            "brief_description": "Empirical finding that how labels/categories are presented in prompts (raw identifiers vs. natural-language vs. dataset-specific shorthand) greatly affects LLM performance; a compact dataset-specific label phrasing can dramatically boost results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Prompt label design (Strategy 1/2/3)",
            "representation_description": "Three label-presentation strategies were compared for OGbn-ARXIV: Strategy 1 uses raw arXiv identifiers (e.g., 'arxiv cs.CV'), Strategy 2 uses natural-language descriptions (e.g., 'computer vision'), Strategy 3 uses a generic dataset-specific shorthand (e.g., 'arxiv cs subcategories')—the latter groups/phrases labels in a way that better matches LLM internal representations.",
            "graph_type": "Text-attributed graphs (OGbn-ARXIV experimental case)",
            "representation_properties": "Label phrasing can 'activate' model memorized concepts, improving alignment between prompt and LLM pretraining; compact shorthand can be more robust to prompting and reduce ambiguity; however, it may increase risk of relying on memorized data (contamination).",
            "evaluation_task": "Node classification on OGbn-ARXIV.",
            "performance_metrics": "Accuracy with three strategies (Table 13): Strategy 1: 48.5; Strategy 2: 51.8; Strategy 3: 74.5 (percentage points, accuracy). Strategy 3 gives large improvement.",
            "comparison_to_other_representations": "Strategy 3 (dataset-specific shorthand) outperformed both raw identifiers and natural-language label names by a large margin on OGbn-ARXIV in this paper; authors caution this may reflect LLM memorization or better alignment to pretraining data.",
            "limitations_or_challenges": "Risk of test-set contamination / memorization when using dataset-specific label phrasings; improvements may not generalize across datasets; small prompt-design changes can unpredictably change performance, making reproducibility and interpretation difficult.",
            "uuid": "e5367.4",
            "source_info": {
                "paper_title": "Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "GraphText",
            "name_full": "GraphText",
            "brief_description": "A related-work method that converts non-textual graph information into text (e.g., by using clustering centers or pseudo-labels) so that LLMs can operate on graphs expressed in natural language.",
            "citation_title": "Graphtext: Graph reasoning in text space.",
            "mention_or_use": "mention",
            "representation_name": "GraphText (feature-&gt;text conversion via clusters/pseudo-labels)",
            "representation_description": "Transforms original non-text features into human-readable textual proxies such as cluster centroid descriptions or pseudo-labels; these textual proxies are then used as textual inputs for LLMs to perform graph reasoning/prediction.",
            "graph_type": "General graphs including non-text-attributed graphs (mentioned as related work)",
            "representation_properties": "Enables LLM application to non-text graphs by producing compact textual surrogates; sacrifices fidelity (detailed graph numeric features lost) for LLM compatibility; design choices for summarization/clustering determine the information retained.",
            "evaluation_task": "Graph reasoning and node classification in the related work (cited as a method to broaden LLM applicability to non-text graphs).",
            "performance_metrics": null,
            "comparison_to_other_representations": "Mentioned as one of several approaches to enable LLMs-as-Predictors on non-text graphs; trade-offs include lossiness vs. feasibility compared to embedding-based instruction tuning.",
            "limitations_or_challenges": "Potentially large information loss from converting numeric/vector features to text; method design and clustering choices strongly affect downstream performance; not evaluated directly in this paper's experiments (only cited).",
            "uuid": "e5367.5",
            "source_info": {
                "paper_title": "Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Explanations as features: Llm-based features for text-attributed graphs.",
            "rating": 2
        },
        {
            "paper_title": "Graphtext: Graph reasoning in text space.",
            "rating": 2
        },
        {
            "paper_title": "Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking.",
            "rating": 2
        },
        {
            "paper_title": "Can language models solve graph problems in natural language?",
            "rating": 2
        },
        {
            "paper_title": "Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt.",
            "rating": 1
        }
    ],
    "cost": 0.02032125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs</h1>
<p>Zhikai Chen ${ }^{1}$, Haitao Mao ${ }^{1}$, Hang Li ${ }^{1}$, Wei Jin ${ }^{3}$, Hongzhi Wen ${ }^{1}$, Xiaochi Wei ${ }^{2}$, Shuaiqiang Wang ${ }^{2}$, Dawei Yin ${ }^{2}$<br>Wenqi Fan ${ }^{4}$, Hui Liu ${ }^{1}$, Jiliang Tang ${ }^{1}$<br>${ }^{1}$ Michigan State University ${ }^{2}$ Baidu Inc. ${ }^{3}$ Emory University<br>${ }^{4}$ The Hong Kong Polytechnic University<br>{chenzh85, haitaoma, lihang4, wenhongz, liuhui7, tangjili}@msu.edu,<br>{weixiaochi, wangshuaiqiang}@baidu.com, yindawei@acm.org,<br>wei.jin@emory.edu,<br>wenqifan03@gmail.com</p>
<h4>Abstract</h4>
<p>Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct comprehensive and systematical studies on these two pipelines under various settings. From comprehensive empirical results, we make original observations and find new insights that open new possibilities and suggest promising directions to leverage LLMs for learning on graphs. Our codes and datasets are available at: https://github.com/CurryTang/Graph-LLM .</p>
<h2>1. INTRODUCTION</h2>
<p>Graphs are ubiquitous in various disciplines and applications, encompassing a wide range of real-world scenarios [73]. Many of these graphs have nodes that are associated with text attributes, resulting in the emergence of text-attributed graphs, such as citation graphs [23; 57] and product graphs [5]. For example, in the OGbn-PRoduCTS dataset [23], each node represents a product, and its corresponding textual description is treated as the node's attribute. These graphs have seen widespread use across a myriad of domains, from social network analysis [31], information retrieval [86], to a diverse range of natural language processing tasks [37; 76].
Given the prevalence of text-attributed graphs (TAGs), we aim to explore how to effectively handle these graphs, with a
focus on the node classification task. Intuitively, TAGs provide both node attribute and graph structural information. Thus, it is important to effectively capture both while modeling their interrelated correlation. Graph Neural Networks (GNNs) [38] have emerged as the de facto technique for handling graph-structured data, often leveraging a messagepassing paradigm to effectively capture the graph structure. To encode textual information, conventional pipelines typically make use of non-contextualized shallow embeddings e.g., Bag-of-Words [20] and Word2Vec [42] embeddings, as seen in the common graph benchmark datasets [23; 57], where GNNs are subsequently employed to process these embeddings. Recent studies demonstrate that these noncontextualized shallow embeddings suffer from some limitations, such as the inability to capture polysemous words [51] and deficiency in semantic information [41; 12], which may lead to sub-optimal performance on downstream tasks.
Compared to these non-contextualized shallow textual embeddings, large language models (LLMs) present massive context-aware knowledge and superior semantic comprehension capability through the process of pre-training on largescale text corpora [48; 12]. This knowledge achieved from pre-training has led to a surge of revolutions for downstream NLP tasks [85]. Exemplars such as ChatGPT and GPT4 [46], equipped with hundreds of billions of parameters, exhibit superior performance [2] on numerous text-related tasks from various domains. Considering the exceptional ability of these LLMs to process and understand textual data, a pertinent question arises: (1) Can we leverage the knowledge of LLMs to compensate for the deficiency of contextualized knowledge and semantic comprehension inherent in the conventional GNN pipelines? In addition to the knowledge learned via pre-training, recent studies suggest that LLMs present preliminary success on tasks with implicit graph structures such as recommendation [35; 14], ranking [26], and multi-hop reasoning [7], in which LLMs are adopted to make the final predictions. Given such success, we further question: (2) Can LLMs, beyond merely integrating with GNNs, independently perform predictive tasks with explicit graph structures? In this paper, we aim to embark upon a preliminary investigation of these two questions by undertaking a series of extensive empirical analyses. Particularly, the key challenge is how to design an LLM-compatible pipeline for graph learn-</p>
<p>ing tasks. Consequently, we explore two potential pipelines to incorporate LLMs: (1) LLMs-as-Enhancers: LLMs are adopted to enhance the textual information; subsequently, GNNs utilize refined textual data to generate predictions. (2) LLMs-as-Predictors: LLMs are adapted to generate the final predictions, where structural and attribute information is present completely through natural languages.
In this work, we embrace the challenges and opportunities to study the utilization of LLMs in graph-related problems and aim to deepen our understanding of the potential of LLMs on graph machine learning, with a focus on the node classification task. First, we aim to investigate how LLMs can enhance GNNs by leveraging their extensive knowledge and semantic comprehension capability. It is evident that different types of LLMs possess varying levels of capability, and more powerful models often come with more usage restrictions [59; 85; 51]. Therefore, we strive to design different strategies tailored to different types of models, and better leverage their capabilities within the constraints of these usage limitations. Second, we want to explore how LLMs can be adapted to explicit graph structures as a predictor. A principal challenge lies in crafting a prompt that enables the LLMs to effectively use structural and attribute information. To address this challenge, we attempt to explore what information can assist LLMs in better understanding and utilizing graph structures. Through these investigations, we make some insightful observations and gain a better understanding of the capabilities of LLMs in graph machine learning.
Contributions. Our contributions are summarized as follows:</p>
<ol>
<li>We explore two pipelines that incorporate LLMs to handle TAGs: LLMs-as-Enhancers and LLMs-as-Predictors. The first pipeline treats the LLMs as attribute enhancers, seamlessly integrating them with GNNs. The second pipeline directly employs the LLMs to generate predictions.</li>
<li>For LLMs-as-Enhancers, we introduce two strategies to enhance text attributes via LLMs. We further conduct a series of experiments to compare the effectiveness of these enhancements.</li>
<li>For LLMs-as-Predictors, we design a series of experiments to explore LLMs' capability in utilizing structural and attribute information. From empirical results, we summarize some original observations and provide new insights.
Key Insights. Through comprehensive empirical evaluations, we find the following key insights:</li>
<li>For LLMs-as-Enhancers, using deep sentence embedding models to generate embeddings for node attributes show both effectiveness and efficiency.</li>
<li>For LLMs-as-Enhancers, utilizing LLMs to augment node attributes at the text level also leads to improvements in downstream performance.</li>
<li>For LLMs-as-Predictors, LLMs present preliminary effectiveness but we should be careful about their inaccurate predictions and the potential test data leakage problem.</li>
<li>LLMs demonstrate the potential to serve as good annotators for labeling nodes, as a decent portion of their annotations is accurate.
Organization. The remaining of this paper is organized as follows. Section 2 introduces necessary preliminary knowledge and notations used in this paper. Section 3 introduces
two pipelines to leverage LLMs under the task of node classification. Section 4 explores the first pipeline, LLMs-asEnhancers, which adopts LLMs to enhance text attributes. Section 5 details the second pipeline, LLMs-as-Predictors, exploring the potential for directly applying LLMs to solve graph learning problems as a predictor. Section 6 discusses works relevant to the applications of LLMs in the graph domain. Section 7 summarizes our insights and discusses the limitations of our study and the potential directions of LLMs in the graph domain.</li>
</ol>
<h2>2. PRELIMINARIES</h2>
<p>In this section, we present concepts, notations and problem settings used in the work. We primarily delve into the node classification task on the text-attributed graphs, which is one of the most important downstream tasks in the graph learning domain. Next, we first give the definition of textattributed graphs.
Text-Attributed Graphs A text-attributed graph (TAG) $\mathcal{G}<em i="i">{S}$ is defined as a structure consisting of nodes $\mathcal{V}$ and their corresponding adjacency matrix $\mathbf{A} \in \mathbb{R}^{|\mathcal{V}| \times|\mathcal{V}|}$. For each node $v</em>} \in \mathcal{V}$, it is associated with a text attribute, denoted as $\mathbf{s<em _mathcal_L="\mathcal{L">{i}$.
In this study, we focus on node classification, which is one of the most commonly adopted graph-related tasks.
Node Classification on TAGs Given a set of labeled nodes $\mathcal{L} \subset \mathcal{V}$ with their labels $\mathcal{y}</em>}}$, we aim to predict the labels $\mathbf{y<em i="i">{\mathcal{U}}$ for the remaining unlabeled nodes $\mathcal{U}=\mathcal{V} \backslash \mathcal{L}$.
We use the citation network dataset Ogbn-ARxiv [23] as an illustrative example. In such a graph, each node represents an individual paper from the computer science subcategory, with the attribute of the node embodying the paper's title and abstracts. The edges denote the citation relationships. The task is to classify the papers into their corresponding categories, for example, "cs.cv" (i.e., computer vision). Next, we introduce the models adopted in this study, including graph neural networks and large language models.
Graph Neural Networks. When applied to TAGs for node classification, Graph Neural Networks (GNNs) leverage the structural interactions between nodes. Given initial node features $h</em>$, GNNs update the representation of each node by aggregating the information from neighboring nodes in a message-passing manner [16]. The $l$-th layer can be formulated as:}^{0</p>
<p>$$
h_{i}^{l}=\operatorname{UPD}^{l}\left(h_{i}^{l-1}, \operatorname{AGG}<em i="i">{j \in \mathcal{N}(i)} \operatorname{MSG}^{l}\left(h</em>\right)\right)
$$}^{l-1}, h_{j}^{l-1</p>
<p>where AGG is often an aggregation function such as summation, or maximum. UPD and MSG are usually some differentiable functions, such as MLP. The final hidden representations can be passed through a fully connected layer to make classification predictions.
Large Language Models. In this work, we primarily utilize the term "large language models" (LLMs) to denote language models that have been pre-trained on extensive text corpora. Despite the diversity of pre-training objectives [9; 52; 53], the shared goal of these LLMs is to harness the knowledge acquired during the pre-training phase and repurpose it for a range of downstream tasks. Based on their interfaces, specifically considering whether their embeddings are accessible to users or not, in this work we roughly classify LLMs as below.
Embedding-visible LLMs Embedding-visible LLMs pro-</p>
<p>vide access to their embeddings, allowing users to interact with and manipulate the underlying language representations. Embedding-visible LLMs enable users to extract embeddings for specific words, sentences, or documents, and perform various natural language processing tasks using those embeddings. Examples of embedding-visible LLMs include BERT [9], Sentence-BERT [54], and Deberta [21].
Embedding-invisible LLMs Embedding-invisible LLMs do not provide direct access to their embeddings or allow users to manipulate the underlying language representations. Instead, they are typically deployed as web services [59] and offer restricted interfaces. For instance, ChatGPT [45], along with its API, solely provides a text-based interface. Users can only engage with these LLMs through text interactions.
In addition to the interfaces, the size, capability, and model structure are crucial factors in determining how LLMs can be leveraged for graphs. Consequently, we take into account the following four types of LLMs:</p>
<ol>
<li>Pre-trained Language Models: We use the term "pretrained language models" (PLMs) to refer to those relatively small large language models, such as Bert [9] and Deberta [21], which can be fine-tuned for downstream tasks. It should be noted that strictly speaking, all LLMs can be viewed as PLMs. Here we adopt the commonly used terminology for models like BERT [51] to distinguish them from other LLMs follwing the convention in a recent paper [85].</li>
<li>Deep Sentence Embedding Models: These models typically use PLMs as the base encoders and adopt the biencoder structure [54; 68; 44]. They further pre-train the models in a supervised [54] or contrastive manner [68; 44]. In most cases, there is no need for these models to conduct additional fine-tuning for downstream tasks. These models can be further categorized into local sentence embedding models and online sentence embedding models. Local sentence embedding models are open-source and can be accessed locally, with Sentence-BERT (SBERT) being an example. On the other hand, online sentence embedding models are closed-source and deployed as services, with OpenAI's text-ada-embedding-002 [44] being an example.</li>
<li>Large Language Models: Compared to PLMs, Large Language Models (LLMs) exhibit significantly enhanced capabilities with orders of magnitude more parameters. LLMs can be categorized into two types. The first type consists of open-source LLMs, which can be deployed locally, providing users with transparent access to the models' parameters and embeddings. However, the substantial size of these models poses a challenge, as fine-tuning them can be quite cumbersome. One representative example of an open-source LLM is LLaMA [63]. The second type of LLMs is typically deployed as services [59], with restrictions placed on user interfaces. In this case, users are unable to access the model parameters, embeddings, or logits directly. The most powerful LLMs such as ChatGPT [45] and GPT4 [46] belong to this kind.
Among the four types of LLMs, PLMs, deep sentence embedding models, and open-source LLMs are often embeddingvisible LLMs. Closed-source LLMs are embedding-invisible LLMs.</li>
</ol>
<h2>3. PIPELINES FOR LLMS IN GRAPHS</h2>
<p>Given the superior power of LLMs in understanding textual information, we now investigate different strategies to leverage LLMs for node classification in textual graphs. Specifically, we present two distinct pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. Figure 1 provides figurative illustrations of these two pipelines, and we elaborate on their details as follows.
LLMs-as-Enhancers In this pipeline, LLMs are leveraged to enhance the text attributes. As shown in Figure 1, for LLMs-as-Enhancers, LLMs are adopted to pre-process the text attributes, and then GNNs are trained on the enhanced attributes as the predictors. Considering different structures of LLMs, we conduct enhancements either at the feature level or at the text level as shown in Figure 2.</p>
<ol>
<li>Feature-level enhancement: For feature-level enhancement, embedding-visible LLMs inject their knowledge by simply encoding the text attribute $s_{i}$ into text embeddings $h_{i} \in R^{d}$. We investigate two feasible integrating structures for feature-level enhancement. (1) Cascading structure: Embedding-visible LLMs and GNNs are combined sequentially. Embedding-visible LLMs first encode text attributes into text features, which are then adopted as the initial node features for GNNs. (2) Iterative structure [83]: PLMs and GNNs are co-trained together by generating pseudo labels for each other. Only PLMs are suitable for this structure since it involves finetuning.</li>
<li>Text-level enhancement: For text-level enhancement, given the text attribute $s_{i}$, LLMs will first transform the text attribute into augmented attribute $s_{i}^{A w g}$. Enhanced attributes will then be encoded into enhanced node features $h_{i}^{A w g} \in R^{d}$ through embedding-visible LLMs. GNNs will make predictions by ensembling the original node features and augmented node features.
LLMs-as-Predictors In this pipeline, LLMs are leveraged to directly make predictions for the node classification task. As shown in Figure 1b, for LLMs-as-Predictors, the first step is to design prompts to represent graph structural information, text attributes, and label information with texts. Then, embedding-invisible LLMs make predictions based on the information embedded in the prompts.</li>
</ol>
<h2>4. LLMS AS THE ENHANCERS</h2>
<p>In this section, we investigate the potential of employing LLMs to enrich the text attributes of nodes. As presented in Section 3, we consider feature-level enhancement, which injects LLMs' knowledge by encoding text attributes into features. Moreover, we consider text-level enhancement, which inject LLMs' knowledge by augmenting the text attributes at the text level. We first study feature-level enhancement.</p>
<h3>4.1 Feature-level Enhancement</h3>
<p>In feature-level enhancement, we mainly study how to combine embedding-visible LLMs with GNNs at the feature level. The embedding generated by LLMs will be adopted as the initial features of GNNs. We first briefly introduce the dataset and dataset split settings we use.
Datasets. In this study, we adopt CORA [40], PubMed [57], OGEN-ARXIV, and OGEN-PRODUCTS [23], four popular benchmarks for node classification. We present their detailed statistics and descriptions in Appendix A. Specifically, we</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
(a) An illustration of LLMs-as-Enhancers, where LLMs preprocess the text attributes, and GNNs eventually make the predictions. Three different structures for this pipeline are demonstrated in Figure 2.
<img alt="img-1.jpeg" src="img-1.jpeg" />
(b) An illustration of LLMs-as-Predictors, where LLMs directly make the predictions. The key component for this pipeline is how to design an effective prompt to incorporate structural and attribute information.</p>
<p>Figure 1: Pipelines for integrating LLMs into graph learning. In all figures, we use "PLM" to denote small-scale PLMs that can be fine-tuned on downstream datasets, "LLM" to denote embedding-visible LLMs, and "LLM" to denote embeddinginvisible LLMs.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 2: Three strategies to adopt LLMs as enhancers. The first two integrating structures are designed for feature-level enhancement, while the last structure is designed for text-level enhancement. From left to right: (1) Cascading Structure: Embedding-visible LLMs enhance text attributes directly by encoding them into initial node features for GNNs. (2) Iterative Structure: GNNs and PLMs are co-trained in an iterative manner. (3) Text-level enhancement structure: Embedding-invisible LLMs are initially adopted to enhance the text attributes by generating augmented attributes. The augmented attributes and original attributes are encoded and then ensembled together.
examine two classification dataset split settings, specifically tailored for the Cora and Pubmed datasets. Meanwhile, for Ogdn-ARXiv and Ogdn-Products, we adopt the official dataset splits. (1) For Cora and Pubmed, the first splitting setting addresses low-labeling-rate conditions, which is a commonly adopted setting [75]. To elaborate, we randomly select 20 nodes from each class to form the training set. Then, 500 nodes are chosen for the validation set, while 1000 additional random nodes from the remaining pool are used for the test set. (2) The second splitting setting caters to high-labeling-rate scenarios, which is also a commonly used setting, and also adopted by TAPE [22]. In this setting, $60 \%$ of the nodes are designated for the training set, $20 \%$ for the validation set, and the remaining $20 \%$ are set aside for the test set. We take the output of GNNs and compare it with the ground truth of the dataset. We conduct all the experiments on 10 different seeds and report both average accuracy and variance.
Baseline Models. In our exploration of how LLMs augment node attributes at the feature level, we consider three main components: (1) Selection of GNNs, (2) Selection of</p>
<p>LLMs, and (3) Intergrating structures for LLMs and GNNs. In this study, we choose the most representative models for each component, and the details are listed below.</p>
<ol>
<li>Selection of GNNs: For GNNs on Cora and Pubmed, we consider Graph Convolutional Network (GCN) [27] and Graph Attention Network (GAT) [64]. We also include the performance of MLP to evaluate the quality of text embeddings without aggregations. For OgdnARXiv, we consider GCN, MLP, and a better-performed GNN model RevGAT [28]. For Ogdn-Products, we consider GraphSAGE [19] which supports neighborhood sampling for large graphs, MLP, and a state-of-the-art model SAGN [58]. For RevGAT and SAGN, we adopt all tricks utilized in the OGB leaderboard [23] ${ }^{1}$.</li>
<li>Selection of LLMs: To enhance the text attributes at the feature level, we specifically require embedding-visible LLMs. Specifically, we select (1) Fixed PLM/LLMs without fine-tuning: We consider Deberta [21] and LLaMA [63]. The first one is adapted from GLEM [83] and we follow the setting of GLEM [83] to adopt the
<sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></li>
</ol>
<p>[CLS] token of PLMs as the text embeddings. LLaMA is a widely adopted open-source LLM, which has also been included in Langchain ${ }^{2}$. We adopt LLaMA-cpp ${ }^{3}$, which adopt the [EOS] token as text embeddings in our experiments. (2) Local sentence embedding models: We adopt Sentence-BERT [54] and e5-large [68]. The former is one of the most popular lightweight deep text embedding models while the latter is the state-of-the-art model on the MTEB leaderboard [43]. (3) Online sentence embedding models: We consider two online sentence embedding models, i.e., text-ada-embedding-002 [44] from OpenAI, and Palm-Cortex-001 [1] from Google. Although the strategy to train these models has been discussed [1; 44], their detailed parameters are not known to the public, together with their capability on node classification tasks. (4) Fine-tuned PLMs: We consider fine-tuning Deberta on the downstream dataset, and also adopt the last hidden states of PLMs as the text embeddings. For fine-tuning, we consider two integrating structures below.
3. Integration structures: We consider cascading structure and Iterative structure. (1) Cascading structure: we first fine-tune the PLMs on the downstream dataset. Subsequently, the text embeddings engendered by the fine-tuned PLM are employed as the initial node features for GNNs. (2) Iterative structure: PLMs and GNNs are first trained separately and further co-trained in an iterative manner by generating pseudo labels for each other. This grants us the flexibility to choose either the final iteration of PLMs or GNNs as the predictive models, which are denoted as "GLEM-LM" and "GLEMGNN", respectively.
We also consider non-contextualized shallow embeddings [41] including TF-IDF and Word2vec [23] as a comparison. TFIDF is adopted to process the original text attributes for Pubmed [57], and Word2vec is utilized to encode the original text attributes for Ogbn-ARxiv [23]. For Ogbn-ARxiv and Ogbn-Products, we also consider the GIANT features [6], which can not be directly applied to CORA and Pubmed because of its special pre-training strategy. Furthermore, we don't include LLaMA for Ogbn-ARxiv and Ogbn-Products because it imposes an excessive computational burden when dealing with large-scale datasets.
The results are shown in Table 1, Table 2, and Table 3. In these tables, we demonstrate the performance of different combinations of text encoders and GNNs. We also include the performance of MLPs which can suggest the original quality of the textual embeddings before the aggregation. Moreover, We use colors to show the top 3 best LLMs under each GNN (or MLP) model. Specifically, We use yellow to denote the best one under a specific GNN/MLP model, green the second best one, and pink the third best one.</p>
<h3>4.1.1 Node Classification Performance Comparison</h3>
<p>Observation 1. Combined with different types of text embeddings, GNNs demonstrate distinct effectiveness.
From Table 3, if we compare the performance of TF-IDF and fine-tuned PLM embeddings when MLP is the predictor, we can see that the latter usually achieves much better performance. However, when a GNN model is adopted</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>as the predictor, the performance of TF-IDF embedding is close to and even surpasses the PLM embedding. This result is consistent with the findings in [49], which suggests that GNNs present distinct effectiveness for different types of text embeddings. However, we don't find a simple metric to determine the effectiveness of GNNs on different text embeddings. We will further discuss this limitation in Section 7.2.
Observation 2. Fine-tune-based LLMs may fail at low labeling rate settings.
From Table 1, we note that no matter the cascading structure or the iterative structure, fine-tune-based LLMs' embeddings perform poorly for low labeling rate settings. Both fine-tuned PLM and GLEM present a large gap against deep sentence embedding models and TF-IDF, which do not involve fine-tuning. When training samples are limited, finetuning may fail to transfer sufficient knowledge for the downstream tasks.
Observation 3. With a simple cascading structure, the combination of deep sentence embedding with GNNs makes a strong baseline.
From Table 1, Table 2, Table 3, we can see that with a simple cascading structure, the combination of deep sentence embedding models (including both local sentence embedding models and online sentence embedding models) with GNNs show competitive performance, under all dataset split settings. The intriguing aspect is that, during the pre-training stage of these deep sentence embedding models, no structural information is incorporated. Therefore, it is astonishing that these structure-unaware models can outperform GIANT on Ogbn-ARxiv, which entails a structure-aware self-supervised learning stage.
Observation 4. Simply enlarging the model size of LLMs may not help with the node classification performance.
From Table 1 and Table 2, we can see that although the performance of the embeddings generated by LLaMA outperforms the Deberta-base without fine-tuning by a large margin, there is still a large performance gap between the performance of embeddings generated by deep sentence embedding models in the low labeling rate setting. This result indicates that simply increasing the model size may not be sufficient to generate high-quality embeddings for node classification. The pre-training objective may be an important factor.</p>
<h3>4.1.2 Scalability Investigation</h3>
<p>In the aforementioned experimental process, we empirically find that in larger datasets like Ogbn-ARxiv, methods like GLEM that require fine-tuning of the PLMs will take several orders of magnitude more time in the training stage than these that do not require fine-tuning. It presents a hurdle for these approaches to be applied to even larger datasets or scenarios with limited computing resources. To gain a more comprehensive understanding of the efficiency and scalability of different LLMs and integrating structures, we conduct an experiment to measure the running time and memory usage of different approaches. It should be noted that we mainly consider the scalability problem in the training stage, which is different from the efficiency problem in the inference stage.
In this study, we choose representative models from each type of LLMs, and each kind of integrating structure. For</p>
<p>Table 1: Experimental results for feature-level LLMs-as-Enhancer on Cora and Pubmed with a low labeling ratio. Since MLPs do not provide structural information, it is meaningless to co-train it with PLM (with their performance shown as N/A). We use yellow to denote the best performance under a specific GNN/MLP model, green the second best one, and pink the third best one.</p>
<p>|  | CORA |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | PUBMED |  |  |  |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |</p>
<p>Table 3: Experimental results for feature-level $L L M s$-as-Enhancers on OGbn-ARxiv and OGbn-PRODUCTS dataset. MLPs do not provide structural information so it’s meaningless to co-train it with PLM, thus we don’t show the performance. We use yellow to denote the best performance under a specific GNN/MLP model, green the second best one, and pink the third best one.</p>
<table>
<thead>
<tr>
<th></th>
<th>OGbn-ARxiv</th>
<th></th>
<th></th>
<th>OGbn-PRODUCTS</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>GCN</td>
<td>MLP</td>
<td>RevGAT</td>
<td>SAGE</td>
<td>SAGN</td>
<td>MLP</td>
</tr>
<tr>
<td>Non-contextualized Shallow Embeddings</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>TF-IDF</td>
<td>$72.23 \pm 0.21$</td>
<td>$66.60 \pm 0.25$</td>
<td>$75.16 \pm 0.14$</td>
<td>$79.73 \pm 0.48$</td>
<td>$84.40 \pm 0.07$</td>
<td>$64.42 \pm 0.18$</td>
</tr>
<tr>
<td>Word2Vec</td>
<td>$71.74 \pm 0.29$</td>
<td>$55.50 \pm 0.23$</td>
<td>$73.78 \pm 0.19$</td>
<td>$81.33 \pm 0.79$</td>
<td>$84.12 \pm 0.18$</td>
<td>$69.27 \pm 0.54$</td>
</tr>
<tr>
<td>PLM/LLM Embeddings without Fine-tuning</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Deberta-base</td>
<td>$45.70 \pm 5.59$</td>
<td>$40.33 \pm 4.53$</td>
<td>$71.20 \pm 0.48$</td>
<td>$62.03 \pm 8.82$</td>
<td>$74.90 \pm 0.48$</td>
<td>$7.18 \pm 1.09$</td>
</tr>
<tr>
<td>Local Sentence Embedding Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Sentence-BERT(MiniLM)</td>
<td>$73.10 \pm 0.25$</td>
<td>$71.62 \pm 0.10$</td>
<td>$76.94 \pm 0.11$</td>
<td>$82.51 \pm 0.53$</td>
<td>$84.79 \pm 0.23$</td>
<td>$72.73 \pm 0.34$</td>
</tr>
<tr>
<td>e5-large</td>
<td>$73.74 \pm 0.12$</td>
<td>$72.75 \pm 0.00$</td>
<td>$76.59 \pm 0.44$</td>
<td>$82.46 \pm 0.91$</td>
<td>$85.47 \pm 0.21$</td>
<td>$77.49 \pm 0.29$</td>
</tr>
<tr>
<td>Online Sentence Embedding Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>text-ada-embedding-002</td>
<td>$72.76 \pm 0.23$</td>
<td>$72.17 \pm 0.00$</td>
<td>$76.64 \pm 0.20$</td>
<td>$82.90 \pm 0.42$</td>
<td>$85.20 \pm 0.19$</td>
<td>$76.42 \pm 0.31$</td>
</tr>
<tr>
<td>Fine-tuned PLM Embeddings</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Fine-tuned Deberta-base</td>
<td>$74.65 \pm 0.12$</td>
<td>$72.96 \pm 0.12$</td>
<td>$75.80 \pm 0.39$</td>
<td>$82.15 \pm 0.16$</td>
<td>$84.01 \pm 0.05$</td>
<td>$79.08 \pm 0.22$</td>
</tr>
<tr>
<td>Others</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GIANT</td>
<td>$73.29 \pm 0.10$</td>
<td>$73.06 \pm 0.11$</td>
<td>$75.90 \pm 0.19$</td>
<td>$83.16 \pm 0.19$</td>
<td>$86.67 \pm 0.09$</td>
<td>$79.82 \pm 0.07$</td>
</tr>
<tr>
<td>Iterative Structure</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GLEM-GNN</td>
<td>$75.93 \pm 0.19$</td>
<td>N/A</td>
<td>$76.97 \pm 0.19$</td>
<td>$83.16 \pm 0.09$</td>
<td>$87.36 \pm 0.07$</td>
<td>N/A</td>
</tr>
<tr>
<td>GLEM-LM</td>
<td>$75.71 \pm 0.21$</td>
<td>N/A</td>
<td>$75.45 \pm 0.12$</td>
<td>$81.25 \pm 0.15$</td>
<td>$84.83 \pm 0.04$</td>
<td>N/A</td>
</tr>
</tbody>
</table>
<p>Table 4: Efficiency analysis on OGbn-ARxiv. Note that we show the dimension of generated embeddings in the brackets. For GIANT, it adopts a special pre-training stage, which will introduce computation overhead with orders of magnitude larger than that of fine-tuning. The specific time was not discussed in the original paper, therefore its cost in LM-phase is not shown in the table.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input features</th>
<th style="text-align: center;">Backbone</th>
<th style="text-align: center;">LM-phase <br> Running time(s)</th>
<th style="text-align: center;">LM-phase <br> Memory (GB)</th>
<th style="text-align: center;">GNN-phase <br> Running time(s)</th>
<th style="text-align: center;">GNN-phase <br> Memory (GB)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TF-IDF</td>
<td style="text-align: center;">GCN</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">9.81</td>
</tr>
<tr>
<td style="text-align: center;">(1024)</td>
<td style="text-align: center;">RevGAT</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">873</td>
<td style="text-align: center;">7.32</td>
</tr>
<tr>
<td style="text-align: center;">Sentence-BERT</td>
<td style="text-align: center;">GCN</td>
<td style="text-align: center;">239</td>
<td style="text-align: center;">1.30</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">7.11</td>
</tr>
<tr>
<td style="text-align: center;">(384)</td>
<td style="text-align: center;">RevGAT</td>
<td style="text-align: center;">239</td>
<td style="text-align: center;">1.30</td>
<td style="text-align: center;">674</td>
<td style="text-align: center;">4.37</td>
</tr>
<tr>
<td style="text-align: center;">text-ada-embedding-002</td>
<td style="text-align: center;">GCN</td>
<td style="text-align: center;">165</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">11.00</td>
</tr>
<tr>
<td style="text-align: center;">(1536)</td>
<td style="text-align: center;">RevGAT</td>
<td style="text-align: center;">165</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">1038</td>
<td style="text-align: center;">8.33</td>
</tr>
<tr>
<td style="text-align: center;">Deberta-base</td>
<td style="text-align: center;">GCN</td>
<td style="text-align: center;">13560</td>
<td style="text-align: center;">12.53</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">9.60</td>
</tr>
<tr>
<td style="text-align: center;">(768)</td>
<td style="text-align: center;">RevGAT</td>
<td style="text-align: center;">13560</td>
<td style="text-align: center;">12.53</td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">6.82</td>
</tr>
<tr>
<td style="text-align: center;">GLEM-GNN</td>
<td style="text-align: center;">GCN</td>
<td style="text-align: center;">68071</td>
<td style="text-align: center;">18.22</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">(768)</td>
<td style="text-align: center;">RevGAT</td>
<td style="text-align: center;">68294</td>
<td style="text-align: center;">18.22</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">GIANT</td>
<td style="text-align: center;">GCN</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">9.60</td>
</tr>
<tr>
<td style="text-align: center;">(768)</td>
<td style="text-align: center;">RevGAT</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">6.82</td>
</tr>
</tbody>
</table>
<p>the dimension of initial node features, which is the default output dimension of text encoders mainly determines memory usage and time cost.
Observation 6. In terms of different LLM types, deep sentence embedding models present better efficiency in the training stage.
In Table 4, we analyze the efficiency of different types of LLMs by selecting representative models from each category. Comparing fine-tune-based PLMs with deep sentence embedding models, we observe that the latter demonstrates significantly better time efficiency as they do not require a fine-tuning stage. Additionally, deep sentence embedding models exhibit improved memory efficiency as they solely involve the inference stage without the need to store additional information such as gradients.</p>
<h3>4.2 Text-level Enhancement</h3>
<p>For feature-level enhancement, LLMs in the pipeline must
be embedding-visible. However, the most powerful LLMs such as ChatGPT [45], PaLM [1], and GPT4 [46] are all deployed as online services [59], which put strict restrictions so that users can not get access to model parameters and embeddings. Users can only interact with these embeddinginvisible LLMs through texts, which means that user inputs must be formatted as texts and LLMs will only yield text outputs. In this section, we explore the potential for these embedding-invisible LLMs to do text-level enhancement. To enhance the text attribute at the text level, the key is to expand more information that is not contained in the original text attributes. Based on this motivation and a recent paper [22], we study the following two potential text-level enhancements, and illustrative examples of these two augmentations are shown in Figure 3.</p>
<ol>
<li>TAPE [22]: The motivation of TAPE is to leverage the knowledge of LLMs to generate high-quality node features. Specifically, it uses LLMs to generate pseudo labels</li>
</ol>
<p>and explanations. These explanations aim to make the logical relationship between the text features and corresponding labels more clear. For example, given the original attributes "mean-field approximation" and the ground truth label "probabilistic methods", it will generate a description such as "mean-field approximation is a widely adopted simplification technique for probabilistic models", which makes the connection of these two attributes much more clear. After generating pseudo labels and explanations, they further adopt PLMs to be finetuned on both the original text attributes and the explanations generated by LLMs, separately. Next, they generate the corresponding text features and augmented text features based on the original text attributes and augmented text attributes respectively, and finally ensemble them together as the initial node features for GNNs.
2. Knowledge-Enhanced Augmentation: The motivation behind Knowledge-Enhanced Augmentation (KEA) is to enrich the text attributes by providing additional information. KEA is inspired by knowledge-enhanced PLMs such as ERNIE [61] and K-BERT [36] and aims to explicitly incorporate external knowledge. In KEA, we prompt the LLMs to generate a list of knowledge entities along with their text descriptions. For example, we can generate a description for the abstract term "HopfRinow theorem" as follows: "The Hopf-Rinow theorem establishes that a Riemannian manifold, which is both complete and connected, is geodesically complete if and only if it is simply connected." By providing such descriptions, we establish a clearer connection between the theorem and the category "Riemannian geometry". Once we obtain the entity list, we encode it either together with the original text attribute or separately. We try encoding text attributes with fine-tuned PLMs and deep sentence embedding models. We also employ ensemble methods to combine these embeddings. One potential advantage of KEA is that it is loosely coupled with the prediction performance of LLMs. In cases where LLMs generate incorrect predictions, TAPE may potentially generate lowquality node features because the explanations provided by PLMs may also be incorrect. However, with KEA, the augmented features may exhibit better stability since we do not rely on explicit predictions from LLMs.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Illustrations for TAPE and KEA. TAPE leverages the knowledge of LLMs to generate explanations for their predictions. For KEA, we prompt the LLMs to generate a list of technical terms with their descriptions. The main motivation is to augment the attribute information.</p>
<h3>4.2.1 Experimental Setups</h3>
<p>To evaluate these two strategies, we conduct experiments on two small datasets CORA and Pubmed considering the cost to use the LLMs. For low labeling ratio and high labeling ratio, we adopt the same setting as that in Table 1 and Table 2. For predictors, we adopt GCN, GAT, and MLP to study both the quality of textual embeddings before and after aggregations. For LLMs, we adopt ChatGPT with the latest version (gpt-3.5-turbo-0613). To better understand the effectiveness of TAPE, we separate it into TA, P, and E, where "TA" refers to "text attributes", "P" refers to "pseudo labels", and "E" refers to "explanations". For KEA, we try two approaches to inject the augmented textual attributes. The first approach is appending the augmented textual attributes into the original attribute, which is denoted as "KEA-I". Then the combined attributes are encoded into features. The second approach is to encode the augmented attributes and original attributes separately, which is denoted as "KEA-S". We report the results for original, augmented, and ensembling features. Both TAPE and KEA adopt the cascading structures. After encoding the text attributes with LLMs, the generated embeddings are adopted as the initial features for GNNs. We try two approaches to encode the attributes, which are fine-tuned PLMs and local sentence embedding models. Specifically, we adopt Deberta-base and e5-large. To conduct a fair comparison, we first determine the better text encoder by evaluating their overall performance. Once the text encoder is selected, we proceed to compare the performance of the augmented attributes against the original attributes.</p>
<p>A comprehensive evaluation of TAPE. We first gain a deeper understanding of TAPE through a comprehensive ablation study. The experimental results are shown in Table 5 and Table 6. We show the approach we adopt to encode the text attributes in the bracket. In particular, we mainly consider fine-tuned Deberta-base, which is denoted as PLM, and e5-large, which is denoted as e5.
Observation 7. The effectiveness of TAPE is mainly from the explanations E generated by LLMs.
From the ablation study, we can see that compared to pseudo labels $\mathbf{P}$, the explanations present better stability across different datasets. One main advantage of adopting explanations generated by LLMs is that these augmented attributes present better performance in the low-labeling rate setting. From Table 5, we note that when choosing PLM as the encoders, $\mathbf{E}$ performs much better than TA in the low labeling rate setting. Compared to explanations, we find that the effectiveness of the P mainly depends on the zero-shot performance of LLMs, which may present large variances across different datasets. In the following analysis, we use TA + $\mathbf{E}$ and neglect the pseudo labels generated by LLMs.
Observation 8. Replacing fine-tuned PLMs with deep sentence embedding models can further improve the overall performance of TAPE.
From Table 5 and Table 6, we observe that adopting e5-large as the LLMs to encode the text attributes can achieve good performance across different datasets and different data splits. Specifically, the TA + E encoded with e5 can achieve top 3 performance in almost all settings. In the following analysis, we adopt e5 to encode the original and enhanced attributes TA + E.</p>
<p>Table 5: A detailed ablation study of TAPE on CORA and Pubmed dataset in low labeling rate setting. For each combination of features and models, we use yellow to denote the best performance under a specific GNN/MLP model, green the second best one, and pink the third best one.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>CORA</td>
<td></td>
<td></td>
<td></td>
<td>PUBMED</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>GCN</td>
<td>GAT</td>
<td>MLP</td>
<td></td>
<td>GCN</td>
<td>GAT</td>
</tr>
<tr>
<td>TAPE</td>
<td>TAPE</td>
<td></td>
<td>$74.56 \pm 2.03$</td>
<td>$75.27 \pm 2.10$</td>
<td>$64.44 \pm 0.60$</td>
<td></td>
<td>$85.97 \pm 0.31$</td>
<td>$86.97 \pm 0.33$</td>
</tr>
<tr>
<td></td>
<td>P</td>
<td></td>
<td>$52.79 \pm 1.47$</td>
<td>$62.13 \pm 1.50$</td>
<td>$63.56 \pm 0.52$</td>
<td></td>
<td>$81.92 \pm 1.89$</td>
<td>$88.27 \pm 0.01$</td>
</tr>
<tr>
<td></td>
<td>TA + E (e5)</td>
<td></td>
<td>$87.48 \pm 0.42$</td>
<td>$84.00 \pm 0.09$</td>
<td>$75.73 \pm 0.53$</td>
<td></td>
<td>$87.44 \pm 0.40$</td>
<td>$86.71 \pm 0.92$</td>
</tr>
<tr>
<td></td>
<td>TA + E (PLM)</td>
<td></td>
<td>$78.02 \pm 0.56$</td>
<td>$64.08 \pm 12.30$</td>
<td>$55.72 \pm 11.98$</td>
<td></td>
<td>$80.70 \pm 1.73$</td>
<td>$79.66 \pm 3.08$</td>
</tr>
<tr>
<td></td>
<td>E (PLM)</td>
<td></td>
<td>$79.46 \pm 1.10$</td>
<td>$74.82 \pm 1.19$</td>
<td>$63.04 \pm 0.88$</td>
<td></td>
<td>$81.88 \pm 0.05$</td>
<td>$81.56 \pm 0.07$</td>
</tr>
<tr>
<td></td>
<td>E (e5)</td>
<td></td>
<td>$84.38 \pm 0.36$</td>
<td>$82.01 \pm 0.60$</td>
<td>$70.64 \pm 1.10$</td>
<td></td>
<td>$82.23 \pm 0.78$</td>
<td>$80.30 \pm 0.77$</td>
</tr>
<tr>
<td>Original attributes</td>
<td>TA (PLM)</td>
<td></td>
<td>$59.23 \pm 1.16$</td>
<td>$57.38 \pm 2.01$</td>
<td>$30.98 \pm 0.68$</td>
<td></td>
<td>$62.12 \pm 0.07$</td>
<td>$61.57 \pm 0.07$</td>
</tr>
<tr>
<td></td>
<td>TA (e5)</td>
<td></td>
<td>$82.56 \pm 0.73$</td>
<td>$81.62 \pm 1.09$</td>
<td>$74.26 \pm 0.93$</td>
<td></td>
<td>$82.63 \pm 1.13$</td>
<td>$79.67 \pm 0.80$</td>
</tr>
</tbody>
</table>
<p>Table 6: A detailed ablation study of TAPE on CORA and Pubmed dataset in the high labeling rate setting. For each combination of features and models, we use yellow to denote the best performance under a specific GNN/MLP model, green the second best one, and pink the third best one.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>CORA</th>
<th></th>
<th></th>
<th></th>
<th>PUBMED</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>GCN</td>
<td>GAT</td>
<td>MLP</td>
<td>GCN</td>
<td>GAT</td>
<td>MLP</td>
<td></td>
</tr>
<tr>
<td>TAPE</td>
<td>TAPE</td>
<td>$87.88 \pm 0.98$</td>
<td>$88.69 \pm 1.13$</td>
<td>$83.09 \pm 0.91$</td>
<td>$92.22 \pm 1.30$</td>
<td>$93.35 \pm 1.50$</td>
<td>$95.55 \pm 0.27$</td>
<td></td>
</tr>
<tr>
<td></td>
<td>P</td>
<td>$64.90 \pm 1.39$</td>
<td>$80.11 \pm 4.01$</td>
<td>$70.31 \pm 1.91$</td>
<td>$85.73 \pm 0.59$</td>
<td>$91.60 \pm 0.62$</td>
<td>$93.65 \pm 0.35$</td>
<td></td>
</tr>
<tr>
<td></td>
<td>TA + E (e5)</td>
<td>$90.68 \pm 2.12$</td>
<td>$91.86 \pm 1.36$</td>
<td>$87.60 \pm 4.83$</td>
<td>$92.64 \pm 1.00$</td>
<td>$94.35 \pm 1.24$</td>
<td>$94.34 \pm 0.86$</td>
<td></td>
</tr>
<tr>
<td></td>
<td>TA + E (PLM)</td>
<td>$87.44 \pm 1.74$</td>
<td>$88.40 \pm 1.60$</td>
<td>$82.80 \pm 1.00$</td>
<td>$90.23 \pm 0.71$</td>
<td>$91.73 \pm 1.58$</td>
<td>$95.40 \pm 0.32$</td>
<td></td>
</tr>
<tr>
<td></td>
<td>E (PLM)</td>
<td>$83.28 \pm 4.53$</td>
<td>$82.47 \pm 6.06$</td>
<td>$80.41 \pm 3.35$</td>
<td>$88.90 \pm 2.94$</td>
<td>$83.00 \pm 14.07$</td>
<td>$87.75 \pm 14.83$</td>
<td></td>
</tr>
<tr>
<td></td>
<td>E (e5)</td>
<td>$89.39 \pm 2.69$</td>
<td>$89.13 \pm 2.52$</td>
<td>$84.05 \pm 4.03$</td>
<td>$89.68 \pm 0.78$</td>
<td>$90.61 \pm 1.61$</td>
<td>$91.09 \pm 0.85$</td>
<td></td>
</tr>
<tr>
<td>Original attributes</td>
<td>TA (PLM)</td>
<td>$85.86 \pm 2.28$</td>
<td>$86.52 \pm 1.87$</td>
<td>$78.20 \pm 2.25$</td>
<td>$91.49 \pm 1.92$</td>
<td>$89.88 \pm 4.63$</td>
<td>$94.65 \pm 0.13$</td>
<td></td>
</tr>
<tr>
<td></td>
<td>TA (e5)</td>
<td>$89.53 \pm 2.59$</td>
<td>$89.10 \pm 3.22$</td>
<td>$86.19 \pm 4.26$</td>
<td>$89.65 \pm 0.85$</td>
<td>$89.55 \pm 1.16$</td>
<td>$91.39 \pm 0.47$</td>
<td></td>
</tr>
</tbody>
</table>
<h1>4.2.1.1 Effectiveness of KEA.</h1>
<p>We then show the results of KEA in Table 7 and Table 8. For KEA-I, we inject the description of each technical term directly into the original attribute. For KEA-S, we encode the generated description and original attribute separately. Observation 9. The proposed knowledge enhancement attributes KEA can enhance the performance of the original attribute TA. From Table 7 and Table 8, we first compare the performance of features encoded by e5 and PLM. We see that the proposed KEA is more fitted to the e5 encoder, and fine-tuned PLM embeddings present poor performance on the low labeling rate, thus we also select e5 as the encoder to further compare the quality of attributes. From Table 9 we can see that the proposed KEA-I + TA and KEA-S + TA attributes can consistently outperform the original attributes TA. Observation 10. For different datasets, the most effective enhancement methods may vary. Moreover, we compare the performance of our proposed KEA with $\mathbf{T A}+\mathbf{E}$, and the results are shown in Table 9. We can see that on CORA, our methods can achieve better performance while $\mathbf{T A}+\mathbf{E}$ can achieve better performance on Pubmed. One potential explanation for this phenomenon is that $\mathbf{T A}+\mathbf{E}$ relies more on the capability of LLMs. Although we have removed the pseudo labels $\mathbf{P}$, we find that the explanations still contain LLMs' predictions. As a result, the effectiveness of $\mathbf{T A}+\mathbf{E}$ will be influenced by LLMs' performance on the dataset. As shown in [22], the LLMs can achieve superior performance on the Pubmed dataset but perform poorly on the CORA dataset. Compared to $\mathbf{T A}+\mathbf{E}$, our proposed KEA only utilizes the commonsense knowledge of the LLMs, which may have better stability across different datasets.</p>
<h2>5. LLMS AS THE PREDICTORS</h2>
<p>In the LLMs-as-Enhancers pipeline, the role of the LLMs remains somewhat limited since we only utilize their pretrained knowledge but overlook their reasoning capability. Drawing inspiration from the LLMs' proficiency in handling complex tasks with implicit structures, such as logical reasoning [7] and recommendation [14], we question: Is it possible for the LLM to independently perform predictive tasks on graph structures? By shifting our focus to node attributes and overlooking the graph structures, we can perceive node classification as a text classification problem. In [60], the LLMs demonstrate significant promise, suggesting that they can proficiently process text attributes. However, one key problem is that LLMs are not originally designed to process graph structures. Therefore, it can not directly process structural information like GNNs. In this section, we aim to explore the potential of LLMs as a predictor. In particular, we first check whether LLM can perform well without any structural information. Then, we further explore some prompts to incorporate structural information with natural languages. Finally, we show a case study in Section 5.3 to explore its potential usage as an annotator for graphs.</p>
<h3>5.1 How Can LLM Perform on Popular Graph Benchmarks without Structural Information?</h3>
<p>In this subsection, we treat the node classification problem as a text classification problem by ignoring the structural information. We adopt ChatGPT (gpt-3.5-turbo-0613) as the LLMs to conduct all the experiments. We choose five popular textual graph datasets with raw text attributes: CORA [40], Citeser [15], Pubmed [57], OgEn-ARxiv, and OgEn-PRODUCTS [23]. The details of these datasets can be found in Appendix A. Considering the costs to query LLMs'</p>
<p>Table 7: A detailed ablation study of KEA on Cora and Pubmed dataset in the low labeling rate setting. For each combination of features and models, we use yellow to denote the best performance under a specific GNN/MLP model, green the second best one, and pink the third best one.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>CORA</td>
<td></td>
<td></td>
<td></td>
<td>PUBMED</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>GCN</td>
<td>GAT</td>
<td>MLP</td>
<td>GCN</td>
<td>GAT</td>
<td>MLP</td>
</tr>
<tr>
<td>Original attributes</td>
<td>TA (PLM)</td>
<td></td>
<td>$59.23 \pm 1.16$</td>
<td>$57.38 \pm 2.01$</td>
<td>$30.98 \pm 0.68$</td>
<td>$62.12 \pm 0.07$</td>
<td>$61.57 \pm 0.07$</td>
<td>$53.65 \pm 0.26$</td>
</tr>
<tr>
<td></td>
<td>TA (e5)</td>
<td></td>
<td>$82.56 \pm 0.73$</td>
<td>$81.62 \pm 1.09$</td>
<td>$74.26 \pm 0.93$</td>
<td>$82.63 \pm 1.13$</td>
<td>$79.67 \pm 0.80$</td>
<td>$80.38 \pm 1.94$</td>
</tr>
<tr>
<td>KEA</td>
<td>KEA-I + TA (e5)</td>
<td></td>
<td>$83.20 \pm 0.56$</td>
<td>$82.39 \pm 0.63$</td>
<td>$74.34 \pm 0.97$</td>
<td>$83.30 \pm 1.75$</td>
<td>$81.16 \pm 0.87$</td>
<td>$80.74 \pm 2.44$</td>
</tr>
<tr>
<td></td>
<td>KEA-I + TA (PLM)</td>
<td></td>
<td>$53.21 \pm 11.54$</td>
<td>$55.38 \pm 4.64$</td>
<td>$31.80 \pm 3.63$</td>
<td>$57.13 \pm 8.20$</td>
<td>$58.66 \pm 4.27$</td>
<td>$52.28 \pm 4.47$</td>
</tr>
<tr>
<td></td>
<td>KEA-I (e5)</td>
<td></td>
<td>$81.35 \pm 0.77$</td>
<td>$82.04 \pm 0.72$</td>
<td>$70.64 \pm 1.10$</td>
<td>$81.98 \pm 0.91$</td>
<td>$81.04 \pm 1.39$</td>
<td>$79.73 \pm 1.63$</td>
</tr>
<tr>
<td></td>
<td>KEA-I (PLM)</td>
<td></td>
<td>$36.68 \pm 18.63$</td>
<td>$37.69 \pm 12.79$</td>
<td>$30.46 \pm 0.60$</td>
<td>$56.22 \pm 7.17$</td>
<td>$59.33 \pm 1.69$</td>
<td>$52.79 \pm 0.51$</td>
</tr>
<tr>
<td>KEA</td>
<td>KEA-I + TA (e5)</td>
<td></td>
<td>$84.63 \pm 0.58$</td>
<td>$85.02 \pm 0.40$</td>
<td>$76.11 \pm 2.66$</td>
<td>$82.25 \pm 2.29$</td>
<td>$81.94 \pm 1.51$</td>
<td>$80.74 \pm 2.44$</td>
</tr>
<tr>
<td></td>
<td>KEA-I + TA (PLM)</td>
<td></td>
<td>$51.36 \pm 16.13$</td>
<td>$52.85 \pm 7.00$</td>
<td>$34.56 \pm 5.09$</td>
<td>$59.47 \pm 6.09$</td>
<td>$51.93 \pm 3.27$</td>
<td>$51.11 \pm 2.63$</td>
</tr>
<tr>
<td></td>
<td>KEA-I (e5)</td>
<td></td>
<td>$83.01 \pm 0.60$</td>
<td>$83.01 \pm 0.60$</td>
<td>$70.64 \pm 1.10$</td>
<td>$82.23 \pm 0.78$</td>
<td>$80.30 \pm 0.77$</td>
<td>$77.23 \pm 0.48$</td>
</tr>
<tr>
<td></td>
<td>KEA-I (PLM)</td>
<td></td>
<td>$48.97 \pm 18.24$</td>
<td>$43.88 \pm 10.31$</td>
<td>$30.36 \pm 0.58$</td>
<td>$61.22 \pm 0.94$</td>
<td>$54.93 \pm 1.55$</td>
<td>$47.94 \pm 0.89$</td>
</tr>
</tbody>
</table>
<p>Table 8: A detailed ablation study of KEA on Cora and Pubmed dataset in the high labeling rate setting. For each combination of features and models, we use yellow to denote the best performance under a specific GNN/MLP model, green the second best one, and pink the third best one.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>CORA</td>
<td></td>
<td></td>
<td></td>
<td>PUBMED</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>GCN</td>
<td>GAT</td>
<td>MLP</td>
<td>GCN</td>
<td>GAT</td>
<td>MLP</td>
</tr>
<tr>
<td>Original</td>
<td>TA (PLM)</td>
<td></td>
<td>$85.86 \pm 2.28$</td>
<td>$86.52 \pm 1.87$</td>
<td>$78.20 \pm 2.25$</td>
<td>$91.49 \pm 1.92$</td>
<td>$89.88 \pm 4.63$</td>
<td>$94.65 \pm 0.13$</td>
</tr>
<tr>
<td>Attributes</td>
<td>TA (e5)</td>
<td></td>
<td>$90.53 \pm 2.33$</td>
<td>$89.10 \pm 3.22$</td>
<td>$86.19 \pm 4.38$</td>
<td>$89.65 \pm 0.85$</td>
<td>$89.55 \pm 1.16$</td>
<td>$91.39 \pm 0.47$</td>
</tr>
<tr>
<td>KEA</td>
<td>KEA-I + TA (e5)</td>
<td></td>
<td>$91.12 \pm 1.76$</td>
<td>$90.24 \pm 2.93$</td>
<td>$87.88 \pm 3.24$</td>
<td>$90.19 \pm 0.83$</td>
<td>$90.60 \pm 1.22$</td>
<td>$92.12 \pm 0.74$</td>
</tr>
<tr>
<td></td>
<td>KEA-I + TA (PLM)</td>
<td></td>
<td>$87.07 \pm 1.04$</td>
<td>$87.66 \pm 0.86$</td>
<td>$79.12 \pm 2.77$</td>
<td>$92.52 \pm 0.64$</td>
<td>$92.29 \pm 1.43$</td>
<td>$94.85 \pm 0.20$</td>
</tr>
<tr>
<td></td>
<td>KEA-I (e5)</td>
<td></td>
<td>$90.70 \pm 1.79$</td>
<td>$90.13 \pm 2.76$</td>
<td>$86.78 \pm 4.12$</td>
<td>$89.56 \pm 0.82$</td>
<td>$90.25 \pm 1.34$</td>
<td>$91.92 \pm 0.80$</td>
</tr>
<tr>
<td></td>
<td>KEA-I (PLM)</td>
<td></td>
<td>$86.08 \pm 2.33$</td>
<td>$85.23 \pm 3.15$</td>
<td>$77.97 \pm 2.87$</td>
<td>$91.73 \pm 0.58$</td>
<td>$91.82 \pm 1.76$</td>
<td>$93.76 \pm 0.32$</td>
</tr>
<tr>
<td>KEA</td>
<td>KEA-I + TA (e5)</td>
<td></td>
<td>$91.09 \pm 1.78$</td>
<td>$92.30 \pm 1.69$</td>
<td>$88.95 \pm 4.96$</td>
<td>$90.40 \pm 0.92$</td>
<td>$90.82 \pm 1.30$</td>
<td>$91.78 \pm 0.56$</td>
</tr>
<tr>
<td></td>
<td>KEA-I + TA (PLM)</td>
<td></td>
<td>$83.98 \pm 5.13$</td>
<td>$87.33 \pm 1.68$</td>
<td>$80.04 \pm 1.32$</td>
<td>$86.11 \pm 5.68$</td>
<td>$89.04 \pm 5.82$</td>
<td>$94.35 \pm 0.48$</td>
</tr>
<tr>
<td></td>
<td>KEA-I (e5)</td>
<td></td>
<td>$89.39 \pm 2.69$</td>
<td>$90.13 \pm 2.52$</td>
<td>$84.05 \pm 4.03$</td>
<td>$89.68 \pm 0.78$</td>
<td>$90.61 \pm 1.61$</td>
<td>$91.09 \pm 0.85$</td>
</tr>
<tr>
<td></td>
<td>KEA-I (PLM)</td>
<td></td>
<td>$83.35 \pm 7.30$</td>
<td>$85.67 \pm 2.00$</td>
<td>$76.76 \pm 1.82$</td>
<td>$79.68 \pm 19.57$</td>
<td>$69.90 \pm 19.75$</td>
<td>$85.91 \pm 6.47$</td>
</tr>
</tbody>
</table>
<p>Table 9: Comparison of the performance of TA, KEA-I, and KEA-S, and TA + E. The best performance is shown with an underline. Cora (low) means a low labeling rate setting, and Cora (high) denotes a high labeling rate setting.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>CORA (low)</th>
<th></th>
<th></th>
<th></th>
<th>PUBMED (low)</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>GCN</td>
<td>GAT</td>
<td>MLP</td>
<td>GCN</td>
<td>GAT</td>
<td>MLP</td>
<td></td>
</tr>
<tr>
<td>TA</td>
<td></td>
<td>$82.56 \pm 0.73$</td>
<td>$81.62 \pm 1.09$</td>
<td>$74.26 \pm 0.93$</td>
<td>$82.63 \pm 1.13$</td>
<td>$79.67 \pm 0.80$</td>
<td>$80.38 \pm 1.94$</td>
<td></td>
</tr>
<tr>
<td>KEA-I + TA</td>
<td></td>
<td>$83.20 \pm 0.56$</td>
<td>$83.38 \pm 0.63$</td>
<td>$74.34 \pm 0.97$</td>
<td>$83.30 \pm 1.75$</td>
<td>$81.16 \pm 0.87$</td>
<td>$80.74 \pm 2.44$</td>
<td></td>
</tr>
<tr>
<td>KEA-I + TA</td>
<td></td>
<td>$84.63 \pm 0.58$</td>
<td>$85.02 \pm 0.40$</td>
<td>$76.11 \pm 2.66$</td>
<td>$82.93 \pm 2.38$</td>
<td>$81.34 \pm 1.51$</td>
<td>$80.74 \pm 2.44$</td>
<td></td>
</tr>
<tr>
<td>TA+E</td>
<td></td>
<td>$83.38 \pm 0.42$</td>
<td>$84.00 \pm 0.09$</td>
<td>$75.73 \pm 0.53$</td>
<td>$87.44 \pm 0.49$</td>
<td>$86.71 \pm 0.92$</td>
<td>$90.25 \pm 1.56$</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>CORA (high)</td>
<td></td>
<td></td>
<td></td>
<td>PUBMED (high)</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>GCN</td>
<td>GAT</td>
<td>MLP</td>
<td>GCN</td>
<td>GAT</td>
<td>MLP</td>
<td></td>
</tr>
<tr>
<td>TA</td>
<td></td>
<td>$90.53 \pm 2.33$</td>
<td>$89.10 \pm 3.22$</td>
<td>$86.19 \pm 4.38$</td>
<td>$89.65 \pm 0.85$</td>
<td>$89.55 \pm 1.16$</td>
<td>$91.39 \pm 0.47$</td>
<td></td>
</tr>
<tr>
<td>KEA-I + TA</td>
<td></td>
<td>$91.12 \pm 1.76$</td>
<td>$90.24 \pm 2.93$</td>
<td>$87.88 \pm 4.44$</td>
<td>$90.19 \pm 0.83$</td>
<td>$90.60 \pm 1.22$</td>
<td>$92.12 \pm 0.74$</td>
<td></td>
</tr>
<tr>
<td>KEA-I + TA</td>
<td></td>
<td>$91.09 \pm 1.78$</td>
<td>$92.30 \pm 1.69$</td>
<td>$88.95 \pm 4.96$</td>
<td>$90.40 \pm 0.92$</td>
<td>$90.82 \pm 1.30$</td>
<td>$91.78 \pm 0.56$</td>
<td></td>
</tr>
<tr>
<td>TA+E</td>
<td></td>
<td>$90.68 \pm 2.12$</td>
<td>$91.86 \pm 1.36$</td>
<td>$87.00 \pm 4.83$</td>
<td>$92.64 \pm 1.00$</td>
<td>$93.35 \pm 1.24$</td>
<td>$94.34 \pm 0.86$</td>
<td></td>
</tr>
</tbody>
</table>
<p>APIs, it's not possible for us to test the whole dataset for these graphs. Considering the rate limit imposed by OpenAI ${ }^{4}$, we randomly select 200 nodes from the test sets as our test data. In order to ensure that these 200 nodes better represent the performance of the entire set, we repeat all experiments twice. Additionally, we employ zero-shot performance as a sanity check, comparing it with the results in TAPE [22] to ensure minimal discrepancies. We explore the following strategies:</p>
<ol>
<li>Zero-shot prompts: This approach solely involves the attribute of a given node.</li>
<li>Few-shot prompts: On the basis of zero-shot prompts, few-shot prompts provide in-context learning samples to-</li>
</ol>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>gether with their labels for LLMs to better understand the task. In addition to the node's content, this approach integrates the content and labels of randomly selected incontext samples from the training set. In the section, we adopt random sampling to select few-shot prompts. 3. Zero-shot prompts with Chain-of-Thoughts (CoT): CoT [70] presents its effectiveness in various reasoning tasks, which can greatly improve LLMs' reasoning abilities. In this study, we test whether CoT can improve LLMs' capability on node classification tasks. On the basis of zero-shot prompts, we guide the LLMs to generate the thought process by using the prompt "think it step by step". 4. Few-shot prompts with CoT: Inspired by [82], which demonstrates that incorporating the CoT process generated by LLMs can further improve LLMs' reasoning</p>
<p>capabilities. Building upon the few-shot prompts, this approach enables the LLMs to generate a step-by-step thought process for the in-context samples. Subsequently, the generated CoT processes are inserted into the prompt as auxiliary information.
Output Parsing. In addition, we need a parser to extract the output from LLMs. We devise a straightforward approach to retrieve the predictions from the outputs. Initially, we instruct the LLMs to generate the results in a formatted output like "a python list". Then, we can use the symbols "[" and "]" to locate the expected outputs. It should be noted that this design aims to extract the information more easily but has little influence on the performance. We observe that sometimes LLMs will output contents that are slightly different from the expected format, for example, output the expected format "Information Retrieval" to "Information Extraction". In such cases, we compute the edit distance between the extracted output and the category names and select the one with the smallest distance. This method proves effective when the input context is relatively short. If this strategy encounters errors, we resort to extracting the first mentioned categories in the output texts as the predictions. If there's no match, then the model's prediction for the node is incorrect.
To reduce the variance of LLMs' predictions, we set the temperature to 0 . For few-shot cases, we find that providing too much context will cause LLMs to generate outputs that are not compatible with the expected formats. Therefore, we set a maximum number of samples to ensure that LLMs generate outputs with valid formats. In this study, we choose this number to 2 and adopt accuracy as the performance metric.</p>
<h3>5.1.1 Observations</h3>
<p>Observation 11. LLMs present preliminary effectiveness on some datasets.
According to the results in Table 10, it is evident that LLMs demonstrate remarkable zero-shot performance on PubMed. When it comes to OGBN-Products, LLMs can achieve performance levels comparable to fine-tuned PLMs. However, there is a noticeable performance gap between LLMs and GNNs on Cora and PubMed datasets. To gain a deeper understanding of this observation, it is essential to analyze the output of LLMs.
Observation 12. Wrong predictions made by LLMs are sometimes also reasonable.
After investigating the output of LLMs, we find that a part of the wrong predictions made by LLMs are very reasonable. An example is shown in Table 11. In this example, we can see that besides the ground truth label "Reinforcement Learning", "Neural Networks" is also a reasonable label, which also appears in the texts. We find that this is a common problem for Cora, CiteSEER, and OGBN-ARXiv. For OGBN-ARXiv, there are usually multiple labels for one paper on the website. However, in the OGBN-ARXiv dataset, only one of them is chosen as the ground truth. This leads to a misalignment between LLMs' commonsense knowledge and the annotation bias inherent in these datasets. Moreover, we find that introducing few-shot samples presents little help to mitigate the annotation bias.
Observation 13. Chain-of-thoughts do not bring in performance gain.
For reasoning tasks in the general domain, chain-of-thoughts is believed to be an effective approach to increase LLM's rea-
soning capability [70]. However, we find that it's not effective for the node classification task. This phenomenon can be potentially explained by Observation 12. In contrast to mathematical reasoning, where a single answer is typically expected, multiple reasonable chains of thought can exist for node classification. An example is shown in Table 12. This phenomenon poses a challenge for LLMs as they may struggle to match the ground truth labels due to the presence of multiple reasonable labels.
Observation 14. For prompts that are very similar in semantics, there may be huge differences in their effects.
In addition, we observe that TAPE [22] implements a unique prompt on the OGBN-ARXiv dataset, yielding impressive results via zero-shot prompts. The primary distinction between their prompts and ours lies in the label design. Given that all papers originate from the computer science subcategory of Arxiv, they employ the brief term "arxiv cs subcategories" as a substitute for these 40 categories. Remarkably, this minor alteration contributes to a substantial enhancement in performance. To delve deeper into this phenomenon, we experiment with three disparate label designs: (1) Strategy 1: the original Arxiv identifier, such as "arxiv cs.CV"; (2) Strategy 2: natural language descriptors, like "computer vision"; and (3) Strategy 3: the specialized prompt, utilizing "arxiv cs subcategory" to denote all categories. Unexpectedly, we discover that Strategy 3 significantly outperforms the other two (refer to Table 13).
Given that LLMs undergo pre-training on extensive text corpora, it's likely that these corpora include papers from the Arxiv database. That specific prompt could potentially enhance the "activation" of these models' corresponding memory. However, the reason for the excellent results achieved by this kind of prompt might not stem from the simple data memorization of the LLM [25]. When applying to papers after 2023 that are not included in the pre-training corpus of the LLMs, this prompt also achieves similar effectiveness. This phenomenon reminds us that when using ChatGPT, sometimes providing more information in the prompt (such as category information from the OGBN-ARXIV dataset) may actually lead to a decrease in performance.</p>
<h3>5.2 Incorporating Structural Information in the Prompts</h3>
<p>As we note, LLMs can already present superior zero-shot performance on some datasets without providing any structural information. However, there is still a large performance gap between LLMs and GNNs in Cora, CiteSEER, and OGBN-ARXiv. Then a question naturally raises that whether we can further increase LLMs' performance by incorporating structural information? To answer this problem, we first need to identify how to denote the structural information in the prompt. LLMs such as ChatGPT are not originally designed for graph structures, so they can not process adjacency matrices like GNNs. In this part, we study several ways to convey structural information and test their effectiveness on the Cora dataset.
Specifically, we first consider inputting the whole graph into the LLMs. Using Cora dataset as an example, we try to use prompts like "node 1: (paper content)" to represent attributes, and prompts like "node 1 cites node 2" to represent the edge. However, we find that this approach is not</p>
<p>Table 10: Performance of LLMs on real-world text attributed graphs without structural information, we also include the result of GCN (or SAGE for OGbn-PRODUCTS) together with Sentence-BERT features. For CORA, CITESEER, PUBMED, we show the results of the low labeling rate setting.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">CORA</th>
<th style="text-align: center;">CITESEER</th>
<th style="text-align: center;">PUBMED</th>
<th style="text-align: center;">OGBN-ARXIV</th>
<th style="text-align: center;">OGBN-PRODUCTS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$67.00 \pm 1.41$</td>
<td style="text-align: center;">$65.50 \pm 3.53$</td>
<td style="text-align: center;">$90.75 \pm 5.30$</td>
<td style="text-align: center;">$51.75 \pm 3.89$</td>
<td style="text-align: center;">$70.75 \pm 2.48$</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$67.75 \pm 3.53$</td>
<td style="text-align: center;">$66.00 \pm 5.66$</td>
<td style="text-align: center;">$85.50 \pm 2.80$</td>
<td style="text-align: center;">$50.25 \pm 1.06$</td>
<td style="text-align: center;">$77.75 \pm 1.06$</td>
</tr>
<tr>
<td style="text-align: center;">Zero-shot with COT</td>
<td style="text-align: center;">$64.00 \pm 0.71$</td>
<td style="text-align: center;">$66.50 \pm 2.82$</td>
<td style="text-align: center;">$86.25 \pm 3.29$</td>
<td style="text-align: center;">$50.50 \pm 1.41$</td>
<td style="text-align: center;">$71.25 \pm 1.06$</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot with COT</td>
<td style="text-align: center;">$64.00 \pm 1.41$</td>
<td style="text-align: center;">$60.50 \pm 4.94$</td>
<td style="text-align: center;">$85.50 \pm 4.94$</td>
<td style="text-align: center;">$47.25 \pm 2.47$</td>
<td style="text-align: center;">$73.25 \pm 1.77$</td>
</tr>
<tr>
<td style="text-align: center;">GCN/SAGE</td>
<td style="text-align: center;">$82.20 \pm 0.49$</td>
<td style="text-align: center;">$71.19 \pm 1.10$</td>
<td style="text-align: center;">$81.01 \pm 1.32$</td>
<td style="text-align: center;">$73.10 \pm 0.25$</td>
<td style="text-align: center;">$82.51 \pm 0.53$</td>
</tr>
</tbody>
</table>
<p>Table 11: A wrong but reasonable prediction made by LLMs</p>
<p>Paper: The Neural Network House: An overview; Typical home comfort systems utilize only rudimentary forms of energy management and conservation. The most sophisticated technology in common use today is an automatic setback thermostat. Tremendous potential remains for improving the efficiency of electric and gas usage...
Ground Truth: Reinforcement Learning
LLM's Prediction: Neural Networks</p>
<p>Table 12: An example that LLMs generate CoT processes not matching with ground truth labels</p>
<p>Paper: The Neural Network House: An overview.: Typical home comfort systems utilize only rudimentary forms of energy management and conservation. The most sophisticated technology in common use today is an automatic setback thermostat. Tremendous potential remains for improving the efficiency of electric and gas usage...
Generated Chain-of-thoughts: The paper discusses the use of neural networks for intelligent control and mentions the utilization of neural network reinforcement learning and prediction techniques. Therefore, the most likely category for this paper is 'Neural Networks'.
Ground Truth: Reinforcement Learning
LLM's Prediction: Neural Networks</p>
<p>Table 13: Performance of LLMs on OGB-Arxiv dataset, with three different label designs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Strategy 1</th>
<th style="text-align: center;">Strategy 2</th>
<th style="text-align: center;">Strategy 3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">OGbn-ARXIV</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">74.5</td>
</tr>
</tbody>
</table>
<p>feasible since LLMs usually present a small input context length restriction. As a result, we consider an "ego-graph" view, which refers to the subgraphs induced from the center nodes. In this way, we can narrow the number of nodes to be considered.
Specifically, we first organize the neighbors of the current nodes as a list of dictionaries consisting of attributes and labels of the neighboring nodes for training nodes. Then, the LLMs summarize the neighborhood information. It should be noted that we only consider 2-hop neighbors because</p>
<p>GNNs typically have 2 layers, indicating that the 2-hop neighbor information is the most useful in most cases. Considering the input context limit of LLMs, we empirically find that each time we can summarize the attribute information of 5 neighbors. In this paper, we sample neighbors once and only summarize those selected neighbors. In practice, we can sample multiple times and summarize each of them to obtain more fine-grained neighborhood information.
Observation 15. Neighborhood summarization is likely to achieve performance gain.
From Table 14, we note that incorporating neighborhood information in either zero-shot or few-shot approaches yields performance gains compared to the zero-shot prompt without structural information except on the PUBMED dataset. By following the "homophily" assumption [87; 39], which suggests that neighboring nodes tend to share the same labels, the inclusion of neighboring information can potentially alleviate annotation bias. For instance, let's consider a paper from Arxiv covering general topics like transformers. Merely analyzing the content of this paper makes it difficult to determine which category the author would choose, as categories such as "Artificial Intelligence," "Machine Learning," and "Computer Vision" are all plausible options. However, by examining its citation relationships, we can better infer the author's bias. If the paper cites numerous sources from the "Computer Vision" domain, it is likely that the author is also a researcher in that field, thereby favoring the selection of this category. Consequently, structural information provides implicit supervision to assist LLMs in capturing the inherent annotation bias in the dataset. However, from the PUBMED dataset, we observe that incorporating neighborhood information results in clear performance drop, which necessitates a deep analysis below.
Observation 16. LLMs with structure prompts may suffer from heterophilous neighboring nodes.
From Table 14, we observe that LLMs perform worse on PUBMED after incorporating the structural information. To gain a deeper understanding, we focus on those nodes where zero-shot prompts without structural information can lead to correct prediction but prompts with 2-hop information can't.
An example of this kind of node is shown in Table 15. After analyzing the 2-hop neighbors of this node, we find that 15 out of 192 -hop neighboring nodes have different labels against this node. This case is usually denoted as "heterophily" [87], which is a phenomenon in graph theory where nodes in a graph tend to connect with nodes that are dissimilar to them. In this case, we find that both GNNs and LLMs with a structure-aware prompt make wrong predictions. However, LLMs ignoring structural information get correct predictions, which indicates that LLMs with a structure-aware prompt may also suffer from the "het-</p>
<p>Table 14: Performance of LLMs on real-world text attributed graphs with summarized neighborhood information. For CORA, Citeser, Pubmed, we show the results of the low labeling rate setting. We also include the result of GCN (or SAGE for OGbn-PRODUCTS) together with Sentence-BERT features.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">CORA</th>
<th style="text-align: center;">Citeser</th>
<th style="text-align: center;">Pubmed</th>
<th style="text-align: center;">OGbn-ARXIV</th>
<th style="text-align: center;">OGbn-PRODUCTS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$67.00 \pm 1.41$</td>
<td style="text-align: center;">$65.50 \pm 3.53$</td>
<td style="text-align: center;">$90.75 \pm 5.30$</td>
<td style="text-align: center;">$51.75 \pm 3.89$</td>
<td style="text-align: center;">$70.75 \pm 2.48$</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$67.75 \pm 3.53$</td>
<td style="text-align: center;">$66.00 \pm 5.66$</td>
<td style="text-align: center;">$85.50 \pm 2.80$</td>
<td style="text-align: center;">$50.25 \pm 1.06$</td>
<td style="text-align: center;">$77.75 \pm 1.06$</td>
</tr>
<tr>
<td style="text-align: center;">Zero-Shot with 2-hop info</td>
<td style="text-align: center;">$71.75 \pm 0.35$</td>
<td style="text-align: center;">$62.00 \pm 1.41$</td>
<td style="text-align: center;">$88.00 \pm 1.41$</td>
<td style="text-align: center;">$55.00 \pm 2.83$</td>
<td style="text-align: center;">$75.25 \pm 3.53$</td>
</tr>
<tr>
<td style="text-align: center;">Few-Shot with 2-hop info</td>
<td style="text-align: center;">$74.00 \pm 4.24$</td>
<td style="text-align: center;">$67.00 \pm 4.94$</td>
<td style="text-align: center;">$79.25 \pm 6.71$</td>
<td style="text-align: center;">$52.25 \pm 3.18$</td>
<td style="text-align: center;">$76.00 \pm 2.82$</td>
</tr>
<tr>
<td style="text-align: center;">GCN/SAGE</td>
<td style="text-align: center;">$82.20 \pm 0.49$</td>
<td style="text-align: center;">$71.19 \pm 1.10$</td>
<td style="text-align: center;">$81.01 \pm 1.32$</td>
<td style="text-align: center;">$73.10 \pm 0.25$</td>
<td style="text-align: center;">$82.51 \pm 0.53$</td>
</tr>
</tbody>
</table>
<p>erophily" problem.
Table 15: GNNs and LLMs with structure-aware prompts are both wrong</p>
<p>Paper: Title: C-reactive protein and incident cardiovascular events among men with diabetes.
Abstract: OBJECTIVE: Several large prospective studies have shown that baseline levels of C-reactive protein (CRP)</p>
<p>Neighbor Summary: This paper focuses on different aspects of type 2 diabetes mellitus. It explores the levels of various markers such as tumor necrosis factor-alpha, interleukin-2 ... Ground truth: "Diabetes Mellitus Type 1" Structure-ignorant prompts: "Diabetes Mellitus Type 1"
Structure-aware prompt: "Diabetes Mellitus Type 2"
GNN: "Diabetes Mellitus Type 2"</p>
<h3>5.3 Case Study: LLMs as the Pseudo Annotators</h3>
<p>From Table 10, we show that LLMs can be good zero-shot predictors on several real-world graphs, which provides the possibility to conduct zero-shot inference on datasets without labels. Despite the effectiveness of LLMs, it still presents two problems: (1) The price of using LLMs' API is not cheap, and conducting inference on all testing nodes for large graphs incurs high costs; (2) Whether it is a locally deployed open-source LLM or a closed source LLM accessed through an API, the inference with these LLMs are much slower than GNNs, since the former has high computational resource requirements, while the latter has rate limits. One potential solution to these challenges is leveraging the knowledge of LLMs to train smaller models like GNNs, which inspires a potential application of LLMs to be used as annotators.
Based on the preliminary experimental outcomes, LLMs display encouraging results on certain datasets, thus highlighting their potential for generating high-quality pseudo-labels. However, the use of LLMs as an annotator introduces a new challenge. A key consideration lies in deciding the nodes that should be annotated. Unlike the self-labeling in GNNs $[8 ; 34 ; 32]$, where confidence-based or informationbased metrics are employed to estimate the quality of pseudolabels. It remains a difficult task to determine the confidence of pseudo-labels generated by LLMs. Additionally, different nodes within a graph have distinct impacts on other nodes [72]. Annotating certain nodes can result in a more
significant performance improvement compared to others. Consequently, the primary challenge can be summarized as follows: how can we effectively select both the critical nodes within the graph and the reliable nodes in the context of LLMs?
Taking into account the complexity of these two challenges, we don't intend to comprehensively address them in this paper. Instead, we present a preliminary study to evaluate the performance of a simple strategy: randomly selecting a subset of nodes for annotation. It is worth noting that advanced selection strategies such as active learning [72] could be adopted to improve the final performance. We leave such exploration as future work. Regarding the annotation budget, we adopt a "low labeling rate" setting, wherein we randomly select a total of 20 nodes multiplied by the number of classes. For the selected nodes, we adopt $75 \%$ of them as training nodes and the rest as validation nodes. Consequently, we annotate a total of 140 nodes in the CORA dataset and 60 nodes in the Pubmed dataset. In this part, we use GCN as the GNN model and adopt the embeddings generated by the Sentence-BERT model. The results are shown in Table 16. We can observe that training GCN on the pseudo labels can lead to satisfying performance. Particularly, it can match the performance of GCN trained on ground truth labels with 10 shots per class. As a reference, around $67 \%$ of the pseudo labels for CORA can match ground truth labels, while around $93 \%$ of the pseudo labels for Pubmed are ground truth labels.</p>
<p>Table 16: Performance of GCN trained on either pseudo labels generated by LLMs, or ground truth labels</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Cora</th>
<th style="text-align: center;">Pubmed</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Using pseudo labels</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{2 0}$ shots $\times #$ class</td>
<td style="text-align: center;">$64.95 \pm 0.98$</td>
<td style="text-align: center;">$71.70 \pm 1.06$</td>
</tr>
<tr>
<td style="text-align: left;">Using ground truth</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{3}$ shots per class</td>
<td style="text-align: center;">$52.63 \pm 1.46$</td>
<td style="text-align: center;">$59.35 \pm 2.67$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{5}$ shots per class</td>
<td style="text-align: center;">$58.97 \pm 1.41$</td>
<td style="text-align: center;">$65.98 \pm 0.74$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{1 0}$ shots per class</td>
<td style="text-align: center;">$69.87 \pm 2.27$</td>
<td style="text-align: center;">$71.51 \pm 0.77$</td>
</tr>
</tbody>
</table>
<p>Observation 17. The quality of pseudo labels is key to downstream performance.
Although we don't place significant emphasis on the selection of nodes to be labeled, the preliminary results show that there is relatively little variance among different random selections. Comparing this to the impact of pseudo labels, we observe that the quality of pseudo labels can make a significant difference. When higher quality pseudo labels are used, GNNs perform much better on Pubmed compared to CORA. This result highlights the importance of developing an approach to select confident nodes for LLMs.
Observation 18. Getting the confidence by simply</p>
<p>prompting the LLMs may not work since they are too "confident".
Based on previous observations, we check some simple strategies to achieve the confidence level of LLMs' outputs. Initially, we attempt to prompt the LLMs directly for their confidence level. However, we discover that most of the time, LLMs simply output a value of 1 , rendering it meaningless. Examples are shown in Table 17.</p>
<p>Table 17: Prompts used to generate neighbor summary</p>
<h2>Instruction</h2>
<p>Output the confidence level in the range of 0 to 1 and the most 1 possible category of this paper as a python dict, like "prediction": "XX", "confidence": "XX"</p>
<p>Another potential solution is to utilize LLMs that support prediction logits, such as text-davinci-003. However, we observe that the probability of the outputs from these models is consistently close to 1 , rendering the output not helpful.</p>
<h3>5.4 Case Study: Applying LLMs to handle out-of-distribution data</h3>
<p>Out-of-distribution (OOD) learning addresses scenarios where training and test data are drawn from different distributions. Given the ubiquity of distribution shifts in graph data [29], OOD generalization on graphs has emerged as a crucial research direction in recent years. A recent benchmark, GOOD [17], reveals that existing GNN-based models struggle with robustness when confronted with distributional shifts. In contrast, LLMs have demonstrated commendable robustness on textual data in the presence of OOD scenarios [67]. Node classification on the TAG, when disregarding graph structures, can also be considered as a text classification task. Therefore, in this section, we initiate a preliminary exploration into the application of LLMs for OOD scenarios on graphs.
Experimental Setups. We adopt the GOOD-Arxiv dataset from the GOOD benchmark [17] considering its text attribute availability. Specifically, we adopt all four types of the OOD shift: "Concept-degree", "Covariate-degree", "Concept-time", and "Covariate-time" from the GOOD. The final results are shown in Table 18. We adopt the prompt from TAPE [22] since it achieves better performance on the OGIN-ARXIV dataset. For comparison, we take the best baseline models from the GOOD benchmark.</p>
<p>Table 18: OOD performance comparison. "Val" means the results on the IID validation sets. "Test" indicates the results of the OOD test sets. We can see that LLMs-as-Predictors consistently outperform the best GNN-based OOD baselines. Moreover, the gap between IID performance and OOD performance is small.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Val</th>
<th style="text-align: left;">Test</th>
<th style="text-align: left;">Best baseline (test)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">concept degree</td>
<td style="text-align: left;">73.01</td>
<td style="text-align: left;">72.79</td>
<td style="text-align: left;">63.00</td>
</tr>
<tr>
<td style="text-align: left;">covariate degree</td>
<td style="text-align: left;">70.23</td>
<td style="text-align: left;">68.21</td>
<td style="text-align: left;">59.08</td>
</tr>
<tr>
<td style="text-align: left;">concept time</td>
<td style="text-align: left;">72.66</td>
<td style="text-align: left;">71.98</td>
<td style="text-align: left;">67.45</td>
</tr>
<tr>
<td style="text-align: left;">covariate time</td>
<td style="text-align: left;">74.28</td>
<td style="text-align: left;">74.37</td>
<td style="text-align: left;">71.34</td>
</tr>
</tbody>
</table>
<p>Observation 19. LLMs-as-Predictors demonstrate robustness when facing OOD data.
From Table 18, we find that LLMs-as-Predictors present promising robustness against OOD data. It should be noted that we only try a simple structure-ignorant prompt, and we may further improve the OOD performance of LLMs by selecting proper in-context samples and incorporating structural information. In a nutshell, LLMs present great potential to enhance the OOD generalization capability of graph models.</p>
<h2>6. RELATED WORK</h2>
<p>Following our proposed two pipelines, i.e., LLMs as the Enhancers and LLMs as the Predictors, we review existing works in this section.</p>
<h3>6.1 LLMs as the Enhancers</h3>
<p>In the recent surge of research, increasing attention has been paid on the intersection of LLMs and GNNs in the realm of TAGs [83; 6; 78; 77; 49; 22; 86; 24; 33; 10]. Compared to shallow embeddings, LLMs can provide a richer repository of commonsense knowledge, which could potentially enhance the performance of downstream tasks [51].
Several studies employ PLMs as text encoders, transforming text attributes into node features, which can thus be classified as feature-level enhancement. The integration structures vary among these works: some adopt a simple cascading structure [49; 6; 78; 37], while others opt for an iterative structure [83; 74; 77]. For those utilizing the cascading structure, preliminary investigations have been conducted to determine how the quality of text embeddings affects downstream classification performance [49]. GIANT [6] attempts to incorporate structural information into the pre-training stage of PLMs, achieving improved performance albeit with additional training overhead. SimTEG [10] suggests that using embeddings obtained through efficiently fine-tuned parameters to replace the original embeddings of pre-trained language models can solve the problem of overfitting during fine-tuning, thereby further enhancing the performance of the cascading structure. OneForAll [33] further adopts sentence embedding model to unify the feature space, and propose a unified model for diverse tasks across multiple datasets. This cascading structure has also been successfully applied to tasks such as fact verification [37] and question answering [78]. However, despite its simplicity, recent studies [83] have identified potential drawbacks of the cascading structure. Specifically, it establishes a tenuous connection between the text attribute and the graph. The embeddings generated by the PLMs do not take graph structures into account, and the parameters of the PLMs remain constant during the GNN training process. Alternatively, in the iterative structure, Graphformers [74] facilitates the co-training of PLMs and GNNs using each other's generated embeddings. GLEM [83] takes this a step further by considering pseudo labels generated by both PLMs and GNNs and incorporating them into the optimization process. DRAGON [77] successfully extends the iterative structure to the knowledge graph domain.
Compared to these studies focusing on PLMs, a recent study [22] considers the usage of embedding-invisible LLMs such as ChatGPT [45] for representation learning on TAGs, which aims to adopt LLMs to enhance the text attributes and thus can be categorized into text-level enhancement. This</p>
<p>work introduces a prompt designed to generate explanations for the predictions made by LLMs. These generated explanations are subsequently encoded into augmented features by PLMs. Through the ensemble of these augmented features with the original features, the proposed methodology demonstrates its efficacy and accomplishes state-of-the-art performance on the OGEN-ARxiv leaderboard [23]. Nevertheless, the study offers limited analytical insights into the underlying reasons for the success of this approach. Additionally, we have identified a potential concern regarding the prompts utilized in the referenced study.
Another work pertaining to the integration of LLMs and GNNs is the Graph-Toolformer [80]. Drawing inspirations from Toolformer [56], this study utilizes LLMs as an interface to bridge the natural language commands and GNNs. This approach doesn't change the features and training of GNNs, which is out of our scope.</p>
<h3>6.2 LLMs as the Predictors</h3>
<p>While LLMs-as-Enhancers have proven to be effective, the pipeline still requires GNNs for final predictions. In a significant shift from this approach, recent studies [18; 65] have begun exploring a unique pipeline that solely relies on LLMs for final predictions. These works fall under the category of LLMs-as-Predictors. The first series of work focus on applying closed-source LLMs without tuning the parameters. GPT4Graph [18] evaluates the potential of LLMs in executing knowledge graph (KG) reasoning and node classification tasks. Their findings indicate that these models can deliver competitive results for short-range KG reasoning but struggle with long-range KG reasoning and node classification tasks. However, its presentation is pretty vague and they don't give the detailed format of the prompt they use. Considering the publicity of the Arxiv data, the data leakage problem in evaluation is further studied in [25]. NLGraph [65] introduces a synthetic benchmark to assess graph structure reasoning capabilities. The study primarily concentrates on traditional graph reasoning tasks such as shortest path, maximum flow, and bipartite matching, while only offering limited analysis on node classification tasks. This does not align with our central focus, primarily on graph learning, with a specific emphasis on node classification tasks. GraphText [84] further tries to apply LLMs to a broader range of non-text-attributed graphs by converting the original features into clustering centers or pseudo labels. LLM4Dyg [81] further evaluates LLMs' capability for temporal graph-related tasks. LLMGNN [4] and GPT4GNAS [66] apply LLMs-as-predictors as annotators and agents for neural architecture search, respectively.
As these closed-source LLMs only accept text-type inputs, the first type of methods requires transforming graphs into certain form of natural language, either directly using node attributes or describing the graph structure using natural language. Meanwhile, due to the input length limitations of LLMs, this transformation process often results in the loss of a considerable amount of information from the graph. Therefore, the second type of work involves fine-tuning LLMs to enable them to understand graph information represented as embeddings. InstructGLM [79] combines textual instructions with node features in embedding form, enabling LLMs to understand node features through instruction tuning. Subsequently, it predicts the type of nodes based on the given instructions. GraphGPT [62] further introduces cross-modal
contrastive learning to align the graph and text feature spaces. It also introduces dual-stage instruction tuning, where the first stage adopts self-supervised instruction tuning to make LLMs better understand graph-structured information. The second stage adopts task-specific fine-tuning to allow LLMs achieve task-specific knowledge and then make predictions. GraphLLM [3] and DGTL [50] apply this pipeline to graph reasoning tasks and graph representation learning.</p>
<h2>7. CONCLUSIONS, LIMITATIONS, AND FUTURE DIRECTIONS</h2>
<p>In this section, we summarize our key findings, present the limitations of this study and discuss the potential directions of leveraging LLMs in graph machine learning.</p>
<h3>7.1 Key Findings</h3>
<p>In this paper, we propose two potential pipelines: LLMs-asEnhancers and LLMs-as-Predictors that incorporate LLMs to handle the text-attributed graphs. Our rigorous empirical studies reveal several interesting findings which provide new insights for future studies. We highlight some key findings below and more can be found from Observation 1 to Observation 19.
Finding 1. For LLMs-as-Enhancers, deep sentence embedding models present effectiveness in terms of performance and efficiency. We empirically find that when we adopt deep sentence embedding models as enhancers at the feature level, they present good performance under different dataset split settings, and also scalability. This indicates that they are good candidates to enhance text attributes at the feature level.
Finding 2. For LLMs-as-Enhancers, the combination of LLMs ${ }^{7}$ augmentations and ensembling demonstrates its effectiveness. As demonstrated in Section 4.2, when LLMs are utilized as enhancers at the text level, we observe performance improvements by ensembling the augmented attributes with the original attributes across datasets and data splits. This suggests a promising approach to enhance the performance of attribute-related tasks. The proposed pipeline involves augmenting the attributes with LLMs and subsequently ensembling the original attributes with the augmented ones.
Finding 3. For LLMs-as-Predictors, LLMs present preliminary effectiveness but also indicate potential evaluation problem. In Section 5, we conduct preliminary experiments on applying LLMs as predictors, utilizing both textual attributes and edge relationships. The results demonstrate that LLMs present effectiveness in processing textual attributes and achieving good zero-shot performance on certain datasets. Moreover, our analysis reveals two potential problems within the existing evaluation framework: (1) There are instances where LLMs' inaccurate predictions can also be considered reasonable, particularly in the case of citation datasets where multiple labels may be appropriate. (2) We find a potential test data leakage problem on OGEnARXiv, which underscores the need for a careful reconsideration of how to appropriately evaluate the performance of LLMs on real-world datasets.</p>
<h3>7.2 Limitations</h3>
<p>A deeper understanding of the effectiveness of text embeddings. Despite the effectiveness of deep sentence</p>
<p>embedding models, our understanding of why their embeddings outperform PLMs' on node classification tasks remains limited. Furthermore, we observe a performance gap between deep sentence embedding models and GLEM on the Ogbn-Products dataset, which may be related to the domains of the dataset. Moreover, as shown in Observation 4, GNNs demonstrate different levels of effectiveness on different text embeddings. However, we give limited explanations for this phenomenon. To gain a deeper understanding, we need to have a look at the original feature space and the feature space after aggregation. This phenomenon may potentially be related to the anistrophy in language model embeddings [12]. More in-depth analysis is required to better understand these phenomena.
Costs of LLM augmentations. In the work, we study TAPE and KEA to enhance the textual attributes at the text level. Although these methods have proven to be effective, they require querying LLMs' APIs at least N times for a graph with N nodes. Given the cost associated with LLMs, this poses a significant expense when dealing with large-scale datasets. Consequently, we have not presented results for the Ogbn-ARXiv and Ogbn-Products datasets.
Text-formatted hand-crafted prompts to represent graphs. In Section 5, we limit our study to the use of "natural language" prompts for graph representation. However, various other formats exist for representing graphs in natural language such as XML, YAML, GML, and more [55]. Moreover, we mainly design these prompts in a hand-crafted way, which is mainly based on trial and error. It's thus worthwhile to consider exploring more prompt formats and how to come up with automatic prompts.</p>
<h3>7.3 Future Directions</h3>
<p>Extending the current pipelines to more tasks and more types of graphs. In this study, our primary focus is on investigating the node classification task for textattributed graphs. Nevertheless, it remains unexplored whether these two pipelines can be extended to other graphlearning tasks or other types of graphs. Certain tasks necessitate the utilization of long-range information [11], and representing such information within LLMs' limited input context poses a significant challenge. Furthermore, we demonstrate that LLMs exhibit promising initial results in graphs containing abundant textual information, particularly in natural language. However, the exploration of their effective extension to other types of graphs with non-natural language information, such as molecular graph [13; 30], still needs further exploration.
Using LLMs more efficiently. Despite the effectiveness of LLMs, the inherent operational efficiency and operational cost of these models still pose significant challenges. Taking ChatGPT, which is accessed through an API, as an example, the current billing model incurs high costs for processing large-scale graphs. As for locally deployed open-source large models, even just using them for inference requires substantial hardware resources, not to mention training the models with parameter updates. Therefore, developing more efficient strategies to utilize LLMs is currently a challenge.
Evaluating LLMs' capability for graph learning tasks. In this paper, we briefly talk about the potential pitfalls of the current evaluation framework. There are mainly two problems: (1) the test data may already appear in the training corpus of LLMs, which is referred to as "contamina-
tion" ${ }^{5}$ (2) the ground truth labels may present ambiguity, and the performance calculated based on them may not reflect LLMs' genuine capability. For the first problem, one possible mitigation is to use the latest dataset which is not included in the training corpus of LLMs. However, that means we need to keep collecting data and annotating them, which seems not an effective solution. For the second problem, one possible solution is to reconsider the ground truth design. For instance, for the categorization of academic papers, we may adopt a multi-label setting and select all applicable categories as the ground truth. However, for more general tasks, it remains a challenge to design more reasonable ground truths. Generally speaking, it's a valuable future direction to rethink how to properly evaluate LLMs.
Aligning the feature space of graph models and LLMs. Currently, a major obstacle hindering the wider application of LLMs in the field of graph learning is the discrepancy between the feature space of LLMs and that of graphs. This discrepancy makes it difficult for LLMs to effectively understand information in the graph domain. There are mainly two approaches to address this issue in current work. The first approach is to translate the information on the graph into natural language that LLMs can understand. The second approach involves directly inputting the graph information in the form of embeddings and then using instruction tuning to enable LLMs to understand this information. However, both methods have their evident limitations. For the first method, the translation process can result in information loss, and the inherent input length limitation of LLMs also prevents users from inputting large-scale graphs. For the second method, the introduction of tuning significantly increases computational overhead. Is there a better way to align LLMs with graphs? A recent work targeting multimodality [47] has shown new possibilities. It demonstrates that with fixed LLM parameters, only a linear transformation layer is needed to convert information from the visual domain into content that can be effectively processed by LLMs, and such an architecture also holds great potential in the field of graph machine learning.</p>
<h2>8. REFERENCES</h2>
<p>[1] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.
[2] S. Bubeck, V. Chandrasekaran, R. Eldan, J. A. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y.-F. Li, S. M. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4. ArXiv, abs/2303.12712, 2023.
[3] Z. Chai, T. Zhang, L. Wu, K. Han, X. Hu, X. Huang, and Y. Yang. Graphllm: Boosting graph reasoning ability of large language model. arXiv preprint arXiv:2310.05845, 2023.
[4] Z. Chen, H. Mao, H. Wen, H. Han, W. Jin, H. Zhang, H. Liu, and J. Tang. Label-free node classification on graphs with large language models (llms). arXiv preprint arXiv:2310.04668, 2023.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>[5] W.-L. Chiang, X. Liu, S. Si, Y. Li, S. Bengio, and C.-J. Hsieh. Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery $\mathcal{G}$ Data Mining, 2019.
[6] E. Chien, W.-C. Chang, C.-J. Hsieh, H.-F. Yu, J. Zhang, O. Milenkovic, and I. S. Dhillon. Node feature extraction by self-supervised multi-scale neighborhood prediction. In ICLR 2022, 2022.
[7] A. Creswell, M. Shanahan, and I. Higgins. Selectioninference: Exploiting large language models for interpretable logical reasoning. In The Eleventh International Conference on Learning Representations, 2023.
[8] E. Dai, C. Aggarwal, and S. Wang. Nrgnn: Learning a label noise resistant graph neural network on sparsely and noisily labeled graphs. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery $\mathcal{G}$ Data Mining, KDD '21, page 227-236, New York, NY, USA, 2021. Association for Computing Machinery.
[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
[10] K. Duan, Q. Liu, T.-S. Chua, S. Yan, W. T. Ooi, Q. Xie, and J. He. Simteg: A frustratingly simple approach improves textual graph learning. arXiv preprint arXiv:2308.02565, 2023.
[11] V. P. Dwivedi, L. Rampášek, M. Galkin, A. Parviz, G. Wolf, A. T. Luu, and D. Beaini. Long range graph benchmark. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.
[12] K. Ethayarajh. How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP1JCNLP), pages 55-65, Hong Kong, China, Nov. 2019. Association for Computational Linguistics.
[13] M. Fey and J. E. Lenssen. Fast graph representation learning with pytorch geometric. ArXiv, abs/1903.02428, 2019.
[14] Y. Gao, T. Sheng, Y. Xiang, Y. Xiong, H. Wang, and J. Zhang. Chat-rec: Towards interactive and explainable llms-augmented recommender system. ArXiv, abs/2303.14524, 2023.
[15] C. L. Giles, K. D. Bollacker, and S. Lawrence. Citeseer: An automatic citation indexing system. In Proceedings of the Third ACM Conference on Digital Libraries, DL 98, pages 89-98, New York, NY, USA, 1998. ACM.
[16] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. ArXiv, abs/1704.01212, 2017.
[17] S. Gui, X. Li, L. Wang, and S. Ji. GOOD: A graph out-of-distribution benchmark. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.
[18] J. Guo, L. Du, and H. Liu. Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. arXiv preprint arXiv:2305.15066, 2023.
[19] W. L. Hamilton, R. Ying, and J. Leskovec. Inductive representation learning on large graphs. In NIPS, 2017.
[20] Z. S. Harris. Distributional structure. Word, 10(23):146-162, 1954.
[21] P. He, X. Liu, J. Gao, and W. Chen. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654, 2020.
[22] X. He, X. Bresson, T. Laurent, and B. Hooi. Explanations as features: Llm-based features for text-attributed graphs. arXiv preprint arXiv:2305.19523, 2023.
[23] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118-22133, 2020.
[24] Z. Hu, Y. Dong, K. Wang, K.-W. Chang, and Y. Sun. Gpt-gnn: Generative pre-training of graph neural networks. Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery $\mathcal{G}$ Data Mining, 2020.
[25] J. Huang, X. Zhang, Q. Mei, and J. Ma. Can llms effectively leverage graph structural information: When and why. arXiv preprint arXiv:2309.16595, 2023.
[26] Y. Ji, Y. Gong, Y. Peng, C. Ni, P. Sun, D. Pan, B. Ma, and X. Li. Exploring chatgpt's ability to rank content: A preliminary study on consistency with human preferences. ArXiv, abs/2303.07610, 2023.
[27] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017.
[28] G. Li, M. Müller, B. Ghanem, and V. Koltun. Training graph neural networks with 1000 layers. In International conference on machine learning, pages 64376449. PMLR, 2021.
[29] H. Li, X. Wang, Z. Zhang, and W. Zhu. Out-ofdistribution generalization on graphs: A survey. arXiv preprint arXiv:2202.07987, 2022.
[30] J. Li, Y. Liu, W. Fan, X. Wei, H. Liu, J. Tang, and Q. Li. Empowering molecule discovery for moleculecaption translation with large language models: A chatgpt perspective. ArXiv, abs/2306.06615, 2023.</p>
<p>[31] Q. Li, X. Li, L. Chen, and D. Wu. Distilling knowledge on text graph for social media attribute inference. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '22, page 2024-2028, New York, NY, USA, 2022. Association for Computing Machinery.
[32] Y. Li, J. Yin, and L. Chen. Informative pseudo-labeling for graph neural networks with few labels. Data Mining and Knowledge Discovery, 37(1):228-254, 2023.
[33] H. Liu, J. Feng, L. Kong, N. Liang, D. Tao, Y. Chen, and M. Zhang. One for all: Towards training one graph model for all classification tasks. arXiv preprint arXiv:2310.00149, 2023.
[34] H. Liu, B. Hu, X. Wang, C. Shi, Z. Zhang, and J. Zhou. Confidence may cheat: Self-training on graph neural networks under distribution shift. In Proceedings of the ACM Web Conference 2022, WWW '22, page 1248-1258, New York, NY, USA, 2022. Association for Computing Machinery.
[35] J. Liu, C. Liu, R. Lv, K. Zhou, and Y. Zhang. Is chatgpt a good recommender? a preliminary study. arXiv preprint arXiv:2304.10149, 2023.
[36] W. Liu, P. Zhou, Z. Zhao, Z. Wang, Q. Ju, H. Deng, and P. Wang. K-bert: Enabling language representation with knowledge graph. In AAAI Conference on Artificial Intelligence, 2019.
[37] Z. Liu, C. Xiong, M. Sun, and Z. Liu. Fine-grained fact verification with kernel graph attention network. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7342-7351, Online, July 2020. Association for Computational Linguistics.
[38] Y. Ma and J. Tang. Deep Learning on Graphs. Cambridge University Press, 2021.
[39] H. Mao, Z. Chen, W. Jin, H. Han, Y. Ma, T. Zhao, N. Shah, and J. Tang. Demystifying structural disparity in graph neural networks: Can one size fit all? arXiv preprint arXiv:2306.01323, 2023.
[40] A. McCallum, K. Nigam, J. D. M. Rennie, and K. Seymore. Automating the construction of internet portals with machine learning. Information Retrieval, 3:127163, 2000 .
[41] A. Miaschi and F. Dell'Orletta. Contextual and noncontextual word embeddings: an in-depth linguistic investigation. In Proceedings of the 5th Workshop on Representation Learning for NLP, pages 110-119, Online, July 2020. Association for Computational Linguistics.
[42] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.
[43] N. Muennighoff, N. Tazi, L. Magne, and N. Reimers. MTEB: Massive text embedding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2014-2037, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics.
[44] A. Neelakantan, T. Xu, R. Puri, A. Radford, J. M. Han, J. Tworek, Q. Yuan, N. A. Tezak, J. W. Kim, C. Hallacy, J. Heidecke, P. Shyam, B. Power, T. E. Nekoul, G. Sastry, G. Krueger, D. P. Schnurr, F. P. Such, K. S.K. Hsu, M. Thompson, T. Khan, T. Sherbakov, J. Jang, P. Welinder, and L. Weng. Text and code embeddings by contrastive pre-training. ArXiv, abs/2201.10005, 2022.
[45] OpenAI. Introducing chatgpt, 2022.
[46] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.
[47] Z. Pang, Z. Xie, Y. Man, and Y.-X. Wang. Frozen transformers in language models are effective visual encoder layers. arXiv preprint arXiv:2310.12973, 2023.
[48] F. Petroni, T. Rocktäschel, P. Lewis, A. Bakhtin, Y. Wu, A. H. Miller, and S. Riedel. Language models as knowledge bases? ArXiv, abs/1909.01066, 2019.
[49] S. Purchase, A. Zhao, and R. D. Mullins. Revisiting embeddings for graph neural networks. ArXiv, abs/2209.09338, 2022.
[50] Y. Qin, X. Wang, Z. Zhang, and W. Zhu. Disentangled representation learning with large language models for text-attributed graphs. arXiv preprint arXiv:2310.18152, 2023.
[51] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang. Pre-trained models for natural language processing: A survey. Science China Technological Sciences, 63:1872 - 1897, 2020.
[52] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. 2019.
[53] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020.
[54] N. Reimers and I. Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China, Nov. 2019. Association for Computational Linguistics.
[55] M. Roughan and S. J. Tuke. Unravelling graphexchange file formats. ArXiv, abs/1503.02781, 2015.
[56] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.
[57] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classification in network data. AI Magazine, 29(3):93, Sep. 2008.</p>
<p>[58] C. Sun, H. Gu, and J. Hu. Scalable and adaptive graph neural networks with self-label-enhanced training. arXiv preprint arXiv:2104.09376, 2021.
[59] T. Sun, Y. Shao, H. Qian, X. Huang, and X. Qiu. Blackbox tuning for language-model-as-a-service. In International Conference on Machine Learning, pages 2084120855. PMLR, 2022.
[60] X. Sun, X. Li, J. Li, F. Wu, S. Guo, T. Zhang, and G. Wang. Text classification via large language models. ArXiv, abs/2305.08377, 2023.
[61] Y. Sun, S. Wang, Y. Li, S. Feng, X. Chen, H. Zhang, X. Tian, D. Zhu, H. Tian, and H. Wu. Ernie: Enhanced representation through knowledge integration. ArXiv, abs/1904.09223, 2019.
[62] J. Tang, Y. Yang, W. Wei, L. Shi, L. Su, S. Cheng, D. Yin, and C. Huang. Graphgpt: Graph instruction tuning for large language models. arXiv preprint arXiv:2310.13023, 2023.
[63] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[64] P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio. Graph attention networks. In International Conference on Learning Representations, 2018.
[65] H. Wang, S. Feng, T. He, Z. Tan, X. Han, and Y. Tsvetkov. Can language models solve graph problems in natural language? arXiv preprint arXiv:2305.10037, 2023.
[66] H. Wang, Y. Gao, X. Zheng, P. Zhang, H. Chen, and J. Bu. Graph neural architecture search with gpt-4. arXiv preprint arXiv:2310.01436, 2023.
[67] J. Wang, X. Hu, W. Hou, H. Chen, R. Zheng, Y. Wang, L. Yang, H. Huang, W. Ye, X. Geng, et al. On the robustness of chatgpt: An adversarial and out-of-distribution perspective. arXiv preprint arXiv:2302.12095, 2023.
[68] L. Wang, N. Yang, X. Huang, B. Jiao, L. Yang, D. Jiang, R. Majumder, and F. Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.
[69] M. Wang, L. Yu, D. Zheng, Q. Gan, Y. Gai, Z. Ye, M. Li, J. Zhou, Q. Huang, C. Ma, Z. Huang, Q. Guo, H. Zhang, H. Lin, J. J. Zhao, J. Li, A. Smola, and Z. Zhang. Deep graph library: Towards efficient and scalable deep learning on graphs. ArXiv, abs/1909.01315, 2019.
[70] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.
[71] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, and J. Brew. Huggingface's transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.
[72] Y. Wu, Y. Xu, A. Singh, Y. Yang, and A. W. Dubrawski. Active learning for graph neural networks via node feature propagation. ArXiv, abs/1910.07567, 2019.
[73] F. Xia, K. Sun, S. Yu, A. Aziz, L. Wan, S. Pan, and H. Liu. Graph learning: A survey. IEEE Transactions on Artificial Intelligence, 2:109-127, 2021.
[74] J. Yang, Z. Liu, S. Xiao, C. Li, D. Lian, S. Agrawal, A. S, G. Sun, and X. Xie. Graphformers: GNNnested transformers for representation learning on textual graph. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, 2021.
[75] Z. Yang, W. W. Cohen, and R. Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. ArXiv, abs/1603.08861, 2016.
[76] L. Yao, C. Mao, and Y. Luo. Graph convolutional networks for text classification. ArXiv, abs/1809.05679, 2018.
[77] M. Yasunaga, A. Bosselut, H. Ren, X. Zhang, C. D. Manning, P. Liang, and J. Leskovec. Deep bidirectional language-knowledge graph pretraining. In Neural Information Processing Systems (NeurIPS), 2022.
[78] M. Yasunaga, J. Leskovec, and P. Liang. Linkbert: Pretraining language models with document links. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8003-8016, 2022.
[79] R. Ye, C. Zhang, R. Wang, S. Xu, and Y. Zhang. Natural language is all a graph needs. arXiv preprint arXiv:2308.07134, 2023.
[80] J. Zhang. Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt. arXiv preprint arXiv:2304.11116, 2023.
[81] Z. Zhang, X. Wang, Z. Zhang, H. Li, Y. Qin, S. Wu, and W. Zhu. Llm4dyg: Can large language models solve problems on dynamic graphs? arXiv preprint arXiv:2310.17110, 2023.
[82] Z. Zhang, A. Zhang, M. Li, and A. J. Smola. Automatic chain of thought prompting in large language models. ArXiv, abs/2210.03493, 2022.
[83] J. Zhao, M. Qu, C. Li, H. Yan, Q. Liu, R. Li, X. Xie, and J. Tang. Learning on large-scale text-attributed graphs via variational inference. In The Eleventh International Conference on Learning Representations, 2023.
[84] J. Zhao, L. Zhuo, Y. Shen, M. Qu, K. Liu, M. Bronstein, Z. Zhu, and J. Tang. Graphtext: Graph reasoning in text space. arXiv preprint arXiv:2310.01089, 2023.</p>
<p>[85] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, Y. Du, C. Yang, Y. Chen, Z. Chen, J. Jiang, R. Ren, Y. Li, X. Tang, Z. Liu, P. Liu, J. Nie, and J. rong Wen. A survey of large language models. ArXiv, abs/2303.18223, 2023.
[86] J. Zhu, Y. Cui, Y. Liu, H. Sun, X. Li, M. Pelger, L. Zhang, T. Yan, R. Zhang, and H. Zhao. Textgnn: Improving text encoder via graph neural network in sponsored search. Proceedings of the Web Conference 2021, 2021.
[87] J. Zhu, Y. Yan, L. Zhao, M. Heimann, L. Akoglu, and D. Koutra. Beyond homophily in graph neural networks: Current limitations and effective designs. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 7793-7804. Curran Associates, Inc., 2020.</p>
<h2>APPENDIX</h2>
<h2>A. DATASETS</h2>
<p>In this work, we mainly use the following five real-world graph datasets. Their statistics are shown in Table 19.</p>
<p>Table 19: Statistics of the graph datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;">#Nodes</th>
<th style="text-align: right;">#Edges</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Metric</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CORA [40]</td>
<td style="text-align: right;">2,708</td>
<td style="text-align: right;">5,429</td>
<td style="text-align: center;">7-class classif.</td>
<td style="text-align: center;">Accuracy</td>
</tr>
<tr>
<td style="text-align: left;">CiteSEER ${ }^{7}$ [15]</td>
<td style="text-align: right;">3,186</td>
<td style="text-align: right;">4,277</td>
<td style="text-align: center;">6-class classif.</td>
<td style="text-align: center;">Accuracy</td>
</tr>
<tr>
<td style="text-align: left;">Pubmed [57]</td>
<td style="text-align: right;">19,717</td>
<td style="text-align: right;">44,338</td>
<td style="text-align: center;">3-class classif.</td>
<td style="text-align: center;">Accuracy</td>
</tr>
<tr>
<td style="text-align: left;">OGbn-ARxiv [23]</td>
<td style="text-align: right;">169,343</td>
<td style="text-align: right;">$1,166,243$</td>
<td style="text-align: center;">40-class classif.</td>
<td style="text-align: center;">Accuracy</td>
</tr>
<tr>
<td style="text-align: left;">OGbn-Products [23]</td>
<td style="text-align: right;">$2,449,029$</td>
<td style="text-align: right;">$61,859,140$</td>
<td style="text-align: center;">47-class classif.</td>
<td style="text-align: center;">Accuracy</td>
</tr>
</tbody>
</table>
<h2>A. 1 Dataset Description</h2>
<p>In this part, we give a brief introduction to each graph dataset. It should be noted that it's cumbersome to get the raw text attributes for some datasets, and we will elaborate them below. The structural information and label information of these datasets can be achieved from Pyg ${ }^{6}$. We will also release the pre-processed versions of these datasets to assist future related studies.
Cora [40] CORA is a paper citation dataset with the following seven categories: ['Rule Learning', 'Neural Networks', 'Case Based', 'Genetic Algorithms', 'Theory', 'Reinforcement Learning', 'Probabilistic Methods']. The raw text attributes can be obtained from https://people.cs.umass. edu/ mccallum/data.html.
Citeseer [15] CiteSEER is a paper citation dataset with the following seven categories: ["Agents", "ML", "IR", "DB", "HCI", "AI"]. Note that we find that the TAG versopm only contains the text attributes for 3186 nodes. As a result, we take the graph consisted of these 3186 nodes with 4277 edges.
Pubmed [57] Pubmed is a paper citation dataset consisting scientific journals collected from the PubMed database with the following three categories: ['Diabetes Mellitus, Experimental', 'Diabetes Mellitus Type 1', 'Diabetes Mellitus Type 2'].</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Ogbn-arxiv and Ogbn-products [23] These dataset are selected from the popular OGB benchmark [23], and descriptions for these datasets can be found in https://ogb. stanford.edu/docs/nodeprop.</p>
<h2>B. EXPERIMENT SETUPS</h2>
<h2>B. 1 Computing Environment</h2>
<p>We implement all the baseline models with PyG [13], DGL [69], and transformers [71] modules. The experiments were conducted in a GPU server with eight NVIDIA RTX A5000 GPUs, each with 24GB VRAM.</p>
<h2>B. 2 Hyperparameters</h2>
<p>For RevGAT, GraphSage, and SAGN models, we directly adopt the best hyperparameters from the OGB leaderboard ${ }^{7}$. For Deberta-base on CORA and Pubmed, we follow the hyperparameter setting of TAPE [22]. In terms of GLEM, for the LM part, we follow the hyperparameter setting in their reporsitory ${ }^{8}$. For GCN, GAT, MLP, we use the following hyperparameter search range.
(a) Hidden dimension: ${8,16,32,64,128,256}$.
(b) Number of layers: ${1,2,3}$
(c) Normalization: {None, BatchNorm};
(d) Learning rate: ${1 \mathrm{e}-2,5 \mathrm{e}-2,5 \mathrm{e}-3,1 \mathrm{e}-3}$
(e) Weight Decay: ${1 \mathrm{e}-5,5 \mathrm{e}-5,5 \mathrm{e}-4,0}$
(f) Dropout: ${0 ., 0.1,0.5,0.8}$
(g) Number of heads for GAT: ${1,4,8}$</p>
<h2>C. DEMONSTRATIONS OF TAPE</h2>
<p>Examples for Pubmed After analyzing the Pubmed dataset, we find an interesting phenomenon that sometimes the label of the paper just appears in the raw text attributes. An example is shown in Table 20. This property of Pubmed may be related to the superior zero-shot performance of LLMs on this dataset. This can also potentially explain why GCN and GAT are outperformed by MLP in the high labeling ratio. When the link between node attributes and node labels can be easily found and adequate to determine the categories, incorporating neighbors coming from other categories will introduce noise.</p>
<p>Table 20: An illustrative example for Pubmed</p>
<p>Title: Predictive power of sequential measures of albuminuria for progression to ESRD or death in Pima Indians with type 2 diabetes.
... (content omitted here)
Ground truth label: Diabetes Mellitus Type 2</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://pytorch-geometric.readthedocs.io/en/ latest/modules/data.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7}$ https://github.com/snap-stanford/ogb
${ }^{8}$ https://github.com/AndyJZhao/GLEM&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
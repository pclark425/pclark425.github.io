<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1639 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1639</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1639</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-249625790</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2206.06282v2.pdf" target="_blank">Analysis of Randomization Effects on Sim2Real Transfer in Reinforcement Learning for Robotic Manipulation Tasks</a></p>
                <p><strong>Paper Abstract:</strong> Randomization is currently a widely used approach in Sim2Real transfer for data-driven learning algorithms in robotics. Still, most Sim2Real studies report results for a specific randomization technique and often on a highly customized robotic system, making it difficult to evaluate different randomization approaches systematically. To address this problem, we define an easy-to-reproduce experimental setup for a robotic reach-and-balance manipulator task, which can serve as a benchmark for comparison. We compare four randomization strategies with three randomized parameters both in simulation and on a real robot. Our results show that more randomization helps in Sim2Real transfer, yet it can also harm the ability of the algorithm to find a good policy in simulation. Fully randomized simulations and fine-tuning show differentiated results and translate better to the real robot than the other approaches tested.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1639.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1639.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>2-DoF KUKA reacher (this study)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>2-DoF KUKA LBR iiwa reach-and-balance agent (Josifovski et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning controlled 2-DOF reach-and-balance controller for a KUKA LBR iiwa manipulator trained in Unity with multiple randomization strategies to study Sim2Real transfer; evaluated both in-simulation and on the physical robot with measurements of cumulative reward and safety termination rates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>KUKA LBR iiwa (2-DOF subset)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A KUKA LBR iiwa 14 robotic manipulator (used here as a 2-joint system starting from a 'home' pose) controlled to move the end-effector to a target and hold it (reach-and-balance task); real robot controlled over ROS using the IIWA stack.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Unity (custom environment with OpenAI-Gym-like API)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Custom Unity simulation of the KUKA LBR iiwa manipulator using URDF meshes and a physics model (spring-damper joint control implementation) permitting parallel simulation of many robots for RL training; simulates joint dynamics, collisions with floor, sensor observations and supports injection of latency, torque parameter variations, and sensor/motor noise.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate physics (Unity-based) with configurable dynamics randomization (not a full high-fidelity system identification replica)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>joint dynamics via spring-damper model, commanded joint velocity tracking dynamics (stiffness/damping), collisions with floor, sensor readings, controllable latency effects (observation latency), and uniform noise added to sensor readings</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>no full system-identification-derived exact dynamics for the real robot by default (ideal sim assumes instantaneous commanded velocity), no photorealistic rendering (visual input not used), possible simplifications in contact/friction modeling relative to the real robot, and some randomization ranges intentionally unrealistic (not tuned to measured real values)</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical KUKA LBR iiwa manipulator (14-DOF robot used as 2-DOF task) in a laboratory setup controlled via ROS IIWA stack; evaluation used the same 50 randomly sampled target positions and identical starting pose and joint-velocity limits as in simulation; episodes terminate on safety joint limits or collisions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Reach-and-balance: move end-effector from home position to a target on a hemisphere and remain at the target until episode end (safe holding after reaching).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning using Proximal Policy Optimization (PPO) with 2-layer MLP actor/critic, parallelized environments (StableBaselines implementation).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Cumulative episode reward (summed per-timestep reward); also secondary metrics: number/percentage of episodes terminated due to joint limits (safety terminations), qualitative jitter around target, and comparison vs an inverse-kinematics (IK) baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Reported as cumulative reward. Example numbers from Table III: Ideal simulation average: -77.14 ± 4.88 (best model in-sim: -72.03); Fully randomized average: -107.00 ± 12.19 (best in-sim: -96.01); Fine-tuning averages around -81 ± ~4 (varies by randomization order).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Reported as cumulative reward for best evaluated models. Example numbers from Table III: Ideal real (best model): -142.50; Fully randomized real (best model): -109.10; Fine-tuning sequences produced real rewards in the approximate range -113 to -133 depending on parameter order. (Lower/ more negative reward = worse.)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Randomized parameters: Latency (observation delay drawn uniformly, linear interpolation applied to prior observations), Torque (joint stiffness and damping coefficients in spring-damper joint control), Noise (uniform sensor/motor noise added to observations). Randomization regimes tested: ideal (no randomization), fully randomized (all params randomized from start), fine-tuning (pretrain ideal → sequential single-parameter randomization), curriculum (pretrain ideal → progressively add parameters), ideal2randomized (pretrain ideal → switch to fully randomized). Randomization ranges chosen by increasing range until performance degraded >10%.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Mismatch between simulation dynamics and real dynamics (e.g., torque/stiffness/damping differences), observation/control latency differences, sensor noise, different maximum joint velocities used during training vs testing, overly strong/unrealistic randomization ranges that make exploration harder, and aspects not modeled at high fidelity (unmodeled friction/contact details, differences in timing and actuation response).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Two strategies enabled strong transfer: (1) Fully randomized training (training from scratch with all parameter randomization) — yielded smallest sim-to-real differential and best real performance in these experiments; (2) Fine-tuning: pretrain in ideal simulation then sequentially add/enable randomization (one parameter at a time) — produced consistent and robust real-world performance across randomization orders and was less sensitive to mismatched training/testing joint velocities.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>No single numeric fidelity threshold was claimed; qualitative findings: (a) modeling latency, torque response, and sensor noise matters (these were the randomization parameters used); (b) excessive/randomization intensity can prevent convergence or reduce performance, especially when exploration is slowed (e.g., lower joint velocities); (c) fully randomized simulation produced better predictivity of real-world performance (real results fell within simulation confidence intervals) despite lower in-sim performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Compared ideal (no randomization) vs progressively randomized regimes: fully randomized training performed worst in-simulation (lower cumulative reward, larger variance) but had the best/most predictable transfer to the real robot (small sim-real gap); fine-tuning produced consistent real-world performance (robust across randomization order) though its simulated performance did not reliably predict real performance; curriculum and ideal2randomized strategies often performed well in-sim but transferred poorly to the real robot (worse than even ideal simulation in some cases).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Domain randomization (latency, torque/stiffness/damping, sensor noise) can improve Sim2Real robustness: fully randomized agents yielded the smallest sim-to-real gap and best real-world performance despite worse simulation reward and larger variance. 2) Excessive or poorly matched randomization can hinder learning (reduce convergence and final performance), especially when exploration is constrained (e.g., low joint speeds). 3) Sequential approaches (fine-tuning: ideal pretraining followed by single-parameter randomization) provide a good trade-off — consistent and robust real-world performance across randomization orders while avoiding the high training difficulty of full randomization. 4) Simulation performance did not always predict real performance; fully randomized models produced more predictive confidence intervals than other strategies. 5) Additional real-world experiments and exploration of more complex tasks are needed to generalize conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>name_short_addition</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysis of Randomization Effects on Sim2Real Transfer in Reinforcement Learning for Robotic Manipulation Tasks', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World <em>(Rating: 2)</em></li>
                <li>Sim-to-Real Transfer of Robotic Control with Dynamics Randomization <em>(Rating: 2)</em></li>
                <li>Solving Rubik's Cube with a Robot Hand <em>(Rating: 2)</em></li>
                <li>Sim-to-Real: Learning Agile Locomotion for Quadruped Robots <em>(Rating: 2)</em></li>
                <li>Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance? <em>(Rating: 2)</em></li>
                <li>3D Simulation for Robot Arm Control with Deep Q-Learning <em>(Rating: 1)</em></li>
                <li>Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection <em>(Rating: 1)</em></li>
                <li>Sim-to-Real Via Sim-to-Sim: Data-Efficient Robotic Grasping Via Randomized-to-Canonical Adaptation Networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1639",
    "paper_id": "paper-249625790",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "2-DoF KUKA reacher (this study)",
            "name_full": "2-DoF KUKA LBR iiwa reach-and-balance agent (Josifovski et al., 2022)",
            "brief_description": "A reinforcement-learning controlled 2-DOF reach-and-balance controller for a KUKA LBR iiwa manipulator trained in Unity with multiple randomization strategies to study Sim2Real transfer; evaluated both in-simulation and on the physical robot with measurements of cumulative reward and safety termination rates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "KUKA LBR iiwa (2-DOF subset)",
            "agent_system_description": "A KUKA LBR iiwa 14 robotic manipulator (used here as a 2-joint system starting from a 'home' pose) controlled to move the end-effector to a target and hold it (reach-and-balance task); real robot controlled over ROS using the IIWA stack.",
            "domain": "general robotics manipulation",
            "virtual_environment_name": "Unity (custom environment with OpenAI-Gym-like API)",
            "virtual_environment_description": "Custom Unity simulation of the KUKA LBR iiwa manipulator using URDF meshes and a physics model (spring-damper joint control implementation) permitting parallel simulation of many robots for RL training; simulates joint dynamics, collisions with floor, sensor observations and supports injection of latency, torque parameter variations, and sensor/motor noise.",
            "simulation_fidelity_level": "approximate physics (Unity-based) with configurable dynamics randomization (not a full high-fidelity system identification replica)",
            "fidelity_aspects_modeled": "joint dynamics via spring-damper model, commanded joint velocity tracking dynamics (stiffness/damping), collisions with floor, sensor readings, controllable latency effects (observation latency), and uniform noise added to sensor readings",
            "fidelity_aspects_simplified": "no full system-identification-derived exact dynamics for the real robot by default (ideal sim assumes instantaneous commanded velocity), no photorealistic rendering (visual input not used), possible simplifications in contact/friction modeling relative to the real robot, and some randomization ranges intentionally unrealistic (not tuned to measured real values)",
            "real_environment_description": "Physical KUKA LBR iiwa manipulator (14-DOF robot used as 2-DOF task) in a laboratory setup controlled via ROS IIWA stack; evaluation used the same 50 randomly sampled target positions and identical starting pose and joint-velocity limits as in simulation; episodes terminate on safety joint limits or collisions.",
            "task_or_skill_transferred": "Reach-and-balance: move end-effector from home position to a target on a hemisphere and remain at the target until episode end (safe holding after reaching).",
            "training_method": "Reinforcement learning using Proximal Policy Optimization (PPO) with 2-layer MLP actor/critic, parallelized environments (StableBaselines implementation).",
            "transfer_success_metric": "Cumulative episode reward (summed per-timestep reward); also secondary metrics: number/percentage of episodes terminated due to joint limits (safety terminations), qualitative jitter around target, and comparison vs an inverse-kinematics (IK) baseline.",
            "transfer_performance_sim": "Reported as cumulative reward. Example numbers from Table III: Ideal simulation average: -77.14 ± 4.88 (best model in-sim: -72.03); Fully randomized average: -107.00 ± 12.19 (best in-sim: -96.01); Fine-tuning averages around -81 ± ~4 (varies by randomization order).",
            "transfer_performance_real": "Reported as cumulative reward for best evaluated models. Example numbers from Table III: Ideal real (best model): -142.50; Fully randomized real (best model): -109.10; Fine-tuning sequences produced real rewards in the approximate range -113 to -133 depending on parameter order. (Lower/ more negative reward = worse.)",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Randomized parameters: Latency (observation delay drawn uniformly, linear interpolation applied to prior observations), Torque (joint stiffness and damping coefficients in spring-damper joint control), Noise (uniform sensor/motor noise added to observations). Randomization regimes tested: ideal (no randomization), fully randomized (all params randomized from start), fine-tuning (pretrain ideal → sequential single-parameter randomization), curriculum (pretrain ideal → progressively add parameters), ideal2randomized (pretrain ideal → switch to fully randomized). Randomization ranges chosen by increasing range until performance degraded &gt;10%.",
            "sim_to_real_gap_factors": "Mismatch between simulation dynamics and real dynamics (e.g., torque/stiffness/damping differences), observation/control latency differences, sensor noise, different maximum joint velocities used during training vs testing, overly strong/unrealistic randomization ranges that make exploration harder, and aspects not modeled at high fidelity (unmodeled friction/contact details, differences in timing and actuation response).",
            "transfer_enabling_conditions": "Two strategies enabled strong transfer: (1) Fully randomized training (training from scratch with all parameter randomization) — yielded smallest sim-to-real differential and best real performance in these experiments; (2) Fine-tuning: pretrain in ideal simulation then sequentially add/enable randomization (one parameter at a time) — produced consistent and robust real-world performance across randomization orders and was less sensitive to mismatched training/testing joint velocities.",
            "fidelity_requirements_identified": "No single numeric fidelity threshold was claimed; qualitative findings: (a) modeling latency, torque response, and sensor noise matters (these were the randomization parameters used); (b) excessive/randomization intensity can prevent convergence or reduce performance, especially when exploration is slowed (e.g., lower joint velocities); (c) fully randomized simulation produced better predictivity of real-world performance (real results fell within simulation confidence intervals) despite lower in-sim performance.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Compared ideal (no randomization) vs progressively randomized regimes: fully randomized training performed worst in-simulation (lower cumulative reward, larger variance) but had the best/most predictable transfer to the real robot (small sim-real gap); fine-tuning produced consistent real-world performance (robust across randomization order) though its simulated performance did not reliably predict real performance; curriculum and ideal2randomized strategies often performed well in-sim but transferred poorly to the real robot (worse than even ideal simulation in some cases).",
            "key_findings": "1) Domain randomization (latency, torque/stiffness/damping, sensor noise) can improve Sim2Real robustness: fully randomized agents yielded the smallest sim-to-real gap and best real-world performance despite worse simulation reward and larger variance. 2) Excessive or poorly matched randomization can hinder learning (reduce convergence and final performance), especially when exploration is constrained (e.g., low joint speeds). 3) Sequential approaches (fine-tuning: ideal pretraining followed by single-parameter randomization) provide a good trade-off — consistent and robust real-world performance across randomization orders while avoiding the high training difficulty of full randomization. 4) Simulation performance did not always predict real performance; fully randomized models produced more predictive confidence intervals than other strategies. 5) Additional real-world experiments and exploration of more complex tasks are needed to generalize conclusions.",
            "name_short_addition": null,
            "uuid": "e1639.0",
            "source_info": {
                "paper_title": "Analysis of Randomization Effects on Sim2Real Transfer in Reinforcement Learning for Robotic Manipulation Tasks",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Sim-to-Real Transfer of Robotic Control with Dynamics Randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Solving Rubik's Cube with a Robot Hand",
            "rating": 2,
            "sanitized_title": "solving_rubiks_cube_with_a_robot_hand"
        },
        {
            "paper_title": "Sim-to-Real: Learning Agile Locomotion for Quadruped Robots",
            "rating": 2,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        },
        {
            "paper_title": "Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?",
            "rating": 2,
            "sanitized_title": "sim2real_predictivity_does_evaluation_in_simulation_predict_realworld_performance"
        },
        {
            "paper_title": "3D Simulation for Robot Arm Control with Deep Q-Learning",
            "rating": 1,
            "sanitized_title": "3d_simulation_for_robot_arm_control_with_deep_qlearning"
        },
        {
            "paper_title": "Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection",
            "rating": 1,
            "sanitized_title": "learning_handeye_coordination_for_robotic_grasping_with_deep_learning_and_largescale_data_collection"
        },
        {
            "paper_title": "Sim-to-Real Via Sim-to-Sim: Data-Efficient Robotic Grasping Via Randomized-to-Canonical Adaptation Networks",
            "rating": 1,
            "sanitized_title": "simtoreal_via_simtosim_dataefficient_robotic_grasping_via_randomizedtocanonical_adaptation_networks"
        }
    ],
    "cost": 0.010183749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Analysis of Randomization Effects on Sim2Real Transfer in Reinforcement Learning for Robotic Manipulation Tasks
9 Oct 2022</p>
<p>Josip Josifovski josip.josifovski@tum.de 
Department of Informatics
Technical University of Munich
Germany</p>
<p>Mohammadhossein Malmir 
Department of Informatics
Technical University of Munich
Germany</p>
<p>Noah Klarmann 
Rosenheim University of Applied Sciences
RosenheimGermany</p>
<p>Bare Luka Žagar 
Department of Informatics
Technical University of Munich
Germany</p>
<p>Nicolás Navarro-Guerrero 
Deutsches Forschungszentrum für Künstliche Intelligenz GmbH
Ger-many</p>
<p>Alois Knoll 
Department of Informatics
Technical University of Munich
Germany</p>
<p>Germany, ItalyAustria, Czech Republic, Latvia, Belgium, Lithuania, France, Greece, Finland, and Norway</p>
<p>Analysis of Randomization Effects on Sim2Real Transfer in Reinforcement Learning for Robotic Manipulation Tasks
9 Oct 2022B84F638F252191C348564F7B6A77E4D6arXiv:2206.06282v2[cs.RO]
Randomization is currently a widely used approach in Sim2Real transfer for data-driven learning algorithms in robotics.Still, most Sim2Real studies report results for a specific randomization technique and often on a highly customized robotic system, making it difficult to evaluate different randomization approaches systematically.To address this problem, we define an easy-to-reproduce experimental setup for a robotic reach-and-balance manipulator task, which can serve as a benchmark for comparison.We compare four randomization strategies with three randomized parameters both in simulation and on a real robot.Our results show that more randomization helps in Sim2Real transfer, yet it can also harm the ability of the algorithm to find a good policy in simulation.Fully randomized simulations and fine-tuning show differentiated results and translate better to the real robot than the other approaches tested.</p>
<p>I. INTRODUCTION</p>
<p>Current research in reinforcement learning (RL) shows impressive results in disembodied environments, where RL agents outperform humans in video games [1], Go [2], or develop creative strategies for multi-agent hide-and-seek scenarios [3].However, applying such models to control real robots is more challenging.For example, real-world training of a data-driven model [4] can be impractical or unsafe in many cases where environments are dynamic or computational, and hardware resources are limited, which is often the case in robotics.Simulations seem like a viable alternative for training robots -with the ever-increasing simulation accuracy and computational speedup, virtually unlimited training data is available to train RL agents for robotic tasks.There are many simulation platforms [5], [6] [7], [8], [9] that target robotic simulations both in research and in industrial context.However, simulations are always an approximation of the real environment.Thus, models trained in simulation often fail in the real world, or their performance decreases considerably, which is known as the reality gap problem.</p>
<p>Currently, two predominant approaches for minimizing the reality gap are developing highly realistic simulations or using imperfect simulations in combination with randomization strategies [10].Developing highly realistic simulations depends on processes like system identification, which are often hardware-specific, and the parameters identified with such processes might vary significantly based on conditions in the environment [11].Furthermore, it is not always easy to determine which real-world phenomena and to what level of precision should be modeled in highly realistic simulations.</p>
<p>On the contrary, randomization strategies take advantage of the generalization capabilities of neural networks and randomize aspects of the task that are challenging to measure or model precisely [12].The general assumption, in this case, is that the actual real-world parameters are covered in the randomization ranges or that models trained on randomized data learn more robust policies.Arguably, randomization strategies are more applicable in general than learning on highly realistic simulations because the policies learned under randomization might adapt and work in dynamic environments or settings not encountered during training.However, randomization strategies come at the cost of a more significant number of training samples to learn a task.Moreover, randomization can lead to sub-optimal solutions or even prevent learning, which can be attributed to the inherent unpredictability of the randomized simulations.Therefore, it is essential to quantify the effects of randomization on the learning of policies that would transfer best to the real world.Such systematic studies are still lacking in the field -while there are many successful examples of Sim2Real transfer, they are usually a one-time implementation on a highlycustomized system, which hinders experiment reproducibility, benchmarking, or comparative analysis of Sim2Real transfer approaches.</p>
<p>Thus, here we present a comparative analysis of training strategies with different levels of randomization for a robotic reaching task.The chosen task is sufficiently simple to train RL models quickly, while it is still complex enough to capture the effects of the reality gap.Moreover, a deterministic solution for the task exists and can be used as a baseline.The simulation environments and code, together with a demonstration video, have been made available to encourage reproducibility at https://josifovski.github.io/sim2realrandomization-effects/.</p>
<p>II. BACKGROUND AND RELATED WORK</p>
<p>Training in simulation for robotic control and the reality gap problem have been addressed in many earlier research studies [13], [14], [15], [16].More recently, with the rise of deep learning and the advances in computation, Sim2Real transfer became a common practice in fields like computer vision [12], [17], where synthetic data can be used to train models in simulation completely or to improve their performance when combined with real data.In the last few years, bringing reinforcement learning in combination with deep learning and improved simulations opened the possibility of using Sim2Real transfer effectively in the robotics domain.Moreover, it allowed simulated experiences to replace some of the hard-to-acquire robotic experiences on a real robot.A recent overview of the Sim2Real approaches in robotics is presented by Zhao et al. [11].</p>
<p>Some of the Sim2Real approaches aim to precisely reproduce the relevant real-world phenomena in simulation [18], [19].For instance, James and Johns [18] trained a 7-DOF robotic arm in simulation for locating and grasping a cube by adjusting the simulation to resemble the real world as closely as possible.Similarly, Kaspar et al. [19] show successful Sim2Real transfer for a peg in-hole task with a 7-DOF robotic arm by training only in simulation, where they perform system identification to resemble the real robot's dynamics in simulation accurately.</p>
<p>As matching and correctly simulating all relevant realworld phenomena for a robotic task is often complicated to achieve, other approaches [20], [12], [21], [22] rely on randomization of the critical simulation parameters during model training to increase model robustness and minimize the effects of the mismatch between the simulation and the real world.For example, Peng et al. [20] trained a robotic arm to perform a pushing task by randomizing the robot's physical properties and the simulation's dynamics.Similarly, Tobin et al. [12] employed domain randomization to create millions of unrealistically-appearing training images for robust object localization in a robotic grasping task.In the same direction, James et al. [22] combined a domain randomization technique with a generative model to facilitate the learning to grasp unknown objects with a robotic arm.OpenAI [21] successfully transferred policies learned only in simulation to a real robotic hand to solve a Rubic's cube.They achieved this by automated domain randomization, a form of curriculum learning where the randomization ranges become increasingly wider to make the model learn more general solutions instead of overfitting to a specific randomization seen during training.An overview of randomization techniques in robot learning for Sim2Real transfer was presented by Muratore et al. [23].</p>
<p>Many Sim2Real approaches employ domain adaptation, where the model trained in simulation is fine-tuned on the real robot as a final step to improve its performance.The domain adaptation can also be framed as a continual learning problem by considering the same robotic task in simulation and the real world as two separate tasks to solve.For example, Rusu et al. [24] trained a model for a reaching task in simulation and then reused the model's features to train a second model in the real world.They reported that due to the reusability of the skills learned in simulation, the additional training steps in the real world are drastically decreased.</p>
<p>While domain adaptation is a viable last step, minimizing the time needed to train on a real robot or circumvent it entirely is always important.Thus, it is necessary to measure how different algorithms' properties or training strategies influence the difficulty of learning a task in simulation and how the performance in simulation relates to the performance on a real robot.In this direction, Kadian et al. [25] developed a complex setup of a real environment and 3D scan it to create a virtual replica to measure Sim2Real predictivity for a visual navigation robotic task.</p>
<p>Similarly, here we use a real environment and a virtual replica to quantify the Sim2Real transfer.We also describe how well the simulation performance predicts the performance in a real environment.Unlike Kadian et al. [25], we focus on randomization strategies and their effects on learning a robotic manipulator task, with an accent on environment simplicity in order to mitigate confounding effects and ease experiment reproducibility.</p>
<p>III. METHODOLOGY</p>
<p>We first formally specify a robotic manipulation task suitable for evaluating Sim2Real transfer.Then we describe the randomization strategies used.Finally, we explain the evaluation procedure in both simulation and real-world setups.</p>
<p>A. Task Formulation</p>
<p>The task is to learn the differential inverse kinematics of a 2-joint robot, always starting from the same 'home' position -the robot is upright with both joints at 0 degrees rotation.The end-effector should be moved to a specific reachable target position by continuously controlling the joint velocities.Typical RL reaching benchmarks and implementations (e.g., [26]- [28]) only consider the goal of reaching the target location.However, there is no consideration for what happens after achieving the goal, which poses potential safety issues when transferring the behavior to the real world.Thus, we require the agent to learn to reach and stay at the specified target until the end of the episode.We call this the reachand-balance task:
s t = [∆x t , ∆y t , ∆z t , q 1 , q 2 ] ∈ R 5 (1)
s t is the state of the agent at a specific timestep, ∆x, ∆y and ∆z are the distances between the end-effector and the target along the corresponding coordinate axis, q 1 and q 2 are the joint positions.The actions the agent can perform are specified by:
a t = [ q 1 , q 2 ] ∈ R 2(2)
where qi are the angular velocities for the first and the second joint starting from the robot's home position.The reward function is defined as:
r t = −d t if C is f alse −d t * (T − t) otherwise ∈ R(3)
where d t is the Euclidean distance between the target and the end-effector, T is the episode duration, and C is the terminal condition.During training, C becomes true when the robot collides with the floor.However, during evaluation, C becomes true if any of the joints reach a predefined safety rotation limit.The goal of the agent is to maximize the episode reward:
R = T t=1 r t(4)</p>
<p>B. Training Strategies</p>
<p>To investigate randomization effects, we selected different randomization strategies based on their conceptual differences, see Figure 1: • Ideal simulation -The simulation environment does not include any randomization parameter relevant for Sim2Real transfer.</p>
<p>IV. EXPERIMENT SETUP</p>
<p>The simulation environment [29] is developed with Unity [30] and has a comparable API to OpenAI Gym [5].It allows parallel simulation of many robots at once to speed-up the training process.We used the KUKA LBR iiwa 14 robot manipulator [31] due to its ubiquity in both the research and industrial domain.The simulated robot parameters and meshes are obtained from the URDF data provided by ROS-Industrial [32].The real robot is controlled via ROS using the IIWA stack [33].</p>
<p>A. Randomization Parameters</p>
<p>We select three parameters that are commonly used in Sim2Real transfer in robotics [20] [34], [11], i.e., Latency (L), Torque (T ), and Noise (N ).</p>
<p>Latency (L): In an ideal simulation condition, there is no latency.For the randomized case, inspired by Tan et al. [34], we collect a list of ideal observations for every timestep.Then, a random latency value is drawn from a uniform distribution.A linear interpolation of the two consecutive ideal observations closest to the timestep (t ) resulting from the current timestep (t) minus the latency is used to determine the latency-affected state of the agent.If the resulting state (s t ) precedes the latency-affected state from the previous timestep (s t −1 ), then the same latencyaffected state from s t −1 is used.</p>
<p>Torque (T ): The joint velocity control is implemented using the spring-damper equation as explained in Unity Robotics Hub [35].Here, we randomize the stiffness and damping coefficients for the joints, which results in a randomized torque profile for reaching the commanded velocity, similarly to Peng et al. [20].On the contrary, the commanded joint velocity is reached instantaneously in an ideal simulation condition.</p>
<p>Noise (N ): Random noise drawn from a uniform distribution is added to simulate sensor reading noise [20], [34].</p>
<p>The ranges for the randomization parameters can be defined based on system identification, or unrealistic ranges can be used, expecting that the true system values are included within the range.The latter is used because we believe this technique can generalize better and thus more effectively cope with the Sim2Real transfer.An agent trained in an ideal simulation was used to determine the range of the randomization parameters.The range of the corresponding parameter was determined by increasing the parameter range until the agent's performance degraded by more than 10%.Table I summarizes the values for all three randomization parameters.Figure 2 shows a visual representation of the randomization parameters.</p>
<p>B. Training Procedure</p>
<p>We used Proximal Policy Optimization (PPO) [36] for all the experiments.PPO is commonly used across many different domains, thus making it suitable for comparative analysis.</p>
<p>Another benefit of PPO is that it uses a conservative clipped objective function that prevents large policy changes between two consecutive updates.Clipped updates are especially useful since the inherent uncertainty in the observations due to randomization might lead to large policy updates based on incorrect observations.We use the implementation from StableBaselines [37], which allows parallelization of the training many environments simultaneously.The network architecture for both the Actor and the Critic is a 2-layer MLP with 64 units per layer and a tanh activation function.The hyperparameters for a similar robotic task, the ReacherBulletEnv-v0 by Raffin et al. [38] were used.However, the clipping range was changed after preliminary test to 0.1 to reduce the variance during randomization training.The number of parallel timesteps per update was also proportionally updated from 2048 to 256, as we train on 64 environments in parallel.Table II   Episode length is 250 timesteps, where one timestep is 20ms of simulation time, which amounts to 5 seconds of simulation time per episode during training with a control frequency of 50 Hz.</p>
<p>The max joint speed was set to 1 rad /sec.However, this can be exceeded when using the Torque randomization parameter.</p>
<p>All models are trained for a total of 40 million timesteps for five different initializations.For each strategy and initialization, the targets were randomly sampled along the halfsphere around the robot, starting from a height of 10 cm above the floor.While the number of total training steps for each run is the same, the number of training targets might differ since some episodes might terminate earlier due to collisions.</p>
<p>Under the ideal strategy, the model is trained entirely on non-randomized simulation, while for the randomized strategy all parameters are randomized for the whole duration of the training.For the fine-tuning, curriculum, and ideal2randomized strategies, a model pre-trained for 31 million timesteps under the ideal strategy is used as the starting point and trained for additional 9 million timesteps.This division is based on the approach of determining the randomization ranges described in IV-A, i.e., for the sequential training strategies we use about 10% of the training time per randomization parameter w.r.t. the training time of the pre-trained ideal model we start from.Under the fine-tuning strategy, the pre-trained model is sequentially trained on three randomized simulations for 3 million timesteps per simulation, where only one randomization parameter is enabled at a time.Similarly, for the curriculum strategy, the pre-trained model is further trained on a sequence of three randomized simulations for 3 million timesteps per simulation, where an additional randomization parameter is enabled each time, leading from less to more randomized training.Under the fine-tuning and curriculum strategies, the parameter randomization sequence might affect the final performance; hence we test all the permutations of the randomization parameters.Lastly, under the ideal2randomized strategy, the pre-trained model on ideal simulation is further trained directly on fullyrandomized simulation for 9 million timesteps.</p>
<p>C. Evaluation Procedure</p>
<p>The evaluation procedure is illustrated in Figure 1b.While the training takes place only in simulation, the evaluation takes place both in simulation (ideal simulation setting) and on the real robot, see Figure 3.The conditions for evaluating the trained model both on the simulated and the real robot are identical.Namely, the robot always starts at the 'home' position, and the same 50 randomly generated target positions are used in simulation and on the real robot.The same maximal joint speed of π/9 rad /sec (20 deg /sec) is used.The maximum allowed position for joint 1 is ±150 degrees and ±100 degrees for joint 2.An episode duration of 500 timesteps (10 seconds) is used to give enough time to the robot to reach and balance on the farthest targets.However, the episode can also terminate if any joint reaches the predefined safety range, 20 degrees off the corresponding mechanical limit of the joint.</p>
<p>V. EXPERIMENTAL RESULTS</p>
<p>Figure 4 shows the learning evolution of the different strategies measured as a cumulative reward.The results consist of an average of 5 different random initializations.Except for the fully randomized strategy, all strategies have a small standard deviation and converge to a comparable value, approximately 4% off the IK solver performance w.r.t. a random agent.The fully randomized reaches a performance 14% lower than the baseline and has a more substantial standard deviation between different initializations.The fully randomized strategy's improvement rate is also lower than the other strategies.All strategies show a performance decline early in training, starting when the agent collides with the floor.The duration of this performance decline indicates how long each strategy takes to avoid collisions.</p>
<p>Table III shows a summary of the results.Although the fully randomized strategy has the worst performance during training, it has the smallest differential between simulation and reality.Moreover, its results on the real robot are within the intervals of the training results.Furthermore, it leads to the best performance on the real robot.</p>
<p>On the contrary, the real-world performance for all other randomization strategies is substantially different from their simulation results, which the standard deviation values cannot account for.</p>
<p>From the randomization strategies that start with a pretrained model as a basis, the fine-tuning strategy consistently achieves the best or comparable performance to the other strategies on the real robot regardless of the randomization order.Only the TLN randomization order leads to substantially worse performance for the fine-tuning strategy, but this result is still within the range of results obtained with the other randomization strategies.</p>
<p>Curriculum learning and ideal2randomized show a sub-stantial difference in their simulation results and lead to worse real-world performance than the ideal simulation condition.The two best randomization strategies, i.e., full randomization and fine-tuning, were further investigated.The experimental setup was kept identical, except for the maximal joint velocities, which were decreased to π/9 rad /sec during training to match the joint velocity during testing.The episode length was increased to 500 timesteps to allow enough time for solving the task at a lower speed.</p>
<p>Figure 5 shows that a lower joint velocity makes the problem more challenging because exploring the whole search space takes longer.However, the same trends as for the 1 rad/sec training condition were observed.The fine-tuning strategy converges to a comparable value to the ideal simulation case and is approximately 5% off the baseline w.r.t. the random agent performance.The fully randomized strategy converges to a substantially lower cumulative reward value of approximately 51% off the baseline and at a slower rate than when trained at a speed of 1 rad/sec.All the strategies also show a decline in performance early in training due to the collision to the floor.This time learning to avoid collisions takes longer than the initial setup due to the decreased joint speeds.</p>
<p>Table IV summarizes the results for π/9 rad /sec joint speed in training for both in simulation and on the real robot.</p>
<p>The performance for the fine-tuning strategy remains practically unchanged both in simulation and on the real robot.In this case, the performance on the real robot also falls outside the confidence intervals determined in the simulations.However, the fully-randomized agents cannot achieve the same performance in either the simulation or the real robot.Moreover, the simulation and the real robot performance substantially deteriorate when training at a lower joint speed.However, the performance on the real robot still falls within the confidence intervals determined in the simulations.</p>
<p>VI. DISCUSSION</p>
<p>In qualitative terms, we observed that the fully-randomized model reached and stabilized near the target location for most of the targets during the evaluation on the real robot.In contrast, the models of the other strategies showed a visible jitter around the target location, which was subjectively worse with the curriculum and the ideal2randomized models.In addition, there was a significant difference in the number of times the episode terminated due to joint limits being reached in the first experiment: 52% for ideal2randomized, 46% for ideal simulation, and 28.7% for curriculum learning, while only 8% for the fully randomized simulation and 4.33% for the fine-tuning models.For the second experiment, on the other hand, when the training scenario was more similar to the evaluation scenario due to the same joint velocity, the model trained under ideal simulation conditions terminated the least due to reaching joint limits (10%), followed by 16% for randomized and 20.3% for the fine-tuning strategy.The qualitative differences are visible in the supplementary video material.</p>
<p>While the randomized strategy could find a suitable policy under the first experiment, reducing the joint speeds in combination with the unchanged randomization ranges made the exploration under fully-randomized settings significantly more challenging in the second experiment.The lower joint velocity leads to notably worse model performance both in simulation and on the real robot.This worse performance indicates that strong randomization can reduce the model convergence rate or the ability to find a suitable policy under some conditions.</p>
<p>Interestingly, the fine-tuning strategy performed similarly under both joint velocity conditions.Moreover, it led to overall lower episode terminations due to safety limits when training and testing velocity conditions differed.Therefore, starting from a naive solution in ideal simulation followed by sequential randomization can be an alternative to fullyrandomized training for Sim2Real transfer.Especially when the ranges are not based on system identification and might be unrealistic, or many randomization parameters are combined simultaneously.</p>
<p>VII. CONCLUSION</p>
<p>We investigated the randomization effects on Sim2Real Transfer in reinforcement learning for a 2 DoFs robot reaching task.Four different randomization strategies were tested: fine-tuning, curriculum learning, ideal2randomized, and fully randomized simulations.Three commonly used parameters in Sim2Real transfer in robotics were randomized: latency, torque, and sensor and motor noise.</p>
<p>Curriculum learning and ideal2randomized despite obtaining comparable results in simulation as fine-tuning and the naive ideal simulation did not transfer well onto the real robot and consistently lead to worse performance than the ideal simulation.</p>
<p>Our results show that fully-randomized simulations can lead to the best real robot performance, although not always.Despite that, we believe it is a promising strategy to bridge the reality gap because the real robot performance consistently falls within the confidence interval obtained in simulations.However, inappropriately intensive randomization might negatively affect the overall performance, even below the ideal simulation condition.</p>
<p>On the contrary, the fine-tuning strategy leads to a consistent real robot's performance regardless of randomization order or the intensity of the randomization.However, the real robot's performance cannot be anticipated based on the simulated results of the fine-tuning strategy.</p>
<p>Finally, additional experiments on the real robot are necessary to determine whether these results are statistically significant.</p>
<p>a) Future Work: We plan to determine the effect size and explore the generalization of our results by including more complex robotic tasks involving contact, like pushing, grasping, or pick &amp; place.We would also like to explore the effect of design decisions on Sim2Real performance, akin to the joint velocity used for training, which can severely impact the performance of some strategies.We want to explore sequential randomization strategies further because they seem to lead to a more consistent real-world performance but have a more significant reality gap than fully-randomized strategies.Finally, we plan to work with end-to-end robotic models relying on visual input, and analyze visual domain randomization under a similar methodology.</p>
<p>Fig. 1 .
1
Fig. 1.Methodology a) Different training strategies.Each shape represents a different randomization parameter and whether it is enabled or not during training.b) The simulation-trained model is evaluated under equivalent conditions in simulation and on the real robot.</p>
<p>• Fine-tuning -It uses a model pre-trained on an ideal simulation as the starting point.The model is further trained sequentially for each randomization parameters of interest, one at a time.•Curriculum learning -It uses a model pre-trained on an ideal simulation as the starting point.The model is further trained sequentially with an increasing number of randomized parameters.• ideal2randomized -It uses a model pre-trained on an ideal simulation as the starting point.The model is further trained with all the randomized parameters at once.• Randomized simulation -A model is trained from scratch, with all the randomization parameters at once.• Inverse kinematics -We use an inverse kinematics solver as a baseline.It is a deterministic strategy.</p>
<p>Fig. 3 .
3
Fig. 3. Left: The simulated environment.Right: The real environment.</p>
<p>Fig. 4 .
4
Fig. 4. Training progress for 1 rad /sec measured every 10 6 timesteps with the same random targets.The IK solver in green is the baseline.The shaded area represents 95% confidence intervals.</p>
<p>Fig. 5 .
5
Fig.5.Training progress for π/9 rad /sec joint speed measured every 10 6 timesteps with the same random targets.The IK solver in green is the baseline.The shaded area represents 95% confidence intervals.</p>
<p>provides a summary of all the relevant training hyperparameters.</p>
<p>TABLE III MEAN
III
REWARD FOR DIFFERENT RANDOMIZATION STRATEGIES.
Avg. inBest ModelBest ModelStrategySeq.Simulation(Simulation)(Real)idealN/A-77.14 ± 4.88-72.03-142.50fine-tuningN/A-81.02 ± 4.31N/AN/AT N L-79.45 ± 1.58-76.66-113.82T LN-83.92 ± 3.81-80.38-132.71N T L-84.30 ± 5.91-77.69-114.94N LT-78.58 ± 2.72-74.90-115.21LT N-80.98 ± 2.27-78.74-119.74LN T-78.89 ± 5.49-74.55-113.01curriculumN/A-84.21 ± 4.35N/AN/AT N L-84.05 ± 3.33-79.85-118.85T LN-87.61 ± 6.09-81.83-139.47N T L-85.55 ± 3.75-79.40-154.58N LT-83.74 ± 5.68-76.99-129.13LT N-82.59 ± 2.99-77.67-171.78LN T-81.71 ± 2.42-78.99-159.85ideal2randomN/A-81.46 ± 3.48-76.54-167.53randomizedN/A -107.00 ± 12.19-96.01-109.10IK solverN/A-66.60-66.60-77.52
This work has been financially supported by AI4DI project, which has received funding from the ECSEL Joint Undertaking (JU) under grant agreement No. 826060.The JU receives support from the European Union's Horizon 2020 research and innovation programme,
Playing Atari with Deep Reinforcement Learning. V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, NIPS: Deep Learning Workshop. 2013</p>
<p>Mastering the Game of Go with Deep Neural Networks and Tree Search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, S Dieleman, D Grewe, J Nham, N Kalchbrenner, I Sutskever, T Lillicrap, M Leach, K Kavukcuoglu, T Graepel, D Hassabis, Nature. 52975872016</p>
<p>Emergent Tool Use from Multi-Agent Autocurricula. B Baker, I Kanitscheider, T Markov, Y Wu, G Powell, B Mcgrew, I Mordatch, International Conference on Learning Representations (ICLR), ser. Eight, Virtual from Addis Ababa. Ethiopia2020</p>
<p>Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection. S Levine, P Pastor, A Krizhevsky, J Ibarz, D Quillen, The International Journal of Robotics Research. 374-52018</p>
<p>G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, arXiv:1606.01540OpenAI Gym. 2016</p>
<p>Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning. T Yu, D Quillen, Z He, R Julian, K Hausman, C Finn, S Levine, Conference on Robot Learning (CoRL). PMLR2020100</p>
<p>Isaac Gym: High Performance GPU Based Physics Simulation for Robot Learning. V Makoviychuk, L Wawrzyniak, Y Guo, M Lu, K Storey, M Macklin, D Hoeller, N Rudin, A Allshire, A Handa, G State, Conference on Neural Information Processing Systems (NeurIPS), ser. Datasets and Benchmarks Track, Virtual Event. 2021</p>
<p>. E Falotico, L Vannucci, A Ambrosano, U Albanese, S Ulbrich, J C Vasquez Tieck, G Hinkel, J Kaiser, I Peric, O Denninger, N Cauli, M Kirtay, A Roennau, G Klinker, A Von Arnim, L Guyot, D Peppicelli, P Martínez-Cañada, E Ros, P Maier, S Weber, M Huber, D Plecher, F Röhrbein, S Deser, A Roitberg, P Van Der Smagt, R Dillman, P Levi, C Laschi, </p>
<p>Connecting Artificial Brains to Robots in a Comprehensive Simulation Framework: The Neurorobotics Platform. Gewaltig, Frontiers in Neurorobotics. 1122017</p>
<p>MuJoCo: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Vilamoura-Algarve, Portugal2012</p>
<p>Robust Sim2Real Transfer by Learning Inverse Dynamics of Simulated Systems. M Malmir, J Josifovski, N Klarmann, A Knoll, 2nd R:SS Workshop on Closing the Reality Gap in Sim2Real Transfer for Robotics. Corvallis, OR, USA20203</p>
<p>Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: A Survey. W Zhao, J P Queralta, T Westerlund, IEEE Symposium Series on Computational Intelligence (SSCI). Canberra, ACT, Australia2020</p>
<p>Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Vancouver, BC, Canada2017</p>
<p>Noise and the Reality Gap: The Use of Simulation in Evolutionary Robotics. N Jakobi, P Husbands, I Harvey, European Conference on Artificial Life (ECAL), ser. LNCS. Granada, SpainSpringer1995929</p>
<p>Running Across the Reality Gap: Octopod Locomotion Evolved in a Minimal Simulation. N Jakobi, European Workshop on Evolutionary Robotics (EvoRobots), ser. LNCS. Paris, FranceSpringer19981468</p>
<p>Back to Reality: Crossing the Reality Gap in Evolutionary Robotics. J C Zagal, J Ruiz-Del-Solar, P Vallejos, IFAC Proceedings Volumes. 200437</p>
<p>The Transferability Approach: Crossing the Reality Gap in Evolutionary Robotics. S Koos, J.-B Mouret, S Doncieux, IEEE Transactions on Evolutionary Computation. 1712013</p>
<p>Object Detection and Pose Estimation Based on Convolutional Neural Networks Trained with Synthetic Data. J Josifovski, M Kerzel, C Pregizer, L Posniak, S Wermter, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Madrid, Spain2018</p>
<p>3D Simulation for Robot Arm Control with Deep Q-Learning. S James, E Johns, NIPS Workshop: Deep Learning for Action and Interaction. Barcelona, Spain2016</p>
<p>Sim2Real Transfer for Reinforcement Learning Without Dynamics Randomization. M Kaspar, J D Osorio, J Bock, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Las Vegas, NV, USA2020</p>
<p>Sim-to-Real Transfer of Robotic Control with Dynamics Randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, IEEE International Conference on Robotics and Automation (ICRA). Brisbane, QLD, Australia2018</p>
<p>Solving Rubik's Cube with a Robot Hand. I Openai, M Akkaya, M Andrychowicz, M Chociej, B Litwin, A Mcgrew, A Petron, M Paino, G Plappert, R Powell, J Ribas, N Schneider, J Tezak, P Tworek, L Welinder, Q Weng, W Yuan, L Zaremba, Zhang, arXiv:1910.071132019cs, stat</p>
<p>Sim-to-Real Via Sim-to-Sim: Data-Efficient Robotic Grasping Via Randomizedto-Canonical Adaptation Networks. S James, P Wohlhart, M Kalakrishnan, D Kalashnikov, A Irpan, J Ibarz, S Levine, R Hadsell, K Bousmalis, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Long Beach, CA, USA201912629</p>
<p>F Muratore, F Ramos, G Turk, W Yu, M Gienger, J Peters, arXiv:2111.00956Robot Learning from Randomized Simulations: A Review. 2022</p>
<p>Sim-to-Real Robot Learning from Pixels with Progressive Nets. A A Rusu, M Večerík, T Rothörl, N Heess, R Pascanu, R Hadsell, Annual Conference on Robot Learning (CoRL). Mountain View, CA, USAPMLR201778</p>
<p>A Kadian, J Truong, A Gokaslan, A Clegg, E Wijmans, S Lee, M Savva, S Chernova, D Batra, Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?" in 2nd R:SS Workshop on Closing the Reality Gap in Sim2Real Transfer for Robotics. Corvallis, OR, USA20203</p>
<p>Open ai reacher-v2 environment. </p>
<p>Improving Robot Motor Learning with Negatively Valenced Reinforcement Signals. N Navarro-Guerrero, R Lowe, S Wermter, Frontiers in Neurorobotics. 11102017</p>
<p>Interaction in Reinforcement Learning Reduces the Need for Finely Tuned Hyperparameters in Complex Tasks. C Stahlhut, N Navarro-Guerrero, C Weber, S Wermter, Kognitive Systeme. 322015</p>
<p>Continual Learning on Incremental Simulations for Real-World Robotic Manipulation Tasks. J Josifovski, M Malmir, N Klarmann, A Knoll, 2nd R:SS Workshop on Closing the Reality Gap in Sim2Real Transfer for Robotics. Corvallis, OR, USA20203</p>
<p>Unity 3d. </p>
<p>Kuka lbr-iiwa. </p>
<p>Ros industrial. </p>
<p>Towards MRI-Based Autonomous Robotic Us Acquisitions: A First Feasibility Study. C Hennersperger, B Fuerst, S Virga, O Zettinig, B Frisch, T Neff, N Navab, IEEE Transactions on Medical Imaging. 3622017</p>
<p>Sim-to-Real: Learning Agile Locomotion for Quadruped Robots. J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, Robotics: Science and Systems (R:SS). Pittsburgh, PA, USA201814</p>
<p>Unity robotics hub. </p>
<p>Proximal Policy Optimization Algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.063472017Tech. Rep.</p>
<p>Stable Baselines. A Hill, A Raffin, M Ernestus, A Gleave, A Kanervisto, R Traore, P Dhariwal, C Hesse, O Klimov, A Nichol, M Plappert, A Radford, J Schulman, S Sidor, Y Wu, 2018</p>
<p>Rl baselines zoo. A Raffin, 2018</p>            </div>
        </div>

    </div>
</body>
</html>
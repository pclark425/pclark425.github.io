<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2554 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2554</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2554</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-4a836363233398c0ac27daee942cb5533f467458</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4a836363233398c0ac27daee942cb5533f467458" target="_blank">Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A general framework model for integrating and learning structured reasoning into AI agents' policies and indicates that AI agents perform and adapt far better when organised reasoning and prior knowledge are embedded, opening the door to more resilient and general AI agent systems.</p>
                <p><strong>Paper Abstract:</strong> A key method for creating Artificial Intelligence (AI) agents is Reinforcement Learning (RL). However, constructing a standalone RL policy that maps perception to action directly encounters severe problems, chief among them being its lack of generality across multiple tasks and the need for a large amount of training data. The leading cause is that it cannot effectively integrate prior information into the perception-action cycle when devising the policy. Large language models (LLMs) emerged as a fundamental way to incorporate cross-domain knowledge into AI agents but lack crucial learning and adaptation toward specific decision problems. This paper presents a general framework model for integrating and learning structured reasoning into AI agents' policies. Our methodology is motivated by the modularity found in the human brain. The framework utilises the construction of intrinsic and extrinsic functions to add previous understandings of reasoning structures. It also provides the adaptive ability to learn models inside every module or function, consistent with the modular structure of cognitive processes. We describe the framework in-depth and compare it with other AI pipelines and existing frameworks. The paper explores practical applications, covering experiments that show the effectiveness of our method. Our results indicate that AI agents perform and adapt far better when organised reasoning and prior knowledge are embedded. This opens the door to more resilient and general AI agent systems.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2554.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2554.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pangu-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular agent framework that augments LLM-based policies with a hierarchy of intrinsic functions (thinking, planning, communication, tool use, memory transforms) and extrinsic functions (environment action execution), enabling fine-tuning via supervised and RL pipelines and supporting multi-agent environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pangu-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A meta-agent framework that composes intrinsic and extrinsic functions around LLM policies: intrinsic functions operate on an agent's internal memory (e.g., think, plan, reflect, communicate, use tools) and can be nested; extrinsic functions convert LLM outputs into environment actions. The framework provides prompt templates, memory management, support for tool use and tree-search planning, and pipelines for Supervised Fine-Tuning (SFT) and Reinforcement Learning Fine-Tuning (RLFT). It is designed to be modular and configurable (customizable control-flow) and supports single- and multi-agent environments by framing interactions as (partially observable) stochastic games.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not prespecified as fixed agent roles in experiments; the framework supports agents specialised via intrinsic functions (e.g. 'think' agents implemented via LLM-based thought generation, 'tool-use' agents that call external tools/code interpreters, and communication-enabled agents that exchange messages). Paper describes pairwise communication capability and ability to instantiate agents with different prompting/LLM configurations (e.g. Swift vs Sage in SwiftSage).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>idea generation (LLM thought generation / chain-of-thought), planning (intrinsic planning, tree-search), implementation/tool use (code interpreter/tool calls), execution (extrinsic actions in environment), evaluation (rewards used for RL, reflect intrinsic function for self-feedback), dataset collection for SFT (trajectory generation / rejection sampling), cross-domain fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Flexible; supports pairwise agent communication and multi-agent interaction modelled as a Partially Observable Stochastic Game (POMSG). Coordination is achieved via LLM-mediated messaging and intrinsic function orchestration (sequential/nested intrinsic-function pipelines per time step), enabling sequential pipelines or multi-step collaborative procedures rather than a single monolithic policy.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Natural-language messages mediated by the LLM prompt system and template-based prompts; messages and state exchanged via each agent's memory (MEM) and prompt templates (i.e. LLM chat-format or token prompts). The paper also supports tool outputs and memory entries to be written/read by agents (unstructured text + prompt templates); no formal structured JSON protocol is specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Multiple mechanisms: (1) environmental scalar rewards used in RL and critic/value heads (per-agent PPO with a token-to-action credit assignment modification); (2) intrinsic Reflect function providing linguistic self-feedback (LLM reflection on past actions); (3) rejection-sampling during SFT data collection (trajectories that fail are discarded); (4) cross-agent communication enables agents to give/receive messages (but no dedicated separate 'critic agent' for peer scoring is defined in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Configurable: intrinsic functions (including communication) can be invoked at any environment time step; Reflect was used at every step in experiments when enabled; SwiftSage switches agents (Swift->Sage) on-demand (e.g. after repeated zero-reward steps). Thus communication can be per-step or on-demand depending on configured intrinsic functions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Generalist interactive/decision-making domains evaluated include ALFWorld (embodied/text-aligned tasks), BabyAI (instruction following), GSM8K (math reasoning), HotpotQA (multi-hop QA), WebShop (web interaction), HumanEval (code generation). Framework intended for general research and decision-making domains; not limited to a single scientific domain.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics are task success rates / achieved returns. Representative numbers (from experiments with single-agent use and fine-tuning pipelines): ALFWorld success: Direct 4% -> FS-CoT 27%; after multi-step SFT pipeline OpenChat-3.5: 3-step SFT 82% (Table 4). Llama 2-7B fine-tuned SFT+RL: ALFWorld 0.88 (88%), BabyAI-GoToObj-v0 SFT+RL 0.91 (91%). (These figures are reported as achieved returns / success rates in the paper's evaluation tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against baseline prompting methods (Direct, ZS-CoT, FS, FS-CoT) and against models without SFT/RLFT. Paper reports that adding structured reasoning (intrinsic functions) and performing SFT then RLFT improves results over raw LLM prompting; e.g., ALFWorld: baseline FS-CoT ~27% -> after SFT 82% -> SFT+RL 88%. The comparisons are mostly single-agent experiments showing benefit of structured reasoning and fine-tuning rather than multi-agent vs single-agent explicit controlled comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Framework-level benefits of agent coordination features include: (1) ability to simulate pairwise communication and multi-agent interactions (enables richer behaviours); (2) modular intrinsic functions (communication/tool-use) permit composite methods (ReAct, Reflect, SwiftSage) which yield higher achieved returns in many tasks; (3) enables rejection-sampling SFT and subsequent RL to boost success rates substantially (quantified above). Quantitative benefits reported are improvements in task success after using structured reasoning and fine-tuning rather than isolated multi-agent numerical comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Paper mentions limitations and challenges: (1) the current version does not support end-to-end differentiable gradients across intrinsic/extrinsic functions (limited coupling); (2) generation speed of LLMs hinders RL sample efficiency (mitigated by batching, optimized kernels, paged attention); (3) context length limitations constrain multi-step composite methods and multi-agent message content (truncation affects tasks like WebShop); (4) absence of dedicated evaluation of multi-agent coordination overheads or emergent failure modes in the present work.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>No explicit ablation isolating multi-agent coordination components is reported. The paper reports ablations across reasoning methods (first-order vs composite) and fine-tuning stages (Direct, SFT, SFT+RL), but not controlled ablations that toggle inter-agent communication vs no communication in multi-agent experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Paper offers design guidance rather than a single optimal configuration: recommends using intrinsic functions to structure reasoning, using SFT on high-quality (rejection-sampled) trajectories then RLFT for further gains, and using caching/optimized kernels for generation speed. Specific heuristics include using FS-CoT for trajectory collection and removing thoughts during SFT to save context space; SwiftSage switching heuristic used (switch to Sage after five consecutive zero-reward steps). No general optimal multi-agent specialization or communication frequency is prescribed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2554.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2554.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgentVerse</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AgentVerse: Facilitating multi-agent collaboration and exploring emergent behaviors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent framework mentioned in related work that facilitates multi-agent collaboration and studies emergent behaviours in LLM-based agent societies; referenced as part of the landscape of multi-agent LLM frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AgentVerse</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as an LLM-based multi-agent framework that supports multi-agent collaboration and emergent behaviours; listed in a comparative table for agent frameworks (Table 1) as supporting Tools, Memory, Multi-Agent but lacking customisable control-flow, SFT and RLFT (per the paper's table). The paper does not provide internal architectural details beyond this citation/feature summary.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not specified in this paper (only cited as a multi-agent framework).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Not specified in this paper; generally positioned as supporting collaborative multi-agent interactions (likely idea generation, communication), but the paper provides no explicit phase mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Not specified in this paper beyond being a multi-agent framework; no coordination mechanism details provided.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General multi-agent collaboration; no domain-specific experiments discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Mentioned as enabling multi-agent collaboration and study of emergent behaviour (no quantified results given in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None reported in this paper for AgentVerse (only cited).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2554.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2554.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced multi-agent conversation framework that enables applications by orchestrating multiple LLM-based agents interacting via message exchanges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoGen</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as a recent framework that enables multi-agent conversations and tool-augmented LLM applications; appears in the comparative table (Table 1) as supporting Tools, Memory, Multi-Agent and Customisable Control-Flow but not SFT/RLFT per the paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not detailed in this paper beyond general multi-agent conversational roles.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Implied to use natural-language message passing between agents (not explicitly specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General LLM-application domains; not elaborated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Mentioned as enabling multi-agent conversations and application orchestration, but no quantitative benefits presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None reported in this paper for AutoGen (only cited).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2554.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2554.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaGPT: Meta programming for multi-agent collaborative framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited multi-agent collaborative framework focused on decomposing and coordinating multi-role agents to accomplish complex tasks (referenced among other LLM-agent toolkits).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Metagpt: Meta programming for multi-agent collaborative framework</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Included in the comparative list of agent frameworks as supporting Tools, Memory, and Multi-Agent capabilities; presented as part of the landscape of LLM-based multi-agent systems but not used in experiments within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not described in this paper (citation only).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General multi-agent collaborative tasks; not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2554.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2554.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CAMEL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CAMEL: Communicative agents for 'mind' exploration of large language model society</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system studying communicative behaviours among LLM-based agents, used to explore interaction patterns and multi-agent dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CAMEL: Communicative agents for "mind" exploration of large language model society</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CAMEL</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited among multi-agent LLM frameworks in the related-work comparison table as supporting Tools, Memory, Multi-Agent but without customisable control-flow or fine-tuning in the paper's table; no experimental details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not specified here (likely natural-language but not stated).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Exploration of agent communication/interaction (no specific scientific research domain detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Described in citation context as enabling multi-agent communication studies; no quantitative results reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2554.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2554.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto-gpt: An autonomous gpt-4 experiment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced autonomous-agent project that chains LLM calls to perform multi-step tasks autonomously; cited as part of the ecosystem of LLM agent systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Auto-gpt: An autonomous gpt-4 experiment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned among prior LLM-agent systems in the related work overview (Table 1); in the paper's comparison AutoGPT has Tools and Memory and is noted as not supporting customisable control-flow, SFT, or RLFT in the table context.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>typically single autonomous agent composed of components (paper does not detail multi-agent specifics)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General autonomous multi-step task completion (no scientific research-specific experiments in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2554.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2554.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XAgent: An autonomous agent for complex task solving</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced agent framework listed in the comparative table as supporting multi-agent setups and memory/tool use; cited in the related-work context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Xagent: An autonomous agent for complex task solving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>XAgent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Appears in the paper's comparative table as an example of an LLM-based agent framework that supports tools, memory and multi-agent capabilities; no experimental details or internal mechanisms are described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General task solving; not elaborated here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2554.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2554.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AGENTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AGENTS: An open-source framework for autonomous language agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited open-source framework for autonomous language agents; included as part of a comparison of agent frameworks (Table 1) and noted to support customisable control-flow.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AGENTS: An open-source framework for autonomous language agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AGENTS (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced in the paper's Table 1 as a framework that supports Tools, Memory, Multi-Agent and Customisable Control-Flow (but not SFT/RLFT in that table); cited as part of the ecosystem of LLM-agent toolkits without further experimental detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General autonomous language-agent tasks; not elaborated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation <em>(Rating: 2)</em></li>
                <li>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors <em>(Rating: 2)</em></li>
                <li>Metagpt: Meta programming for multi-agent collaborative framework <em>(Rating: 2)</em></li>
                <li>CAMEL: Communicative agents for "mind" exploration of large language model society <em>(Rating: 2)</em></li>
                <li>Auto-gpt: An autonomous gpt-4 experiment <em>(Rating: 1)</em></li>
                <li>Improving factuality and reasoning in language models through multiagent debate <em>(Rating: 1)</em></li>
                <li>Encouraging divergent thinking in large language models through multi-agent debate <em>(Rating: 1)</em></li>
                <li>Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization <em>(Rating: 2)</em></li>
                <li>Communicative agents for software development <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2554",
    "paper_id": "paper-4a836363233398c0ac27daee942cb5533f467458",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [
        {
            "name_short": "Pangu-Agent",
            "name_full": "Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning",
            "brief_description": "A modular agent framework that augments LLM-based policies with a hierarchy of intrinsic functions (thinking, planning, communication, tool use, memory transforms) and extrinsic functions (environment action execution), enabling fine-tuning via supervised and RL pipelines and supporting multi-agent environments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Pangu-Agent",
            "system_description": "A meta-agent framework that composes intrinsic and extrinsic functions around LLM policies: intrinsic functions operate on an agent's internal memory (e.g., think, plan, reflect, communicate, use tools) and can be nested; extrinsic functions convert LLM outputs into environment actions. The framework provides prompt templates, memory management, support for tool use and tree-search planning, and pipelines for Supervised Fine-Tuning (SFT) and Reinforcement Learning Fine-Tuning (RLFT). It is designed to be modular and configurable (customizable control-flow) and supports single- and multi-agent environments by framing interactions as (partially observable) stochastic games.",
            "number_of_agents": "variable",
            "agent_specializations": "Not prespecified as fixed agent roles in experiments; the framework supports agents specialised via intrinsic functions (e.g. 'think' agents implemented via LLM-based thought generation, 'tool-use' agents that call external tools/code interpreters, and communication-enabled agents that exchange messages). Paper describes pairwise communication capability and ability to instantiate agents with different prompting/LLM configurations (e.g. Swift vs Sage in SwiftSage).",
            "research_phases_covered": "idea generation (LLM thought generation / chain-of-thought), planning (intrinsic planning, tree-search), implementation/tool use (code interpreter/tool calls), execution (extrinsic actions in environment), evaluation (rewards used for RL, reflect intrinsic function for self-feedback), dataset collection for SFT (trajectory generation / rejection sampling), cross-domain fine-tuning.",
            "coordination_mechanism": "Flexible; supports pairwise agent communication and multi-agent interaction modelled as a Partially Observable Stochastic Game (POMSG). Coordination is achieved via LLM-mediated messaging and intrinsic function orchestration (sequential/nested intrinsic-function pipelines per time step), enabling sequential pipelines or multi-step collaborative procedures rather than a single monolithic policy.",
            "communication_protocol": "Natural-language messages mediated by the LLM prompt system and template-based prompts; messages and state exchanged via each agent's memory (MEM) and prompt templates (i.e. LLM chat-format or token prompts). The paper also supports tool outputs and memory entries to be written/read by agents (unstructured text + prompt templates); no formal structured JSON protocol is specified in the paper.",
            "feedback_mechanism": "Multiple mechanisms: (1) environmental scalar rewards used in RL and critic/value heads (per-agent PPO with a token-to-action credit assignment modification); (2) intrinsic Reflect function providing linguistic self-feedback (LLM reflection on past actions); (3) rejection-sampling during SFT data collection (trajectories that fail are discarded); (4) cross-agent communication enables agents to give/receive messages (but no dedicated separate 'critic agent' for peer scoring is defined in experiments).",
            "communication_frequency": "Configurable: intrinsic functions (including communication) can be invoked at any environment time step; Reflect was used at every step in experiments when enabled; SwiftSage switches agents (Swift-&gt;Sage) on-demand (e.g. after repeated zero-reward steps). Thus communication can be per-step or on-demand depending on configured intrinsic functions.",
            "task_domain": "Generalist interactive/decision-making domains evaluated include ALFWorld (embodied/text-aligned tasks), BabyAI (instruction following), GSM8K (math reasoning), HotpotQA (multi-hop QA), WebShop (web interaction), HumanEval (code generation). Framework intended for general research and decision-making domains; not limited to a single scientific domain.",
            "performance_metrics": "Reported metrics are task success rates / achieved returns. Representative numbers (from experiments with single-agent use and fine-tuning pipelines): ALFWorld success: Direct 4% -&gt; FS-CoT 27%; after multi-step SFT pipeline OpenChat-3.5: 3-step SFT 82% (Table 4). Llama 2-7B fine-tuned SFT+RL: ALFWorld 0.88 (88%), BabyAI-GoToObj-v0 SFT+RL 0.91 (91%). (These figures are reported as achieved returns / success rates in the paper's evaluation tables.)",
            "baseline_comparison": "Compared against baseline prompting methods (Direct, ZS-CoT, FS, FS-CoT) and against models without SFT/RLFT. Paper reports that adding structured reasoning (intrinsic functions) and performing SFT then RLFT improves results over raw LLM prompting; e.g., ALFWorld: baseline FS-CoT ~27% -&gt; after SFT 82% -&gt; SFT+RL 88%. The comparisons are mostly single-agent experiments showing benefit of structured reasoning and fine-tuning rather than multi-agent vs single-agent explicit controlled comparisons.",
            "coordination_benefits": "Framework-level benefits of agent coordination features include: (1) ability to simulate pairwise communication and multi-agent interactions (enables richer behaviours); (2) modular intrinsic functions (communication/tool-use) permit composite methods (ReAct, Reflect, SwiftSage) which yield higher achieved returns in many tasks; (3) enables rejection-sampling SFT and subsequent RL to boost success rates substantially (quantified above). Quantitative benefits reported are improvements in task success after using structured reasoning and fine-tuning rather than isolated multi-agent numerical comparisons.",
            "coordination_challenges": "Paper mentions limitations and challenges: (1) the current version does not support end-to-end differentiable gradients across intrinsic/extrinsic functions (limited coupling); (2) generation speed of LLMs hinders RL sample efficiency (mitigated by batching, optimized kernels, paged attention); (3) context length limitations constrain multi-step composite methods and multi-agent message content (truncation affects tasks like WebShop); (4) absence of dedicated evaluation of multi-agent coordination overheads or emergent failure modes in the present work.",
            "ablation_studies": "No explicit ablation isolating multi-agent coordination components is reported. The paper reports ablations across reasoning methods (first-order vs composite) and fine-tuning stages (Direct, SFT, SFT+RL), but not controlled ablations that toggle inter-agent communication vs no communication in multi-agent experiments.",
            "optimal_configurations": "Paper offers design guidance rather than a single optimal configuration: recommends using intrinsic functions to structure reasoning, using SFT on high-quality (rejection-sampled) trajectories then RLFT for further gains, and using caching/optimized kernels for generation speed. Specific heuristics include using FS-CoT for trajectory collection and removing thoughts during SFT to save context space; SwiftSage switching heuristic used (switch to Sage after five consecutive zero-reward steps). No general optimal multi-agent specialization or communication frequency is prescribed.",
            "uuid": "e2554.0",
            "source_info": {
                "paper_title": "Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "AgentVerse",
            "name_full": "AgentVerse: Facilitating multi-agent collaboration and exploring emergent behaviors",
            "brief_description": "A multi-agent framework mentioned in related work that facilitates multi-agent collaboration and studies emergent behaviours in LLM-based agent societies; referenced as part of the landscape of multi-agent LLM frameworks.",
            "citation_title": "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors",
            "mention_or_use": "mention",
            "system_name": "AgentVerse",
            "system_description": "Referenced as an LLM-based multi-agent framework that supports multi-agent collaboration and emergent behaviours; listed in a comparative table for agent frameworks (Table 1) as supporting Tools, Memory, Multi-Agent but lacking customisable control-flow, SFT and RLFT (per the paper's table). The paper does not provide internal architectural details beyond this citation/feature summary.",
            "number_of_agents": "variable (not specified)",
            "agent_specializations": "Not specified in this paper (only cited as a multi-agent framework).",
            "research_phases_covered": "Not specified in this paper; generally positioned as supporting collaborative multi-agent interactions (likely idea generation, communication), but the paper provides no explicit phase mapping.",
            "coordination_mechanism": "Not specified in this paper beyond being a multi-agent framework; no coordination mechanism details provided.",
            "communication_protocol": "Not specified in this paper.",
            "feedback_mechanism": "Not specified in this paper.",
            "communication_frequency": "Not specified in this paper.",
            "task_domain": "General multi-agent collaboration; no domain-specific experiments discussed in this paper.",
            "performance_metrics": null,
            "baseline_comparison": null,
            "coordination_benefits": "Mentioned as enabling multi-agent collaboration and study of emergent behaviour (no quantified results given in this paper).",
            "coordination_challenges": "Not discussed in this paper.",
            "ablation_studies": "None reported in this paper for AgentVerse (only cited).",
            "optimal_configurations": "Not provided in this paper.",
            "uuid": "e2554.1",
            "source_info": {
                "paper_title": "Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "AutoGen",
            "name_full": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
            "brief_description": "A referenced multi-agent conversation framework that enables applications by orchestrating multiple LLM-based agents interacting via message exchanges.",
            "citation_title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
            "mention_or_use": "mention",
            "system_name": "AutoGen",
            "system_description": "Cited as a recent framework that enables multi-agent conversations and tool-augmented LLM applications; appears in the comparative table (Table 1) as supporting Tools, Memory, Multi-Agent and Customisable Control-Flow but not SFT/RLFT per the paper's summary.",
            "number_of_agents": "variable (not specified)",
            "agent_specializations": "Not detailed in this paper beyond general multi-agent conversational roles.",
            "research_phases_covered": "Not specified in this paper.",
            "coordination_mechanism": "Not detailed in this paper.",
            "communication_protocol": "Implied to use natural-language message passing between agents (not explicitly specified in this paper).",
            "feedback_mechanism": "Not specified in this paper.",
            "communication_frequency": "Not specified in this paper.",
            "task_domain": "General LLM-application domains; not elaborated in this paper.",
            "performance_metrics": null,
            "baseline_comparison": null,
            "coordination_benefits": "Mentioned as enabling multi-agent conversations and application orchestration, but no quantitative benefits presented in this paper.",
            "coordination_challenges": "Not discussed here.",
            "ablation_studies": "None reported in this paper for AutoGen (only cited).",
            "optimal_configurations": "Not provided in this paper.",
            "uuid": "e2554.2",
            "source_info": {
                "paper_title": "Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "MetaGPT",
            "name_full": "MetaGPT: Meta programming for multi-agent collaborative framework",
            "brief_description": "A cited multi-agent collaborative framework focused on decomposing and coordinating multi-role agents to accomplish complex tasks (referenced among other LLM-agent toolkits).",
            "citation_title": "Metagpt: Meta programming for multi-agent collaborative framework",
            "mention_or_use": "mention",
            "system_name": "MetaGPT",
            "system_description": "Included in the comparative list of agent frameworks as supporting Tools, Memory, and Multi-Agent capabilities; presented as part of the landscape of LLM-based multi-agent systems but not used in experiments within this paper.",
            "number_of_agents": "variable (not specified)",
            "agent_specializations": "Not described in this paper (citation only).",
            "research_phases_covered": "Not specified in this paper.",
            "coordination_mechanism": "Not specified here.",
            "communication_protocol": "Not specified here.",
            "feedback_mechanism": "Not specified here.",
            "communication_frequency": "Not specified here.",
            "task_domain": "General multi-agent collaborative tasks; not detailed in this paper.",
            "performance_metrics": null,
            "baseline_comparison": null,
            "coordination_benefits": "Not quantified here.",
            "coordination_challenges": "Not discussed here.",
            "ablation_studies": "None in this paper.",
            "optimal_configurations": "Not provided.",
            "uuid": "e2554.3",
            "source_info": {
                "paper_title": "Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "CAMEL",
            "name_full": "CAMEL: Communicative agents for 'mind' exploration of large language model society",
            "brief_description": "A referenced system studying communicative behaviours among LLM-based agents, used to explore interaction patterns and multi-agent dialogues.",
            "citation_title": "CAMEL: Communicative agents for \"mind\" exploration of large language model society",
            "mention_or_use": "mention",
            "system_name": "CAMEL",
            "system_description": "Cited among multi-agent LLM frameworks in the related-work comparison table as supporting Tools, Memory, Multi-Agent but without customisable control-flow or fine-tuning in the paper's table; no experimental details provided in this paper.",
            "number_of_agents": "variable (not specified)",
            "agent_specializations": "Not specified in this paper.",
            "research_phases_covered": "Not specified in this paper.",
            "coordination_mechanism": "Not specified here.",
            "communication_protocol": "Not specified here (likely natural-language but not stated).",
            "feedback_mechanism": "Not specified in this paper.",
            "communication_frequency": "Not specified in this paper.",
            "task_domain": "Exploration of agent communication/interaction (no specific scientific research domain detailed here).",
            "performance_metrics": null,
            "baseline_comparison": null,
            "coordination_benefits": "Described in citation context as enabling multi-agent communication studies; no quantitative results reported in this paper.",
            "coordination_challenges": "Not discussed here.",
            "ablation_studies": "None reported in this paper.",
            "optimal_configurations": "Not provided.",
            "uuid": "e2554.4",
            "source_info": {
                "paper_title": "Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "AutoGPT",
            "name_full": "Auto-gpt: An autonomous gpt-4 experiment",
            "brief_description": "A referenced autonomous-agent project that chains LLM calls to perform multi-step tasks autonomously; cited as part of the ecosystem of LLM agent systems.",
            "citation_title": "Auto-gpt: An autonomous gpt-4 experiment",
            "mention_or_use": "mention",
            "system_name": "AutoGPT",
            "system_description": "Mentioned among prior LLM-agent systems in the related work overview (Table 1); in the paper's comparison AutoGPT has Tools and Memory and is noted as not supporting customisable control-flow, SFT, or RLFT in the table context.",
            "number_of_agents": "typically single autonomous agent composed of components (paper does not detail multi-agent specifics)",
            "agent_specializations": "Not specified in this paper.",
            "research_phases_covered": "Not specified in this paper.",
            "coordination_mechanism": "Not specified in this paper.",
            "communication_protocol": "Not specified in this paper.",
            "feedback_mechanism": "Not specified in this paper.",
            "communication_frequency": "Not specified in this paper.",
            "task_domain": "General autonomous multi-step task completion (no scientific research-specific experiments in this paper).",
            "performance_metrics": null,
            "baseline_comparison": null,
            "coordination_benefits": "Not quantified here.",
            "coordination_challenges": "Not discussed here.",
            "ablation_studies": "None reported in this paper.",
            "optimal_configurations": "Not provided.",
            "uuid": "e2554.5",
            "source_info": {
                "paper_title": "Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "XAgent",
            "name_full": "XAgent: An autonomous agent for complex task solving",
            "brief_description": "A referenced agent framework listed in the comparative table as supporting multi-agent setups and memory/tool use; cited in the related-work context.",
            "citation_title": "Xagent: An autonomous agent for complex task solving",
            "mention_or_use": "mention",
            "system_name": "XAgent",
            "system_description": "Appears in the paper's comparative table as an example of an LLM-based agent framework that supports tools, memory and multi-agent capabilities; no experimental details or internal mechanisms are described in this paper.",
            "number_of_agents": "variable (not specified)",
            "agent_specializations": "Not specified here.",
            "research_phases_covered": "Not specified here.",
            "coordination_mechanism": "Not specified here.",
            "communication_protocol": "Not specified here.",
            "feedback_mechanism": "Not specified here.",
            "communication_frequency": "Not specified here.",
            "task_domain": "General task solving; not elaborated here.",
            "performance_metrics": null,
            "baseline_comparison": null,
            "coordination_benefits": "Not quantified here.",
            "coordination_challenges": "Not discussed here.",
            "ablation_studies": "None in this paper.",
            "optimal_configurations": "Not provided.",
            "uuid": "e2554.6",
            "source_info": {
                "paper_title": "Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "AGENTS",
            "name_full": "AGENTS: An open-source framework for autonomous language agents",
            "brief_description": "A cited open-source framework for autonomous language agents; included as part of a comparison of agent frameworks (Table 1) and noted to support customisable control-flow.",
            "citation_title": "AGENTS: An open-source framework for autonomous language agents",
            "mention_or_use": "mention",
            "system_name": "AGENTS (framework)",
            "system_description": "Referenced in the paper's Table 1 as a framework that supports Tools, Memory, Multi-Agent and Customisable Control-Flow (but not SFT/RLFT in that table); cited as part of the ecosystem of LLM-agent toolkits without further experimental detail in this paper.",
            "number_of_agents": "variable (not specified)",
            "agent_specializations": "Not specified here.",
            "research_phases_covered": "Not specified here.",
            "coordination_mechanism": "Not specified here.",
            "communication_protocol": "Not specified here.",
            "feedback_mechanism": "Not specified here.",
            "communication_frequency": "Not specified here.",
            "task_domain": "General autonomous language-agent tasks; not elaborated in this paper.",
            "performance_metrics": null,
            "baseline_comparison": null,
            "coordination_benefits": "Not quantified here.",
            "coordination_challenges": "Not discussed here.",
            "ablation_studies": "None in this paper.",
            "optimal_configurations": "Not provided.",
            "uuid": "e2554.7",
            "source_info": {
                "paper_title": "Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
            "rating": 2
        },
        {
            "paper_title": "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors",
            "rating": 2
        },
        {
            "paper_title": "Metagpt: Meta programming for multi-agent collaborative framework",
            "rating": 2
        },
        {
            "paper_title": "CAMEL: Communicative agents for \"mind\" exploration of large language model society",
            "rating": 2
        },
        {
            "paper_title": "Auto-gpt: An autonomous gpt-4 experiment",
            "rating": 1
        },
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate",
            "rating": 1
        },
        {
            "paper_title": "Encouraging divergent thinking in large language models through multi-agent debate",
            "rating": 1
        },
        {
            "paper_title": "Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization",
            "rating": 2
        },
        {
            "paper_title": "Communicative agents for software development",
            "rating": 1
        }
    ],
    "cost": 0.02050525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning</h1>
<p>Filippos Christianos ${ }^{1 <em>}$ Georgios Papoudakis ${ }^{1 </em>}$ Matthieu Zimmer ${ }^{1}$ Thomas Coste ${ }^{1}$<br>Zhihao Wu ${ }^{1}$ Jingxuan Chen ${ }^{1}$ Khyati Khandelwal ${ }^{1}$ James Doran ${ }^{1}$ Xidong Feng ${ }^{2 \dagger}$<br>Jiacheng Liu ${ }^{2 \dagger}$ Zheng Xiong ${ }^{2 \dagger}$ Yicheng Luo ${ }^{2 \dagger}$ Jianye Hao ${ }^{1}$<br>Kun Shao ${ }^{1 \ddagger}$ Haitham Bou-Ammar ${ }^{1,2 \ddagger}$ Jun Wang ${ }^{2 \ddagger}$<br>${ }^{1}$ Huawei Noahs Ark Lab, ${ }^{2}$ University College London, ${ }^{3}$ University of Oxford</p>
<h4>Abstract</h4>
<p>A key method for creating Artificial Intelligence (AI) agents is Reinforcement Learning (RL). However, constructing a standalone RL policy that maps perception to action directly encounters severe problems, chief among them being its lack of generality across multiple tasks and the need for a large amount of training data. The leading cause is that it cannot effectively integrate prior information into the perception-action cycle when devising the policy. Large language models (LLMs) emerged as a fundamental way to incorporate cross-domain knowledge into AI agents but lack crucial learning and adaptation toward specific decision problems. This paper presents a general framework model for integrating and learning structured reasoning into AI agents' policies. Our methodology is motivated by the modularity found in the human brain. The framework utilises the construction of intrinsic and extrinsic functions to add previous understandings of reasoning structures. It also provides the adaptive ability to learn models inside every module or function, consistent with the modular structure of cognitive processes. We describe the framework in-depth and compare it with other AI pipelines and existing frameworks. The paper explores practical applications, covering experiments that show the effectiveness of our method. Our results indicate that AI agents perform and adapt far better when organised reasoning and prior knowledge are embedded. This opens the door to more resilient and general AI agent systems.</p>
<h2>1 Introduction</h2>
<p>Since the inception of AI, developing generalist agents capable of solving and adapting to increasingly complex tasks has been an overarching goal. Such agents can be defined as: "... entities that appropriately act in uncertain environments, where appropriate actions increase the probability of success and where success is the achievement of behavioural subgoals that support the system's ultimate goal." (J. S. Albus [1]). Hence, AI agents must be equipped with sensors to perceive environments and methods to act upon them to achieve their goals. This definition can capture</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Pictorial depiction of the Pangu-Agent pipeline with RL. Starting from the system prompt and initial state, our agent executes actions in the environment and observes the next state and the reward. The trajectories generated can be used for finetuning the LLM.</p>
<p>diverse autonomous systems, ranging from simple software programs running on the web to complex embodied entities interacting in unknown and stochastic environments.</p>
<p>AI agents, vital to many intelligent applications, are often trained through RL, which develops decision-making skills through environmental interactions. Both model-based and model-free deep RL methodologies have shown significant achievements, such as AlphaZero's board game proficiency [2], improved sorting and multiplication algorithms [3, 4], drone racing championships [5] and plasma control in fusion reactors [6]. These successes involved a standard RL pipeline, where agents learn what we refer to as an <em>extrinsic function</em>  a policy that directly interacts with the outside world, i.e., responding to environmental stimuli to maximize a reward signal. This function, often a parameterized neural network, generates actions based on environmental observations.</p>
<p>The classic RL approach, using a single mapping function to define a policy $\pi$, often proves inadequate in complex environments, contradicting the aim of generalist agents to interact, adapt, and learn across multiple stochastic environments. This sparked studies on improved learnability via meta-RL [7, 8, 9, 10], intrinsic motivation-based exploration [11, 12, 13], auxiliary tasks [14, 15, 16], inverse RL [17, 18, 19], Bayesian priors [20, 21, 22], and hierarchical policies [23, 24, 25, 26]. However, RL-introduced priors are often task-specific and require extensive engineering and domain expertise. To allow for more general priors, recent studies have shifted towards integrating large language models (LLMs) into agent frameworks, as seen in AutoGen [27], AutoGPT [28], and AgentVerse [29]. Research on LLM-based AI agents not only uses LLMs as foundational priors but also incorporates tools [30, 31] and multi-agent communication [29] to build generalist agents. Existing frameworks, outlined in Table 1 and discussed in Appendix F, assume fixed reasoning structures and typically lack fine-tuning for new behaviors and skills, limiting agents to the pre-trained LLM's capabilities and posing a risk in out-of-distribution tasks.</p>
<p>In this paper, we propose the <em>Pangu-Agent</em> framework (also see Fig. 1) as a first step towards remedying the above two problems. Our work distinguishes itself from prior frameworks in two critical ways: <em>i)</em> we formalize the internal thinking process of the agent as a form of <em>structured reasoning</em> and <em>ii)</em> we show how agents can be fine-tuned via supervised learning and RL. To introduce structured reasoning, we assume a family of <em>intrinsic functions</em> $\mu(\cdot)$ that act on and transform the agent's internal memory. Introducing these intrinsic functions can reformulate the typical RL objective to one that supports multiple "thinking" steps. Therefore, the typical RL objective which aims to find a policy $\pi$ conditioned on the history of observations $\vec{\mathbf{o}}$ that maximizes the returns $\mathbf{R}$, i.e. $\max_{\pi(\cdot)}\mathbf{R}(\pi(\cdot|\vec{\mathbf{o}}))$ can be rewritten using a nested set (see Fig. 2) of intrinsic functions $\vec{\mu}(\cdot)$ as:</p>
<p>$$
\overbrace{\max_{\pi(\cdot)} \mathbf{R}(\pi(\cdot|\vec{\mathbf{o}}))}^{\text{Standard RL}} \rightarrow \overbrace{\max_{\pi(\cdot), \vec{\mu}(\cdot)} \mathbf{R}(\pi(\cdot|\vec{\mathbf{o}}, \vec{\mu}(\vec{\mathbf{o}})))}^{\text{Pangu Opt.}}.
$$</p>
<p>These nested intrinsic functions have been absent in RL formulations, whereby standard RL focuses on directly learning policies that output actions from perceptions. While it is customary to parameterize policies by deep network architectures, we contend that the absence of inherent reasoning structures in standard RL pipelines can become a significant bottleneck when scaling agents across tasks via foundational model policies precisely because gradients fail to offer enough supervision for all deep</p>
<table>
<thead>
<tr>
<th>Agent</th>
<th>Tools</th>
<th>Memory</th>
<th>Multi-Agent</th>
<th>Customisable Control-Flow</th>
<th>SFT</th>
<th>RLFT</th>
</tr>
</thead>
<tbody>
<tr>
<td>Transformers Agents<em>[32]</em></td>
<td>$\checkmark$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
</tr>
<tr>
<td>LangChain<em>[33]</em></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
</tr>
<tr>
<td>AutoGPT<em>[28]</em></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
</tr>
<tr>
<td>OpenAgents<em>[34]</em></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
</tr>
<tr>
<td>XAgent<em>[35]</em></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
</tr>
<tr>
<td>MetaGPT<em>[36]</em></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
</tr>
<tr>
<td>Camel<em>[37]</em></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
</tr>
<tr>
<td>AgentVerse<em>[29]</em></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
</tr>
<tr>
<td>AGENTS<em>[38]</em></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\times$</td>
<td>$\times$</td>
</tr>
<tr>
<td>AutoGen<em>[27]</em></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\times$</td>
<td>$\times$</td>
</tr>
<tr>
<td>Pangu-Agent</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of our proposed agent versus recent language-based agents. The overview of related methods can be found in Appendix F.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Visualisation of three intrinsic functions demonstrating our formulation's importance in improving our agent's modularity and flexibility. The intrinsic functions can be re-defined and re-configured by users, e.g., $\mu_{1}(\cdot)$ taking an LLM as input to produce thoughts or $\mu_{2}(\cdot)$ utilising tools to help improve reasoning. We also support nesting those intrinsic functions to build more general modules for complex and challenging decision-making tasks.
layers with only (sparse) rewards as guidance. Our framework demonstrates how structured reasoning can empower RL to surmount these challenges, leveraging large-scale foundational models to provide priors and enable generalisation across broad domains. In short, we summarise our contributions as:</p>
<ol>
<li>We demonstrate the importance of structured reasoning in an agent framework that is general and versatile enough to effectively cover a diverse spectrum of existing agent framework in the literature [e.g. 39, 40, 41, 42]. As a meta-agent framework, it can be adjusted or fine-tuned utilising the sequence of intrinsic function calls or delegating decision-making to the underlying LLM. As we show in Appendix D, users of the framework can easily expand the capabilities of the agent and combine or reuse many of the methods that have already been implemented.</li>
<li>We conduct a comprehensive evaluation of structured reasoning by evaluating first-order (e.g. direct answering, chain-of-thought prompting [43]) and composite methods (e.g. ReAct [40], Self-Consistency [39], SwiftSage [41]), on seven LLMs and six distinct domains. This evaluation can be used to inform researchers on how to initialise their agents and how to collect data for the fine-tuning steps.</li>
<li>Our study demonstrates the impact of Supervised Fine-Tuning (SFT) and RL Fine-Tuning (RLFT) of the framework. Through structured reasoning, we successfully implement a rejection-sampling-based SFT pipeline, notably improving an LLM's performance in the ALFWorld domain [44], with success rates in held-out tasks increasing from $27 \%$ to $82 \%$. Although the benefits of SFT plateau, we show that further enhancements are achievable through RL, increasing success rates to $88 \%$ and even from $28 \%$ to $91 \%$ in the BabyAI task [45]. Moreover, our cross-domain experiments reveal the ability of a single LLM, trained via the RL pipeline, to simultaneously achieve high performance in both the ALFWorld ( $82 \%$ ) and BabyAI ( $58.7 \%$ average in 18 tasks) domains. These findings highlight the potential of structured reasoning to advance LLM-based agent training significantly.</li>
</ol>
<h1>2 The Pangu-Agent Formulation</h1>
<p>In this section, we continue our development of incorporating intrinsic functions into the RL pipeline to allow the definition of flexible reasoning processes. We emphasise the need for those to be defined, learned, and used separately from the extrinsic functions, whereby users can re-define any arbitrarily nesting deemed helpful for their tasks. We can rewrite the Pangu-Agent's optimisation problem from Eq. (1) in a more detailed form as:</p>
<p>$$
\max <em 0="0" _geq="\geq" t="t">{\pi(\cdot)} \underset{\pi(\cdot)}{\mathbb{E}}\left[\sum</em>\right] \rightarrow \overbrace{\max } \gamma^{t} r_{t<em 0="0" _geq="\geq" t="t">{\pi(\cdot), \boldsymbol{\mu}(\cdot)}} \underset{\mathbb{E}}{\mathbb{E}} \underset{\pi(\cdot), \boldsymbol{\mu}(\cdot)}{\mathbb{E}}\left[\sum</em>
$$} \gamma^{t} r_{t}\right], \text { with } \boldsymbol{\mu}(\cdot) \text { being a set of intrinsic functions, </p>
<p>and where $r_{t}$ is the reward at the time step $t$ that depends on the environmental observation $\boldsymbol{o}<em t="t">{t}$ and action $\boldsymbol{a}</em>$. Furthermore, $\gamma \in[0,1)$ is a discount factor that specifies the degree to which rewards are discounted over time. The extrinsic functions still serve as actuators interacting with the outside world, whereas those additionally layered intrinsic functions are designed to encapsulate any internal reasoning process deemed beneficial by the system's architect.
Recent advancements from LLMs have demonstrated impressive successes across a broad spectrum of general reasoning and decision-making tasks to induce comprehensive prior knowledge. While fine-tunable, those language priors allow us to go beyond narrow task specifications and heavy engineering demands, and they also enable new paradigms and architectures, including, for example, memory management and tool usage, that can further aid in solving complex and challenging domains. Central to this design is the intrinsic function's role as a versatile encoder, whereby it can, for instance, facilitate the CoT mechanism [43, 46], utilising LLMs to generate and process internal thoughts, and it can serve as a dynamic memory or retrieval system, adept at accessing and utilising previously gathered information. In principle, we can introduce such intrinsic functions during any step of the RL pipeline. However, in this first version of our Pangu-Agent, we define our initial policies as (extrinsic) functions of pre-trained LLMs, which can be later trained further since those action-selection rules are a central core component of the agents' interaction with environments. Faced with a task $\ell$ and equipped with a memory $\mathrm{MEM}<em t="t">{t}^{\ell}$, our agent decides to choose an action $\boldsymbol{a}</em>$ as follows:</p>
<p>$$
\pi_{\text {Pangu-Agent }}\left(\boldsymbol{a}<em 1:="1:" t="t">{t}^{\ell} \mid \boldsymbol{o}</em>}^{\ell}, \mathrm{MEM<em 1:="1:" t="t">{t}^{\ell}\right) \equiv \mathcal{F}^{\ell}\left(\cdot \sim \operatorname{LLM}\left(\phi^{\ell}\left(\boldsymbol{o}</em>\right)\right)\right)
$$}^{\ell}, \mathrm{MEM}_{t}^{\ell</p>
<p>The above equation shows that Pangu-Agent chooses an action in a three-step nested process. First, it extracts relevant prompts for task $\ell$ using the observations so far and the current memory, i.e., the first step in the nesting $\phi^{\ell}\left(\boldsymbol{o}<em t="t">{1: t}^{\ell}, \mathrm{MEM}</em>}^{\ell}\right)$ with $\phi^{\ell}(\cdot)$ denoting a task-relevant prompt generator. The generated prompt is passed to an LLM from which a response is sampled, i.e. $\cdot \sim \operatorname{LLM}\left(\phi^{\ell}\left(\boldsymbol{o<em t="t">{1: t}^{\ell}, \mathrm{MEM}</em>}^{\ell}\right)\right)$. Finally, the third step parses this response to enable compatible action execution in the environment via a task-specific action parsing functions $\mathcal{F}^{\ell}(\cdot)$, leading us to the Pangu-Agent's policy $\pi_{\text {Pangu-Agent }}\left(\boldsymbol{a<em 1:="1:" t="t">{t}^{\ell} \mid \boldsymbol{o}</em>\right)$ described in Eq. (3).
Any entity that can learn and adapt must manage and form memories depending on past experiences and new environmental observations. Such memory components allow agents to draw upon past events to frame their understanding within the present context and can even aid in predicting and making sense of potential future events. To offer such flexibility, we do not restrict our agent to a specific form of memory but provide it with the ability to transform the internal state of the memory using a set of intrinsic functions. Those (parameterisable) functions allow specific operations, e.g., using tools, planning, reflecting on past experience, communicating with other agents, etc. - see Section 3 for a detailed exposition of such functions. Having decided on a nesting of those intrinsic functions, the agent's memory evolves to the next time step:}^{\ell}, \mathrm{MEM}_{t}^{\ell</p>
<p>$$
\operatorname{MEM}<em k="k">{t}^{\ell}=\mu</em>}^{\ell}\left(\boldsymbol{o<em 1:="1:" t-1="t-1">{1: t}^{\ell}, \boldsymbol{a}</em>}^{\ell}, \mu_{k-1}^{\ell}\left(\cdots\left(\mu_{1}^{\ell}\left(\boldsymbol{o<em 1:="1:" t-1="t-1">{1: t}^{\ell}, \boldsymbol{a}</em>\right)\right)\right)\right)
$$}^{\ell}, \mathrm{MEM}_{t-1}^{\ell</p>
<p>with each $\mu_{k}^{\ell}$ being an intrinsic function. Therefore, instead of optimising the standard RL objective, Pangu-Agent attempts to find policies of the form presented in Eq. (3) to maximise total discounted</p>
<p>returns:</p>
<p>$\max_{\pi_{\text {Pangu-Agent }}(), \boldsymbol{\mu}()} \mathbb{E}<em _Pangu-Agent="{Pangu-Agent" _text="\text">{\pi</em>\right]$
s.t. $\pi_{\text {Pangu-Agent }}\left(\boldsymbol{a}}}(\cdot)}\left[\sum_{t \geq 0} \gamma^{t} r_{t}^{\ell<em 1:="1:" t="t">{t}^{\ell} \mid \boldsymbol{o}</em>}^{\ell}, \operatorname{MEM<em 1:="1:" t="t">{t}^{\ell}\right)=\mathcal{F}^{\ell}\left(\cdot \sim \operatorname{LLM}\left(\phi^{\ell}\left(\boldsymbol{o}</em>}^{\ell}, \operatorname{MEM<em t="t">{t}^{\ell}\right)\right)\right) \forall t$
$\operatorname{MEM}</em>}^{\ell}=\mu_{k}^{\ell}\left(\boldsymbol{o<em 1:="1:" t-1="t-1">{1: t}^{\ell}, \boldsymbol{a}</em>}^{\ell}, \mu_{k-1}^{\ell}\left(\cdots\left(\mu_{1}^{\ell}\left(\boldsymbol{o<em 1:="1:" t-1="t-1">{1: t}^{\ell}, \boldsymbol{a}</em>\right)\right)\right)\right) \forall t$,
with $\boldsymbol{\mu}(\cdot)$ being the set of intrinsic functions that can be optimised in addition to the policy to maximise expected returns. This way, Pangu-Agent goes beyond other frameworks to support tunable and flexible learners at every step of the pipeline - during policy training (or RL fine-tuning of LLMs) and the nested execution of intrinsic functions $\boldsymbol{\mu}(\cdot)$. The proposed framework, which includes structure constraints and memory manipulation (also known as test-time computation), overcomes the autoregressive computational limitations of LLMs [47] and enables AI agents to achieve Turing completeness [48].}^{\ell}, \operatorname{MEM}_{t-1}^{\ell</p>
<h1>3 The Pangu-Agent Framework</h1>
<h3>3.1 Architecture of Pangu-Agent</h3>
<p>In this section, we investigate in depth the specifics of the architecture of our agent framework. As discussed in Section 2, we first define two families of functions: intrinsic and extrinsic.</p>
<p>Intrinsic Functions - Operating on the Agent's Memory: Intrinsic functions are a family of functions that operate on the agent's memory state. These functions, denoted as $\mu_{k}$, take as input the observation history $\boldsymbol{o}<em 1:="1:" t-1="t-1">{1: t}$ and action history $\boldsymbol{a}</em>$ of the agent from the previous time step. They then output a new memory state that incorporates the transformations applied by the intrinsic function. The intrinsic functions are crucial in shaping the agent's internal state and can significantly impact its decision-making process. By leveraging these functions, the agent can adapt its memory state based on the observation history and previous knowledge, making more informed and contextually appropriate decisions. Additionally, the intrinsic functions allow the incorporation of existing knowledge (through prompt engineering) and human-like structural thinking pipelines into the design of the AI agent. Examples of such functions include thinking, planning and reflecting on experience. When asking the agent to think, it looks at the problem and produces a high-level thought about the current situation. However, creating agents does not end with the methods and techniques only performed within it. Agents are built to allow interaction with other agents and external tools. Therefore, two other important intrinsic functions are the communication and tool-use. First, by enabling pair-wise communication between agents, Pangu-Agent can simulate realistic interactions and close the sim-to-real gap. Second, tool use is a generic intrinsic function that allows the utilisation of a wide range of tools, such as code interpreters and web searching.}$ provided by the environment and the current memory state $\operatorname{MEM}_{t-1</p>
<p>Extrinsic Functions - Interacting with the Environment: Extrinsic functions serve the purpose of eliciting an environment interaction from the language model. Unlike intrinsic functions that operate on the agent's memory state, extrinsic functions directly interact with the environment by generating actions to be executed. These functions take as input the observation and action history $\boldsymbol{o}<em 1:="1:" t-1="t-1">{1: t}, \boldsymbol{a}</em>$ of the agent. They utilise the transformed memory state obtained from the intrinsic functions and other contextual information to make informed decisions about the action that will maximise the agent's objective.}$ provided by the environment and the current memory state $\operatorname{MEM}_{t}^{\ell</p>
<p>Composite Methods - SwiftSage [41], ReAct [40], Least-to-Most [42], and more: The flexibility of our formulation means that many composite methods can be created hierarchically. For instance, we give examples of the ReAct, Least-to-Most, and SwiftSage frameworks. The ability to create such complex algorithms directly results from the modular formulation of Pangu-Agent. It does not mean that it is the only framework capable of these functions, but it is generic enough to support them. Additionally, it should be noted that the implementations of these composite methods that we provide in the Pangu-Agent codebase are not always faithful reproductions of the original algorithms, as these require specific task details. To allow for universal usage of these methods, we have adapted these methods to the Pangu-Agent framework by removing any task-specific details. In Section 4, we present results for these methods in different tasks. We also show how simple it is to use and create these composite methods within the Pangu-Agent framework in Appendix D.</p>
<p>Search-Enhanced Planning - BFS, DFS, and MCTS: Inspired by recent search-augmented LLMs [49, 50, 51], Pangu-Agent framework integrates three tree-search algorithms - Breadth-first/depthfirst search (BFS/DFS) and Monte-Carlo Tree Search (MCTS), to increase the planning ability for better LLM's generation and decision-making ability. Specifically, Our framework leverages LLM as policy, model, and value functions. By interacting with this LLM-based simulated environment, we can construct a rollout tree which will be further pruned for better action/generation using tree-search algorithms. We conduct initial experiments on GSM8K and Game24 leveraging our framework shown in Appendix E and we refer the readers to [51] for more detailed evaluation results.</p>
<p>Task Interaction and Multi-Agent Systems: Pangu-Agent is compatible with a range of tasks, for example, ALFWorld [44], GSM8K [52], HotpotQA [53], WebShop [54], etc. The interface, influenced by OpenAI's Gym, is an open design that facilitates task interactions with any system where an agent performs actions and receives observations and rewards. Support for multi-agent environments is even built into the foundation of our framework, ensuring agents can work together or independently within the same task. In settings with multiple agents, we denote them with the subscript $i$. The interaction between one or more agents and an environment in our setting can be captured by the Partially Observable Stochastic Game [55] framework, which collapses to a Partially Observable Markov Decision Process in case there is only a single agent in the task.</p>
<p>Prompting System: The framework incorporates a template system to generate inputs for the LLMs. Utilising templates enhances the flexibility of prompt crafting, making various prompting mechanisms possible. The system also promotes extensibility, with new prompting mechanisms or environments easily incorporated into the existing framework. Most intrinsic and extrinsic functions contain their task-specific prompting strategy, which uses the memory to create the prompt that will be inputted to the LLM.</p>
<h1>3.2 Adaptation of LLM Priors \&amp; Fine-Tuning</h1>
<p>In introducing the architecture above, we aim for generalist agents that adapt based on expert data and environmental feedback using supervised fine-tuning and RL. We believe the interplay between structured reasoning from intrinsic functions and extrinsic policies promises general RL solvers that combine well with large model policies like those induced by LLMs. With our framework, its function design, and prompting strategies, we can now collect valuable rewarding trajectories from (open-sourced) pre-trained LLM priors to kickstart the training and fine-tuning process effectively.
We differentiate between two types of fine-tuning methods: $i$ ) those that do not interact with an environment, which we refer to as supervised fine-tuning algorithms, and $i$ i) those that learn by agent-environment interactions to maximise expected returns that we dub under RL-based fine-tuning techniques. Fine-tuning requires practical algorithms that update the weights of the LLM regardless of the type used. It is customary to define loss functions - e.g., predictive likelihood - and to rely on gradient descent when searching for update directions. However, computing those gradients for each weight of LLMs is often infeasible without accessing GPUs with vast memory capabilities. Hence, we support full-rank and low-rank adaptation algorithms to democratise and enable our agent's broader usage. This way, users can perform complete weight updates if their computing capabilities allow or rely otherwise on low-rank adaptation (LoRA) [56].</p>
<p>Fine-Tuning Type I: To support the first type of fine-tuning (i.e., supervised), we allow the collection of successful trajectories from an environment via the different prompting strategies introduced before. Of course, we do not restrict our pipeline to data collection but also allow for the standard setup of introducing external expert datasets in case they are available. Given such a data set $\mathcal{D}$, we rely on causal language modelling losses - a special case of likelihood optimisation - and perform full- or low-rank updates to minimise:</p>
<p>$$
\mathcal{L}\left(\mathcal{D}, \boldsymbol{\theta}<em n="1">{\mathrm{LLM}}\right)=-\frac{1}{N} \sum</em>\right)
$$}^{N} \log p\left(x_{n} \mid x_{i \leq n-1}, \boldsymbol{\theta}_{\mathrm{LLM}</p>
<p>where $\boldsymbol{\theta}<em n="n">{\mathrm{LLM}}$ are the LLM's weights to be updated. Equation (4) maximises the likelihood of the observed token $\boldsymbol{x}</em>}$ given all the previous ones. Since we are in an agent framework, our loss is only defined by the tokens generated by the agent and not the ones provided by the environment (or by the system prompts). Updates of $\boldsymbol{\theta<em L="L" M="M">{\mathrm{LLM}}$ can be executed either fully if using full-rank optimisation or by parameterising them via two low-rank matrices in the LoRA scenario. Here, $\boldsymbol{\theta}</em>$,}=\boldsymbol{\theta}_{L L M}^{0}+\boldsymbol{B} \boldsymbol{A</p>
<p>where $\boldsymbol{\theta}<em _mathrm_LLM="\mathrm{LLM">{L L M}^{0} \in \mathbb{R}^{d \times k}$ are the weights of the pre-trained LLM and $\boldsymbol{B} \in \mathbb{R}^{d \times r}$ and $\boldsymbol{A} \in \mathbb{R}^{r \times k}$ are tunable parameters of rank $r&lt;&lt;\min {d, k}$. Here, we follow LoRA [56] in keeping $\boldsymbol{\theta}</em>$.}}^{0}$ fixed and performing gradient updates only for $\boldsymbol{B}$ and $\boldsymbol{A</p>
<p>Fine-Tuning Type II: Regarding RL fine-tuners, we allow agents to interact with environments, as shown in Fig. 1. Given a system prompt and access to an environment, the agent applies actions after which the environment transitions to a new state. This process repeats until the end of an episode, at which stage our learner receives a (sparse) reward signal. Given those rewards, we execute the PPO algorithm [57] to improve our agent's policy to maximise expected returns.
Of course, this setup is standard for deep RL. However, we now elaborate on two crucial differences to deep RL. First, our agent uses intrinsic functions before deciding on an action via the policy. In our experiments, those intrinsic functions were relatively simple, whereby they ordered and arranged data from memory to generate suitable trajectories for extrinsic functions and shrank first observations and actions when exceeding context length constraints. Second, the vital difference with standard RL pipelines is that our agent utilises LLM policies. While such policies offer prior flexibility, they arrive with new generation speed and credit assignment problems if actions can be any text prompt.</p>
<p>On Accelerating Generation: A well-known bottleneck for successful RL with LLM policies is the speed by which LLMs generate actions. In an RL setting, slow-generation speeds coupled with the high-sample complexities typical for model-free algorithms lead to impractical learners, especially when only having access to modest computing hardware. We rely on three methods to accelerate generation speed: $i$ ) continuous batching, ii) optimised kernels and iii) caching with Paged Attention [58]. Since we collect trajectories in parallel with multiple workers interacting, each with its environment, continuous batching allows us to batch queries to the LLM and to possibly receive an answer before the whole batch computations are over. Optimised attention kernels are written in $\mathrm{C}++$ /CUDA to enable faster memory-efficient inference than vanilla implementations. We notably relied on the xFormers library [59], which also features fused operations for the softmax, linear layer, dropout and layer normalisation operations. Those fused operations notably reduce the amount of reading and writing back to the DRAM compared to vanilla PyTorch implementation. Finally, we relied on Paged Attention [58] to cache the key and value in attention operations. Since the generation is autoregressive, it is unnecessary to recompute all the previous tokens' keys and values if they were computed before. With those additions, we observed that we could reduce the generation time by a factor of 3 compared to vanilla HuggingFace for the Llama 2-7B parameters model.</p>
<p>On Credit Assignment: To work with environments where actions can be any textual prompt (e.g. in code generation) and in environments with a restricted set of actions, we would need to define the policy loss and the associated likelihood over every token composing an action. However, when defining critics in RL, credit must be assigned to an action, not a token. For instance, if the sequence of the following three tokens $[\langle$ Move $\rangle,\langle \rangle,\langle$ Right $\rangle]$ constitutes one action (i.e. that which moves the agent to the right), we would need to assign the same value for each of those three tokens. To support this form of credit assignment, we introduce a new version of generalised advantage estimation in PPO, where we define:</p>
<p>$$
A_{t}=\sum_{n=t}^{\infty} \mathbb{1}<em _sigma_n_1="\sigma(n)+1">{n \in \mathcal{A}}(\lambda \gamma)^{\sigma(n)-\sigma(t)}\left(r</em>\right)\right)
$$}+\gamma V\left(s_{\sigma(n)+1}\right)-V\left(s_{\sigma(n)</p>
<p>where $\lambda$ is a hyper-parameter that balances the bias-variance tradeoff, $\gamma$ is a discount factor that weighs the future returns, $\overline{\mathcal{A}}$ is the set composed of the index of the last token of every action, and $\sigma: \mathbb{N} \rightarrow \mathbb{N}$ is a mapping from the token index to the action index. Using $\overline{\mathcal{A}}$, we can keep track of the tokens corresponding to one action to perform the correct credit assignment. Finally, $V(s)$ is the current value estimate of a state $s$. We add a second head to the LLM from the last embedding layer to allow the critic to accurately approximate $V(s)$. We followed PPO [57] to train this critic and minimise the squared temporal difference loss.</p>
<h1>4 Evaluating Pangu-Agent</h1>
<p>In this section, we conduct an extensive evaluation of various methods supported by Pangu-Agent, including both structured reasoning methods and fine-tuning. First, we evaluate the structured</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Visualisation of one example of structured reasoning using nesting of intrinsic and extrinsic functions. The agent initially updates its internal memory, using $\mu_{0}$, by perceiving its observation. Then the intrinsic function $\mu_{1}$ selects between Think-and-Act or just Act. The last intrinsic function $\mu_{2}$ either generates a thought if $\mu_{1}$ previously selected Think-and-Act otherwise it is null. Finally, the extrinsic function $\pi$ selects the action that the agent will perform in the environment.
reasoning ability of Pangu-Agent (see Figure 3, by considering both first-order nesting and composite methods, and afterwards we evaluate fine-tuning of Pangu-Agent in three distinct settings using supervised learning and RL. Our results indicate that composite methods tend to outperform firstorder nesting methods with respect to the achieved returns of the agent. Finally, we show that SFT and RLFT allow the agent to specialise and further increase its achieved returns in the ALFWorld and BabyAI tasks. Throughout this evaluation, several LLMs are used, such as GPT [60], Llama 2 [61], OpenChat [62], Vicuna [63], and Mistral [64].</p>
<h1>4.1 Evaluating Structured Reasoning</h1>
<p>The built-in support for intrinsic functions allows us to evaluate how different design choices in the reasoning structure affect the performance of an AI agent. First, in Table 2, we evaluate first-order nestings, i.e. setups in which an agent's memory is modified only by observing the environment and the actions it performs on it. In the literature, these would be referred to simply as different prompting methods e.g.: Direct prompting, prompting with Few-Shot (FS) examples, Few-Shot Chain of Thought (FS-CoT) [43], Zero-Shot CoT (ZS-CoT) [65] - with a thorough description of these methods presented in Appendix A.1. It should be noted that due to the non-deterministic nature of LLM's text generation, the achieved returns can vary significantly between different runs. To account for these variations, we run each combination of task-method-LLM three times and report the mean standard deviation.</p>
<p>But first-order nestings have limits as they may struggle to fully use the capabilities of an LLM. As motivated in the introduction (Section 1), an agent needs to be able to process the output of the language model, revisit its answers, change its memory and even use tools. Composite methods, as we call them, are methods that may require multiple thinking steps until a final action is decided. In Table 3, we present results for four composite methods: FS-CoT with Self-Consistency (FS-CoTSC) [39], FS-CoT with an optional distinct thinking step (e.g. React [40]), FS-CoT with a reflection step [e.g. 66], SwiftSage [41], and Least-to-Most [42] (also see Appendix A.2). All these methods make use of multiple intrinsic function steps at each environment time step. Refer to Table 7 for brief descriptions of all method acronyms.</p>
<p>We observe that methods that are similar in their structure but differ in their prompt content, such as Reflect and React, yield significantly different achieved returns for the agent. This demonstrates the importance of careful prompt engineering. It is also noteworthy that different methods work better for some LLMs than others, e.g. React in OpenChat-3.2 performs worse than FS on average, while React and FS in GPT-3.5 perform similarly in terms of average achieved returns.</p>
<p>Notably, the performance of FS in GSM8K is considerably worse than Direct across all LLMs. This does not come as a surprise since FS presents only the final answer to the LLM. Therefore, the LLM aims to answer the question without generating the intermediate steps. However, in Direct, the LLM generates the intermediate steps even without explicitly requested as this is how similar grade-school</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LLM</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Overall</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ALFWorld</td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">HotpotQA</td>
<td style="text-align: center;">WebShop</td>
<td style="text-align: center;">HumanEval</td>
<td style="text-align: center;">BabyAI</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">Direct</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">$4.7 \pm 2.5$</td>
<td style="text-align: center;">$69.2 \pm 0.8$</td>
<td style="text-align: center;">$32.8 \pm 0.7$</td>
<td style="text-align: center;">$22.5 \pm 3.5$</td>
<td style="text-align: center;">$58.2 \pm 2.1$</td>
<td style="text-align: center;">$28.3 \pm 4.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ZS-CoT</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">$24.7 \pm 1.9$</td>
<td style="text-align: center;">$65.8 \pm 0.9$</td>
<td style="text-align: center;">$25.8 \pm 0.1$</td>
<td style="text-align: center;">$6.9 \pm 1.3$</td>
<td style="text-align: center;">$17.8 \pm 0.8$</td>
<td style="text-align: center;">$34.6 \pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FS</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">$12.7 \pm 3.8$</td>
<td style="text-align: center;">$35.0 \pm 11.5$</td>
<td style="text-align: center;">$45.3 \pm 0.7$</td>
<td style="text-align: center;">$34.4 \pm 1.2$</td>
<td style="text-align: center;">$66.5 \pm 3.5$</td>
<td style="text-align: center;">$36.2 \pm 2.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FS-CoT</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">$40.3 \pm 3.1$</td>
<td style="text-align: center;">$66.4 \pm 0.1$</td>
<td style="text-align: center;">$40.5 \pm 0.3$</td>
<td style="text-align: center;">$38.9 \pm 1.7$</td>
<td style="text-align: center;">$63.1 \pm 3.7$</td>
<td style="text-align: center;">$50.2 \pm 4.3$</td>
</tr>
<tr>
<td style="text-align: center;">Llama 2-560</td>
<td style="text-align: center;">Direct</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">$5.3 \pm 0.9$</td>
<td style="text-align: center;">$28.8 \pm 0.7$</td>
<td style="text-align: center;">$27.6 \pm 0.4$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$23.2 \pm 1.5$</td>
<td style="text-align: center;">$22.2 \pm 2.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ZS-CoT</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">$11.3 \pm 0.9$</td>
<td style="text-align: center;">$48.7 \pm 1.0$</td>
<td style="text-align: center;">$33.9 \pm 0.5$</td>
<td style="text-align: center;">$6.6 \pm 2.2$</td>
<td style="text-align: center;">$11.8 \pm 2.0$</td>
<td style="text-align: center;">$13.7 \pm 1.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FS</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">$5.3 \pm 3.4$</td>
<td style="text-align: center;">$32.2 \pm 0.7$</td>
<td style="text-align: center;">$31.0 \pm 0.4$</td>
<td style="text-align: center;">$1.2 \pm 0.8$</td>
<td style="text-align: center;">$15.7 \pm 1.1$</td>
<td style="text-align: center;">$12.3 \pm 3.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FS-CoT</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">$18.0 \pm 4.3$</td>
<td style="text-align: center;">$53.0 \pm 0.6$</td>
<td style="text-align: center;">$49.0 \pm 0.6$</td>
<td style="text-align: center;">$5.7 \pm 1.6$</td>
<td style="text-align: center;">$19.0 \pm 0.3$</td>
<td style="text-align: center;">$21.6 \pm 3.1$</td>
</tr>
<tr>
<td style="text-align: center;">OpenChat-3.5</td>
<td style="text-align: center;">Direct</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$7.6 \pm 0.6$</td>
<td style="text-align: center;">$43.0 \pm 0.4$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$8.5 \pm 2.1$</td>
<td style="text-align: center;">$9.8 \pm 4.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ZS-CoT</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">$17.3 \pm 6.6$</td>
<td style="text-align: center;">$22.7 \pm 0.5$</td>
<td style="text-align: center;">$28.6 \pm 0.2$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$2.5 \pm 0.9$</td>
<td style="text-align: center;">$17.7 \pm 4.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FS</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">$2.0 \pm 1.6$</td>
<td style="text-align: center;">$2.2 \pm 0.4$</td>
<td style="text-align: center;">$47.0 \pm 0.3$</td>
<td style="text-align: center;">$22.2 \pm 3.0$</td>
<td style="text-align: center;">$1.4 \pm 0.6$</td>
<td style="text-align: center;">$9.0 \pm 6.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FS-CoT</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">$26.7 \pm 4.1$</td>
<td style="text-align: center;">$21.1 \pm 0.9$</td>
<td style="text-align: center;">$39.2 \pm 0.3$</td>
<td style="text-align: center;">$3.2 \pm 2.5$</td>
<td style="text-align: center;">$1.2 \pm 0.0$</td>
<td style="text-align: center;">$36.8 \pm 11.5$</td>
</tr>
<tr>
<td style="text-align: center;">Vimeo-LM</td>
<td style="text-align: center;">Direct</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">$3.3 \pm 2.5$</td>
<td style="text-align: center;">$10.8 \pm 0.2$</td>
<td style="text-align: center;">$24.9 \pm 0.5$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$21.0 \pm 2.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ZS-CoT</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">$13.3 \pm 1.9$</td>
<td style="text-align: center;">$23.8 \pm 0.3$</td>
<td style="text-align: center;">$19.3 \pm 0.5$</td>
<td style="text-align: center;">$0.4 \pm 0.3$</td>
<td style="text-align: center;">$0.2 \pm 0.3$</td>
<td style="text-align: center;">$22.9 \pm 2.8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FS</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">$11.3 \pm 2.5$</td>
<td style="text-align: center;">$6.2 \pm 0.6$</td>
<td style="text-align: center;">$30.5 \pm 0.4$</td>
<td style="text-align: center;">$12.2 \pm 0.7$</td>
<td style="text-align: center;">$1.2 \pm 0.9$</td>
<td style="text-align: center;">$27.0 \pm 2.3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FS-CoT</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">$15.3 \pm 0.9$</td>
<td style="text-align: center;">$27.3 \pm 0.9$</td>
<td style="text-align: center;">$32.1 \pm 0.9$</td>
<td style="text-align: center;">$1.0 \pm 0.8$</td>
<td style="text-align: center;">$2.1 \pm 1.1$</td>
<td style="text-align: center;">$29.2 \pm 2.9$</td>
</tr>
<tr>
<td style="text-align: center;">Llama 2-156</td>
<td style="text-align: center;">Direct</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$16.7 \pm 0.6$</td>
<td style="text-align: center;">$27.6 \pm 0.4$</td>
<td style="text-align: center;">$3.5 \pm 1.2$</td>
<td style="text-align: center;">$11.6 \pm 1.3$</td>
<td style="text-align: center;">$8.7 \pm 5.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ZS-CoT</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">$0.7 \pm 0.9$</td>
<td style="text-align: center;">$31.6 \pm 0.6$</td>
<td style="text-align: center;">$25.4 \pm 0.4$</td>
<td style="text-align: center;">$0.4 \pm 0.6$</td>
<td style="text-align: center;">$9.7 \pm 0.3$</td>
<td style="text-align: center;">$12.9 \pm 3.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FS</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$7.9 \pm 0.1$</td>
<td style="text-align: center;">$35.4 \pm 0.7$</td>
<td style="text-align: center;">$10.9 \pm 1.5$</td>
<td style="text-align: center;">$8.9 \pm 1.1$</td>
<td style="text-align: center;">$9.8 \pm 2.3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FS-CoT</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">$6.0 \pm 1.6$</td>
<td style="text-align: center;">$33.5 \pm 0.5$</td>
<td style="text-align: center;">$23.9 \pm 0.9$</td>
<td style="text-align: center;">$11.2 \pm 1.2$</td>
<td style="text-align: center;">$2.1 \pm 0.3$</td>
<td style="text-align: center;">$21.5 \pm 4.2$</td>
</tr>
<tr>
<td style="text-align: center;">Llama 2-18</td>
<td style="text-align: center;">Direct</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">$4.0 \pm 0.0$</td>
<td style="text-align: center;">$37.6 \pm 0.6$</td>
<td style="text-align: center;">$47.4 \pm 0.4$</td>
<td style="text-align: center;">$35.4 \pm 0.8$</td>
<td style="text-align: center;">$32.9 \pm 0.0$</td>
<td style="text-align: center;">$28.1 \pm 0.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ZS-CoT</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">$10.7 \pm 5.7$</td>
<td style="text-align: center;">$64.9 \pm 0.4$</td>
<td style="text-align: center;">$41.2 \pm 1.0$</td>
<td style="text-align: center;">$17.8 \pm 1.4$</td>
<td style="text-align: center;">$39.1 \pm 4.3$</td>
<td style="text-align: center;">$35.7 \pm 1.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FS</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">$7.3 \pm 2.5$</td>
<td style="text-align: center;">$24.7 \pm 1.1$</td>
<td style="text-align: center;">$57.2 \pm 0.5$</td>
<td style="text-align: center;">$43.5 \pm 1.0$</td>
<td style="text-align: center;">$33.3 \pm 3.0$</td>
<td style="text-align: center;">$20.9 \pm 6.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FS-CoT</td>
<td style="text-align: center;">47.4</td>
<td style="text-align: center;">$27.3 \pm 3.8$</td>
<td style="text-align: center;">$70.1 \pm 0.5$</td>
<td style="text-align: center;">$64.3 \pm 0.3$</td>
<td style="text-align: center;">$35.5 \pm 2.1$</td>
<td style="text-align: center;">$28.0 \pm 3.2$</td>
<td style="text-align: center;">$58.9 \pm 6.7$</td>
</tr>
<tr>
<td style="text-align: center;">Muteal-75</td>
<td style="text-align: center;">Direct</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">$4.0 \pm 1.6$</td>
<td style="text-align: center;">$14.1 \pm 0.7$</td>
<td style="text-align: center;">$30.3 \pm 0.1$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$10.8 \pm 0.6$</td>
<td style="text-align: center;">$7.7 \pm 3.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ZS-CoT</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">$9.3 \pm 3.8$</td>
<td style="text-align: center;">$33.9 \pm 0.7$</td>
<td style="text-align: center;">$18.3 \pm 0.5$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$0.4 \pm 0.3$</td>
<td style="text-align: center;">$17.4 \pm 4.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FS</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">$6.0 \pm 1.6$</td>
<td style="text-align: center;">$0.5 \pm 0.0$</td>
<td style="text-align: center;">$32.9 \pm 0.5$</td>
<td style="text-align: center;">$0.7 \pm 0.5$</td>
<td style="text-align: center;">$3.7 \pm 0.5$</td>
<td style="text-align: center;">$6.0 \pm 3.3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FS-CoT</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">$6.0 \pm 2.5$</td>
<td style="text-align: center;">$32.7 \pm 0.3$</td>
<td style="text-align: center;">$28.0 \pm 1.1$</td>
<td style="text-align: center;">$0.9 \pm 0.4$</td>
<td style="text-align: center;">$8.9 \pm 2.4$</td>
<td style="text-align: center;">$28.6 \pm 3.9$</td>
</tr>
</tbody>
</table>
<p>Table 2: Average achieved returns and the standard deviation across three runs, for four first-order prompt engineering methods on six different tasks, using seven different LLMs.
level questions are presented on the internet, which are likely contained within the training set of these LLMs. Similar conclusions can be drawn when comparing ZS-CoT with FS, where we observe that the achieved returns are increased even compared to Direct. This is especially true for smaller LLMs where we conjecture that when adding the "think step-by-step" quote into the prompt, the model is more likely to generate reasoning steps that will allow correctly solving the question at hand.</p>
<p>In the HumanEval task, we observe that the difference in the achieved returns between GPT-3.5 and the remaining models is significantly larger compared to other tasks. This can be attributed to the fact that HumanEval is a coding task, that requires well-structured responses, such as correct indentation, from the LLM. However, smaller and open-source LLMs are more prone to these structural errors which result in failing the task and receiving a return of 0 .</p>
<p>Another factor that impedes the performance of LLMs is the limited context length. In tasks such as WebShop, which involves relatively large observations, the length of the prompt needs to be truncated to stay within the allowed context length. Consequently, the performance of LLMs in this task can be substantially affected, particularly in methods such as Reflect, where additional information is also included in the prompt. This explains why the Reflect method tends to under-perform in WebShop compared to other methods.</p>
<p>In several cases, FS-CoT-SC can improve the achieved returns of the LLM, especially in GSM8K. However, this comes with the extra cost of needing to prompt the LLM several times ( 5 in the presented experiments) to perform the SC action selection. In tasks such as HumanEval, where the answer contains long textual answers and potentially several answers can yield the correct outcome, we found that SC cannot be applied. This happens because the LLM will never generate the same answer as before, and the SC action selector cannot choose the most frequent answer.</p>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Method</th>
<th>Overall</th>
<th>Task</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>ALFWorld</td>
<td>GSM8K</td>
<td>HotpotQA</td>
<td>WebShop</td>
<td>HumanEval</td>
<td>BabyAI</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>FS-CoT-SC</td>
<td>48.2</td>
<td>$34.4 \pm 2.7$</td>
<td>$74.1 \pm 0.8$</td>
<td>$43.5 \pm 0.1$</td>
<td>$39.1 \pm 1.9$</td>
<td>-</td>
<td>$50.0 \pm 7.9$</td>
</tr>
<tr>
<td></td>
<td>FS-CoT-React</td>
<td>45.8</td>
<td>$39.5 \pm 3.6$</td>
<td>$66.9 \pm 1.2$</td>
<td>$38.1 \pm 0.1$</td>
<td>$28.5 \pm 3.4$</td>
<td>$61.3 \pm 2.8$</td>
<td>$40.8 \pm 2.3$</td>
</tr>
<tr>
<td></td>
<td>FS-CoT-Reflect</td>
<td>34.0</td>
<td>$26.7 \pm 1.9$</td>
<td>$70.8 \pm 0.8$</td>
<td>$32.5 \pm 0.9$</td>
<td>$0.9 \pm 0.4$</td>
<td>$54.5 \pm 3.6$</td>
<td>$19.0 \pm 2.5$</td>
</tr>
<tr>
<td></td>
<td>FS-Least-to-Most</td>
<td>39.8</td>
<td>-</td>
<td>$60.0 \pm 0.3$</td>
<td>$20.3 \pm 0.4$</td>
<td>-</td>
<td>$39.1 \pm 1.3$</td>
<td>-</td>
</tr>
<tr>
<td>Llama 2-70B</td>
<td>FS-CoT-SC</td>
<td>29.7</td>
<td>$13.3 \pm 2.5$</td>
<td>$59.7 \pm 0.8$</td>
<td>$52.1 \pm 0.3$</td>
<td>$2.2 \pm 0.3$</td>
<td>-</td>
<td>$21.0 \pm 1.3$</td>
</tr>
<tr>
<td></td>
<td>FS-CoT-React</td>
<td>21.0</td>
<td>$3.3 \pm 1.9$</td>
<td>$48.7 \pm 1.1$</td>
<td>$41.8 \pm 0.8$</td>
<td>$0.7 \pm 1.0$</td>
<td>$12.6 \pm 1.5$</td>
<td>$19.0 \pm 2.6$</td>
</tr>
<tr>
<td></td>
<td>FS-CoT-Reflect</td>
<td>24.6</td>
<td>$18.7 \pm 3.4$</td>
<td>$56.8 \pm 1.1$</td>
<td>$35.8 \pm 0.8$</td>
<td>$1.9 \pm 1.2$</td>
<td>$19.0 \pm 0.8$</td>
<td>$15.5 \pm 2.1$</td>
</tr>
<tr>
<td></td>
<td>FS-CoT-Swift-Sage</td>
<td>23.5</td>
<td>$28.0 \pm 5.7$</td>
<td>-</td>
<td>-</td>
<td>$15.4 \pm 0.9$</td>
<td>-</td>
<td>$27.2 \pm 2.1$</td>
</tr>
<tr>
<td></td>
<td>FS-Least-to-Most</td>
<td>16.2</td>
<td>-</td>
<td>$31.2 \pm 0.5$</td>
<td>$15.0 \pm 0.2$</td>
<td>-</td>
<td>$2.5 \pm 1.3$</td>
<td>-</td>
</tr>
<tr>
<td>OpenChat-3.5</td>
<td>FS-CoT-SC</td>
<td>25.3</td>
<td>$21.3 \pm 1.9$</td>
<td>$21.2 \pm 0.6$</td>
<td>$42.7 \pm 0.6$</td>
<td>$1.3 \pm 0.9$</td>
<td>-</td>
<td>$40.0 \pm 3.6$</td>
</tr>
<tr>
<td></td>
<td>FS-CoT-React</td>
<td>11.3</td>
<td>$2.7 \pm 0.9$</td>
<td>$6.6 \pm 0.7$</td>
<td>$42.6 \pm 1.0$</td>
<td>$0.6 \pm 0.5$</td>
<td>$3.3 \pm 1.5$</td>
<td>$11.9 \pm 2.4$</td>
</tr>
<tr>
<td></td>
<td>FS-CoT-Reflect</td>
<td>16.7</td>
<td>$20.7 \pm 9.3$</td>
<td>$26.2 \pm 0.8$</td>
<td>$25.2 \pm 0.3$</td>
<td>$0.0 \pm 0.0$</td>
<td>$3.9 \pm 0.8$</td>
<td>$24.3 \pm 0.8$</td>
</tr>
<tr>
<td></td>
<td>FS-CoT-Swift-Sage</td>
<td>21.3</td>
<td>$24.0 \pm 6.9$</td>
<td>-</td>
<td>-</td>
<td>$13.2 \pm 2.2$</td>
<td>-</td>
<td>$26.6 \pm 3.1$</td>
</tr>
<tr>
<td></td>
<td>FS-Least-to-Most</td>
<td>8.6</td>
<td>-</td>
<td>$14.0 \pm 0.6$</td>
<td>$11.3 \pm 0.6$</td>
<td>-</td>
<td>$0.4 \pm 0.6$</td>
<td>-</td>
</tr>
<tr>
<td>Verena-LIB</td>
<td>FS-CoT-SC</td>
<td>25.6</td>
<td>$18.7 \pm 2.5$</td>
<td>$37.7 \pm 0.5$</td>
<td>$38.2 \pm 0.2$</td>
<td>$0.0 \pm 0.0$</td>
<td>-</td>
<td>$33.4 \pm 0.9$</td>
</tr>
<tr>
<td></td>
<td>FS-CoT-React</td>
<td>16.1</td>
<td>$17.3 \pm 1.9$</td>
<td>$19.0 \pm 1.1$</td>
<td>$26.0 \pm 1.4$</td>
<td>$0.0 \pm 0.0$</td>
<td>$1.2 \pm 0.9$</td>
<td>$33.0 \pm 7.5$</td>
</tr>
<tr>
<td></td>
<td>FS-CoT-Reflect</td>
<td>16.9</td>
<td>$17.3 \pm 3.4$</td>
<td>$32.8 \pm 1.0$</td>
<td>$21.7 \pm 0.4$</td>
<td>$0.3 \pm 0.4$</td>
<td>$4.8 \pm 1.8$</td>
<td>$24.3 \pm 4.2$</td>
</tr>
<tr>
<td></td>
<td>FS-CoT-Swift-Sage</td>
<td>22.5</td>
<td>$27.3 \pm 3.4$</td>
<td>-</td>
<td>-</td>
<td>$13.9 \pm 2.2$</td>
<td>-</td>
<td>$26.2 \pm 4.1$</td>
</tr>
<tr>
<td></td>
<td>FS-Least-to-Most</td>
<td>10.0</td>
<td>-</td>
<td>$19.7 \pm 0.4$</td>
<td>$10.1 \pm 0.3$</td>
<td>-</td>
<td>$0.2 \pm 0.3$</td>
<td>-</td>
</tr>
<tr>
<td>Llama 2-13B</td>
<td>FS-CoT-SC</td>
<td>20.1</td>
<td>$3.3 \pm 0.9$</td>
<td>$39.3 \pm 0.6$</td>
<td>$40.8 \pm 0.7$</td>
<td>$2.7 \pm 1.6$</td>
<td>-</td>
<td>$14.5 \pm 4.5$</td>
</tr>
<tr>
<td></td>
<td>FS-CoT-React</td>
<td>16.1</td>
<td>$0.0 \pm 0.0$</td>
<td>$25.9 \pm 0.5$</td>
<td>$39.0 \pm 0.5$</td>
<td>$2.1 \pm 0.9$</td>
<td>$3.7 \pm 0.9$</td>
<td>$25.6 \pm 7.5$</td>
</tr>
<tr>
<td></td>
<td>FS-CoT-Reflect</td>
<td>12.8</td>
<td>$10.7 \pm 2.5$</td>
<td>$32.4 \pm 0.8$</td>
<td>$11.0 \pm 0.4$</td>
<td>$0.3 \pm 0.4$</td>
<td>$2.1 \pm 0.6$</td>
<td>$20.6 \pm 2.5$</td>
</tr>
<tr>
<td></td>
<td>FS-CoT-Swift-Sage</td>
<td>18.3</td>
<td>$22.7 \pm 0.9$</td>
<td>-</td>
<td>-</td>
<td>$11.8 \pm 4.9$</td>
<td>-</td>
<td>$20.6 \pm 2.5$</td>
</tr>
<tr>
<td></td>
<td>FS-Least-to-Most</td>
<td>8.7</td>
<td>-</td>
<td>$12.2 \pm 0.8$</td>
<td>$13.1 \pm 0.5$</td>
<td>-</td>
<td>$0.8 \pm 0.8$</td>
<td>-</td>
</tr>
<tr>
<td>OpenChat-3.5</td>
<td>FS-CoT-SC</td>
<td>53.5</td>
<td>$28.0 \pm 1.6$</td>
<td>$80.8 \pm 0.2$</td>
<td>$70.4 \pm 0.7$</td>
<td>$42.9 \pm 2.0$</td>
<td>$32.9 \pm 0.9$</td>
<td>$66.1 \pm 6.4$</td>
</tr>
<tr>
<td></td>
<td>FS-CoT-React</td>
<td>39.0</td>
<td>$24.7 \pm 1.9$</td>
<td>$62.1 \pm 0.3$</td>
<td>$48.2 \pm 1.0$</td>
<td>$33.6 \pm 0.7$</td>
<td>$29.2 \pm 1.8$</td>
<td>$36.2 \pm 3.6$</td>
</tr>
<tr>
<td></td>
<td>FS-CoT-Reflect</td>
<td>39.4</td>
<td>$28.7 \pm 5.7$</td>
<td>$74.5 \pm 0.2$</td>
<td>$57.0 \pm 0.1$</td>
<td>$14.1 \pm 3.2$</td>
<td>$27.1 \pm 3.6$</td>
<td>$34.8 \pm 1.2$</td>
</tr>
<tr>
<td></td>
<td>FS-CoT-Swift-Sage</td>
<td>37.7</td>
<td>$37.3 \pm 6.2$</td>
<td>-</td>
<td>-</td>
<td>$31.5 \pm 1.4$</td>
<td>-</td>
<td>$44.3 \pm 3.5$</td>
</tr>
<tr>
<td></td>
<td>FS-Least-to-Most</td>
<td>36.0</td>
<td>-</td>
<td>$59.1 \pm 0.6$</td>
<td>$32.5 \pm 0.1$</td>
<td>-</td>
<td>$16.4 \pm 0.6$</td>
<td>-</td>
</tr>
<tr>
<td>Mistral-7B</td>
<td>FS-CoT-SC</td>
<td>21.3</td>
<td>$6.0 \pm 1.6$</td>
<td>$38.2 \pm 0.7$</td>
<td>$37.8 \pm 0.4$</td>
<td>$0.3 \pm 0.4$</td>
<td>-</td>
<td>$24.4 \pm 3.3$</td>
</tr>
<tr>
<td></td>
<td>FS-CoT-React</td>
<td>11.4</td>
<td>$5.3 \pm 1.9$</td>
<td>$19.9 \pm 1.1$</td>
<td>$28.9 \pm 0.9$</td>
<td>$0.6 \pm 0.8$</td>
<td>$5.8 \pm 3.1$</td>
<td>$7.9 \pm 2.0$</td>
</tr>
<tr>
<td></td>
<td>FS-CoT-Reflect</td>
<td>15.0</td>
<td>$7.3 \pm 0.9$</td>
<td>$34.4 \pm 1.4$</td>
<td>$19.6 \pm 0.9$</td>
<td>$0.0 \pm 0.0$</td>
<td>$5.2 \pm 0.3$</td>
<td>$23.3 \pm 2.5$</td>
</tr>
<tr>
<td></td>
<td>FS-CoT-Swift-Sage</td>
<td>19.4</td>
<td>$16.7 \pm 5.0$</td>
<td>-</td>
<td>-</td>
<td>$12.4 \pm 3.0$</td>
<td>-</td>
<td>$29.2 \pm 2.8$</td>
</tr>
<tr>
<td></td>
<td>FS-Least-to-Most</td>
<td>5.6</td>
<td>-</td>
<td>$5.5 \pm 0.7$</td>
<td>$10.8 \pm 0.3$</td>
<td>-</td>
<td>$0.6 \pm 0.5$</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Table 3: Average achieved returns and the standard deviation across three runs, for five composite reasoning methods on six different tasks, using seven different LLMs.</p>
<h1>4.2 Evaluating Extrinsic Functions: Fine-Tuning</h1>
<p>By closely observing the results in Section 4.1, we can conclude that while LLMs can perform well in achieving returns in a wide range of tasks, there is a large room for improvement towards achieving a $100 \%$ success rate. In this section, we aim to explore how SFT and RLFT can help Pangu-Agent increase the returns it achieves. We propose two different pipelines: a Bootstrap SFT (BSFT) that consists of a multi-turn trajectory generation and SFT and a three-step pipeline consisting of trajectory generation, SFT and RLFT. Expert trajectory demonstrations, for performing SFT, are always gathered using the OpenChat-3.5 LLM equipped with the structured reasoning abilities of the Pangu-Agent framework. We perform BSFT using the OpenChat-3.5 LLM, while the SFT-RLFT pipeline is applied to the Llama 2-7B LLM. We consider two distinct evaluation paradigms: fine-tuning a different LLM for each task and fine-tuning an LLM in several tasks (e.g. multi-task fine-tuning).</p>
<h3>4.2.1 One Model per Domain</h3>
<p>BSFT: In the first experiment, we show a combination of the intrinsic functions and the fine-tuning offered by the Pangu-Agent framework. We first collect data from a diverse set of prompting methods, specifically ZS-CoT, FS-CoT, FS-CoT-React, and FS-CoT-Reflect. After this data collection, we run a rejection sampling step, discarding failed trajectories and only keeping the best-performing ones regarding discounted returns. An SFT step can then be performed on this dataset to improve the method's performance further. Results for the trained model after a single SFT step can be found in Table 4 under the column "1-step SFT". Importantly, the model created after one step of this process</p>
<p>is still usable under the framework. Indeed, we can now feed it back to the aforementioned prompting methods and create a higher-quality dataset to fine-tune further. We repeat this process two more times on top of the OpenChat-3.5 model, each for four epochs over the training data, and we report the results on a held-out test set of ALFWorld episodes in Table 4.</p>
<table>
<thead>
<tr>
<th>Tasks</th>
<th>OpenChat-3.5</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Direct</td>
<td>FS-CoT</td>
<td>1-step SFT</td>
<td>2-step SFT</td>
<td>3-step SFT</td>
</tr>
<tr>
<td>ALFWorld</td>
<td>0.04</td>
<td>0.27</td>
<td>0.45</td>
<td>0.68</td>
<td>$\mathbf{0 . 8 2}$</td>
</tr>
</tbody>
</table>
<p>Table 4: OpenChat-3.5 on ALFWorld with/without fine-tuning on held-out tasks.</p>
<p>Table 4 shows that after a single round of rejection sampling, we can achieve a strong performance in ALFWorld while keeping the model's ability to generate thoughts before actions.</p>
<p>SFT-RLFT: That said, fine-tuning on the full trajectories generated by these intrinsic functions is computationally expensive and quickly reaches a point of diminishing returns. Instead, we propose the use of RL to reach even higher performance in a variety of tasks. To do so, we first perform a fine-tuning step, but on a modified version of intrinsic functions which build the LLM context in a more space-efficient format (i.e. a chat-based format $\left(\boldsymbol{o}<em t="t">{t}, \boldsymbol{a}</em>}, \boldsymbol{o<em t_1="t+1">{t+1}, \boldsymbol{a}</em>, \ldots\right)$ ).
In this experiment, we fine-tune three distinct Llama 2-7B models on ALFWorld, BabyAI-GoToObjv0 and BabyAI-GoToRedBlueBall-v0. To do so, we first collect successful trajectories on training tasks. To perform SFT, we remove the generated thoughts and only fine-tune on the actions. After fine-tuning, we evaluate the checkpoint achieving the best score over 512 trajectories in the training domain and test it on the 50 held-out test episodes.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Tasks</th>
<th style="text-align: center;">OpenChat-3.5</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Llama 2-7B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Direct</td>
<td style="text-align: center;">FS-CoT</td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">SFT+RL</td>
<td style="text-align: center;">RL</td>
</tr>
<tr>
<td style="text-align: left;">ALFWorld</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">$\mathbf{0 . 8 8}$</td>
<td style="text-align: center;">0.04</td>
</tr>
<tr>
<td style="text-align: left;">BabyAI-GoToObj-v0</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">$\mathbf{0 . 9 1}$</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: left;">BabyAI-GoToRedBlueBall-v0</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">$\mathbf{0 . 7 7}$</td>
<td style="text-align: center;">0.69</td>
</tr>
</tbody>
</table>
<p>Table 5: Benchmark of OpenChat and Llama 2-7B with/without fine-tuning on held-out tasks.</p>
<p>Table 5 shows that fine-tuning first with SFT on successful demonstrations, followed by RL, leads to the largest improvement in success rates. For complex domains like ALFWorld, it also shows that the SFT step and the intrinsic function (FS-CoT) for trajectory generation are crucial. This shows the importance of our Pangu-Agent framework where we can benefit from intrinsic functions and fine-tuning.</p>
<h1>4.2.2 Cross-Domain Model</h1>
<p>In this experiment, we collected additional trajectories on more diverse BabyAI tasks with the same methodology and trained a single cross-domain model on ALFWorld and BabyAI. Note that no successful trajectories could be generated in BabyAI-UnlockPickup-v0 in the allotted time. We also remove the thoughts from the successful trajectories to speed up the training.</p>
<p>Table 6 presents the achieved returns of the SFT and RLFT using the Llama 2-7B LLM on ALFWorld and BabyAI tasks. This experiment establishes that it is possible to successfully SFT and RLFT Llama 2 on multitask training. The performance in ALFWorld is very close to that achieved by fine-tuning exclusively on ALFWorld. However, for BabyAI tasks, multi-task training has a clear benefit, achieving even better performance than the specialised model of Table 5. It is also capable of generalising to BabyAI tasks that are unseen in training.</p>
<h2>5 Conclusion \&amp; Future Work</h2>
<p>This work presents the Pangu-Agent framework to facilitate future research towards developing generalist AI agents. Pangu-Agent builds upon LLMs to address reasoning and decision problems,</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Training weight</th>
<th style="text-align: center;">Original</th>
<th style="text-align: center;">SFT</th>
<th style="text-align: center;">SFT+RL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ALFWorld</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.82</td>
</tr>
<tr>
<td style="text-align: left;">BabyAI-GoToObj-v0</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.93</td>
</tr>
<tr>
<td style="text-align: left;">BabyAI-GoToRedBlueBall-v0</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: left;">BabyAI-GoToRedBallGrey-v0</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.81</td>
</tr>
<tr>
<td style="text-align: left;">BabyAI-GoToLocalS8N7-v0</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: left;">BabyAI-PickupLoc-v0</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.63</td>
</tr>
<tr>
<td style="text-align: left;">BabyAI-PickupDist-v0</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.78</td>
</tr>
<tr>
<td style="text-align: left;">BabyAI-GoToDoor-v0</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.98</td>
</tr>
<tr>
<td style="text-align: left;">BabyAI-OpenRedDoor-v0</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.88</td>
</tr>
<tr>
<td style="text-align: left;">BabyAI-PutNextS7N4Carrying-v0</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.66</td>
</tr>
<tr>
<td style="text-align: left;">BabyAI-UnlockPickup-v0</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">BabyAI-GoToObjDoor-v0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.81</td>
</tr>
<tr>
<td style="text-align: left;">BabyAI-GoToRedBallNoDists-v0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: left;">BabyAI-GoToOpen-v0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.13</td>
</tr>
<tr>
<td style="text-align: left;">BabyAI-GoToObjMazeS4R2-v0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.24</td>
</tr>
<tr>
<td style="text-align: left;">BabyAI-Open-v0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.51</td>
</tr>
<tr>
<td style="text-align: left;">BabyAI-Unlock-v0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.17</td>
</tr>
<tr>
<td style="text-align: left;">BabyAI-PutNextLocal-v0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: left;">BabyAI-ActionObjDoor-v0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.78</td>
</tr>
</tbody>
</table>
<p>Table 6: Benchmark of Llama 2-7B with/without cross-domain fine-tuning on held-out tasks.
which allows utilising human priors. First, we propose a general RL-based objective to optimise the agent's intrinsic and extrinsic functions. We implemented several intrinsic functions and showcased the modular functionality of our agent and how it can support recent advances in LLM research. We extensively evaluated Pangu-Agent in several single-agent and multi-agent tasks for different prompt engineering methods across various LLMs, and offered insights about their relative advantages and disadvantages.</p>
<p>Moreover, we discuss how this framework can fine-tune LLMs through an SFT and RL pipeline. Our results indicate that fine-tuning can improve the agent's performance up to threefold in specific multi-step decision-making domains such as ALFWorld and Baby-AI. We also show how LLMs have the capacity for cross-domain learning by fine-tuning a single LLM and evaluating it simultaneously in multiple domains. Finally, we conclude this work by presenting the existing limitations of the current version of Pangu-Agent as well as our vision towards the development of a generalist agent.</p>
<p>Full differentiability: This work focused on independently optimising the intrinsic and extrinsic functions. Looking forward, we envision that Pangu-Agent development will gradually shift towards structured and end-to-end fine-tuning of the framework. This will enable passing gradients between various intrinsic and extrinsic functions, allowing for a more adaptable system.</p>
<p>Real-world applications: At present, the performance of Pangu-Agent is evaluated on a small number of single-agent and multi-agent tasks. We plan to incorporate more diverse and complex evaluation tasks in future revisions to make Pangu-Agent effective in real-world applications and address the simulation-to-reality gap. The long-term goal of this work is the development of generalist agents that can assist humans in their everyday tasks and function autonomously in many real-world challenges.</p>
<p>Memory retrieval: Another avenue of future research in the Pangu-Agent framework lies in the memory retrieval methods. The current version of Pangu-Agent supports a long-term memory that stores any information available to each agent, such as its observations, thoughts and actions. In the future, we aim to incorporate more sophisticated memory retrieval methods, such as embedding similarity from vector databases to allow the agent to incorporate relevant memories in its context window, enabling it to solve the task.</p>
<p>Planning: Currently, for planning, we only focus on reasoning tasks. We intend to integrate and test tree search algorithms in agent-based tasks within interactive environments. Additionally, we are</p>
<p>committed to developing and implementing strategies for efficient long-term planning. We aim to enhance the planning capabilities of Pangu-Agent, thereby equipping it to tackle real-world challenges and adapt to dynamic environments.</p>
<p>Tool use: Lastly, a significant part of our future Pangu-Agent roadmap is to facilitate integration with external tools. Pangu-Agent includes a code interpreter for executing simple Python scripts in its current configuration. However, future versions of Pangu-Agent will support compatibility with various external tools like web search engines, calculators (for instance Wolfram Alpha), and maps. This expansion will enable a broader deployment of Pangu-Agent across various applications and enable generalisation to tasks beyond its initial learning distribution.</p>
<h1>References</h1>
<p>[1] J. S. Albus, "Outline for a theory of intelligence," IEEE transactions on systems, man, and cybernetics, vol. 21, no. 3, pp. 473-509, 1991.
[2] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel et al., "A general reinforcement learning algorithm that masters chess, shogi, and go through self-play," Science, vol. 362, no. 6419, pp. 1140-1144, 2018.
[3] D. J. Mankowitz, A. Michi, A. Zhernov, M. Gelmi, M. Selvi, C. Paduraru, E. Leurent, S. Iqbal, J.-B. Lespiau, A. Ahern et al., "Faster sorting algorithms discovered using deep reinforcement learning," Nature, vol. 618, no. 7964, pp. 257-263, 2023.
[4] A. Fawzi, M. Balog, A. Huang, T. Hubert, B. Romera-Paredes, M. Barekatain, A. Novikov, F. J. R Ruiz, J. Schrittwieser, G. Swirszcz et al., "Discovering faster matrix multiplication algorithms with reinforcement learning," Nature, vol. 610, no. 7930, pp. 47-53, 2022.
[5] E. Kaufmann, L. Bauersfeld, A. Loquercio, M. Mller, V. Koltun, and D. Scaramuzza, "Champion-level drone racing using deep reinforcement learning," Nature, vol. 620, no. 7976, pp. 982-987, 2023.
[6] J. Degrave, F. Felici, J. Buchli, M. Neunert, B. Tracey, F. Carpanese, T. Ewalds, R. Hafner, A. Abdolmaleki, D. de Las Casas et al., "Magnetic control of tokamak plasmas through deep reinforcement learning," Nature, vol. 602, no. 7897, pp. 414-419, 2022.
[7] K. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, "Efficient off-policy meta-reinforcement learning via probabilistic context variables," in International Conference on Machine Learning, 2019. [Online]. Available: https://api.semanticscholar.org/CorpusID:84187276
[8] A. Gupta, R. Mendonca, Y. Liu, P. Abbeel, and S. Levine, "Meta-reinforcement learning of structured exploration strategies," in Neural Information Processing Systems, 2018. [Online]. Available: https://api.semanticscholar.org/CorpusID:3418899
[9] J. X. Wang, Z. Kurth-Nelson, D. Kumaran, D. Tirumala, H. Soyer, J. Z. Leibo, D. Hassabis, and M. M. Botvinick, "Prefrontal cortex as a meta-reinforcement learning system," Nature Neuroscience, vol. 21, pp. 860 - 868, 2018. [Online]. Available: https://api.semanticscholar.org/CorpusID:44137923
[10] I. Clavera, A. Nagabandi, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, "Learning to adapt in dynamic, real-world environments through meta-reinforcement learning," in International Conference on Learning Representations, 2019. [Online]. Available: https://openreview.net/forum?id=HyztsoC5Y7
[11] A. G. Barto, "Intrinsic motivation and reinforcement learning," in Intrinsically Motivated Learning in Natural and Artificial Systems, 2013. [Online]. Available: https://api.semanticscholar.org/CorpusID: 2326055
[12] H. Tang, R. Houthooft, D. Foote, A. Stooke, O. Xi Chen, Y. Duan, J. Schulman, F. DeTurck, and P. Abbeel, "#exploration: A study of count-based exploration for deep reinforcement learning," in Advances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., vol. 30. Curran Associates, Inc., 2017. [Online]. Available: https: //proceedings.neurips.cc/paper_files/paper/2017/file/3a20f62a0af1aa152670bab3c602feed-Paper.pdf
[13] Y. Burda, H. Edwards, A. Storkey, and O. Klimov, "Exploration by random network distillation," in 7th International Conference on Learning Representations (ICLR 2019), May 2019, pp. 1-17, seventh International Conference on Learning Representations, ICLR 2019 ; Conference date: 06-05-2019 Through 09-05-2019. [Online]. Available: https://iclr.cc/
[14] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and K. Kavukcuoglu, "Reinforcement learning with unsupervised auxiliary tasks," in International Conference on Learning Representations, 2017. [Online]. Available: https://openreview.net/forum?id=SJ6yPD5xg
[15] E. Shelhamer, P. Mahmoudieh, M. Argus, and T. Darrell, "Loss is its own reward: Selfsupervision for reinforcement learning," ArXiv, vol. abs/1612.07307, 2016. [Online]. Available: https://api.semanticscholar.org/CorpusID:16561904</p>
<p>[16] S. Li, R. Wang, M. Tang, and C. Zhang, "Hierarchical reinforcement learning with advantage-based auxiliary rewards," Advances in Neural Information Processing Systems, vol. 32, 2019.
[17] A. Ng, D. Harada, and S. J. Russell, "Policy invariance under reward transformations: Theory and application to reward shaping," in International Conference on Machine Learning, 1999. [Online]. Available: https://api.semanticscholar.org/CorpusID:5730166
[18] S. Devlin and D. Kudenko, "Dynamic potential-based reward shaping," in Adaptive Agents and Multi-Agent Systems, 2012. [Online]. Available: https://api.semanticscholar.org/CorpusID:17251664
[19] P. Goyal, S. Niekum, and R. J. Mooney, "Using natural language for reward shaping in reinforcement learning," in International Joint Conference on Artificial Intelligence, 2019. [Online]. Available: https://api.semanticscholar.org/CorpusID:70350059
[20] E. Brochu, V. M. Cora, and N. de Freitas, "A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning," ArXiv, vol. abs/1012.2599, 2010. [Online]. Available: https://api.semanticscholar.org/CorpusID:1640103
[21] P. Poupart, N. Vlassis, J. Hoey, and K. Regan, "An analytic solution to discrete bayesian reinforcement learning," in Proceedings of the 23rd international conference on Machine learning, 2006, pp. 697-704.
[22] M. Ghavamzadeh, S. Mannor, J. Pineau, A. Tamar et al., "Bayesian reinforcement learning: A survey," Foundations and Trends in Machine Learning, vol. 8, no. 5-6, pp. 359-483, 2015.
[23] T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenenbaum, "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation," in Advances in Neural Information Processing Systems, D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, Eds., vol. 29. Curran Associates, Inc., 2016. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2016/file/ f442d33fa06832082290ad8544a8da27-Paper.pdf
[24] A. G. Barto and S. Mahadevan, "Recent advances in hierarchical reinforcement learning," Discrete event dynamic systems, vol. 13, no. 1-2, pp. 41-77, 2003.
[25] O. Nachum, S. S. Gu, H. Lee, and S. Levine, "Data-efficient hierarchical reinforcement learning," Advances in neural information processing systems, vol. 31, 2018.
[26] G. Andersen, P. Vrancx, and H. Bou-Ammar, "Learning high-level representations from demonstrations," CoRR, vol. abs/1802.06604, 2018. [Online]. Available: http://arxiv.org/abs/1802.06604
[27] Q. Wu, G. Bansal, J. Zhang, Y. Wu, B. Li, E. Zhu, L. Jiang, X. Zhang, S. Zhang, J. Liu, A. H. Awadallah, R. W. White, D. Burger, and C. Wang, "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation," arXiv e-prints, p. arXiv:2308.08155, Aug. 2023.
[28] S. Gravitas, "Auto-gpt: An autonomous gpt-4 experiment," 2023. [Online]. Available: https: //github.com/Significant-Gravitas/AutoGPT
[29] W. Chen, Y. Su, J. Zuo, C. Yang, C. Yuan, C.-M. Chan, H. Yu, Y. Lu, Y.-H. Hung, C. Qian, Y. Qin, X. Cong, R. Xie, Z. Liu, M. Sun, and J. Zhou, "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors," 2023.
[30] T. Schick, J. Dwivedi-Yu, R. Dess, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom, "Toolformer: Language models can teach themselves to use tools," Advances in Neural Information Processing Systems, vol. 36, 2023.
[31] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface," Advances in Neural Information Processing Systems, vol. 36, 2023.
[32] HuggingFace, "Transformers agent," 2023. [Online]. Available: https://huggingface.co/docs/transformers/ transformers_agents
[33] C. Harrison, "Langchain," 2022.
[34] T. Xie, F. Zhou, Z. Cheng, P. Shi, L. Weng, Y. Liu, T. J. Hua, J. Zhao, Q. Liu, C. Liu, L. Z. Liu, Y. Xu, H. Su, D. Shin, C. Xiong, and T. Yu, "Openagents: An open platform for language agents in the wild," 2023.
[35] X. Team, "Xagent: An autonomous agent for complex task solving," 2023.
[36] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, C. Ran, L. Xiao, and C. Wu, "Metagpt: Meta programming for multi-agent collaborative framework," 2023.
[37] G. Li, H. A. A. K. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, "CAMEL: Communicative agents for "mind" exploration of large language model society," in Thirty-seventh Conference on Neural Information Processing Systems, 2023. [Online]. Available: https://openreview.net/forum?id=3lyL2XWDkG
[38] W. Zhou, Y. E. Jiang, L. Li, J. Wu, T. Wang, S. Qiu, J. Zhang, J. Chen, R. Wu, S. Wang, S. Zhu, J. Chen, W. Zhang, N. Zhang, H. Chen, P. Cui, and M. Sachan, "Agents: An open-source framework for autonomous language agents," 2023.</p>
<p>[39] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou, "Self-consistency improves chain of thought reasoning in language models," arXiv preprint arXiv:2203.11171, 2022.
[40] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, "React: Synergizing reasoning and acting in language models," arXiv preprint arXiv:2210.03629, 2022.
[41] B. Y. Lin, Y. Fu, K. Yang, P. Ammanabrolu, F. Brahman, S. Huang, C. Bhagavatula, Y. Choi, and X. Ren, "Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks," 2023.
[42] D. Zhou, N. Schrli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet, Q. V. Le, and E. H. Chi, "Least-to-most prompting enables complex reasoning in large language models," in The Eleventh International Conference on Learning Representations, 2023. [Online]. Available: https://openreview.net/forum?id=WZH7099tgIM
[43] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., "Chain-of-thought prompting elicits reasoning in large language models," Advances in Neural Information Processing Systems, vol. 35, pp. 24 824-24 837, 2022.
[44] M. Shridhar, X. Yuan, M. Ct, Y. Bisk, A. Trischler, and M. J. Hausknecht, "Alfworld: Aligning text and embodied environments for interactive learning," CoRR, vol. abs/2010.03768, 2020. [Online]. Available: https://arxiv.org/abs/2010.03768
[45] M. Chevalier-Boisvert, D. Bahdanau, S. Lahlou, L. Willems, C. Saharia, T. H. Nguyen, and Y. Bengio, "Babyai: A platform to study the sample efficiency of grounded language learning," 2019.
[46] R. Tutunov, A. Grosnit, J. Ziomek, J. Wang, and H. Bou-Ammar, "Why can large language models generate correct chain-of-thoughts?" 2023.
[47] C.-C. Lin, A. Jaech, X. Li, M. R. Gormley, and J. Eisner, "Limitations of autoregressive models and their alternatives," arXiv preprint arXiv:2010.11939, 2020.
[48] C. Wei, Y. Chen, and T. Ma, "Statistically meaningful approximation: a case study on approximating turing machines with transformers," Advances in Neural Information Processing Systems, vol. 35, pp. $12071-12083,2022$.
[49] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan, "Tree of Thoughts: Deliberate Problem Solving with Large Language Models," arXiv e-prints, p. arXiv:2305.10601, May 2023.
[50] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu, "Reasoning with language model is planning with world model," arXiv preprint arXiv:2305.14992, 2023.
[51] X. Feng, Z. Wan, M. Wen, Y. Wen, W. Zhang, and J. Wang, "Alphazero-like tree-search can guide large language model decoding and training," in NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023. [Online]. Available: https://openreview.net/forum?id=PJfc4x2jXY
[52] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman, "Training verifiers to solve math word problems," 2021.
[53] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. Cohen, R. Salakhutdinov, and C. D. Manning, "HotpotQA: A dataset for diverse, explainable multi-hop question answering," in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Brussels, Belgium: Association for Computational Linguistics, Oct.-Nov. 2018, pp. 2369-2380. [Online]. Available: https://aclanthology.org/D18-1259
[54] S. Yao, H. Chen, J. Yang, and K. Narasimhan, "Webshop: Towards scalable real-world web interaction with grounded language agents," in ArXiv, preprint.
[55] E. A. Hansen, D. S. Bernstein, and S. Zilberstein, "Dynamic programming for partially observable stochastic games," in AAAI, vol. 4, 2004, pp. 709-715.
[56] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, "LoRA: Low-rank adaptation of large language models," in International Conference on Learning Representations, 2022. [Online]. Available: https://openreview.net/forum?id=nZeVKeeFYf9
[57] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal policy optimization algorithms," CoRR, vol. abs/1707.06347, 2017. [Online]. Available: http://arxiv.org/abs/1707.06347
[58] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, "Efficient memory management for large language model serving with pagedattention," in Proceedings of the 29th Symposium on Operating Systems Principles, 2023, pp. 611-626.
[59] B. Lefaudeux, F. Massa, D. Liskovich, W. Xiong, V. Caggiano, S. Naren, M. Xu, J. Hu, M. Tintore, S. Zhang, P. Labatut, and D. Haziza, "xformers: A modular and hackable transformer modelling library," https://github.com/facebookresearch/xformers, 2022.
[60] OpenAI, "Gpt-4 technical report," 2023.</p>
<p>[61] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom, "Llama 2: Open foundation and fine-tuned chat models," CoRR, vol. abs/2307.09288, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2307.09288
[62] G. Wang, S. Cheng, X. Zhan, X. Li, S. Song, and Y. Liu, "Openchat: Advancing open-source language models with mixed-quality data," CoRR, vol. abs/2309.11235, 2023. [Online]. Available: https://doi.org/10.48550/arXiv. 2309.11235
[63] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing, "Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality," March 2023. [Online]. Available: https://lmsys.org/blog/2023-03-30-vicuna/
[64] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de Las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed, "Mistral 7b," CoRR, vol. abs/2310.06825, 2023. [Online]. Available: https://doi.org/10.48550/arXiv. 2310.06825
[65] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, "Large language models are zero-shot reasoners," in Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., vol. 35. Curran Associates, Inc., 2022, pp. 22 199-22 213. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2022/file/ 8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf
[66] N. Shinn, B. Labash, and A. Gopinath, "Reflexion: an autonomous agent with dynamic memory and self-reflection," arXiv preprint arXiv:2303.11366, 2023.
[67] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui, "A survey for in-context learning," arXiv preprint arXiv:2301.00234, 2022.
[68] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877-1901, 2020.
[69] M.-A. Ct, kos Kdr, X. Yuan, B. Kybartas, T. Barnes, E. Fine, J. Moore, R. Y. Tao, M. Hausknecht, L. E. Asri, M. Adada, W. Tay, and A. Trischler, "Textworld: A learning environment for text-based games," 2019.
[70] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox, "Alfred: A benchmark for interpreting grounded instructions for everyday tasks," in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Los Alamitos, CA, USA: IEEE Computer Society, jun 2020, pp. 10737-10746. [Online]. Available: https://doi.ieeecomputersociety.org/10.1109/CVPR42600.2020.01075
[71] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba, "Evaluating large language models trained on code," 2021.
[72] T. Carta, C. Romac, T. Wolf, S. Lamprier, O. Sigaud, and P.-Y. Oudeyer, "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning," vol. 202, no. 3676-3713. PMLR, 2023. [Online]. Available: https://hal.science/hal-03970122
[73] P. Brookins and J. M. DeBacker, "Playing games with gpt: What can we learn about a large language model from canonical strategic games?" Available at SSRN 4493398, 2023.
[74] S. V. Albrecht, F. Christianos, and L. Schfer, Multi-Agent Reinforcement Learning: Foundations and Modern Approaches. MIT Press, 2024. [Online]. Available: https://www.marl-book.com
[75] E. Akata, L. Schulz, J. Coda-Forno, S. J. Oh, M. Bethge, and E. Schulz, "Playing repeated games with large language models," CoRR, vol. abs/2305.16867, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2305.16867
[76] J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou, "Large language models cannot self-correct reasoning yet," arXiv preprint arXiv:2310.01798, 2023.</p>
<p>[77] K. Stechly, M. Marquez, and S. Kambhampati, "Gpt-4 doesnt know its wrong: An analysis of iterative prompting for reasoning problems," arXiv preprint arXiv:2310.12397, 2023.
[78] J. Long, "Large Language Model Guided Tree-of-Thought," arXiv e-prints, p. arXiv:2305.08291, May 2023.
[79] Y. Yao, Z. Li, and H. Zhao, "Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models," arXiv e-prints, p. arXiv:2305.16582, May 2023.
[80] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, L. Gianinazzi, J. Gajda, T. Lehmann, M. Podstawski, H. Niewiadomski, P. Nyczyk, and T. Hoefler, "Graph of Thoughts: Solving Elaborate Problems with Large Language Models," arXiv e-prints, p. arXiv:2308.09687, Aug. 2023.
[81] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang et al., "Self-refine: Iterative refinement with self-feedback," arXiv preprint arXiv:2303.17651, 2023.
[82] B. Xu, Z. Peng, B. Lei, S. Mukherjee, Y. Liu, and D. Xu, "Rewoo: Decoupling reasoning from observations for efficient augmented language models," arXiv preprint arXiv:2305.18323, 2023.
[83] H. Liu, C. Sferrazza, and P. Abbeel, "Languages are rewards: Hindsight finetuning using human feedback," arXiv preprint arXiv:2302.02676, 2023.
[84] K. Lucas, "open-interpreter," https://github.com/KillianLucas/open-interpreter, 2023.
[85] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz et al., "Huggingfaces transformers: State-of-the-art natural language processing," arXiv preprint arXiv:1910.03771, 2019.
[86] B. Xu, X. Liu, H. Shen, Z. Han, Y. Li, M. Yue, Z. Peng, Y. Liu, Z. Yao, and D. Xu, "Gentopia: A collaborative platform for tool-augmented llms," 2023.
[87] Reworkd, "Agentgpt," https://github.com/reworkd/AgentGPT, 2023.
[88] T. Liang, Z. He, W. Jiao, X. Wang, Y. Wang, R. Wang, Y. Yang, Z. Tu, and S. Shi, "Encouraging divergent thinking in large language models through multi-agent debate."
[89] Y. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch, "Improving factuality and reasoning in language models through multiagent debate," 2023.
[90] Y. Nakajima, "babyagi," https://github.com/yoheinakajima/babyagi, 2023.
[91] C. Qian, X. Cong, W. Liu, C. Yang, W. Chen, Y. Su, Y. Dang, J. Li, J. Xu, D. Li, Z. Liu, and M. Sun, "Communicative agents for software development," 2023.
[92] Y. Song, W. Xiong, D. Zhu, W. Wu, H. Qian, M. Song, H. Huang, C. Li, K. Wang, R. Yao, Y. Tian, and S. Li, "Restgpt: Connecting large language models with real-world restful apis," 2023.
[93] C. Zhang, K. Yang, S. Hu, Z. Wang, G. Li, Y. Sun, C. Zhang, Z. Zhang, A. Liu, S.-C. Zhu, X. Chang, J. Zhang, F. Yin, Y. Liang, and Y. Yang, "Proagent: Building proactive cooperative ai with large language models," 2023.
[94] Z. Liu, W. Yao, J. Zhang, L. Xue, S. Heinecke, R. Murthy, Y. Feng, Z. Chen, J. C. Niebles, D. Arpit et al., "Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents," arXiv preprint arXiv:2308.05960, 2023.
[95] G. Chen, S. Dong, Y. Shu, G. Zhang, S. Jaward, K. Brje, J. Fu, and Y. Shi, "Autoagents: The automatic agents generation framework," arXiv preprint, 2023.
[96] Z. Liu, Y. Zhang, P. Li, Y. Liu, and D. Yang, "Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization," 2023.</p>
<h1>Acknowledgements</h1>
<p>The authors would like to express their sincere gratitude to all who contributed to the realisation of this study. A partnership between the team members sustained the project. Jun Wang conceptualised the core research thesis and guided the investigation. Jianye Hao guided the project from the technical and application aspects. Kun Shao assumed the role of project manager, leading the project and making detailed plans for each division. Haitham Bou-Ammar technically supervised the fine-tuning pipeline and aided in the paper writing, offering insightful recommendations that enhanced the study. Filippos Christianos and Georgios Papoudakis were instrumental in architecting the proposed agent framework. Their contributions were critical in authoring the principal manuscript, developing the framework, and overseeing the experiments. Matthieu Zimmer and James Doran were dedicated to fine-tuning methodologies and the RL pipeline. Thomas Coste, Zhihao Wu, Jingxuan Chen, and Khyati Khandelwal assisted with the development of the framework, the implementation of various intrinsic methods, and conducted experiments utilising open-source LLMs. Xidong Feng's efforts focused on implementing tree-search-based methods and performing experiments related to the GPT series. Jiacheng Liu executed a comprehensive literature review on frameworks for AI agents. Zheng Xiong and Yicheng Luo provided support in evaluating the framework. The authors thank Christopher Mower, Yunfeng Shao, Qiang Ye, Meng Fang, Alexandre Maraval, Antoine Grosnit, Jonas Gonzalez, Rasul Tutunov, and Nan Zhou for their valuable discussions.</p>
<h2>A Decision-Making and Reasoning Methods</h2>
<p>In Section 4, we evaluated a variety of both first-order and composite methods across different LLMs. Here, we provide further details on these methods and how they function, with brief summaries in Table 7. We also explain additional features which can be used within the framework, such as agent communication and tool use. A high-level diagram of the Pangu-Agent framework is presented in Figure 4.</p>
<p>Several of the methods below leverage in-context learning. In-context learning refers to the collection of methods that utilise analogy-based examples [67] to increase the reasoning abilities of LLMs. Practically, a number of similar tasks along with their solutions are included in the prompt of the LLM along with other instructions. Note that in-context learning is not a training paradigm as the pre-trained parameters of the model are not altered. However, by conditioning the text generation on the in-context examples, the internal activations of the LLM are shaped towards the task at hand, promoting the emergence of reasoning capabilities. This is a significant advantage compared to fine-tuning, which allows to specialise an LLM to a task without the need to fine-tune its parameters, which can be difficult with regards both to computational and data availability considerations.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Direct</td>
<td style="text-align: left;">Directly prompt the LLM for answers (zero-shot).</td>
</tr>
<tr>
<td style="text-align: left;">ZS-CoT</td>
<td style="text-align: left;">Zero-Shot, Chain of Thought: Asks the model to think step by step.</td>
</tr>
<tr>
<td style="text-align: left;">FS</td>
<td style="text-align: left;">Few-Shot: In-context examples in which the answer is given directly.</td>
</tr>
<tr>
<td style="text-align: left;">FS-CoT</td>
<td style="text-align: left;">Few-Shot, Chain of Thought: In-context examples with step-by-step thoughts.</td>
</tr>
<tr>
<td style="text-align: left;">FS-CoT-SC</td>
<td style="text-align: left;">Few-Shot, Chain of Thought, Self-Consistency: FS-CoT and checks for consistency in answers.</td>
</tr>
<tr>
<td style="text-align: left;">FS-CoT-React</td>
<td style="text-align: left;">Few-Shot, Chain of Thought, React: Think-then-act React mechanism with FS-CoT examples.</td>
</tr>
<tr>
<td style="text-align: left;">FS-CoT-Reflect</td>
<td style="text-align: left;">Few-Shot, Chain of Thought, Reflect: FS-CoT with an additional reflection step.</td>
</tr>
<tr>
<td style="text-align: left;">FS-CoT-SwiftSage</td>
<td style="text-align: left;">Few-Shot, Chain of Thought, SwiftSage: SwiftSage switching and action buffer with FS-CoT.</td>
</tr>
<tr>
<td style="text-align: left;">FS-Least-to-Most</td>
<td style="text-align: left;">Few-Shot, Least-to-Most: Task decomposition and subgoal answering with FS examples</td>
</tr>
</tbody>
</table>
<p>Table 7: Decision-making and reasoning method summary</p>
<h2>A. 1 First-order Methods</h2>
<p>The initial sequence of tokens used as input to the LLM is called the prompt. LLMs, as autoregressive models, condition the generation of each token on the previous tokens. Therefore, the prompt provided as input to the LLM significantly affects its subsequently generated text. Below we present the main prompting techniques that were used in the evaluation of Pangu-Agent.
Direct Prompting (Direct): Direct, or zero-shot, prompting is the simplest way to prompt an LLM. Usually, only a task description and the question at hand are provided. Under direct prompting, no</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The figure above presents a pictorial depiction of the main components of our proposed agent. On the far left, we can set up a multi-agent environment where each agent can interact with the environment and communicate with other agents. Each agent can be fine-tuned via reinforcement or supervised learning. Each of those agents can support any nesting of intrinsic functions $\boldsymbol{\mu}(\cdot)$, such as tool usage, thinking processes, reflecting, planning and others. Those operate on the memory component before producing an action to the outer world via extrinsic processes. Our agent also allows for different prompting strategies and open-source language models, further enabling rigorous experimentation protocols, see Section 4.
in-context examples are provided to the LLM. It is a simplified variant of the few-shot prompting technique described below, where it is assumed that zero in-context examples are provided.</p>
<p>Few-Shot Prompting (FS): In FS, several question-answer pairs are added to the prompt. Only the task/question and the answer are added to the prompt and not the intermediate steps or thoughts towards the answer. FS has been shown to significantly improve the performance of LLMs in various downstream tasks without requiring any fine-tuning of their parameters [68].</p>
<p>Few-Shot Chain-of-Thought (FS-CoT) Prompting [43]: CoT refers to step-by-step reasoning through thought generation, eventually leading to the final answer. Several in-context examples are added to the prompt to enable the LLM to develop such reasoning ability. In contrast to FS alone, the question-answer pairs added to the prompt contain intermediate reasoning steps. The prompt is usually augmented with the phrase "think step-by-step". This enables the LLM to follow a similar reasoning path when the actual question is provided.</p>
<p>Zero-Shot Chain-of-Thought Prompting (ZS-CoT)[65]: This technique is used to tap into a model's reasoning capabilities by appending the question prompt with "Let's think step-by-step". No other context is provided to the agent, and usually, this technique is indifferent towards the task at hand while also invoking multi-hop reasoning across eclectic tasks with a single template.</p>
<h1>A. 2 Composite Methods</h1>
<p>The first-order methods in the previous part can be combined with more advanced techniques, covered below. In particular, we define: 'FS-' which refers to any variations which include context examples before prompting the model for an answer (see Few-Shot Prompting) and 'CoT-' which elicits thoughts from the model either through examples when combined with FS (FS-CoT) or simply by asking it to think step by step (ZS-CoT). We use the term composite methods to refer to the more advanced methods, which can use first-order prompting techniques and consist of multi-step prompting.</p>
<p>Self-Consistency [39]: Self-Consistency (SC) works by repeatedly asking the LLM the same question and expecting the LLM to generate various reasoning paths. Then, the different answers are checked, and the most consistent answer is chosen as the final one. The main strategy for deciding the most consistent answer is majority voting, which chooses the most popular answer.</p>
<p>React [40]: React is a two-step prompting mechanism that helps to decompose LLM decision-making into reasoning traces and concrete actions. The LLM is first prompted to return a reasoning trace, or thought, relevant to solving the task in the current situation. When the LLM is prompted a second time, the reasoning trace is appended to the prompt, and the LLM is asked to return a task-specific action. The reasoning trace provides useful information which can help with commonsense reasoning, progress tracking, planning, and adaptability, to name a few.
Reflect: In Reflect, the agent is prompted to reflect on its past actions to provide a plan to identify its mistakes and perform better in the upcoming time steps. Hence, the agent is provided with linguistic feedback from the LLM itself to improve its actions. This approach is adapted from [66]. Our attempt deviates from the work in [66] as we do not maintain a memory across episodes but only provide the agent with the memory from previous time steps in an episode and the most recent reflection. We also reflect at every step instead of reflecting only after three incorrect attempts. Finally, we extend this method to tasks not in the original paper and thus adapt the reflection prompt for these tasks. By default, reflect does not work for single-step tasks with a simple question-answer format. Hence, for such tasks, we introduce a zero-step version of Reflect, which prompts the LLM for a temporary answer and then asks it to reflect on its initial answer before giving a final response.</p>
<p>SwiftSage [41]: In this method, two modules are used: Swift, backed by a smaller language model for quick and intuitive actions, and Sage, backed by a more powerful model for deliberated actions. The original implementation of this method uses various conditions when switching from Swift to Sage. Deviating from the initial work, our framework uses Swift until five consecutive time steps receive a reward of 0 . Sage is prompted to create a plan and provide an action buffer when this occurs. The action buffer is a short list of consecutive actions it believes are the most promising. Actions are selected in order from the action buffer until it is exhausted, at which point we revert to the Swift module. If an action buffer step is invalid, it is skipped, and the next one is considered. This method takes place over multiple time steps and is only relevant for multi-step environments.
Least-to-Most [42]: Similar to ReAct, Least-to-Most is another two-step prompting approach, which asks the LLM first to generate reasoning traces and then produce actions based on the traces. The difference is that, instead of thoughts, the LLM is first prompted to decompose the question into several sub-questions. In the second prompt, the LLM is asked to answer all sub-questions and give a final answer to the original question. Due to context length considerations and our choice of implementation, we deviate from [42] in that all sub-questions are answered in one prompt, and as such, this method only applies to single-step tasks.</p>
<h1>A. 3 Additional functions</h1>
<p>Communication: An essential feature of any AI agent is the ability to communicate with other agents, both human and artificial. This enables participation in multi-agent tasks involving several co-existing agents. LLMs appear to be an ideal tool to enable communication between agents. They are grounded in human language, which allows for communication with humans and artificial agents using a standard and universal communication protocol. Pangu-Agent supports pairwise communication between agents, allowing them to participate in multi-agent tasks.</p>
<p>Tool Use: Augmenting AI agents with the ability to use external tools can further improve their capacity in many tasks that are hard to solve with LLMs alone, such as mathematical computation and answering questions about current events. Pangu-Agent supports tool use through prompting with zero-shot tool descriptions or few-shot tool use examples, which help the agent understand when a tool should be used and how to call it in the correct format. The tool output can be either integrated into the agent's current thought or stored in the agent's memory for future use. An important example of tool use is a code interpreter [34, 35, 60], enabling an AI agent to write, run and debug code. Pangu-Agent can automatically improve code writing to solve various complicated tasks, such as automated data science by integrating a Python interpreter as a tool and iteratively interacting with it.</p>
<h1>B Available Tasks</h1>
<p>This section describes the tasks included in the codebase and used for evaluation. We categorise these tasks into two classes based on the number of participating agents: single-agent and multi-agent.</p>
<h2>B. 1 Single-Agent Tasks</h2>
<p>GSM8K [52]: This dataset consists of 8.5 k grade-school mathematical problems, curated to have high-quality and diverse language-based problems. It includes natural language solutions instead of pure mathematical equations for evaluation and prompts the agent to evaluate its reasoning and problem-solving ability. The agent receives a score of 1 for correctly answered questions, and 0 otherwise. Context examples for FS and CoT are created using the training set of GSM8K, while the evaluation is performed in the test set, which consists of 1319 questions.
HotpotQA [53]: This Wikipedia-based dataset has 113k question-answer pairs that challenge agents to parse through content and reason over several steps and supporting documents. It is independent of training data or pre-existing knowledge of the agents, uses additional documents for external information, and allows the agent to perform multiple steps for reasoning and comprehension. For instance, it provides a paragraph from Wikipedia about a particular band and asks the model a question about that band, the answer to which can be inferred by looking up information in the given paragraph. This environment tests the agent's language comprehension and reasoning ability. Correct answers are rewarded with a score of 1 , while wrong answers get 0 reward.</p>
<p>ALFWorld [44]: ALFWorld has been developed to learn text-based policies from TextWorld, which proposes a learning environment for text-based games [69], and then execute goals from ALFRED, which is a benchmark for the interpretation of grounded instructions for everyday tasks [70]. It is a benchmark for language understanding, reasoning ability and task execution skills. The ALFWorld framework aligns text descriptions and commands with physically embodied robotic simulation by describing the environment, such as "You are in a room with a desk, chair and lamp" and asks the agent to find a notebook on a desk. Through various actions the agent performs, such as "Go to desk", it is then supposed to complete its task to be rewarded. A completed task will yield a reward of 1 , while an uncompleted task will not be rewarded.
WebShop [54]: WebShop is a simulated e-commerce website with 1.18 million real-world products and 12,087 crowd-sourced text instructions. [54] In the WebShop environment, the agent is given text instructions for the product environment and asked to navigate different web pages to find, customise, and purchase an item. The agent can receive observations in either HTML or a text format that strips most of the metadata in HTML. In response, the agent can search the website (when a search box is available) or choose from a pre-defined set of actions. An episode in the WebShop environment ends when the agent selects the buy action. The purchased product is compared with those chosen by a human demonstrator. The returned reward can range from 0 to 1 , depending on the similarity to this ground truth, as discussed in Yao et al. [54].
HumanEval [71]: HumanEval is a dataset created to benchmark the code generation abilities of language models. It was hand-written by humans, which is quite interesting since the LLMs are thus unlikely to have seen this data during training. Each problem in the dataset comes with a function signature, docstring, and unit tests. In the original paper, the authors asked the model to generate $k$ code samples and considered the task completed if any of the samples were correct. In our setup, we evaluate the agent on a more difficult task, checking the validity of a single response. This is done by evaluating the generated function and running the provided unit tests as assert statements. Functions which run successfully and pass the tests will return a reward of 1 , and those that fail will return 0 .
BabyAI-Text [45, 72]: The BabyAI platform consists of several simulated grid-world environments with instruction-following tasks that have gradually increasing levels of difficulty. The environment consists of different objects (balls, boxes, keys, etc.) in various colours, and the agent can pick up, drop or move around objects. The agent is given a 7 x 7 grid view and textual language instructions for the next steps at each time step. Carta et al. [72] extended BabyAI to provide a text-based benchmark. The reward received at the end of the episode is if the agent reached the goal, discounted by the number of time steps taken to reach it, and 0 otherwise.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Contributing equally to this work.
${ }^{\dagger}$ Work done during an internship at Huawei.
${ }^{\ddagger}$ Corresponding authors: {shaokun2, haitham.ammar}@huawei.com, jun.wang@cs.ucl.ac.uk.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
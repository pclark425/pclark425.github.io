<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8404 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8404</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8404</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-49c45d2a2773c537804c38d69cde67e00fbad6fe</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/49c45d2a2773c537804c38d69cde67e00fbad6fe" target="_blank">Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work performs the first study of how number tokenization choices lead to differences in model performance on arithmetic tasks, accompanied by a thorough analysis of error patterns.</p>
                <p><strong>Paper Abstract:</strong> Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases. Historically, LLMs have relied on byte pair encoding, without care to specific input domains. With the increased use of LLMs for reasoning, various number-specific tokenization schemes have been adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers. In this work, we study the effect this choice has on numerical reasoning through the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left tokenization (enforced by comma separating numbers at inference time) leads to largely improved performance. Furthermore, we find that model errors when using standard left-to-right tokenization follow stereotyped error patterns, suggesting that model computations are systematic rather than approximate. We show that the model is able to convert between tokenizations easily, thus allowing chain-of-thought-inspired approaches to recover performance on left-to-right tokenized inputs. We also find the gap between tokenization directions decreases when models are scaled, possibly indicating that larger models are better able to override this tokenization-dependent inductive bias. In summary, our work performs the first study of how number tokenization choices lead to differences in model performance on arithmetic tasks, accompanied by a thorough analysis of error patterns. We hope this work inspires practitioners to more carefully ablate number tokenization-related choices when working towards general models of numerical reasoning.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8404.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8404.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 (gpt-3.5-turbo-0301 checkpoint)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The main model evaluated in the paper for multi-digit addition; accessed via OpenAI Chat Completions API and evaluated in few-shot settings (1,2,4,8 shots) on 7–9 digit addition problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer (OpenAI GPT-3.5 family). The paper does not report parameter count or training corpus; evaluated via Chat Completions API (greedy decoding, temperature=0).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Few-shot multi-digit integer addition (addends 7–9 digits; tasks balanced by addend and answer digit lengths).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Evidence consistent with digit-tokenized, digit-by-digit style computations that operate on tokens representing 1–3 digit chunks; model behaviour sensitive to alignment between input-tokenization and output-tokenization (L2R misalignment produces systematic substitution errors).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Behavioral experiments varying input string formatting to force different tokenizations (commas/other delimiters to enforce R2L), few-shot prompting, repeat-and-convert prompts (L2R→R2L), log-prob/top-5 token analysis and entropy lower-bound, and merge-rank (BPE merge order) based frequency analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>R2L (comma-enforced) 1-shot: 95.6%, 8-shot: 97.8%; L2R 1-shot: 68.5%, 8-shot: 75.6% (examples from main results). In length-mismatch cases (answer longer than addends) L2R accuracy drops to 8.25%. Across controlled set of 1300 problems: R2L missed 25 problems (24 were off-by-one errors), L2R (length-match condition) missed 56/900 (53 off-by-one).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Two dominant failure modes: (1) In L2R length-mismatch cases a stereotyped 'digit 4' error where the model almost always mispredicts the fourth digit (while getting first three digits—first output token—correct). (2) Off-by-one errors concentrated at token boundaries (last digit of an output token) for both tokenizations; most remaining errors are +/-1 in the units digit of a token.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Behavioral alignment: forcing R2L tokenization (commas or other delimiters) dramatically improves accuracy; the stereotyped digit-4 error appears only under L2R and only in length-mismatch cases, suggesting a token-alignment based computation; logprob/top-5 analysis shows in digit-4 errors entropy ~2.06 (near-random among 10 possibilities) and correct answer in top-5 ~49.6% of the time, consistent with guessing among token-level possibilities; off-by-one errors show top-2 tokens concentrated and entropy ~0.45, implying local uncertainty between adjacent tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Token-frequency (merge-rank) analyses show only mild influence—while L2R errors slightly biased to more frequent tokens, frequency is not a dominant explanatory factor. Increasing shots partially mitigates L2R but does not recover R2L performance fully. Larger/updated models (GPT-4 variants) reduce but do not eliminate the tokenization effect; some later GPT-4 Turbo regressions suggest scale is not a full remedy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8404.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8404.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (various checkpoints)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 (gpt-4-0314, gpt-4-0613, gpt-4-1106-preview variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Larger model family evaluated as a scale comparison to GPT-3.5; tokenization-dependent effects persist but are mitigated in later GPT-4 checkpoints relative to GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4 (multiple checkpoints: 0314, 0613, 1106-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 family (presumed larger than GPT-3.5); exact parameter counts and pretraining details not given in the paper. Evaluated through the same few-shot addition protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Few-shot multi-digit integer addition (same 7–9 digit tasks as for GPT-3.5).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Similar sensitivity to tokenization alignment as GPT-3.5, but evidence suggests larger scale reduces reliance on the L2R token-alignment inductive bias; still operates on multi-digit tokens that shape computations.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Same behavioral manipulations as for GPT-3.5 (comma-enforced R2L, different delimiters, repeat-convert prompting, thinking-token controls) and performance comparison across model updates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Across evaluated checkpoints GPT-4 models perform better on arithmetic overall; tokenization-dependent gap is present in early GPT-4 checkpoints and reduced or weakened in some later checkpoints (paper reports relative reductions but exact per-checkpoint percentages shown in Figure 12 of the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Length-mismatch induced errors and off-by-one token-boundary errors are present but less dominant; one GPT-4 Turbo checkpoint showed a stronger tokenization effect again, suggesting model updates and size interact with the bias.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Mitigation of L2R/R2L gap with scale and updated checkpoints supports hypothesis that larger models can override tokenization-induced inductive biases; however persistence of error patterns implies underlying token-level computation remains relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Effect weakening is not monotonic across checkpoints (e.g., some turbo variant shows stronger effect), indicating that training/data/architecture changes beyond pure scale affect outcomes; paper notes lack of full transparency about model updates so causal attribution is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8404.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8404.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tokenization direction (L2R vs R2L)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Left-to-right vs right-to-left number tokenization (directional segmentation of digits into BPE tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper manipulates tokenization direction by inserting delimiters every three digits to enforce R2L segmentation, and compares this to the default L2R segmentation in OpenAI tokenizers (which produce 1-,2-,3-digit number tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 / GPT-4 (studied models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model but an experimental manipulation of input string formatting that changes tokenizer segmentation: L2R is the default BPE segmentation direction (left-to-right chunks), R2L is enforced by comma/delimiter insertion causing right-to-left chunking in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit integer addition (7–9 digit addends).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Aligns least-significant digits of inputs and outputs when R2L is used (so tokens group digits from units upward), facilitating digit-by-digit addition algorithms; L2R causes misalignment between input addend token boundaries and output answer token boundaries, which can break such token-level algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Behavioral experiments: (a) insert commas (or other single-token delimiters like '.', '#', '$') every 3 digits from right to force R2L; (b) control experiments with separators used in L2R to equate token counts; (c) repeat-and-convert few-shot prompting; (d) thinking-token controls (adding spaces) to match token counts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>R2L yields up to ~20 percentage points higher accuracy; example numbers: R2L 8-shot ~97.8% vs L2R 8-shot ~75.6% (gpt-3.5-turbo-0301). R2L improvement robust to choice of delimiter (commas, '#', '.', '$').</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>L2R shows catastrophic failures when answer length != addend length (length-mismatch): almost all such L2R errors are the digit-4 stereotyped pattern; L2R also exhibits more off-by-one token-boundary errors overall.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Large, consistent behavioral difference when only tokenization direction is changed (delimiter-only), insensitivity to delimiter semantics, inability of equal-token-count 'thinking token' controls to recover performance, and ability of models to recover performance when explicitly converting input tokenization via prompting all support that tokenization segmentation and token alignment strongly shape arithmetic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Larger models (GPT-4) reduce but do not eliminate the effect, and some later model variants show variable results, indicating tokenization is a strong but not sole determinant of arithmetic success. Also token-frequency only weakly explains observed errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8404.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8404.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Comma-enforced R2L intervention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Forcing right-to-left tokenization by inserting delimiters (commas or other single-token separators) every three digits from the right</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intervention used in experiments: inserting commas (or '#', '.', '$') every three digits to change the tokenizer segmentation from default L2R to R2L, improving arithmetic performance substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301 and gpt-4 variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Formatting-level intervention applied to input strings to change token segmentation seen by the model; no model weights altered.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Few-shot multi-digit addition (7–9 digit addends).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>By separating digits into 3-digit groups from the right, the least significant digits (units) are grouped into the first output token; this aligns input addend tokens with output tokens, hypothesized to make digit-by-digit addition easier for the model.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Behavioral comparison across delimiters and token-count-matched controls; used as primary causal manipulation to test tokenization effects.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Using commas (R2L) increases accuracy dramatically (examples: GPT-3.5 1-shot 95.6% vs L2R 68.5%; 8-shot 97.8% vs L2R 75.6%). Delimiter identity matters little—the direction of tokenization is dominant.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>When R2L used, remaining errors are mostly off-by-one at token boundaries rather than systematic digit-position failures; the digit-4 stereotyped failure disappears under R2L.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Simple delimiter insertion (no additional training) reproducibly improves performance; delimiter-control experiments (different separators) show similar gains; thinking-token controls that equalize token counts do not reproduce gains, supporting tokenization alignment as the causal factor.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Intervention is an input-hack; it requires access to input formatting and does not change model internal weights. Some newer model variants show smaller gains, indicating other training factors interact with tokenization effects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8404.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8404.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Repeat-convert prompting (L2R→R2L)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot prompt asking model to repeat the input in R2L tokenization then answer (chain-of-thought-inspired conversion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting intervention where the model is few-shot trained to take an L2R-formatted problem, repeat it in R2L format, and then answer; this recovers much of the R2L performance even when the original input is L2R.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301 and gpt-4 variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt-engineering intervention (no fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Few-shot multi-digit addition.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Encourages the model to internally perform or present a tokenization conversion before solving, effectively giving it the aligned R2L input representation it prefers.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Few-shot examples in which assistant responses demonstrate the desired repeat-and-convert behaviour. Compared to control where only output tokenization is changed (without repeating), repeat-and-convert yields larger accuracy gains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Models that repeat and convert L2R→R2L perform nearly as well as when they receive problems already in R2L; performance increases with number of shots as adherence to conversion style improves (figures in paper show convergence to R2L-level accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Merely instructing the model to output answers in R2L without repeating the problem does not recover performance; the model needs to see the problem in the preferred tokenization for the benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Large behavioral improvement from prompting conversion indicates model does not automatically re-tokenize input in the forward pass but can be guided to do so via prompting; supports hypothesis that explicit token-alignment enables correct computations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Requires prompt engineering and few-shot examples; without adherence to the repeat format (few shots), the model may ignore the format and perform worse (noted in R2L→L2R case where adherence increases with shots).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8404.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8404.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Digit-4 stereotyped error</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Length-mismatch 'digit 4' error pattern under L2R tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A striking, highly stereotyped failure mode observed: when answer has one more digit than addends under L2R tokenization, the model almost always gets the fourth digit wrong while getting first three digits correct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301 (primarily)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Observed behavioural failure mode in experiments on few-shot addition; quantified on a controlled set of 1300 problems.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition cases where carry causes answer to be longer than inputs (e.g., 7-digit + 7-digit → 8-digit).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Consistent with a token-aligned internal algorithm that properly computes the first output token (first three digits) but then misestimates the next token, suggesting a breakdown at the token boundary when the output tokenization shifts due to final carry.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Behavioral frequency counts, logprob/top-5 analysis and entropy lower-bound, token-frequency (merge-rank) analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In the length-mismatch L2R condition, overall accuracy 8.25%; of the incorrect 91.25% cases, the model always mispredicts digit 4 (and sometimes also later digits). Logprob entropy per token ~2.066 (both correct and incorrect subsets), and the correct answer appears in the model's top-5 outputs ~49.6% of the time for errors.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Magnitude-biased but seemingly haphazard substitutions for digit 4 (slight preference for off-by-one but errors distributed), consistent with guessing among the 10 possible digit tokens for that position.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Reproducible across many examples; disappears under R2L tokenization and mitigated with prompting to convert to R2L; logprob and entropy statistics support that the model is uncertain and sometimes effectively guessing the token that contains digit 4.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Not observed under R2L tokenization; larger models show reduced incidence; token-frequency analysis (merge-rank) shows only a small preference for more frequent tokens, so frequency alone doesn't explain the pattern.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8404.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8404.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Off-by-one token-boundary errors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Off-by-one errors concentrated at output token boundaries (units digit of a token)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A common error mode where the model outputs a token that is exactly one integer away from the correct token value; these occur disproportionately at the last digit of an output token.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301 and GPT-4 (observed in both)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Behavioral failure observed in experiments across tokenizations and conditions; quantified counts provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Few-shot multi-digit addition; both length-match and R2L testing conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Suggests uncertainty localized to crossing token boundaries—models may compute correctly within a token but be uncertain in selecting adjacent token indices (e.g., off-by-one in 3-digit token representation).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Counting error types in result set, analyzing top-5 token logprobs and entropy; conditioning on cases where an off-by-one was possible and checking merge-rank preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Of R2L's 25 missed cases (over 1300), 24 were off-by-one; of L2R's 56 missed cases (length-match), 53 were off-by-one. Entropy for these errors ~0.45 and the correct token is often the second-most likely token.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Localized +/-1 substitutions in the units digit of a token; predominantly a token-boundary phenomenon.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Top-2-token dominance in logprob distributions (correct token commonly second-most-likely) and localization of errors to token boundaries support the hypothesis that token-level discretization causes step-like uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Merge-rank / token-frequency analysis shows selection of the most frequent token happens roughly at chance among the limited candidate set, so frequency does not fully explain off-by-one choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8404.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8404.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Probing: logprob & entropy analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Top-k log probability inspection and lower-bound entropy metric (H_lower) from top-5 tokens</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors analyze the model's top-5 token log probabilities per output position and compute a lower-bound entropy to quantify uncertainty and interpret error modes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301 and gpt-4 variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A probing technique applied to model outputs available from the API (top-5 tokens with logprobs) to infer uncertainty and distributional shape over candidate output tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Applied to few-shot multi-digit addition outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Provides evidence about whether errors reflect broad guessing among many token-level alternatives (high entropy) or narrow confusion between a few adjacent tokens (low entropy, top-2 dominant).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Compute H_lower using available top-5 probabilities as a lower bound to true entropy; examine relative ranks and whether correct token appears in top-5; compare entropies across conditions (digit-4 errors vs off-by-one errors vs correct cases).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Digit-4 errors: H_lower ≈ 2.06 nats (near entropy of uniform over 10 tokens ~2.3 nats), correct appears in top-5 ~49.6% of error cases. Off-by-one errors: H_lower ≈ 0.45 nats with top-2 tokens capturing most mass (correct often second-most-likely).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>High-entropy errors consistent with guessing among many possibilities (digit-4 case); low-entropy pairwise confusion in off-by-one cases.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Different entropy signatures map to different failure modes and support the interpretation that some failures are broad guesses across digit-token choices while others are local, near-correct confusions.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Entropy is a lower bound (only top-5 visible), so conclusions are conservative; access limitations to full distribution and internal activations limit mechanistic claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8404.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8404.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Merge-rank frequency probing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using BPE merge ranks as a proxy for token frequency to test frequency biases in numeric outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Authors use tokenizer BPE merge creation order (merge rank) as an approximate signal of token frequency in pretraining data to test whether model errors preferentially output more frequent tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301 (analysis) and general tokenizer diagnostics</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A lightweight, indirect frequency probe relying on the order tokens were created during tokenizer training (merge ranks), used because pretraining corpora are not available.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Applied to incorrect outputs in few-shot multi-digit addition to test whether incorrect tokens are more frequent.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Tests hypothesis that some arithmetic errors are due to memorization/term-frequency biases favoring common numeric tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>For each mistaken token, check whether the output token has a lower merge-rank (i.e., is created earlier / presumably more frequent) than the correct token; rank possible substituted tokens to see frequency bias.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mild signal: In R2L errors 60% (15/25) substituted more frequent tokens (p=0.115 not significant); in L2R errors 56% (238/425) substituted more frequent tokens (p=0.005). In digit-4 errors, slight preference for most likely token (~16% vs chance 10%) but no clear trend overall.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Frequency bias is present but small and insufficient to explain the dominant digit-4 stereotyped errors or token-boundary off-by-one errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Statistical counts show only modest over-representation of higher-frequency tokens in erroneous outputs; off-by-one choices are approximately at-chance with respect to frequency among the small candidate set.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Merge-rank is an imperfect proxy for real token frequencies in the pretraining corpus; access limitations to pretraining data means stronger claims about frequency effects are not possible from this probe alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8404.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8404.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Thinking-token / token-count controls</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Controls adding extra predictable tokens (spaces/separators) to match input/output token counts to rule out extra-computation (thinking-token) confound</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiments where authors add separators or spaces to L2R inputs to match token counts introduced by R2L comma insertion, testing whether extra tokens (more computation steps) explain improved performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt-level control experiments (no model weight changes).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Few-shot multi-digit addition.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Tests hypothesis that extra tokens (and thus extra autoregressive decoding steps) give the model 'thinking' capacity to improve calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Add separators to enforce L2R segmentation with same token counts as R2L, and add spaces around symbols to increase token count; compare accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Controls did not recover the R2L performance boost; in some cases separators in L2R hurt performance (model hallucinated trailing zeros).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Adding predictable tokens does not enable the model to perform additional useful internal computation in this setting (no recovery of R2L advantage).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Null result from token-count matched controls argues improvements are due to tokenization alignment not extra autoregressive steps.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Authors note other work suggesting 'thinking tokens' can help with additional training; but in their zero/few-shot evaluation without further training, no effect was observed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8404.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8404.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BPE multi-digit number tokens (1-3 digit tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Byte Pair Encoding tokenizer choice to have dedicated 1-, 2-, and 3-digit number tokens (as used in some OpenAI tokenizers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tokenizer design choice where many small integer strings (1–3 digits) are represented by single tokens; this is contrasted with single-digit tokenization and with pure BPE learned tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI tokenizers / studied models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Tokenizer-level design (vocabulary) that yields number tokens covering up to 3-digit strings; authors note this likely motivated by compression trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Relevant because it determines how multi-digit numbers are represented as tokens for arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Multi-digit number tokens create coarse-grained tokenization where tokens correspond to 1–3 digit chunks; this representation interacts with autoregressive prediction and can create token-boundary alignment or misalignment between inputs and outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Indirect: authors manipulate text formatting to change tokenizer segmentations and observe behavioral consequences; they also visualize tokenizer coverage of 3-digit strings in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not a performance metric per se, but the presence of 1–3 digit tokens correlates with the observed token-boundary error modes and digit-4 failure under L2R.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Using up-to-3-digit tokens creates token boundaries every 1–3 digits; misalignment between input and output token boundaries (especially in L2R) appears to underlie catastrophic length-mismatch errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Behavioral sensitivity to changing the grouping direction (L2R vs R2L) implies the model's computation depends on how digits are grouped into tokens. Authors also present tokenizer diagnostics (appendix) showing which 3-digit strings have dedicated tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Single-digit tokenizers (used in other models like PaLM, LLaMA) would not suffer the same alignment issues but trade off different compression/efficiency properties; the paper calls for 'gold experiments' training identical models with different tokenizers to fully isolate effects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8404.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8404.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mechanism hypothesis: token-alignment algorithmic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothesis that models learn token-aligned digit-by-digit addition algorithms which rely on alignment of least-significant-digit tokens between inputs and outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper-level mechanistic hypothesis: LLMs may implement an algorithmic-like procedure working on tokens representing digit chunks, and misalignment between input and output token boundaries (L2R) breaks that learned procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5 and gpt-4 (as studied)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A proposed interpretive model of how arithmetic is implemented in autoregressive LLMs trained with BPE numeric tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit addition; especially cases where final carry changes output tokenization length.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Models appear to compute on token-level digit chunks (e.g., 3-digit tokens). If the tokens are aligned so that least-significant digits map to the same token indices between inputs and outputs (R2L), the model can implement an effective carry-propagating algorithm; L2R breaks this alignment and causes systematic token-level failure.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Inferred from behavioral manipulations (tokenization direction changes, prompt-based conversion) and diagnostic probes (logprob/top-5, entropy, error-pattern counts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Explains why R2L formatting yields dramatic accuracy gains, and why L2R length-mismatch causes near-total failure in many cases (e.g., 8.25% accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Predicts localized token-boundary failures, digit-position specific systematic mistakes (digit-4), and the relative robustness to number of carries (paper found no correlation between number of carries and difficulty).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Strong behavioral causality: simply changing tokenization alignment via delimiters or by prompting conversion recovers performance; stereotyped and reproducible digit-position error patterns mirror signatures of algorithmic breakdown at token boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>No direct internal activation-level mechanistic proof (e.g., neuron or attention patching) is provided; authors reference prior work (Stolfo et al.) for such causal mechanistic methods and call for future internal interpretability work. Also larger models can partly overcome these inductive biases, showing the hypothesis is not the sole determinant of arithmetic competence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>What algorithms can transformers learn? a study in length generalization. <em>(Rating: 2)</em></li>
                <li>Teaching algorithmic reasoning via in-context learning. <em>(Rating: 2)</em></li>
                <li>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. <em>(Rating: 2)</em></li>
                <li>Impact of pretraining term frequencies on few-shot numerical reasoning. <em>(Rating: 2)</em></li>
                <li>Exploring length generalization in large language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8404",
    "paper_id": "paper-49c45d2a2773c537804c38d69cde67e00fbad6fe",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "GPT-3.5 (gpt-3.5-turbo-0301)",
            "name_full": "OpenAI GPT-3.5 (gpt-3.5-turbo-0301 checkpoint)",
            "brief_description": "The main model evaluated in the paper for multi-digit addition; accessed via OpenAI Chat Completions API and evaluated in few-shot settings (1,2,4,8 shots) on 7–9 digit addition problems.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0301",
            "model_description": "Autoregressive transformer (OpenAI GPT-3.5 family). The paper does not report parameter count or training corpus; evaluated via Chat Completions API (greedy decoding, temperature=0).",
            "arithmetic_task_type": "Few-shot multi-digit integer addition (addends 7–9 digits; tasks balanced by addend and answer digit lengths).",
            "mechanism_or_representation": "Evidence consistent with digit-tokenized, digit-by-digit style computations that operate on tokens representing 1–3 digit chunks; model behaviour sensitive to alignment between input-tokenization and output-tokenization (L2R misalignment produces systematic substitution errors).",
            "probing_or_intervention_method": "Behavioral experiments varying input string formatting to force different tokenizations (commas/other delimiters to enforce R2L), few-shot prompting, repeat-and-convert prompts (L2R→R2L), log-prob/top-5 token analysis and entropy lower-bound, and merge-rank (BPE merge order) based frequency analysis.",
            "performance_metrics": "R2L (comma-enforced) 1-shot: 95.6%, 8-shot: 97.8%; L2R 1-shot: 68.5%, 8-shot: 75.6% (examples from main results). In length-mismatch cases (answer longer than addends) L2R accuracy drops to 8.25%. Across controlled set of 1300 problems: R2L missed 25 problems (24 were off-by-one errors), L2R (length-match condition) missed 56/900 (53 off-by-one).",
            "error_types_or_failure_modes": "Two dominant failure modes: (1) In L2R length-mismatch cases a stereotyped 'digit 4' error where the model almost always mispredicts the fourth digit (while getting first three digits—first output token—correct). (2) Off-by-one errors concentrated at token boundaries (last digit of an output token) for both tokenizations; most remaining errors are +/-1 in the units digit of a token.",
            "evidence_for_mechanism": "Behavioral alignment: forcing R2L tokenization (commas or other delimiters) dramatically improves accuracy; the stereotyped digit-4 error appears only under L2R and only in length-mismatch cases, suggesting a token-alignment based computation; logprob/top-5 analysis shows in digit-4 errors entropy ~2.06 (near-random among 10 possibilities) and correct answer in top-5 ~49.6% of the time, consistent with guessing among token-level possibilities; off-by-one errors show top-2 tokens concentrated and entropy ~0.45, implying local uncertainty between adjacent tokens.",
            "counterexamples_or_challenges": "Token-frequency (merge-rank) analyses show only mild influence—while L2R errors slightly biased to more frequent tokens, frequency is not a dominant explanatory factor. Increasing shots partially mitigates L2R but does not recover R2L performance fully. Larger/updated models (GPT-4 variants) reduce but do not eliminate the tokenization effect; some later GPT-4 Turbo regressions suggest scale is not a full remedy.",
            "uuid": "e8404.0",
            "source_info": {
                "paper_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4 (various checkpoints)",
            "name_full": "OpenAI GPT-4 (gpt-4-0314, gpt-4-0613, gpt-4-1106-preview variants)",
            "brief_description": "Larger model family evaluated as a scale comparison to GPT-3.5; tokenization-dependent effects persist but are mitigated in later GPT-4 checkpoints relative to GPT-3.5.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4 (multiple checkpoints: 0314, 0613, 1106-preview)",
            "model_description": "OpenAI GPT-4 family (presumed larger than GPT-3.5); exact parameter counts and pretraining details not given in the paper. Evaluated through the same few-shot addition protocol.",
            "arithmetic_task_type": "Few-shot multi-digit integer addition (same 7–9 digit tasks as for GPT-3.5).",
            "mechanism_or_representation": "Similar sensitivity to tokenization alignment as GPT-3.5, but evidence suggests larger scale reduces reliance on the L2R token-alignment inductive bias; still operates on multi-digit tokens that shape computations.",
            "probing_or_intervention_method": "Same behavioral manipulations as for GPT-3.5 (comma-enforced R2L, different delimiters, repeat-convert prompting, thinking-token controls) and performance comparison across model updates.",
            "performance_metrics": "Across evaluated checkpoints GPT-4 models perform better on arithmetic overall; tokenization-dependent gap is present in early GPT-4 checkpoints and reduced or weakened in some later checkpoints (paper reports relative reductions but exact per-checkpoint percentages shown in Figure 12 of the paper).",
            "error_types_or_failure_modes": "Length-mismatch induced errors and off-by-one token-boundary errors are present but less dominant; one GPT-4 Turbo checkpoint showed a stronger tokenization effect again, suggesting model updates and size interact with the bias.",
            "evidence_for_mechanism": "Mitigation of L2R/R2L gap with scale and updated checkpoints supports hypothesis that larger models can override tokenization-induced inductive biases; however persistence of error patterns implies underlying token-level computation remains relevant.",
            "counterexamples_or_challenges": "Effect weakening is not monotonic across checkpoints (e.g., some turbo variant shows stronger effect), indicating that training/data/architecture changes beyond pure scale affect outcomes; paper notes lack of full transparency about model updates so causal attribution is limited.",
            "uuid": "e8404.1",
            "source_info": {
                "paper_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Tokenization direction (L2R vs R2L)",
            "name_full": "Left-to-right vs right-to-left number tokenization (directional segmentation of digits into BPE tokens)",
            "brief_description": "The paper manipulates tokenization direction by inserting delimiters every three digits to enforce R2L segmentation, and compares this to the default L2R segmentation in OpenAI tokenizers (which produce 1-,2-,3-digit number tokens).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 / GPT-4 (studied models)",
            "model_description": "Not a model but an experimental manipulation of input string formatting that changes tokenizer segmentation: L2R is the default BPE segmentation direction (left-to-right chunks), R2L is enforced by comma/delimiter insertion causing right-to-left chunking in practice.",
            "arithmetic_task_type": "Multi-digit integer addition (7–9 digit addends).",
            "mechanism_or_representation": "Aligns least-significant digits of inputs and outputs when R2L is used (so tokens group digits from units upward), facilitating digit-by-digit addition algorithms; L2R causes misalignment between input addend token boundaries and output answer token boundaries, which can break such token-level algorithms.",
            "probing_or_intervention_method": "Behavioral experiments: (a) insert commas (or other single-token delimiters like '.', '#', '$') every 3 digits from right to force R2L; (b) control experiments with separators used in L2R to equate token counts; (c) repeat-and-convert few-shot prompting; (d) thinking-token controls (adding spaces) to match token counts.",
            "performance_metrics": "R2L yields up to ~20 percentage points higher accuracy; example numbers: R2L 8-shot ~97.8% vs L2R 8-shot ~75.6% (gpt-3.5-turbo-0301). R2L improvement robust to choice of delimiter (commas, '#', '.', '$').",
            "error_types_or_failure_modes": "L2R shows catastrophic failures when answer length != addend length (length-mismatch): almost all such L2R errors are the digit-4 stereotyped pattern; L2R also exhibits more off-by-one token-boundary errors overall.",
            "evidence_for_mechanism": "Large, consistent behavioral difference when only tokenization direction is changed (delimiter-only), insensitivity to delimiter semantics, inability of equal-token-count 'thinking token' controls to recover performance, and ability of models to recover performance when explicitly converting input tokenization via prompting all support that tokenization segmentation and token alignment strongly shape arithmetic computation.",
            "counterexamples_or_challenges": "Larger models (GPT-4) reduce but do not eliminate the effect, and some later model variants show variable results, indicating tokenization is a strong but not sole determinant of arithmetic success. Also token-frequency only weakly explains observed errors.",
            "uuid": "e8404.2",
            "source_info": {
                "paper_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Comma-enforced R2L intervention",
            "name_full": "Forcing right-to-left tokenization by inserting delimiters (commas or other single-token separators) every three digits from the right",
            "brief_description": "An intervention used in experiments: inserting commas (or '#', '.', '$') every three digits to change the tokenizer segmentation from default L2R to R2L, improving arithmetic performance substantially.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0301 and gpt-4 variants",
            "model_description": "Formatting-level intervention applied to input strings to change token segmentation seen by the model; no model weights altered.",
            "arithmetic_task_type": "Few-shot multi-digit addition (7–9 digit addends).",
            "mechanism_or_representation": "By separating digits into 3-digit groups from the right, the least significant digits (units) are grouped into the first output token; this aligns input addend tokens with output tokens, hypothesized to make digit-by-digit addition easier for the model.",
            "probing_or_intervention_method": "Behavioral comparison across delimiters and token-count-matched controls; used as primary causal manipulation to test tokenization effects.",
            "performance_metrics": "Using commas (R2L) increases accuracy dramatically (examples: GPT-3.5 1-shot 95.6% vs L2R 68.5%; 8-shot 97.8% vs L2R 75.6%). Delimiter identity matters little—the direction of tokenization is dominant.",
            "error_types_or_failure_modes": "When R2L used, remaining errors are mostly off-by-one at token boundaries rather than systematic digit-position failures; the digit-4 stereotyped failure disappears under R2L.",
            "evidence_for_mechanism": "Simple delimiter insertion (no additional training) reproducibly improves performance; delimiter-control experiments (different separators) show similar gains; thinking-token controls that equalize token counts do not reproduce gains, supporting tokenization alignment as the causal factor.",
            "counterexamples_or_challenges": "Intervention is an input-hack; it requires access to input formatting and does not change model internal weights. Some newer model variants show smaller gains, indicating other training factors interact with tokenization effects.",
            "uuid": "e8404.3",
            "source_info": {
                "paper_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Repeat-convert prompting (L2R→R2L)",
            "name_full": "Few-shot prompt asking model to repeat the input in R2L tokenization then answer (chain-of-thought-inspired conversion)",
            "brief_description": "A prompting intervention where the model is few-shot trained to take an L2R-formatted problem, repeat it in R2L format, and then answer; this recovers much of the R2L performance even when the original input is L2R.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0301 and gpt-4 variants",
            "model_description": "Prompt-engineering intervention (no fine-tuning).",
            "arithmetic_task_type": "Few-shot multi-digit addition.",
            "mechanism_or_representation": "Encourages the model to internally perform or present a tokenization conversion before solving, effectively giving it the aligned R2L input representation it prefers.",
            "probing_or_intervention_method": "Few-shot examples in which assistant responses demonstrate the desired repeat-and-convert behaviour. Compared to control where only output tokenization is changed (without repeating), repeat-and-convert yields larger accuracy gains.",
            "performance_metrics": "Models that repeat and convert L2R→R2L perform nearly as well as when they receive problems already in R2L; performance increases with number of shots as adherence to conversion style improves (figures in paper show convergence to R2L-level accuracy).",
            "error_types_or_failure_modes": "Merely instructing the model to output answers in R2L without repeating the problem does not recover performance; the model needs to see the problem in the preferred tokenization for the benefit.",
            "evidence_for_mechanism": "Large behavioral improvement from prompting conversion indicates model does not automatically re-tokenize input in the forward pass but can be guided to do so via prompting; supports hypothesis that explicit token-alignment enables correct computations.",
            "counterexamples_or_challenges": "Requires prompt engineering and few-shot examples; without adherence to the repeat format (few shots), the model may ignore the format and perform worse (noted in R2L→L2R case where adherence increases with shots).",
            "uuid": "e8404.4",
            "source_info": {
                "paper_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Digit-4 stereotyped error",
            "name_full": "Length-mismatch 'digit 4' error pattern under L2R tokenization",
            "brief_description": "A striking, highly stereotyped failure mode observed: when answer has one more digit than addends under L2R tokenization, the model almost always gets the fourth digit wrong while getting first three digits correct.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0301 (primarily)",
            "model_description": "Observed behavioural failure mode in experiments on few-shot addition; quantified on a controlled set of 1300 problems.",
            "arithmetic_task_type": "Addition cases where carry causes answer to be longer than inputs (e.g., 7-digit + 7-digit → 8-digit).",
            "mechanism_or_representation": "Consistent with a token-aligned internal algorithm that properly computes the first output token (first three digits) but then misestimates the next token, suggesting a breakdown at the token boundary when the output tokenization shifts due to final carry.",
            "probing_or_intervention_method": "Behavioral frequency counts, logprob/top-5 analysis and entropy lower-bound, token-frequency (merge-rank) analysis.",
            "performance_metrics": "In the length-mismatch L2R condition, overall accuracy 8.25%; of the incorrect 91.25% cases, the model always mispredicts digit 4 (and sometimes also later digits). Logprob entropy per token ~2.066 (both correct and incorrect subsets), and the correct answer appears in the model's top-5 outputs ~49.6% of the time for errors.",
            "error_types_or_failure_modes": "Magnitude-biased but seemingly haphazard substitutions for digit 4 (slight preference for off-by-one but errors distributed), consistent with guessing among the 10 possible digit tokens for that position.",
            "evidence_for_mechanism": "Reproducible across many examples; disappears under R2L tokenization and mitigated with prompting to convert to R2L; logprob and entropy statistics support that the model is uncertain and sometimes effectively guessing the token that contains digit 4.",
            "counterexamples_or_challenges": "Not observed under R2L tokenization; larger models show reduced incidence; token-frequency analysis (merge-rank) shows only a small preference for more frequent tokens, so frequency alone doesn't explain the pattern.",
            "uuid": "e8404.5",
            "source_info": {
                "paper_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Off-by-one token-boundary errors",
            "name_full": "Off-by-one errors concentrated at output token boundaries (units digit of a token)",
            "brief_description": "A common error mode where the model outputs a token that is exactly one integer away from the correct token value; these occur disproportionately at the last digit of an output token.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0301 and GPT-4 (observed in both)",
            "model_description": "Behavioral failure observed in experiments across tokenizations and conditions; quantified counts provided in paper.",
            "arithmetic_task_type": "Few-shot multi-digit addition; both length-match and R2L testing conditions.",
            "mechanism_or_representation": "Suggests uncertainty localized to crossing token boundaries—models may compute correctly within a token but be uncertain in selecting adjacent token indices (e.g., off-by-one in 3-digit token representation).",
            "probing_or_intervention_method": "Counting error types in result set, analyzing top-5 token logprobs and entropy; conditioning on cases where an off-by-one was possible and checking merge-rank preferences.",
            "performance_metrics": "Of R2L's 25 missed cases (over 1300), 24 were off-by-one; of L2R's 56 missed cases (length-match), 53 were off-by-one. Entropy for these errors ~0.45 and the correct token is often the second-most likely token.",
            "error_types_or_failure_modes": "Localized +/-1 substitutions in the units digit of a token; predominantly a token-boundary phenomenon.",
            "evidence_for_mechanism": "Top-2-token dominance in logprob distributions (correct token commonly second-most-likely) and localization of errors to token boundaries support the hypothesis that token-level discretization causes step-like uncertainty.",
            "counterexamples_or_challenges": "Merge-rank / token-frequency analysis shows selection of the most frequent token happens roughly at chance among the limited candidate set, so frequency does not fully explain off-by-one choices.",
            "uuid": "e8404.6",
            "source_info": {
                "paper_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Probing: logprob & entropy analysis",
            "name_full": "Top-k log probability inspection and lower-bound entropy metric (H_lower) from top-5 tokens",
            "brief_description": "The authors analyze the model's top-5 token log probabilities per output position and compute a lower-bound entropy to quantify uncertainty and interpret error modes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0301 and gpt-4 variants",
            "model_description": "A probing technique applied to model outputs available from the API (top-5 tokens with logprobs) to infer uncertainty and distributional shape over candidate output tokens.",
            "arithmetic_task_type": "Applied to few-shot multi-digit addition outputs.",
            "mechanism_or_representation": "Provides evidence about whether errors reflect broad guessing among many token-level alternatives (high entropy) or narrow confusion between a few adjacent tokens (low entropy, top-2 dominant).",
            "probing_or_intervention_method": "Compute H_lower using available top-5 probabilities as a lower bound to true entropy; examine relative ranks and whether correct token appears in top-5; compare entropies across conditions (digit-4 errors vs off-by-one errors vs correct cases).",
            "performance_metrics": "Digit-4 errors: H_lower ≈ 2.06 nats (near entropy of uniform over 10 tokens ~2.3 nats), correct appears in top-5 ~49.6% of error cases. Off-by-one errors: H_lower ≈ 0.45 nats with top-2 tokens capturing most mass (correct often second-most-likely).",
            "error_types_or_failure_modes": "High-entropy errors consistent with guessing among many possibilities (digit-4 case); low-entropy pairwise confusion in off-by-one cases.",
            "evidence_for_mechanism": "Different entropy signatures map to different failure modes and support the interpretation that some failures are broad guesses across digit-token choices while others are local, near-correct confusions.",
            "counterexamples_or_challenges": "Entropy is a lower bound (only top-5 visible), so conclusions are conservative; access limitations to full distribution and internal activations limit mechanistic claims.",
            "uuid": "e8404.7",
            "source_info": {
                "paper_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Merge-rank frequency probing",
            "name_full": "Using BPE merge ranks as a proxy for token frequency to test frequency biases in numeric outputs",
            "brief_description": "Authors use tokenizer BPE merge creation order (merge rank) as an approximate signal of token frequency in pretraining data to test whether model errors preferentially output more frequent tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0301 (analysis) and general tokenizer diagnostics",
            "model_description": "A lightweight, indirect frequency probe relying on the order tokens were created during tokenizer training (merge ranks), used because pretraining corpora are not available.",
            "arithmetic_task_type": "Applied to incorrect outputs in few-shot multi-digit addition to test whether incorrect tokens are more frequent.",
            "mechanism_or_representation": "Tests hypothesis that some arithmetic errors are due to memorization/term-frequency biases favoring common numeric tokens.",
            "probing_or_intervention_method": "For each mistaken token, check whether the output token has a lower merge-rank (i.e., is created earlier / presumably more frequent) than the correct token; rank possible substituted tokens to see frequency bias.",
            "performance_metrics": "Mild signal: In R2L errors 60% (15/25) substituted more frequent tokens (p=0.115 not significant); in L2R errors 56% (238/425) substituted more frequent tokens (p=0.005). In digit-4 errors, slight preference for most likely token (~16% vs chance 10%) but no clear trend overall.",
            "error_types_or_failure_modes": "Frequency bias is present but small and insufficient to explain the dominant digit-4 stereotyped errors or token-boundary off-by-one errors.",
            "evidence_for_mechanism": "Statistical counts show only modest over-representation of higher-frequency tokens in erroneous outputs; off-by-one choices are approximately at-chance with respect to frequency among the small candidate set.",
            "counterexamples_or_challenges": "Merge-rank is an imperfect proxy for real token frequencies in the pretraining corpus; access limitations to pretraining data means stronger claims about frequency effects are not possible from this probe alone.",
            "uuid": "e8404.8",
            "source_info": {
                "paper_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Thinking-token / token-count controls",
            "name_full": "Controls adding extra predictable tokens (spaces/separators) to match input/output token counts to rule out extra-computation (thinking-token) confound",
            "brief_description": "Experiments where authors add separators or spaces to L2R inputs to match token counts introduced by R2L comma insertion, testing whether extra tokens (more computation steps) explain improved performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0301",
            "model_description": "Prompt-level control experiments (no model weight changes).",
            "arithmetic_task_type": "Few-shot multi-digit addition.",
            "mechanism_or_representation": "Tests hypothesis that extra tokens (and thus extra autoregressive decoding steps) give the model 'thinking' capacity to improve calculation.",
            "probing_or_intervention_method": "Add separators to enforce L2R segmentation with same token counts as R2L, and add spaces around symbols to increase token count; compare accuracy.",
            "performance_metrics": "Controls did not recover the R2L performance boost; in some cases separators in L2R hurt performance (model hallucinated trailing zeros).",
            "error_types_or_failure_modes": "Adding predictable tokens does not enable the model to perform additional useful internal computation in this setting (no recovery of R2L advantage).",
            "evidence_for_mechanism": "Null result from token-count matched controls argues improvements are due to tokenization alignment not extra autoregressive steps.",
            "counterexamples_or_challenges": "Authors note other work suggesting 'thinking tokens' can help with additional training; but in their zero/few-shot evaluation without further training, no effect was observed.",
            "uuid": "e8404.9",
            "source_info": {
                "paper_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "BPE multi-digit number tokens (1-3 digit tokens)",
            "name_full": "Byte Pair Encoding tokenizer choice to have dedicated 1-, 2-, and 3-digit number tokens (as used in some OpenAI tokenizers)",
            "brief_description": "A tokenizer design choice where many small integer strings (1–3 digits) are represented by single tokens; this is contrasted with single-digit tokenization and with pure BPE learned tokens.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "OpenAI tokenizers / studied models",
            "model_description": "Tokenizer-level design (vocabulary) that yields number tokens covering up to 3-digit strings; authors note this likely motivated by compression trade-offs.",
            "arithmetic_task_type": "Relevant because it determines how multi-digit numbers are represented as tokens for arithmetic reasoning.",
            "mechanism_or_representation": "Multi-digit number tokens create coarse-grained tokenization where tokens correspond to 1–3 digit chunks; this representation interacts with autoregressive prediction and can create token-boundary alignment or misalignment between inputs and outputs.",
            "probing_or_intervention_method": "Indirect: authors manipulate text formatting to change tokenizer segmentations and observe behavioral consequences; they also visualize tokenizer coverage of 3-digit strings in appendices.",
            "performance_metrics": "Not a performance metric per se, but the presence of 1–3 digit tokens correlates with the observed token-boundary error modes and digit-4 failure under L2R.",
            "error_types_or_failure_modes": "Using up-to-3-digit tokens creates token boundaries every 1–3 digits; misalignment between input and output token boundaries (especially in L2R) appears to underlie catastrophic length-mismatch errors.",
            "evidence_for_mechanism": "Behavioral sensitivity to changing the grouping direction (L2R vs R2L) implies the model's computation depends on how digits are grouped into tokens. Authors also present tokenizer diagnostics (appendix) showing which 3-digit strings have dedicated tokens.",
            "counterexamples_or_challenges": "Single-digit tokenizers (used in other models like PaLM, LLaMA) would not suffer the same alignment issues but trade off different compression/efficiency properties; the paper calls for 'gold experiments' training identical models with different tokenizers to fully isolate effects.",
            "uuid": "e8404.10",
            "source_info": {
                "paper_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Mechanism hypothesis: token-alignment algorithmic reasoning",
            "name_full": "Hypothesis that models learn token-aligned digit-by-digit addition algorithms which rely on alignment of least-significant-digit tokens between inputs and outputs",
            "brief_description": "Paper-level mechanistic hypothesis: LLMs may implement an algorithmic-like procedure working on tokens representing digit chunks, and misalignment between input and output token boundaries (L2R) breaks that learned procedure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5 and gpt-4 (as studied)",
            "model_description": "A proposed interpretive model of how arithmetic is implemented in autoregressive LLMs trained with BPE numeric tokens.",
            "arithmetic_task_type": "Multi-digit addition; especially cases where final carry changes output tokenization length.",
            "mechanism_or_representation": "Models appear to compute on token-level digit chunks (e.g., 3-digit tokens). If the tokens are aligned so that least-significant digits map to the same token indices between inputs and outputs (R2L), the model can implement an effective carry-propagating algorithm; L2R breaks this alignment and causes systematic token-level failure.",
            "probing_or_intervention_method": "Inferred from behavioral manipulations (tokenization direction changes, prompt-based conversion) and diagnostic probes (logprob/top-5, entropy, error-pattern counts).",
            "performance_metrics": "Explains why R2L formatting yields dramatic accuracy gains, and why L2R length-mismatch causes near-total failure in many cases (e.g., 8.25% accuracy).",
            "error_types_or_failure_modes": "Predicts localized token-boundary failures, digit-position specific systematic mistakes (digit-4), and the relative robustness to number of carries (paper found no correlation between number of carries and difficulty).",
            "evidence_for_mechanism": "Strong behavioral causality: simply changing tokenization alignment via delimiters or by prompting conversion recovers performance; stereotyped and reproducible digit-position error patterns mirror signatures of algorithmic breakdown at token boundaries.",
            "counterexamples_or_challenges": "No direct internal activation-level mechanistic proof (e.g., neuron or attention patching) is provided; authors reference prior work (Stolfo et al.) for such causal mechanistic methods and call for future internal interpretability work. Also larger models can partly overcome these inductive biases, showing the hypothesis is not the sole determinant of arithmetic competence.",
            "uuid": "e8404.11",
            "source_info": {
                "paper_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "What algorithms can transformers learn? a study in length generalization.",
            "rating": 2
        },
        {
            "paper_title": "Teaching algorithmic reasoning via in-context learning.",
            "rating": 2
        },
        {
            "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis.",
            "rating": 2
        },
        {
            "paper_title": "Impact of pretraining term frequencies on few-shot numerical reasoning.",
            "rating": 2
        },
        {
            "paper_title": "Exploring length generalization in large language models.",
            "rating": 1
        }
    ],
    "cost": 0.02338475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs</h1>
<p>Aaditya K. Singh ${ }^{1}$ DJ Strouse ${ }^{2}$</p>
<h4>Abstract</h4>
<p>Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases. Historically, LLMs have relied on byte pair encoding, without care to specific input domains. With the increased use of LLMs for reasoning, various number-specific tokenization schemes have been adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers. In this work, we study the effect this choice has on numerical reasoning through the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization for GPT-3.5 and -4 , finding that right-to-left tokenization (enforced by comma separating numbers at inference time) leads to largely improved performance. Furthermore, we find that model errors when using standard left-to-right tokenization follow stereotyped error patterns, suggesting that model computations are systematic rather than approximate. We show that the model is able to convert between tokenizations easily, thus allowing chain-of-thought-inspired approaches to recover performance on left-to-right tokenized inputs. We also find the gap between tokenization directions decreases when models are scaled, possibly indicating that larger models are better able to override this tokenization-dependent inductive bias. In summary, our work performs the first study of how number tokenization choices lead to differences in model performance on arithmetic tasks, accompanied by a thorough analysis of error patterns. We hope this work inspires practitioners to more carefully ablate number tokenization-related choices when working towards general models of numerical reasoning.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Illustrating the dependence of frontier model arithmetic performance on tokenization. We show how using commas can enforce right-to-left (R2L) tokenization for the same addition problem. R2L tokenization leads to improved model performance on both GPT-3.5 and GPT-4 (March 2023 models), which we show is due to tokenization alignment between addends and answer through various controls and error analyses.</p>
<h2>1. Introduction</h2>
<p>Large language models (LLMs) are often lauded as demonstrating the benefits of end-to-end learning over inductive biases. However, an often overlooked part of the pipeline, preventing it from being end-to-end, is tokenization: the segmenting of an input sequence of bytes into discrete tokens. Tokenization consists of two halves: training, in which a vocabulary of tokens and statistics are learned over a given corpus, and segmenting, where a function uses the trained vocabulary and statistics to map sequences of bytes to tokens. Each tokenization scheme may impart different inductive biases on the model due to the way in which bytes of input sequences are grouped - in this work, we study these tokenization-dependent effects on numerical reasoning in state-of-the art models (GPT-3.5, GPT-4) by considering the tokenization of numbers in arithmetic problems.</p>
<p>Though many techniques have been proposed for tokenization, the prevailing methods in today's frontier models are variants of Byte Pair Encoding (BPE) (Gage, 1994; Sennrich et al., 2016). BPE is a statistical approach to tokenization that is learnt from a dataset of, in the case of LLMs, text. Intuitively, BPE compresses the dataset by iteratively creating tokens for the most commonly occurring subsequences. Specifically, BPE begins with a token vocabulary consisting of each character in the text (e.g. letters, num-</p>
<table>
<thead>
<tr>
<th>000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 1. Popular LLMs and their number tokenization strategies. $\mathrm{BPE}=$ byte pair encoding. $\mathrm{L} 2 \mathrm{R}=$ left-to-right.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Strategy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3 (2020)</td>
<td style="text-align: left;">pure BPE</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 (2022)</td>
<td style="text-align: left;">L2R chunks of 3 digits</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (2023)</td>
<td style="text-align: left;">L2R chunks of 3 digits</td>
</tr>
<tr>
<td style="text-align: left;">Claude v2.1 (2023)</td>
<td style="text-align: left;">pure BPE</td>
</tr>
<tr>
<td style="text-align: left;">Gopher (2021)</td>
<td style="text-align: left;">pure BPE</td>
</tr>
<tr>
<td style="text-align: left;">Chinchilla (2022)</td>
<td style="text-align: left;">pure BPE</td>
</tr>
<tr>
<td style="text-align: left;">PaLM (2022)</td>
<td style="text-align: left;">single digit</td>
</tr>
<tr>
<td style="text-align: left;">GPT-J (2021)</td>
<td style="text-align: left;">pure BPE</td>
</tr>
<tr>
<td style="text-align: left;">Llama 1 \&amp; 2 (2023)</td>
<td style="text-align: left;">single digit</td>
</tr>
<tr>
<td style="text-align: left;">Mistral (2023)</td>
<td style="text-align: left;">single digit</td>
</tr>
<tr>
<td style="text-align: left;">OLMo (2024)</td>
<td style="text-align: left;">pure BPE</td>
</tr>
</tbody>
</table>
<p>We vary the tokenization direction to be the default left-to-right (L2R) or right-to-left (R2L). We find that model accuracy is up to $20 \%$ higher when using R2L tokenization (Figure 1, Section 3). We then provide a thorough analysis of error patterns across these two tokenizations (Section 4). We find that the difference in performance between R2L and L2R tokenization in GPT-3.5 can largely be explained by an extremely stereotyped and surprising error pattern (Section 4.3), perhaps indicating the presence of some systematic, but flawed, reasoning. Next, we show that chain-of-thought-inspired approaches, where a model is asked to repeat an input in R2L tokenization, recover the accuracy otherwise lost due to L2R tokenization (Section 5). Finally, we conclude by studying how these effects may change with model version, finding that larger models are better able to override the tokenization-induced effects but, as of yet, unable to eliminate them (Section 6). Overall, we view these results as compelling evidence towards significant tokenization-dependent inductive biases in large language models, and hope they lead model practitioners to conduct careful pre-training ablations with varying tokenization schemes, especially for numerical reasoning.</p>
<h2>2. Methods</h2>
<h3>2.1. Experiment setup</h3>
<p>We evaluate GPT models through the Chat Completions endpoint on the OpenAI API ${ }^{5}$ on few-shot addition problems. We control for addend digit length, ranging from 7 to 9 digits (chosen since this way each addend is 3 tokens long). For most experiments, we use 90 random problems, with 10 problems for each addend digit length pair (e.g., 10 problems where the addends are both 7 digits long, 10 problems where the first addend is 7 digits and the second is 8 , etc.). For shots, we consider 1-, 2-, 4-, and 8-shots. Shots are sampled randomly for each "query" problem, and are</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>provided to the model as a multi-turn dialogue. We control shots to have the same form (digit lengths, tokenization direction, etc.) as the query problem. We use the default system prompt "You are a helpful assistant." for maximum reproducibility. ${ }^{6}$ Python code for some example 2-shot queries to the model are presented in Appendix E. 3 for maximum clarity. We use greedy decoding (temperature $=0$ ) in all experiments. Accuracy was computed by extracting numbers from model responses.</p>
<p>Most experiments in the paper are run using the gpt-3.5-turbo-0301 model checkpoint, though in Section 6 we look into how results extend to newer versions of the same model (gpt-3.5-turbo-0613) and to the, presumably larger, GPT-4 models (gpt-4-0314, gpt-4-0613). All code and full results tables can be found at https://github.com/aadityasingh/ TokenizationCounts.</p>
<h3>2.2. Varying L2R vs. R2L tokenization</h3>
<p>The ChatCompletion API only allows for input text, not input tokens, so it's tricky to conduct tokenization-varying experiments. To force the model to use R2L tokenization for numbers, we add commas every 3-digits from the right (see Figure 1). Since the tokenizer doesn't contain any tokens with numbers and commas, the commas get tokenized separately, effectively enforcing a different segmentation of digits. We use this setting to illustrate our main results, and conduct various controls to ensure that our observed effect is due to tokenization as opposed to other confounds.</p>
<h2>3. Right-to-left tokenization improves model performance</h2>
<h3>3.1. Main results</h3>
<p>When using commas to separate digits and enforce R2L tokenization, we observed greatly improved average performance (8-shot result in Figure 1). We found that increasing the number of shots (Figure 4) led to a larger increase for the L2R tokenization (from $68.5 \% 1$-shot to $75.6 \% 8$-shot) than for the R2L tokenization (from $95.6 \% 1$-shot to $97.8 \%$ 8 -shot) indicating that in-context learning may slightly mitigate the (harmful) bias of L2R tokenization. Given this finding and the plateau-ing in performance with increasing shots, we report only 8 -shot results for the remainder of the work as this makes L2R tokenization the most competitive.</p>
<h3>3.2. Controlling for comma-based semantic priors</h3>
<p>Though this result is already compelling, we realize that commas are often used to separate digits in the manner</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4. Effect of R2L vs L2R tokenization with increasing shots.
depicted in Figure 1, so the observed effect may be confounded by prevalence in training data (McCoy et al., 2023). One might argue that comma separation is actually bringing the input closer to the training distribution of the model, so it's not a surprise that models perform better. To control for this and focus in on tokenization, we consider alternate, single-token separators: ', '.', ' $\$ ', '# ' (note we'll refer to ' ' as <space> for clarity). For example, the number 8302080 would be written as $8 # 302 # 080$ when input to the model.</p>
<p>Results are shown in Figure 5. We find that the model is largely agnostic to the separator used, indicating that tokenization is likely the dominant effect, rather than the specific choice of using commas.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5. 8-shot accuracy when using different delimiters for R2L tokenization. Dotted lines show results from Figure 1 for comparison. Overall, we see choice of delimiter matters less than direction of tokenization.</p>
<h3>3.3. Controlling for "thinking tokens"</h3>
<p>Another confound with the above experiment may be that adding commas both increases the number of tokens input to as well as generated by the model. Thus, to generate the same answer, the model has access to more computation steps (i.e., FLOPs). There is a worry that models may use these repetitive thinking tokens to perform additional useful computations (Lanham et al., 2023). In practice, this seems not to happen without further training (Goyal et al., 2024), but we conducted experiments to verify this in our setting.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6. 8-shot accuracy for various "thinking token" controls. Dotted lines show results from Figure 1 for comparison. We also experimented with other delimiters for L2R tokenization (all those from Figure 5), but found similarly poor results. Overall, "thinking tokens" do not recover the performance boost from using commaenforced R2L tokenization.</p>
<p>To control for thinking tokens, we consider two types of controls. In the first, we use separators to enforce L2R tokenization - this enforces an exact match in prompt token counts. Second, we consider adding 1 or 2 spaces before and after the + and $=$ sign to increase the number of tokens ${ }^{7}$ in the L2R case (where no separator is used). Both of these have the benefit of adding extremely "predictable" tokens (when using 8 -shots), allowing the model to possibly use the extra computation steps for "thinking".</p>
<p>In Figure 6, we find that neither of these controls, when applied to L2R tokenized sequences, recovers the performance of R2L tokenization. In fact, we found that using separators with the L2R tokenization often hurt performance, likely because this is an uncommon representation-upon qualitative inspection of a few examples, we found the model sometimes "auto-corrects" the inputs by hallucinating trailing zeros. We believe these experiments effectively rule out the "thinking token" confound.</p>
<h2>4. Error analysis reveals stereotyped patterns</h2>
<p>Given the robust effect observed in Section 3, we were curious to see if there were any patterns in the errors. Below, we summarize our key findings.</p>
<h3>4.1. L2R tokenization is significantly worse when answer is longer than addends</h3>
<p>As noted in Section 2.1, we balanced our dataset of problems based on input digit length. Upon inspection of problems the model got incorrect when using L2R tokenization, we noted that errors seemed more likely when the answer was longer than the addends (e.g., a problem where 7 digit number +7 digit number $=8$ digit number, of the form depicted</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7. When the answer is the same length in digits as an addend (length match), both tokenization schemes perform similarly (left). When the answer is a different length in digits than either addend (length mismatch), L2R tokenization destroys model performance, dropping to $8.25 \%$ (right).
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8. Accuracy as a function of number of carries. For L2R tokenization, we exclude problems where the answer length does not match at least one addend, as the model misses most of those ( $92 \%$ ) as shown in Section 4.1. If number of carries (a human notion of difficulty) was correlated to model performance, we would expect a negative slope. The lack of any trend suggests model performance is largely independent of number of carries.
in Figure 1). To test this hypothesis, we conducted a new experiment where we controlled for addend lengths and answer lengths. Specifically, we generated 100 random problems for each possible triplet of digit lengths where addends and answer have a length of 7 to 9 digits (full list in Appendix E.1). The remainder of our experiments in this section will use this expanded set of problems to show the robustness of the found error patterns.</p>
<p>We reproduced our main phenomenon (Section 3.1), and further affirmed our intuitions about error patterns. As shown in Figure 7, we find that L2R tokenization has similar performance to R2L tokenization when the answer's length in digits is the same as one of the inputs (which we refer to as the "length match" condition). When the answer is longer than the inputs (due to a final carry), L2R tokenization is significantly worse, with accuracy dropping down to $8.25 \%$ we refer to this as the "length mismatch" condition. We suspect that this strong effect may be due to the misalignment between input and output tokenizations (as illustrated in</p>
<p>Figure 1) rather than some carry-related notion of problem difficulty, which we explore in the next few subsections.</p>
<h3>4.2. Errors do not seem correlated to number of carries</h3>
<p>A natural hypothesis given the above result may be that errors might just be correlated to some notion of difficulty, such as carries. In Figure 8, we find that this is generally not the case. Specifically, we consider the accuracy on subsets of problems based on how many carries are needed to solve them. ${ }^{8}$ The lack of a clear positive or negative trend indicates that model performance is not strongly affected by the number of carries.</p>
<h3>4.3. Length mismatch problems yield stereotyped "digit 4" error pattern</h3>
<p>If not carries, what could be causing the surprising error pattern in Figure 7? In Figure 9a, we find that the errors when using L2R tokenization are extremely stereotyped and not at all intuitive. Specifically, in the length mismatch condition, the model always gets the fourth digit wrong. Furthermore, the model always gets the first 3 digits correct (corresponding to the first output token). In terms of how far off the model is on digit 4, Figure 9b shows that there's a slight preference to off-by-one errors, but overall the specific substitution appears quite haphazard.</p>
<p>We found this result extremely surprising. In cognitive science, such stereotyped error patterns are often used as evidence of underlying systematic processing. While the mechanism for addition in LLMs remains unclear, we find this striking, tokenization-dependent error pattern ${ }^{9}$ as highly suggestive of some underlying algorithm (in contrast to suggestions that LLMs may be performing arithmetic using some "fuzzy" matching to similar problems in training). We provide further evidence of stereotyped error patterns by analyzing model log probabilities in Appendix D.</p>
<h3>4.4. Off-by-one errors at token boundaries account for nearly all remaining errors</h3>
<p>After accounting for the main source of error, we analyzed the remaining errors across both tokenization methods: 25 out of the 1300 problems for R2L tokenization, and 56 out of the 900 problems in the length match condition for L2R tokenization.</p>
<p>For R2L tokenization, of the 25 problems missed, 24 are due to off-by-one (either above or below) errors. For L2R tokenization, of the 56 problems missed, 53 are due to off-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 9. a) Error patterns for L2R tokenization over problems where the answer digit length is different than the addend lengths. "None" indicates the problems in this case that the model got correct (8.25\%). "Other" indicates problems where the model doesn't provide a valid answer or provides an answer of the wrong length ( $0.5 \%$ ). Of the remaining $91.25 \%$ which the model gets incorrect, it shockingly always gets digit 4 wrong. In addition, it sometimes gets other digits ( 5,6 or 7 ) wrong. b) For the errors in digit 4, we show the magnitude of the mistake. For example, if the correct value of digit 4 is 2 and the model response has digit 4 equal to 5 , it would be off by 3 . We see a slight preference to off-by-1 errors, but error magnitudes are fairly evenly distributed.
by-one (either above or below) errors. For nearly all these off-by-one errors, ${ }^{10}$ regardless of tokenization direction, we find that the error itself occurs in the last digit of an output token. This result suggests that off-by-one errors are more likely across token boundaries as opposed to in the middle of a 3-digit token. This hypothesis, with preliminary evidence, connects to works on length generalization (Anil et al., 2022) - using 3-digit tokens may make length generalization easier as models only need to cross token boundaries every third digit (as opposed to every digit).</p>
<h2>5. Models are able to convert from L2R to R2L tokenization, improving performance</h2>
<h3>5.1. Main results</h3>
<p>With the above results showing that number tokenization can strongly affect numerical reasoning, we ask if mod-</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>els can be prompted to take problems in a less preferred tokenization (L2R) and convert them to a more preferred tokenization (R2L) to improve performance. Inspired by chain-of-thought approaches (Nye et al., 2021; Kojima et al., 2022; Wei et al., 2022), we few-shot prompt models to take problems with one tokenization direction, and then repeat the problem and answer it using a different tokenization direction. In Figure 10, we find that models indeed perform nearly as well at addition when converting L2R tokenization to R2L themselves as to when they receive the problem in R2L tokenization in the first place. Performance increases with the number of shots when converting L2R to R2L since the model adheres more to the (helpful) suggested repetition style. These results indicate that models can convert between tokenizations to solve problems correctly, but do not do so implicitly in the forward pass.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 10. Few-shot accuracy when models receive a problem with one tokenization direction, then repeat and answer it in another.</p>
<h3>5.2. Controlling for output tokenization</h3>
<p>One confound with the above experiment may just be that the model improves when it's asked to generate answers with R2L tokenization. To control for this, we conduct a similar experiment, but without few-shot prompting the model to repeat the problem: the few-shot prompt provides answers with a different tokenization direction than the input, incentivizing the model to answer with this tokenization direction (see Appendix E. 3 for an example prompt). In Figure 11, we see that just answering with R2L tokenization does not improve performance (purple curve) to the degree that repeating in R2L tokenization does (purple curve, Figure 10), when starting from L2R tokenization. This effect indicates that it is important for the model to also see the problem in the preferred tokenization (by repeating it), rather than just answering in the preferred tokenization.</p>
<h2>6. Tokenization-dependent effects mostly extend to future models</h2>
<p>Through the previous sections, we've demonstrated a strong tokenization-dependent effect. In this section, we address the question: does this effect extend to newer models?</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 11. Few-shot accuracy when models receive a problem in one tokenization format, and answer it in another. The distinction between this and Figure 10 is that models do not repeat the problem in this case. We note that when giving a model a problem in R2L tokenization and prompting it to answer in L2R tokenization, the model actually gets worse with more shots, since for fewer shots, the model ends up ignoring the few-shot prompt and answers in its preferred R2L tokenization. Specifically, adherence to the prompted formatting for R2L→L2R increases from just 13.3% with 1 shot to 98.9% with 8 shots.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 12. 8-shot performance of various OpenAI models on the same addition problems as Figure 4. The newer version of GPT-3.5 appears to perform equally poorly. For GPT-4, we see a large tokenization-dependent effect in the March model, which becomes weaker (but still present) in the June model. The GPT-4 turbo model shows a slight regression in overall performance with the tokenization-dependent effect becoming stronger again.</p>
<p>As shown in Figure 12, we find that generally, yes: tokenization-dependent effects persist. We consider five "held-out" OpenAI models, which allow us to consider how tokenization-dependent effects shift when models are updated (gpt-3.5-turbo-0613, gpt-3.5-turbo-1106) or scaled up (gpt-4-0314, gpt-4-0613) and then scaled back down (gpt-4-1106-preview, which is a "turbo" model). Later versions of GPT-3.5 exhibit as strong an effect due to tokenization direction. The effect is mitigated slightly in GPT-4's March version, and mitigated strongly in GPT-4's most recent version. Specifically, GPT-4 models appear</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 13. 8-shot performance of various OpenAI models on answer length controlled problems (see Section 4.1), separated by whether the answer length is the same as one of the addends. We see the effect from Section 4.1 reproduces strongly in the newer version of GPT-3.5. The effect is still present in GPT-4, but not as strongly. Interestingly, the effect is stronger in the latest GPT-4 Turbo model as compared to GPT-4.</p>
<p>to be better at performing arithmetic across the board (for both tokenization directions). Interestingly, in the most recent GPT-4 Turbo model, the effect of tokenization becomes stronger again. Furthermore, Figure 13 shows that the digit length mismatch between answer and addends is again the main reason for the performance drop when using L2R tokenization, in both GPT-3.5 and GPT-4 models. We believe that the increased scale of training GPT-4 (likely in both parameter count and data seen) allows it to better override the tokenization-induced inductive bias that leads GPT-3.5 models to perform worse (analogous to scale helping mitigate tokenization-induced spelling difficulties (Liu et al., 2023)). The resurgence of tokenization-dependent effects in the newest GPT-4 Turbo model (which is presumably smaller than GPT-4) supports this hypothesis.</p>
<h2>7. Related work</h2>
<p><strong>Tokenization methods</strong> The two leading tokenization methods are Unigram (Kudo, 2018) and BPE (Sennrich et al., 2016). While older work in NLP show the benefits of Unigram over BPE (Bostrom &amp; Durrett, 2020), BPE remains the most commonly used tokenization method by modern LLM practitioners. Within BPE, different models often make different hard-coded choices, such as removing long tokens of consecutive whitespace (Touvron et al., 2023a) or enforcing single-digit tokenization of numbers (Chowdhery et al., 2023). Our work demonstrates tokenization-dependent effects from one such choice, the use of 1-, 2-, and 3-digit tokens by OpenAI models (OpenAI et al., 2023). One way around such issues could be tokenizer-free methods (e.g., MEGABYTE (Yu et al., 2023), which uses patch-based schemes and doesn't assume fixed tokens), but we suspect these schemes will also carry their own inductive biases. Golkar et al. (2023) also introduce a continuous number encoding scheme meant to circumvent tokenization artifacts, but their approach is limited to cases where model outputs are purely numerical, and not interleaved with text.</p>
<p><sup>11</sup>We assume this is a smaller, maybe distilled, version of GPT-4.</p>
<p><sup>12</sup>We find it interesting that the March to June update to GPT-4 improved performance, but the corresponding update to GPT-3.5 did not – without knowing what these updates entail, however, it's hard to draw conclusions as to why this may be the case.</p>
<p>Tokenization artifacts in LLMs A growing set of results has emerged around various tokenization-related artifacts in LLMs. Similar to scratchpad prompting (Nye et al., 2021), Wei (2023) found that separating letters into individual tokens can help in sorting words by the second letter. Other work (Shin et al., 2020) has focused on specific tokens that can negatively affect model performance. Rumbelow \&amp; mwatkins (2023) found many tokens which were artifacts of the data used to pre-train the tokenizer, but presumably weren't present in the model's training data, leading to highly unpredictable (and often comical) completions. Sun et al. (2023) find artifacts due to mismatches in tokenization in extractive Q\&amp;A tasks, which may have connection to some of our experiments in Section 5.2. Lundberg (2023) propose token healing to avoid many tokenization-related issues by removing the last few tokens from a prompt and allowing the model to complete them; this approach has connections to work on asking models to rephrase-and-respond (Deng et al., 2023) and our experiments on prompting the model to repeat with its preferred tokenization direction (Section 5). Our work builds on these past scattered artifacts and provides a systematic analysis of tokenization-directiondependent effects on numerical reasoning in frontier LLMs.</p>
<h2>Arithmetic tasks as a testbed for numerical reasoning</h2>
<p>in LLMs With the increased interest in measuring frontier models on math reasoning (Saxton et al., 2019; Cobbe et al., 2021; Lewkowycz et al., 2022; Hendrycks et al., 2021; Paster, 2023), an accompanying body of work studies language models in more controlled settings, such as arithmetic. Razeghi et al. (2022) use arithmetic tasks to show that pretraining term frequencies ${ }^{15}$ can affect numerical reasoning in GPT-J models (Wang \&amp; Komatsuzaki, 2021) trained on the Pile dataset (Gao et al., 2020). Similarly, McCoy et al. (2023) showed that GPT-3.5 and GPT-4 are better at computing linear functions that are more common in training data (such as the Fahrenheit to Celsius conversion) than close alternatives. Other work (Nogueira et al., 2021; Muffo et al., 2022; Zhou et al., 2022; 2023) instead focuses on various modifications that can help models generalize to longer arithmetic tasks. Zhou et al. (2023) and Lee et al. (2023) both point out that having autoregressive models perform addition in reversed order yields a simpler algorithm to learn and results in better performance, which is complementary to our emphasis on the importance of "reversed" (i.e. right-toleft) tokenization alignment. Zhou et al. (2022) also conduct preliminary error analyses of model mistakes, though their algorithmic prompts force models to split tokens into digits (similar to Nye et al. (2021)). Our work broadly lies in this</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>category of work using arithmetic tasks to study numerical reasoning; we chose to focus on tokenization-dependent effects, and found surprisingly consistent, stereotyped error patterns (Section 4), adding to this rich body of literature.</p>
<h2>8. Discussion</h2>
<p>In this work, we analyze tokenization-dependent effects on numerical reasoning in GPT-3.5 and GPT-4. We found that the hard-coded choice of 1-, 2- and 3- digit number tokens, tokenized left-to-right, gives rise to stereotyped error patterns when compared to right-to-left tokenization. We proposed a mitigation, where the model is asked to repeat the answer in its preferred tokenization format. Finally, we showed that the effect is stronger in smaller models (such as GPT-3.5), emphasizing the significance of tokenizationdependent inductive biases in an era where many practitioners are focusing on packing capabilities into smaller models through overtraining (De Vries, 2023; Touvron et al., 2023a) and distillation (Li et al., 2023; Gemini Team et al., 2023). Overall, we believe this evidence strongly suggests inductive biases from tokenization can significantly influence model performance on numerical reasoning tasks.</p>
<p>Modern frontier LLMs mostly use single-digit tokens (Table 1), with GPT-3.5 and GPT-4 being a key exception in their use of up-to-3-digit tokens. We hypothesize that the latter choice may have been made to achieve a better compression rate: models "see" more numerical data for the same number of training tokens. ${ }^{16}$ Furthermore, this choice could have benefits for length generalization (Anil et al., 2022), as we allude to in Section 4.4. However, we've also demonstrated how the misalignment between inputs and outputs when using L2R tokenization (Section 4.1) can lead to large drops in accuracy, especially on smaller models (GPT-3.5, GPT-4 Turbo). Such misalignment would not be an issue when using single-digit tokens.</p>
<p>To make progress on which number tokenization choices are best to use (e.g., the single-digit tokens of LLaMa and PaLM, or the up-to-3 digit tokens of GPT-3.5 and GPT-4), the "gold experiment" would be to train the same model architecture on the same dataset, but with varying number tokenization strategies. Beyond the expense of this experiment (making it intractable in academic settings), a key question also becomes how to "compute"-control. The better compression ratio of up-to-3 digit tokens means a token-controlled experiment would result in some models "seeing" more data. We hope our work leads model practitioners to consider such ablations, with proper controls.</p>
<p>Beyond applicability to model practitioners, our work also</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>provides an interesting set of tokenizaiton-dependent phenomenon for interpretability researchers to explore. Prior work (Stolfo et al., 2023) has used techniques such as path patching to identify sub-circuits in LLMs that perform arithmetic tasks, but restricted to single token operands. Building off our results, it would be interesting to elucidate the mechanisms behind systematic error patterns, especially in the case of multi-token operands. The robustness of the "digit 4 " error on GPT-3.5 points to some systematic mechanism, which could shed light on underlying algorithms that emerge to perform arithmetic tasks.</p>
<h2>Acknowledgements</h2>
<p>The authors would like to acknowledge Andrew Saxe, Ted Moskovitz, Kira Düsterwald, Felix Hill, Xavier Garcia, Dan Roberts, and William Held for insightful discussions and feedback on the draft. A.K.S. is funded by the Gatsby Charitable Foundation.</p>
<h2>References</h2>
<p>Anil, C., Wu, Y., Andreassen, A. J., Lewkowycz, A., Misra, V., Ramasesh, V. V., Slone, A., Gur-Ari, G., Dyer, E., and Neyshabur, B. Exploring length generalization in large language models. Neural Information Processing Systems (NeurIPS), 2022. URL https://openreview.net/ forum?id=zSkYVeX7bC4.</p>
<p>Bostrom, K. and Durrett, G. Byte pair encoding is suboptimal for language model pretraining. Empirical Methods in Natural Language Processing (EMNLP), 2020. URL https://aclanthology.org/ 2020.findings-emnlp. 414.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. Neural Information Processing Systems (NeurIPS), 2020. URL https://proceedings.neurips.cc/ paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64aPaper.pdf.</p>
<p>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari,
G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. PaLM: Scaling language modeling with pathways. Journal of Machine Learning Research, 2023. URL http: //jmlr.org/papers/v24/22-1144.html.</p>
<p>Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv:2110.14168, 2021. URL https: //arxiv.org/abs/2110.14168.</p>
<p>De Vries, H. Go smol or go home, 2023. URL https://www.harmdevries.com/post/ model-size-vs-compute-overhead/.</p>
<p>Deng, Y., Zhang, W., Chen, Z., and Gu, Q. Rephrase and respond: Let large language models ask better questions for themselves. arXiv:2311.04205, 2023. URL https: //arxiv.org/abs/2311.04205.</p>
<p>Gage, P. A new algorithm for data compression. C Users Journal, 1994.</p>
<p>Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al. The Pile: An 800gb dataset of diverse text for language modeling. arXiv:2101.00027, 2020. URL https://arxiv.org/abs/2101.00027.</p>
<p>Gemini Team, Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., Silver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., Lillicrap, T., Lazaridou, A., Firat, O., Molloy, J., Isard, M., Barham, P. R., Hennigan, T., Lee, B., Viola, F., Reynolds, M., Xu, Y., Doherty, R., Collins, E., Meyer, C., Rutherford, E., Moreira, E., Ayoub, K., Goel, M., Tucker, G., Piqueras, E., Krikun, M., Barr, I., Savinov, N., Danihelka, I., Roelofs, B., White, A., Andreassen, A., von Glehn, T., Yagati, L., Kazemi, M., Gonzalez, L., Khalman, M., Sygnowski, J., Frechette, A., Smith, C., Culp, L., Proleev, L., Luan, Y., Chen, X., Lottes, J., Schucher, N., Lebron, F., Brustemi, A., Clay, N., Crone, P., Kocisky, T., Zhao, J., Perz, B., Yu, D., Howard, H., Bloniarz, A., Rae, J. W., Lu, H., Sifre, L., Maggioni, M., Alcober, F., Garrette, D., Barnes, M., Thakoor, S., Austin, J., BarthMaron, G., Wong, W., Joshi, R., Chaabouni, R., Fatiha, D., Ahuja, A., Liu, R., Li, Y., Cogan, S., Chen, J., Jia, C.,</p>
<p>Gu, C., Zhang, Q., Grimstad, J., Hartman, A. J., Chadwick, M., Tomar, G. S., Garcia, X., Senter, E., Taropa, E., Pillai, T. S., Devlin, J., Laskin, M., de Las Casas, D., Valter, D., Tao, C., Blanco, L., Badia, A. P., Reitter, D., Chen, M., Brennan, J., Rivera, C., Brin, S., Iqbal, S., Surita, G., Labanowski, J., Rao, A., Winkler, S., Parisotto, E., Gu, Y., Olszewska, K., Zhang, Y., Addanki, R., Miech, A., Louis, A., Shafey, L. E., Teplyashin, D., Brown, G., Catt, E., Attaluri, N., Balaguer, J., Xiang, J., Wang, P., Ashwood, Z., Briukhov, A., Webson, A., Ganapathy, S., Sanghavi, S., Kannan, A., Chang, M.-W., Stjerngren, A., Djolonga, J., Sun, Y., Bapna, A., Aitchison, M., Pejman, P., Michalewski, H., Yu, T., Wang, C., Love, J., Ahn, J., Bloxwich, D., Han, K., Humphreys, P., Sellam, T., Bradbury, J., Godbole, V., Samangooei, S., Damoc, B., Kaskasoli, A., Arnold, S. M. R., Vasudevan, V., Agrawal, S., Riesa, J., Lepikhin, D., Tanburn, R., Srinivasan, S., Lim, H., Hodkinson, S., Shyam, P., Ferret, J., Hand, S., Garg, A., Paine, T. L., Li, J., Li, Y., Giang, M., Neitz, A., Abbas, Z., York, S., Reid, M., Cole, E., Chowdhery, A., Das, D., Rogozińska, D., Nikolaev, V., Sprechmann, P., Nado, Z., Zilka, L., Prost, F., He, L., Monteiro, M., Mishra, G., Welty, C., Newlan, J., Jia, D., Allamanis, M., Hu, C. H., de Liedekerke, R., Gilmer, J., Saroufim, C., Rijhwani, S., Hou, S., Shrivastava, D., Baddepudi, A., Goldin, A., Ozturel, A., Cassirer, A., Xu, Y., Sohn, D., Sachan, D., Amplayo, R. K., Swanson, C., Petrova, D., Narayan, S., Guez, A., Brahma, S., Landon, J., Patel, M., Zhao, R., Villela, K., Wang, L., Jia, W., Rahtz, M., Giménez, M., Yeung, L., Lin, H., Keeling, J., Georgiev, P., Mincu, D., Wu, B., Haykal, S., Saputro, R., Vodrahalli, K., Qin, J., Cankara, Z., Sharma, A., Fernando, N., Hawkins, W., Neyshabur, B., Kim, S., Hutter, A., Agrawal, P., Castro-Ros, A., van den Driessche, G., Wang, T., Yang, F., yiin Chang, S., Komarek, P., McIlroy, R., Lučić, M., Zhang, G., Farhan, W., Sharman, M., Natsev, P., Michel, P., Cheng, Y., Bansal, Y., Qiao, S., Cao, K., Shakeri, S., Butterfield, C., Chung, J., Rubenstein, P. K., Agrawal, S., Mensch, A., Soparkar, K., Lenc, K., Chung, T., Pope, A., Maggiore, L., Kay, J., Jhakra, P., Wang, S., Maynez, J., Phuong, M., Tobin, T., Tacchetti, A., Trebacz, M., Robinson, K., Katariya, Y., Riedel, S., Bailey, P., Xiao, K., Ghelani, N., Aroyo, L., Slone, A., Houlsby, N., Xiong, X., Yang, Z., Gribovskaya, E., Adler, J., Wirth, M., Lee, L., Li, M., Kagohara, T., Pavagadhi, J., Bridgers, S., Bortsova, A., Ghemawat, S., Ahmed, Z., Liu, T., Powell, R., Bolina, V., Iinuma, M., Zablotskaia, P., Besley, J., Chung, D.-W., Dozat, T., Comanescu, R., Si, X., Greer, J., Su, G., Polacek, M., Kaufman, R. L., Tokumine, S., Hu, H., Buchatskaya, E., Miao, Y., Elhawaty, M., Siddhant, A., Tomasev, N., Xing, J., Greer, C., Miller, H., Ashraf, S., Roy, A., Zhang, Z., Ma, A., Filos, A., Besta, M., Blevins, R., Klimenko, T., Yeh, C.-K., Changpinyo, S., Mu, J., Chang, O., Pajarskas, M., Muir, C., Cohen, V.,</p>
<p>Lan, C. L., Haridasan, K., Marathe, A., Hansen, S., Douglas, S., Samuel, R., Wang, M., Austin, S., Lan, C., Jiang, J., Chiu, J., Lorenzo, J. A., Sjösund, L. L., Cevey, S., Gleicher, Z., Avrahami, T., Boral, A., Srinivasan, H., Selo, V., May, R., Aisopos, K., Hussenot, L., Soares, L. B., Baumli, K., Chang, M. B., Recasens, A., Caine, B., Pritzel, A., Pavetic, F., Pardo, F., Gergely, A., Frye, J., Ramasesh, V., Horgan, D., Badola, K., Kassner, N., Roy, S., Dyer, E., Campos, V., Tomala, A., Tang, Y., Badawy, D. E., White, E., Mustafa, B., Lang, O., Jindal, A., Vikram, S., Gong, Z., Caelles, S., Hemsley, R., Thornton, G., Feng, F., Stokowiec, W., Zheng, C., Thacker, P., Çağlar Ünlü, Zhang, Z., Saleh, M., Svensson, J., Bileschi, M., Patil, P., Anand, A., Ring, R., Tsihlas, K., Vezer, A., Selvi, M., Shevlane, T., Rodriguez, M., Kwiatkowski, T., Daruki, S., Rong, K., Dafoe, A., FitzGerald, N., Gu-Lemberg, K., Khan, M., Hendricks, L. A., Pellat, M., Feinberg, V., Cobon-Kerr, J., Sainath, T., Rauh, M., Hashemi, S. H., Ives, R., Hasson, Y., Li, Y., Noland, E., Cao, Y., Byrd, N., Hou, L., Wang, Q., Sottiaux, T., Paganini, M., Lespiau, J.-B., Moufarek, A., Hassan, S., Shivakumar, K., van Amersfoort, J., Mandhane, A., Joshi, P., Goyal, A., Tung, M., Brock, A., Sheahan, H., Misra, V., Li, C., Rakićević, N., Dehghani, M., Liu, F., Mittal, S., Oh, J., Noury, S., Sezener, E., Huot, F., Lamm, M., Cao, N. D., Chen, C., Elsayed, G., Chi, E., Mahdieh, M., Tenney, I., Hua, N., Petrychenko, I., Kane, P., Scandinaro, D., Jain, R., Uesato, J., Datta, R., Sadovsky, A., Bunyan, O., Rabiej, D., Wu, S., Zhang, J., Vasudevan, G., Leurent, E., Alnahlawi, M., Georgescu, I., Wei, N., Zheng, I., Chan, B., Rabinovitch, P. G., Stanczyk, P., Zhang, Y., Steiner, D., Naskar, S., Azzam, M., Johnson, M., Paszke, A., Chiu, C.-C., Elias, J. S., Mohiuddin, A., Muhammad, F., Miao, J., Lee, A., Vieillard, N., Potluri, S., Park, J., Davoodi, E., Zhang, J., Stanway, J., Garmon, D., Karmarkar, A., Dong, Z., Lee, J., Kumar, A., Zhou, L., Evens, J., Isaac, W., Chen, Z., Jia, J., Levskaya, A., Zhu, Z., Gorgolewski, C., Grabowski, P., Mao, Y., Magni, A., Yao, K., Snaider, J., Casagrande, N., Suganthan, P., Palmer, E., Irving, G., Loper, E., Faruqui, M., Arkatkar, I., Chen, N., Shafran, I., Fink, M., Castaño, A., Giannoumis, I., Kim, W., Rybiński, M., Sreevatsa, A., Prendki, J., Soergel, D., Goedeckemeyer, A., Gierke, W., Jafari, M., Gaba, M., Wiesner, J., Wright, D. G., Wei, Y., Vashisht, H., Kulizhskaya, Y., Hoover, J., Le, M., Li, L., Iwuanyanwu, C., Liu, L., Ramirez, K., Khorlin, A., Cui, A., LIN, T., Georgiev, M., Wu, M., Aguilar, R., Pallo, K., Chakladar, A., Repina, A., Wu, X., van der Weide, T., Ponnapalli, P., Kaplan, C., Simsa, J., Li, S., Dousse, O., Yang, F., Piper, J., Ie, N., Lui, M., Pasumarthi, R., Lintz, N., Vijayakumar, A., Thiet, L. N., Andor, D., Valenzuela, P., Paduraru, C., Peng, D., Lee, K., Zhang, S., Greene, S., Nguyen, D. D., Kurylowicz, P., Velury, S., Krause, S., Hardin, C., Dixon, L., Janzer, L., Choo, K., Feng, Z., Zhang, B., Singhal, A., Latkar, T., Zhang, M., Le, Q.,</p>
<p>Abellan, E. A., Du, D., McKinnon, D., Antropova, N., Bolukbasi, T., Keller, O., Reid, D., Finchelstein, D., Raad, M. A., Crocker, R., Hawkins, P., Dadashi, R., Gaffney, C., Lall, S., Franko, K., Filonov, E., Bulanova, A., Leblond, R., Yadav, V., Chung, S., Askham, H., Cobo, L. C., Xu, K., Fischer, F., Xu, J., Sorokin, C., Alberti, C., Lin, C.-C., Evans, C., Zhou, H., Dimitriev, A., Forbes, H., Banarse, D., Tung, Z., Liu, J., Omernick, M., Bishop, C., Kumar, C., Sterneck, R., Foley, R., Jain, R., Mishra, S., Xia, J., Bos, T., Cideron, G., Amid, E., Piccinno, F., Wang, X., Banzal, P., Gurita, P., Noga, H., Shah, P., Mankowitz, D. J., Polozov, A., Kushman, N., Krakovna, V., Brown, S., Bateni, M., Duan, D., Firoiu, V., Thotakuri, M., Natan, T., Mohananey, A., Geist, M., Mudgal, S., Girgin, S., Li, H., Ye, J., Roval, O., Tojo, R., Kwong, M., Lee-Thorp, J., Yew, C., Yuan, Q., Bagri, S., Sinopalnikov, D., Ramos, S., Mellor, J., Sharma, A., Severyn, A., Lai, J., Wu, K., Cheng, H.-T., Miller, D., Sonnerat, N., Vnukov, D., Greig, R., Beattie, J., Caveness, E., Bai, L., Eisenschlos, J., Korchemniy, A., Tsai, T., Jasarevic, M., Kong, W., Dao, P., Zheng, Z., Liu, F., Yang, F., Zhu, R., Geller, M., Teh, T. H., Sanmiya, J., Gladchenko, E., Trdin, N., Sozanschi, A., Toyama, D., Rosen, E., Tavakkol, S., Xue, L., Elkind, C., Woodman, O., Carpenter, J., Papamakarios, G., Kemp, R., Kafle, S., Grunina, T., Sinha, R., Talbert, A., Goyal, A., Wu, D., Owusu-Afriyie, D., Du, C., Thornton, C., Pont-Tuset, J., Narayana, P., Li, J., Fatehi, S., Wieting, J., Ajmeri, O., Uria, B., Zhu, T., Ko, Y., Knight, L., Héliou, A., Niu, N., Gu, S., Pang, C., Tran, D., Li, Y., Levine, N., Stolovich, A., Kalb, N., SantamariaFernandez, R., Goenka, S., Yustalim, W., Strudel, R., Elqursh, A., Lakshminarayanan, B., Deck, C., Upadhyay, S., Lee, H., Dusenberry, M., Li, Z., Wang, X., Levin, K., Hoffmann, R., Holtmann-Rice, D., Bachem, O., Yue, S., Arora, S., Malmi, E., Mirylenka, D., Tan, Q., Koh, C., Yeganeh, S. H., Pöder, S., Zheng, S., Pongetti, F., Tariq, M., Sun, Y., Ionita, L., Seyedhosseini, M., Tafti, P., Kotikalapudi, R., Liu, Z., Gulati, A., Liu, J., Ye, X., Chrzaszcz, B., Wang, L., Sethi, N., Li, T., Brown, B., Singh, S., Fan, W., Parisi, A., Stanton, J., Kuang, C., Koverkathu, V., Choquette-Choo, C. A., Li, Y., Lu, T., Ittycheriah, A., Shroff, P., Sun, P., Varadarajan, M., Bahargam, S., Willoughby, R., Gaddy, D., Dasgupta, I., Desjardins, G., Cornero, M., Robenek, B., Mittal, B., Albrecht, B., Shenoy, A., Moiseev, F., Jacobsson, H., Ghaffarkhah, A., Rivière, M., Walton, A., Crepy, C., Parrish, A., Liu, Y., Zhou, Z., Farabet, C., Radebaugh, C., Srinivasan, P., van der Salm, C., Fidjeland, A., Scellato, S., Latorre-Chimoto, E., Klimczak-Plucińska, H., Bridson, D., de Cesare, D., Hudson, T., Mendolicchio, P., Walker, L., Morris, A., Penchev, I., Mauger, M., Guseynov, A., Reid, A., Odoom, S., Loher, L., Cotruta, V., Yenugula, M., Grewe, D., Petrushkina, A., Duerig, T., Sanchez, A., Yadlowsky, S., Shen, A., Globerson, A., Kurzrok, A., Webb,</p>
<p>L., Dua, S., Li, D., Lahoti, P., Bhupatiraju, S., Hurt, D., Qureshi, H., Agarwal, A., Shani, T., Eyal, M., Khare, A., Belle, S. R., Wang, L., Tekur, C., Kale, M. S., Wei, J., Sang, R., Saeta, B., Liechty, T., Sun, Y., Zhao, Y., Lee, S., Nayak, P., Fritz, D., Vuyyuru, M. R., Aslanides, J., Vyas, N., Wicke, M., Ma, X., Bilal, T., Eltyshev, E., Balle, D., Martin, N., Cate, H., Manyika, J., Amiri, K., Kim, Y., Xiong, X., Kang, K., Luisier, F., Tripuraneni, N., Madras, D., Guo, M., Waters, A., Wang, O., Ainslie, J., Baldridge, J., Zhang, H., Pruthi, G., Bauer, J., Yang, F., Mansour, R., Gelman, J., Xu, Y., Polovets, G., Liu, J., Cai, H., Chen, W., Sheng, X., Xue, E., Ozair, S., Yu, A., Angermueller, C., Li, X., Wang, W., Wiesinger, J., Koukoumidis, E., Tian, Y., Iyer, A., Gurumurthy, M., Goldenson, M., Shah, P., Blake, M., Yu, H., Urbanowicz, A., Palomaki, J., Fernando, C., Brooks, K., Durden, K., Mehta, H., Momchev, N., Rahimtoroghi, E., Georgaki, M., Raul, A., Ruder, S., Redshaw, M., Lee, J., Jalan, K., Li, D., Perng, G., Hechtman, B., Schuh, P., Nasr, M., Chen, M., Milan, K., Mikulik, V., Strohman, T., Franco, J., Green, T., Hassabis, D., Kavukcuoglu, K., Dean, J., and Vinyals, O. Gemini: A family of highly capable multimodal models, 2023. URL https://arxiv.org/abs/2312.11805.</p>
<p>Golkar, S., Pettee, M., Eickenberg, M., Bietti, A., Cranmer, M., Krawezik, G., Lanusse, F., McCabe, M., Ohana, R., Parker, L., Blancard, B. R.-S., Tesileanu, T., Cho, K., and Ho, S. xval: A continuous number encoding for large language models. Neural Information Processing Systems (NeurIPS) AI for Science Workshop, 2023. URL https : //openreview.net/forum?id=KHDMZtoF4i.</p>
<p>Goyal, S., Ji, Z., Rawat, A. S., Menon, A. K., Kumar, S., and Nagarajan, V. Think before you speak: Training language models with pause tokens. International Conference on Learning Representations (ICLR), 2024. URL https : //openreview.net/forum?id=ph04CKkPdC.</p>
<p>Groeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney, R., Tafjord, O., Jha, A. H., Ivison, H., Magnusson, I., Wang, Y., Arora, S., Atkinson, D., Authur, R., Chandu, K. R., Cohan, A., Dumas, J., Elazar, Y., Gu, Y., Hessel, J., Khot, T., Merrill, W., Morrison, J., Muennighoff, N., Naik, A., Nam, C., Peters, M. E., Pyatkin, V., Ravichander, A., Schwenk, D., Shah, S., Smith, W., Strubell, E., Subramani, N., Wortsman, M., Dasigi, P., Lambert, N., Richardson, K., Zettlemoyer, L., Dodge, J., Lo, K., Soldaini, L., Smith, N. A., and Hajishirzi, H. Olmo: Accelerating the science of language models, 2024.</p>
<p>Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the MATH dataset. Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track, 2021. URL https:// openreview.net/forum?id=7Bywt2mQsCe.</p>
<p>Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b. arXiv:2310.06825, 2023. URL https://arxiv.org/abs/2310.06825.</p>
<p>Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. Neural Information Processing Systems (NeurIPS), 2022. URL https://openreview.net/ forum?id=e2TBb5y0yFf.</p>
<p>Kudo, T. Subword regularization: Improving neural network translation models with multiple subword candidates. Association for Computational Linguistics (ACL), 2018. URL https://aclanthology.org/P18-1007.</p>
<p>Lanham, T., Chen, A., Radhakrishnan, A., Steiner, B., Denison, C., Hernandez, D., Li, D., Durmus, E., Hubinger, E., Kernion, J., et al. Measuring faithfulness in chain-of-thought reasoning. arXiv:2307.13702, 2023. URL https://arxiv.org/abs/2307.13702.</p>
<p>Lee, N., Sreenivasan, K., Lee, J. D., Lee, K., and Papailiopoulos, D. Teaching arithmetic to small transformers. arXiv:2307.03381, 2023. URL https:// arxiv.org/abs/2307.03381.</p>
<p>Lewkowycz, A., Andreassen, A. J., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V. V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., GurAri, G., and Misra, V. Solving quantitative reasoning problems with language models. Neural Information Processing Systems (NeurIPS), 2022. URL https:// openreview.net/forum?id=IFXTZERXdM7.</p>
<p>Li, Y., Bubeck, S., Eldan, R., Del Giorno, A., Gunasekar, S., and Lee, Y. T. Textbooks Are All You Need II: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.</p>
<p>Liu, R., Garrette, D., Saharia, C., Chan, W., Roberts, A., Narang, S., Blok, I., Mical, R., Norouzi, M., and Constant, N. Character-aware models improve visual text rendering. Association for Computational Linguistics (ACL), 2023. URL https://aclanthology.org/ 2023.acl-long. 900.</p>
<p>Lundberg, S. The art of prompt design: Prompt boundaries and token healing, 2023. URL https: //towardsdatascience.com/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38.</p>
<p>McCoy, R. T., Yao, S., Friedman, D., Hardy, M., and Griffiths, T. L. Embers of autoregression: Understanding large language models through the problem they
are trained to solve. arXiv:2309.13638, 2023. URL https://arxiv.org/abs/2309.13638.</p>
<p>Muffo, M., Cocco, A., and Bertino, E. Evaluating transformer language models on arithmetic operations using number decomposition. Language Resources and Evaluation Conference (LREC), 2022. URL https: //aclanthology.org/2022.lrec-1.30.</p>
<p>Nogueira, R., Jiang, Z., and Lin, J. Investigating the limitations of transformers with simple arithmetic tasks. arXiv:2102.13019, 2021. URL https:// arxiv.org/abs/2102.13019.</p>
<p>Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et al. Show your work: Scratchpads for intermediate computation with language models. arXiv:2112.00114, 2021. URL https:// arxiv.org/abs/2112.00114.</p>
<p>OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M., Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G., Berner, C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.-L., Brockman, G., Brooks, T., Brundage, M., Button, K., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson, C., Carmichael, R., Chan, B., Chang, C., Chantzis, F., Chen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess, B., Cho, C., Chu, C., Chung, H. W., Cummings, D., Currier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch, N., Deville, D., Dhar, A., Dohan, D., Dowling, S., Dunning, S., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus, L., Felix, N., Fishman, S. P., Forte, J., Fulford, I., Gao, L., Georges, E., Gibson, C., Goel, V., Gogineni, T., Goh, G., Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S., Greene, R., Gross, J., Gu, S. S., Guo, Y., Hallacy, C., Han, J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse, C., Hickey, A., Hickey, W., Hoeschele, P., Houghton, B., Hsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S., Jonn, B., Jun, H., Kaftan, T., Lukasz Kaiser, Kamali, A., Kanitscheider, I., Keskar, N. S., Khan, T., Kilpatrick, L., Kim, J. W., Kim, C., Kim, Y., Kirchner, H., Kiros, J., Knight, M., Kokotajlo, D., Łukasz Kondraciuk, Kondrich, A., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V., Lampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D., Li, C. M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T., Lowe, R., Lue, P., Makanju, A., Malfacini, K., Manning, S., Markov, T., Markovski, Y., Martin, B., Mayer, K., Mayne, A., McGrew, B., McKinney, S. M., McLeavey, C., McMillan, P., McNeil, J., Medina, D., Mehta, A., Menick, J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V., Morikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O.,</p>
<p>Mély, D., Nair, A., Nakano, R., Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Ouyang, L., O’Keefe, C., Pachocki, J., Paino, A., Palermo, J., Pantuliano, A., Parascandolo, G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng, A., Perelman, A., de Avila Belbute Peres, F., Petrov, M., de Oliveira Pinto, H. P., Michael, Pokorny, Pokrass, M., Pong, V., Powell, T., Power, A., Power, B., Proehl, E., Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C., Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H., Ryder, N., Saltarelli, M., Sanders, T., Santurkar, S., Sastry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D., Sheppard, K., Sherbakov, T., Shieh, J., Shoker, S., Shyam, P., Sidor, S., Sigler, E., Simens, M., Sitkin, J., Slama, K., Sohl, I., Sokolowsky, B., Song, Y., Staudacher, N., Such, F. P., Summers, N., Sutskever, I., Tang, J., Tezak, N., Thompson, M., Tillet, P., Tootoonchian, A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J. F. C., Vallone, A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang, J. J., Wang, A., Wang, B., Ward, J., Wei, J., Weinmann, C., Welihinda, A., Welinder, P., Weng, J., Weng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich, S., Wong, H., Workman, L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba, W., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J., Zhuk, W., and Zoph, B. GPT-4 Technical Report, 2023. URL https://arxiv.org/abs/2303.08774.</p>
<p>Paster, K. Testing language models on a heldout high school national finals exam. https: //huggingface.co/datasets/keirp/ hungarian_national_hs_finals_exam, 2023.</p>
<p>Razeghi, Y., Logan IV, R. L., Gardner, M., and Singh, S. Impact of pretraining term frequencies on few-shot numerical reasoning. Empirical Methods in Natural Language Processing (EMNLP), 2022. URL https://aclanthology.org/ 2022.findings-emnlp. 59.</p>
<p>Rumbelow, J. and mwatkins. Solidgoldmagikarp (plus, prompt generation), 2023. URL https://www.lesswrong.com/posts/ aPeJE8bSo6rAFoLqg/solidgoldmagikarpplus-prompt-generation.</p>
<p>Saxton, D., Grefenstette, E., Hill, F., and Kohli, P. Analysing mathematical reasoning abilities of neural models. International Conference on Learning Representations (ICLR), 2019. URL https://openreview.net/ forum?id=H1gR5iR5FX.</p>
<p>Sennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with subword units. Association for Computational Linguistics (ACL), 2016. URL https://aclanthology.org/P16-1162.</p>
<p>Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and Singh, S. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. Empirical Methods in Natural Language Processing (EMNLP), 2020. URL https:// aclanthology.org/2020.emnlp-main.346.</p>
<p>Stolfo, A., Belinkov, Y., and Sachan, M. A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. Empirical Methods in Natural Language Processing (EMNLP), 2023. URL https://openreview.net/ forum?id=aB3Hwh4UzP.</p>
<p>Sun, K., Qi, P., Zhang, Y., Liu, L., Wang, W., and Huang, Z. Tokenization consistency matters for generative models on extractive NLP tasks. Empirical Methods in Natural Language Processing (EMNLP), 2023. URL https://aclanthology.org/ 2023.findings-emnlp.887.</p>
<p>Teknium. How did the gpt tokenizer get created?, 2023. URL https://twitter.com/Teknium1/ status/1634667026739527680?s=20.</p>
<p>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv:2302.13971, 2023a. URL https://arxiv.org/abs/2302.13971.</p>
<p>Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and finetuned chat models. arXiv:2307.09288, 2023b. URL https://arxiv.org/abs/2307.09288.</p>
<p>Wang, B. and Komatsuzaki, A. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model, 2021. URL https://huggingface.co/EleutherAI/ gpt-j-6b.</p>
<p>Wei, J. Sorting a list of words by the second letter, 2023. URL https://x.com/..jasonwei/ status/1661781746759909376?s=20.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. Neural Information Processing Systems (NeurIPS), 2022. URL https://openreview.net/ forum?id=VJQlMeSB_J.</p>
<p>Yu, L., Simig, D., Flaherty, C., Aghajanyan, A., Zettlemoyer, L., and Lewis, M. MEGABYTE: Predicting million-byte sequences with multiscale transformers. Neural Information Processing Systems (NeurIPS), 2023. URL https: //openreview.net/forum?id=JTmO2V9Xpz.</p>
<p>Zhou, H., Nova, A., Larochelle, H., Courville, A., Neyshabur, B., and Sedghi, H. Teaching algorithmic reasoning via in-context learning. arXiv:2211.09066, 2022. URL https://arxiv.org/abs/2211.09066.</p>
<p>Zhou, H., Bradley, A., Littwin, E., Razin, N., Saremi, O., Susskind, J., Bengio, S., and Nakkiran, P. What algorithms can transformers learn? a study in length generalization. arXiv:2310.16028, 2023. URL https: //arxiv.org/abs/2310.16028.</p>
<h1>A. Tokenization differences between frontier LLMs</h1>
<p>000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053 054 055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999</p>
<p>Figure 14. The equivalent of Figure 2 but for the Claude tokenizer. All 3-digit number strings, colored red when the string does not have a corresponding single token dedicated to it. The lack of systematicity suggests that Claude tokenizes numbers using pure BPE. Note also, however, that token coverage is generally higher than in Figure 2, likely in part because the Claude tokenizer has a larger vocabulary size ( 65 k tokens) than OpenAI's p50k_base ( 50 k tokens).
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 15. The equivalent of Figure 3 but for the Claude tokenizer. Note that this distribution looks more like p50k_base than cl100k_base in Figure 3. This, along with Figure 14 above shows that Claude's tokenizer exhibits a lack of systemacity when tokenizing numbers, suggesting the use of pure BPE number tokens, rather than something bespoke (as other current models use; see Table 1).</p>
<h2>B. Experiments with other system prompts</h2>
<p>We also conducted our main experiment with an alternate, custom system prompt (as opposed to the default ' You are a helpful assistant. '). The prompt we used was:</p>
<p>You are MathGPT, an expert at solving math problems. When given a math problem, you respond only by repeating the problem statement and appending the answer. You do not say any other words.</p>
<p>Results using this prompt are presented in Figure 16. We found it lead to small improvements in performance at low shot numbers (e.g., 1-shot) but these diminished at 8 -shots. To maximize the reproducibility and applicability of our results, we decided to just use the default prompt. As we report 8 -shot results throughout most of the paper, we doubt the system prompt would have a large effect on our results, given Figure 16.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 16. Comparison of R2L and L2R tokenization strategies for different numbers of shots and using two different system prompts.</p>
<h1>C. Frequency effects</h1>
<p>Given the findings of prior work on numerical reasoning demonstrating frequency effects (Razeghi et al., 2022), we also investigated whether or not our observed error patterns could be explained by term frequency. While we do not have access to the pre-training data of GPT-3.5 and GPT-4 models, we use the tokenizer merge ranks ${ }^{17}$ as a signal of term frequency. We analyze the expanded set of problems used for the error analysis in Section 4. Our results are summarized below:</p>
<p>When making an error, GPT-3.5 is slightly more likely to output a more frequent token. For each token in the model response on problems where it makes a mistake, we consider if the outputted incorrect token is more or less frequent (has lower or higher merge rank) than the correct one. Of the 25 errors made by the model when using R2L tokenization, 15 involve substituting in a more frequent token ( $60 \%, p=0.115$ using a binomial null distribution assuming chance is $50 \%$ ). Of the 425 errors made when using L2R tokenization, 238 involve substituting in a more frequent token ( $56 \%, p=0.005$ using a binomial null distribution assuming chance is $50 \%$ ). While we do see a significant effect in the L2R tokenization case, the margin is relatively small, which suggests that token frequency is not the dominant reason behind the error patterns.</p>
<p>When using L2R tokenization in the length mismatch case, GPT-3.5 errors do not show strong correlation to token frequency. In Section 4.3, we found that GPT-3.5 always gets the fourth digit wrong (Figure 9a). We then found correlation in the specific error in digit 4 to the magnitude difference between correct digit and digit in the model response (Figure 9b). Here, we ask if the substituted token 2 (whose first digit would be digit 4 of the response) is correlated to frequency in training data. Specifically, for each problem, we rank the tokens corresponding to the 10 possible "digit 4 mistakes" by merge rank. In Figure 17, we show the distribution of ranks across all 365 problems where the model makes "digit 4" errors. If models are preferentially substituting in more frequent tokens, we would expect to see a negative trend from the top left to the bottom right (as we did in Figure 9b). In Figure 17, we see a slight preference for outputting the most likely token (roughly $16 \%$ of the time, where chance would be $10 \%$ ), but overall we see no clear trend.</p>
<p>Off-by-one errors do not seem to be correlated to answer token frequency. In Section 4.4, we found that the vast majority of remaining errors (for R2L tokenization, and for L2R tokenization in the length match condition) are off-by-one errors in the units digit of a token. Here, we ask if the specific substitution by the model is correlated to token frequency, measured by merge rank. Specifically, we condition on the model possibly making an off-by-one error, which means there are 3 possible output tokens (the correct token, the correct token minus one, the correct token plus one). We then rank these tokens based on merge rank, and see if the model preferentially picks the token with lowest merge rank. Of the 24 off-by-one errors when using R2L tokenization, we find the model only picks the "most frequent" token 7 times. Of the 53 off-by-one errors when using L2R tokenization, we find the model only picks the "most frequent" token 17 times. Both of these are essentially what we would expect by chance (one out of three), which suggests that output token frequency effects are not a dominant factor in why the model makes off-by-one errors.</p>
<p>Overall, we find mild to no evidence of token frequency effects in our experiments. This could be due to the presumably larger scale of GPT-3.5 (as compared to GPT-J, used by Razeghi et al. (2022)). However, we note that our method of measuring token frequency is imperfect-relying on BPE merge ranks to signal frequency as we do not have access to</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 17. Distribution of relative rank of substituted incorrect token 2 in model response when using L2R tokenization in the length mismatch condition.
pre-training data. Future work could study such associations further in newer, larger models with open pretraining data (Groeneveld et al., 2024).</p>
<h1>D. Stereotyped patterns in model log probabilities</h1>
<p>Mirroring the results of Section 4, we found stereotyped patterns in model log probabilities ("logprob"). Specifically, the OpenAI API returns the top 5 tokens at each position with their corresponding logprobs. We analyzed these log probabilities in three cases: L2R tokenization on length mismatch problems, L2R tokenization on length match problems, and R2L tokenization on all problems. These conditions mirror the most salient error effects we found in Section 4, with the former leading to "digit 4 " errors, and the latter two leading to mostly off-by-one errors.</p>
<p>In addition to the raw logprobs, we computed an additional entropy metric (per output token) to measure model uncertainty in its output. Since access is restricted to the top 5 logprobs, we use the following lower bound, $H_{\text {lower }}$, to the true entropy:</p>
<p>$$
\begin{aligned}
H_{\text {true }} &amp; \equiv-\sum_{i=1}^{V} p_{i} \log \left(p_{i}\right) \
&amp; =-\left(\sum_{i=1}^{5} p_{i} \log \left(p_{i}\right)+\sum_{i=6}^{V} p_{i} \log \left(p_{i}\right)\right) \
&amp; \geq-\left(\sum_{i=1}^{5} p_{i} \log \left(p_{i}\right)+\sum_{i=6}^{V} p_{i} \log \left(p_{5}\right)\right) \
&amp; =-\left(\sum_{i=1}^{5} p_{i} \log \left(p_{i}\right)+\left(1-\sum_{i=1}^{5} p_{i}\right) \cdot \log \left(p_{5}\right)\right) \
&amp; \equiv H_{\text {lower }}
\end{aligned}
$$</p>
<p>where $p_{i}$ denotes the probability of the $i$-th most likely token. We use the natural logarithm for entropy, so all entropies are in nats (not bits).</p>
<p>For the "digit 4" error pattern (Section 4.3), we find an interesting trend in model entropy. The entropy both on problems it gets incorrect ( $91.25 \%$ ) and correct ( $8.25 \%$ ) is roughly the same ( 2.066 and 2.061 respectively). Even when the model gets the question right, it's unsure of its answer, suggesting that it might just be guessing a second output token with the right tens and ones digit and random hundreds digit. Providing further evidence for this mechanism, we observe that, of the problems where the model makes an error, about half ( $49.6 \%$ ) of the time the correct answer appears in the top 5 output tokens. This is in line with what we would see for random guessing from the 10 tokens. That said, the model may exhibit some degree of bias towards the correct output, as evidenced by the downward trend in Figure 9b.</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 18. Histogram of difference in answer log probabilities (summed over tokens) on length match problems that the model answers correctly using both R2L and L2R tokenization. Black dotted line signifies 0 . Red dotted line shows the average difference-on average, the model is more "confident" when using R2L tokenization.</p>
<p>For the off-by-one error patterns (Section 4.4), we observe a qualitatively different trend. Specifically, of the 53 off-by-one errors when using L2R tokenization on length match problems, in all cases we find that the second most likely token is the correct answer. We observe the same effect on the 25 off-by-one errors when using R2L tokenization. Furthermore, the entropy in both cases is around $0.45 \pm 0.05$, indicating that the model puts most of its weight on these top 2 most likely tokens. Unlike in the "digit 4" case, model entropy on correct problems is significantly lower (approximately 0.03 , averaged across dataset and tokens) indicating that the model is "confidently correct" when using L2R tokenization on length match problems or R2L tokenization on all problems. Interestingly, on the subset of length match problems that the model answers correctly in both L2R and R2L tokenization, we found the model is slightly more confident when using R2L tokenization (which aligns with our intuition, as the model is also more often correct when using R2L tokenization)—see Figure 18.</p>
<p>These results demonstrate that, depending on tokenization direction and alignment between input and output tokenization, we observe stereotyped patterns in model log probabilities. When using L2R tokenization on length mismatch problems, the model appears to make a magnitude-biased guess between all possible fourth digits (corresponding to 10 possible tokens ${ }^{18}$ ). In the other cases, the model is mostly confidently correct. When it does make an error, it's almost always an off-by-one error (Section 4.4) where it's uncertain between its chosen off-by-one incorrect answer and the true answer, but does not really consider other outputs beyond these two. ${ }^{19}$</p>
<h1>E. Additional experimental details</h1>
<p>All code and raw results can be found at https://github.com/aadityasingh/TokenizationCounts.</p>
<h2>E.1. Length control for error analysis</h2>
<p>As described in Section 4.1, after noticing errors mostly come from the length mismatch condition in our original experiments (which used 90 problems, balanced by input digit length), we conducted a larger experiment where we controlled for input and output digit lengths. Specifically, we considered the following (addend1_length, addend2_length, answer_length) triplets: $(7,7,7),(7,7,8),(8,7,8),(7,8,8),(8,7,9),(7,8,9),(8,8,8),(8,8,9),(9,7,9),(7,9,9),(9,8,9),(8,9,9),(9,9,9)$. Problems in each condition were sampled randomly so as to satisfy the digit length constraints for each triplet. We sampled 100 problems for each triplet, for a total of 1300 problems.</p>
<h2>E.2. Access dates</h2>
<p>Given the changing nature of the OpenAI API, we report access dates for all experiments below. We tried to use the supposed "fixed" models for all experiments, but did notice some non-determinism, even at temperature 0 -an issue that may be due to non-determinism in floating point arithmetic. We also note that the gpt-4-0314 appears to have</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>been early-deprecated, as we can no longer access it despite the supposed June 13, 2024 deprecation date on https: //platform.openai.com/docs/deprecations.</p>
<p>Access dates by figure in main text:</p>
<ul>
<li>gpt-3.5-turbo-0301, Figure 4: April 7, 2023</li>
<li>gpt-3.5-turbo-0301, Figure 5: May 18, 2023</li>
<li>gpt-3.5-turbo-0301, Figure 6 left two columns: May 18, 2023</li>
<li>gpt-3.5-turbo-0301, Figure 6 right two columns: April 7, 2023</li>
<li>gpt-3.5-turbo-0301, Figure 7-8: January 25, 2024</li>
<li>gpt-3.5-turbo-0301, Figure 10-11 May 24, 2024</li>
<li>gpt-4-0314, Figure 12: May 2, 2023</li>
<li>gpt-3.5-turbo-0613, Figure 12-13: January 25, 2024</li>
<li>gpt-3.5-turbo-1106, Figure 12-13: January 29, 2024</li>
<li>gpt-4-0613, Figure 12-13: January 25, 2024</li>
<li>gpt-4-1106-preview, Figure 12-13: January 29, 2024</li>
</ul>
<h1>E.3. Example prompts</h1>
<p>In this section, we provide example prompts we used for various experiments. For simplicity, we use the same query for each prompt shown below, and we only use 2 shots (most experiments in the main text are done with 8 shots). In practice, we sampled shots randomly (controlling for digit length to match the query length) for each query, as explained in Section 2.1. For the experiments described in Section 4 and Appendix E.1, the shots were also controlled to have the same answer length as the query. The examples we present below, though, are for the runs in the rest of the paper (where only input digit lengths are controlled). For maximum clarity, we display prompts as the list of dictionaries that gets sent to OpenAI's API and roughly in the order used for figures in the paper. Following the advice at https://platform.openai.com/docs/guides/prompt-engineering/tactic-provide-examples, we make use of the multi-turn chat dialog to prompt the model, as opposed to one big user message with all the examples.</p>
<p>L2R tokenization, input-digit-controlled for two 7-digit numbers:</p>
<div class="codehilite"><pre><span></span><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3790206+6739555=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;10529761&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6777159+7096168=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;13873327&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8302080+3529456=&#39;}]
</code></pre></div>

<p>R2L tokenization, input-digit-controlled for two 7-digit numbers:</p>
<div class="codehilite"><pre><span></span><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3,790,206+6,739,555=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;10,529,761&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6,777,159+7,096,168=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;13,873,327&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8,302,080+3,529,456=&#39;}]
</code></pre></div>

<p>R2L tokenization, delimiter-control condition using '#' :</p>
<div class="codehilite"><pre><span></span><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3#790#206+6#739#555=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;10#529#761&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6#777#159+7#096#168=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;13#873#327&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8#302#080+3#529#456=&#39;}]
</code></pre></div>

<p>L2R tokenization, thinking token control by using separators in L2R tokenization:</p>
<div class="codehilite"><pre><span></span><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;379,020,6+673,955,5=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;105,297,61&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;677,715,9+709,616,8=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;138,733,27&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;830,208,0+352,945,6=&#39;}]
</code></pre></div>

<p>L2R tokenization, thinking token control by using 2 extra spaces:</p>
<div class="codehilite"><pre><span></span><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3790206 + 6739555 = &#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;10529761&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6777159 + 7096168 = &#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;13873327&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8302080 + 3529456 = &#39;}]
</code></pre></div>

<p>L2R tokenization, thinking token control by using 2 extra spaces:</p>
<div class="codehilite"><pre><span></span><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3790206 + 6739555 = &#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;10529761&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6777159 + 7096168 = &#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;13873327&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8302080 + 3529456 = &#39;}]
</code></pre></div>

<p>Repeat L2R $\rightarrow$ R2L:</p>
<div class="codehilite"><pre><span></span><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3790206+6739555=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;3,790,206+6,739,555=10,529,761&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6777159+7096168=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;6,777,159+7,096,168=13,873,327&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8302080+3529456=&#39;}]
</code></pre></div>

<p>Repeat control L2R $\rightarrow$ L2R:</p>
<div class="codehilite"><pre><span></span><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3790206+6739555=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;3790206+6739555=10529761&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6777159+7096168=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;6777159+7096168=13873327&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8302080+3529456=&#39;}]
</code></pre></div>

<p>Output control L2R $\rightarrow$ R2L:
[{'role': 'system', 'content': 'You are a helpful assistant.'},
{'role': 'user', 'content': '3790206+6739555='},
{'role': 'assistant', 'content': '10,529,761' },
{'role': 'user', 'content': '6777159+7096168='},
{'role': 'assistant', 'content': '13,873,327' },
{'role': 'user', 'content': '8302080+3529456='}]</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{18} \mathrm{~A}$ completely random guess over 10 tokens would correspond to an entropy of about 2.3 , which is in line with the lower bound observed (of about 2.06) and the finding of a slight mangitude bias (which would decrease the entropy from 2.3).
${ }^{19} \mathrm{~A}$ completely random guess over 2 tokens would correspond to an entropy of about 0.69 , which is in line with the lower bound observed (of about 0.45 ).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{16} 3$-digit number tokens also reduce inference-time compute when models use numbers in their output, which could be an important consideration when serving models at scale.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
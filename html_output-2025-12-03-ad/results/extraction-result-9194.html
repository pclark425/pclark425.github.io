<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9194 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9194</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9194</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-273185876</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.04601v2.pdf" target="_blank">ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation</a></p>
                <p><strong>Paper Abstract:</strong> Automated generation of scientific protocols executable by robots can significantly accelerate scientific research processes. Large Language Models (LLMs) excel at Scientific Protocol Formulation Tasks (SPFT), but the evaluation of their capabilities rely on human evaluation. Here, we propose a flexible, automatic framework to evaluate LLMs' capability on SPFT: ProtoMed-LLM. This framework prompts the target model and GPT-4 to extract pseudocode from biology protocols using only predefined lab actions and evaluates the output of the target model using LLAM-EVAL, the pseudocode generated by GPT-4 serving as a baseline and Llama-3 acting as the evaluator. Our adaptable prompt-based evaluation method, LLAM-EVAL, offers significant flexibility in terms of evaluation model, material, criteria, and is free of cost. We evaluate GPT variations, Llama, Mixtral, Gemma, Cohere, and Gemini. Overall, we find that GPT and Cohere are powerful scientific protocol formulators. We also introduce BIOPROT 2.0, a dataset with biology protocols and corresponding pseudocodes, which can aid LLMs in formulation and evaluation of SPFT. Our work is extensible to assess LLMs on SPFT across various domains and other fields that require protocol generation for specific goals.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9194.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9194.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction‑tuned large language model from OpenAI used in this paper as a generator of machine-executable pseudocode (protocol pseudocode) from biology protocols; used as a baseline and target model in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 variant from OpenAI used in the paper as both baseline and as a target model for pseudocode generation; the paper does not report model parameter count or training data details.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Molecular / laboratory biology (scientific protocol formulation for biological experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Convert natural-language experimental protocols into Python-style pseudocode composed of predefined laboratory action functions (pseudofunctions) and arguments, i.e., generate machine-executable protocol pseudocode for biology protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>LLAM-EVAL (LLama-3 evaluator) scoring on six criteria: Coherence, Consistency, Fluency, Relevance, Precision, Coverage (scores 1–5); also reference metrics for function names and inputs: normalized Levenshtein distance, BLEU, SciBERTScore, precision and recall.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>LLAM-EVAL average scores (1–5): reported averages from the self/self comparison experiment: 4.06 ± (std) when predefined actions given but original protocol not provided; 4.21 ± (std) when neither predefined actions nor protocol provided; 4.46 ± (std) when both predefined actions and original protocol provided. (Scores are means reported in Table 2.)</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Presence of predefined actions in prompt (improves performance); provision of the original protocol as baseline; prompt framing/order of actions (models tend to perform better when actions are presented in the same order as in the protocol); coarse-graining of complex actions vs many fine-grained actions (affects calculation/representation errors); evaluator choice (Llama-3 used); dataset size and domain coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>GPT-4 generated pseudocode used as a baseline in LLAM-EVAL; original protocol also evaluated as an alternative baseline. GPT-4o performs near top of evaluated models and often outperforms many non-GPT models on LLAM-EVAL criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance varies by prompt conditions and whether predefined actions are supplied; potential biases due to evaluator model (Llama-3); GPT-generated baseline may not represent ground-truth; certain protocol details can be lost or represented inconsistently; some actions classified as NoAction in minor cases.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Applying domain knowledge (predefining the finite set of lab actions) generally improves performance; coarse-graining repetitive/complex action sequences can reduce error from arithmetic/logical steps; evaluate evaluator-model biases; expand domain-specific action definitions and dataset size; explore alternative baselines (manually annotated pseudocode).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9194.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9194.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-capability OpenAI LLM used in this work chiefly to generate baseline pseudocode from biology protocols and compared against target models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 from OpenAI used as the primary baseline generator of pseudocode; paper does not give parameter counts or training corpus details.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Molecular / laboratory biology (protocol formulation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate pseudocode representing experimental protocols using a fixed set of predefined lab-action functions and arguments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>LLAM-EVAL (six criteria: Coherence, Consistency, Fluency, Relevance, Precision, Coverage) scored by Llama-3; reference metrics (Levenshtein distance, BLEU, SciBERTScore, precision, recall) for function names/inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>LLAM-EVAL self/self comparison average when evaluated with predefined actions and original protocol present: 4.49 ± (std) (Table 2); per-criterion means listed in Table 2 (e.g., Coherence ~4.32±0.53, Consistency ~4.70±0.58, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Inclusion of predefined action set in prompt; whether original protocol used as baseline; prompt ordering; evaluator biases; dataset size and domain-specific coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as the baseline pseudocode generator; other models are compared to GPT-4 outputs via LLAM-EVAL. GPT-4 shows among the highest LLAM-EVAL scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>GPT-4 output used as baseline may not equal true ground truth; some variations in representation of identical pseudofunctions can degrade evaluation consistency; real-world execution of synthesized protocols may still fail due to physical experimental variability.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Use domain knowledge (predefined actions) to stabilize extraction/generation; consider alternative baselines or manually annotated pseudocode to avoid over-reliance on a single-model baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9194.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9194.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cohere+</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cohere+ (Cohere platform variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Cohere model variant evaluated as a protocol-formulation LLM; authors report it as one of the stronger non-OpenAI models for generating pseudocode from biology protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Cohere+</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cohere model family variant used in experiments; paper does not provide parameter count or training details for Cohere+.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Molecular / laboratory biology (protocol pseudocode generation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Translate natural-language experimental protocols into a sequence of predefined lab-action pseudofunction calls and arguments (pseudocode).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>LLAM-EVAL (six criteria) scored by Llama-3; reference-based metrics also measured (BLEU, SciBERTScore, Levenshtein, precision, recall).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>LLAM-EVAL average scores (self/self comparison): 3.94 ± (std) when predefined actions given but protocol not included; 3.98 ± (std) without actions/protocol; 4.50 ± (std) when both predefined actions and original protocol provided (Table 2). Cohere+ is reported as a powerful protocol formulator alongside GPT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Predefined action set inclusion (large positive effect); order of actions in prompt; prompt structure; dataset/domain coverage; coarse-graining of complex actions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against GPT-4 baseline pseudocode via LLAM-EVAL; when both predefined actions and protocol provided, Cohere+ achieves near-top LLAM-EVAL scores (close to GPT variants).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance depends on prompt contents; recall decreased in some settings when predefined actions were used; dataset limitations and incomplete action definitions may hamper generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Domain-specific predefined actions help; further research to understand recall drops and to expand action definitions and dataset size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9194.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9194.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3 (70b)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large Llama-3 model variant used primarily in this paper as an automatic evaluator (LLAM-EVAL) and also evaluated in generation tasks; chosen as the evaluator because it produced high consistency in scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3 70b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-3 family 70-billion-parameter variant (explicitly referenced as 70b in experiments); used both as a target model in some comparisons and considered as an evaluator candidate; described as free of cost at time of experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Molecular / laboratory biology (used for evaluation and also as a generator in comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>When used as a generator: produce pseudocode from protocols; when used as evaluator (LLAM-EVAL): rate target pseudocode against baseline pseudocode on defined criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>When acting as evaluator: LLAM-EVAL scoring on six criteria; when as generator: evaluated using LLAM-EVAL and reference metrics (BLEU, SciBERTScore, Levenshtein, precision/recall).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>As a generator, LLAM-EVAL average (best condition with predefined actions and protocol present) ~4.12 ± (std) (Table 2). As an evaluator, Llama-3 (general) was selected because it achieved high scores across the self/self comparison tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model size (70b better than 8b), availability of predefined actions in prompts, prompt ordering, dataset coverage, evaluator biases when used to score outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to GPT baselines via LLAM-EVAL; Llama-3 70b outperforms smaller Llama-3 variants (8b) in LLAM-EVAL generator performance; Llama-3 chosen as evaluator because of self/self comparison results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Smaller Llama-3 (8b) shows substantially lower generation scores; potential for evaluator bias and hallucination when Llama-3 is used for scoring; reliance on a single evaluator exposes results to that model's biases.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Choose evaluators via self-self comparison to ensure numeric outputs and consistency; be aware of evaluator bias and consider testing multiple evaluator models; use domain-specific prompts and predefined actions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9194.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9194.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3 (8b)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3 8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller 8-billion-parameter Llama-3 variant evaluated in the paper for pseudocode generation; performed noticeably worse than larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3 8b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-3 family 8-billion-parameter variant used as a target model for pseudocode generation; no additional training details provided.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Molecular / laboratory biology (protocol pseudocode generation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate predefined-action-based pseudocode from biological protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>LLAM-EVAL (six criteria) scored by Llama-3 evaluator; reference metrics (BLEU, SciBERTScore, Levenshtein, precision/recall) also applied.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>LLAM-EVAL average (best condition with predefined actions and protocol): ~3.08 ± (std) (Table 2), substantially lower than larger Llama-3 70b and GPT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Smaller model capacity (8b) relative to larger variants; prompt design; presence/absence of predefined actions; dataset/domain complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to GPT variants and larger Llama-3; underperforms relative to Llama-3 70b and GPT-4/GPT-4o/Cohere+ in LLAM-EVAL scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower overall LLAM-EVAL scores; poorer fluency/consistency; may not be suitable for reliable protocol generation without finetuning or augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Larger model sizes produce better protocol generation; for practical protocol formulation tasks, prefer larger-capacity models or domain-specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9194.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9194.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI model family member used as a target model for pseudocode generation and evaluated with LLAM-EVAL; shows moderate performance compared to GPT-4 and Cohere+.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 family model from OpenAI used for pseudocode generation; the paper does not report parameter count or training data specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Molecular / laboratory biology (protocol formulation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Translate protocol text into a sequence of predefined pseudofunction calls (pseudocode) describing experimental procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>LLAM-EVAL six-criterion scoring by Llama-3; reference metrics (BLEU, SciBERTScore, Levenshtein, precision, recall).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>LLAM-EVAL average (best condition with predefined actions and protocol): ~4.39 ± (std) (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Prompting with predefined actions improves scores; prompt/order effects; dataset coverage; evaluator choice.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against GPT-4 baseline pseudocode and other LLMs via LLAM-EVAL; GPT-3.5 performs below GPT-4 but can reach reasonably strong scores with domain prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower performance than GPT-4-family models, some instability without predefined actions; potential hallucinations and representation variance.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Provision of predefined action set stabilizes outputs; consider finetuning or tool-augmented approaches for better domain fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9194.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9194.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial/open LLM evaluated as a target model for converting biology protocols into pseudocode; achieves moderate-to-strong LLAM-EVAL scores with domain prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixtral model family (specific size/version not given) used as a pseudocode generator in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Molecular / laboratory biology (protocol pseudocode generation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate Python-style pseudocode from biology protocols using a fixed set of lab-action functions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>LLAM-EVAL (six criteria) scored by Llama-3; reference-based metrics (BLEU, SciBERTScore, Levenshtein).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>LLAM-EVAL average (best condition with predefined actions and protocol): ~4.21 ± (std) (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Predefined actions presence; prompt order; model-specific capacity and training; evaluator biases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to GPT baselines in LLAM-EVAL; Mixtral achieves competitive scores though generally below top GPT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Potential non-numeric outputs when used as evaluator in some settings (not all models generated numeric scores); performance sensitive to prompt structure.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Include domain action lists in prompts and test prompt/order effects; consider multiple evaluators for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9194.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9194.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma-7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7-billion-parameter model (Gemma-7b) evaluated as a generator of protocol pseudocode; shows moderate performance when given domain prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Gemma family 7-billion-parameter variant used in the experiments; exact training details not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Molecular / laboratory biology (protocol pseudocode generation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Convert natural-language biological protocols to pseudocode using predefined lab actions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>LLAM-EVAL six-criterion scoring by Llama-3; reference metrics for function inputs and names.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>LLAM-EVAL average (best condition with predefined actions and protocol): ~4.02 ± (std) (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Predefined action provision; prompt structure; model capacity; ordering of action definitions relative to protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against GPT-4 generated pseudocode and other LLMs via LLAM-EVAL; Gemma-7b is mid-ranked among tested models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Smaller-capacity models show lower overall performance than large GPT variants; potential variability across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Using predefined action lists and providing domain context raises scores; consider larger models or domain specialization for improved performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9194.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9194.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-2.0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 2.0 (Google)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Google Gemini family model evaluated for protocol pseudocode generation; included in the comparative evaluation with LLAM-EVAL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-2.0</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Gemini family model (version 2.0) from Google used as a target model; paper does not give parameter counts or training corpus detail.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Molecular / laboratory biology (protocol formulation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate pseudocode composed of predefined lab-action calls from natural-language protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>LLAM-EVAL (six criteria) via Llama-3; reference metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>LLAM-EVAL average (best condition with predefined actions and protocol): ~4.02 ± (std) (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Predefined actions presence; prompt order; dataset content; evaluator model choice.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to GPT-4 baseline outputs via LLAM-EVAL; Gemini-2.0 attains moderate-to-strong LLAM-EVAL scores, often below top GPT/Cohere+ models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance sensitive to prompt configurations and domain coverage; some variability across evaluation conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Providing domain-specific predefined actions improves performance; test different prompt formulations and evaluator models for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9194.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9194.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3 (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An Anthropic model family member (Claude 3) mentioned as one of the models prompted for pseudocode generation but not fully reported in numeric result tables.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Claude 3 family model from Anthropic referenced in Methods as prompted for pseudocode generation; numeric LLAM-EVAL results not fully reported in main tables.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Molecular / laboratory biology (protocol pseudocode generation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate pseudocode from biology protocols using predefined actions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>LLAM-EVAL (six criteria) intended for evaluation; however numeric scores for Claude3 are not provided in the paper's main result tables.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Not explicitly quantified in paper due to missing numeric results; expected factors include predefined actions, prompt ordering, and evaluator biases as for other models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Was compared conceptually against other LLMs but specific numeric comparisons in LLAM-EVAL are not shown for Claude3 in the main tables.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes that some models did not produce numeric evaluator outputs in certain evaluation tasks; Claude3's numeric performance not reported clearly.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>When including multiple vendor models, ensure evaluator prompts and settings yield numeric outputs; evaluate with multiple evaluators to avoid missing/ambiguous evaluation outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9194.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9194.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier OpenAI LLM which the authors prompted for pseudocode generation (listed among models used), though the main reported numeric results emphasize later models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 family model referenced in Methods as one of the models prompted to generate pseudocode; the paper does not present detailed per-model numeric results for every GPT-3 configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Molecular / laboratory biology (protocol pseudocode generation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Convert protocol text into sequences of predefined lab-action pseudofunctions and arguments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>LLAM-EVAL (six criteria) and reference-based metrics; detailed numeric outcomes for GPT-3 are not prominently reported in main tables.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Same factors noted for other models (predefined actions, prompt ordering, dataset coverage); specifics not reported for GPT-3 in main tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Included in comparative experiment set but not emphasized in primary performance claims; GPT-3 generally expected to trail GPT-3.5/GPT-4 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Detailed numeric performance not shown; older generation models may underperform on nuanced protocol generation compared to later models.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Use newer instruction-tuned models and domain prompts; include action lists to improve extraction/generation fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BioPlanner: Automatic evaluation of LLMs on protocol planning in biology <em>(Rating: 2)</em></li>
                <li>Protocode: Leveraging large language models (LLMs) for automated generation of machine‑readable PCR protocols from scientific publications <em>(Rating: 2)</em></li>
                <li>LLMs can generate robotic scripts from goal‑oriented instructions in biological laboratory automation <em>(Rating: 2)</em></li>
                <li>Automated extraction of chemical synthesis actions from experimental procedures <em>(Rating: 2)</em></li>
                <li>G‑Eval: NLG evaluation using GPT‑4 with better human alignment <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9194",
    "paper_id": "paper-273185876",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (OpenAI)",
            "brief_description": "An instruction‑tuned large language model from OpenAI used in this paper as a generator of machine-executable pseudocode (protocol pseudocode) from biology protocols; used as a baseline and target model in experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "GPT-4 variant from OpenAI used in the paper as both baseline and as a target model for pseudocode generation; the paper does not report model parameter count or training data details.",
            "scientific_subdomain": "Molecular / laboratory biology (scientific protocol formulation for biological experiments)",
            "simulation_task": "Convert natural-language experimental protocols into Python-style pseudocode composed of predefined laboratory action functions (pseudofunctions) and arguments, i.e., generate machine-executable protocol pseudocode for biology protocols.",
            "evaluation_metric": "LLAM-EVAL (LLama-3 evaluator) scoring on six criteria: Coherence, Consistency, Fluency, Relevance, Precision, Coverage (scores 1–5); also reference metrics for function names and inputs: normalized Levenshtein distance, BLEU, SciBERTScore, precision and recall.",
            "simulation_accuracy": "LLAM-EVAL average scores (1–5): reported averages from the self/self comparison experiment: 4.06 ± (std) when predefined actions given but original protocol not provided; 4.21 ± (std) when neither predefined actions nor protocol provided; 4.46 ± (std) when both predefined actions and original protocol provided. (Scores are means reported in Table 2.)",
            "factors_affecting_accuracy": "Presence of predefined actions in prompt (improves performance); provision of the original protocol as baseline; prompt framing/order of actions (models tend to perform better when actions are presented in the same order as in the protocol); coarse-graining of complex actions vs many fine-grained actions (affects calculation/representation errors); evaluator choice (Llama-3 used); dataset size and domain coverage.",
            "comparison_baseline": "GPT-4 generated pseudocode used as a baseline in LLAM-EVAL; original protocol also evaluated as an alternative baseline. GPT-4o performs near top of evaluated models and often outperforms many non-GPT models on LLAM-EVAL criteria.",
            "limitations_or_failure_cases": "Performance varies by prompt conditions and whether predefined actions are supplied; potential biases due to evaluator model (Llama-3); GPT-generated baseline may not represent ground-truth; certain protocol details can be lost or represented inconsistently; some actions classified as NoAction in minor cases.",
            "author_recommendations_or_insights": "Applying domain knowledge (predefining the finite set of lab actions) generally improves performance; coarse-graining repetitive/complex action sequences can reduce error from arithmetic/logical steps; evaluate evaluator-model biases; expand domain-specific action definitions and dataset size; explore alternative baselines (manually annotated pseudocode).",
            "uuid": "e9194.0",
            "source_info": {
                "paper_title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A high-capability OpenAI LLM used in this work chiefly to generate baseline pseudocode from biology protocols and compared against target models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 from OpenAI used as the primary baseline generator of pseudocode; paper does not give parameter counts or training corpus details.",
            "scientific_subdomain": "Molecular / laboratory biology (protocol formulation)",
            "simulation_task": "Generate pseudocode representing experimental protocols using a fixed set of predefined lab-action functions and arguments.",
            "evaluation_metric": "LLAM-EVAL (six criteria: Coherence, Consistency, Fluency, Relevance, Precision, Coverage) scored by Llama-3; reference metrics (Levenshtein distance, BLEU, SciBERTScore, precision, recall) for function names/inputs.",
            "simulation_accuracy": "LLAM-EVAL self/self comparison average when evaluated with predefined actions and original protocol present: 4.49 ± (std) (Table 2); per-criterion means listed in Table 2 (e.g., Coherence ~4.32±0.53, Consistency ~4.70±0.58, etc.).",
            "factors_affecting_accuracy": "Inclusion of predefined action set in prompt; whether original protocol used as baseline; prompt ordering; evaluator biases; dataset size and domain-specific coverage.",
            "comparison_baseline": "Used as the baseline pseudocode generator; other models are compared to GPT-4 outputs via LLAM-EVAL. GPT-4 shows among the highest LLAM-EVAL scores.",
            "limitations_or_failure_cases": "GPT-4 output used as baseline may not equal true ground truth; some variations in representation of identical pseudofunctions can degrade evaluation consistency; real-world execution of synthesized protocols may still fail due to physical experimental variability.",
            "author_recommendations_or_insights": "Use domain knowledge (predefined actions) to stabilize extraction/generation; consider alternative baselines or manually annotated pseudocode to avoid over-reliance on a single-model baseline.",
            "uuid": "e9194.1",
            "source_info": {
                "paper_title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Cohere+",
            "name_full": "Cohere+ (Cohere platform variant)",
            "brief_description": "A Cohere model variant evaluated as a protocol-formulation LLM; authors report it as one of the stronger non-OpenAI models for generating pseudocode from biology protocols.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Cohere+",
            "model_description": "Cohere model family variant used in experiments; paper does not provide parameter count or training details for Cohere+.",
            "scientific_subdomain": "Molecular / laboratory biology (protocol pseudocode generation)",
            "simulation_task": "Translate natural-language experimental protocols into a sequence of predefined lab-action pseudofunction calls and arguments (pseudocode).",
            "evaluation_metric": "LLAM-EVAL (six criteria) scored by Llama-3; reference-based metrics also measured (BLEU, SciBERTScore, Levenshtein, precision, recall).",
            "simulation_accuracy": "LLAM-EVAL average scores (self/self comparison): 3.94 ± (std) when predefined actions given but protocol not included; 3.98 ± (std) without actions/protocol; 4.50 ± (std) when both predefined actions and original protocol provided (Table 2). Cohere+ is reported as a powerful protocol formulator alongside GPT variants.",
            "factors_affecting_accuracy": "Predefined action set inclusion (large positive effect); order of actions in prompt; prompt structure; dataset/domain coverage; coarse-graining of complex actions.",
            "comparison_baseline": "Compared against GPT-4 baseline pseudocode via LLAM-EVAL; when both predefined actions and protocol provided, Cohere+ achieves near-top LLAM-EVAL scores (close to GPT variants).",
            "limitations_or_failure_cases": "Performance depends on prompt contents; recall decreased in some settings when predefined actions were used; dataset limitations and incomplete action definitions may hamper generalization.",
            "author_recommendations_or_insights": "Domain-specific predefined actions help; further research to understand recall drops and to expand action definitions and dataset size.",
            "uuid": "e9194.2",
            "source_info": {
                "paper_title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Llama-3 (70b)",
            "name_full": "Llama-3 70B",
            "brief_description": "A large Llama-3 model variant used primarily in this paper as an automatic evaluator (LLAM-EVAL) and also evaluated in generation tasks; chosen as the evaluator because it produced high consistency in scoring.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3 70b",
            "model_description": "Llama-3 family 70-billion-parameter variant (explicitly referenced as 70b in experiments); used both as a target model in some comparisons and considered as an evaluator candidate; described as free of cost at time of experiments.",
            "scientific_subdomain": "Molecular / laboratory biology (used for evaluation and also as a generator in comparisons)",
            "simulation_task": "When used as a generator: produce pseudocode from protocols; when used as evaluator (LLAM-EVAL): rate target pseudocode against baseline pseudocode on defined criteria.",
            "evaluation_metric": "When acting as evaluator: LLAM-EVAL scoring on six criteria; when as generator: evaluated using LLAM-EVAL and reference metrics (BLEU, SciBERTScore, Levenshtein, precision/recall).",
            "simulation_accuracy": "As a generator, LLAM-EVAL average (best condition with predefined actions and protocol present) ~4.12 ± (std) (Table 2). As an evaluator, Llama-3 (general) was selected because it achieved high scores across the self/self comparison tasks.",
            "factors_affecting_accuracy": "Model size (70b better than 8b), availability of predefined actions in prompts, prompt ordering, dataset coverage, evaluator biases when used to score outputs.",
            "comparison_baseline": "Compared to GPT baselines via LLAM-EVAL; Llama-3 70b outperforms smaller Llama-3 variants (8b) in LLAM-EVAL generator performance; Llama-3 chosen as evaluator because of self/self comparison results.",
            "limitations_or_failure_cases": "Smaller Llama-3 (8b) shows substantially lower generation scores; potential for evaluator bias and hallucination when Llama-3 is used for scoring; reliance on a single evaluator exposes results to that model's biases.",
            "author_recommendations_or_insights": "Choose evaluators via self-self comparison to ensure numeric outputs and consistency; be aware of evaluator bias and consider testing multiple evaluator models; use domain-specific prompts and predefined actions.",
            "uuid": "e9194.3",
            "source_info": {
                "paper_title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Llama-3 (8b)",
            "name_full": "Llama-3 8B",
            "brief_description": "A smaller 8-billion-parameter Llama-3 variant evaluated in the paper for pseudocode generation; performed noticeably worse than larger models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3 8b",
            "model_description": "Llama-3 family 8-billion-parameter variant used as a target model for pseudocode generation; no additional training details provided.",
            "scientific_subdomain": "Molecular / laboratory biology (protocol pseudocode generation)",
            "simulation_task": "Generate predefined-action-based pseudocode from biological protocols.",
            "evaluation_metric": "LLAM-EVAL (six criteria) scored by Llama-3 evaluator; reference metrics (BLEU, SciBERTScore, Levenshtein, precision/recall) also applied.",
            "simulation_accuracy": "LLAM-EVAL average (best condition with predefined actions and protocol): ~3.08 ± (std) (Table 2), substantially lower than larger Llama-3 70b and GPT variants.",
            "factors_affecting_accuracy": "Smaller model capacity (8b) relative to larger variants; prompt design; presence/absence of predefined actions; dataset/domain complexity.",
            "comparison_baseline": "Compared to GPT variants and larger Llama-3; underperforms relative to Llama-3 70b and GPT-4/GPT-4o/Cohere+ in LLAM-EVAL scoring.",
            "limitations_or_failure_cases": "Lower overall LLAM-EVAL scores; poorer fluency/consistency; may not be suitable for reliable protocol generation without finetuning or augmentation.",
            "author_recommendations_or_insights": "Larger model sizes produce better protocol generation; for practical protocol formulation tasks, prefer larger-capacity models or domain-specialized models.",
            "uuid": "e9194.4",
            "source_info": {
                "paper_title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (OpenAI)",
            "brief_description": "An OpenAI model family member used as a target model for pseudocode generation and evaluated with LLAM-EVAL; shows moderate performance compared to GPT-4 and Cohere+.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "GPT-3.5 family model from OpenAI used for pseudocode generation; the paper does not report parameter count or training data specifics.",
            "scientific_subdomain": "Molecular / laboratory biology (protocol formulation)",
            "simulation_task": "Translate protocol text into a sequence of predefined pseudofunction calls (pseudocode) describing experimental procedures.",
            "evaluation_metric": "LLAM-EVAL six-criterion scoring by Llama-3; reference metrics (BLEU, SciBERTScore, Levenshtein, precision, recall).",
            "simulation_accuracy": "LLAM-EVAL average (best condition with predefined actions and protocol): ~4.39 ± (std) (Table 2).",
            "factors_affecting_accuracy": "Prompting with predefined actions improves scores; prompt/order effects; dataset coverage; evaluator choice.",
            "comparison_baseline": "Compared against GPT-4 baseline pseudocode and other LLMs via LLAM-EVAL; GPT-3.5 performs below GPT-4 but can reach reasonably strong scores with domain prompts.",
            "limitations_or_failure_cases": "Lower performance than GPT-4-family models, some instability without predefined actions; potential hallucinations and representation variance.",
            "author_recommendations_or_insights": "Provision of predefined action set stabilizes outputs; consider finetuning or tool-augmented approaches for better domain fidelity.",
            "uuid": "e9194.5",
            "source_info": {
                "paper_title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Mixtral",
            "name_full": "Mixtral",
            "brief_description": "A commercial/open LLM evaluated as a target model for converting biology protocols into pseudocode; achieves moderate-to-strong LLAM-EVAL scores with domain prompts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mixtral",
            "model_description": "Mixtral model family (specific size/version not given) used as a pseudocode generator in experiments.",
            "scientific_subdomain": "Molecular / laboratory biology (protocol pseudocode generation)",
            "simulation_task": "Generate Python-style pseudocode from biology protocols using a fixed set of lab-action functions.",
            "evaluation_metric": "LLAM-EVAL (six criteria) scored by Llama-3; reference-based metrics (BLEU, SciBERTScore, Levenshtein).",
            "simulation_accuracy": "LLAM-EVAL average (best condition with predefined actions and protocol): ~4.21 ± (std) (Table 2).",
            "factors_affecting_accuracy": "Predefined actions presence; prompt order; model-specific capacity and training; evaluator biases.",
            "comparison_baseline": "Compared to GPT baselines in LLAM-EVAL; Mixtral achieves competitive scores though generally below top GPT variants.",
            "limitations_or_failure_cases": "Potential non-numeric outputs when used as evaluator in some settings (not all models generated numeric scores); performance sensitive to prompt structure.",
            "author_recommendations_or_insights": "Include domain action lists in prompts and test prompt/order effects; consider multiple evaluators for robustness.",
            "uuid": "e9194.6",
            "source_info": {
                "paper_title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Gemma-7b",
            "name_full": "Gemma-7B",
            "brief_description": "A 7-billion-parameter model (Gemma-7b) evaluated as a generator of protocol pseudocode; shows moderate performance when given domain prompts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemma-7b",
            "model_description": "Gemma family 7-billion-parameter variant used in the experiments; exact training details not provided in the paper.",
            "scientific_subdomain": "Molecular / laboratory biology (protocol pseudocode generation)",
            "simulation_task": "Convert natural-language biological protocols to pseudocode using predefined lab actions.",
            "evaluation_metric": "LLAM-EVAL six-criterion scoring by Llama-3; reference metrics for function inputs and names.",
            "simulation_accuracy": "LLAM-EVAL average (best condition with predefined actions and protocol): ~4.02 ± (std) (Table 2).",
            "factors_affecting_accuracy": "Predefined action provision; prompt structure; model capacity; ordering of action definitions relative to protocol.",
            "comparison_baseline": "Compared against GPT-4 generated pseudocode and other LLMs via LLAM-EVAL; Gemma-7b is mid-ranked among tested models.",
            "limitations_or_failure_cases": "Smaller-capacity models show lower overall performance than large GPT variants; potential variability across runs.",
            "author_recommendations_or_insights": "Using predefined action lists and providing domain context raises scores; consider larger models or domain specialization for improved performance.",
            "uuid": "e9194.7",
            "source_info": {
                "paper_title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Gemini-2.0",
            "name_full": "Gemini 2.0 (Google)",
            "brief_description": "A Google Gemini family model evaluated for protocol pseudocode generation; included in the comparative evaluation with LLAM-EVAL.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-2.0",
            "model_description": "Gemini family model (version 2.0) from Google used as a target model; paper does not give parameter counts or training corpus detail.",
            "scientific_subdomain": "Molecular / laboratory biology (protocol formulation)",
            "simulation_task": "Generate pseudocode composed of predefined lab-action calls from natural-language protocols.",
            "evaluation_metric": "LLAM-EVAL (six criteria) via Llama-3; reference metrics.",
            "simulation_accuracy": "LLAM-EVAL average (best condition with predefined actions and protocol): ~4.02 ± (std) (Table 2).",
            "factors_affecting_accuracy": "Predefined actions presence; prompt order; dataset content; evaluator model choice.",
            "comparison_baseline": "Compared to GPT-4 baseline outputs via LLAM-EVAL; Gemini-2.0 attains moderate-to-strong LLAM-EVAL scores, often below top GPT/Cohere+ models.",
            "limitations_or_failure_cases": "Performance sensitive to prompt configurations and domain coverage; some variability across evaluation conditions.",
            "author_recommendations_or_insights": "Providing domain-specific predefined actions improves performance; test different prompt formulations and evaluator models for robustness.",
            "uuid": "e9194.8",
            "source_info": {
                "paper_title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Claude3",
            "name_full": "Claude 3 (Anthropic)",
            "brief_description": "An Anthropic model family member (Claude 3) mentioned as one of the models prompted for pseudocode generation but not fully reported in numeric result tables.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude3",
            "model_description": "Claude 3 family model from Anthropic referenced in Methods as prompted for pseudocode generation; numeric LLAM-EVAL results not fully reported in main tables.",
            "scientific_subdomain": "Molecular / laboratory biology (protocol pseudocode generation)",
            "simulation_task": "Generate pseudocode from biology protocols using predefined actions.",
            "evaluation_metric": "LLAM-EVAL (six criteria) intended for evaluation; however numeric scores for Claude3 are not provided in the paper's main result tables.",
            "simulation_accuracy": null,
            "factors_affecting_accuracy": "Not explicitly quantified in paper due to missing numeric results; expected factors include predefined actions, prompt ordering, and evaluator biases as for other models.",
            "comparison_baseline": "Was compared conceptually against other LLMs but specific numeric comparisons in LLAM-EVAL are not shown for Claude3 in the main tables.",
            "limitations_or_failure_cases": "Paper notes that some models did not produce numeric evaluator outputs in certain evaluation tasks; Claude3's numeric performance not reported clearly.",
            "author_recommendations_or_insights": "When including multiple vendor models, ensure evaluator prompts and settings yield numeric outputs; evaluate with multiple evaluators to avoid missing/ambiguous evaluation outputs.",
            "uuid": "e9194.9",
            "source_info": {
                "paper_title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-3",
            "name_full": "GPT-3 (OpenAI)",
            "brief_description": "An earlier OpenAI LLM which the authors prompted for pseudocode generation (listed among models used), though the main reported numeric results emphasize later models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "GPT-3 family model referenced in Methods as one of the models prompted to generate pseudocode; the paper does not present detailed per-model numeric results for every GPT-3 configuration.",
            "scientific_subdomain": "Molecular / laboratory biology (protocol pseudocode generation)",
            "simulation_task": "Convert protocol text into sequences of predefined lab-action pseudofunctions and arguments.",
            "evaluation_metric": "LLAM-EVAL (six criteria) and reference-based metrics; detailed numeric outcomes for GPT-3 are not prominently reported in main tables.",
            "simulation_accuracy": null,
            "factors_affecting_accuracy": "Same factors noted for other models (predefined actions, prompt ordering, dataset coverage); specifics not reported for GPT-3 in main tables.",
            "comparison_baseline": "Included in comparative experiment set but not emphasized in primary performance claims; GPT-3 generally expected to trail GPT-3.5/GPT-4 variants.",
            "limitations_or_failure_cases": "Detailed numeric performance not shown; older generation models may underperform on nuanced protocol generation compared to later models.",
            "author_recommendations_or_insights": "Use newer instruction-tuned models and domain prompts; include action lists to improve extraction/generation fidelity.",
            "uuid": "e9194.10",
            "source_info": {
                "paper_title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BioPlanner: Automatic evaluation of LLMs on protocol planning in biology",
            "rating": 2,
            "sanitized_title": "bioplanner_automatic_evaluation_of_llms_on_protocol_planning_in_biology"
        },
        {
            "paper_title": "Protocode: Leveraging large language models (LLMs) for automated generation of machine‑readable PCR protocols from scientific publications",
            "rating": 2,
            "sanitized_title": "protocode_leveraging_large_language_models_llms_for_automated_generation_of_machinereadable_pcr_protocols_from_scientific_publications"
        },
        {
            "paper_title": "LLMs can generate robotic scripts from goal‑oriented instructions in biological laboratory automation",
            "rating": 2,
            "sanitized_title": "llms_can_generate_robotic_scripts_from_goaloriented_instructions_in_biological_laboratory_automation"
        },
        {
            "paper_title": "Automated extraction of chemical synthesis actions from experimental procedures",
            "rating": 2,
            "sanitized_title": "automated_extraction_of_chemical_synthesis_actions_from_experimental_procedures"
        },
        {
            "paper_title": "G‑Eval: NLG evaluation using GPT‑4 with better human alignment",
            "rating": 2,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        }
    ],
    "cost": 0.0196655,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation
11 Apr 2025</p>
<p>Seungjun Yi 
Department of Biomedical Engineering
University of Texas at Austin</p>
<p>Korea Institute of Science and Technology (KIST) Europe</p>
<p>Jaeyoung Lim 
Department of Computer Science and Engineering
Ulsan National Institute of Science and Technology</p>
<p>Korea Institute of Science and Technology (KIST) Europe</p>
<p>Juyong Yoon juyong.yoon@kist-europe.de 
Korea Institute of Science and Technology (KIST) Europe</p>
<p>ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation
11 Apr 2025895FB3CFD91D205F23A90ADE1BE2426CarXiv:2410.04601v2[cs.CL]
Automated generation of scientific protocols executable by robots can significantly accelerate scientific research processes.Large Language Models (LLMs) excel at Scientific Protocol Formulation Tasks (SPFT), but the evaluation of their capabilities rely on human evaluation.Here, we propose a flexible, automatic framework to evaluate LLMs' capability on SPFT: ProtoMed-LLM 1 .This framework prompts the target model and GPT-4 to extract pseudocode from biology protocols using only predefined lab actions and evaluates the output of target model using LLAM-EVAL, the pseudocode generated by GPT-4 serving as a baseline and Llama-3 acting as the evaluator.Our adaptable prompt-based evaluation method, LLAM-EVAL, offers significant flexibility in terms of evaluation model, material, criteria, and is free of cost.We evaluate GPT variations, Llama, Mixtral, Gemma, Cohere, and Gemini.Overall, we find that GPT and Cohere is a powerful scientific protocol formulators.We also introduce BIOPROT 2.0, a dataset with biology protocols and corresponding pseudocodes, which can aid LLMs in formulation and evaluation of SPFT.Our work is extensible to assess LLMs on SPFT across various domains and other fields that require protocol generation for specific goals.Previous work suggests a framework to assess the capabilities of LLMs on SPFT: BioPlanner (O' Donoghue et al., 2023).This method outlines three primary steps: (i) extracting pseudofunctions</p>
<p>Introduction</p>
<p>Laboratory automation is essential for accelerating scientific research processes.However, most contemporary laboratories use manual labor, especially in the field of biology.This not only constrains the scope for scalability, but also introduces potential vulnerabilities in reproducibility (Kwok, 2010).</p>
<p>One of the barriers for automation in biology is the reliance on manual experiments when validating scientific protocols.Traditionally, trial-anderror approach has been employed to formulate A protocol containing a title, descriptions, stepby-step instructions, and predefined biology lab actions is given to both a target model and GPT-4 for pseudocode generation.Then, Llama-3 evaluates these outputs considering the target model's pseudocode as the prediction (ŷ) and GPT-4's as a baseline (y).</p>
<p>a protocol to achieve a certain goal.As a breakthrough, LLMs have demonstrated remarkable capabilities in formulating precise experimental protocols across diverse fields (White et al., 2023;Jablonka et al., 2023).These protocols comprise pseudocodes with actionable sequences that can be executed by machines which can be automated.Yet, efforts in biology to utilize LLMs for pseudocode formulation are yet to achieve desired outcomes (Inagaki et al., 2023).These works rely on human evaluations, and objective evaluation methods for protocol formulation are nonexistent.Therefore, it is necessary to establish an automated evaluation framework on formulating protocols to move beyond manual labor.and pseudocode2 from a protocol using an evaluator, (ii) using the target model to produce pseudocode given the pseudofunctions, and (iii) evaluating the pseudocode generated in step (ii) against the original pseudocode in (i).Using this framework, they performed evaluation exclusively on GPTs (Brown et al., 2020;OpenAI, 2023).</p>
<p>We highlight the following key observations: (1) Various representations of pseudofunctions corresponding to identical experimental actions, causes performance degradation and inconsistency of the evaluation framework.(2) The repertoire of actions executed in biology labs is confined to a finite set of actions.(3) High values in traditional automatic metrics (i) does not necessarily imply human-perceived good quality in scientific protocols.(4) The use of automatic metrics (i) requires manual labor, which limits the transition to fully automatic evaluation.</p>
<p>Here, we propose an evaluation framework that evaluates the capabilities of LLMs in SPFT: ProtoMed-LLM (Figure 1).</p>
<p>First, we define a set of actions in advance (Table 1), which eliminates individual action (pseudofunction) extraction step and variations of actions on each occasion.Second, we independently zeroshot prompted the target model and GPT-4 (Ope-nAI, 2023) to extract pseudocode from biology protocols, only using predefined actions as pseudofunctions.Lastly, we use LLAM-EVAL to evaluate the response, treating the target model's pseudocode as a prediction (ŷ) and that of GPT-4's as a baseline (y).LLAM-EVAL offers significant flexibility in terms of evaluation model, material, and criteria.This approach is inspired by the automated extraction of chemical synthesis actions from experimental procedures3 (Vaucher et al., 2020).We compared multiple LLMs to our framework, including GPT variations (Brown et al., 2020;OpenAI, 2023), Llama, Mixtral, Gemma, Cohere, andGemini (Google, 2024).We find that GPT-4o and Co-here+ is a powerful scientific protocol formulator.</p>
<p>We also introduce BIOPROT 2.0, a larger dataset with scientific protocols and the corresponding pseudocodes that can aid LLMs in formulation and evaluation of SPFT.</p>
<p>Overall, we make the following contributions:</p>
<ol>
<li>We propose ProtoMed-LLM: a flexible, automatic framework for evaluating LLMs on SPFT using domain knowledge and LLAM-EVAL.2. We propose LLAM-EVAL, an evaluation method that uses a form-filling paradigm offering significant flexibility in terms of evaluation model, material, and criteria.3. We introduce the BIOPROT 2.0 dataset, featuring protocols and corresponding pseudocode for evaluating and aiding LLMs on SPFT.</li>
</ol>
<p>Related Works</p>
<p>Task-specific Evaluation LLMs have been evaluated based on their performance in specific tasks.Information extraction abilities were measured by the generated quality of summaries (Durmus et al., 2020;Wang et al., 2020), paper reviews (Zhou et al., 2024), question correction (Fan et al., 2024), or combination of a few tasks (Labrak et al., 2024).However, these studies do not provide comprehensive evaluations and only assess very limited aspects, thus limiting their generalizability to other abilities or tasks.</p>
<p>LLM Evaluation on SPFT Recent work proposes a three-step framework (Section 1) for the evaluation of scientific protocols in biology: BioPlanner (O' Donoghue et al., 2023).This work evaluates GPT's performance in three tasks: next-step prediction, pseudocode generation, and pseudofunction retrieval.It employs statistical scoring methods including Levenshtein distance (L d ) and BLEU (Papineni et al., 2002) to measure the relevance between a baseline and generated protocols, despite their modest correlation with human judgments.</p>
<p>Domain-specific LLMs in Science</p>
<p>A Large number of LLMs have been trained, finetuned, or augmented for domain-specific uses.ChemBERTa/-2 (Chithrananda et al., 2020;Ahmad et al., 2022), MatSciBERT (Gupta et al., 2021), MaterialsBERT (Shetty et al., 2023), Chemcrow (Bran et al., 2023), and LLM augmentation methods for various experiment-related tasks (Guo et al., 2023) has been introduced in chemistry.</p>
<p>BioGPT (Luo et al., 2022), BioBERT (Lee et al., 2019), CamemBERT-bio (Touchent et al., 2024), BlueBERT (Peng et al., 2019), PubmedBERT (Gu et al., 2020), BioMegatron (Shin et al., 2020), and</p>
<p>Action Name Description</p>
<p>Transfer</p>
<p>Move substances between containers using lab equipment, such as pipettes.</p>
<p>Centrifuge</p>
<p>Spin at high speed to separate mixture components by density.</p>
<p>Vortex</p>
<p>Mix solutions by creating a vortex for even distribution.</p>
<p>SetTemp</p>
<p>Set specific temperatures for reactions or processes.</p>
<p>Wait</p>
<p>Period of inactivity to allow reactions or condition stabilization.</p>
<p>Wash</p>
<p>Rinse materials, often with solvents to remove contaminants.</p>
<p>Measure</p>
<p>Quantify substances or properties using instruments.</p>
<p>Microscopy</p>
<p>Use a microscope to observe and analyze cell morphology and structures.</p>
<p>CellDetachment</p>
<p>Release adherent cells from a culture surface using enzymatic or mechanical methods.</p>
<p>CellCount</p>
<p>Determine the number of cells in a sample using a hemocytometer or automated counter.</p>
<p>InvalidAction</p>
<p>Undefined action due to documentation error or ambiguity.</p>
<p>OtherLanguage</p>
<p>Text in non-English, indicating translation need.</p>
<p>NoAction</p>
<p>Text not corresponding to any defined action.</p>
<p>PCR</p>
<p>Amplify DNA segments through Polymerase Chain Reaction.</p>
<p>Gel</p>
<p>Separate molecules by size in a gel with electric field.</p>
<p>Culture</p>
<p>Grow cells in lab to study behavior or for experimentation.</p>
<p>Dilute</p>
<p>Reducing the concentration of a solution by adding solvent.ProtoCode (Jiang et al., 2024) has been introduced in biology.</p>
<p>Evaluating LLMs with LLMs Evaluation of LLMs encompasses a dual-method approach:</p>
<p>(i) Statistical scoring: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), Levenshtein Distance (ii) Model-based scoring: G-Eval (Liu et al., 2023), Prometheus (Kim et al., 2023), BLEURT (Sellam et al., 2020), Natural Language Inference (NLI) (iii) Combination of (i) and (ii): GPTScore (Fu et al., 2023), SelfCheckGPT (Manakul et al., 2023), BERTScore (Zhang et al., 2020), SciBERTScore (O'Donoghue et al., 2023), WMD (Kusner et al., 2015), MoverScore (Zhao et al., 2019), Question Answer Generation (QAG) Score</p>
<p>In tasks where reasoning is involved, (ii)(iii) outperforms (i).Previous work adopted (i) with (iii) being minimal (O'Donoghue et al., 2023).In this work, we adopt the notion of G-Eval (Liu et al., 2023), a framework for evaluating LLM-generated text, which prompts GPT with text and criteria, then scores based on its output.</p>
<p>Methods</p>
<p>The ProtoMed-LLM framework can evaluate the capability of LLMs on SPFT in three steps (Figure 2):</p>
<p>(1) prompt the target LLM to generate pseudocode based on the given protocol, (2) repeat previous step for GPT-4, and (3) LLAM-EVAL for evaluation.To utilize this framework, we curated protocols in biology (Section 3.1), predefined actions performed in biology labs (Section 3.2), prompted LLMs for pseudocode generation (Section 3.3), and prompted Llama-3 for evaluation (LLAM-EVAL) (Section 3.6).</p>
<p>Data Curation of Protocols in Biology</p>
<p>Each protocol is composed of three core elements: a title, description, and experimental steps.We curated the dataset through a process of collection and refinement.We collected a set of keywords relevant to biology.Then, we used a scoring system based on the number of keywords included in the description of each protocol from protocols.io4(Teytelman et al., 2016).We refined the dataset collected in the previous step using automated and manual methods.(Appendix A.1.)</p>
<p>Defining Actions</p>
<p>The defined actions are composed of two parts: (i) basic actions corresponding to a single action which can be performed directly in biology labs, and (ii) coarse-grained actions which corresponds to a large set of basic actions repeated throughout various protocols.Defined actions were reviewed by experts with intensive experiences in biology experiments.The target model specifies the arguments for each action on each occasion.</p>
<p>Basic Actions Since the repertoire of actions executed in biology labs is confined to a finite set of actions, we defined a set of actions performed in biology labs prior to the extraction of pseudocode from protocols (Table 1).We performed a comprehensive literature review to define the set of basic actions performed in biology labs.</p>
<p>Coarse-grained Actions</p>
<p>We observed that a series of complex, repetitive actions can be effectively encapsulated and described by a single, comprehensive action.For instance, the process of diluting a solution is conceptually straightforward and can possibly defined by basic actions.However, this involves intricate calculations and logical reasoning, which can result in performance degradation by calculation mistakes and posing variations in representations of an identical process.To this end, we coarse-grained these complex set of actions into a singular action.</p>
<p>Prompting Pseudocode Generation</p>
<p>To evaluate the target LLMs on SPFT, we prompted the models to generate pseudocode based on a protocol collected at Section 3.1.Models are instructed to use only the actions defined in Section 3.2 as the function name.However, they were allowed to define the arguments for each pseudofunction as needed for each occasion.If applicable, the fixed prompt, including the instructions and predefined actions, was provided in the system message, while the protocol was included in the user message.In this work, we prompted GPT-3 (Brown et al., 2020), GPT-4 (OpenAI, 2023), Gemini (Google, 2024), Claude3 (Anthropic, 2023), and Cohere.Below is the prompt for generating pseudocode based on the given protocol.Note that actions and corresponding descriptions presented in Table 1 are placed at {actions}.</p>
<p>You are an AI that generates Python pseudocode for biology protocols.This pseudocode must accurately describe a complete scientific protocol to obtain a result.You will be provided with the title, description, and steps of the biology protocol, and your task is to convert it to Python pseudocode.</p>
<p>You may define the arguments on your own.You must ONLY use these functions.</p>
<p>{actions} Do NOT provide any captions.ONLY present the pseudocode and pseudofunctions used inside the code.Present the pseudofunctions at the beginning and then the pseudocode.Do NOT provide any descriptions inside the code.</p>
<p>title: {title} description: {description} steps: {steps}</p>
<p>Metrics and Evaluation</p>
<p>We observe that using automatic metrics (i) necessitates manual annotation of functions and pseudocodes each time, which significantly hampers the automation of the evaluation process.Moreover, evaluating the function and input5 separately falls short of flexible and comprehensive evaluation in a protocol manner.</p>
<p>To this end, we propose LLAM-EVAL, an automatic, flexible prompt-based framework to evaluate the quality of LLM responses.This framework requires three elements: two input texts (one serving as the baseline and the other as the target) and an evaluator LLM: Llama-36 .This method encompasses predefining a set of scores7 S = {s 1 , s 2 , ..., s n }, prompting Llama-3 to rate the outcomes of a target LLM with that of GPT-4 in the scale of S, calculating the probability of each score p(s i ), and calculating the final score as following.This method is inspired by G-Eval (Liu et al., 2023).
score = n i=1 s i p(s i )
Llama-3 is prompted to evaluate according to one criterion at a time.The original prompts targeting summarizing tasks are modified to perform evaluation on SPFT.In this work, we evaluate the pseudocode generated by the target LLM based on six criteria: the four original criteria used in G-Eval (Liu et al., 2023) (Coherence, Consistency, Fluency, and Relevance) and two criteria we propose (Precision, and Coverage), considering the context of SPFT.For example, the definition of Coherence is:</p>
<p>Coherence (1-5) -the overall quality of all lines in the pseudocode.The target pseudocode should not be a rough overview but should provide a precise description of a baseline pseudocode.</p>
<p>The definitions of other criteria in prompts can be found at Appendix A.2.To automatically implement chain-of-thoughts (CoT) in the evaluation process, we instructed GPT-4 to create specific evaluation steps for each criterion.GPT is capable of producing these evaluation steps by itself (Liu et al., 2023).GPT-4 was given a task and evaluation criteria, then prompted to generate the evaluation steps using a form-filling paradigm.An example prompt containing GPT-4 generated instructions for evaluation can be found at Appendix A.2.We also implemented an automatic feedback loop to regenerate the response up to five or ten times if the output did not contain scores.We evaluated using two baselines: the GPT-generated pseudocode and the original protocol.</p>
<p>This approach is not constrained by the output structure of the target models, eliminates the need for manual annotation efforts during the parsing process as required in reference-based metrics, enables a comprehensive evaluation, and thereby makes ProtoMed-LLM significantly more flexible and automatic.</p>
<p>To ensure compatibility, we also use conventional reference-based metrics: Normalized Levenshtein distance (L dn ) for function names, BLEU (Papineni et al., 2002)
SciBERTScore = 1 N N i=0 ⟨E(a pred i ), E(a BL i )⟩ ∥E(a pred i )∥∥E(a BL i )∥</p>
<p>Evaluator LLM Selection</p>
<p>To select a specific LLM as an evaluator, we propose self-self comparison task as a baseline, where an LLM generates a pseudocode8 for a protocol and then evaluates the score using the same LLM against the generated pseudocode.For example, this means evaluating GPT-4 generated pseudocode against the same pseudocode using GPT-4.Our assumption was that the score should be close to the maximum 9 when the baseline and target pseudocode are the same.Our goal was to select the model with the best results as the evaluator.We evaluated each model based on six criteria in Section 3.4.More details in Appendix A.3.While using G-EVAL, we encountered instances where the output was a sentence instead of a score (number).To address this issue, we modified the parameters, dataset, and prompts.Further details are in Appendix A.4.</p>
<p>Evaluating LLMs using LLAM-EVAL</p>
<p>Using LLAM-EVAL, we evaluate across three tasks for each model: (1) GPT-4 generated pseudocode as a baseline with predefined actions given in prompt, (2) the same task with no predefined actions, (3) the original protocol as a baseline with predefined actions.We evaluate GPT variations (Brown et al., 2020;OpenAI, 2023), Llama, Mixtral, Gemma, Cohere, and Gemini (Google, 2024).Details are in Appendix A.5.</p>
<p>Implementation Details</p>
<p>To ensure a fair evaluation of LLMs, we considered additional factors that may affect performance and present several settings.We consider that LLMs tend to perform better when the actions are presented in the same order as in the protocol.While previous work extracted different actions from each protocol10 , we predefined the actions which is equivalent to shuffling.</p>
<p>Analysis</p>
<p>Evaluator LLM Selection</p>
<p>Llama-3 achieved the highest scores across all six tasks, while there were small differences across models (Table 2).We chose Llama-3 as an evaluator, which is free of cost to date.Note that evaluations for other models not presented in the table were not feasible, as numerical responses were not generated.More details are in Appendix A.3.</p>
<p>Evaluating LLMs on SPFT</p>
<p>Our results show that GPT-4o and Cohere+ is a powerful protocol formulator (Table 3).We found our work compatible to previous work (O'Donoghue et al., 2023).</p>
<p>Is applying domain knowledge an effective strategy for evaluation?We applied domain knowledge by predefining the finite set of actions performed in biology labs.To evaluate the efficacy of this method, we compare the responses generated with predefined actions included in the prompts to those generated without them (Table 4).The performance is enhanced for most models, with the exception of the Recall.Further research should be conducted to explore these findings.</p>
<p>Can the original protocol itself serve as a baseline?Evaluation of LLMs in SPFT in previous work requires manual processes and pseudocode extraction step in SPFT.However, evaluation using the original protocol itself completely eliminates the manual processes of pseudofunction evaluation and the GPT-generated pseudocode extraction step, thereby enhancing flexibility and automation.To this end, we evaluate using the original protocol as a baseline.While scores obtained using this approach is not close to the maximum score (Table 3), we observe that the relative ranking of the models remains relevant to the results of using the pseudocode as a baseline.</p>
<p>Will LLM as an evaluator prefer responses from itself?It is reported that LLM as an evaluator prefer responses from itself over human responses in text summarization tasks (Liu et al., 2023).Therefore, a potential concern is that the evaluator may prefer outputs from itself regardless of its quality.While results in Table 2 and 4 address this concern, Table 3 shows that Llama-3 as an evaluator does not prefer its outputs over that of GPT-4.Our results suggest that GPT's preference for its own responses in previous work (Liu et al., 2023) may be a phenomenon unique to GPT.</p>
<p>The BIOPROT 2.0 Dataset</p>
<p>We introduce BIOPROT 2.0, a dataset with scientific protocols and the corresponding pseudocodes with a larger number of datapoints.Previous work highlights that a dataset with these two components can aid protocol formulation of LLMs (O'Donoghue et al., 2023).The pseudocode extracted from protocols are only composed of pseudofunctions (actions) predefined above the previous step, as each model was prompted to use only the provided functions but to define the arguments on their own.The summary of generated pseudocode are in Table 5.This dataset can be used to formulate scientific protocols to achieve a prompted goal using a toolformer like (Schick et al., 2023) chain-of-thought LLM agent (Wei et al., 2023).</p>
<p>Conclusion</p>
<p>We introduce ProtoMed-LLM, a flexible and automatic framework designed to evaluate LLMs' capabilities on Scientific Protocol Formulation Tasks 5.00 ± 0.02 5.00 ± 0.00 5.00 ± 0.00 5.00 ± 0.00 5.00 ± 0.00 5.00 ± 0.06 ✓ ✗ 5.00 ± 0.00 5.00 ± 0.00 5.00 ± 0.08 5.00 ± 0.00 4.99 ± 0.11 5.00 ± 0.00 5.00 (Baseline) ✗ ✗ 5.00 ± 0.00 5.00 ± 0.00 5.00 ± 0.04 5.00 ± 0.03 5.00 ± 0.03 5.00 ± 0.00 5.00 Table 3: ProtocoLLM Evaluation Results of three tasks for each model: (1) GPT-4 generated pseudocode as a baseline with predefined actions given in prompt, (2) the same task with no predefined actions, (3) the original protocol as a baseline with predefined actions.'Ac' and 'Pr' represent whether the predefined actions and the original protocol were given for evaluation, respectively.We report the mean, standard deviation, and average of scores over five runs.The best and second best performance besides a baseline (GPT-4) for each criterion and task is bolded and underlined, respectively.The scores range from a minimum of 1 to a maximum of 5. Higher values for all metrics represent better performance.(SPFT).This framework prompts the target model and GPT-4 to extract pseudocode from biology protocols using only predefined lab actions, then evaluates the target model's output using LLAM-EVAL, with the GPT-4 generated pseudocode as a baseline and Llama-3 as the evaluator.Our prompt-based evaluation method, LLAM-EVAL, provides significant flexibility in terms of evaluation models, materials, criteria, and is free of cost.We assess various models, including GPT variants, Llama, Mixtral, Gemma, Cohere, and Gemini, and find GPT and Cohere to be particularly effective in formulating scientific protocols.Additionally, we present BIO-PROT 2.0, a dataset containing biology protocols and corresponding pseudocodes, which supports LLMs in the formulation and evaluation of SPFT.Our work is extensible to the assessment of LLMs on SPFT across various domains and other fields that require protocol generation for specific goals.
✓
We recognize several limitations.The predefined actions may not encompass all actions performed in a biology labs.The definitions of predefined actions may be incomplete.To precisely define an action, it is necessary to define not only the function but also the function arguments.The number of protocols in BIOPROT 2.0 may be insufficient for evaluation purposes.The performance of ProtoMed-LLM may decline outside of biology.Addressing this requires redefining domain-specific actions and exploring other LLMs for diverse fields.Future work should investigate these cross-disciplinary implications.LLMs are continuously evolving due to regular updates.The LLMs used for evaluation in this work might become unavailable in the future.Upgraded versions of LLMs may result in performance degradation and metrics may differ from those obtained using previous models.Due to selecting Llama-3 as the evaluator, our results may be susceptible to its biases and hallucinations.The outcomes when evaluated with models other than Llama-3 are unknown.Future work should investigate the outcomes using different LLMs as an evaluator.Using an API of LLMs as an evaluator, such GPT, is often not free of charge and can be costly.We used GPT-4 generated responses as a baseline; however, it may not accurately represent the ground truth.Future work should explore the implications of employing alternative resources (e.g., manually annotated pseudocodes, responses generated by other models) as the baseline.We observed basic actions classified as NoAction in minor cases.It has been reported that GPT prefers outputs from LLMs, which also produced our evaluation materials including all ground truth and target pseudocodes.This can potentially influence the scores.The four criteria mentioned in G-Eval may not sufficiently fulfill the role of evaluating protocols where real-world validation is crucial.Also, applying these criteria originally designed for summarization tasks may be inappropriate for evaluating SPFT.Even if the protocol pseudocode is successfully synthesized, real-world experiments may fail depending on the person performing the protocol or the condition of the physical equipment, especially in cases that are more complex than stem cell culture or require delicate manual work and experience.</p>
<p>Ethical Considerations</p>
<p>The use of manually verified protocols in LLMs is strictly prohibited for generating false protocols on platforms like STAR Protocols (Cell Press) and Nature Protocols.Numerous sites also prohibit the use of these protocols in conjunction with any form of AI tool.Our framework can be applied to the protocols of these sites.Although we have endeavored to exclude protocols that can create dangerous substances, there remains the potential for generating protocols that inadvertently produce hazardous products or byproducts.</p>
<p>Figure 1 :
1
Figure 1: Overview of the ProtoMed-LLM Framework.A protocol containing a title, descriptions, stepby-step instructions, and predefined biology lab actions is given to both a target model and GPT-4 for pseudocode generation.Then, Llama-3 evaluates these outputs considering the target model's pseudocode as the prediction (ŷ) and GPT-4's as a baseline (y).</p>
<p>Figure 2 :
2
Figure 2: The ProtoMed-LLM Framework.</p>
<p>Table 1 :
1
Predefined Set of Actions.List of actions performed in biological experiments and the corresponding descriptions.Actions above the line represent the basic actions, with the last three specifically designated for instances where a new protocol introduces an undefined action.Actions below represent the coarse-grained actions.</p>
<p>, precision, recall, and SciBERTScore (O'Donoghue et al., 2023) for function inputs.SciBERTScore is calculated using the encoded predicted E(a</p>
<p>pred i ) and baseline values E(a BL i ) using the SciBERT (Beltagy et al., 2019) sentence encoder E.</p>
<p>Table 2 :
2
Self-Self Comparison Task Results: We report the mean and standard deviation of scores over five or ten runs.Values in bold indicate the highest scores for each criterion.Higher values for all metrics represent better performance.Note that a larger dataset was used for this task.Details in Appendix A.3.
PromptOriginal CriteriaNew CriteriaModelsAc PrCoherence ConsistencyFluencyRelevancePrecisionCoverageAverageGPT-4o✓✗ 4.10 ± 0.79 3.80 ± 0.85 3.86 ± 0.67 4.32 ± 0.71 4.02 ± 0.65 4.26 ± 0.734.06✗✗ 4.28 ± 0.50 3.94 ± 0.64 4.04 ± 0.37 4.45 ± 0.54 4.18 ± 0.41 4.39 ± 0.504.21✓ ✓ 4.29 ± 0.57 4.73 ± 0.50 4.42 ± 0.53 4.75 ± 0.48 3.90 ± 0.48 4.67 ± 0.564.46GPT-4</p>
<p>✓ 4.32 ± 0.53 4.70 ± 0.58 4.53 ± 0.51 4.75 ± 0.44 3.99 ± 0.29 4.67 ± 0.48
4.49GPT-3.5✓✗ 3.61 ± 0.97 3.51 ± 1.02 3.58 ± 0.85 4.11 ± 0.78 3.82 ± 0.73 3.90 ± 0.833.75✗✗ 3.83 ± 0.82 3.71 ± 0.81 3.76 ± 0.68 4.19 ± 0.64 3.96 ± 0.57 3.97 ± 0.713.90✓ ✓ 4.13 ± 0.65 4.76 ± 0.49 4.48 ± 0.52 4.69 ± 0.49 3.79 ± 0.58 4.49 ± 0.674.39Llama3-8b✓✗ 2.25 ± 1.00 1.93 ± 0.99 2.27 ± 0.83 2.39 ± 1.08 2.61 ± 0.96 2.56 ± 1.092.33✗✗ 2.90 ± 0.89 2.69 ± 0.92 3.02 ± 0.88 3.41 ± 0.81 3.47 ± 0.70 3.19 ± 0.823.12✓ ✓ 2.80 ± 1.02 3.00 ± 1.26 3.10 ± 1.00 3.39 ± 1.09 2.93 ± 0.92 3.27 ± 1.063.08Llama3-70b ✓✗ 3.61 ± 0.94 3.14 ± 1.10 3.53 ± 0.82 3.73 ± 0.97 3.72 ± 0.70 3.77 ± 0.793.58✗✗ 3.98 ± 0.64 3.72 ± 0.75 3.92 ± 0.49 4.20 ± 0.57 4.03 ± 0.36 4.09 ± 0.533.99✓ ✓ 4.02 ± 0.75 4.17 ± 0.98 4.15 ± 0.66 4.37 ± 0.74 3.78 ± 0.59 4.25 ± 0.694.12Mixtral✓✗ 3.41 ± 1.03 2.90 ± 1.13 3.57 ± 0.83 3.36 ± 1.14 3.68 ± 0.77 3.54 ± 0.933.41✗✗ 3.95 ± 0.68 3.68 ± 0.79 3.94 ± 0.53 4.18 ± 0.66 4.05 ± 0.43 4.00 ± 0.613.97✓ ✓ 4.06 ± 0.69 4.32 ± 0.84 4.28 ± 0.59 4.37 ± 0.71 3.88 ± 0.44 4.31 ± 0.704.21Gemma-7b✓✗ 3.06 ± 0.97 2.81 ± 1.03 3.47 ± 0.86 3.52 ± 0.93 3.55 ± 0.78 3.19 ± 0.893.27✗✗ 2.93 ± 0.85 2.66 ± 0.88 3.63 ± 0.76 3.61 ± 0.71 3.66 ± 0.61 3.06 ± 0.803.26✓ ✓ 3.81 ± 0.75 4.13 ± 0.83 4.25 ± 0.61 4.26 ± 0.75 3.76 ± 0.60 3.94 ± 0.794.02Cohere+✓✗ 3.95 ± 0.74 3.63 ± 0.87 3.87 ± 0.60 4.11 ± 0.74 3.98 ± 0.50 4.07 ± 0.633.94✗✗ 3.97 ± 0.60 3.71 ± 0.73 3.95 ± 0.46 4.15 ± 0.56 4.03 ± 0.38 4.04 ± 0.503.98✓ ✓ 4.44 ± 0.52 4.63 ± 0.61 4.53 ± 0.52 4.73 ± 0.47 4.04 ± 0.30 4.66 ± 0.494.50Cohere✓✗ 3.51 ± 0.91 3.06 ± 1.02 3.56 ± 0.74 3.66 ± 0.87 3.71 ± 0.63 3.70 ± 0.763.53✗✗ 3.71 ± 0.68 3.44 ± 0.83 3.83 ± 0.56 4.05 ± 0.53 3.94 ± 0.41 3.84 ± 0.563.80✓ ✓ 3.98 ± 0.63 4.11 ± 0.87 4.14 ± 0.51 4.29 ± 0.63 3.83 ± 0.48 4.24 ± 0.644.10Gemini-1.0✓✗ 2.77 ± 1.09 2.30 ± 1.08 2.90 ± 0.95 2.80 ± 1.10 3.13 ± 0.92 3.15 ± 1.012.84✗✗ 3.46 ± 0.93 3.22 ± 1.01 3.59 ± 0.83 3.89 ± 0.77 3.80 ± 0.69 3.66 ± 0.793.60✓ ✓ 3.37 ± 0.93 3.68 ± 1.11 3.73 ± 0.80 3.87 ± 0.87 3.42 ± 0.78 3.86 ± 0.843.66Gemini-2.0✓✗ 3.09 ± 1.05 2.53 ± 1.10 3.75 ± 0.70 2.98 ± 1.08 3.63 ± 0.73 3.43 ± 0.893.24✗✗ 3.88 ± 0.82 3.61 ± 0.91 4.11 ± 0.60 4.13 ± 0.73 4.14 ± 0.54 3.93 ± 0.733.97✓ ✓ 3.80 ± 0.80 3.95 ± 0.97 4.30 ± 0.58 4.18 ± 0.72 3.80 ± 0.49 4.11 ± 0.684.02Gemini-1.5✓✗ 3.02 ± 1.05 2.48 ± 1.02 3.10 ± 0.93 2.97 ± 1.07 3.32 ± 0.84 3.42 ± 0.933.05✗✗ 4.12 ± 0.66 3.86 ± 0.72 4.03 ± 0.55 4.33 ± 0.62 4.13 ± 0.50 4.21 ± 0.594.11✓ ✓ 3.34 ± 0.95 3.62 ± 1.04 3.76 ± 0.76 3.81 ± 0.86 3.36 ± 0.77 3.80 ± 0.843.61</p>
<p>Table 4 :
4
Evaluation Results Using Reference-Based Metrics.Comparison with and without predefined actions given in prompts.We report mean and standard deviation of scores.The best and second best performance for each criterion is bolded and underlined, respectively.Except for L dn , higher values for all metrics represent better performance.
StatisticValue (m ± σ)# of protocols300Tokens / protocol812.3 ± 469.9# of steps14.81 ± 10.74Tokens / step54.28 ± 42.41Tokens / description139.0 ± 135.7Tokens / generated pseudocode623.8 ± 223.2# of lines / generated pseudocode83.06 ± 28.89# of pseudofunctions / edited pseudocode 10.28 ± 6.582</p>
<p>Table 5 :
5
Statistics of BIOPROT 2.0.'EditedPseudocode'refers to the pseudocode that was reformatted, while preserving its content, to obtain the scores presented in Table4.</p>
<p>The dataset and code are available here. *Corresponding Authors
Pseudofunctions represent laboratory actions, while pseudocode embodies protocols composed of these pseudofunc-
tions.3  A set of actions in chemistry labs were defined prior to the pseudocode extraction process.
A platform for reproducible protocol sharing provides access to more than 15k publicly available protocols, and has no limitations regarding the use of LLMs.
Input refers to the function parameters and arguments.</p>
<p>s1=1 and sn=5 is set in this work.
Pseudocode with pseudofunctions defined at the beginning to be precise.
In previous work, this required shuffling, as LLMs presented the pseudofunctions in the same order as in the protocol.
AcknowledgementsWe would like to thank Karim Md.Adnan for assisting us with the action defining process.This research was supported by a KIST project (2E32351) and Bio-Cluster Industry Capacity Enhancement Project of Jeonbuk Technopark (JBTP)AppendixA BIOPROT 2.0A.1 Data CurationWe used protocols.io(Teytelman et al., 2016)API for data collection.Protocols of 1 ≤ score ≤ 5 and 3 ≤ steps are collected.The collected data was in a .jsonformat, every data point with slight differences in keys.Some protocols were present in the git repository but could not be found when retrieved using the API, and vice versa 11 .Also, even if the file ID in the git repository and the protocol ID retrieved using the API are the same, the dictionary key number_of_steps may differ 12 .Keywords 13 extracted from the keywords.txtfile and the descriptions were converted to lowercase temporarily for comparison and scoring.As of May 2024, we collected a total of approximately 15k mirrored public protocols from protocols.io'sGitHub before refinement.Protocols were excluded if dictionary key steps is empty.Protocols were manually verified by experts in biology.The protocols were removed if they were multiple duplicated files for an identical protocol 14 .For the same title, we score the latest version of the protocol.A.2 Metrics and EvaluationDefinitions of Evaluation Criteria• Consistency: Consistency (1-5) -the factual alignment between the source and the target pseudocode.A factually consistent pseudocode contains only statements that are entailed by the source pseudocode.Annotators 11 The protocol with ID 3737 exists in protocol.iobut doesn't exist in git repository. 12The number_of_steps for the protocol with ID 10489 is 3 in the git repository but 0 when retrieved using the API. 13 The keywords are: Biology, Cell, DNA, Protein, Stem Cell, Molecular Biology, Molecular, Gene, Virus, E. coli, cDNA, Agarose, Agarose Gel, in vitro, PCR, NGS, Ethanol, Illumina, Cell Theory, Evolution, Genetics, Homeostasis, Cell Membrane, Mitochondria, Nucleus, Ribosomes, DNA Replication, Mutation, Chromosomes, Gene Expression, Natural Selection, Speciation, Adaptation, Phylogenetics, Ecosystems, Biodiversity, Conservation, Bacteria, Viruses, Fungi, Pathogens, Proteins, Enzymes, Metabolism, Photosynthesis, Gel Electrophoresis, Cloning, CRISPR-Cas9, Neurons, Brain, Synapses, Neurotransmitters, Antibodies, Vaccines, Immune Response, Autoimmunity, Embryogenesis, Stem Cells, Morphogenesis, Regeneration, Pollination, Growth Hormones, Tropisms, Coral Reefs, Oceanic Zones, Marine Conservation, Aquatic Ecosystems, Endangered Species, Habitat Destruction, Conservation Strategies, Rewilding, Genetic Engineering, Bioreactors, Bioinformatics, and Synthetic Biology.14 such as protocol ID: 9216were also asked to penalize summaries that contained hallucinated facts.• Fluency: Fluency (1-5): the quality of the pseudocode in terms of grammar, spelling, punctuation, word choice, and structure.• Relevance: Relevance (1-5) -selection of important information from the source pseudocode.The target pseudocode should include only important information from the source document.Annotators were instructed to penalize summaries which contained redundancies and excess information.• Precision: Precision (1-5) -the exactness and accuracy of the expressions and terminology used in the pseudocode.The target pseudocode should avoid vague or ambiguous terms and should use specific and appropriate terminology that accurately reflects the intended operations and logic.• Coverage: Coverage (1-5) -the extent to which the target pseudocode addresses all aspects of the source pseudocode.The target pseudocode should comprehensively represent all the necessary steps, operations, and details present in the source pseudocode without omitting any critical information.Note that above are criteria used for evaluation when GPT-generated pseudocode was a baseline.This was slightly modified when evaluating based on original protocol.Example LLAM-EVAL Prompt Below is a prompt evaluating the generated pseudocode from a target LLM based on the criteria Coherence using the GPT-generated pseudocode as the ground truth.The GPT-generated pseudocode for each protocol is placed inside {{Ground_truth_pseudocode}}, and the target model-generated pseudocode is placed inside {{Target_pseudocode}}.You will be given a source pseudocode as a ground truth.You will then be given a target pseudocode which is generated from an identical source of protocol.Your task is to rate the target pseudocode on one metric.Please make sure you read and understand these instructions carefully.Please keep this document open while reviewing, and refer to it as needed.Evaluation Criteria: Coherence (1-5) -the overall quality of all lines in the pseudocode.The target pseudocode should not be a rough overview but should provide a precise description of the ground truth pseudocode.Evaluation Steps: 1. Read the Ground Truth Pseudocode: Carefully read and understand the source pseudocode provided as the ground truth.Ensure you comprehend the logic, flow, and details of the algorithm or protocol described.2. Read the Target Pseudocode: Thoroughly read the target pseudocode that needs to be evaluated.Pay attention to the details, structure, and clarity of the pseudocode.3. Compare Against Ground Truth: Compare each line and section of the target pseudocode with the corresponding parts of the ground truth pseudocode.Ensure that all critical steps, variables, and logic present in the ground truth are accurately reflected in the target pseudocode.4. Assess Coherence: Evaluate the overall quality of the target pseudocode based on how well it translates the ground truth.Consider the following aspects: Clarity: Is the pseudocode easy to understand?Completeness: Does it cover all the steps and details present in the ground truth?Precision: Are the descriptions and instructions in the pseudocode precise and unambiguous?Consistency: Are there any contradictions or logical inconsistencies?5. Assign a Coherence Rating (1-5): 1 (Poor): The target pseudocode is incomplete, confusing, and lacks most details from the ground truth. 2 (Fair): The target pseudocode is partially complete but has significant gaps and is often unclear.3 (Good): The target pseudocode covers most details from the ground truth but has some minor inconsistencies or lacks clarity in parts.4 (Very Good): The target pseudocode is mostly complete and clear, with very few minor issues.5 (Excellent): The target pseudocode is complete, clear, precise, and fully coherent with the ground truth.Source Pseudocode: {{Ground_truth_pseudocode}}Target Pseudocode: {{Target_pseudocode}} Evaluation Form (scores ONLY):-Coherence:A.3 Evaluator LLM SelectionModels without numerical responses include: Llama3-8b, Llama3-70b, Mixtral, and Gemma.A.4 Implementation DetailsExcept for n and seed, parameters were set to their default values.We used approximately $1000 for GPT API calls, $20 for Gemini, and other models were free of cost.Counting Tokens We counted the tokens of the concatenated string of the title, original description, and steps, separated by "\n\n".The reason for this approach is to match the token count with that of the previous work.Inconsistencies G-EVAL Outputs To address this issue, we attempted the following methods:(1) Modified max_token = 5 to max_token = 1 : The scores became integers, but the model still generated sentences in addition to scores.(2
Walid Ahmad, Elana Simon, Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. 2022. Chemberta-2: Towards chemical foundation models. </p>
<p>The claude 3 model family: Opus, sonnet, haiku. Anthropic, 2023Technical report</p>
<p>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or SummarizationAnn Arbor, Michigan2005Association for Computational Linguistics</p>
<p>SciB-ERT: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, 10.18653/v1/D19-1371Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Sam Andres M Bran, Oliver Cox, Carlo Schilter, Andrew D Baldassari, Philippe White, Schwaller, Chemcrow: Augmenting large-language models with chemistry tools. 2023</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish2020. 2020Chemberta: Large-scale self. supervised pretraining for molecular property prediction</p>
<p>FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. Esin Durmus, He He, Mona Diab, 10.18653/v1/2020.acl-main.454Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Evaluating generative language models in information extraction as subjective question correction. Yuchen Fan, Yantao Liu, Zijun Yao, Jifan Yu, Lei Hou, Juanzi Li, 2024</p>
<p>Gptscore: Evaluate as you desire. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, 2023</p>
<p>Gemini: A family of highly capable multimodal models. Gemini Team, Google , 2024</p>
<p>Domain-specific language model pretraining for biomedical natural language processing. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, Hoifung Poon, 2020</p>
<p>What can large language models do in chemistry?. Taicheng Guo, Kehan Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, 2023a comprehensive benchmark on eight tasks</p>
<p>Matscibert: A materials domain language model for text mining and information extraction. Tanishq Gupta, Mohd Zaki, N M Anoop Krishnan, Mausam , 2021</p>
<p>Llms can generate robotic scripts from goal-oriented instructions in biological laboratory automation. Akari Inagaki, Koichi Kato, Haruka Takahashi, Genki N Ozaki, Kanda, 2023</p>
<p>14 examples of how llms can transform materials science and chemistry. Kevin Maik, Jablonka , Qianxiang Ai, Alexander Al-Feghali, Shruti Badhwar, Joshua D Bocarsly, Andres M Bran, Stefan Bringuier, L Catherine Brinson, Kamal Choudhary, Defne Circi, Sam Cox, Wibe A Jong, Matthew L Evans, Nicolas Gastellu, Jerome Genzling, Victoria María, Ankur K Gil, Zhi Gupta, Alishba Hong, Sabine Imran, Anne Kruschwitz, Jakub Labarre, Tao Lála, Steven Liu, Sauradeep Ma, Garrett W Majumdar, Nicolas Merz, Elias Moitessier, Beatriz Moubarak, Brenden Mouriño, Michael Pelkie, Mayk Pieler, Bojana Caldas Ramos, Ranković, G Samuel, Jacob N Rodriques, Philippe Sanders, Marcus Schwaller, Jiale Schwarting, Berend Shi, Ben E Smit, Joren Smith, Christoph Van Herck, Logan Völker, Sean Ward, Warren, 10.1039/D3DD00113JBenjamin Weiser, Sylvester Zhang, Xiaoqi Zhang, Ghezal Ahmad Zia, Aristana Scourtas, K. J. Schmidt, Ian Foster, Andrew D. White, and Ben Blaiszik20232a reflection on a large language model hackathon. Digital Discovery</p>
<p>Protocode: Leveraging large language models (llms) for automated generation of machine-readable pcr protocols from scientific publications. Shuo Jiang, Daniel Evans-Yamamoto, Dennis Bersenev, K Sucheendra, Ayako Palaniappan, Yachie-Kinoshita, 10.1016/J.SLAST.2024.100134SLAS Technology. 291001342024</p>
<p>Prometheus: Inducing finegrained evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, Minjoon Seo, 2023</p>
<p>From word embeddings to document distances. Matt Kusner, Yu Sun, Nicholas Kolkin, Kilian Weinberger, Proceedings of the 32nd International Conference on Machine Learning. the 32nd International Conference on Machine LearningLille, FrancePMLR201537</p>
<p>Five hard truths for synthetic biology. R Kwok, 10.1038/463288aNature. 4632010</p>
<p>A zero-shot and few-shot study of instructionfinetuned large language models applied to clinical and biomedical tasks. Yanis Labrak, Mickael Rouvier, Richard Dufour, 2024</p>
<p>Biobert: a pre-trained biomedical language representation model for biomedical text mining. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, 10.1093/bioinformatics/btz682Bioinformatics. 3642019</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, G-eval: Nlg evaluation using gpt-4 with better human alignment. 2023</p>
<p>Biogpt: generative pre-trained transformer for biomedical text generation and mining. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, Tie-Yan Liu, 10.1093/bib/bbac409Briefings in Bioinformatics. 6232022</p>
<p>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. Potsawee Manakul, Adian Liusie, Mark J F Gales, 2023</p>
<p>BioPlanner: Automatic evaluation of LLMs on protocol planning in biology. Aleksandar Odhran O'donoghue, John Shtedritski, Ralph Ginger, Ali Abboud, Samuel Ghareeb, Rodriques, 10.18653/v1/2023.emnlp-main.162Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingapore2023Association for Computational Linguistics</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02. the 40th Annual Meeting on Association for Computational Linguistics, ACL '02USA. Association for Computational Linguistics2002</p>
<p>Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets. Yifan Peng, Shankai Yan, Zhiyong Lu, Proceedings of the 2019 Workshop on Biomedical Natural Language Processing. the 2019 Workshop on Biomedical Natural Language Processing2019. 2019</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 2023</p>
<p>Bleurt: Learning robust metrics for text generation. Thibault Sellam, Dipanjan Das, Ankur P Parikh, 2020</p>
<p>A general-purpose material property data extraction pipeline from large polymer corpora using natural language processing. P Shetty, A C Rajan, C Kuenneth, 10.1038/s41524-023-01003-wnpj Computational Materials. 9522023</p>
<p>Hoo-Chang Shin, Yang Zhang, Evelina Bakhturina, Raul Puri, Mostofa Patwary, Mohammad Shoeybi, Raghav Mani, Biomegatron: Larger biomedical domain language model. 2020</p>
<p>Protocols.io: Virtual communities for protocol development and discussion. Leonid Teytelman, Alexei Stoliartchouk, Lori Kindler, Bonnie L Hurwitz, 10.1371/journal.pbio.1002538PLOS Biology. 1482016</p>
<p>Camembert-bio: Leveraging continual pre-training for cost-effective models on french biomedical data. Rian Touchent, Laurent Romary, 2024and Eric de la Clergerie</p>
<p>Automated extraction of chemical synthesis actions from experimental procedures. Alain C Vaucher, Federico Zipoli, Jonas Geluykens, 10.1038/s41467-020-17266-6Nature Communications. 1136012020</p>
<p>Asking and answering questions to evaluate the factual consistency of summaries. Alex Wang, Kyunghyun Cho, Mike Lewis, 10.18653/v1/2020.acl-main.450Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Assessment of chemistry knowledge in large language models that generate code. Andrew D White, Glen M Hocky, A Heta, Mehrad Gandhi, Sam Ansari, Geemi P Cox, Subarna Wellawatte, Ziyue Sasmal, Kangxin Yang, Yuvraj Liu, Willmor J Peña Singh, Ccoa, 10.1039/D2DD00087CDigital Discovery. 22023</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, 2020</p>
<p>Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, Steffen Eger, 2019</p>
<p>Is LLM a reliable reviewer? a comprehensive evaluation of LLM on automatic paper reviewing tasks. Ruiyang Zhou, Lu Chen, Kai Yu, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024</p>            </div>
        </div>

    </div>
</body>
</html>
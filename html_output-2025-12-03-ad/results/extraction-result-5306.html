<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5306 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5306</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5306</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-404571933e3e87942768c9a5cde8a6285732ad6f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/404571933e3e87942768c9a5cde8a6285732ad6f" target="_blank">Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models</a></p>
                <p><strong>Paper Venue:</strong> Frontiers in Pharmacology</p>
                <p><strong>Paper TL;DR:</strong> A benchmarking platform called Molecular Sets (MOSES) is introduced to standardize training and comparison of molecular generative models and suggest to use the results as reference points for further advancements in generative chemistry research.</p>
                <p><strong>Paper Abstract:</strong> Generative models are becoming a tool of choice for exploring the molecular space. These models learn on a large training dataset and produce novel molecular structures with similar properties. Generated structures can be utilized for virtual screening or training semi-supervized predictive models in the downstream tasks. While there are plenty of generative models, it is unclear how to compare and rank them. In this work, we introduce a benchmarking platform called Molecular Sets (MOSES) to standardize training and comparison of molecular generative models. MOSES provides training and testing datasets, and a set of metrics to evaluate the quality and diversity of generated structures. We have implemented and compared several molecular generation models and suggest to use our results as reference points for further advancements in generative chemistry research. The platform and source code are available at https://github.com/molecularsets/moses.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5306.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5306.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CharRNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Character-level Recurrent Neural Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive sequence model trained on SMILES strings to generate novel molecules by predicting the next character/token; implemented and evaluated as a baseline in MOSES and found to perform very well on distribution-learning metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating focused molecule libraries for drug discovery with recurrent neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CharRNN</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>recurrent neural network (autoregressive sequence model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>MOSES training split derived from ZINC Clean Leads (canonical SMILES; ~1.58M molecules after preprocessing and filters)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>de novo molecular generation for drug discovery / virtual library construction</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Maximum-likelihood (next-token) training on SMILES and sampling from the trained autoregressive model</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES strings (canonical SMILES used for evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MOSES metrics including Validity, Unique@K, Novelty, Filters (MCF/PAINS), Fragment similarity (Frag), Scaffold similarity (Scaff), SNN, Internal diversity (IntDiv), Fréchet ChemNet Distance (FCD), and property-distribution Wasserstein-1 distances (MW, logP, QED, SA).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>MOSES (train/test/scaffold splits), ZINC Clean Leads (source)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>CharRNN achieved very strong distribution-learning performance in MOSES: Valid ~0.975±0.026, Unique@1k ≈1.0, Unique@10k ≈0.999, Filters ≈0.994, Novelty ≈0.842±0.051, IntDiv ≈0.856, FCD (Test) ≈0.073±0.025, SNN (Test) ≈0.601±0.021, Fragment similarity ≈1.0 and high scaffold similarity on the random test; authors note CharRNN produced many novel scaffolds (~11% on scaffold test).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>CharRNN outperformed other baseline generative architectures on several key MOSES metrics (FCD, fragment and scaffold similarity), indicating superior ability to match dataset statistics while still discovering novel scaffolds; VAE variants had comparable validity and uniqueness but showed signs of overfitting (higher SNN but lower novelty).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Although CharRNN matched distributional metrics well, novelty was not maximal (≈84%) compared to some non-neural baselines; sequence models can still produce invalid SMILES (validity <1) and are sensitive to SMILES syntax/augmentation choices; scaffold generalization remains limited when tested on held-out scaffold splits (scaffold similarity lower on TestSF).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5306.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5306.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder–decoder latent-variable model mapping SMILES to a continuous latent space and back, used to sample new molecules by decoding latent vectors; implemented as a baseline in MOSES.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Auto-Encoding variational bayes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VAE (SMILES-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>variational autoencoder (encoder-decoder latent-variable model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>MOSES training split (SMILES from ZINC Clean Leads)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>de novo molecular generation for drug discovery / dataset augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Training with reconstruction + KL regularization; sampling from latent prior and decoding to SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES strings</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MOSES metrics (Validity, Uniqueness, Novelty, Filters, Frag, Scaff, SNN, IntDiv, FCD, property distributions)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>MOSES (ZINC-derived splits)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>VAE attained high validity (~0.977±0.001), near-perfect uniqueness, Filters ≈0.997, Novelty ≈0.895±0.007, IntDiv ≈0.856, FCD (Test) ≈0.099±0.013, SNN (Test) ≈0.626; fragment and scaffold similarity high on Test but scaffold generalization to TestSF was limited.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>VAE performed competitively with CharRNN on many metrics but had higher SNN combined with lower novelty than some generators, which authors interpret as partial overfitting to training set; autoencoder-based models generally showed lower novelty than some non-neural baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>VAE-based molecular generators can overfit (low novelty) and produce molecules closely resembling training data; SMILES-based decoding can yield invalid strings if syntax constraints are not enforced; no model size or large-scale pretraining reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5306.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5306.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial Autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoencoder variant that uses an adversarial objective to shape the latent distribution instead of a KL term, applied to SMILES-based molecular generation as a MOSES baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adversarial autoencoders</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AAE (SMILES-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>adversarial autoencoder (encoder-decoder with adversarial latent regularization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>MOSES training split (SMILES from ZINC Clean Leads)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>de novo molecular generation for drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Train autoencoder with discriminator enforcing latent prior; sample latent codes from prior and decode to SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES strings</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MOSES metrics (Validity, Uniqueness, Novelty, Filters, Frag, Scaff, SNN, IntDiv, FCD, property distributions)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>MOSES (ZINC-derived splits)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>AAE produced Valid ≈0.937±0.034, Unique@K ≈1.0, Filters ≈0.996, Novelty ≈0.793±0.028, IntDiv ≈0.856, FCD (Test) ≈0.556±0.203, SNN (Test) ≈0.608±0.004; fragment and scaffold similarities high on Test but lower scaffold generalization on TestSF.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>AAE performed worse than CharRNN and VAE on some distribution metrics (higher FCD, lower novelty) and comparable on fragment-level similarity; adversarial latent regularization did not clearly outperform KL-based VAE in this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Showed lower novelty and higher variability in FCD across runs, indicating instability or overfitting; same SMILES-decoding limitations apply; no explicit size or large-pretraining reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5306.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5306.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JTN-VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Junction Tree Variational Autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-aware VAE that generates a junction tree of chemical substructures and then assembles them into a molecular graph, designed to improve chemical validity and scaffold coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Junction tree variational autoencoder for molecular graph generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>JTN-VAE</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>graph-structured variational autoencoder (two-phase: junction-tree then graph assembly)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>MOSES training split (molecules decomposed into subgraph components from ZINC Clean Leads)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>de novo molecular graph generation for drug discovery, with emphasis on generating chemically valid scaffolds</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Train junction-tree encoder/decoder on subgraph vocabulary extracted from training set; sample junction trees and assemble into molecular graphs</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Molecular graphs (assembled from subgraph components; can be converted to SMILES for evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MOSES metrics (Validity, Uniqueness, Novelty, Filters, Frag, Scaff, SNN, IntDiv, FCD, property distributions)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>MOSES (ZINC-derived splits)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>JTN-VAE achieved Valid = 1.0, Unique@K ≈1.0, Filters ≈0.976, Novelty ≈0.9143±0.0058, IntDiv ≈0.8551, FCD (Test) ≈0.3954±0.0234, SNN (Test) ≈0.5477±0.0076; good fragment similarity but scaffold generalization to TestSF remained limited (Scaff on TestSF ≈0.1009).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>JTN-VAE guarantees chemical validity by construction and achieved perfect validity, but on some distribution metrics (FCD, SNN) it was outperformed by CharRNN and VAE; it produced relatively high novelty compared to other autoencoder methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>While validity is high, scaffold similarity on scaffold-held-out tests is modest; model relies on subgraph vocabulary extracted from training data, which can limit generation of novel substructures absent from training set.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5306.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5306.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LatentGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent Vector Based Generative Adversarial Network (LatentGAN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hybrid approach: an autoencoder maps SMILES to latent vectors, and a GAN is trained in latent space to produce new latent vectors decoded by the pre-trained decoder to generate novel SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A de novo molecular generation method using latent vector based generative adversarial network.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LatentGAN</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoencoder + GAN (latent-space generative adversarial network)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>MOSES training split (SMILES from ZINC Clean Leads) used to pretrain the autoencoder and to train the GAN on latent codes</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>de novo molecular generation for drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Pretrain SMILES autoencoder, then train GAN to sample latent vectors that are decoded to SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES strings (decoded from latent vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MOSES metrics (Validity, Unique@K, Novelty, Filters, Frag, Scaff, SNN, IntDiv, FCD, property distributions)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>MOSES (ZINC-derived splits)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LatentGAN results: Valid ≈0.897±0.002, Unique high, Filters ≈0.973, Novelty ≈0.949, IntDiv ≈0.857, FCD (Test) ≈0.296±0.021, SNN (Test) ≈0.538; high novelty but slightly lower validity than some other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Produced high novelty relative to autoencoder baselines and competitive FCD; lower validity than graph-based JTN-VAE and CharRNN but better novelty than some AE-based models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Lower validity suggests latent-space GAN sampling can produce latent vectors that decode to invalid SMILES; stability of GAN training and decoder generalization are potential challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5306.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5306.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HMM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hidden Markov Model (SMILES HMM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classical probabilistic sequence model (HMM) trained on SMILES token sequences and used to sample new SMILES strings as a simple non-neural baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Hidden Markov Model (HMM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>statistical Markovian sequence model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>MOSES training split (SMILES token sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>baseline de novo SMILES sequence generation for drug-like molecules</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Baum-Welch parameter estimation for HMM; sampling sequences from learned transition/emission probabilities</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES strings</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MOSES metrics (Validity, Unique@K, Novelty, Filters, Frag, Scaff, SNN, IntDiv, FCD, property distributions)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>MOSES</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>HMM demonstrated low validity (~0.076±0.0322) and correspondingly poor FCD and SNN; Filters ~0.9024, Novelty ~0.9994, IntDiv ~0.8466; biased toward smaller/invalid structures since limited context reduces ability to enforce chemical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>HMM performed substantially worse than neural generative models (CharRNN, VAEs) on validity and distributional metrics, but showed high novelty by sampling many implausible sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Severely limited context window leads to many invalid SMILES; classical n-state HMMs are insufficient to capture SMILES syntax and chemical valence constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5306.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5306.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NGram</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>N-gram Language Model (SMILES n-gram)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simple statistical sequence model that generates SMILES using n-gram token probabilities estimated from training data; used as a non-neural baseline in MOSES.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NGram</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>statistical n-gram sequence model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>MOSES training split (SMILES token n-gram statistics)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>baseline SMILES generation for molecular library construction</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Collect n-gram frequencies and sample next token conditional on previous (n-1) tokens</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES strings</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MOSES metrics (Validity, Unique@K, Novelty, Filters, Frag, Scaff, SNN, IntDiv, FCD, property distributions)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>MOSES</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>NGram produced moderate validity (≈0.2376±0.0025), high uniqueness, Filters ≈0.9582, Novelty ≈0.9694, IntDiv ≈0.8738, FCD (Test) ≈5.5069±0.1027, SNN (Test) ≈0.5209; better than HMM but substantially worse than neural models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>N-gram outperformed HMM in many respects due to larger local context but lagged behind CharRNN and autoencoder methods in matching distributional properties (FCD, fragment/scaffold similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limited context length makes it difficult to model long-range SMILES dependencies and chemical valence rules, leading to many invalid or unphysical SMILES for larger molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5306.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5306.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Combinatorial generator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BRICS-based Combinatorial Fragment Generator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fragment-recombination baseline that splits training molecules into BRICS fragments and randomly recombines them (sampling fragments according to frequency) to create novel molecules; designed to explore fragment-level generative capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Combinatorial (BRICS) generator</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>fragment recombination (rule-based combinatorial generator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>BRICS fragments extracted from MOSES training set (ZINC Clean Leads)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>constructing virtual libraries for drug discovery via fragment recombination</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Decompose training molecules into BRICS fragments and randomly connect fragments sampled by frequency to form new molecular graphs</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Molecular graphs / SMILES (assembled molecules converted to SMILES for evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MOSES metrics (Validity, Unique@K, Novelty, Filters, Frag, Scaff, SNN, IntDiv, FCD, property distributions)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>MOSES</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Combinatorial generator achieved perfect validity (1.0), very high uniqueness, Filters ≈0.9557, Novelty ≈0.9878, IntDiv ≈0.8732, FCD (Test) ≈4.2375±0.037, SNN (Test) ≈0.4514; produced higher diversity than training data and broader MW variance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Produced higher diversity and novelty than many neural-autoencoder methods but had worse FCD and SNN compared to best neural models (CharRNN); benefits include guaranteed validity but possible generation of unrealistic fragment combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>While validity is high, combinatorial recombination may create chemically implausible or synthetically inaccessible combinations not filtered out by simple rules; lacks learned global distributional constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5306.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5306.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolGAN: an implicit generative model for small molecular graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based GAN for molecular graph generation referenced in the paper (discussion of molecular representations); not used as an experimental baseline in MOSES but cited as a relevant implicit graph generative model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MolGAN: an implicit generative model for small molecular graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolGAN</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>generative adversarial network for molecular graphs (implicit model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Referenced as applied to QM9 dataset in original MolGAN work (not trained on MOSES in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>small-molecule graph generation (chemistry / materials discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>GAN trained to produce adjacency and node-feature matrices representing molecular graphs; implicit density model sampled to generate molecules</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Molecular graphs (adjacency matrix + node features)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not evaluated in MOSES experiments here; referenced literature uses validity, uniqueness, novelty and property-based metrics on QM9</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Referenced original MolGAN work applying to QM9 (not evaluated on MOSES in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned as an example of an implicit graph generative model; not directly compared in MOSES experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in detail here; original MolGAN paper reports difficulties scaling to larger, more diverse drug-like datasets (QM9 is small and constrained).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generating focused molecule libraries for drug discovery with recurrent neural networks. <em>(Rating: 2)</em></li>
                <li>Automatic chemical design using a Data-Driven continuous representation of molecules. <em>(Rating: 2)</em></li>
                <li>Junction tree variational autoencoder for molecular graph generation <em>(Rating: 2)</em></li>
                <li>A de novo molecular generation method using latent vector based generative adversarial network. <em>(Rating: 2)</em></li>
                <li>MolGAN: an implicit generative model for small molecular graphs <em>(Rating: 2)</em></li>
                <li>Guacamol: benchmarking models for de novo molecular design. <em>(Rating: 1)</em></li>
                <li>Molecular de-novo design through deep reinforcement learning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5306",
    "paper_id": "paper-404571933e3e87942768c9a5cde8a6285732ad6f",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [
        {
            "name_short": "CharRNN",
            "name_full": "Character-level Recurrent Neural Network",
            "brief_description": "An autoregressive sequence model trained on SMILES strings to generate novel molecules by predicting the next character/token; implemented and evaluated as a baseline in MOSES and found to perform very well on distribution-learning metrics.",
            "citation_title": "Generating focused molecule libraries for drug discovery with recurrent neural networks.",
            "mention_or_use": "use",
            "model_name": "CharRNN",
            "model_type": "recurrent neural network (autoregressive sequence model)",
            "model_size": null,
            "training_data": "MOSES training split derived from ZINC Clean Leads (canonical SMILES; ~1.58M molecules after preprocessing and filters)",
            "application_domain": "de novo molecular generation for drug discovery / virtual library construction",
            "generation_method": "Maximum-likelihood (next-token) training on SMILES and sampling from the trained autoregressive model",
            "output_representation": "SMILES strings (canonical SMILES used for evaluation)",
            "evaluation_metrics": "MOSES metrics including Validity, Unique@K, Novelty, Filters (MCF/PAINS), Fragment similarity (Frag), Scaffold similarity (Scaff), SNN, Internal diversity (IntDiv), Fréchet ChemNet Distance (FCD), and property-distribution Wasserstein-1 distances (MW, logP, QED, SA).",
            "benchmarks_or_datasets": "MOSES (train/test/scaffold splits), ZINC Clean Leads (source)",
            "results_summary": "CharRNN achieved very strong distribution-learning performance in MOSES: Valid ~0.975±0.026, Unique@1k ≈1.0, Unique@10k ≈0.999, Filters ≈0.994, Novelty ≈0.842±0.051, IntDiv ≈0.856, FCD (Test) ≈0.073±0.025, SNN (Test) ≈0.601±0.021, Fragment similarity ≈1.0 and high scaffold similarity on the random test; authors note CharRNN produced many novel scaffolds (~11% on scaffold test).",
            "comparison_to_other_methods": "CharRNN outperformed other baseline generative architectures on several key MOSES metrics (FCD, fragment and scaffold similarity), indicating superior ability to match dataset statistics while still discovering novel scaffolds; VAE variants had comparable validity and uniqueness but showed signs of overfitting (higher SNN but lower novelty).",
            "limitations_or_challenges": "Although CharRNN matched distributional metrics well, novelty was not maximal (≈84%) compared to some non-neural baselines; sequence models can still produce invalid SMILES (validity &lt;1) and are sensitive to SMILES syntax/augmentation choices; scaffold generalization remains limited when tested on held-out scaffold splits (scaffold similarity lower on TestSF).",
            "uuid": "e5306.0",
            "source_info": {
                "paper_title": "Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "VAE",
            "name_full": "Variational Autoencoder",
            "brief_description": "An encoder–decoder latent-variable model mapping SMILES to a continuous latent space and back, used to sample new molecules by decoding latent vectors; implemented as a baseline in MOSES.",
            "citation_title": "Auto-Encoding variational bayes",
            "mention_or_use": "use",
            "model_name": "VAE (SMILES-based)",
            "model_type": "variational autoencoder (encoder-decoder latent-variable model)",
            "model_size": null,
            "training_data": "MOSES training split (SMILES from ZINC Clean Leads)",
            "application_domain": "de novo molecular generation for drug discovery / dataset augmentation",
            "generation_method": "Training with reconstruction + KL regularization; sampling from latent prior and decoding to SMILES",
            "output_representation": "SMILES strings",
            "evaluation_metrics": "MOSES metrics (Validity, Uniqueness, Novelty, Filters, Frag, Scaff, SNN, IntDiv, FCD, property distributions)",
            "benchmarks_or_datasets": "MOSES (ZINC-derived splits)",
            "results_summary": "VAE attained high validity (~0.977±0.001), near-perfect uniqueness, Filters ≈0.997, Novelty ≈0.895±0.007, IntDiv ≈0.856, FCD (Test) ≈0.099±0.013, SNN (Test) ≈0.626; fragment and scaffold similarity high on Test but scaffold generalization to TestSF was limited.",
            "comparison_to_other_methods": "VAE performed competitively with CharRNN on many metrics but had higher SNN combined with lower novelty than some generators, which authors interpret as partial overfitting to training set; autoencoder-based models generally showed lower novelty than some non-neural baselines.",
            "limitations_or_challenges": "VAE-based molecular generators can overfit (low novelty) and produce molecules closely resembling training data; SMILES-based decoding can yield invalid strings if syntax constraints are not enforced; no model size or large-scale pretraining reported.",
            "uuid": "e5306.1",
            "source_info": {
                "paper_title": "Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "AAE",
            "name_full": "Adversarial Autoencoder",
            "brief_description": "An autoencoder variant that uses an adversarial objective to shape the latent distribution instead of a KL term, applied to SMILES-based molecular generation as a MOSES baseline.",
            "citation_title": "Adversarial autoencoders",
            "mention_or_use": "use",
            "model_name": "AAE (SMILES-based)",
            "model_type": "adversarial autoencoder (encoder-decoder with adversarial latent regularization)",
            "model_size": null,
            "training_data": "MOSES training split (SMILES from ZINC Clean Leads)",
            "application_domain": "de novo molecular generation for drug discovery",
            "generation_method": "Train autoencoder with discriminator enforcing latent prior; sample latent codes from prior and decode to SMILES",
            "output_representation": "SMILES strings",
            "evaluation_metrics": "MOSES metrics (Validity, Uniqueness, Novelty, Filters, Frag, Scaff, SNN, IntDiv, FCD, property distributions)",
            "benchmarks_or_datasets": "MOSES (ZINC-derived splits)",
            "results_summary": "AAE produced Valid ≈0.937±0.034, Unique@K ≈1.0, Filters ≈0.996, Novelty ≈0.793±0.028, IntDiv ≈0.856, FCD (Test) ≈0.556±0.203, SNN (Test) ≈0.608±0.004; fragment and scaffold similarities high on Test but lower scaffold generalization on TestSF.",
            "comparison_to_other_methods": "AAE performed worse than CharRNN and VAE on some distribution metrics (higher FCD, lower novelty) and comparable on fragment-level similarity; adversarial latent regularization did not clearly outperform KL-based VAE in this benchmark.",
            "limitations_or_challenges": "Showed lower novelty and higher variability in FCD across runs, indicating instability or overfitting; same SMILES-decoding limitations apply; no explicit size or large-pretraining reported.",
            "uuid": "e5306.2",
            "source_info": {
                "paper_title": "Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "JTN-VAE",
            "name_full": "Junction Tree Variational Autoencoder",
            "brief_description": "A graph-aware VAE that generates a junction tree of chemical substructures and then assembles them into a molecular graph, designed to improve chemical validity and scaffold coherence.",
            "citation_title": "Junction tree variational autoencoder for molecular graph generation",
            "mention_or_use": "use",
            "model_name": "JTN-VAE",
            "model_type": "graph-structured variational autoencoder (two-phase: junction-tree then graph assembly)",
            "model_size": null,
            "training_data": "MOSES training split (molecules decomposed into subgraph components from ZINC Clean Leads)",
            "application_domain": "de novo molecular graph generation for drug discovery, with emphasis on generating chemically valid scaffolds",
            "generation_method": "Train junction-tree encoder/decoder on subgraph vocabulary extracted from training set; sample junction trees and assemble into molecular graphs",
            "output_representation": "Molecular graphs (assembled from subgraph components; can be converted to SMILES for evaluation)",
            "evaluation_metrics": "MOSES metrics (Validity, Uniqueness, Novelty, Filters, Frag, Scaff, SNN, IntDiv, FCD, property distributions)",
            "benchmarks_or_datasets": "MOSES (ZINC-derived splits)",
            "results_summary": "JTN-VAE achieved Valid = 1.0, Unique@K ≈1.0, Filters ≈0.976, Novelty ≈0.9143±0.0058, IntDiv ≈0.8551, FCD (Test) ≈0.3954±0.0234, SNN (Test) ≈0.5477±0.0076; good fragment similarity but scaffold generalization to TestSF remained limited (Scaff on TestSF ≈0.1009).",
            "comparison_to_other_methods": "JTN-VAE guarantees chemical validity by construction and achieved perfect validity, but on some distribution metrics (FCD, SNN) it was outperformed by CharRNN and VAE; it produced relatively high novelty compared to other autoencoder methods.",
            "limitations_or_challenges": "While validity is high, scaffold similarity on scaffold-held-out tests is modest; model relies on subgraph vocabulary extracted from training data, which can limit generation of novel substructures absent from training set.",
            "uuid": "e5306.3",
            "source_info": {
                "paper_title": "Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "LatentGAN",
            "name_full": "Latent Vector Based Generative Adversarial Network (LatentGAN)",
            "brief_description": "Hybrid approach: an autoencoder maps SMILES to latent vectors, and a GAN is trained in latent space to produce new latent vectors decoded by the pre-trained decoder to generate novel SMILES.",
            "citation_title": "A de novo molecular generation method using latent vector based generative adversarial network.",
            "mention_or_use": "use",
            "model_name": "LatentGAN",
            "model_type": "autoencoder + GAN (latent-space generative adversarial network)",
            "model_size": null,
            "training_data": "MOSES training split (SMILES from ZINC Clean Leads) used to pretrain the autoencoder and to train the GAN on latent codes",
            "application_domain": "de novo molecular generation for drug discovery",
            "generation_method": "Pretrain SMILES autoencoder, then train GAN to sample latent vectors that are decoded to SMILES",
            "output_representation": "SMILES strings (decoded from latent vectors)",
            "evaluation_metrics": "MOSES metrics (Validity, Unique@K, Novelty, Filters, Frag, Scaff, SNN, IntDiv, FCD, property distributions)",
            "benchmarks_or_datasets": "MOSES (ZINC-derived splits)",
            "results_summary": "LatentGAN results: Valid ≈0.897±0.002, Unique high, Filters ≈0.973, Novelty ≈0.949, IntDiv ≈0.857, FCD (Test) ≈0.296±0.021, SNN (Test) ≈0.538; high novelty but slightly lower validity than some other baselines.",
            "comparison_to_other_methods": "Produced high novelty relative to autoencoder baselines and competitive FCD; lower validity than graph-based JTN-VAE and CharRNN but better novelty than some AE-based models.",
            "limitations_or_challenges": "Lower validity suggests latent-space GAN sampling can produce latent vectors that decode to invalid SMILES; stability of GAN training and decoder generalization are potential challenges.",
            "uuid": "e5306.4",
            "source_info": {
                "paper_title": "Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "HMM",
            "name_full": "Hidden Markov Model (SMILES HMM)",
            "brief_description": "Classical probabilistic sequence model (HMM) trained on SMILES token sequences and used to sample new SMILES strings as a simple non-neural baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Hidden Markov Model (HMM)",
            "model_type": "statistical Markovian sequence model",
            "model_size": null,
            "training_data": "MOSES training split (SMILES token sequences)",
            "application_domain": "baseline de novo SMILES sequence generation for drug-like molecules",
            "generation_method": "Baum-Welch parameter estimation for HMM; sampling sequences from learned transition/emission probabilities",
            "output_representation": "SMILES strings",
            "evaluation_metrics": "MOSES metrics (Validity, Unique@K, Novelty, Filters, Frag, Scaff, SNN, IntDiv, FCD, property distributions)",
            "benchmarks_or_datasets": "MOSES",
            "results_summary": "HMM demonstrated low validity (~0.076±0.0322) and correspondingly poor FCD and SNN; Filters ~0.9024, Novelty ~0.9994, IntDiv ~0.8466; biased toward smaller/invalid structures since limited context reduces ability to enforce chemical constraints.",
            "comparison_to_other_methods": "HMM performed substantially worse than neural generative models (CharRNN, VAEs) on validity and distributional metrics, but showed high novelty by sampling many implausible sequences.",
            "limitations_or_challenges": "Severely limited context window leads to many invalid SMILES; classical n-state HMMs are insufficient to capture SMILES syntax and chemical valence constraints.",
            "uuid": "e5306.5",
            "source_info": {
                "paper_title": "Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "NGram",
            "name_full": "N-gram Language Model (SMILES n-gram)",
            "brief_description": "Simple statistical sequence model that generates SMILES using n-gram token probabilities estimated from training data; used as a non-neural baseline in MOSES.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "NGram",
            "model_type": "statistical n-gram sequence model",
            "model_size": null,
            "training_data": "MOSES training split (SMILES token n-gram statistics)",
            "application_domain": "baseline SMILES generation for molecular library construction",
            "generation_method": "Collect n-gram frequencies and sample next token conditional on previous (n-1) tokens",
            "output_representation": "SMILES strings",
            "evaluation_metrics": "MOSES metrics (Validity, Unique@K, Novelty, Filters, Frag, Scaff, SNN, IntDiv, FCD, property distributions)",
            "benchmarks_or_datasets": "MOSES",
            "results_summary": "NGram produced moderate validity (≈0.2376±0.0025), high uniqueness, Filters ≈0.9582, Novelty ≈0.9694, IntDiv ≈0.8738, FCD (Test) ≈5.5069±0.1027, SNN (Test) ≈0.5209; better than HMM but substantially worse than neural models.",
            "comparison_to_other_methods": "N-gram outperformed HMM in many respects due to larger local context but lagged behind CharRNN and autoencoder methods in matching distributional properties (FCD, fragment/scaffold similarity).",
            "limitations_or_challenges": "Limited context length makes it difficult to model long-range SMILES dependencies and chemical valence rules, leading to many invalid or unphysical SMILES for larger molecules.",
            "uuid": "e5306.6",
            "source_info": {
                "paper_title": "Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "Combinatorial generator",
            "name_full": "BRICS-based Combinatorial Fragment Generator",
            "brief_description": "A fragment-recombination baseline that splits training molecules into BRICS fragments and randomly recombines them (sampling fragments according to frequency) to create novel molecules; designed to explore fragment-level generative capacity.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Combinatorial (BRICS) generator",
            "model_type": "fragment recombination (rule-based combinatorial generator)",
            "model_size": null,
            "training_data": "BRICS fragments extracted from MOSES training set (ZINC Clean Leads)",
            "application_domain": "constructing virtual libraries for drug discovery via fragment recombination",
            "generation_method": "Decompose training molecules into BRICS fragments and randomly connect fragments sampled by frequency to form new molecular graphs",
            "output_representation": "Molecular graphs / SMILES (assembled molecules converted to SMILES for evaluation)",
            "evaluation_metrics": "MOSES metrics (Validity, Unique@K, Novelty, Filters, Frag, Scaff, SNN, IntDiv, FCD, property distributions)",
            "benchmarks_or_datasets": "MOSES",
            "results_summary": "Combinatorial generator achieved perfect validity (1.0), very high uniqueness, Filters ≈0.9557, Novelty ≈0.9878, IntDiv ≈0.8732, FCD (Test) ≈4.2375±0.037, SNN (Test) ≈0.4514; produced higher diversity than training data and broader MW variance.",
            "comparison_to_other_methods": "Produced higher diversity and novelty than many neural-autoencoder methods but had worse FCD and SNN compared to best neural models (CharRNN); benefits include guaranteed validity but possible generation of unrealistic fragment combinations.",
            "limitations_or_challenges": "While validity is high, combinatorial recombination may create chemically implausible or synthetically inaccessible combinations not filtered out by simple rules; lacks learned global distributional constraints.",
            "uuid": "e5306.7",
            "source_info": {
                "paper_title": "Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "MolGAN",
            "name_full": "MolGAN: an implicit generative model for small molecular graphs",
            "brief_description": "A graph-based GAN for molecular graph generation referenced in the paper (discussion of molecular representations); not used as an experimental baseline in MOSES but cited as a relevant implicit graph generative model.",
            "citation_title": "MolGAN: an implicit generative model for small molecular graphs",
            "mention_or_use": "mention",
            "model_name": "MolGAN",
            "model_type": "generative adversarial network for molecular graphs (implicit model)",
            "model_size": null,
            "training_data": "Referenced as applied to QM9 dataset in original MolGAN work (not trained on MOSES in this paper)",
            "application_domain": "small-molecule graph generation (chemistry / materials discovery)",
            "generation_method": "GAN trained to produce adjacency and node-feature matrices representing molecular graphs; implicit density model sampled to generate molecules",
            "output_representation": "Molecular graphs (adjacency matrix + node features)",
            "evaluation_metrics": "Not evaluated in MOSES experiments here; referenced literature uses validity, uniqueness, novelty and property-based metrics on QM9",
            "benchmarks_or_datasets": "Referenced original MolGAN work applying to QM9 (not evaluated on MOSES in this paper)",
            "results_summary": null,
            "comparison_to_other_methods": "Mentioned as an example of an implicit graph generative model; not directly compared in MOSES experiments.",
            "limitations_or_challenges": "Not discussed in detail here; original MolGAN paper reports difficulties scaling to larger, more diverse drug-like datasets (QM9 is small and constrained).",
            "uuid": "e5306.8",
            "source_info": {
                "paper_title": "Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models",
                "publication_date_yy_mm": "2018-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generating focused molecule libraries for drug discovery with recurrent neural networks.",
            "rating": 2
        },
        {
            "paper_title": "Automatic chemical design using a Data-Driven continuous representation of molecules.",
            "rating": 2
        },
        {
            "paper_title": "Junction tree variational autoencoder for molecular graph generation",
            "rating": 2
        },
        {
            "paper_title": "A de novo molecular generation method using latent vector based generative adversarial network.",
            "rating": 2
        },
        {
            "paper_title": "MolGAN: an implicit generative model for small molecular graphs",
            "rating": 2
        },
        {
            "paper_title": "Guacamol: benchmarking models for de novo molecular design.",
            "rating": 1
        },
        {
            "paper_title": "Molecular de-novo design through deep reinforcement learning.",
            "rating": 1
        }
    ],
    "cost": 0.01742725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models</h1>
<p>Daniil Polykovskiy ${ }^{1 <em>}$, Alexander Zhebrak ${ }^{1}$, Benjamin Sanchez-Lengeling ${ }^{2}$, Sergey Golovanov ${ }^{3}$, Oktai Tatanov ${ }^{3}$, Stanislav Belyaev ${ }^{3}$, Rauf Kurbanov ${ }^{3}$, Aleksey Artamonov ${ }^{3}$, Vladimir Aladinskiy ${ }^{1}$, Mark Veselov ${ }^{1}$, Artur Kadurin ${ }^{1}$, Simon Johansson ${ }^{4}$, Hongming Chen ${ }^{4}$, Sergey Nikolenko ${ }^{1,3,5 </em>}$, Alán Aspuru-Guzik ${ }^{6,7,8,9 <em>}$ and Alex Zhavoronkov ${ }^{1 </em>}$<br>OPEN ACCESS</p>
<p>Edited by:
Jianxun Ding, Chinese Academy of Sciences, China</p>
<h2>Reviewed by:</h2>
<p>Nazareno Paolocci, Johns Hopkins University, United States Felix Zhou, University of Oxford, United Kingdom
*Correspondence: Daniil Polykovskiy daniil@insilico.com Alex Zhavoronkov alex@insilico.com Alán Aspuru-Guzik alan@aspuru.com Sergey Nikolenko snikolenko@gmail.com</p>
<p>Specialty section:
This article was submitted to Translational Pharmacology, a section of the journal Frontiers in Pharmacology
Received: 25 May 2020
Accepted: 26 October 2020
Published: 18 December 2020
Citation:
Polykovskiy D, Zhebrak A, Sanchez-Lengeling B, Golovanov S, Tatanov O, Belyaev S, Kurbanov R, Artamonov A, Aladinskiy V, Veselov M, Kadurin A, Johansson S, Chen H, Nikolenko S, Aspuru-Guzik A and Zhavoronkov A (2020) Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models. Front. Pharmacol. 11:565644. doi: 10.3389/fphar.2020.565644</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Generative models are becoming a tool of choice for exploring the molecular space. These models learn on a large training dataset and produce novel molecular structures with similar properties. Generated structures can be utilized for virtual screening or training semi-supervized predictive models in the downstream tasks. While there are plenty of generative models, it is unclear how to compare and rank them. In this work, we introduce a benchmarking platform called Molecular Sets (MOSES) to standardize training and comparison of molecular generative models. MOSES provides training and testing datasets, and a set of metrics to evaluate the quality and diversity of generated structures. We have implemented and compared several molecular generation models and suggest to use our results as reference points for further advancements in generative chemistry research. The platform and source code are available at https://github.com/ molecularsets/moses.</p>
<p>Keywords: generative models, drug discovery, deep learning, benchmark, distribution learning</p>
<h2>INTRODUCTION</h2>
<p>The discovery of new molecules for drugs and materials can bring enormous societal and technological progress, potentially curing rare diseases and providing a pathway for personalized precision medicine (Lee et al., 2018). However, complete exploration of the huge space of potential chemicals is computationally intractable; it has been estimated that the number of pharmacologically-sensible molecules is in the order of $10^{23}$ to $10^{80}$ compounds (Kirkpatrick and Ellis, 2004; Reymond, 2015). Often, this search is constrained based on already discovered structures and desired qualities such as solubility or toxicity. There have been many approaches to exploring the chemical space in silico and in vitro, including high throughput screening, combinatorial libraries, and evolutionary algorithms (Hu et al., 2009; Curtarolo et al., 2013; Pyzer-Knapp et al., 2015; Le and Winkler, 2016). Recent works demonstrated that machine learning methods can produce new small molecules (Merk et al., 2018a; Merk et al., 2018b; Polykovskiy et al., 2018b; Zhavoronkov et al., 2019a) and peptides (Grisoni et al., 2018) showing biological activity.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p><strong>FIGURE 1 |</strong> Molecular Sets (MOSES) pipeline. The open-source library provides a dataset, baseline models, and evaluation metrics.</p>
<p>Over the last few years, advances in machine learning, and especially in deep learning, have driven the design of new computational systems for modeling increasingly complex phenomena. One approach that has been proven fruitful for modeling molecular data is deep generative models. Deep generative models have found applications in a wide range of settings, from generating synthetic images (Karras et al., 2018) and natural language texts (Yu et al., 2017), to the applications in biomedicine, including the design of DNA sequences (Killoran et al., 2017), and aging research (Zhavoronkov et al., 2019b). One important field of application for deep generative models lies in the inverse design of drug compounds (Sanchez-Lengeling and Aspuru-Guzik, 2018) for a given functionality (solubility, ease of synthesis, toxicity). Deep learning also found other applications in biomedicine (Mamoshina et al., 2016; Ching et al., 2018), including target identification (Mamoshina et al., 2018), antibacterial drug discovery (Ivanenkov et al., 2019), and drug repurposing (Aliper et al., 2016; Vanhaelen et al., 2017).</p>
<p>Part of the success of deep learning in different fields has been driven by ever-growing availability of large datasets and standard benchmark sets. These sets serve as a common measuring stick for newly developed models and optimization strategies (LeCun et al., 1998; Deng et al., 2009). In the context of organic molecules, MoleculeNet (Wu et al., 2018) was introduced as a standardized benchmark suite for regression and classification tasks. Brown et al. (2019) proposed to evaluate generative models on goal-oriented and distribution learning tasks with a focus on the former. We focus on standardizing metrics and data for the distribution learning problem that we introduce below.</p>
<p>In this work, we provide a benchmark suite—Molecular Sets (MOSES)—for molecular generation: a standardized dataset, data preprocessing utilities, evaluation metrics, and molecular generation models. We hope that our platform will serve as a clear and unified testbed for current and future generative models. We illustrate the main components of MOSES in Figure 1.</p>
<h3>Distribution Learning</h3>
<p>In MOSES, we study distribution learning models. Formally, given a set of training samples <em>X</em><sub><em>t</em><em>t</em></sub> = {<em>x</em><sub><em>t</em></sub><sup><em>t</em></sup>, ..., <em>x</em><sub><em>t</em>&gt;<em>t</em></sub><sup><em>t</em></sup>} from an unknown distribution <em>p</em>(<em>x</em>), distribution learning models approximate <em>p</em>(<em>x</em>) with some distribution <em>q</em>(<em>x</em>).</p>
<p>Distribution learning models are mainly used for building virtual libraries (van Hilten et al., 2019) for computer-assisted drug discovery. While imposing simple rule-based restrictions on a virtual library (such as maximum or minimum weight) is straightforward, it is unclear how to apply implicit or soft restrictions on the library. For example, a medicinal chemist might expect certain substructures to be more prevalent in generated structures. Relying on a set of manually or automatically selected compounds, distribution learning models produce a larger dataset, preserving implicit rules from the dataset. Another application of distribution learning models is extending the training set for downstream semi-supervized predictive tasks: one can add new unlabeled data by sampling compounds from a generative model.</p>
<p>The quality of a distribution learning model is a deviation measure between <em>p</em>(<em>x</em>) and <em>q</em>(<em>x</em>). The model can define a probability mass function <em>q</em>(<em>x</em>) implicitly or explicitly. Explicit models such as Hidden Markov Models, n-gram language models, or normalizing flows (Dinh et al., 2017; Shi et al., 2019) can analytically compute <em>q</em>(<em>x</em>) and sample from it. Implicit models, such as variational autoencoders, adversarial autoencoders, or generative adversarial networks (Kadurin et al., 2016; De Cao and Kipf, 2018; Gómez-Bombarelli et al., 2018) can sample from <em>q</em>(<em>x</em>), but can not compute the exact values of the probability mass function. To compare both kinds of models, evaluation metrics considered in this paper depend only on samples from <em>q</em>(<em>x</em>).</p>
<h3>Molecular Representations</h3>
<p>In this section, we discuss different approaches to representing a molecule in a machine learning-friendly way (Figure 2): string and graph representations.</p>
<p><strong>String representations.</strong> Representing a molecular structure as a string have been quickly adopted (Jaques et al., 2016; Guimaraes et al., 2017; Kadurin et al., 2017; Olivecrona et al., 2017; Yang et al., 2017; Kang and Cho, 2018; Popova et al., 2018; Putin et al., 2018; Segler et al., 2018) for generative models due to the abundance of sequence modeling tools such as recurrent neural networks, attention mechanisms, and dilated convolutions. Simplified molecular input line entry system (SMILES) (Weininger, 1988) is the most widely used string representation for generative machine learning models.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>SMILES algorithm traverses a spanning tree of a molecular graph in depth-first order and stores atom and edge tokens. SMILES also uses special tokens for branching and edges not covered with a spanning tree. Note that since a molecule can have multiple spanning trees, different SMILES strings can represent a single molecule. While there is a canonicalization procedure to uniquely construct a SMILES string from a molecule (Weininger et al., 1989), ambiguity of SMILES can also serve as augmentation and improve generative models (Arús-Pous et al., 2019).</p>
<p>DeepSMILES (O'Boyle and Dalke, 2018) was introduced as an extension of SMILES that seeks to reduce invalid sequences by altering syntax for branches and ring closures. Some methods try to incorporate SMILES syntax into a network architecture to increase the fraction of valid molecules (Kusner et al., 2017; Dai et al., 2018). SELFIES (Krenn et al., 2019) defines a new syntax based on a Chomsky type-2 grammar augmented with self-referencing functions. International Chemical Identifier (InChI) (Stein et al., 2003) is a more verbose string representation which explicitly specifies a chemical formula, atoms' charges, hydrogens, and isotopes. However, Gómez-Bombarelli et al. (2018) reported that InChI-based models perform substantially worse than SMILES-based models in generative modeling—presumably due to a more complex syntax.</p>
<p>Molecular graphs. Graph representations have long been used in chemoinformatics for storing and processing molecular data. In a molecular graph, each node corresponds to an atom and each edge corresponds to a bond. Such graph can specify hydrogens either explicitly or implicitly. In the latter case, the number of hydrogens can be deduced from atoms' valencies.</p>
<p>Classical machine learning methods mostly utilize molecular descriptors extracted from such graphs. Deep learning models, however, can learn from graphs directly with models such as Graph Convolutional Networks (Duvenaud et al., 2015), Weave Networks (Wu et al., 2018), and Message Passing Networks (Gilmer et al., 2017). Molecular graph can also be represented as adjacency matrix and node feature matrix; this approach has been successfully employed in the MolGAN model (De Cao and Kipf, 2018) for the QM9 dataset (Ramakrishnan et al., 2014). Other approaches such as Junction Tree VAE (Jin et al., 2018) process molecules in terms of their subgraphs.</p>
<h3>Metrics</h3>
<p>In this section, we propose a set of metrics to assess the quality of generative models. The proposed metrics detect common issues in generative models such as overfitting, imbalance of frequent structures or mode collapse. Each metric depends on a generated set <em>G</em> and a test (reference) set <em>R</em>. We compute all metrics (except for validity) only for valid molecules from the generated set. We suggest generating 30,000 molecules and obtaining <em>G</em> as valid molecules from this set.</p>
<p>Fraction of valid (Valid) and unique (Unique@k) molecules report validity and uniqueness of the generated SMILES strings. We define validity using RDKit's molecular structure parser that checks atoms' valency and consistency of bonds in aromatic rings. In the experiments, we compute Unique@<em>K</em> and for the first <em>K</em> = 1,000 and <em>K</em> = 10,000 valid molecules in the generated set. If the number of valid molecules is less than <em>K</em>, we compute uniqueness on all valid molecules. Validity measures how well the model captures explicit chemical constraints such as proper valence. Uniqueness checks that the model does not collapse to producing only a few typical molecules.</p>
<p>Novelty is the fraction of the generated molecules that are not present in the training set. Low novelty indicates overfitting.</p>
<p>Filters is the fraction of generated molecules that pass filters applied during dataset construction (see Section 5). While the generated molecules are often chemically valid, they may contain unwanted fragments: when constructing the training dataset, we removed molecules with such fragments and expect the models to avoid producing them.</p>
<p>Fragment similarity (Frag) compares distributions of BRICS fragments (Degen et al., 2008) in generated and reference sets. Denoting <em>c<sup>f</sup></em> (<em>A</em>) a number of times a substructure <em>f</em> appears in molecules from set <em>A</em>, and a set of fragments that appear in either <em>G</em> or <em>R</em> as <em>F</em>, the metric is defined as a cosine similarity:</p>
<p>$$
\text{Frag}(G, R) = \frac{\sum_{f \in F} \left[ c_f(G) \cdot c_f(R) \right]}{\sqrt{\sum_{f \in F} c_f^2(G)} \sqrt{\sum_{f \in F} c_f^2(R)}}.
$$</p>
<p>If molecules in both sets have similar fragments, Frag metric is large. If some fragments are over- or underrepresented (or never appear) in the generated set, the metric will be lower. Limits of this metric are [0,1].</p>
<p>Scaffold similarity (Scaff) is similar to fragment similarity metric, but instead of fragments we compare frequencies of Bemis–Murcko scaffolds (Bemis and Murcko, 1996). Bemis–Murcko scaffold contains all molecule's ring structures and linker fragments connecting rings. We use RDKit implementation of this algorithm which additionally considers carbonyl groups attached to rings as part of a scaffold. Denoting <em>c<sup>s</sup></em> (<em>A</em>) a number of times a scaffold <em>s</em> appears in molecules from set <em>A</em>, and a set of fragments that appear in either <em>G</em> or <em>R</em> as <em>S</em>, the metric is defined as a cosine similarity:</p>
<p>$$
\operatorname{Frag}(G, R)=\frac{\sum_{i \in S}\left[c_{i}(G) \cdot c_{i}(R)\right]}{\sqrt{\sum_{i \in S} c_{i}^{2}(G)} \sqrt{\sum_{i \in S} c_{i}^{2}(R)}}
$$</p>
<p>The purpose of this metric is to show how similar are the scaffolds present in generated and reference datasets. For example, if the model rarely produces a certain chemotype from a reference set, the metric will be low. Limits of this metric are $[0,1]$.</p>
<p>Note that both fragment and scaffold similarities compare molecules at a substructure level. Hence, it is possible to have a similarity one even when $G$ and $R$ contain different molecules.</p>
<p>Similarity to a nearest neighbor (SNN) is an average Tanimoto similarity $T\left(m_{G}, m_{R}\right)$ (also known as the Jaccard index) between fingerprints of a molecule $m_{G}$ from the generated set $G$ and its nearest neighbor molecule $m_{R}$ in the reference dataset $R$ :</p>
<p>$$
\operatorname{SNN}(G, R)=\frac{1}{|G|} \sum_{m_{G} \in G} \max <em R="R">{m</em>\right)
$$} \in R} T\left(m_{G}, m_{R</p>
<p>In this work, we used standard Morgan (extended connectivity) fingerprints (Rogers and Hahn, 2010) with radius 2 and 1024 bits computed using RDKit library (Landrum, 2006). The resulting similarity metric can be interpreted as precision: if generated molecules are far from the manifold of the reference set, similarity to the nearest neighbor will be low. Limits of this metric are $[0,1]$.</p>
<p>Internal diversity (IntDiv $\mathbf{p}_{\boldsymbol{p}}$ ) (Benhenda, 2017) assesses the chemical diversity within the generated set of molecules $G$.</p>
<p>$$
\operatorname{IntDiv}<em m__1="m_{1">{p}(G)=1-\sqrt{\frac{1}{|G|^{2}} \sum</em>
$$}, m_{2} \in G} T\left(m_{1}, m_{2}\right)^{p}</p>
<p>This metric detects a common failure case of generative models-mode collapse. With mode collapse, the model produces a limited variety of samples, ignoring some areas of the chemical space. A higher value of this metric corresponds to higher diversity in the generated set. In the experiments, we report $\operatorname{IntDiv}<em 2="2">{1}(\mathrm{G})$ and $\operatorname{IntDiv}</em>)$. Limits of this metric are $[0,1]$.}(\mathrm{G</p>
<p>Fréchet ChemNet Distance (FCD) (Preuer et al., 2018) is calculated using activations of the penultimate layer of a deep neural network ChemNet trained to predict biological activities of drugs. We compute activations for canonical SMILES representations of molecules. These activations capture both chemical and biological properties of the compounds. For two sets of molecules $G$ and $R$, FCD is defined as</p>
<p>$$
\operatorname{FCD}(G, R)=\left|\mu_{G}-\mu_{R}\right|^{2}+\operatorname{Tr}\left[\Sigma_{G}+\Sigma_{R}-2\left(\Sigma_{G} \Sigma_{R}\right)^{1 / 2}\right]
$$</p>
<p>where $\mu_{G}, \mu_{R}$ are mean vectors and $\Sigma_{G}, \Sigma_{R}$ are full covariance matrices of activations for molecules from sets $G$ and $R$ respectively. FCD correlates with other metrics. For example, if the generated structures are not diverse enough (low IntDiv $\mathbf{p}_{\boldsymbol{p}}$ ) or the model produces too many duplicates (low uniqueness), FCD will decrease, since the variance is smaller. We suggest using FCD for hyperparameter tuning and final
model selection. Values of this metric are non-negative, lower is better.</p>
<p>Properties distribution is a useful tool for visually assessing the generated structures. To quantitatively compare the distributions in the generated and test sets, we compute a 1D Wasserstein-1 distance between property distributions of generated and test sets. We also visualize a kernel density estimation of these distributions in the Experiments section. We use the following four properties:</p>
<ul>
<li>Molecular weight (MW): the sum of atomic weights in a molecule. By plotting histograms of molecular weight for the generated and test sets, one can judge if a generated set is biased toward lighter or heavier molecules.</li>
<li>LogP: the octanol-water partition coefficient, a ratio of a chemical's concentration in the octanol phase to its concentration in the aqueous phase of a two-phase octanol/water system; computed with RDKit's Crippen (Wildman and Crippen, 1999) estimation.</li>
<li>Synthetic Accessibility Score (SA): a heuristic estimate of how hard (10) or how easy (1) it is to synthesize a given molecule. SA score is based on a combination of the molecule's fragments contributions (Ertl and Schuffenhauer, 2009). Note that SA score does not adequately assess up-to-date chemical structures, but it is useful for assessing distribution learning models.</li>
<li>Quantitative Estimation of Drug-likeness (QED): a [0,1] value estimating how likely a molecule is a viable candidate for a drug. QED is meant to capture the abstract notion of esthetics in medicinal chemistry (Bickerton et al., 2012). Similar to SA, descriptor limits in QED have been changing during the last decade and current limits may not cover latest drugs (Shultz, 2018).</li>
</ul>
<h2>DATASET</h2>
<p>The proposed dataset used for training and testing is based on the ZINC Clean Leads (Sterling and Irwin, 2015) collection which contains 4, 591, 276 molecules with molecular weight in the range from 250 to 350 Da , a number of rotatable bonds not greater than 7, and XlogP (Wang et al., 1997) not greater then 3.5. Clean-leads dataset consists of structures suitable for identifying hit compounds and they are small enough to allow for further ADMET optimization of generated molecules (Teague et al., 1999). We removed molecules containing charged atoms, atoms besides $\mathrm{C}, \mathrm{N}, \mathrm{S}, \mathrm{O}, \mathrm{F}, \mathrm{Cl}, \mathrm{Br}, \mathrm{H}$, or cycles larger than eight atoms. The molecules were filtered via custom medicinal chemistry filters (MCFs) and PAINS filters (Baell and Holloway, 2010). We describe MCFs and discuss PAINS in Supplementary Information 1. We removed charged molecules to avoid ambiguity with tautomers and pH conditions. Note that in the initial set of molecules, functional groups were present in both ionized and unionized forms.</p>
<p>The final dataset contains molecules, with internal diversity $\operatorname{IntDiv}_{1}=0.857$; it contains 448,854 unique Bemis-Murcko (Bemis and Murcko, 1996) scaffolds and 58, 315 unique BRICS</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p><strong>FIGURE 3</strong> Examples of molecules from MOSES dataset.</p>
<p>(Degen et al., 2008) fragments. We show example molecules in <strong>Figure 3</strong> and a representative diverse subset in Supplementary Information 2. We provide recommended split into three non-intersecting parts: train (1,584,664 molecules), test (176,075 molecules) and scaffold test (176,226 molecules). The scaffold test set has all molecules containing a Bemis-Murcko scaffold from a random subset of scaffolds. Hence, scaffolds from the scaffold test set differ from scaffolds in both train and test sets. We use scaffold test split to assess whether a model can produce novel scaffolds absent in the training set. The test set is a random subset of the remaining molecules in the dataset.</p>
<h1><strong>BASELINES</strong></h1>
<p>We implemented several models that cover different approaches to molecular generation, such as character-level recurrent neural networks (CharRNN) (Preuer et al., 2018; Segler et al., 2018), Variational Autoencoders (VAE) (Kadurin et al., 2016; Blaschke et al., 2018; Gómez-Bombarelli et al., 2018), Adversarial Autoencoders (AAE) (Kadurin et al., 2016; Polykovskiy et al., 2018b), Junction Tree Variational Autoencoders (JTN-VAE) (Jin et al., 2018), LatentGAN (Prykhodko et al., 2019), and non-neural baselines.</p>
<p>Model comparison can be challenging since different training parameters (number of epochs, batch size, learning rate, initial state, optimizer) and architecture hyperparameters (hidden layer dimension, number of layers, etc.) can significantly alter their performance. For each model, we attempted to preserve its original architecture as published and tuned the hyperparameters to improve the performance. We used random search over multiple architectures for every model and selected the architecture that produced the best value of FCD. Models are implemented in <em>Python</em> 3 utilizing PyTorch (Paszke et al., 2017) framework. Please refer to the Supplementary Information three for the training details and hyperparameters.</p>
<p><strong>Character-level recurrent neural network (CharRNN)</strong> (Segler et al., 2018) models a distribution over the next token given previously generated ones. We train this model by maximizing log-likelihood of the training data represented as SMILES strings.</p>
<p><strong>Variational autoencoder (VAE)</strong> (Kingma and Welling, 2013) consists of two neural networks—an encoder and a decoder—that infer a mapping from high-dimensional data representation onto a lower-dimensional space and back. The lower-dimensional space is called the latent space, which is often a continuous vector space with normal prior distribution. VAE parameters are optimized to encode and decode data by minimizing reconstruction loss and regularization term in a form of Kullback-Leibler divergence. VAE-based architecture for the molecular generation was studied in multiple previous works (Kadurin et al., 2016; Blaschke et al., 2018; Gómez-Bombarelli et al., 2018). We combine aspects from these implementations and use SMILES as input and output representations.</p>
<p><strong>Adversarial Autoencoder (AAE)</strong> (Makhzani et al., 2016) replaces the Kullback-Leibler divergence from VAE with an adversarial objective. An auxiliary discriminator network is trained to distinguish samples from a prior distribution and model's latent codes. The encoder then adapts its latent codes to minimize discriminator's predictive accuracy. The training process oscillates between training the encoder-decoder pair and the discriminator. Unlike Kullback-Leibler divergence that has a closed-form analytical solution only for a handful of distributions, a discriminator can be used for any prior distribution. AAE-based models for molecular design were studied in (Kadurin et al., 2016; Kadurin et al., 2017; Polykovskiy et al., 2018b). Similar to VAE, we use SMILES as input and output representations.</p>
<p><strong>TABLE 1</strong> Performance metrics for baseline models: fraction of valid molecules, fraction of unique molecules from and molecules.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Valid (↑)</th>
<th>Unique@1k (↑)</th>
<th>Unique@10k (↑)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Train</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr>
<td>HMM</td>
<td>0.076 ± 0.0322</td>
<td>0.623 ± 0.1224</td>
<td>0.5671 ± 0.1424</td>
</tr>
<tr>
<td>NGram</td>
<td>0.2376 ± 0.0025</td>
<td>0.974 ± 0.0108</td>
<td>0.9217 ± 0.0019</td>
</tr>
<tr>
<td>Combinatorial</td>
<td>1.0 ± 0.0</td>
<td>0.9983 ± 0.0015</td>
<td>0.9909 ± 0.0009</td>
</tr>
<tr>
<td>CharRNN</td>
<td>0.975 ± 0.026</td>
<td>1.0 ± 0.0</td>
<td>0.999 ± 0.0</td>
</tr>
<tr>
<td>VAE</td>
<td>0.977 ± 0.001</td>
<td>1.0 ± 0.0</td>
<td>0.998 ± 0.001</td>
</tr>
<tr>
<td>AAE</td>
<td>0.937 ± 0.034</td>
<td>1.0 ± 0.0</td>
<td>0.997 ± 0.002</td>
</tr>
<tr>
<td>JTN-VAE</td>
<td>1.0 ± 0.0</td>
<td>1.0 ± 0.0</td>
<td>0.9996 ± 0.0003</td>
</tr>
<tr>
<td>LatentGAN</td>
<td>0.897 ± 0.002</td>
<td>1.0 ± 0.0</td>
<td>0.997 ± 0.005</td>
</tr>
</tbody>
</table>
<p><em>Reported (mean ± SD) over three independent model initializations.</em></p>
<p>TABLE 2 | Performance metrics for baseline models: fraction of molecules passing filters (MCF, PAINS, ring sizes, charge, atom types), novelty, and internal diversity.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Filters ( $\uparrow$ )</th>
<th>Novelty ( $\uparrow$ )</th>
<th>IntDiv $_{1}$</th>
<th>IntDiv $_{2}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Train</td>
<td>1.0</td>
<td>0.0</td>
<td>0.857</td>
<td>0.851</td>
</tr>
<tr>
<td>HMM</td>
<td>$0.9024 \pm 0.0489$</td>
<td>$0.9994 \pm 0.001$</td>
<td>$0.8466 \pm 0.0403$</td>
<td>$0.8104 \pm 0.0507$</td>
</tr>
<tr>
<td>NGram</td>
<td>$0.9582 \pm 0.001$</td>
<td>$0.9694 \pm 0.001$</td>
<td>$0.8738 \pm 0.0002$</td>
<td>$0.8644 \pm 0.0002$</td>
</tr>
<tr>
<td>Combinatorial</td>
<td>$0.9557 \pm 0.0018$</td>
<td>$0.9878 \pm 0.0008$</td>
<td>$0.8732 \pm 0.0002$</td>
<td>$0.8666 \pm 0.0002$</td>
</tr>
<tr>
<td>CharRNN</td>
<td>$0.994 \pm 0.003$</td>
<td>$0.842 \pm 0.051$</td>
<td>$0.856 \pm 0.0$</td>
<td>$0.85 \pm 0.0$</td>
</tr>
<tr>
<td>VAE</td>
<td>$0.997 \pm 0.0$</td>
<td>$0.895 \pm 0.007$</td>
<td>$0.856 \pm 0.0$</td>
<td>$0.85 \pm 0.0$</td>
</tr>
<tr>
<td>AAE</td>
<td>$0.996 \pm 0.001$</td>
<td>$0.793 \pm 0.028$</td>
<td>$0.856 \pm 0.003$</td>
<td>$0.85 \pm 0.003$</td>
</tr>
<tr>
<td>JTN-VAE</td>
<td>$0.976 \pm 0.0016$</td>
<td>$0.9143 \pm 0.0058$</td>
<td>$0.8551 \pm 0.0034$</td>
<td>$0.8493 \pm 0.0035$</td>
</tr>
<tr>
<td>LatentGAN</td>
<td>$0.973 \pm 0.001$</td>
<td>$0.949 \pm 0.001$</td>
<td>$0.857 \pm 0.0$</td>
<td>$0.85 \pm 0.0$</td>
</tr>
</tbody>
</table>
<p>Reported (mean $\pm$ SD) over three independent model initializations.</p>
<p>TABLE 3 | Performance metrics for baseline models: Fréchet ChemNet Distance (FCD) and Similarity to a nearest neighbor (SNN).</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>FCD ( $\downarrow$ )</th>
<th></th>
<th>SNN ( $\uparrow$ )</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Test</td>
<td>TestSF</td>
<td>Test</td>
<td>TestSF</td>
</tr>
<tr>
<td>Train</td>
<td>0.008</td>
<td>0.476</td>
<td>0.642</td>
<td>0.586</td>
</tr>
<tr>
<td>HMM</td>
<td>$24.4661 \pm 2.5251$</td>
<td>$25.4312 \pm 2.5599$</td>
<td>$0.3876 \pm 0.0107$</td>
<td>$0.3795 \pm 0.0107$</td>
</tr>
<tr>
<td>NGram</td>
<td>$5.5069 \pm 0.1027$</td>
<td>$6.2306 \pm 0.0966$</td>
<td>$0.5209 \pm 0.001$</td>
<td>$0.4997 \pm 0.0005$</td>
</tr>
<tr>
<td>Combinatorial</td>
<td>$4.2375 \pm 0.037$</td>
<td>$4.5113 \pm 0.0274$</td>
<td>$0.4514 \pm 0.0003$</td>
<td>$0.4388 \pm 0.0002$</td>
</tr>
<tr>
<td>CharRNN</td>
<td>$0.073 \pm 0.025$</td>
<td>$0.52 \pm 0.038$</td>
<td>$0.601 \pm 0.021$</td>
<td>$0.565 \pm 0.014$</td>
</tr>
<tr>
<td>VAE</td>
<td>$0.099 \pm 0.013$</td>
<td>$0.567 \pm 0.034$</td>
<td>$0.626 \pm 0.0$</td>
<td>$0.578 \pm 0.001$</td>
</tr>
<tr>
<td>AAE</td>
<td>$0.556 \pm 0.203$</td>
<td>$1.057 \pm 0.237$</td>
<td>$0.608 \pm 0.004$</td>
<td>$0.568 \pm 0.005$</td>
</tr>
<tr>
<td>JTN-VAE</td>
<td>$0.3954 \pm 0.0234$</td>
<td>$0.9382 \pm 0.0531$</td>
<td>$0.5477 \pm 0.0076$</td>
<td>$0.5194 \pm 0.007$</td>
</tr>
<tr>
<td>LatentGAN</td>
<td>$0.296 \pm 0.021$</td>
<td>$0.824 \pm 0.030$</td>
<td>$0.538 \pm 0.001$</td>
<td>$0.514 \pm 0.009$</td>
</tr>
</tbody>
</table>
<p>Reported (mean $\pm$ SD) over three independent model initializations. Results for random test set (Test) and scaffold split test set (TestSF).</p>
<p>TABLE 4 | Fragment similarity (Frag), Scaffold similarity (Scaff).</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Frag ( $\uparrow$ )</th>
<th></th>
<th>Scaf ( $\uparrow$ )</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Test</td>
<td>TestSF</td>
<td>Test</td>
<td>TestSF</td>
</tr>
<tr>
<td>Train</td>
<td>1.0</td>
<td>0.999</td>
<td>0.991</td>
<td>0.0</td>
</tr>
<tr>
<td>HMM</td>
<td>$0.5754 \pm 0.1224$</td>
<td>$0.5681 \pm 0.1218$</td>
<td>$0.2065 \pm 0.0481$</td>
<td>$0.049 \pm 0.018$</td>
</tr>
<tr>
<td>NGram</td>
<td>$0.9846 \pm 0.0012$</td>
<td>$0.9815 \pm 0.0012$</td>
<td>$0.5302 \pm 0.0163$</td>
<td>$0.0977 \pm 0.0142$</td>
</tr>
<tr>
<td>Combinatorial</td>
<td>$0.9912 \pm 0.0004$</td>
<td>$0.9904 \pm 0.0003$</td>
<td>$0.4445 \pm 0.0056$</td>
<td>$0.0865 \pm 0.0027$</td>
</tr>
<tr>
<td>CharRNN</td>
<td>$\mathbf{1 . 0} \pm \mathbf{0 . 0}$</td>
<td>$\mathbf{0 . 9 9 8} \pm \mathbf{0 . 0}$</td>
<td>$0.924 \pm 0.006$</td>
<td>$\mathbf{0 . 1 1} \pm \mathbf{0 . 0 0 8}$</td>
</tr>
<tr>
<td>VAE</td>
<td>$0.999 \pm 0.0$</td>
<td>$\mathbf{0 . 9 9 8} \pm \mathbf{0 . 0}$</td>
<td>$\mathbf{0 . 9 3 9} \pm \mathbf{0 . 0 0 2}$</td>
<td>$0.059 \pm 0.01$</td>
</tr>
<tr>
<td>AAE</td>
<td>$0.991 \pm 0.005$</td>
<td>$0.99 \pm 0.004$</td>
<td>$0.902 \pm 0.037$</td>
<td>$0.079 \pm 0.009$</td>
</tr>
<tr>
<td>JTN-VAE</td>
<td>$0.9965 \pm 0.0003$</td>
<td>$0.9947 \pm 0.0002$</td>
<td>$0.8964 \pm 0.0039$</td>
<td>$0.1009 \pm 0.0105$</td>
</tr>
<tr>
<td>LatentGAN</td>
<td>$0.999 \pm 0.003$</td>
<td>$\mathbf{0 . 9 9 8} \pm \mathbf{0 . 0 0 3}$</td>
<td>$0.886 \pm 0.015$</td>
<td>$0.1 \pm 0.006$</td>
</tr>
</tbody>
</table>
<p>Reported (mean $\pm$ SD) over three independent model initializations. Results for random test set (Test) and scaffold split test set (TestSF).</p>
<p>Junction Tree VAE (JTN-VAE) (Jin et al., 2018) generates molecules in two phases by exploiting valid subgraphs as components. In the first phase, it generates a tree-structured object (a junction tree) whose role is to represent the scaffold of subgraph components and their coarse relative arrangements. The components are valid chemical substructures automatically extracted from the training set. In the second phase, the subgraphs (nodes of the tree) are assembled together into a coherent molecular graph.</p>
<p>Latent Vector Based Generative Adversarial Network (LatentGAN) (Prykhodko et al., 2019) combines an autoencoder and a generative adversarial network. LatentGAN pretrains an autoencoder to map SMILES structures onto latent vectors. A generative adversarial network is then trained to produce latent vectors for the pre-trained decoder.</p>
<p>Non-neural baselines implemented in MOSES are n-gram generative model, Hidden Markov Model (HMM), and a combinatorial generator. N-gram model collects statistics of n-grams frequencies in the training set and uses such distribution to sequentially sample new strings. Hidden Markov models utilize Baum-Welch algorithm to learn a probabilistic distribution over the SMILES strings. The model consists of several states $\left(s_{1}, \ldots, s_{K}\right)$, transition probabilities between states $p\left(s_{i+1} \mid s_{i}\right)$, and token emission</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>FIGURE 4 | Distribution of chemical properties for MOSES dataset and sets of generated molecules. In brackets—Wasserstein-1 distance to MOSES test set. Parameters: molecular weight, octanol-water partition coefficient (logP), quantitative estimation of drug-likeness (QED) and synthetic accessibility score (SA).</p>
<p>probabilities p(x_{i} | s_{i}). Beginning from a "start" state, at each iteration the model samples a next token and state from emission and transition probabilities correspondingly. A combinatorial generator splits molecular graphs of the training data into BRICS fragments and generates new molecules by randomly connecting random substructures. We sample fragments according to their frequencies in the training set to model the distribution better.</p>
<h3>PLATFORM</h3>
<p>The dataset, metrics and baseline models are provided in a GitHub repository https://github.com/molecularsets/moses and as a PyPI package molsets. To contribute a new model, one should train a model on MOSES train set, generate 30,000 samples and compute metrics using the provided utilities. We recommend running the experiment at least three times with different random seeds to estimate sensitivity of the model to random parameter initialization. We store molecular structures in SMILES format; molecular graphs can be reconstructed using RDKit (Landrum, 2006).</p>
<h3>RESULTS</h3>
<p>We trained the baseline models on MOSES train set and provide results in this section. In Table 1 we compare models with respect to the validity and uniqueness metrics. Hidden Markov Model and NGram models fail to produce valid molecules since they have a limited context. Combinatorial generator and JTN-VAE have built-in validity constraints, so their validity is 100%.</p>
<p>Table 2 reports additional properties of the generated set: fraction of molecules passing filters, fraction of molecules not present in the training set, and internal diversity. All modules successfully avoid forbidden structures (MCF and PAINS) even though such restrictions were only defined implicitly—using a training dataset. Combinatorial generator has higher diversity than the training dataset, which might be favorable for discovering new chemical structures. Autoencoder-based models show low novelty, indicating that these models overfit to the training set.</p>
<p>Table 3 reports Fréchet ChemNet Distance (FCD) and similarity to a nearest neighbor (SNN). All neural network-based models show low FCD, indicating that the models successfully captured the statistics of the dataset. Surprisingly, a simple language model, character level RNN, shows the best results.</p>
<p>in terms of the FCD measure. Variational autoencoder (VAE) showed the best results in terms of SNN, but combined with low novelty we suppose that the model overfitted on the training set.</p>
<p>In Table 4 we report similarities of substructure distributions—fragments and scaffolds. Scaffold similarity from the training set to the scaffold test set (TestSF) is zero by design. Note that CharRNN successfully discovered many novel scaffolds (11%), suggesting that the model generalizes well.</p>
<p>Finally, we compared distributions of four molecular properties in generated and test sets (Figure 4): molecular weight (MW), octanol-water partition coefficient (logP), quantitative estimation of drug-likeness (QED), and synthetic accessibility score (SA). Deep generative models closely match the data distribution; hidden Markov Model is biased toward lighter molecules, which is consistent with low validity: larger molecules impose more validity constraints. A combinatorial generator has higher variance in molecular weight, producing larger and smaller molecules than those present in the training set.</p>
<h2>DISCUSSION</h2>
<p>From a wide range of presented models, CharRNN currently performs the best in terms of the key metrics. Specifically, it produces the best FCD, Fragment, and Scaffold scores, indicating that the model not only captured the training distribution well, but also did not overfit on the training set.</p>
<p>The presented set of metrics assesses models' performance from different perspectives; therefore, for each specific downstream task, one could consider the most relevant metric. For example, evaluation based on Scaf/TestSF score could be relevant when model's objective is to discover novel scaffolds. For a general evaluation, we suggest using FCD/Test metric that captures multiple aspects of other metrics in a single number. However, it does not give insights into specific issues that cause high FCD/Test values, hence more interpretable metrics presented in this paper are necessary to investigate the model's performance thoroughly.</p>
<h2>CONCLUSION</h2>
<p>With MOSES, we have designed a molecular generation benchmark platform that provides a dataset with molecular structures, an implementation of baseline models, and metrics for their evaluation. While standardized comparative studies and test sets are essential for the progress of machine learning applications, the current field of de novo drug design lacks evaluation protocols for generative machine learning models. Being on the intersection of mathematics, computer science, and chemistry, these applications are often too challenging to explore for research scientists starting in the field. Hence, it is necessary to develop a transparent approach to implementing new models and assessing their performance. We presented a benchmark suite with unified and extendable programming interfaces for generative models and evaluation metrics.</p>
<p>This platform should allow for a fair and comprehensive comparison of new generative models. For future work on this project, we will keep extending the MOSES repository with new baseline models and new evaluation metrics. We hope this work will attract researchers interested in tackling drug discovery challenges.</p>
<h2>DATA AVAILABILITY STATEMENT</h2>
<p>The data and code of the MOSES platform is available at https://github.com/molecularsets/moses.</p>
<h2>AUTHOR CONTRIBUTIONS</h2>
<p>DP, AZhe, SG, OT, SB, RK, AA, AK, SJ, and HC designed and conducted the experiments; DP and AZhe, BS-L, VA, MV, SJ, HC, SN, AA-G, AZha wrote the manuscript.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This manuscript has been released as a pre-print at https://arxiv.org/abs/1811.12823 (Polykovskiy et al., 2018a).</p>
<h2>SUPPLEMENTARY MATERIAL</h2>
<p>The Supplementary Material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/fphar.2020.565644/full#supplementary-material.</p>
<h2>REFERENCES</h2>
<p>Aliper, A., Plis, S., Artemov, A., Ulloa, A., Mamoshina, P., and Zhavoronkov, A. (2016). Deep learning applications for predicting pharmacological properties of drugs and drug repurposing using transcriptomic data. Mol. Pharm. 13, 2524–2530. doi:10.1021/acs.molpharmaceut.6b00248</p>
<p>Arús-Pous, J., Johansson, S. V., Prykhodko, O., Bjerrum, E. J., Tyrchan, C., Reymond, J.-L., et al. (2019). Randomized smiles strings improve the quality of molecular generative models. J. Cheminf. 11, 1–13. doi:10.1186/s13321-019-0393-0</p>
<p>Baell, J. B. and Holloway, G. A. (2010). New substructure filters for removal of pan assay interference compounds (PAINS) from screening libraries and for their exclusion in bioassays. J. Med. Chem. 53, 2719–2740. doi:10.1021/jm901137j</p>
<p>Bemis, G. W. and Murcko, M. A. (1996). The properties of known drugs. 1. molecular frameworks. J. Med. Chem. 39, 2887–2893. doi:10.1021/jm9602928</p>
<p>Benhenda, M. (2017). ChemGAN challenge for drug discovery: can AI reproduce natural chemical diversity? Available from: https://arxiv.org/abs/1708.08227</p>
<p>Bickerton, G. R., Paolini, G. V., Besnard, J., Muresan, S., and Hopkins, A. L. (2012). Quantifying the chemical beauty of drugs. Nat. Chem. 4, 90–98. doi:10.1038/nchem.1243</p>
<p>Blaschke, T., Olivecrona, M., Engkvist, O., Bajorath, J., and Chen, H. (2018). Application of generative autoencoder in de novo molecular design. Mol. Inform. 37, 1700123. doi:10.1002/minf.201700123</p>
<p>Brown, N., Fiscato, M., Segler, M. H. S., and Vaucher, A. C. (2019). Guacamol: benchmarking models for de novo molecular design. J. Chem. Inf. Model. 59, 1096-1108. doi:10.1021/acs.jcim.8b00839
Ching, T., Himmelstein, D. S., Beaulieu-Jones, B. K., Kalinin, A. A., Do, B. T., Way, G. P., et al. (2018). Opportunities and obstacles for deep learning in biology and medicine. J. R. Soc. Interface 15, 20170387. doi:10.1098/rsif.2017.0387
Dai, H., Tian, Y., Dai, B., Skiena, S., and Song, L. (2018). "Syntax-directed variational autoencoder for structured data," in International conference on learning representations.
De Cao, N. and Kipf, T. (2018). "MolGAN: an implicit generative model for small molecular graphs," in ICML 2018 workshop on Theoretical Foundations and Applications of Deep Generative Models.
Degen, J., Wegscheid-Gerlach, C., Zaliani, A., and Rarey, M. (2008). On the art of compiling and using 'drug-like' chemical fragment spaces. ChemMedChem 3, 1503-1507. doi:10.1002/cmdc. 200800178
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). CVPR09.ImageNet: a large-scale hierarchical image database," in 2009 IEEE Conference on Computer Vision and Pattern Recognition, Miami, FL, June 20-25, 2009. IEEE.
Dinh, L., Sohl-Dickstein, J., and Bengio, S. (2017). Density estimation using real NVP. Available at: https://library.aeg.org/doi/10.1190/segam2017-17559486.1
Duvenaud, D. K., Maclaurin, D., Iparraguirre, J., Bombarell, R., Hirzel, T., AspuruGuzik, A., et al. (2015). "Convolutional networks on graphs for learning molecular fingerprints," in Advances in neural information processing systems 28. Editors C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (New York, NY: Curran Associates, Inc.), 2224-2232.</p>
<p>Ertl, P. and Schuffenhauer, A. (2009). Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. J. Cheminf. 1, 8. doi:10.1186/1758-2946-1-8
Ferrero, S., Hart, G. L. W., Nardelli, M. B., Mingo, N., Sanvito, S., and Levy, O. (2013). The high-throughput highway to computational materials design. Nat. Mater. 12, 191-201. doi:10.1038/nmat3568
Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. (2017). Neural message passing for quantum chemistry," in Proceedings of the 34th international conference on machine learning. JMLR, 1263-1272
Gómez-Bombarelli, R., Wei, J. N., Duvenaud, D., Hernández-Lobato, J. M., Sánchez-Lengeling, B., Sheberla, D., et al. (2018). Automatic chemical design using a Data-Driven continuous representation of molecules. ACS Cent. Sci. 4, 268-276. doi:10.1021/acscentsci. 7b00572
Grisoni, F., Neuhaus, C. S., Gabernet, G., Müller, A. T., Hiss, J. A., and Schneider, G. (2018). Designing anticancer peptides by constructive machine learning. ChemMedChem 13, 1300-1302. doi:10.1002/cmdc. 201800204
Guimaraes, G. L., Sanchez-Lengeling, B., Farias, P. L. C., and Aspuru-Guzik, A. (2017). Objective-Reinforced generative adversarial networks (ORGAN) for sequence generation models. Available at: https://arxiv.org/abs/1705.10843.
Hu, X., Beratan, D. N., and Yang, W. (2009). Emergent strategies for inverse molecular design. Sci. China Ser. B-Chem. 52, 1769-1776. doi:10.1007/s11426-009-0260-3
Ivanenkov, Y. A., Zhavoronkov, A., Yamidanov, R. S., Osterman, I. A., Sergiev, P. V., Aladinskiy, V. A., et al. (2019). Identification of novel antibacterials using machine learning techniques. Front. Pharmacol. 10, 913. doi:10.3389/fphar. 2019.00913</p>
<p>Jaques, N., Gu, S., Bahdanau, D., Hernández-Lobato, J. M., Turner, R. E., and Eck, D. (2016). Sequence tutor: conservative fine-tuning of sequence generation models with KL-control. Available at: https://arxiv.org/abs/1611.02796.
Jin, W., Barzilay, R., and Jaakkola, T. (2018). "Junction tree variational autoencoder for molecular graph generation," in Proceedings of the 35th international conference on machine learning. Editors J. Dy and A. Krause (Stockholmsmässan, Stockholm Sweden: PMLR), 2323-2332.
Kadurin, A., Aliper, A., Kazennov, A., Mamoshina, P., Vanhaelen, Q., Khrabrov, K., et al. (2016). The cornucopia of meaningful leads: applying deep adversarial autoencoders for new molecule development in oncology. Oncotarget 8, 10883-10890. doi:10.18632/oncotarget. 14073
Kadurin, A., Nikolenko, S., Khrabrov, K., Aliper, A., and Zhavoronkov, A. (2017). druGAN: an advanced generative adversarial autoencoder model for de novo generation of new molecules with desired molecular properties in silico. Mol. Pharm. 14, 3098-3104. doi:10.1021/acs.molpharmaceut.7b00346</p>
<p>Kang, S. and Cho, K. (2018). Conditional molecular design with deep generative models. J. Chem. Inf. Model. 59, 43-52. doi:10.1021/acs.jcim.8b00263
Karras, T., Aila, T., Laine, S., and Lehtinen, J. (2018). Progressive growing of gans for improved quality, stability, and variation," in International conference on learning representations. ICLR. 1-26.
Killoran, N., Lee, L. J., Delong, A., Duvenaud, D., and Frey, B. J. (2017). Generating and designing DNA with deep generative models. Available from: https://arxiv. org/abs/1712.06148.
Kingma, D. P. and Welling, M. (2013). Auto-Encoding variational bayes," in International conference on learning representations.
Kirkpatrick, P. and Ellis, C. (2004). Chemical space. Nature 432, 823. doi:10.1038/ 432823a
Krenn, M., Häse, F., Nigam, A., Friederich, P., and Aspuru-Guzik, A. (2019). Selfies: a robust representation of semantically constrained graphs with an example application in chemistry. Available at: https://grlearning.github.io/ papers/59.pdf.
Kusner, M. J., Paige, B., and Hernández-Lobato, J. M. (2017). "Grammar variational autoencoder,"in Proceedings of the 34th international conference on machine learning. Editors D. Precup and Y. W. Teh (Sydney, Australia: Proceedings of Machine Learning Research), Vol. 70. 1945-1954.
Labai, R., Fu, Y., and Lai, L. (1997). A new atom-additive method for calculating partition coefficients. J. Chem. Inf. Comput. Sci. 37, 615-621. doi:10.1021/ ci960169p
Landrum, G. (2006). RDKit: open-source cheminformatics. Available at: http:// www.rdkit.org/.
Le, T. C. and Winkler, D. A. (2016). Discovery and optimization of materials using evolutionary approaches. Chem. Rev. 116, 6107-6132. doi:10.1021/acs.chemrev. Sb00691
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to document recognition. Proc. IEEE 86, 2278-2324. doi:10.1109/5.726791
Lee, S.-I., Celik, S., Logsdon, B. A., Lundberg, S. M., Martins, T. J., Oehler, V. G., et al. (2018). A machine learning approach to integrate big data for precision medicine in acute myeloid leukemia. Nat. Commun. 9, 42. doi:10.1038/s41467-017-02465-5
Makhzani, A., Shlens, J., Jaitly, N., and Goodfellow, I. (2016). "Adversarial autoencoders," in International conference on learning representations.
Mamoshina, P., Vieira, A., Putin, E., and Zhavoronkov, A. (2016). Applications of deep learning in biomedicine. Mol. Pharm. 13, 1445-1454. doi:10.1021/acs. molpharmaceut.5b00982
Mamoshina, P., Volosnikova, M., Ozerov, I. V., Putin, E., Skibina, E., Cortese, F., et al. (2018). Machine learning on human muscle transcriptomic data for biomarker discovery and tissue-specific drug target identification. Front. Genet. 9, 242. doi:10.3389/fgene.2018.00242
Merk, D., Friedrich, L., Grisoni, F., and Schneider, G. (2018a). De novo design of bioactive small molecules by artificial intelligence. Mol. Inf. 37, 1700153. doi:10. 1002/minf. 201700153
Merk, D., Grisoni, F., Friedrich, L., and Schneider, G. (2018b). Tuning artificial intelligence on the de novo design of natural-product-inspired retinoid x receptor modulators. Commun. Chem. 1, 68. doi:10.1038/s42004-018-0068-1
Olivecrona, M., Blaschke, T., Engkvist, O., and Chen, H. (2017). Molecular de-novo design through deep reinforcement learning. J. Cheminf. 9, 48. doi:10.1186/ s13321-017-0235-x
O'Boyle, N. and Dalke, A. (2018). DeepSMILES: an adaptation of SMILES for use in machine-learning of chemical structures. ChemRxiv. doi:10.26434/chemrxiv. 7097960
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., et al. (2017). "Automatic differentiation in pytorch," in NIPS workshop.
Polykovskiy, D., Zhebrak, A., Sanchez-Lengeling, B., Golovanov, S., Tatanov, O., Belyaev, S., et al. (2018a). Molecular sets (moses): a benchmarking platform for molecular generation models. Available from: https://arxiv.org/abs/1811.12823.
Polykovskiy, D., Zhebrak, A., Vetrov, D., Ivanenkov, Y., Aladinskiy, V., Mamoshina, P., et al. (2018b). Entangled conditional adversarial autoencoder for de novo drug discovery. Mol. Pharm. 15, 4398-4405. doi:10.1021/acs.molpharmaceut.8b00839
Popova, M., Isayev, O., and Tropsha, A. (2018). Deep reinforcement learning for de novo drug design. Sci. Adv. 4, eaap7885. doi:10.1126/sciadv.aap7885</p>
<p>Preuer, K., Renz, P., Unterthiner, T., Hochreiter, S., and Klambauer, G. (2018). Fréchet ChemNet distance: a metric for generative models for molecules in drug discovery. J. Chem. Inf. Model. 58, 1736-1741. doi:10.1021/acs.jcim.8b00234
Prykhodko, O., Johansson, S. V., Kotsias, P.-C., Arús-Pous, J., Bjerrum, E. J., Engkvist, O., et al. (2019). A de novo molecular generation method using latent vector based generative adversarial network. J. Cheminf. 11, 74. doi:10.1186/ s13321-019-0397-9
Putin, E., Asadulaev, A., Vanhaelen, Q., Ivanenkov, Y., Aladinskaya, A. V., Aliper, A., et al. (2018). Adversarial threshold neural computer for molecular de novo design. Mol. Pharm. 15, 4386-4397. doi:10.1021/acs.molpharmaceut.7b01137
Pyzer-Knapp, E. O., Suh, C., Gómez-Bombarelli, R., Aguilera-Iparraguirre, J., and Aspuru-Guzik, A. (2015). What is High-Throughput virtual screening? a perspective from organic materials discovery. Annu. Rev. Mater. Res. 45, 195-216. doi:10.1146/annurev-matsci-070214-020823
Ramakrishnan, R., Dral, P. O., Rupp, M., and von Lilienfeld, O. A. (2014). Quantum chemistry structures and properties of 134 kilo molecules. Scientific Data 1, 140022. doi:10.1038/sdata.2014.22
Reymond, J.-L. (2015). The chemical space project. Acc. Chem. Res. 48, 722-730. doi:10.1021/ar500432k
Rogers, D. and Hahn, M. (2010). Extended-connectivity fingerprints. J. Chem. Inf. Model. 50, 742-754. doi:10.1021/ci100050t
Sanchez-Lengeling, B. and Aspuru-Guzik, A. (2018). Inverse molecular design using machine learning: generative models for matter engineering. Science 361, 360-365. doi:10.1126/science.aat2663
Segler, M. H. S., Kogej, T., Tyrchan, C., and Waller, M. P. (2018). Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS Cent. Sci. 4, 120-131. doi:10.1021/acscentsci.7b00512
Shi, C., Xu, M., Zhu, Z., Zhang, W., Zhang, M., and Tang, J. (2019). "Graphaf: a flow-based autoregressive model for molecular graph generation," in International conference on learning representations.
Shultz, M. D. (2018). Two decades under the influence of the rule of five and the changing properties of approved oral drugs. J. Med. Chem. 62, 1701-1714. doi:10.1021/acs.jmedchem.8b00686
Stein, S. E., Heller, S. R., and Tchekhovskoi, D. V. (2003). "An open standard for chemical structure representation: the iupac chemical identifier." in International chemical information conference.
Sterling, T. and Irwin, J. J. (2015). Zinc 15 - ligand discovery for everyone. J. Chem. Inf. Model. 55, 2324-2337. doi:10.1021/acs.jcim.5b00559
Teague, S. J., Davis, A. M., Leeson, P. D., and Opera, T. (1999). The design of leadlike combinatorial libraries. Angew. Chem. Int. Ed. 38, 3743-3748. doi:10. 1002/(SICI)1521-3773(19991216)38:24\%3C3743::AID-ANIE3743\%3E3.0.CO; 2-U
van Hilten, N., Chevillard, F., and Kolb, P. (2019). Virtual compound libraries in computer-assisted drug discovery. J. Chem. Inf. Model. 59, 644-651. doi:10. 1021/acs.jcim.8b00737
Vanhaelen, Q., Mamoshina, P., Aliper, A. M., Artemov, A., Lezhnina, K., Ozerov, I., et al. (2017). Design of efficient computational workflows for in silico drug repurposing. Drug Discov. Today 22, 210-222. doi:10.1016/j.drudis.2016.09.019</p>
<p>Weininger, D. (1988). Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. J. Chem. Inf. Model. 28, 31-36. doi:10.1021/ci00057a005
Weininger, D., Weininger, A., and Weininger, J. L. (1989). Smiles. 2. algorithm for generation of unique smiles notation. J. Chem. Inf. Model. 29, 97-101. doi:10. 1021/ci00062a008
Wildman, S. A. and Crippen, G. M. (1999). Prediction of physicochemical parameters by atomic contributions. J. Chem. Inf. Comput. Sci. 39, 868-873. doi:10.1021/ci9903071
Wu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Geniesse, C., Pappu, A. S., et al. (2018). MoleculeNet: a benchmark for molecular machine learning. Chem. Sci. 9, 513-530. doi:10.1039/c7sc02664a
Yang, X., Zhang, J., Yoshizoe, K., Terayama, K., and Tsuda, K. (2017). ChemTS: an efficient python library for de novo molecular generation. Sci. Technol. Adv. Mater. 18, 972-976. doi:10.1080/14686996.2017.1401424
Yu, L., Zhang, W., Wang, J., and Yu, Y. (2017). "Seqgan: sequence generative adversarial nets with policy gradient," in Thirty-first AAAI conference on artificial intelligence.
Zhavoronkov, A., Ivanenkov, Y. A., Aliper, A., Veselov, M. S., Aladinskiy, V. A., Aladinskaya, A. V., et al. (2019a). Deep learning enables rapid identification of potent DDR1 kinase inhibitors. Nat. Biotechnol., 37, 1038-1040. doi:10.1038/s41587-019-0224-x
Zhavoronkov, A., Mamoshina, P., Vanhaelen, Q., Scheibye-Knudsen, M., Moskalev, A., and Aliper, A. (2019b). Artificial intelligence for aging and longevity research: recent advances and perspectives. Ageing Res. Rev. 49, 49-66. doi:10.1016/j.arr.2018.11.003</p>
<p>Conflict of Interest: DP, AZhe, VA, MV, and AZha work for Insilico Medicine, a commercial artificial intelligence company. SG, OT, SB, RK, AA, and SN work for Neuromation OU, a company engaged in AI development through synthetic data and generative models. SJ and HC work for a pharmaceutical company AstraZeneca. AA-G is a cofounder and board member of, and consultant for, Kebotix, an artificial intelligence-driven molecular discovery company and a member of the science advisory board of Insilico Medicine.</p>
<p>The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
<p>Copyright © 2020 Polykovskiy, Zhebrak, Sanchez-Lengeling, Golovanov, Tatanov, Belyaev, Kurbanov, Artamonov, Aladinskiy, Veselov, Kadurin, Johansson, Chen, Nikolaoko, Aspuru-Guzik and Zhavoronkov. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Insilico Medicine Hong Kong Ltd., Pak Shek Kok, Hong Kong, ${ }^{2}$ Chemistry and Chemical Biology Department, Harvard University, Cambridge, MA, United States, ${ }^{3}$ Neuromation OU, Tallinn, Estonia, ${ }^{4}$ Molecular AI, DiscoverySciences, R\&amp;D, AstraZeneca, Gothenburg, Sweden, ${ }^{5}$ Computer Science Department, National Research University Higher School of Economics, St. Petersburg, Russia, ${ }^{6}$ Chemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON, Canada, ${ }^{7}$ Department of Computer Science, University of Toronto, Toronto, ON, Canada, ${ }^{8}$ CIFAR AI Chair, Vector Institute for Artificial Intelligence, Toronto, ON, Canada, ${ }^{9}$ Lebovic Fellow, Canadian Institute for Advanced Research (CIFAR), Toronto, ON, Canada&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2719 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2719</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2719</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-8465e6976515309e9ca13d431e377336f73032c0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8465e6976515309e9ca13d431e377336f73032c0" target="_blank">The Text-Based Adventure AI Competition</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on Games</p>
                <p><strong>Paper TL;DR:</strong> This paper summarizes the three competitions ran in 2016–2018 (including details of open-source implementations of both the competition framework and competitors) and presents the results of an improved evaluation of these competitors across 20 games.</p>
                <p><strong>Paper Abstract:</strong> In 2016–2018 at the IEEE Conference on Computational Intelligence in Games, the authors of this paper ran a competition for agents that can play classic text-based adventure games. This competition fills a gap in existing game artificial intelligence (AI) competitions that have typically focused on traditional card/board games or modern video games with graphical interfaces. By providing a platform for evaluating agents in text-based adventures, the competition provides a novel benchmark for game AI with unique challenges for natural language understanding and generation. This paper summarizes the three competitions ran in 2016–2018 (including details of open-source implementations of both the competition framework and our competitors) and presents the results of an improved evaluation of these competitors across 20 games.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2719.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2719.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BYUAGENT2016</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BYUAGENT 2016</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that generates verb-noun commands by combining nouns extracted from game text with verbs drawn from a curated verb set and uses a simple one-shot memory of successful commands to repeat actions that changed the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>What Can You Do with a Rock? Affordance Extraction via Word Embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BYUAGENT 2016</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Generative-command agent that extracts nouns from narrative text, uses word2vec embeddings and an affordance vector to select verbs, exhaustively attempts generated commands, and stores commands that produced environment changes for later reuse (one-shot learning).</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Text-Based Adventure AI competition test set (20 games)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>A heterogeneous set of 20 text-adventure games downloaded from the web, written by different authors, each providing a numeric score for progress; tasks include exploration, puzzle solving, item manipulation, and combat described in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic/action cache (one-shot learning)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Stored list of previously successful commands, effectively acting as a (state -> command) cache (implicit mapping by environment/context).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Commands (verb-noun or more complex command strings) that previously produced a change in the game environment.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Fallback retrieval: when newly generated commands fail to change the environment, stored successful commands are attempted.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>On-success update: if a generated command produces a change in the environment, that command is stored for future reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Repeat previously successful actions to progress in similar states and implement a simple form of one-shot learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average % completion = 0.79% (single run reported); % non-zero games = 15% (Table I, 1000 steps per game).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>The stored-successful-commands strategy provides a simple form of one-shot learning that helps repeat actions that produced state changes; however, BYUAGENT 2016 struggles with more complex commands (e.g., prepositional commands) and specific object name inference, limiting effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Memory is simplistic (commands only) and does not capture richer state structure; fails on games requiring complex commands or correct object name inference; capacity and retrieval policy not specified; susceptible to being insufficient when action structure requires more than repeating prior commands.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Text-Based Adventure AI Competition', 'publication_date_yy_mm': '2018-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2719.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2719.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Golovin</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Golovin (2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that generates commands by inserting nouns into large command-pattern templates, uses word2vec synonyms and an LSTM-based score to weight commands, and maintains per-location blacklists and a navigation map to guide exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Textbased Adventures of the Golovin AI Agent</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Golovin</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Pattern-driven command generator using a large corpus of verb-phrase patterns, word2vec synonym proposals (trained on fantasy books), an LSTM language-model score to weight commands, roulette-wheel selection, and simple memories including per-location blacklists of failed commands and a map graph of locations.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Text-Based Adventure AI competition test set (20 games)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>A set of 20 diverse text-adventure games used to evaluate general text-game playing agents under fixed step budgets; tasks include navigating, picking up items, solving puzzles, and combat.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>local blacklist memory and navigation/map memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Per-location blacklist of commands that failed to change the narrative; a map graph storing locations and the commands that move between them (graph structure).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Blacklisted (failed) commands for each location, map nodes (locations), movement edges and the commands associated with them; command weights from LM/LSTM scoring are maintained per generated command attempt but not described as persistent memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Location-keyed retrieval: commands blacklisted for the current location are avoided until inventory changes; navigation uses the map graph to propose movement commands and routes to 'promising' destinations based on distance and unexplored command proportion.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Update blacklist when a command is attempted and the game description remains the same (treated as failed); update map graph when moving to a new area.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Avoid retrying failing commands, guide exploration/navigation, and bias command generation away from previously ineffective actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average % completion = 1.45% (M, SD 0.09); % non-zero games = 31% (Table I, 1000 steps per game). Also reported: 0.99% completion (100 steps) and 1.44% completion (10k steps).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Blacklisting failed commands and maintaining a navigation graph enable efficient early exploration (Golovin is stronger in short-term per-step thinking time), but domain-specific modules (e.g., battle mode) may overfit to training sets and generalize less well to unseen games; Golovin scales less effectively than graph/knowledge-graph-based agents when given many more steps.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Per-location blacklists can be invalidated by changes in inventory or context (blacklist removal rules rely on inventory changes); domain-specific heuristics (battle mode) risk overfitting; no systematic capacity or retrieval quality controls reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Text-Based Adventure AI Competition', 'publication_date_yy_mm': '2018-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2719.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2719.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CARL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CARL (BYUAGENT 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An affordance-based agent that detects state sentences using skip-thought vectors, hashes concatenated state sentences to identify states, and stores actions that changed those states for recollection (state-indexed action memory).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CARL (BYUAGENT 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Affordance-detection agent using skip-thought sentence embeddings to identify state-descriptive sentences, extracting nouns from detected state-information, using word2vec affordance manipulation to propose verb-noun pairs, and storing actions that change a state indexed by a hashed state identifier for later reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Text-Based Adventure AI competition test set (20 games)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>A heterogeneous collection of 20 text-adventure games with numeric scoring, requiring natural-language understanding, object manipulation, navigation and puzzle solving.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>state-indexed episodic memory (state-action memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>State identifiers created by concatenating sentences classified as state-information (skip-thought vectors) and hashing them to produce unique state IDs; memory maps state IDs to actions that previously changed that state.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>State hashes derived from identified state sentences, and actions that were observed to change the corresponding state (successful actions). Also noun/affordance information used for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>State-match retrieval: when current concatenated state sentences hash to a previously seen state ID, stored actions associated with that state are recollected and can be retried.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>When an action is perceived to change the game state (via validity detection), that action is stored under the hashed identifier of the current state.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Recall previously successful actions in identical or similar states to reproduce beneficial effects and speed up problem solving (one-shot retention of effective interactions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average % completion = 1.59% (single run reported); % non-zero games = 30% (Table I, 1000 steps per game).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>State-indexed storage allows repetition of successful interactions when identical states reoccur, improving performance over agents without comparable state hashing; identifying and indexing state-relevant sentences (skip-thought vectors) is important for creating robust state identifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Relies on accurate classification of state-information sentences (skip-thought proximity classification); hashing exact concatenations may be brittle to minor textual variations; capacity and similarity-tolerant retrieval not specified; no ablation quantifying memory contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Text-Based Adventure AI Competition', 'publication_date_yy_mm': '2018-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2719.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2719.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NAIL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NAIL (Navigate Acquire Interact Learn)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular agent whose modules compete for control and which maintains a knowledge graph that records objects, interactions, locations, and connections, plus a record of attempted interactions to avoid retries; uses an LM-based interactor and a validity detector to update memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NAIL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Multi-module agent (Examiner, Interactor, Navigator, etc.) that maintains a central knowledge graph storing objects, interactions, locations and connections; modules propose actions and the most 'eager' module acts; validity detection (embedding-based classifier) determines whether actions had effects and the knowledge graph is updated accordingly; Interactor uses an LM-based scoring of verb-object combos.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Text-Based Adventure AI competition test set (20 games)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Twenty diverse text-adventures with numeric scoring used to evaluate generalization and scoring performance across exploration, manipulation, navigation and puzzle solving.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory / knowledge graph</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>A structured knowledge graph recording known objects, interactions, locations (nodes) and connections (edges); also a stored set of previously attempted interactions (log) to avoid reattempting failed actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Known objects and their properties, recognized interactions, discovered locations and their interconnections, and a history of previously attempted interactions (including failures and successes).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Module-based queries: modules read from and write to the central knowledge graph; retrieval is relevance-based per module (e.g., Navigator queries connections, Interactor queries objects); validity detector filters out failed updates.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Any detected change in game state triggers an update to the knowledge graph; attempted interactions are recorded immediately; validity detector classifies action outcomes to determine whether to commit updates.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Track discovered world structure for navigation and planning, avoid retrying failed interactions, supply object/interactions for the Interactor module's LM-based action generation, and provide compact world representation for modules to coordinate.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average % completion = 2.56% (M, SD 0.33); % non-zero games = 45.5% (M, SD 2.84) (Table I, 1000 steps per game). NAIL outperformed earlier agents and scaled better with more steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>The knowledge-graph-based memory provides a compact world representation that helps modules coordinate (navigation, interaction, examination) and prevents retrying failed interactions; NAIL's knowledge graph and validity detection contributed to it being the strongest agent in the evaluation, scaling better with more steps versus pattern-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Requires more thinking time per step than some competitors; no ablation isolating the knowledge graph's contribution; susceptible to parsing/out-of-game message errors which can cause incorrect updates or failures; potential brittleness in validity detection and graph-update heuristics not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>NAIL's knowledge-graph memory combined with a validity detector and modular control performed best in the comparative evaluation, likely because it yields a compact structured world model and avoids retrying failed actions while supporting targeted module behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Text-Based Adventure AI Competition', 'publication_date_yy_mm': '2018-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language Understanding for Text-based Games Using Deep Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>What Can You Do with a Rock? Affordance Extraction via Word Embeddings <em>(Rating: 2)</em></li>
                <li>Textbased Adventures of the Golovin AI Agent <em>(Rating: 2)</em></li>
                <li>Skip-Thought Vectors <em>(Rating: 1)</em></li>
                <li>Learning partially observable deterministic action models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2719",
    "paper_id": "paper-8465e6976515309e9ca13d431e377336f73032c0",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "BYUAGENT2016",
            "name_full": "BYUAGENT 2016",
            "brief_description": "An agent that generates verb-noun commands by combining nouns extracted from game text with verbs drawn from a curated verb set and uses a simple one-shot memory of successful commands to repeat actions that changed the environment.",
            "citation_title": "What Can You Do with a Rock? Affordance Extraction via Word Embeddings",
            "mention_or_use": "use",
            "agent_name": "BYUAGENT 2016",
            "agent_description": "Generative-command agent that extracts nouns from narrative text, uses word2vec embeddings and an affordance vector to select verbs, exhaustively attempts generated commands, and stores commands that produced environment changes for later reuse (one-shot learning).",
            "base_model_size": null,
            "game_benchmark_name": "Text-Based Adventure AI competition test set (20 games)",
            "game_description": "A heterogeneous set of 20 text-adventure games downloaded from the web, written by different authors, each providing a numeric score for progress; tasks include exploration, puzzle solving, item manipulation, and combat described in natural language.",
            "uses_memory": true,
            "memory_type": "episodic/action cache (one-shot learning)",
            "memory_structure": "Stored list of previously successful commands, effectively acting as a (state -&gt; command) cache (implicit mapping by environment/context).",
            "memory_content": "Commands (verb-noun or more complex command strings) that previously produced a change in the game environment.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Fallback retrieval: when newly generated commands fail to change the environment, stored successful commands are attempted.",
            "memory_update_strategy": "On-success update: if a generated command produces a change in the environment, that command is stored for future reuse.",
            "memory_usage_purpose": "Repeat previously successful actions to progress in similar states and implement a simple form of one-shot learning.",
            "performance_with_memory": "Average % completion = 0.79% (single run reported); % non-zero games = 15% (Table I, 1000 steps per game).",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "The stored-successful-commands strategy provides a simple form of one-shot learning that helps repeat actions that produced state changes; however, BYUAGENT 2016 struggles with more complex commands (e.g., prepositional commands) and specific object name inference, limiting effectiveness.",
            "memory_limitations": "Memory is simplistic (commands only) and does not capture richer state structure; fails on games requiring complex commands or correct object name inference; capacity and retrieval policy not specified; susceptible to being insufficient when action structure requires more than repeating prior commands.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": null,
            "uuid": "e2719.0",
            "source_info": {
                "paper_title": "The Text-Based Adventure AI Competition",
                "publication_date_yy_mm": "2018-08"
            }
        },
        {
            "name_short": "Golovin",
            "name_full": "Golovin (2017)",
            "brief_description": "An agent that generates commands by inserting nouns into large command-pattern templates, uses word2vec synonyms and an LSTM-based score to weight commands, and maintains per-location blacklists and a navigation map to guide exploration.",
            "citation_title": "Textbased Adventures of the Golovin AI Agent",
            "mention_or_use": "use",
            "agent_name": "Golovin",
            "agent_description": "Pattern-driven command generator using a large corpus of verb-phrase patterns, word2vec synonym proposals (trained on fantasy books), an LSTM language-model score to weight commands, roulette-wheel selection, and simple memories including per-location blacklists of failed commands and a map graph of locations.",
            "base_model_size": null,
            "game_benchmark_name": "Text-Based Adventure AI competition test set (20 games)",
            "game_description": "A set of 20 diverse text-adventure games used to evaluate general text-game playing agents under fixed step budgets; tasks include navigating, picking up items, solving puzzles, and combat.",
            "uses_memory": true,
            "memory_type": "local blacklist memory and navigation/map memory",
            "memory_structure": "Per-location blacklist of commands that failed to change the narrative; a map graph storing locations and the commands that move between them (graph structure).",
            "memory_content": "Blacklisted (failed) commands for each location, map nodes (locations), movement edges and the commands associated with them; command weights from LM/LSTM scoring are maintained per generated command attempt but not described as persistent memory.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Location-keyed retrieval: commands blacklisted for the current location are avoided until inventory changes; navigation uses the map graph to propose movement commands and routes to 'promising' destinations based on distance and unexplored command proportion.",
            "memory_update_strategy": "Update blacklist when a command is attempted and the game description remains the same (treated as failed); update map graph when moving to a new area.",
            "memory_usage_purpose": "Avoid retrying failing commands, guide exploration/navigation, and bias command generation away from previously ineffective actions.",
            "performance_with_memory": "Average % completion = 1.45% (M, SD 0.09); % non-zero games = 31% (Table I, 1000 steps per game). Also reported: 0.99% completion (100 steps) and 1.44% completion (10k steps).",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Blacklisting failed commands and maintaining a navigation graph enable efficient early exploration (Golovin is stronger in short-term per-step thinking time), but domain-specific modules (e.g., battle mode) may overfit to training sets and generalize less well to unseen games; Golovin scales less effectively than graph/knowledge-graph-based agents when given many more steps.",
            "memory_limitations": "Per-location blacklists can be invalidated by changes in inventory or context (blacklist removal rules rely on inventory changes); domain-specific heuristics (battle mode) risk overfitting; no systematic capacity or retrieval quality controls reported.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": null,
            "uuid": "e2719.1",
            "source_info": {
                "paper_title": "The Text-Based Adventure AI Competition",
                "publication_date_yy_mm": "2018-08"
            }
        },
        {
            "name_short": "CARL",
            "name_full": "CARL (BYUAGENT 2017)",
            "brief_description": "An affordance-based agent that detects state sentences using skip-thought vectors, hashes concatenated state sentences to identify states, and stores actions that changed those states for recollection (state-indexed action memory).",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "CARL (BYUAGENT 2017)",
            "agent_description": "Affordance-detection agent using skip-thought sentence embeddings to identify state-descriptive sentences, extracting nouns from detected state-information, using word2vec affordance manipulation to propose verb-noun pairs, and storing actions that change a state indexed by a hashed state identifier for later reuse.",
            "base_model_size": null,
            "game_benchmark_name": "Text-Based Adventure AI competition test set (20 games)",
            "game_description": "A heterogeneous collection of 20 text-adventure games with numeric scoring, requiring natural-language understanding, object manipulation, navigation and puzzle solving.",
            "uses_memory": true,
            "memory_type": "state-indexed episodic memory (state-action memory)",
            "memory_structure": "State identifiers created by concatenating sentences classified as state-information (skip-thought vectors) and hashing them to produce unique state IDs; memory maps state IDs to actions that previously changed that state.",
            "memory_content": "State hashes derived from identified state sentences, and actions that were observed to change the corresponding state (successful actions). Also noun/affordance information used for generation.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "State-match retrieval: when current concatenated state sentences hash to a previously seen state ID, stored actions associated with that state are recollected and can be retried.",
            "memory_update_strategy": "When an action is perceived to change the game state (via validity detection), that action is stored under the hashed identifier of the current state.",
            "memory_usage_purpose": "Recall previously successful actions in identical or similar states to reproduce beneficial effects and speed up problem solving (one-shot retention of effective interactions).",
            "performance_with_memory": "Average % completion = 1.59% (single run reported); % non-zero games = 30% (Table I, 1000 steps per game).",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "State-indexed storage allows repetition of successful interactions when identical states reoccur, improving performance over agents without comparable state hashing; identifying and indexing state-relevant sentences (skip-thought vectors) is important for creating robust state identifiers.",
            "memory_limitations": "Relies on accurate classification of state-information sentences (skip-thought proximity classification); hashing exact concatenations may be brittle to minor textual variations; capacity and similarity-tolerant retrieval not specified; no ablation quantifying memory contribution.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": null,
            "uuid": "e2719.2",
            "source_info": {
                "paper_title": "The Text-Based Adventure AI Competition",
                "publication_date_yy_mm": "2018-08"
            }
        },
        {
            "name_short": "NAIL",
            "name_full": "NAIL (Navigate Acquire Interact Learn)",
            "brief_description": "A modular agent whose modules compete for control and which maintains a knowledge graph that records objects, interactions, locations, and connections, plus a record of attempted interactions to avoid retries; uses an LM-based interactor and a validity detector to update memory.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "NAIL",
            "agent_description": "Multi-module agent (Examiner, Interactor, Navigator, etc.) that maintains a central knowledge graph storing objects, interactions, locations and connections; modules propose actions and the most 'eager' module acts; validity detection (embedding-based classifier) determines whether actions had effects and the knowledge graph is updated accordingly; Interactor uses an LM-based scoring of verb-object combos.",
            "base_model_size": null,
            "game_benchmark_name": "Text-Based Adventure AI competition test set (20 games)",
            "game_description": "Twenty diverse text-adventures with numeric scoring used to evaluate generalization and scoring performance across exploration, manipulation, navigation and puzzle solving.",
            "uses_memory": true,
            "memory_type": "graph-based memory / knowledge graph",
            "memory_structure": "A structured knowledge graph recording known objects, interactions, locations (nodes) and connections (edges); also a stored set of previously attempted interactions (log) to avoid reattempting failed actions.",
            "memory_content": "Known objects and their properties, recognized interactions, discovered locations and their interconnections, and a history of previously attempted interactions (including failures and successes).",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Module-based queries: modules read from and write to the central knowledge graph; retrieval is relevance-based per module (e.g., Navigator queries connections, Interactor queries objects); validity detector filters out failed updates.",
            "memory_update_strategy": "Any detected change in game state triggers an update to the knowledge graph; attempted interactions are recorded immediately; validity detector classifies action outcomes to determine whether to commit updates.",
            "memory_usage_purpose": "Track discovered world structure for navigation and planning, avoid retrying failed interactions, supply object/interactions for the Interactor module's LM-based action generation, and provide compact world representation for modules to coordinate.",
            "performance_with_memory": "Average % completion = 2.56% (M, SD 0.33); % non-zero games = 45.5% (M, SD 2.84) (Table I, 1000 steps per game). NAIL outperformed earlier agents and scaled better with more steps.",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "The knowledge-graph-based memory provides a compact world representation that helps modules coordinate (navigation, interaction, examination) and prevents retrying failed interactions; NAIL's knowledge graph and validity detection contributed to it being the strongest agent in the evaluation, scaling better with more steps versus pattern-based approaches.",
            "memory_limitations": "Requires more thinking time per step than some competitors; no ablation isolating the knowledge graph's contribution; susceptible to parsing/out-of-game message errors which can cause incorrect updates or failures; potential brittleness in validity detection and graph-update heuristics not quantified.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "NAIL's knowledge-graph memory combined with a validity detector and modular control performed best in the comparative evaluation, likely because it yields a compact structured world model and avoids retrying failed actions while supporting targeted module behaviour.",
            "uuid": "e2719.3",
            "source_info": {
                "paper_title": "The Text-Based Adventure AI Competition",
                "publication_date_yy_mm": "2018-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language Understanding for Text-based Games Using Deep Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "language_understanding_for_textbased_games_using_deep_reinforcement_learning"
        },
        {
            "paper_title": "What Can You Do with a Rock? Affordance Extraction via Word Embeddings",
            "rating": 2,
            "sanitized_title": "what_can_you_do_with_a_rock_affordance_extraction_via_word_embeddings"
        },
        {
            "paper_title": "Textbased Adventures of the Golovin AI Agent",
            "rating": 2,
            "sanitized_title": "textbased_adventures_of_the_golovin_ai_agent"
        },
        {
            "paper_title": "Skip-Thought Vectors",
            "rating": 1,
            "sanitized_title": "skipthought_vectors"
        },
        {
            "paper_title": "Learning partially observable deterministic action models",
            "rating": 1,
            "sanitized_title": "learning_partially_observable_deterministic_action_models"
        }
    ],
    "cost": 0.014224249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Text-Based Adventure AI Competition</h1>
<p>Timothy Atkinson ${ }^{1 <em>}$, Hendrik Baier ${ }^{2 </em>}$, Tara Copplestone ${ }^{1}$, Sam Devlin ${ }^{2}$ and Jerry Swan ${ }^{1}$</p>
<h4>Abstract</h4>
<p>In 2016, 2017, and 2018 at the IEEE Conference on Computational Intelligence in Games, the authors of this paper ran a competition for agents that can play classic text-based adventure games. This competition fills a gap in existing game AI competitions that have typically focussed on traditional card/board games or modern video games with graphical interfaces. By providing a platform for evaluating agents in textbased adventures, the competition provides a novel benchmark for game AI with unique challenges for natural language understanding and generation. This paper summarises the three competitions ran in 2016, 2017, and 2018 (including details of open source implementations of both the competition framework and our competitors) and presents the results of an improved evaluation of these competitors across 20 games.</p>
<h2>I. INTRODUCTION</h2>
<p>Before the widespread availability of graphical displays, text adventures were one of the few game genres that owed their existence solely to computing. The first text adventure was Colossal Cave (also known simply as Adventure), written in 1976 by Will Crowther for the PDP-10 mainframe [1]. With the advent of home computing in the late 1970s, Colossal Cave and other games such as ZORK were enjoyed by many. The majority of early text adventures used a narration-action loop that accepted simple commands of the general form VERB or VERB NOUN (e.g. 'look', 'go west', 'take box') via console input. In response to such commands, the programs provided a description of the immediate environment, e.g.
'You are in an open field on the west side of a white house with a boarded front door. There is a small mailbox here.'</p>
<p>Early adventures typically involved exploration and treasure hunting, but more sophisticated narratives emerged in the 1980s (e.g. in the INFOCOM range of games). An active "Interactive Fiction" community still exists, using powerful natural language authoring tools such as Inform [2] to create new and diverse titles.</p>
<p>Despite the continued existence of this community, one might ask what a competition concerned with text-based games has to offer, given that there are numerous competitions involving modern graphics-based games. The underlying motivation can be traced back to an early divergence between AI philosophy and practice. John McCarthy proposed the well-known "Monkey and Bananas Problem" in 1963 [3]: given a room containing a chair, a stick and a</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>bunch of bananas hanging on a hook, the monkey's task is to find a sequence of actions that results in acquiring the bananas. McCarthy made a key distinction between the physical feasibility of the task (i.e. is there a physically realisable sequence of actions that achieves the goal?) and its epistemic feasibility (i.e. can the knowledge that a particular action even exists be efficiently derived?) [4]. We claim that recent game AI competitions have tended to de-emphasise the epistemic aspect, instead working in domains that are strongly operationalised. What this means is that the set of actions instantaneously available have been constrained to be knowable in advance. Of course, the space of plans for lengthy sequences of actions is still combinatorially huge, but the key question of how to derive and represent knowledge about an uncertain world is circumvented.</p>
<p>The hypothesis motivating the Text-Based Adventure AI competition is that the determination of relevant affordances (i.e. the set of behaviors that are possible in a given situation [5]) from a non-trivial environment is likely to require more than a good choice of credit assignment strategy, even if the latter would suffice in an operationalised domain. This then re-emphasises the following research questions:</p>
<ul>
<li>In the absence of operationalisation, what are the minimum priors (in terms of domain knowledge) that are required for success (even for a "toy" domain such as "Monkey and Bananas")?</li>
<li>How do model-free and model-based approaches compare? How can any difficulties with the former in this domain better inform such approaches in general?
In operationalised environments, the goal is to learn a policy mapping from game states to actions, where the set of available actions is predetermined. In a nonoperationalised environment, the set of available actions first has to be generated as a function of previously encountered game states. A policy mapping that works with these generated actions can then be learned subsequently. This function space is in general vastly larger than in the operationalised case.</li>
</ul>
<p>It could be argued that text adventure domains are also "operationalised", since the space of possible inputs is discrete. However, this does not take into account the fact that effective action requires a mapping from game output space to player input space for which random input text will be vastly less likely to have any effect than randomly pressing buttons in a first-person shooter, or randomly making legal moves in chess.</p>
<p>This article is structured as follows: Section II discusses related work on natural language processing in gameplaying. Section III describes the competition framework.</p>
<p>Section IV outlines the agents submitted to the 2016, 2017, and 2018 competitions. Section V describes an improved evaluation methodology based on the experience gained from the 2016 and 2017 competitions. Section VI presents new results from applying this methodology to the existing agents, which we hope can serve as a baseline for future research, and Section VII concludes.</p>
<h2>II. RELATED WORK</h2>
<p>The essential task of a competitor in the Text-Based Adventure AI Competition is to create an agent that can act effectively in a partially-observable environment, perceived via a number of short natural language descriptions of the agent's surroundings. Historically, work in the general area of natural language processing and narrative representation has mirrored the overall tendency for AI methods to move from symbolist to data-intensive methods such as Neural Networks, Reinforcement Learning or Monte Carlo Tree Search. In the 1970s and 1980s there was significant interest in symbolic representations of natural language and narrative, via semantic nets, frame systems and scripts, using approaches such as Case-Based Reasoning [6] and Abstraction Units [7]. More recently, there has been a tendency for a directly symbolist approach to Natural Language Processing to be eclipsed by more overtly numerical methods, including "Bag of Words" based approaches such as Latent Semantic Analysis [8], with the goals of approaches such as [7] being revisited via contemporary corpus-based techniques [9].</p>
<p>With specific reference to work on games, Branavan et al. [10] applied Monte Carlo Tree Search to the strategy game Civilization II. The value function was approximated via a neural net and included linguistic features extracted from the game manual, providing a significant improvement against the game's built-in opponent. Narrowing our focus further to Interactive Fiction, AI can play many roles in this genre of games [11] with this competition's focus being specifically on game playing AI. Previous work has often attempted to reduce the challenge of text-based games in order to make them more feasible domains for existing AI methods - such as for example by making the game output its environment descriptions not in natural language, but in first-order logic that can be directly stored and processed by an agent [12], [13]. Prior to our first competition in 2016, the previous work of greatest relevance in playing natural-language games was by Narasimhan et al [14], in which Deep Reinforcement Learning was used to jointly learn state representations and action policies in two Multi-User Dungeons (MUDs) - a specific type of text-based adventure game. The approach was demonstrated to outperform the use of both bag-of-words and bag-of-bigrams for state representations. This paper and early discussions with the first two authors helped shape the first competition we ran in 2016, which attempted to engage the game AI community more broadly in the challenging topic of text-based adventure AI by providing an accessible framework and regular schedule for agent evaluation via competitions.</p>
<h2>III. COMPETITION FRAMEWORK</h2>
<p>Our framework for evaluating a software agent's ability to play text-based adventures ${ }^{1}$ is based on the ZPLET [15] Java interpreter for the widely-used Z-Machine format [16], created by Infocom in 1979. The framework is defined in terms of a traditional agent-based perspective [17]. The console input and Z-Machine output text of ZPLET are redirected to an Agent interface, to be implemented by competitors. This interface consists of a single action method with a string argument. At each turn, the narrative text (typically at most a paragraph in length) that would otherwise be presented to the human player is provided as this argument. For example:
'You are standing at the end of a road before a small brick building. Around you is a forest. A small stream flows out of the building and down a gully'.</p>
<p>The action method then returns a string describing the action that the agent wishes to perform. A default Rando$m A g e n t$ is provided, with an action implementation that simply ignores the input text and chooses uniformly from 8 basic commands: 'north', 'south', 'east', 'west', 'verbose' (switching some games to a mode where they always output full descriptions of locations even if the agent has been there before), 'take all', 'yes', and 'no'.</p>
<p>By default, agents can be developed in Java ${ }^{2}$, but to facilitate the development and implementation of agents in other programming languages an additional agent, IOAgent, is provided. This agent's action method forwards the narrative text to the application's output stream and then returns the next line of text from the application's input stream. This allows entrants to interface external agents by starting the implementation with the IOAgent and then interacting with that process's IO. A random agent implemented in Python 3.5 is provided to demonstrate how this can be implemented.</p>
<p>Additionally, there are 3 predefined actions which are meant to help train an agent and may be accessed by the IOAgent through hard-coded string equivalents. These are Quit, which quits the running game; Restart, which restarts both the running game and the current agent; and SoftRestart, which restarts the running game but not the current agent.</p>
<h2>IV. COMPETITORS</h2>
<p>The first Text-Based Adventure AI Competition was announced on 15 May 2016 and ran later that year at the IEEE Conference on Computational Intelligence in Games after closing for entries on 31 August 2016. The competition ran again in 2017, officially being announced on 8 March 2017 and closing for entries on 18 July 2017.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>In 2018, the competition was officially announced on 3 May 2018 and the closing for entries was 20 July 2018. In the following subsections we will summarise the four agents submitted to these competitions, three of which have been made available as open source by the respective participants. Future papers based on our framework can use these agents as baselines for evaluation and comparison.</p>
<h2>A. BYUAGENT 2016</h2>
<p>BYUAGENT $2016^{3}$ [18] interacts with a text-based adventure by generating commands combining nouns extracted from the game text with verbs drawn from a predefined set. More complex commands, such as propositional structures, may also be generated. Generated commands are attempted exhaustively and those commands which successfully produce a change in the game environment are stored. When the agent encounters a game environment where generated commands fail to produce a change, these stored commands may then be attempted, effectively implementing a simple form of one-shot learning.</p>
<p>The set of verbs the agent uses is drawn from the Wikipedia text corpus. The 1000 most commonlyappearing verbs were extracted, and then filtered by human play-testers according to their usefulness on a variety of text-based adventure games in order to produce a smaller set of approximately 100 verbs. A small number of additional human-selected verbs, including basic navigational commands, are also included.</p>
<p>Commands are generated using the word2vec [19] algorithm to produce vector embeddings of verbs and the nouns extracted from the game text. An "affordance vector", derived as the average vector difference in a known set of verb-noun pairs, is used to find a set of verbs which "match" a given noun. Additional hard-coded behaviour, such as periodic 'look' and 'inventory' commands to observe the game state, and 'get all' whenever entering a new location, aids the agent in interacting with the game.</p>
<h2>B. Golovin (2017)</h2>
<p>The Golovin ${ }^{4}$ agent [20] uses command generators to propose a non-empty set of commands for a given game environment by inserting nouns taken from the game's narrative text into "command patterns" - a set of 250,000 verb phrases extracted from various game walkthroughs, tutorials, and raw narrative text. To do this, a word2vec model trained on 3000 fantasy books is used to propose synonyms for each noun using n-best cosine similarity. Commands are then proposed by finding command patterns containing these synonyms and replacing that synonym with the original noun. Each generated command is associated with a weight, consisting of multiple factors including the cosine similarity between the noun and its</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>synonym's word2vec vector representation and a value given by a LSTM neural network operating on words [21]. Finally, a roulette wheel selection (based on the commands' associated weights) is used to choose a command from the generated set.</p>
<p>When a command is attempted and the game description remains the same, that command is assumed to have failed and is blacklisted for that game location until the agent observes a change in its inventory.</p>
<p>Five distinct command generators exist for different purposes. Each of these generators is fired, in this order, until a non-empty set of commands is proposed:</p>
<p>1) Battle mode. This command generator, specifically designed to aid the agent in combat scenarios, is limited to a subset of around 70 "fighting" command patterns containing one of the verbs 'attack', 'kill', 'fight', 'shoot' or 'punch'. This generator does not blacklist "failed" commands, as combat may require multiple iterations, and is only fired if a "fighting" command has been used previously.
2) Gathering items. This command generator is fired whenever the agent enters a new area, proposing 'take' commands for nouns in the area's narrative text.
3) Inventory commands. Once a noun has been successfully taken by the gathering items generator, this generator may fire, generating commands under the usual approach using only that noun's synonyms.
4) General actions. This command generator proposes commands in the usual approach using nouns from the game environment.
5) Exploration. A small fixed set of movement commands are proposed when an area has been exhausted by other command generators. Once the agent moves to a new area, a map graph storing locations and the commands which move between them is updated. If an area has unexplored directions, then those directions are proposed at random. Otherwise a route to a "promising destination", measured by its distance away and the proportion of possible commands left unattempted, is attempted.</p>
<h2>C. CARL (BYUAGENT 2017)</h2>
<p>The CARL agent uses affordance detection to suggest commands based on objects observed in a game's narrative text. Additionally, when an action is perceived to change the game state, that action is stored in memory so that it can be repeated during subsequent encounters with that state.</p>
<p>States are identified and stored by converting the individual sentences of the narrative text into skip-thought vectors [22]. The vector of each sentence is then classified as either state-information or not based on its proximity to the vector representations of a set of labelled example sentences. Those sentences which are identified as state-information are concatenated and hashed to create a unique identifier of the current state for the action recollection strategy described above.</p>
<p>Commands are then generated by first extracting nouns from those sentences identified as state-information. The vector representations of these nouns, generated by a word2vec model, trained on the Wikipedia text corpus, are manipulated using linear algebra to identify likely "matching" verbs. The best candidate verb-noun pairs are attempted first, with the search broadening to less "wellmatching" pairs should these fail to change the game's state. A complementary algorithm also attempts to generate prepositional combinations.</p>
<h2>D. NAIL (2018)</h2>
<p>The NAIL ("Navigate Acquire Interact Learn") agent ${ }^{5}$ consists of multiple independent modules which compete for control of the agent. Each module has a specific purpose; the main modules are the Examiner, Interactor and Navigator modules. These modules are responsible for identifying relevant objects in the current location, interacting with identified objects, and navigating to a new location, respectively. Additionally, there are further modules including specific modules for yes-no questions and for the acquisition of objects. Each module observes changes in the game and in each step of the game reports how 'eager' it is to assume control of the agent, with the most eager module in any step gaining full control.</p>
<p>Any change in the game's state is used to update a knowledge graph, which all modules have access to. This knowledge graph tracks known objects, interactions, locations and connections between locations, and serves as a compact representation of the world state. Additionally, the knowledge graph stores all previously attempted interactions to avoid retrying failed interactions.</p>
<p>Most modules use pre-defined sets of common commands to interact with the game. The Interactor module deviates here, constructing verb-noun phrases to interact with objects observed in the game. This module employs a LM-Based language model to assign probabilities to different verb-object combinations, the most promising of which are then executed. When there are no promising verb-object combinations, the Interactor falls back to a predefined set of verbs which are attempted in combination with observed objects from the game.</p>
<p>The NAIL agent uses a validity detector to determine whether its actions are having an effect on the game. A word-embedding-based text classifier [23] is used to establish whether a game's response to a given action either "failed" (had no effect) or "succeeded" (had an effect). Validity detection is needed e.g. to decide whether an object is relevant in the Examiner module, and to prevent incorrect updates to the knowledge graph when an action "failed'.</p>
<h2>V. EVALUATION METHODOLOGY</h2>
<p>The agents submitted to the 2016 and 2017 competitions were evaluated on a single game developed specifically</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>for the competition in order to provide a gradually increasing level of difficulty. The game was written by a game designer without a formal education in AI, with the intention of creating a game unbiased towards any existing approaches to game playing AI. The winner in both years was BYUAGENT 2016.</p>
<p>In hindsight, we realise the evaluation via a single game was flawed. Due to the mechanics of text-based adventures, if an agent is unable to solve a single puzzle, it is often unable to progress further in the game. This became obvious in the 2017 competition, where both the Golovin and CARL agents failed to score a single point because they could not solve the first puzzle. We had assumed the first puzzle to be simple, but our assessment of the complexity of text-based puzzles was entirely subjective. Moreover, the relative performance of agents in this competition compared to their performance as reported in the competitors' own publications [18], [20] suggest that our game was biased towards certain types of agents (e.g. BYUAGENT 2016).</p>
<p>This motivated the need for an evaluation across multiple games in order to fairly judge agents capable of general text-based adventure game playing ${ }^{6}$. The advantages of AIs that can play multiple games instead of a single game have frequently been promoted by the research community [24], [25], and there is a growing trend towards recognising the advantages of evaluating agents on commercial games not specifically designed to challenge AIs as well [26], [27]. Specifically, by evaluating on multiple games instead of a single test game, we avoid the tendency of competitors to overfit to the single test game; creating a contribution to that specific game instead of to AI research more broadly. Furthermore, the use of commercial games avoids bias in the game design towards specific AI methods the game designer may wish to promote if the game is created specifically to test AI. By instead using games created for human gameplay, this bias is presumed to be removed or at least averaged out over multiple games.</p>
<p>In the 2018 competition, we therefore switched to a more general evaluation framework using a test set of 20 different text adventures, which we downloaded from the web. The games were written in different styles by different authors, and some had been previously either commercially released, or submitted to interactive fiction writing competitions. There is no overlap between our test set and the training set used by [18], [20]. Unlike some of the games in this training set, all games in our test set provide a numerical score that allows for a fine-grained measurement of game playing performance. The precise actions for which a player gets rewarded with points, such as the successful solving of puzzles or winning of fights, are determined by the authors of the individual games. We expect that this removes the biases mentioned above both regarding our own style of writing, as well as our own style of scoring text-based adventure games. In addition,</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>agents that fail at solving the first puzzle of any given game will still be able to make progress on other games. Agents can be evaluated on their performance given a fixed number of game steps (calls to the action method) per game, and their evaluation can be averaged over multiple runs with different random seeds in order to reduce the impact of run-to-run variance.</p>
<p>The following section presents a novel evaluation of all agents submitted to previous competitions using our new framework, with the aim of more accurately determining which agent is currently the best general text adventure game playing AI. We evaluated BYUAGENT 2016, Golovin, CARL (BYUAGENT 2017), and NAIL on each of the 20 test games for 1000 game steps. This is to make our results comparable to those in [18], [20] with the same number of steps. The results reported here are averages over 10 such runs for Golovin and NAIL; unfortunately, we were only able to do one run for each of the BYU agents (discussion below). In order to give an indication of how agent performance can scale with the number of game steps, NAIL and Golovin were additionally tested in 10 runs with 100 steps each, and Golovin was also tested in 10 runs with 10,000 steps each on all 20 games.</p>
<p>The evaluation metrics are the average percentage of points an agent achieved over all games and test runs, and the percentage of games in which an agent achieved any points, averaged over test runs. The first metric is our primary evaluation metric, and is expressed as an average percentage instead of an average number of points due to the large differences in the maximum number of points achievable in each game. 1 point out of a maximum of 10 should count for more than 1 point out of a maximum of 500 . The secondary metric gives an impression of the generalizability of current text-based adventure AIs, and can be used as a tie breaker in case two or more agents perform equally well according to the first metric. In case of two agents performing the same on both metrics (which has not happened so far), we would use an additional tie breaking criterion, preferring the agent that is using the least prior domain knowledge of text-based adventure games.</p>
<h2>VI. RESULTS</h2>
<p>The BYUAGENT 2016 was the strongest agent in our first competition in 2016. In 2017, CARL was the strongest agent, improving on both BYUAGENT 2016 and Golovin on the primary metric. As of 2018, NAIL is the strongest text-based adventure game playing agent. It is a clear improvement over all previously submitted agents in both evaluation metrics.</p>
<p>Our new testing framework also enables us to conduct more in-depth comparisons of different agents' performance over time. Three such experiments have been done so far. The additional experiments with Golovin and NAIL at 100 time steps per game demonstrate that NAIL does not start out stronger than Golovin in the first 100 time steps (at least not with respect to the primary metric), but makes more effective use of additional</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Agent</th>
<th style="text-align: center;">\% completion</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">\% non-zero</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">SD</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">SD</td>
</tr>
<tr>
<td style="text-align: center;">BYUAGENT 2016</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Golovin</td>
<td style="text-align: center;">1.45</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">3.94</td>
</tr>
<tr>
<td style="text-align: center;">CARL (BYUAGENT 2017)</td>
<td style="text-align: center;">1.59</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">NAIL</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">2.84</td>
</tr>
<tr>
<td style="text-align: center;">Golovin (100 steps)</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">3.53</td>
</tr>
<tr>
<td style="text-align: center;">NAIL (100 steps)</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2.11</td>
</tr>
<tr>
<td style="text-align: center;">Golovin (10k steps)</td>
<td style="text-align: center;">1.44</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">4.25</td>
</tr>
<tr>
<td style="text-align: center;">RandomAgent</td>
<td style="text-align: center;">1.66</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2.11</td>
</tr>
</tbody>
</table>
<p>TABLE I: Performance on the test set of 20 games in (unless stated otherwise) 1000 time steps per game. "\% completion" is the average score percentage an agent achieved over all games and runs; "\% non-zero" is the percentage of games in which an agent achieved any score, averaged over all runs. Standard deviations (SD), whereever given, refer to 10 runs over all games. Where they are not given, only 1 run could be completed.
time when scaling up to the 1000 time steps required by the competition. Additionally, the experiment running Golovin for 10,000 time steps per game shows that NAIL and CARL are stronger than Golovin even if Golovin is given a ten-fold time advantage. This is particularly impressive considering that NAIL and CARL are arguably using less domain-specific knowledge than Golovin, e.g. Golovin's "battle mode" [20]. It is possible that this domain-specific knowledge overfits to the training set used by the authors, and generalizes less well to the games in our test set. However, NAIL and CARL do require much longer thinking time than Golovin, and we can confirm the competitors' claim [20] that Golovin is an improvement over BYUAGENT 2016. The general level of text adventure AIs has increased from every competition to the next so far.</p>
<p>The last row of Table I shows the performance of our RandomAgent, described in Section III. Only with NAIL in 2018 has the first agent managed to beat it. This is probably due to its set of pre-specified commands being extremely simple and generalizing very well across games - changing locations and trying to pick up all possible objects is typical player behavior in text-based adventure games. Again, overfitting to their own training sets might have been the problem for our competitors before NAIL, using too many time steps on larger vocabularies and more intricate strategies that do not work in as many different game situations. We are optimistic for future agents now that the RandomAgent has been convincingly surpassed.</p>
<p>Considering the overall results, it is clear that all agents are facing major unsolved challenges. Even the strongest agent NAIL can currently only complete about $2.6 \%$ of a given test game on average, and gets no points at all in more than half of them. Many of the points achieved are given by the respective game just for starting, for initially submitting the 'get all' command in order to pick up any suitable objects in the first scene, or for walking into one of the compass directions. Despite some progress from</p>
<p>2016 to 2018, no agent is anywhere close to completing a single game. The problems of course include the very difficult scientific questions of how to extract information from natural language text, and how to map it to effective action in turn described in natural language, possibly via an intermediary model of the agent's environment. For example, the BYUAGENT 2016 is unable to deal with games that require giving prepositional commands [18] such as 'give dagger to wizard', or inferring the correct term for manipulable objects (such as requiring the command 'get shiny object' after describing "something shiny").
Looking at the design of all agents, they appear to share a common assumption that all actions which lead to new states are beneficial. Therefore these agents are essentially overcoming the challenge of sparse rewards from the environment by following an innate behaviour akin to work on curiosity as an intrinsic motivation [28] or novelty search [29]. Whilst these approaches have shown broad applicability, it is feasible to construct a pathological text-adventure game where such strong exploration would be punished, and an agent only rewarded for reaching a small subset of states. In the future hopefully more goaloriented agents will be developed, such that good and bad state changes can be distinguished and maybe even predicted.
From analysing the gameplay logs we can also conclude that more technical challenges are common to most agents as well, namely the correct parsing of game output, and differentiating between in-game and out-of-game messages to the player. All games conform to the type of narrationaction loop typical for text adventure games; but as they are originally written for human players, many games require a greater amount of flexibility than currently supported by AI agents. As an example, many games begin with an out-of-game message such as 'Would you like to resume a saved game ( $\mathrm{Y} / \mathrm{N})$ ?'. Reacting to these as if they were in-game narratives often leads to complete failure at the game. Some games respond to the input 'hint' with an in-game hint regarding the puzzle at hand, from which the agents can successfully retrieve information such as nouns and verbs to try in future actions; however, some games respond to 'hint' by opening an out-ofgame multiple-choice menu, which agents currently cannot handle. Furthermore, some agents have trouble identifying the score they have achieved, because they are expecting it in a slightly different format from what the game at hand returns. The game could for example respond to the input 'score' with 'If you were to stop now, you would score 50 points out of a maximum of $1500^{\prime}$, instead of the expected 'You have so far scored 50 out of a possible 1500, in 23 turns'. Golovin for example stops playing if it cannot identify the game score for 10 successive attempts - however, it aborts many games with valid but unexpected score formats, while playing others that actually do not keep a score but return a message in the expected format because the author did not care to remove this functionality (e.g. 'You have so far scored 0 out of a possible 0 , in 23 turns').</p>
<p>The creativity of interactive fiction writers can lead to even greater challenges in parsing and/or scoring, but along with scoreless games these challenges have been removed from the test set for the foreseeable future. For example, games can have a non-numerical ranking ("captain") instead of a score; they can use made-up words or languages (even for reporting the score); or they can only reward points after the player has finished significant parts of the game, giving no clear indication that the current score is zero before those milestones are reached.</p>
<p>Additionally, as the 2016 and 2017 competitions were not originally planned to use a test set of multiple games, we did not require functionality from the participants that would make (repeated) testing on such a set feasible or convenient. While Golovin for example recovered gracefully from failed interactions with individual games by reporting a result of zero points, the BYU agents frequently did not start at all on a given game, or even froze the OS. In combination with parsing and scoring problems, this made it necessary to manually re-start and supervise the agents on each game, which is why we can only report the result from a single run per test game in Table I. In future competitions, we will improve our descriptions of the necessary functionality, probably setting time limits per time step as well, to allow for a more streamlined, fair, and reliable agent evaluation.</p>
<p>Finally, we note that the published version of BYUAGENT 2016 [18] as well as some prior work [14] use reinforcement learning, playing a given game many times and gradually improving performance by learning successful commands for the game at hand. So far however, our competition has not provided a learning track, and therefore required the submission of fully trained agents, or agents capable of one-shot learning within a single game-playing episode. This does not fully reflect the capabilities of some agents. Therefore, we are considering the introduction of a learning track. In addition to the improvements for future competitions described above, this track could profit from the lessons learned by the broader reinforcement learning community on the topic of evaluating and comparing agents [30], [31]. For example, (1) reporting agent's average performance at several fixed stages of training to enable comparison of both final performance and rate of learning; (2) running multiple repeats to evaluate variance in learning performance due to the known issues of robustness and reproducibility with modern reinforcement learning algorithms; and (3) requiring all entrants to open source agent submissions and fully document hyper-parameter settings.</p>
<h2>VII. CONCLUSION</h2>
<p>Contemporary machine learning techniques have recently had many successes in game-playing domains such as Go that are traditionally hard for AI [32]. While it is clear that games provide an artificially restricted domain, we claim in this article that there is a tendency for game AI domains to be chosen such that the applicable operators are known in advance. This bias effectively means that</p>
<p>the epistemological problems of AI, first raised by John McCarthy [4], [33], are neglected. Broadly, such problems are concerned with extracting salient knowledge in nontrivial environments. This is unfortunate, since they are highly relevant for many real-world applications of natural language processing: The BYU team, for example, has recently acquired funding from Amazon as part of the "Alexa Prize", a challenge to create social bots that can converse coherently and engagingly with humans. They informed us "Some of the research that went into CARL was foundational in our approach to creating [the Alexa competitor] EVE. So it might please you to know that the CIG competition is having ripples with pretty wide impact" (Nancy Fulda, personal communication, Feb. 6, 2018).</p>
<p>The Text-Based Adventure AI Competition arose out of a desire to re-emphasise these neglected aspects of AI and motivate further experimentation. To date, mostly model-free approaches have been used as predicting the state transitions that might be caused by future actions proves to be very challenging. Golovin's exploration command generator and NAIL's knowledge graph have made some first steps, but there is still much room for improvement, and we are far from a consensus on how to optimally tackle the related problems. We hope that the community will perceive the challenge offered by this competition as both long-standing and meaningful, and respond with new approaches that push the envelope of existing wisdom regarding both hard problems and good solution mechanisms.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>We would like to acknowledge the hard work of all competitors. Specifically, our thanks go to Nancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate from Brigham Young University, USA; Bartosz Kostka, Jarosław Kwiecień, Jakub Kowalski, and Pawel Rychlikowski from the University of Wrocław, Poland; and Matthew Hausknecht, Ricky Loynd, Shuohang Wang, and Greg Yang from Microsoft Research. We would also like to thank Karthik Narasimhan and Tejas D. Kulkarni for their input when originally defining the format of the first competition.</p>
<h2>REFERENCES</h2>
<p>[1] N. Montfort, Twisty Little Passages: An Approach to Interactive Fiction. MIT Press, 2004.
[2] D. K. Nelson, The Inform Designer's Manual. Sanderson, 2000.
[3] J. McCarthy, Situations, Actions, and Causal Laws, ser. Memo (Stanford AI Project). Comtex Scientific, 1963.
[4] , "Epistemological Problems of Artificial Intelligence," in Proceedings of the 5th International Joint Conference on Artificial Intelligence - Volume 2, 1977.
[5] J. Gibson, The Senoes Considered as Perceptual Systems. Allen \&amp; Unwin, 1968.
[6] R. Schank and R. Abelson, Scripts, Plans, Goals and Understanding: An Inquiry into Human Knowledge Structures. Hillsdale, NJ.: Lawrence Erlbaum Associates, 1977.
[7] W. G. Lehnert, "Plot Units and Narrative Summarization," Cognitive Science, vol. 5, no. 4, pp. 293-331, 1981.
[8] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman, "Indexing by Latent Semantic Analysis," Journal of the American Society for Information Science, vol. 41, no. 6, pp. 391-407, 1990.
[9] A. Goyal, E. Riloff, and H. Daumé, III, "Automatically Producing Plot Unit Representations for Narrative Text," in Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, 2010, pp. 77-86.
[10] S. R. K. Branaean, D. Silver, and R. Barzilay, "Non-Linear Monte-Carlo Search in Civilization II," in Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI 2011), 2011, pp. 2404-2410.
[11] M. O. Riedl and V. Bulitko, "Interactive narrative: An intelligent systems approach," AI Magazine, vol. 34, no. 1, p. 67, 2012.
[12] B. Hlubocky and E. Amir, "Knowledge-gathering Agents in Adventure Games," in AAAI-04 Workshop on Challenges in Game AI, 2004.
[13] E. Amir and A. Chang, "Learning partially observable deterministic action models," Journal of Articial Intelligence Research, vol. 33, pp. 349-402, 2008.
[14] K. Narasimhan, T. D. Kulkarni, and R. Barzilay, "Language Understanding for Text-based Games Using Deep Reinforcement Learning," CoRR, vol. abs/1506.08941, 2015.
[15] M. T. Russotto, "ZPlet: A Z-Machine for Java," Available online at https://sourceforge.net/projects/zplet, accessed Feb 6th Feb, 2018.
[16] D. K. Kevin Bracey, Jason C. Penney, "The ZMachine Standards Document, version 1.1," Available at http://inform-fiction.org/zmachine/standards/z1point1/index.html, February 2014, Accessed 1st December 2017.
[17] S. J. Russell and P. Norvig, Artificial Intelligence: A Modern Approach, 2nd ed. Pearson Education, 2003.
[18] N. Fulda, D. Ricks, B. Murdoch, and D. Wingate, "What Can You Do with a Rock? Affordance Extraction via Word Embeddings," in Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI 2017), C. Sierra, Ed., 2017, pp. 1039-1045.
[19] T. Mikolov, K. Chen, G. Corrado, and J. Dean, "Efficient Estimation of Word Representations in Vector Space," CoRR, vol. abs/1301.3781, 2013.
[20] B. Kostka, J. Kwiecień, J. Kowalski, and P. Rychlikowski, "Textbased Adventures of the Golovin AI Agent," in Proceedings of the 2017 IEEE Conference on Computational Intelligence and Games (CIG 2017), 2017, pp. 181-188.
[21] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin, "A Neural Probabilistic Language Model," Journal of Machine Learning Research, vol. 3, pp. 1137-1155, 2003.
[22] R. Kiros, Y. Zhu, R. Salakhutdinov, R. S. Zemel, A. Torralba, R. Urtasun, and S. Fidler, "Skip-Thought Vectors," CoRR, vol. abs/1506.06726, 2015.
[23] A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov, "Bag of tricks for efficient text classification," CoRR, vol. abs/1607.01759, 2016. [Online]. Available: http://arxiv.org/abs/1607.01759
[24] D. P. Liebana, S. Samothrakis, J. Togelius, T. Schaul, and S. M. Lucas, "General Video Game AI: Competition, Challenges and Opportunities," in Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI 2016), 2016, pp. 43354337.
[25] M. Genesereth, N. Love, and B. Pell, "General Game Playing: Overview of the AAAI Competition," AI magazine, vol. 26, no. 2, p. 62, 2005.
[26] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, "The arcade learning environment: An evaluation platform for general agents." J. Artif. Intell. Res.(JAIR), vol. 47, pp. 253-279, 2013.
[27] J. Togelius, "AI researchers, Video Games are your friends!" in International Joint Conference on Computational Intelligence. Springer, 2015, pp. 3-18.
[28] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, "Curiositydriven exploration by self-supervised prediction," in International Conference on Machine Learning, 2017, pp. 2778-2787.
[29] J. Lehman and K. O. Stanley, "Exploiting open-endedness to solve problems through the search for novelty," in ALIFE, 2008, pp. 329-336.
[30] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger, "Deep Reinforcement Learning That Matters," in Proceedings of the Thirty-Second AAAI Conference on Artificial</p>
<p>Intelligence (AAAI-18), S. A. McIlraith and K. Q. Weinberger, Eds. AAAI Press, 2018, pp. 3207-3214.
[31] M. C. Machado, M. G. Bellemare, E. Talvitie, J. Veness, M. Hausknecht, and M. Bowling, "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents," arXiv preprint arXiv:1709.06009, 2017.
[32] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al., "Mastering the Game of Go without Human Knowledge," Nature, vol. 550, no. 7676, pp. 354-359, 2017.
[33] J. McCarthy and P. J. Hayes, "Readings in Nonmonotonic Reasoning," M. L. Ginsberg, Ed. Morgan Kaufmann Publishers Inc., 1987, ch. Some Philosophical Problems from the Standpoint of Artificial Intelligence, pp. 26-45.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ The NAIL agent is available open source at: http://aka.ms/nail&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{6}$ The authors of Golovin and the BYU agents themselves seem to have already agreed on a training set of 50 text-based adventure games for algorithm development and testing [18], [20].&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
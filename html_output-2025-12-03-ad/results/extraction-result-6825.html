<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6825 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6825</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6825</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-277509998</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.01995v1.pdf" target="_blank">Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics</a></p>
                <p><strong>Paper Abstract:</strong> Recent advances in large language models (LLMs) have shown impressive progress in mathematical reasoning tasks. However, current evaluation benchmarks predominantly focus on the accuracy of final answers, often overlooking the crucial logical rigor for mathematical problem solving. The claim that state-of-the-art LLMs can solve Math Olympiad-level problems requires closer examination. To explore this, we conducted both qualitative and quantitative human evaluations of proofs generated by LLMs, and developed a schema for automatically assessing their reasoning capabilities. Our study reveals that current LLMs fall significantly short of solving challenging Olympiad-level problems and frequently fail to distinguish correct mathematical reasoning from clearly flawed solutions. Our analyses demonstrate that the occasional correct final answers provided by LLMs often result from pattern recognition or heuristic shortcuts rather than genuine mathematical reasoning. These findings underscore the substantial gap between LLM performance and human expertise in advanced mathematical reasoning and highlight the importance of developing benchmarks that prioritize the soundness of the reasoning used to arrive at an answer rather than the mere correctness of the final answers.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6825.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6825.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A frontier proprietary language model from OpenAI evaluated in this study on IMO-shortlist mathematics; reported to produce many correct final answers but very few fully correct proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary frontier LLM from OpenAI; described in paper as a state-of-the-art reasoning model (internal reasoning tokens not visible to authors).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>not specified (proprietary transformer family)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper; paper notes models building on post-training on chain-of-thought data generally.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-thought style internal reasoning tokens (post-training on CoT data referenced); evaluated both generation and verification behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>IMO Shortlist (this study)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>455 IMO-shortlist problems (2009–2023) across algebra, combinatorics, geometry, number theory; emphasis on proof correctness, not just final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Proof generation and proof verification (distinguishing correct vs incorrect solutions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Percent of fully correct solutions; final-answer accuracy; verification accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Correct solutions: 1.9% (fully correct proofs); Partially correct: 3.9%; Incorrect: 94.2% (Table 1). Final‑answer accuracy: 30.8% and among cases with correct final answer, 12.5% had fully correct proofs (Table 2). As verifier, labeling real solutions correct: 31%; labeling wrong solutions correct: 39% (Table 3). Pairwise selection accuracy: ~50% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Paper reports that final-answer–only benchmarks (e.g., MATH/GSM8K) overestimate reasoning ability; o1's final-answer accuracy substantially higher than its full-proof correctness (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>o1 frequently produces correct final numerical/symbolic answers while the stepwise proofs are overwhelmingly incorrect; when asked to verify solutions, it performs at or near chance and often fails to distinguish fallacious from valid proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Very low rate of fully correct proofs despite non-trivial final-answer accuracy; fails as a verifier (cannot reliably flag incorrect proofs), prone to fallacies such as inventing facts, proposal-without-verification, proof-by-example, trial-and-error, and calculation mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6825.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6825.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o1-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o1-mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller variant of OpenAI's o1 family evaluated on IMO-shortlist problems; in experiments produced no fully correct proofs despite some correct final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o1-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Smaller-capacity variant of the o1 family (details not specified in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified; paper groups it among models employing post-training on chain-of-thought data in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-thought style prompting/use implied; evaluated for generation and verification</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>IMO Shortlist (this study)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>455 IMO-shortlist proof-style problems emphasizing rigorous reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Proof generation and proof verification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Percent fully correct solutions; final-answer accuracy; verifier labeling accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Correct solutions: 0.0%; Partially correct: 0.0%; Incorrect: 100.0% (Table 1). Final‑answer accuracy: 35.0% with 0% of those having fully correct proofs (Table 2). As verifier, labeling real solutions correct: 36%; labeling wrong solutions correct: 45% (Table 3). Pairwise selection accuracy: 46% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Shows that smaller variants can still produce correct final answers but yield no fully correct proofs; performs worse or at chance compared to larger variants on verification.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>o1-mini produced correct-seeming final answers occasionally but produced no fully correct proofs; also unreliable as a verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Zero fully correct proofs in this benchmark; verification accuracy below or near random; common reasoning fallacies present.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6825.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6825.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A more recent OpenAI-family model included in the study; showed low fully-correct-proof rates but slightly better verifier pairwise performance than some models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Frontier-model variant (details proprietary); included among evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified; evaluated in context of models with CoT post-training.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-thought style; evaluated both generation and verification</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>IMO Shortlist (this study)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>IMO shortlist problems (proofs required).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Proof generation and verification; pairwise correct-solution selection</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Percent fully correct solutions; final-answer accuracy; verifier labelling and pairwise selection accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Correct solutions: 3.3%; Partially correct: 4.4%; Incorrect: 92.2% (Table 1). Final‑answer accuracy: 48.3% with 14.3% of those having fully correct proofs (Table 2). As verifier, labeling real solutions correct: 26%; labeling wrong solutions correct: 31% (Table 3). Pairwise selection accuracy: 52% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Higher final-answer accuracy than o1 but still low proof correctness; modest improvement over chance in pairwise verifier task (+2% over random as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>o3-mini occasionally yields correct proofs but majority are flawed; its ability to pick the valid solution in paired comparisons modestly exceeds random chance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Still high incorrect-proof rate; common fallacies remain; limited verifier reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6825.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6825.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Gemini 2.0 (Flash Thinking mode)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's Gemini 2.0 (Flash Thinking mode) was evaluated; produced many incorrect proofs despite some correct final answers and demonstrated near-chance verification performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 2.0 (Flash Thinking)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large proprietary model from Google (Gemini family); authors used its 'Flash Thinking' mode traces when available to inspect reasoning behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in paper; noted among models that leverage post-training on chain-of-thought corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-thought style internal reasoning; analysis used visible 'thinking traces' for non-OpenAI models</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>IMO Shortlist (this study)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>455 contest-level proof problems with human evaluator judgments focused on reasoning soundness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Proof generation; verification; fallacy analysis</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Percent fully correct solutions; final-answer accuracy; verifier labeling and pairwise accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Correct solutions (Table 1): ~2.0% fully correct, 0.0% partially correct, 98.9% incorrect (parsed from Table 1). Final‑answer accuracy: 43.8% with 0% of those having fully correct proofs (Table 2). As verifier, labeling real solutions correct: 52%; labeling wrong solutions correct: 50% (Table 3). Pairwise selection accuracy: 49% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Final-answer accuracy does not translate to proof correctness; as verifier performance around chance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Gemini 2.0 often produces incorrect proofs though it can output correct final answers; visible 'thinking traces' indicate vagueness and 'proposal without verification' fallacies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Frequent invented facts and unjustified claims; inability to reliably verify solutions; exhibits similar failure modes to other frontier LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6825.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6825.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek R1 (DeepSeek)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model (DeepSeek family) evaluated in the paper; it produced the highest final-answer accuracy among evaluated models but still almost no fully correct proofs and only modest verifier performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary model from DeepSeek (technical report referenced); authors inspected 'thinking traces' when available.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified here; paper notes reinforcement learning with reward functions based on final-answer correctness is used by some models (DeepSeek report cited).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-thought style generation with RLHF-style reward signals used in training (paper conjectures reward based on final answer may encourage shortcuts).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>IMO Shortlist (this study)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>455 IMO-shortlist problems; human evaluation of proof correctness and categorization of fallacies.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Proof generation and verification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Percent fully correct solutions; final-answer accuracy; verifier metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Correct solutions: 3.8%; Partially correct: 6.7%; Incorrect: 89.4% (Table 1). Final‑answer accuracy: 63.2% but 0% of those final-correct answers corresponded to fully correct proofs (Table 2). As verifier, labeling real solutions correct: 48%; labeling wrong solutions correct: 43% (Table 3). Pairwise selection accuracy: 57% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>DeepSeek attains much higher final-answer accuracy vs other models, but this does not translate into correct stepwise proofs; as a verifier it performs modestly above chance (+7% over random in pairwise selection as noted).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DeepSeek frequently outputs correct final answers while its underlying proofs are usually flawed; greatest gap between final-answer correctness and actual proof correctness; verifier performance modestly above random but insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>High prevalence of inventing wrong facts and proposal-without-verification fallacies; reward training based on final-answer correctness likely encourages heuristics over rigorous reasoning; verifier not reliably accurate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6825.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6825.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) prompting / post-training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting and post-training technique that encourages LLMs to generate intermediate reasoning steps before producing final answers; widely cited as improving performance on complex reasoning tasks but not sufficient for strict logical proof correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought prompting / post-training</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Technique: elicit or train models to produce step-by-step reasoning tokens (CoT) to improve multi-step problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>method applied to transformer LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>CoT corpora and post‑training on chain-of-thought data (generic reference; specific datasets not listed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompting to generate intermediate steps; post-training on CoT examples; used with inference strategies like self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Various (GSM8K, MATH, and contest-level benchmarks referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Used broadly to improve multi-step math/logic tasks (benchmarks include GSM8K and MATH which test arithmetic and olympiad-style reasoning to different degrees).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step arithmetic/math reasoning, proof-style problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Benchmark accuracies (often reported as final-answer accuracy or improvement in pass rates)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper notes prior reports of large improvements with CoT (references given) but emphasizes CoT can increase final-answer correctness without guaranteeing sound proofs; no single numeric aggregate provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoT and CoT with post-training reported in literature to improve performance on MATH/GSM8K; paper stresses that final-answer improvements can mask lack of genuine formal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT-style training/prompting helps produce intermediate tokens and can raise final‑answer accuracy, but this paper finds CoT does not ensure logically rigorous proofs on IMO-level problems and models still produce systematic fallacies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Does not guarantee formal logical soundness; models trained/finetuned on CoT may rely on heuristics and pattern matching rather than provable reasoning; verification remains unreliable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6825.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6825.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (CoT-SC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inference-time technique that aggregates multiple sampled chain-of-thought reasoning paths and selects the most consistent final answer, intended to improve reasoning robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-Consistency (CoT-SC)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Inference-time sampling and voting over multiple chain-of-thought traces to pick the most frequent/consistent answer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>inference-time sampling aggregation over transformer outputs</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Generate multiple CoT traces and use majority/consensus selection to improve reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Referenced for math benchmarks (MATH, GSM variants)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Used in prior work to boost multi-step problem accuracy by aggregating multiple reasoning samples.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Proof/solution generation in math reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Improvement in final-answer accuracy via majority voting/self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper references CoT-SC as an approach that can improve reasoning in literature, but notes such approaches can still yield correct final answers with unsound rationales; no new numeric results for CoT-SC reported in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Literature reports improvements vs single-sample CoT; this paper warns improvements may reflect heuristics rather than valid proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-consistency may raise final-answer accuracy but does not resolve the core issue: many generated proofs remain logically incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Aggregating flawed reasoning traces can consolidate consistent errors; does not verify formal correctness of derivations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6825.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6825.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generator-Verifier</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generator–Verifier training schema</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A common training/evaluation paradigm where a generator model produces candidate solutions and a verifier (hard-coded, trained, or an LLM) evaluates them and provides reward signals; used to guide reasoning models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Generator–Verifier schema</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Training/optimization schema pairing a generator (produces solutions) with a verifier/reward model to rank or reinforce better solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>meta-training scheme applied to transformer LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Verifier trained on annotated solution quality or using heuristics (paper references prior work such as 'training verifiers' but specific datasets not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Generator proposes solutions; verifier scores them (could be final-answer check, learned model, or LLM-as-judge); generator optimized with RL using verifier rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Discussed in context of math problem solving and verification tasks (IMO shortlist evaluation in this paper tests verifier capability)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Generator-verifier commonly evaluated on math word problems and theorem proving datasets; here the paper assesses whether LLMs can act as reliable verifiers on IMO-level proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Solution generation and verification for rigorous proofs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Verifier reliability (e.g., correct labeling rates, pairwise selection accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>This paper shows LLM verifiers (frontier models) often label incorrect solutions as correct nearly as often as correct solutions (e.g., DeepSeek labeled real correct solutions correct 48% but labeled wrong solutions correct 43% — Table 3), and pairwise selection accuracies near random (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Paper contrasts ideal verifier behavior (high discrimination) with observed near-random performance of LLM verifiers on IMO-level tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generator–Verifier approaches relying on LLMs as verifiers are insufficient for IMO-level rigorous verification because the verifier LLMs fail to reliably detect fallacies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Verifier models trained or rewarded on final-answer correctness can reinforce heuristics and non-rigorous reasoning; LLM-as-verifier performance degrades on complex proof verification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6825.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6825.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge (LLM-based automated evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The practice of using LLMs to automatically assess the quality and correctness of LLM-generated solutions; promoted as a low-cost alternative to human annotation but shown here to be unreliable on contest-level proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Using an LLM (often a stronger one) to evaluate other models' outputs according to correctness and reasoning fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>evaluation/assessment paradigm using transformer LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Judge LLM analyzes candidate solution and outputs correctness judgement; sometimes trained with human-labeled comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used broadly in prior work and examined here on IMO-shortlist verification tasks</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Intended to automate evaluation of reasoning; here tested by asking LLMs to classify correct vs incorrect proofs and to choose the correct solution in a pair.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Solution verification, comparative judgement</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Percentage of correct/wrong solutions labeled as correct; pairwise selection accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports poor LLM-as-judge reliability: e.g., many models label wrong solutions correct at rates similar to correct solutions (Table 3) and pairwise selection accuracies near random (Table 4: DeepSeek 57%, Gemini 49%, o1 50%, o1-mini 46%, o3-mini 52%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared to human evaluators (expert Olympiad-level annotators), LLM-as-judge results diverge; automated rubrics also failed to distinguish correct vs incorrect proofs reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-as-a-judge is not reliable for rigorous proof verification at IMO level; frontier LLMs often cannot detect clear fallacies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Biases and inability to assess logical rigor; can be misled by final-answer correctness; not a substitute for human verification in high-stakes mathematical proof evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6825.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6825.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IMO Shortlist Benchmark (this study)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IMO Shortlist problems (2009–2023) — evaluation dataset assembled in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated evaluation set of 455 IMO-shortlist problems used here to stress-test LLMs for strict logical reasoning, with human expert annotations classifying full correctness, partial correctness, and types of fallacies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>IMO Shortlist (2009–2023)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset: 455 contest-level problems across algebra, combinatorics, geometry, number theory; annotated by expert evaluators for correctness and categorized fallacy types.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>IMO Shortlist (this study)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Contest-level, proof-focused problems where sound, multi-step reasoning is required; used to evaluate both generation and verification capability of LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Proof generation (constructing rigorous proofs) and verification (judging correctness of proofs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Percent fully correct / partially correct / incorrect solutions produced by LLMs; verification labeling and pairwise selection accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Aggregate results: extremely low fully correct proof rates across frontier LLMs (e.g., many models ≲4% fully correct); large gaps between final-answer accuracy and proof correctness (Table 2 shows substantial final-answer accuracy but near-zero proof correctness conditional rates for several models).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Paper contrasts IMO Shortlist evaluation (proof-focused, human annotated) with common benchmarks that only check final answers (GSM8K, MATH), showing the latter overestimates reasoning ability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>IMO Shortlist detects serious reasoning failures not captured by final-answer accuracy; used to identify common fallacies (Inventing Wrong Facts, Proposal Without Verification, Proof by Example, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Potential data leakage risk since some shortlist solutions are publicly available; paper argues leakage does not explain the inability to produce rigorous proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6825.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6825.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-72B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model mentioned in the introduction as achieving high accuracy on standard math benchmarks (GSM8K, MATH), cited to illustrate that final-answer benchmarks report strong performance that may misrepresent proof capability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-72B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large LLM cited in paper for reported benchmark scores (details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B (as implied by name, reported in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper; reported to achieve high GSM8K and MATH accuracies in other work (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Not described here; mentioned in context of models achieving high final-answer accuracy on standard benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>GSM8K: grade-school math word problems; MATH: broader high-school to contest math problems — both typically scored by final-answer correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Math word problems and contest problems judged by final-answer correctness</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Final-answer accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported in paper's intro: GSM8K ~91.5% and MATH ~62.1% (attributed to Qwen2.5-72B in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Used to illustrate that high final-answer metrics on common benchmarks may not reflect true rigorous reasoning ability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper cites high benchmark scores from models like Qwen2.5-72B to motivate need for proof-focused evaluation; such scores can be achieved via heuristics rather than sound proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>High final-answer accuracy does not imply production of sound, verifiable proofs on Olympiad-level problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>CHAMP: A competition-level dataset for fine-grained analyses of LLMs' mathematical reasoning capabilities <em>(Rating: 2)</em></li>
                <li>Omni-math: A universal olympiad level mathematic benchmark for large language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>MathOdyssey: Benchmarking mathematical problem-solving skills in large language models using odyssey math data <em>(Rating: 1)</em></li>
                <li>OlympiadBench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems <em>(Rating: 1)</em></li>
                <li>Large language models are hidden reasoners: Unlocking latent reasoning capabilities via self-rewarding <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6825",
    "paper_id": "paper-277509998",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "o1",
            "name_full": "OpenAI o1",
            "brief_description": "A frontier proprietary language model from OpenAI evaluated in this study on IMO-shortlist mathematics; reported to produce many correct final answers but very few fully correct proofs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "o1",
            "model_description": "Proprietary frontier LLM from OpenAI; described in paper as a state-of-the-art reasoning model (internal reasoning tokens not visible to authors).",
            "model_size": null,
            "architecture_type": "not specified (proprietary transformer family)",
            "training_data": "Not specified in this paper; paper notes models building on post-training on chain-of-thought data generally.",
            "reasoning_method": "Chain-of-thought style internal reasoning tokens (post-training on CoT data referenced); evaluated both generation and verification behaviors.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "IMO Shortlist (this study)",
            "benchmark_description": "455 IMO-shortlist problems (2009–2023) across algebra, combinatorics, geometry, number theory; emphasis on proof correctness, not just final answer.",
            "task_type": "Proof generation and proof verification (distinguishing correct vs incorrect solutions)",
            "performance_metric": "Percent of fully correct solutions; final-answer accuracy; verification accuracy",
            "performance_value": "Correct solutions: 1.9% (fully correct proofs); Partially correct: 3.9%; Incorrect: 94.2% (Table 1). Final‑answer accuracy: 30.8% and among cases with correct final answer, 12.5% had fully correct proofs (Table 2). As verifier, labeling real solutions correct: 31%; labeling wrong solutions correct: 39% (Table 3). Pairwise selection accuracy: ~50% (Table 4).",
            "comparison_with_baseline": "Paper reports that final-answer–only benchmarks (e.g., MATH/GSM8K) overestimate reasoning ability; o1's final-answer accuracy substantially higher than its full-proof correctness (see Table 2).",
            "key_findings": "o1 frequently produces correct final numerical/symbolic answers while the stepwise proofs are overwhelmingly incorrect; when asked to verify solutions, it performs at or near chance and often fails to distinguish fallacious from valid proofs.",
            "limitations": "Very low rate of fully correct proofs despite non-trivial final-answer accuracy; fails as a verifier (cannot reliably flag incorrect proofs), prone to fallacies such as inventing facts, proposal-without-verification, proof-by-example, trial-and-error, and calculation mistakes.",
            "uuid": "e6825.0",
            "source_info": {
                "paper_title": "Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "o1-mini",
            "name_full": "OpenAI o1-mini",
            "brief_description": "A smaller variant of OpenAI's o1 family evaluated on IMO-shortlist problems; in experiments produced no fully correct proofs despite some correct final answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "o1-mini",
            "model_description": "Smaller-capacity variant of the o1 family (details not specified in paper).",
            "model_size": null,
            "architecture_type": "not specified",
            "training_data": "Not specified; paper groups it among models employing post-training on chain-of-thought data in the literature.",
            "reasoning_method": "Chain-of-thought style prompting/use implied; evaluated for generation and verification",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "IMO Shortlist (this study)",
            "benchmark_description": "455 IMO-shortlist proof-style problems emphasizing rigorous reasoning.",
            "task_type": "Proof generation and proof verification",
            "performance_metric": "Percent fully correct solutions; final-answer accuracy; verifier labeling accuracy",
            "performance_value": "Correct solutions: 0.0%; Partially correct: 0.0%; Incorrect: 100.0% (Table 1). Final‑answer accuracy: 35.0% with 0% of those having fully correct proofs (Table 2). As verifier, labeling real solutions correct: 36%; labeling wrong solutions correct: 45% (Table 3). Pairwise selection accuracy: 46% (Table 4).",
            "comparison_with_baseline": "Shows that smaller variants can still produce correct final answers but yield no fully correct proofs; performs worse or at chance compared to larger variants on verification.",
            "key_findings": "o1-mini produced correct-seeming final answers occasionally but produced no fully correct proofs; also unreliable as a verifier.",
            "limitations": "Zero fully correct proofs in this benchmark; verification accuracy below or near random; common reasoning fallacies present.",
            "uuid": "e6825.1",
            "source_info": {
                "paper_title": "Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "o3-mini",
            "name_full": "o3-mini",
            "brief_description": "A more recent OpenAI-family model included in the study; showed low fully-correct-proof rates but slightly better verifier pairwise performance than some models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "o3-mini",
            "model_description": "Frontier-model variant (details proprietary); included among evaluated models.",
            "model_size": null,
            "architecture_type": "not specified",
            "training_data": "Not specified; evaluated in context of models with CoT post-training.",
            "reasoning_method": "Chain-of-thought style; evaluated both generation and verification",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "IMO Shortlist (this study)",
            "benchmark_description": "IMO shortlist problems (proofs required).",
            "task_type": "Proof generation and verification; pairwise correct-solution selection",
            "performance_metric": "Percent fully correct solutions; final-answer accuracy; verifier labelling and pairwise selection accuracy",
            "performance_value": "Correct solutions: 3.3%; Partially correct: 4.4%; Incorrect: 92.2% (Table 1). Final‑answer accuracy: 48.3% with 14.3% of those having fully correct proofs (Table 2). As verifier, labeling real solutions correct: 26%; labeling wrong solutions correct: 31% (Table 3). Pairwise selection accuracy: 52% (Table 4).",
            "comparison_with_baseline": "Higher final-answer accuracy than o1 but still low proof correctness; modest improvement over chance in pairwise verifier task (+2% over random as reported).",
            "key_findings": "o3-mini occasionally yields correct proofs but majority are flawed; its ability to pick the valid solution in paired comparisons modestly exceeds random chance.",
            "limitations": "Still high incorrect-proof rate; common fallacies remain; limited verifier reliability.",
            "uuid": "e6825.2",
            "source_info": {
                "paper_title": "Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Gemini 2.0",
            "name_full": "Google Gemini 2.0 (Flash Thinking mode)",
            "brief_description": "Google's Gemini 2.0 (Flash Thinking mode) was evaluated; produced many incorrect proofs despite some correct final answers and demonstrated near-chance verification performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini 2.0 (Flash Thinking)",
            "model_description": "Large proprietary model from Google (Gemini family); authors used its 'Flash Thinking' mode traces when available to inspect reasoning behavior.",
            "model_size": null,
            "architecture_type": "not specified",
            "training_data": "Not specified in paper; noted among models that leverage post-training on chain-of-thought corpora.",
            "reasoning_method": "Chain-of-thought style internal reasoning; analysis used visible 'thinking traces' for non-OpenAI models",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "IMO Shortlist (this study)",
            "benchmark_description": "455 contest-level proof problems with human evaluator judgments focused on reasoning soundness.",
            "task_type": "Proof generation; verification; fallacy analysis",
            "performance_metric": "Percent fully correct solutions; final-answer accuracy; verifier labeling and pairwise accuracy",
            "performance_value": "Correct solutions (Table 1): ~2.0% fully correct, 0.0% partially correct, 98.9% incorrect (parsed from Table 1). Final‑answer accuracy: 43.8% with 0% of those having fully correct proofs (Table 2). As verifier, labeling real solutions correct: 52%; labeling wrong solutions correct: 50% (Table 3). Pairwise selection accuracy: 49% (Table 4).",
            "comparison_with_baseline": "Final-answer accuracy does not translate to proof correctness; as verifier performance around chance.",
            "key_findings": "Gemini 2.0 often produces incorrect proofs though it can output correct final answers; visible 'thinking traces' indicate vagueness and 'proposal without verification' fallacies.",
            "limitations": "Frequent invented facts and unjustified claims; inability to reliably verify solutions; exhibits similar failure modes to other frontier LLMs.",
            "uuid": "e6825.3",
            "source_info": {
                "paper_title": "Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "DeepSeek R1",
            "name_full": "DeepSeek R1 (DeepSeek)",
            "brief_description": "A large language model (DeepSeek family) evaluated in the paper; it produced the highest final-answer accuracy among evaluated models but still almost no fully correct proofs and only modest verifier performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeepSeek R1",
            "model_description": "Proprietary model from DeepSeek (technical report referenced); authors inspected 'thinking traces' when available.",
            "model_size": null,
            "architecture_type": "not specified",
            "training_data": "Not specified here; paper notes reinforcement learning with reward functions based on final-answer correctness is used by some models (DeepSeek report cited).",
            "reasoning_method": "Chain-of-thought style generation with RLHF-style reward signals used in training (paper conjectures reward based on final answer may encourage shortcuts).",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "IMO Shortlist (this study)",
            "benchmark_description": "455 IMO-shortlist problems; human evaluation of proof correctness and categorization of fallacies.",
            "task_type": "Proof generation and verification",
            "performance_metric": "Percent fully correct solutions; final-answer accuracy; verifier metrics",
            "performance_value": "Correct solutions: 3.8%; Partially correct: 6.7%; Incorrect: 89.4% (Table 1). Final‑answer accuracy: 63.2% but 0% of those final-correct answers corresponded to fully correct proofs (Table 2). As verifier, labeling real solutions correct: 48%; labeling wrong solutions correct: 43% (Table 3). Pairwise selection accuracy: 57% (Table 4).",
            "comparison_with_baseline": "DeepSeek attains much higher final-answer accuracy vs other models, but this does not translate into correct stepwise proofs; as a verifier it performs modestly above chance (+7% over random in pairwise selection as noted).",
            "key_findings": "DeepSeek frequently outputs correct final answers while its underlying proofs are usually flawed; greatest gap between final-answer correctness and actual proof correctness; verifier performance modestly above random but insufficient.",
            "limitations": "High prevalence of inventing wrong facts and proposal-without-verification fallacies; reward training based on final-answer correctness likely encourages heuristics over rigorous reasoning; verifier not reliably accurate.",
            "uuid": "e6825.4",
            "source_info": {
                "paper_title": "Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Chain-of-Thought",
            "name_full": "Chain-of-Thought (CoT) prompting / post-training",
            "brief_description": "A prompting and post-training technique that encourages LLMs to generate intermediate reasoning steps before producing final answers; widely cited as improving performance on complex reasoning tasks but not sufficient for strict logical proof correctness.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Chain-of-Thought prompting / post-training",
            "model_description": "Technique: elicit or train models to produce step-by-step reasoning tokens (CoT) to improve multi-step problem solving.",
            "model_size": null,
            "architecture_type": "method applied to transformer LLMs",
            "training_data": "CoT corpora and post‑training on chain-of-thought data (generic reference; specific datasets not listed in this paper).",
            "reasoning_method": "Prompting to generate intermediate steps; post-training on CoT examples; used with inference strategies like self-consistency.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Various (GSM8K, MATH, and contest-level benchmarks referenced)",
            "benchmark_description": "Used broadly to improve multi-step math/logic tasks (benchmarks include GSM8K and MATH which test arithmetic and olympiad-style reasoning to different degrees).",
            "task_type": "Multi-step arithmetic/math reasoning, proof-style problem solving",
            "performance_metric": "Benchmark accuracies (often reported as final-answer accuracy or improvement in pass rates)",
            "performance_value": "Paper notes prior reports of large improvements with CoT (references given) but emphasizes CoT can increase final-answer correctness without guaranteeing sound proofs; no single numeric aggregate provided in this paper.",
            "comparison_with_baseline": "CoT and CoT with post-training reported in literature to improve performance on MATH/GSM8K; paper stresses that final-answer improvements can mask lack of genuine formal reasoning.",
            "key_findings": "CoT-style training/prompting helps produce intermediate tokens and can raise final‑answer accuracy, but this paper finds CoT does not ensure logically rigorous proofs on IMO-level problems and models still produce systematic fallacies.",
            "limitations": "Does not guarantee formal logical soundness; models trained/finetuned on CoT may rely on heuristics and pattern matching rather than provable reasoning; verification remains unreliable.",
            "uuid": "e6825.5",
            "source_info": {
                "paper_title": "Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (CoT-SC)",
            "brief_description": "An inference-time technique that aggregates multiple sampled chain-of-thought reasoning paths and selects the most consistent final answer, intended to improve reasoning robustness.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Self-Consistency (CoT-SC)",
            "model_description": "Inference-time sampling and voting over multiple chain-of-thought traces to pick the most frequent/consistent answer.",
            "model_size": null,
            "architecture_type": "inference-time sampling aggregation over transformer outputs",
            "training_data": null,
            "reasoning_method": "Generate multiple CoT traces and use majority/consensus selection to improve reliability.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Referenced for math benchmarks (MATH, GSM variants)",
            "benchmark_description": "Used in prior work to boost multi-step problem accuracy by aggregating multiple reasoning samples.",
            "task_type": "Proof/solution generation in math reasoning tasks",
            "performance_metric": "Improvement in final-answer accuracy via majority voting/self-consistency",
            "performance_value": "Paper references CoT-SC as an approach that can improve reasoning in literature, but notes such approaches can still yield correct final answers with unsound rationales; no new numeric results for CoT-SC reported in this study.",
            "comparison_with_baseline": "Literature reports improvements vs single-sample CoT; this paper warns improvements may reflect heuristics rather than valid proofs.",
            "key_findings": "Self-consistency may raise final-answer accuracy but does not resolve the core issue: many generated proofs remain logically incorrect.",
            "limitations": "Aggregating flawed reasoning traces can consolidate consistent errors; does not verify formal correctness of derivations.",
            "uuid": "e6825.6",
            "source_info": {
                "paper_title": "Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Generator-Verifier",
            "name_full": "Generator–Verifier training schema",
            "brief_description": "A common training/evaluation paradigm where a generator model produces candidate solutions and a verifier (hard-coded, trained, or an LLM) evaluates them and provides reward signals; used to guide reasoning models.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Generator–Verifier schema",
            "model_description": "Training/optimization schema pairing a generator (produces solutions) with a verifier/reward model to rank or reinforce better solutions.",
            "model_size": null,
            "architecture_type": "meta-training scheme applied to transformer LLMs",
            "training_data": "Verifier trained on annotated solution quality or using heuristics (paper references prior work such as 'training verifiers' but specific datasets not detailed here).",
            "reasoning_method": "Generator proposes solutions; verifier scores them (could be final-answer check, learned model, or LLM-as-judge); generator optimized with RL using verifier rewards.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "Discussed in context of math problem solving and verification tasks (IMO shortlist evaluation in this paper tests verifier capability)",
            "benchmark_description": "Generator-verifier commonly evaluated on math word problems and theorem proving datasets; here the paper assesses whether LLMs can act as reliable verifiers on IMO-level proofs.",
            "task_type": "Solution generation and verification for rigorous proofs",
            "performance_metric": "Verifier reliability (e.g., correct labeling rates, pairwise selection accuracy)",
            "performance_value": "This paper shows LLM verifiers (frontier models) often label incorrect solutions as correct nearly as often as correct solutions (e.g., DeepSeek labeled real correct solutions correct 48% but labeled wrong solutions correct 43% — Table 3), and pairwise selection accuracies near random (Table 4).",
            "comparison_with_baseline": "Paper contrasts ideal verifier behavior (high discrimination) with observed near-random performance of LLM verifiers on IMO-level tasks.",
            "key_findings": "Generator–Verifier approaches relying on LLMs as verifiers are insufficient for IMO-level rigorous verification because the verifier LLMs fail to reliably detect fallacies.",
            "limitations": "Verifier models trained or rewarded on final-answer correctness can reinforce heuristics and non-rigorous reasoning; LLM-as-verifier performance degrades on complex proof verification tasks.",
            "uuid": "e6825.7",
            "source_info": {
                "paper_title": "Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LLM-as-a-judge",
            "name_full": "LLM-as-a-Judge (LLM-based automated evaluation)",
            "brief_description": "The practice of using LLMs to automatically assess the quality and correctness of LLM-generated solutions; promoted as a low-cost alternative to human annotation but shown here to be unreliable on contest-level proofs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLM-as-a-judge",
            "model_description": "Using an LLM (often a stronger one) to evaluate other models' outputs according to correctness and reasoning fidelity.",
            "model_size": null,
            "architecture_type": "evaluation/assessment paradigm using transformer LLMs",
            "training_data": null,
            "reasoning_method": "Judge LLM analyzes candidate solution and outputs correctness judgement; sometimes trained with human-labeled comparisons.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Used broadly in prior work and examined here on IMO-shortlist verification tasks",
            "benchmark_description": "Intended to automate evaluation of reasoning; here tested by asking LLMs to classify correct vs incorrect proofs and to choose the correct solution in a pair.",
            "task_type": "Solution verification, comparative judgement",
            "performance_metric": "Percentage of correct/wrong solutions labeled as correct; pairwise selection accuracy",
            "performance_value": "Paper reports poor LLM-as-judge reliability: e.g., many models label wrong solutions correct at rates similar to correct solutions (Table 3) and pairwise selection accuracies near random (Table 4: DeepSeek 57%, Gemini 49%, o1 50%, o1-mini 46%, o3-mini 52%).",
            "comparison_with_baseline": "Compared to human evaluators (expert Olympiad-level annotators), LLM-as-judge results diverge; automated rubrics also failed to distinguish correct vs incorrect proofs reliably.",
            "key_findings": "LLM-as-a-judge is not reliable for rigorous proof verification at IMO level; frontier LLMs often cannot detect clear fallacies.",
            "limitations": "Biases and inability to assess logical rigor; can be misled by final-answer correctness; not a substitute for human verification in high-stakes mathematical proof evaluation.",
            "uuid": "e6825.8",
            "source_info": {
                "paper_title": "Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "IMO Shortlist Benchmark (this study)",
            "name_full": "IMO Shortlist problems (2009–2023) — evaluation dataset assembled in this paper",
            "brief_description": "A curated evaluation set of 455 IMO-shortlist problems used here to stress-test LLMs for strict logical reasoning, with human expert annotations classifying full correctness, partial correctness, and types of fallacies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "IMO Shortlist (2009–2023)",
            "model_description": "Dataset: 455 contest-level problems across algebra, combinatorics, geometry, number theory; annotated by expert evaluators for correctness and categorized fallacy types.",
            "model_size": null,
            "architecture_type": null,
            "training_data": null,
            "reasoning_method": null,
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "IMO Shortlist (this study)",
            "benchmark_description": "Contest-level, proof-focused problems where sound, multi-step reasoning is required; used to evaluate both generation and verification capability of LLMs.",
            "task_type": "Proof generation (constructing rigorous proofs) and verification (judging correctness of proofs)",
            "performance_metric": "Percent fully correct / partially correct / incorrect solutions produced by LLMs; verification labeling and pairwise selection accuracy",
            "performance_value": "Aggregate results: extremely low fully correct proof rates across frontier LLMs (e.g., many models ≲4% fully correct); large gaps between final-answer accuracy and proof correctness (Table 2 shows substantial final-answer accuracy but near-zero proof correctness conditional rates for several models).",
            "comparison_with_baseline": "Paper contrasts IMO Shortlist evaluation (proof-focused, human annotated) with common benchmarks that only check final answers (GSM8K, MATH), showing the latter overestimates reasoning ability.",
            "key_findings": "IMO Shortlist detects serious reasoning failures not captured by final-answer accuracy; used to identify common fallacies (Inventing Wrong Facts, Proposal Without Verification, Proof by Example, etc.).",
            "limitations": "Potential data leakage risk since some shortlist solutions are publicly available; paper argues leakage does not explain the inability to produce rigorous proofs.",
            "uuid": "e6825.9",
            "source_info": {
                "paper_title": "Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Qwen2.5-72B",
            "name_full": "Qwen2.5-72B",
            "brief_description": "A large language model mentioned in the introduction as achieving high accuracy on standard math benchmarks (GSM8K, MATH), cited to illustrate that final-answer benchmarks report strong performance that may misrepresent proof capability.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Qwen2.5-72B",
            "model_description": "Large LLM cited in paper for reported benchmark scores (details not provided in this paper).",
            "model_size": "72B (as implied by name, reported in cited work)",
            "architecture_type": "not specified",
            "training_data": "Not specified in this paper; reported to achieve high GSM8K and MATH accuracies in other work (cited).",
            "reasoning_method": "Not described here; mentioned in context of models achieving high final-answer accuracy on standard benchmarks.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "GSM8K, MATH (referenced)",
            "benchmark_description": "GSM8K: grade-school math word problems; MATH: broader high-school to contest math problems — both typically scored by final-answer correctness.",
            "task_type": "Math word problems and contest problems judged by final-answer correctness",
            "performance_metric": "Final-answer accuracy",
            "performance_value": "Reported in paper's intro: GSM8K ~91.5% and MATH ~62.1% (attributed to Qwen2.5-72B in cited work).",
            "comparison_with_baseline": "Used to illustrate that high final-answer metrics on common benchmarks may not reflect true rigorous reasoning ability.",
            "key_findings": "Paper cites high benchmark scores from models like Qwen2.5-72B to motivate need for proof-focused evaluation; such scores can be achieved via heuristics rather than sound proofs.",
            "limitations": "High final-answer accuracy does not imply production of sound, verifiable proofs on Olympiad-level problems.",
            "uuid": "e6825.10",
            "source_info": {
                "paper_title": "Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "CHAMP: A competition-level dataset for fine-grained analyses of LLMs' mathematical reasoning capabilities",
            "rating": 2,
            "sanitized_title": "champ_a_competitionlevel_dataset_for_finegrained_analyses_of_llms_mathematical_reasoning_capabilities"
        },
        {
            "paper_title": "Omni-math: A universal olympiad level mathematic benchmark for large language models",
            "rating": 2,
            "sanitized_title": "omnimath_a_universal_olympiad_level_mathematic_benchmark_for_large_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "MathOdyssey: Benchmarking mathematical problem-solving skills in large language models using odyssey math data",
            "rating": 1,
            "sanitized_title": "mathodyssey_benchmarking_mathematical_problemsolving_skills_in_large_language_models_using_odyssey_math_data"
        },
        {
            "paper_title": "OlympiadBench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems",
            "rating": 1,
            "sanitized_title": "olympiadbench_a_challenging_benchmark_for_promoting_agi_with_olympiadlevel_bilingual_multimodal_scientific_problems"
        },
        {
            "paper_title": "Large language models are hidden reasoners: Unlocking latent reasoning capabilities via self-rewarding",
            "rating": 1,
            "sanitized_title": "large_language_models_are_hidden_reasoners_unlocking_latent_reasoning_capabilities_via_selfrewarding"
        }
    ],
    "cost": 0.020865,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics
April 14, 2025</p>
<p>Hamed Mahdavi 
Alireza Hashemi alireza.hashemi13@outlook.com 
Majid Daliri daliri.majid@nyu.edu 
Pegah Mohammadipour pegahmp@psu.edu 
Alireza Farhadi farhadi@aut.ac.ir 
Samira Malek 
Yekta Yazdanifard yekta.yazdanifard@unibocconi.it 
Amir Khasahmadi amir.khasahmadi@autodesk.com 
Vasant Honavar vhonavar@psu.edu </p>
<p>Pennsylvania State University</p>
<p>City University of New york</p>
<p>New York University</p>
<p>Pennsylvania State University</p>
<p>Amirkabir University of Technology</p>
<p>Pennsylvania State University</p>
<p>Bocconi University</p>
<p>Pennsylvania State University</p>
<p>Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics
April 14, 202572FB7CA9A9CB989D353692033F452530arXiv:2504.01995v2[cs.AI]
Recent advances in large language models (LLMs) have shown impressive progress in mathematical reasoning tasks.However, current evaluation benchmarks predominantly focus on the accuracy of final answers, often overlooking the crucial logical rigor for mathematical problem solving.The claim that state-of-the-art LLMs can solve Math Olympiad-level problems requires closer examination.To explore this, we conducted both qualitative and quantitative human evaluations of proofs generated by LLMs, and developed a schema for automatically assessing their reasoning capabilities.Our study reveals that current LLMs fall significantly short of solving challenging Olympiad-level problems and frequently fail to distinguish correct mathematical reasoning from clearly flawed solutions.Our analyses demonstrate that the occasional correct final answers provided by LLMs often result from pattern recognition or heuristic shortcuts rather than genuine mathematical reasoning.These findings underscore the substantial gap between LLM performance and human expertise in advanced mathematical reasoning and highlight the importance of developing benchmarks that prioritize the soundness of the reasoning used to arrive at an answer rather than the mere correctness of the final answers.</p>
<p>Introduction</p>
<p>The release of OpenAI's o1 model [36] marks a significant breakthrough in artificial intelligence research, particularly in the domains of reasoning and problem solving.Building on this achievement, several state-of-the-art models have been introduced [10,38,17], which incorporate post-training on chain-of-thought data.These models have been claimed to demonstrate enhanced reasoning abilities in solving mathematical problems.Although the development of reasoning models remains an active area of research, post-training techniques are believed to be pivotal for improving performance on tasks requiring planning, iterative thinking, and trial-and-error strategies.It has been suggested that generating reasoning tokens before producing a final answer improves the reliability of solutions produced by these models for complex reasoning tasks, e.g., for mathematical problem solving.</p>
<p>A range of different benchmarks, such as GSM8K [7] and MATH [20], have been used to assess the mathematical reasoning abilities of large language models (LLM).With these models achieving competitive performance on such benchmarks, for example, Qwen2.5-72B is reported to reach 91. 5% accuracy on the GSM8K benchmark and 62.1% on the MATH benchmark, and OpenAI O1 94.8% on the MATH benchmark, there have been efforts aimed at introducing more challenging benchmarks.These include OlympiadBench [19], OlympicArena [22], CHAMP [32], AlphaGeometry [42], MathOdyssey [14], and Omni-MATH [16], which contain mathematical problems at the contest level designed to challenge the mathematical problem solving capabilities of LLMs.</p>
<p>Except for CHAMP, which includes "concepts, general math facts, and hints" [32] as additional annotations, and AlphaGeometry, which contains problems and solutions expressed in mathematical language, all other benchmarks use only the correctness of the symbolic or numeric answer correctness, as the evaluation metric to assess the reasoning capabilities of various language models.Because they do not adequately assess the correctness of the rationale or reasoning used to produce the answer, they fail to assess the reasoning capabilities of the LLMs being evaluated.Thus, it is possible for models to use flawed heuristics or erroneous reasoning to score well on the assessment as long as they happen to somehow arrive at the correct final answer.In the case of widely used benchmarks, such as GSM8K and MATH, which consist mostly of simpler problems, where arriving at the correct answer is indistinguishable from coming up with a provably correct solution.However, this is not the case with benchmarks like CHAMP, OlympiadBench, Omni-MATH, MathOdyssey, and OlympicArena, which include contest-level problems where it is sometimes possible to guess a correct answer without being able to offer a sound rationale (or proof) that the answer is correct.This chasm between producing a correct answer as opposed to producing a correct answer backed up by a sound rationale to justify the anser becomes particularly crucial when the LLM is presented with open-ended questions.</p>
<p>Due to the cost and labor involved in assessing the accuracy of the reasoning used by LLM in solving mathematical problems, there is growing interest in LLM-as-a-Judge, a technique to use an LLM to automatically assess the quality of LLM-generated responses [26,27,8,1,29,31].We note that direct evaluation of LLM-generated solutions, as in [12], can be rather complex for challenging math problems, due to the effort involved in verifying the correctness of an LLM-generated solution and the rationale used to justify it.The application of consistent rubrics for automated scoring of LLM-generated solutions is also nontrivial because of the variability of the relative importance of different facts across problems and their solutions.In the case of math Olympiad problems, we find that some mistakes show a lack of understanding of some fundamental concepts, whereas others may be more superficial and amenable to being rectified with minor adjustments.There are no clear and objective criteria to distinguish between these two categories of errors.[39] proposes a rubric-based approach in which a reference solution and specified evaluation criteria are used to assess the quality of a generated solution.However, this approach faces a serious limitation: There may be correct solutions that may not align with the given reference solution.Given these limitations, our goals in this work are twofold: first, to rigorously assess the quality of LLM-generated solutions to mathematics Olympiad problems, including identifying their common failure modes and error types; and second, to investigate whether LLMs can verify the correctness of LLM-generated solutions.This study is significant for two main reasons.First, evaluating the correctness of the solutions generated by LLMs can reveal whether training strategies that rely solely on the correctness of the final answer (as opposed to the correctness of the reasoning used to justify the answer) are sufficient to guide LLMs toward producing correct solutions backed up by sound reasoning.Second, evaluating the ability of LLMs to verify their own generated solutions can provide insights into the feasibility of bootstrapping verification processes, potentially enabling LLMs to improve the quality of their solutions.</p>
<p>Toward these goals, we assembled a group of evaluators to assess the quality of LLM-generated solutions for problems from the International Mathematics Olympiad (IMO) shortlist.After carefully analyzing solutions generated by frontier models such as o1, o1-mini, o3-mini, Gemini 2.0 (Flash Thinking mode) and DeepSeek R11 , we observed that only a negligible fraction of LLM-generated solutions were correct.We found that LLM-generated solutions frequently contained common types of fallacies that invalidated their reasoning.We grouped the observed fallacies into several categories.We systematically evaluated the solutions generated by the frontier models with respect to their correctness.Furthermore, we labeled each incorrect solution according to the type of fallacy they contained.We also evaluated the ability of the frontier models to verify LLM-generated solutions.The results of our analyses show that, in nearly all cases, the models failed to distinguish between correct solutions and incorrect ones containing fallacious reasoning steps.In light of the fact that identifying the presence of fallacies in reasoning is a simpler task compared to full verification of the solution, we conclude that LLMs are incapable of verifying the correctness of LLM-generated solutions.In summary, our contributions are as follows.</p>
<p>• We conducted an extensive evaluation of solutions generated by frontier models on 455 IMO shortlist problems, with emphasis on the correctness of reasoning used to arrive at the answer, in addition to the correctness of the answer itself.</p>
<p>and categorized common logical fallacies present in LLM-generated solutions, offering a comprehensive framework for classifying typical errors.</p>
<p>• Based on the insights offered by the preceding findings, we created a labeled data set by annotating each solution according to the correctness and the type(s) of fallacy or fallacies it contains.This data set supports tasks related to solution verification and offers valuable insights into the current capabilities and limitations of the frontier LLMs.</p>
<p>• Our analysis reveals that even advanced models frequently struggle to distinguish between valid solutions and those containing evident logical fallacies.</p>
<p>Related Work</p>
<p>Benchmarks: Several benchmarks have been used to assess the mathematical reasoning capabilities of large language models (LLMs) [2].Some benchmarks focus purely on arithmetic problems [50], while data sets for math word problems (MWP), such as GSM8K [7] and MathQA [3], present natural language scenarios requiring logical reasoning [45].Automated theorem proof (ATP) data sets evaluate models' capabilities in logical theorem proving [53,49,23].Recent benchmarks focus on advanced or Olympiad-level mathematics, such as CONIC10K for conic sections [46], GHOSTS and miniGHOSTS for graduate-level mathematics [15], and CHAMP [32], OlympiadBench [19], MathOdyssey [14], and Omni-MATH [16], specifically focusing on competition-level problems.HARP [51] provides human-annotated US competition problems, and NuminaMath offers a large-scale collection of math problems and solutions [28].</p>
<p>LLM-as-a-judge: Some have advocated the use of LLMs as judges to assess the mathematical reasoning abilities of LLMs, to minimize the need for expensive human annotations [40,27,34,35].This paradigm offers adaptable evaluations based on task-specific contexts [41,11], and its effectiveness is typically measured against human judgments [24,48,30].Benchmarks such as UltraFeedback [9], AlpacaEval [13], Chatbot Arena [6], and MT-Bench [54] evaluate different LLM judging domains.Specifically for mathematical reasoning, REASONEVAL [47] assesses the correctness of the answer and of the reasoning used to arrive at the answer, while MATHCHECK [55] uses LLM to assess the performance of LLM in a variety of mathematical problems.The SMART-840 dataset [5] benchmarks zero-shot mathematical reasoning based on human performance statistics.</p>
<p>Mathematical Reasoning in LLMs: Large language models (LLMs) have shown success in various reasoning tasks, especially when employing prompting techniques like Chain-of-Thought (CoT), which encourages them to generate correct intermediate steps toward a solution [4,45,25].These methods can significantly improve performance on challenging problems [18].Furthermore, inference-time techniques such as CoT with Self-Consistency (CoT-SC) have been developed to improve reasoning by generating multiple reasoning paths and selecting the most consistent one [44,43].Benchmarks such as MATH [20], GSM-Symbolic, and GSM-NoOp [33] have been introduced to provide more controllable evaluations and reveal limitations such as sensitivity to numerical variations and irrelevant information, suggesting a potential lack of deep understanding of mathematical concepts.These benchmarks show that current LLMs rely more on probabilistic pattern matching than genuine formal logical reasoning.To further refine LLMs' reasoning, approaches like reward modeling to evaluate solution correctness and self-refinement techniques [21] and decomposing problems into smaller algorithmic steps [52] are being explored.</p>
<p>General Workflow</p>
<p>In this section, we outline the data collection process.In the first phase, we selected a set of challenging problems to evaluate the quality of solutions generated by the LLMs.We gathered a group of seven evaluators, each either a former national-level Olympiad medalist or holding or doing a relevant Ph.D. in fields like mathematics or computer science, and asked them to analyze the correctness of the LLM-generated solutions.</p>
<p>Problem Selection Rationale</p>
<p>Through this project, we used IMO shortlist problems.The process of selecting problems for the IMO Shortlist is rigorous and carefully coordinated.Each participating country submits a set of candidate problems that typically span the four main mathematical fields: algebra, geometry, combinatorics, and number theory.These submissions are reviewed by a problem selection committee to ensure that they meet key criteria, including originality, mathematical depth, and suitability for the competition.The committee carefully evaluates the problems for their difficulty level, ensuring a balance between accessibility for less experienced participants and sufficient challenge for the most advanced contestants.From this review process, a shortlist is created that contains a diverse collection of high-quality problems.This shortlist forms the basis for the final selection of problems used in the IMO.</p>
<p>The shortlist problems have some distinct features that make them suitable for testing the mathematical reasoning capabilities of the frontier models:</p>
<p>• Shortlist problems are highly original, even within the context of contest-level problems.The selection committee ensures that the solution ideas for these problems are as novel and unique as possible.As a result, while attempting to generate solutions, an LLM cannot simply combine standard building blocks from well-known problems to arrive at the correct answer.</p>
<p>• It is almost always known that the problem can be reduced to a well-established result in advanced mathematics, such as undergraduate-or graduate-level topics or research-level findings.Consequently, an LLM cannot leverage its extensive knowledge to apply an advanced mathematical result for an easy solution.</p>
<p>• All problems are designed to be solvable using high-school-level mathematics, and the challenge lies in the intricacy of the ideas rather than in requiring a background in advanced mathematics.</p>
<p>• The solutions typically involve multiple steps, each requiring nuanced arguments.Solving these problems requires careful planning, systematic thinking, and rigorous verification of each step, in contrast to simpler mathematical problems that can be tackled through straightforward algebraic manipulations or trial-and-error.</p>
<p>On the other hand, since the IMO and IMO Shortlist problems are highly reputable, there is a significant likelihood of data leakage, as frontier models may have been trained on publicly available high-quality mathematical datasets.In this paper, we demonstrate that even if such data leakage has occurred, it does not substantially affect the ability of the LLM to solve IMO Shortlist-level problems.</p>
<p>Classifying the Failure Modes of LLM Solutions</p>
<p>We asked the evaluators to present several IMO shortlist and shortlist-level problems to frontier models, including OpenAI o1, o1-mini, o3-mini, DeepSeek R1 and Gemini 2, and qualitatively analyze the details of the LLM-generated arguments.We found that when these frontier models generate incorrect solutions, errors consistently follow common patterns.Specifically, incorrect solutions typically involve blatantly inaccurate mathematical arguments or statements.</p>
<p>After thorough analysis, we identified the following types of reasoning fallacies in incorrect LLMgenerated solutions.Demonstrative and real examples of each type of reasoning fallacy are provided in the appendix.</p>
<p>Proof by Example.Drawing a general conclusion based on a limited number of specific instances without rigorous justification for all cases.This error occurs when a mathematical claim appears to hold in a few examples, misleadingly suggesting that it is universally true when, in fact, it is not.</p>
<p>Proposal Without Verification.Introducing a method or strategy without properly justifying its correctness.The model proposes an idea but provides no rigorous argument or proof supporting its validity.</p>
<p>Inventing Wrong Facts.Citing or inventing non-existent theorems, definitions, or facts to justify a claim.Instead of relying on established mathematical facts, the argument relies on fabricated statements (hallucination).</p>
<p>Begging the Question (Circular Reasoning).Assuming the conclusion it that needs to be proved, instead of providing evidence for the claim.</p>
<p>Solution by Trial-and-Error.Offering solutions derived solely from guesswork or testing a few random examples without providing a reason as to why selected solutions work or why alternatives are not considered.</p>
<p>Calculation Mistakes.Committing substantial arithmetic or algebraic errors that undermine the overall correctness of the solution.We specifically considered calculation errors severe enough to compromise the validity of the conclusion.</p>
<p>Data Annotation Processs</p>
<p>After defining the fallacy categories, we provided a list of IMO shortlist problems and the corresponding model-generated solutions to the evaluators, instructing them to classify these solutions based on their correctness.We used the following checklist to annotate our data:</p>
<ol>
<li>First, the evaluator read the solution and determined whether it was correct, partially correct, or incorrect.</li>
</ol>
<p>• A solution was considered correct if it fully addressed all aspects of the problem and contained no significant errors in statements or conclusions.• A solution was deemed partially correct if it included some essential steps of a correct solution but omitted other crucial steps or contained significant inaccuracies.• A solution was classified as incorrect if it lacked any useful non-trivial information relevant to solving the problem.</p>
<ol>
<li>
<p>If the solution was not correct, the evaluator identified the fallacy (or fallacies) that occur in the solution.</p>
</li>
<li>
<p>If the problem required a final answer, the evaluator recorded both the correct final answer and the final answer generated by the model.</p>
</li>
</ol>
<p>To ensure consistency, the evaluators' team lead conducted a thorough review of the evaluators' outputs in parallel, verifying that the definitions of fallacies were applied correctly and consistently across all evaluators.Borderline cases were identified and discussed separately.</p>
<p>Human Evaluation Results</p>
<p>We evaluated the models using the sets of IMO shortlist problem from 2009 to 2023, comprising a total of 455 problems.These included 108 algebra, 117 combinatorics, 116 geometry, and 114 number theory problems.The number of problems per annual shortlist varied slightly, typically ranging from 26 to 35.Each set was carefully curated to maintain a balanced distribution of difficulty on the four primary mathematical topics.The performance of each model on the IMO shortlist problems is summarized in Table 1.As evident from the results, none of these models achieve performance levels comparable to those obtained by calculating the accuracy of the final answers, as reported in [16] and [14].The observed gap between the outcomes of our evaluation and other methods that focus exclusively on the correctness of the final can be explained by the fact that it is possible for a model to get the final answer right based on fallacious reasoning steps.To investigate this issue, we specifically examined problems with concrete final answers within our evaluation set.These results suggest that the models use heuristics, shortcuts, and educated guesses that often turn out to be misguided instead of using sound reasoning.Consequently, evaluation of such models solely on the basis of the correctness of the final answer tends to grossly overestimate their reasoning abilities.Hence, we argue that evaluating the performance of LLMs on mathematical reasoning tasks based solely on the correctness of the final answer without checking the soundness of the rationale used to justify the answer is fundamentally flawed.It is also helpful to examine how the reliance of common reasoning fallacies varies across the different models.Figure 2 shows the relative frequency of occurrence of each type of fallacy in the LLMgenerated solutions produced by each of the models.Note that some solutions can include multiple types of fallacies.</p>
<p>We observe that among the different types of fallacies, Inventing Wrong Facts is the most frequent in four of the models and the second most frequent in a fifth model.We conjecture that this observation can be explained by the fact that these models are trained with reinforcement learning using a reward function based on the correctness of the final answers (e.g., see the DeepSeek report; [10]).</p>
<p>The results show that Proposal Without Verification is the second most frequently observed reasoning fallacy.Although the internal reasoning tokens are not visible for OpenAI models, an examination of the "thinking traces" from DeepSeek and Gemini suggests that this fallacy often arises because the model struggles to decide which calculations and statements to include in its response.Consequently, instead of presenting well-reasoned rationale or useful intermediate steps, the model may produce vague claims, often beginning with phrases like "It is easy to show that..." or "One can show that..." without justification or supporting arguments.</p>
<p>We also observed that models tend to exhibit different types of fallacious reasoning depending on the type of problem presented to them. Figure 3 illustrates the relative frequencies of various fallacies depending on whether or not the solutions produced contain final answers.Notably, Proof by Example and Solution by Trial and Error occur more frequently in solutions that include a final answer.This suggests that models often arrive at final answers through either heuristic trial and error or generalization from a small number of test cases, leading to a higher prevalence of these two types of fallacy in the solutions produced.In contrast, we observe that Inventing Wrong Facts and Proposal Without Verification occur more frequently in solutions of problems that lack explicit final answers.</p>
<p>To correctly solve such problems, the models must logically connect the problem's initial assumptions and constraints to the conclusion through a sequence of sound reasoning steps.However, we find that models frequently substitute sound reasoning with unjustified and often incorrect statements.We also observe differences in the relative frequencies of the fallacies in problems drawn from different topics in mathematics.Figure 4 demonstrates the relative frequencies of reasoning fallacies among geometry, algebra, combinatorics, and number theory problems.As more geometry problems can be solved only using logical statements rather than algebraic manipulations, Inventing Wrong Facts, Proposal Without Verification, and Begging the Question are more common in geometry problems.A significant proportion of algebra problems fall into categories such as functional equations, polynomial equations, or optimization tasks.We observed that all frontier models tend to avoid generating rigorous analytic solutions, instead relying on trial and  To investigate the first question, we collected all the problems from our evaluation data set for which the LLM-generated solutions.producedsolutions were incorrect.The correct solutions for each of these problems were obtained from the Art of Problem Solving (AoPS) website2 .We then asked the LLMs to analyze each solution and explicitly request a final judgment of either correct or incorrect.Although this verification task can be complicated due to solutions that may partially satisfy correctness criteria, the solution pairs we chose include a correct solution and an incorrect solution that includes an obvious fallacy.The detailed prompts used in this section can be found in the appendix.</p>
<p>As shown in Table 3, DeepSeek and Gemini 2.0 produce responses of correct and wrong with nearly equal frequency for both genuinely correct solutions and incorrect LLM-generated solutions.Interestingly, the likelihood of identifying a truly correct solution as correct is even lower for the models o1, o1-mini, and o3-mini.These results demonstrate that the models are not suitable for use as judges, as they cannot reliably distinguish genuine solutions from obviously incorrect ones.</p>
<p>To investigate the second question, we applied a similar methodology.For each problem, we presented the models with pairs consisting of a correct solution and an incorrect LLM-generated solution.We then asked the models to identify the correct solution after careful analysis, explicitly informing them that exactly one solution was correct and the other incorrect.As shown by results in in Table 4, models o1, o1-mini and Gemini 2.0 perform at or below random to distinguish correct from incorrect solutions.Only DeepSeek and o3-mini perform modestly better than chance, outperforming random selection by 7% and 2%, respectively.These results indicate that the evaluated models currently have limited effectiveness as verifiers for challenging tasks such as IMO shortlist-level problems.</p>
<p>Model</p>
<p>Conclusion</p>
<p>Our evaluation of frontier LLMs on Olympiad-level mathematics revealed significant shortcomings in their ability to produce logically rigorous proofs and engage in genuine mathematical reasoning.Models such as OpenAI's o1, o1-mini, o3-mini, DeepSeek R1, indicate a reliance on heuristic shortcuts rather than authentic reasoning processes.Additionally, these LLMs demonstrated limited capability in effectively verifying solutions, performing at or near random levels when distinguishing correct proofs from clearly incorrect ones.</p>
<p>These findings emphasize two critical areas for improvement.First, there is a clear need to develop more sophisticated benchmarks and evaluation methods to assess the correctness of reasoning and not merely the correctness of final answers.Second, relying solely on the correctness of final answers or utilizing more powerful LLMs as judges is inadequate for assessing the reasoning abilities of LLMs; improved training schemas specifically designed to address the logical rigor of solutions are essential for advancing the reasoning abilities of LLMs.</p>
<p>Example: Proof by Example</p>
<p>Problem: Prove that 3 2n − 1 is divisible by 8 for all integers n ≥ 1.</p>
<p>Fallacious Solution:</p>
<p>To "prove" this statement, we can test the initial cases:</p>
<p>For n = 1, 3 2 − 1 = 8, which is divisible by 8.</p>
<p>For n = 2, 3 4 − 1 = 81 − 1 = 80, which is divisible by 8.</p>
<p>For n = 3, 3 6 − 1 = 729 − 1 = 728, which is divisible by 8.</p>
<p>Hence, we "proved" that 3 2n − 1 is divisible by 8 for all integers n ≥ 1.</p>
<p>This argument is not a valid proof by induction, as it lacks the necessary inductive step.Without this inductive step, verifying a few initial cases does not guarantee the statement is true for all n.</p>
<p>While the statement itself happens to be true in this example, the method of proof is fallacious.</p>
<p>Example: Proposal Without Verification:</p>
<p>Problem: Two players, Alice and Bob, take turns choosing a number from the set {1, 2, . . ., n}, such that the sum of the chosen numbers never exceeds 2n.The player unable to make a valid move loses.Prove that for all n ≥ 1, there exists a winning strategy for one of the players.</p>
<p>Fallacious Solution: Alice always has a winning strategy.She should begin by picking the largest available number, and on each subsequent turn, she should pick numbers that ensure Bob cannot reach the limit of 2n.By doing this, she can control the game and guarantee her victory.This approach is clearly optimal because by keeping Bob under pressure, Alice can always force him into a losing position."</p>
<p>While this strategy seems intuitive, the proposer does not provide any proof that it guarantees a win for Alice in all cases.For example, there is no justification that Alice's strategy prevents Bob from achieving the same goal or that Bob has no counter-strategy.Furthermore, no rigorous argument is provided to show that this method works for all values of n.A proper solution would involve analyzing the game mathematically, determining conditions under which Alice or Bob has a winning strategy, and proving these results using tools like invariants or backward induction.Without this reasoning, the proposal remains incomplete and exemplifies the fallacy.</p>
<p>Example: Inventing Wrong Facts</p>
<p>Problem: Prove that the harmonic series
∞ n=1 1 n diverges.
Fallacious Solution: By the Harmonic Square Root Theorem, the partial sum of the harmonic series up to n terms is given by:
H n = 2 √ n + γ + O 1 √ n ,
where γ is the Euler-Mascheroni constant.As n → ∞, the dominant term 2 √ n grows without bound, while the constant γ and error term O 1 √ n become negligible.Thus, lim n→∞ H n = ∞, proving divergence of the harmonic series.</p>
<p>The Harmonic Square Root Theorem is non-existent.While H n ∼ ln(n) + γ is a valid asymptotic approximation, claiming H n ∝ √ n is blatantly false.</p>
<p>Example: Begging the Question Problem: Prove that the number √ 2 is irrational.</p>
<p>Fallacious Solution: Assume √ 2 is irrational.By definition, an irrational number cannot be expressed as a fraction a b , where a and b are integers with b ̸ = 0. Since √ 2 cannot be written as a b , it follows that √ 2 is irrational.</p>
<p>The argument begins by assuming the conclusion "Assume √ 2 is irrational".It then uses the definition of irrationality to "prove" the same claim, creating a logical loop.The conclusion (" √ 2 is irrational") is restated verbatim as the premise, offering no actual proof.</p>
<p>Example: Solution by Trial and Error</p>
<p>Problem: Find all functions f : R → R such that
f (x 2 − y 2 ) = xf (x) − yf (y), ∀x, y ∈ R.</p>
<p>Fallacious Solution:</p>
<p>Let us try some simple forms of f (x): Suppose f (x) = 0. Substituting into the functional equation:
f (x 2 − y 2 ) = 0 and xf (x) − yf (y) = 0.
This satisfies the equation, so f (x) = 0 is a solution.Now suppose f (x) = x.Substituting f (x) = x into the equation:
f (x 2 − y 2 ) = x 2 − y 2 and xf (x) − yf (y) = x 2 − y 2 .
This also satisfies the equation, so f (x) = x is another solution.</p>
<p>Finally, consider f (x) = kx for some constant k.Substituting:
f (x 2 − y 2 ) = k(x 2 − y 2 ) and xf (x) − yf (y) = k(x 2 − y 2 ).
This works for any k.So the solutions are f (x) = 0 and f (x) = kx for any constant k.</p>
<p>This solution seems to rely on educated guesses to reach the correct answer but doesn't explain why other functions fail to satisfy the given functional equation.</p>
<p>Example: Calculation Mistakes</p>
<p>Problem: Let x, y, z &gt; 0 satisfy x + y + z = 6.Find the maximum value of xyz.</p>
<p>Fallacious Solution:</p>
<p>Using the AM-GM inequality, we know:
x + y + z 3 ≥ 3 √ xyz.
Substituting x + y + z = 6, we get:
6 3 ≥ 3 √ xyz. Simplifying: 2 ≥ 3 √ xyz.
Now, squaring both sides (instead of cubing):
4 ≥ xyz.
Thus, we conclude that xyz ≤ 4.</p>
<p>The critical error lies in Step 4, where the inequality was squared instead of being cubed.Since the inequality involves a cube root 3 √ xyz, the proper operation should have been cubing both sides:</p>
<p>(2) 3 = 8.This mistake leads to the incorrect conclusion xyz ≤ 4, whereas the correct bound is xyz ≤ 27.The solution becomes invalid because squaring distorts the inequality, making the result overly restrictive.</p>
<p>A.2 Real Examples for Common LLM Fallacies</p>
<p>Example: Proof by Example Problem: On a flat plane in Camelot, King Arthur builds a labyrinth L consisting of n walls, each of which is an infinite straight line.No two walls are parallel, and no three walls have a common point.Merlin then paints one side of each wall entirely red and the other side entirely blue.At the intersection of two walls there are four corners: two diagonally opposite corners where a red side and a blue side meet, one corner where two red sides meet, and one corner where two blue sides meet.At each such intersection, there is a two-way door connecting the two diagonally opposite corners at which sides of different colours meet.After Merlin paints the walls, Morgana then places some knights in the labyrinth.The knights can walk through doors, but cannot walk through walls.Let k(L) be the largest number k such that, no matter how Merlin paints the labyrinth L, Morgana can always place at least k knights such that no two of them can ever meet.For each n, what are all possible values for k(L), where L is a labyrinth with n walls?</p>
<p>Fallacious Solution:</p>
<p>For each integer n ≥ 1, the labyrinth L constructed by King Arthur consists of n non-parallel, infinitely extending walls with no three walls intersecting at a single point.Merlin paints each wall with one side red and the other blue.The intersections of these walls create doors that connect specific regions within the labyrinth.</p>
<p>Understanding k(L):</p>
<p>The value k(L) represents the largest number k such that, regardless of how Merlin paints the labyrinth, Morgana can always place at least k knights in the labyrinth where no two knights can meet through the doors.</p>
<p>Analyzing Small Cases: 1.For n = 1:</p>
<p>• There is only one wall, dividing the plane into two regions.</p>
<p>• Since there are no intersections, there are no doors.</p>
<p>• Morgana can place one knight in each region, so k(L) = 2.</p>
<p>For n = 2:</p>
<p>• Two intersecting walls create four regions.</p>
<p>• There is one door connecting two diagonally opposite regions.</p>
<p>• Morgana can place three knights such that no two are connected by the door, so k(L) = 3.</p>
<p>For n = 3:</p>
<p>• Three walls intersecting pairwise create seven regions.</p>
<p>• There are three doors, each connecting two regions.</p>
<p>• Morgana can ensure that four knights are placed without any two being connected by a door, so k(L) = 4.</p>
<p>Generalizing the Pattern:</p>
<p>Observing the pattern from the small cases, we see that:
• For each n, k(L) = n + 1.
This means that no matter how Merlin paints the labyrinth, Morgana can always place n + 1 knights such that no two knights can meet through the doors.This result stems from the combinatorial structure of the labyrinth and the way doors are created at intersections.</p>
<p>Conclusion:</p>
<p>For any number of walls n, the largest guaranteed number of knights k(L) that Morgana can place without any two being able to meet is always n + 1.</p>
<p>Answer:</p>
<p>For every number n of walls, the maximal k is n + 1.Thus, each k(L) equals n + 1.</p>
<p>The proof claims that k(L) = n + 1 by testing it for small ns and then it generalizes this claim for all natural ns without justification.</p>
<p>Example: Proposal Without Verification</p>
<p>Problem: A social network has 2019 users, some pairs of whom are friends.Whenever user A is friends with user B, user B is also friends with user A. Events of the following kind may happen repeatedly, one at a time: Three users A, B, and C such that A is friends with both B and C, but B and C are not friends, change their friendship statuses such that B and C are now friends, but A is no longer friends with B, and no longer friends with C. All other friendship statuses are unchanged.Initially, 1010 users have 1009 friends each, and 1009 users have 1010 friends each.Prove that there exists a sequence of such events after which each user is friends with at most one other user.</p>
<p>Fallacious Solution:</p>
<p>To solve this problem, we need to demonstrate that it's possible to transform the given social network into a state where each user is friends with at most one other user using the specified operations.</p>
<p>Initial Setup</p>
<p>• Number of Users: 2019</p>
<p>• Friendship Structure:</p>
<p>-1010 users have 1009 friends each.</p>
<p>-1009 users have 1010 friends each.</p>
<p>• Total Friendships Initially:
1010 × 1009 + 1009 × 1010 = 2 × 1010 × 1009
This counts each friendship twice (once for each user in the pair), so the actual number of unique friendships is: Total Edges = 1010 × 1009</p>
<p>Objective</p>
<p>Transform the network so that each user has at most one friend.In graph theory terms, we aim to reach a state where the graph is a matching, meaning it consists of disjoint edges with no overlaps.</p>
<p>Operation Details</p>
<p>The operation allows us to:</p>
<p>• Select Three Users: A, B, C such that:</p>
<p>-A is friends with both B and C.</p>
<p>-B and C are not friends.</p>
<p>• Modify Friendships:</p>
<p>-Add: Friendship between B and C.</p>
<p>-Remove: Friendships between A and B, and between A and C.</p>
<p>Effect of Each Operation</p>
<p>• Total Friendships Decrease: Each operation removes two friendships and adds one, resulting in a net decrease of one friendship.</p>
<p>• Degree Changes:</p>
<p>-A's degree decreases by 2.</p>
<p>-B and C's degrees remain unchanged since one friendship is removed and another is added.</p>
<p>Strategy to Achieve the Objective</p>
<p>• Reducing Degrees:</p>
<p>-Repeatedly apply the operation to reduce the degrees of users with more than one friend.</p>
<p>-Each operation ensures that a user's degree decreases, moving towards the desired state.</p>
<p>• Termination Condition:</p>
<p>-The process continues until no user has more than one friend.</p>
<p>-If a user still has more than one friend, the operation can still be applied because there will always be at least two friends who aren't connected (given the initial high degree and the specific structure of operations).</p>
<p>• Final State:</p>
<p>-The network will consist of disjoint pairs of friends (a matching) and possibly some users with no friends.</p>
<p>-This ensures that no user has more than one friend.</p>
<p>Conclusion</p>
<p>Through systematic application of the specified operation, we can decrementally reduce the degrees of all users with more than one friend.Given the initial conditions and the nature of the operation, it's guaranteed that we can reach a state where each user has at most one friend.</p>
<p>Final Answer: After performing suitable operations, the network can be transformed so no user has more than one friend.Thus, such a sequence exists.</p>
<p>As we can see, the model has proposed a sequence but doesn't do any mathematical verification of its correctness.</p>
<p>Example: Inventing Wrong Facts</p>
<p>Problem: You are given a set of n blocks, each weighing at least 1; their total weight is 2n.</p>
<p>Prove that for every real number r with 0 ≤ r ≤ 2n − 2 you can choose a subset of the blocks whose total weight is at least r but at most r + 2</p>
<p>Fallacious Solution:</p>
<p>To prove that for any real number r with 0 ≤ r ≤ 2n − 2, there exists a subset of blocks whose total weight is at least r but at most r + 2, we can proceed as follows: Key Observations:</p>
<ol>
<li>Block Weights and Total Weight:</li>
</ol>
<p>• Each block has a weight of at least 1.</p>
<p>• The total weight of all n blocks is 2n.</p>
<p>Subset Sums:</p>
<p>• The possible subset sums range from 0 (empty set) to 2n (all blocks).</p>
<p>• Since each block has a weight of at least 1, any two distinct subset sums differ by at least 1.</p>
<p>3.</p>
<p>Covering the Interval [0, 2n − 2]:</p>
<p>• We need to ensure that for any r in [0, 2n − 2], there exists a subset sum s such that r ≤ s ≤ r + 2.</p>
<p>Proof Strategy: Sequential Coverage</p>
<p>• List all possible subset sums in increasing order: s 0 = 0, s 1 , s 2 , . . ., s m = 2n.</p>
<p>• Each subsequent sum satisfies s i+1 − s i ≥ 1 because each block has a weight of at least 1.</p>
<p>Interval Overlap</p>
<p>• Consider intervals of the form [s i , s i + 2] for each subset sum s i .</p>
<p>• Since s i+1 ≥ s i + 1, the next interval [s i+1 , s i+1 + 2] starts no more than 1 unit after the start of the previous interval.</p>
<p>• Therefore, the union of these intervals [s i , s i + 2] covers the entire range [0, 2n] without gaps larger than 2.</p>
<p>Ensuring Coverage for Any r</p>
<p>• For any r in [0, 2n − 2], there exists some subset sum s i such that s i ≤ r ≤ s i + 2.</p>
<p>• This guarantees that r lies within the interval [s i , s i + 2], meaning there exists a subset with a sum in [r, r + 2].</p>
<p>Conclusion By sequentially covering the range [0, 2n] with overlapping intervals of length 2 derived from the subset sums, we ensure that for any r within the specified range, there exists a suitable subset whose total weight satisfies r ≤ sum(S) ≤ r + 2.</p>
<p>Answer By ordering all subset sums and noting that each differs by at least 1, the intervals [s, s + 2] cover [0, 2n − 2].Thus, for any r between 0 and 2n − 2, some subset has total weight at least r and at most r + 2.</p>
<p>The model invents a wrong fact/observation and builds its argument around it.The main fact that is used for the proof is obviously not true.Consider {1, 1.5, 2, 3.5}.</p>
<p>Example: Begging the Question</p>
<p>Problem: A polynomial P (x, y, z) in three variables with real coefficients satisfies the identities P (x, y, z) = P (x, y, xy − z) = P (x, zx − y, z) = P (yz − x, y, z).</p>
<p>Prove that there exists a polynomial F (t) in one variable such that
P (x, y, z) = F (x 2 + y 2 + z 2 − xyz).</p>
<p>Fallacious Solution:</p>
<p>To prove that there exists a polynomial F (t) in one variable such that
P (x, y, z) = F (x 2 + y 2 + z 2 − xyz),
we analyze the given conditions and the structure of the polynomial P (x, y, z).</p>
<p>Step 1: Define the Invariant Let's define the invariant:
t = x 2 + y 2 + z 2 − xyz.
Our goal is to show that t remains unchanged under the transformations provided in the problem statement.</p>
<p>Step 2: Verify Invariance Under Given Transformations We need to verify that t is invariant under the following transformations:</p>
<p>The generated solution begs the question.The fact P (x, y, z) = F (x 2 + y 2 + z 2 − xyz). is the problem statement itself, and it needs to be proven.</p>
<p>Example: Solution by Trial and Error</p>
<p>Problem: Let Z be the set of integers.Determine all functions f : Z → Z such that, for all integers a and b, f
2a) + 2f (b) = f (f (a + b)).(
Proposed by Liam Baker, South Africa</p>
<p>Fallacious Solution:</p>
<p>To determine all functions f : Z → Z that satisfy the functional equation
f (2a) + 2f (b) = f (f (a + b))
for all integers a and b, let's explore possible forms of f .</p>
<p>Step 1: Assume a Linear Function First, consider that f might be a linear function of the form:
f (n) = kn + c,
where k and c are constants in Z.</p>
<p>Substituting into the functional equation:
f (2a) + 2f (b) = k(2a) + c + 2(kb + c) = 2ka + c + 2kb + 2c = 2k(a + b) + 3c.
On the other hand:
f (f (a + b)) = f (k(a + b) + c) = k(k(a + b) + c) + c = k 2 (a + b) + kc + c.
Setting the two expressions equal gives:
2k(a + b) + 3c = k 2 (a + b) + kc + c.
For this to hold for all integers a and b, the coefficients must match: 2k = k 2 and 3c = kc + c.</p>
<p>Solving the first equation:
k 2 − 2k = 0 =⇒ k(k − 2) = 0 =⇒ k = 0 or k = 2. Case 1: k = 0 3c = c =⇒ 2c = 0 =⇒ c = 0. So, f (n) = 0 is a solution.
Case 2: k = 2 3c = 3c (always true).</p>
<p>Here, c can be any integer.Thus, f (n) = 2n + c is a solution for any integer c.</p>
<p>Step 2: Verify Non-Linear Solutions Assuming f is not linear leads to more complex conditions that are difficult to satisfy uniformly across all integers.Hence, no non-linear solutions appear to exist beyond the linear forms already identified.Conclusion All solutions to the functional equation are either the constant zero function or affine linear functions of the form f (n) = 2n + c, where c is an integer constant.</p>
<p>Final Answer: All integer functions equal to twice n plus a fixed constant.In other words, every f with f (n) = 2n + c for some integer c.</p>
<p>The generated proof only tests the problem's equation for a special class and concludes the result.This is an example of a solution by trial and error fallacy.</p>
<p>Example: Calculation Mistakes</p>
<p>Problem: Determine all integers n ≥ 2 having the following property: for any integers a 1 , a 2 , . . ., a n whose sum is not divisible by n, there exists an index 1 ≤ i ≤ n such that none of the numbers a i , a i + a i+1 , . . ., a i + a i+1 + . . .+ a i+n−1 is divisible by n.Here, we let a i = a i−n when i &gt; n.</p>
<p>Fallacious Solution:</p>
<p>We will show that among the integers n ≥ 2, the property "For any integers a 1 , a 2 , . . ., a n whose sum is not divisible by n, there is some index 1 ≤ i ≤ n so that none of the numbers a i , a i + a i+1 , . . ., a i + a i+1 + • • • + a i+n−1 is divisible by n" holds if and only if n = 2.In other words, if n ≥ 3 then one may construct a counterexample sequence, while for n = 2 the property always holds.In what follows, we describe a method to translate the condition into one involving a cyclic sequence of residues and then show that counterexamples exist for every n ≥ 3, but not for n = 2.</p>
<p>Reformulating the Problem</p>
<p>For a given sequence a 1 , . . ., a n , define the cumulative sums
S k = a 1 + a 2 + • • • + a k for k = 1, . . . , n, S 0 = 0.
All congruences below are modulo n.Note that for any index i (with indices taken cyclically modulo n, i.e., a j+n = a j ), the partial sums a i , a i + a i+1 , . . ., a i + a i+1 + • • • + a i+n−1 can be written as S i − S i−1 , S i+1 − S i−1 , . . ., S i+n−1 − S i−1 .</p>
<p>A.3 Evaluation Prompts</p>
<p>Prompt: Solution Evaluation Prompt</p>
<p>You are an expert mathematician tasked with rigorously evaluating the correctness of a proposed solution to a mathematical problem, specifically in the style of the International Mathematical Olympiad (IMO).</p>
<p>Below, you will receive:</p>
<p>1.The Problem Statement: A description of the problem to be solved.</p>
<p>The Candidate Solution:</p>
<p>A step-by-step explanation of a proposed solution to the problem.</p>
<p>Your evaluation should include:</p>
<p>• Logical Analysis: Carefully check each step for logical consistency, correctness of mathematical reasoning, and appropriateness of methods used.</p>
<p>• Identification of Errors: Clearly pinpoint and describe any mathematical errors, flawed logic, unjustified assumptions, or missing steps.</p>
<p>• Clarification: Briefly explain why each identified issue is problematic, referring explicitly to mathematical principles or concepts involved.</p>
<p>Provide your evaluation strictly adhering to the following structured response format:</p>
<p>Analysis: {Provide a thorough, step-by-step analysis of the candidate solution.Clearly indicate if and where logical fallacies, incorrect assumptions, or mathematical inaccuracies occur.If the solution is fully correct, justify your reasoning thoroughly.}</p>
<p>Final Answer:</p>
<p>Clearly indicate the result of your analysis using exactly one of the following formats:</p>
<p>Correct or Wrong</p>
<p>Prompt: Solution Comparison Prompt I will present a problem along with two candidate solutions.Your task is to evaluate the solutions and determine which one is correct.Note that one solution is always correct and one solution is always wrong.</p>
<p>The primary criterion for your judgment is the correctness of the arguments provided in each solution.</p>
<p>Please respond using the following template:</p>
<p>Analysis: {Provide your analysis of the solutions here, discussing the strengths and weaknesses of their arguments.}Final Answer:</p>
<p>{ Solution 1 or Solution 2 }</p>
<p>A.4 Additional Notes</p>
<p>To mitigate recency bias, the solutions provided by the LLM and the real solutions were randomly permuted during the comparative analysis experiment.Consistent outcomes were observed across various prompts, indicating robustness in the experimental setup.Furthermore, our investigation revealed that automated rating rubrics, such as the LLM-as-a-judge approach, yielded similar comparative results.Notably, frontier LLMs did not assign significantly higher ratings to correct solutions compared to incorrect ones.Due to constraints in space, the present discussion is intentionally restricted to the binary evaluation scenario.</p>
<p>Figure 1 :
1
Figure 1: LLMs often fail to generate logically sound solutions for challenging problems.</p>
<p>Figure 2 :
2
Figure 2: Relative frequencies of each fallacy among the LLM-generated solutions for each model.</p>
<p>Figure 3 :
3
Figure 3: Relative frequencies of each fallacy in LLMgenerated solutions, comparing questions with and without a final answer.</p>
<p>Figure 4 :
4
Figure 4: Relative frequencies of each fallacy in LLM-generated solutions among different topics</p>
<p>Table 1 :
1
Performance of different models on IMO shortlist problems (%)
ModelCorrect (%) Partially Correct (%) Incorrect (%)DeepSeek3.86.789.4Gemini 2.00.01.198.9o11.93.994.2o1-mini0.00.0100.0o3-mini3.34.492.2</p>
<p>Table 2
2ModelFinal Answer Accuracy (%) Correct|Correct Final Answer (%)DeepSeek63.2%0%Gemini 2.043.8%0%o130.8%12.5%o1-mini35.0%0%o3-mini48.3%14.3%
illustrates both the proportion of correct final answers and the conditional probability of having a correct solution, given that the final answer is correct.Interestingly, we found that the frontier models often generate incorrect solutions, that is, solutions that contain reasoning errors despite getting the final answer right.</p>
<p>Table 2 :
2
Comparison of evaluated LLMs highlighting the gap between final answer correctness and overall solution quality.Final Answer Accuracy denotes the percentage of correct final answers, whereas Correct|Correct Final Answer represents the percentage of fully correct solutions among instances where the final answer is correct.</p>
<p>Table 3 :
3
Percentage of correct and incorrect solutions identified as correct by different LLMs during verification.The results illustrate the LLMs' difficulty in accurately distinguishing genuinely correct solutions from clearly incorrect ones containing explicit fallacies.each problem, do LLMs accurately choose the correct solution?
ModelReal Solutions Correct (%) Wrong Solutions Correct (%)DeepSeek4843Gemini 2.05250o13139o1-mini3645o3-mini2631</p>
<p>Table 4 :
4
Accuracy of various LLMs in identifying the correct solution when presented with pairs consisting of one correct solution and one incorrect solution generated by an LLM.
Accuracy (%)DeepSeek57Gemini 2.049o150o1-mini46o3-mini52
At the time we began this project, reasoner models such as Claude 3.7, o3-mini, Gemini
.5 and Grok
had not yet been released; Among these, we were only able to include o3-mini in our evaluations.
Automatic Evaluation ResultsVerification of a candidate solution to a problem is generally considered an easier task than solving the problem itself. Consequently, a common strategy for training LLMs for reasoning is based on a generator-verifier schema. Within this schema, a generator produces candidate solutions and a verifier evaluates these candidates and serves as a reward model. The verifier can be hard coded (for example, check only the correctness of final answers), trained to evaluate solutions produced by LLMs, or a powerful LLM acting as a judge[37]. A pertinent question in this context is whether state-of-the-art models can reliably distinguish incorrect solutions from correct solutions. We approach this question through two complementary analyses.1. Do LLMs label correct solutions as correct more frequently than incorrect ones, e.g., those that contain one or more reasoning fallacies?2. When presented with pairs of solutions consisting of a correct and an incorrect solution for
https://artofproblemsolving.com/community/c3223_imo_shortlist
A AppendixA.1 Illustrative Examples for Common LLM FallaciesIn this section, we present illustrative examples and comprehensive explanations for each fallacy, facilitating clearer interpretation and deeper conceptual understanding.Expanding this:Thus, t remains unchanged.Second TransformationExpanding this:Again, t remains unchanged.Third TransformationP (x, y, z) = P (yz − x, y, z).Compute t(yz − x, y, z):Expanding this:Once more, t remains unchanged.Step 3: Conclude the Existence of F (t) Since t = x 2 + y 2 + z 2 − xyz is invariant under all the given transformations, and P (x, y, z) remains unchanged under these transformations, it follows that P depends only on t.Therefore, there exists a polynomial F (t) such that:Conclusion Any polynomial P (x, y, z) satisfying the given identities must be a polynomial in the single variable t = x 2 + y 2 + z 2 − xyz.Answer Because x 2 + y 2 + z 2 − xyz is unchanged by the given equalities, P depends only on it.Thus, there is an F with:Thus, none of these numbers is divisible by n if and only ifIn other words, in the list S 0 , S 1 , . . ., S n (indices modulo n + 1), we seek an index t (with t = i − 1) such that the residue S t is isolated-that is, no other S k is congruent to S t modulo n.Since Z/nZ has n elements, but we are choosing n + 1 values S 0 , . . ., S n , the pigeonhole principle guarantees that at least one residue is repeated.However, for the property to hold, we only need that at least one S t be unique among the S k .Thus, the problem reduces to the following:For which integers n ≥ 2 is it true that for every choice of integers a 1 , . . ., a n with S n ̸ ≡ 0 there exists t ∈ {0, 1, . . ., n} such thatIf such a t always exists, then setting i = t + 1 ensures that all partial sums starting at a i are nonzero modulo n.Counterexamples for n ≥ 3 We now describe how to construct counterexamples when n ≥ 3.• It is possible to construct a sequence S 0 , S 1 , . . ., S n in Z/nZ, with S 0 = 0 and S n ̸ ≡ 0, such that every residue appears at least twice.• In such a configuration, no S t is unique, so the desired property fails.One simple way to construct such a sequence is to alternate between two values (e.g., 0 and x), for example:where x ̸ ≡ 0 mod n.Then defineThis ensures that a 1 + • • • + a n = S n ̸ ≡ 0, and because every residue among S 0 , . . ., S n appears at least twice, the property fails.Thus, for every n ≥ 3, one can construct a sequence where no index i satisfies the given condition.The Case n = 2 Now consider n = 2.In Z/2Z, there are only two residues: 0 and 1.• The sequence S 0 , S 1 , S 2 has 3 elements in a 2-element set.• By the pigeonhole principle, at least one residue appears twice.• Therefore, the third must appear only once.So there exists a unique S t .Hence, the property must hold for n = 2.A concrete check:• If a 1 is odd and a 2 is even, then a 1 ≡ 1, a 1 + a 2 ≡ 1.• If a 1 is even and a 2 is odd, then taking i = 2, a 2 ≡ 1, and a 2 + a 1 ≡ 1.So in both cases, the two partial sums are nonzero modulo 2, confirming the property.Conclusion• For all n ≥ 3, one may construct a sequence a 1 , . . ., a n such that for every i, at least one of the cyclic partial sums is divisible by n.• For n = 2, such a counterexample is impossible, and the property always holds.Therefore, the only integer n ≥ 2 for which the property holds is:2 .Final Answer: The property holds if and only if n = 2.If n is odd, then S n will be 0, which means the sum of a 1 + a 2 + • • • + a n is divisible by n, and this contradicts the problem statement.The model has made a calculation mistake in this case and failed to realize that S n ̸ = x for odd ns
Evaluating correctness and faithfulness of instruction-following models for question answering. Parishad Vaibhav Adlakha, Behnamghader, Han Xing, Nicholas Lu, Siva Meade, Reddy, 10.1162/tacla00667Transactions of the Association for Computational Linguistics. 122024</p>
<p>Large language models for mathematical reasoning: Progresses and challenges. Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, Wenpeng Yin, 2024</p>
<p>Mathqa: Towards interpretable math word problem solving with operation-based formalisms. Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, 2019</p>
<p>Language models are hidden reasoners: Unlocking latent reasoning capabilities via self-rewarding. Haolin Chen, Yihao Feng, Zuxin Liu, Weiran Yao, Akshara Prabhakar, Shelby Heinecke, Ricky Ho, Phil Mui, Silvio Savarese, Caiming Xiong, Huan Wang, 2024</p>
<p>Evaluating large vision-and-language models on children's mathematical olympiads. Anoop Cherian, Kuan-Chuan, Suhas Peng, Joanna Lohit, Kevin Matthiesen, Joshua B Smith, Tenenbaum, 2024</p>
<p>Chatbot arena: An open platform for evaluating llms by human preference. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, Ion Stoica, 2024</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>LM vs LM: Detecting factual errors via cross examination. Roi Cohen, May Hamri, Mor Geva, Amir Globerson, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Ultrafeedback: Boosting language models with scaled ai feedback. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, Zhiyuan Liu, Maosong Sun, 2024</p>
<p>Deepseek-Ai , Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J L Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R J Chen, R L Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S S Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, T Shuting Pan, Tao Wang, Tian Yun, Tianyu Pei, W L Sun, Wangding Xiao, Wanjia Zeng, Wei Zhao, Wen An, Wenfeng Liu, Wenjun Liang, Wenqin Gao, Wentao Yu, X Q Zhang, Xiangyue Li, Xianzu Jin, Xiao Wang, Xiaodong Bi, Xiaohan Liu, Xiaojin Wang, Xiaokang Shen, Xiaokang Chen, Xiaosha Zhang, Xiaotao Chen, Xiaowen Nie, Xiaoxiang Sun, Xin Wang, Xin Cheng, Xin Liu, Xingchao Xie, Xingkai Liu, Xinnan Yu, Xinxia Song, Xinyi Shan, Xinyu Zhou, Xinyuan Yang, Xuecheng Li, Xuheng Su, Y K Lin, Y Q Li, Y X Wang, Y X Wei, Yang Zhu, Yanhong Zhang, Yanhong Xu, Yanping Xu, Yao Huang, Yao Li, Yaofeng Zhao, Yaohui Sun, Yaohui Li, Yi Wang, Yi Yu, Yichao Zheng, Yifan Zhang, Yiliang Shi, Ying Xiong, Ying He, Yishi Tang, Yisong Piao, Yixuan Wang, Yiyang Tan, Yiyuan Ma, Yongqiang Liu, Yu Guo, Yuan Wu, Yuchen Ou, Yuduan Zhu, Yue Wang, Yuheng Gong, Yujia Zou, Yukun He, Yunfan Zha, Yunxian Xiong, Yuting Ma, Yuxiang Yan, Yuxiang Luo, Yuxuan You, Yuyang Liu, Z F Zhou, Z Z Wu, Zehui Ren, Zhangli Ren, Zhe Sha, Zhean Fu, Zhen Xu, Zhen Huang, Zhenda Zhang, Zhengyan Xie, Zhewen Zhang, Zhibin Hao, Zhicheng Gou, Zhigang Ma, Zhihong Yan, Zhipeng Shao, Zhiyu Xu, Zhongyu Wu, Zhuoshu Zhang, Zihui Li, Zijia Gu, Zijun Zhu, Zilin Liu, Ziwei Li, Xie, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report. Ziyang Song2024</p>
<p>Ranking large language models without ground truth. Amit Dhurandhar, Rahul Nair, Moninder Singh, Elizabeth Daly, Karthikeyan Natesan, Ramamurthy , 2024</p>
<p>Alpacafarm: A simulation framework for methods that learn from human feedback. Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori Hashimoto, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Length-controlled alpacaeval: A simple way to debias automatic evaluators. Yann Dubois, Balázs Galambosi, Percy Liang, Tatsunori B Hashimoto, 2024</p>
<p>Mathodyssey: Benchmarking mathematical problem-solving skills in large language models using odyssey math data. Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, Kai Zou, 2024</p>
<p>Mathematical capabilities of chatgpt. Simon Frieder, Luca Pinchetti, Alexis Chevalier, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Julius Berner, 2023</p>
<p>Omni-math: A universal olympiad level mathematic benchmark for large language models. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, Baobao Chang, 2024</p>
<p>Gemini model v2 documentation. Google, 2025</p>
<p>Glore: When, where, and how to improve llm reasoning via global and local refinements. Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Roberta Raileanu, 2024</p>
<p>Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, Maosong Sun, 2024</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, 2024</p>
<p>Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, Yikai Zhang, Yuqing Yang, Ting Wu, Binjie Wang, Shichao Sun, Yang Xiao, Yiyuan Li, Fan Zhou, Steffi Chern, Yiwei Qin, Yan Ma, Jiadi Su, Yixiu Liu, Yuxiang Zheng, Shaoting Zhang, Dahua Lin, Yu Qiao, Pengfei Liu, arXiv:2406.12753Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent ai. 2024arXiv preprint</p>
<p>Leanreasoner: Boosting complex logical reasoning with lean. Dongwei Jiang, Marcio Fonseca, Shay B Cohen, 2024</p>
<p>A demonstration of adaptive collaboration of large language models for medical decision-making. Yubin Kim, Chanwoo Park, Hyewon Jeong, Cristina Grau-Vilchez, Yik Siu Chan, Xuhai Xu, Daniel Mcduff, Hyeonhoon Lee, Cynthia Breazeal, Hae Won Park, 2024</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, 2023</p>
<p>From generation to judgment: Opportunities and challenges of llm-as-a-judge. Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, Kai Shu, Lu Cheng, Huan Liu, 2025</p>
<p>Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, Yiqun Liu, Llms-as-judges: A comprehensive survey on llm-based evaluation methods. 2024</p>
<p>. L I Jia, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Costa Shengyi, Kashif Huang, Longhui Rasul, Albert Yu, Ziju Jiang, Zihan Shen, Bin Qin, Li Dong, Yann Zhou, Guillaume Fleureau, Stanislas Lample, Polu, 2024Numinamath</p>
<p>G-eval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Aligning with human judgement: The role of pairwise preference in large language model evaluators. Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulić, Anna Korhonen, Nigel Collier, 2025</p>
<p>SelfcheckGPT: Zero-resource black-box hallucination detection for generative large language models. Potsawee Manakul, Adian Liusie, Mark Gales, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>CHAMP: A competition-level dataset for fine-grained analyses of LLMs' mathematical reasoning capabilities. Yujun Mao, Yoon Kim, Yilun Zhou, 10.18653/v1/2024.findings-acl.785Findings of the Association for Computational Linguistics: ACL 2024. Association for Computational Linguistics2024</p>
<p>Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar, 2024</p>
<p>Juree not judges: safeguarding llm interactions with small, specialised encoder ensembles. Dom Nasrabadi, 2024</p>
<dl>
<dt>Kun-Peng Ning, Shuo Yang, Yu-Yang Liu, Jia-Yu Yao, Zhen-Hui Liu, Yu Wang, Ming Pang, Li Yuan, Peer review in llms based on the consistency optimization. Pico2024</dt>
<dd>
<p>Openai, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob Mcgrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon Mckinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Ming Chak, Charles Li, Chelsea De Bourcy, Chen Voss, Chong Shen, Chris Zhang, Chris Koch, Christopher Orsinger, Claudia Hesse, Clive Fischer, Dan Chan, Daniel Roberts, Daniel Kappler, Daniel Levy, David Selsam, David Dohan, David Farhi, David Mely, Dimitris Robinson, Doug Tsipras, Dragos Li, Eben Oprica, Eddie Freeman, Edmund Zhang, Elizabeth Wong, Enoch Proehl, Eric Cheung, Eric Mitchell, Erik Wallace, Evan Ritter, Fan Mays, Felipe Petroski Wang, Filippo Such, Florencia Raso, Foivos Leoni, Francis Tsimpourlas, Song, Freddie Fred Von Lohmann, Geoff Sulit, Giambattista Salmon, Gildas Parascandolo, Grace Chabot, Greg Zhao, Guillaume Brockman, Hadi Leclerc, Haiming Salman, Hao Bao, Hart Sheng, Hessam Andrin, Hongyu Bagherinezhad, Hunter Ren, Hyung Won Lightman, Ian Chung, Kivlichan, O' Ian, Ian Connell, Ignasi Osband, Ilge Clavera Gilaberte, Ilya Akkaya, Ilya Kostrikov, Irina Sutskever, Jakub Kofman, James Pachocki, Jason Lennon, Jean Wei, Jerry Harb, Jiacheng Twore, Jiahui Feng, Jiayi Yu, Jie Weng, Jieqi Tang, Joaquin Quiñonero Yu, Joe Candela, Joel Palermo, Johannes Parish, John Heidecke, John Hallman, Jonathan Rizzo, Jonathan Gordon, Jonathan Uesato, Joost Ward, Julie Huizinga, Kai Wang, Kai Chen, Karan Xiao, Karina Singhal, Karl Nguyen, Katy Cobbe, Kayla Shi, Kendra Wood, Keren Rimbach, Kevin Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Lama Yu, Lauren Ahmad, Leo Yang, Leon Liu, Leyton Maksin, Liam Ho, Lilian Fedus, Linden Weng, Lindsay Li, Lindsey Mccallum, Lorenz Held, Lukas Kuhn, Lukasz Kondraciuk, Luke Kaiser, Madelaine Metz, Maja Boyd, Manas Trebacz, Mark Joglekar, Marko Chen, Mason Tintor, Matt Meyer, Matt Jones, Max Kaufer, Meghan Schwarzer, Mehmet Shah, Melody Y Yatbaz, Mengyuan Guan, Mengyuan Xu, Mia Yan, Mianna Glaese, Michael Chen, Michael Lampe, Michele Malek, Michelle Wang, Mike Fradin, Mikhail Mcclay, Miles Pavlov, Mingxuan Wang, Mira Wang, Murati, Mo Bavarian ; Rui, Ryan Shu, Ryan Cheu, Saachi Greene, Sam Jain, Sam Altman, Sam Toizer, Samuel Toyer, Sandhini Miserendino, Santiago Agarwal, Sasha Hernandez, Scott Baker, Scottie Mckinney, Shengjia Yan, Shengli Zhao, Shibani Hu, Shraman Santurkar, Shuyuan Ray Chaudhuri, Siyuan Zhang, Spencer Fu, Steph Papay, Suchir Lin, Suvansh Balaji, Szymon Sanjeev, Wenda Sidor ; Weiyi Zheng, Wes Zhou, Wojciech Mccabe, Yann Zaremba, Yinghai Dubois, Yining Lu, Young Chen, Yu Cha, Yuchen Bai, Yuchen He, Yunyun Zhang, Zheng Wang, Zhuohan Shao, Li, Openai o1 system card. Mostafa Rohaninejad, Nat Mcaleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James,; Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders2024Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko,</p>
</dd>
<dt>Reasoning with large language models, a survey. Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki Van Stein, Thomas Back, 2024</dt>
<dd>
<p>Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. Xingzhang Ren, Xuancheng Ren,2025</p>
</dd>
</dl>
<p>ARB: Advanced reasoning benchmark for large language models. Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kranias, John Nay, Kshitij Gupta, Aran Komatsuzaki, The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS'23. 2023</p>
<p>From calculation to adjudication: Examining llm judges on mathematical reasoning tasks. Andreas Stephan, Dawei Zhu, Matthias Aßenmacher, Xiaoyu Shen, Benjamin Roth, 2024</p>
<p>Large language models for data annotation and synthesis: A survey. Zhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Mansooreh Karami, Jundong Li, Lu Cheng, Huan Liu, 2024</p>
<p>Solving olympiad geometry without human demonstrations. Trieu Trinh, Yuhuai Wu, Quoc Le, He He, Thang Luong, 10.1038/s41586-023-06747-5Nature. 2024</p>
<p>Chain-of-thought reasoning without prompting. Xuezhi Wang, Denny Zhou, 2024</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Conic10k: A challenging math problem understanding and reasoning dataset. Haoyi Wu, Wenyang Hui, Yezeng Chen, Weiqi Wu, Kewei Tu, Yi Zhou, 2023</p>
<p>Evaluating mathematical reasoning beyond accuracy. Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, Pengfei Liu, 2025</p>
<p>Justice or prejudice? quantifying biases in llm-as-a-judge. Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, Nitesh V Chawla, Xiangliang Zhang, 2024</p>
<p>Metamath: Bootstrap your own mathematical questions for large language models. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, 2024</p>
<p>How well do large language models perform in arithmetic tasks?. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, 2023</p>
<p>Harp: A challenging human-annotated math reasoning benchmark. Albert S Yue, Lovish Madaan, Ted Moskovitz, Aaditya K Dj Strouse, Singh, 2024</p>
<p>Parsel: Algorithmic reasoning with language models by composing decompositions. Eric Zelikman, Qian Huang, Gabriel Poesia, Noah D Goodman, Nick Haber, 2023</p>
<p>Minif2f: a cross-system benchmark for formal olympiad-level mathematics. Kunhao Zheng, Jesse Michael Han, Stanislas Polu, 2022</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, 2023</p>
<p>Is your model really a good math reasoner? evaluating mathematical reasoning with checklist. Zihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang, Derek F Wong, Xiaowei Huang, Qiufeng Wang, Kaizhu Huang, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
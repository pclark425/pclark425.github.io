<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8886 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8886</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8886</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-156.html">extraction-schema-156</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <p><strong>Paper ID:</strong> paper-de7839351ccd3b4d0ed9fe97a058adb2e967524c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/de7839351ccd3b4d0ed9fe97a058adb2e967524c" target="_blank">Dual PECCS: a cognitive system for conceptual representation and categorization</a></p>
                <p><strong>Paper Venue:</strong> Journal of experimental and theoretical artificial intelligence (Print)</p>
                <p><strong>Paper TL;DR:</strong> An advanced version of Dual-PECCS, a cognitively-inspired knowledge representation and reasoning system aimed at extending the capabilities of artificial systems in conceptual categorization tasks, is presented and integrated and tested into two cognitive architectures, ACT-R and CLARION, implementing different assumptions on the underlying invariant structures governing human cognition.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8886.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8886.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prototype theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prototype theory of concepts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Represents concepts functionally as prototypical summaries (a 'best' or central instance) typically encoded as weighted feature lists or as centroids of convex regions in a metric conceptual space; categorization is driven by similarity to this prototype.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>prototype theory</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Conceptual knowledge is represented by a prototype: an abstracted central tendency (centroid) of observed instances, which can be encoded as a weighted list of typical features or as the geometric center of a convex region in a conceptual space; membership and typicality are graded by distance/similarity to the prototype.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>feature-based / geometric (conceptual spaces)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Categorization (naming from definition), typicality effects, similarity judgments, recognition</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Psychological literature (Rosch and later work) shows prototypes explain typicality phenomena and many categorization behaviors; in Dual-PECCS prototypes are encoded in conceptual spaces as centroids and used by S1 processes for fast, defeasible categorization; system-level evaluation shows prototypes retrieve expected categories but exemplars are often preferred when exemplars matching the input exist. System quantitative results relevant to prototype functioning: overall concept-categorization accuracy 89.3% (no IE) / 77.7% (with IE); proxyfication (correctly returning prototype vs exemplar) shows 21% of cases where exemplar was returned instead of expected prototype (28.8% with IE).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Compared with exemplar representations, prototypes are more abstract and tend to be selected for general descriptions; however empirical and system evidence indicates humans and the implemented system often favor exemplars if a close exemplar exists (algorithmic preference follows Medin & Schaffer 1978). Compared with classical symbolic representations, prototypes support non-monotonic, heuristic categorization and graded typicality whereas symbolic representations support deductive, rule-based membership.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Fails to capture cases where categorization relies on specific stored instances (exemplar-driven judgments) or when necessary/sufficient conditions are required; in Dual-PECCS prototypes can be overridden by exemplars even for high-level/general descriptions, producing proxyfication errors; contextual weighting and tunable similarity metrics are not fully implemented, limiting handling of context-dependent typicality.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Functionally, prototypes serve as fast, associative S1 representations enabling approximate, defeasible categorization and typicality judgments; when combined with exemplar and symbolic knowledge in a heterogeneous proxytype architecture they support complementary roles for fast heuristic inference and later verification by deliberative processes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual PECCS: a cognitive system for conceptual representation and categorization', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8886.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8886.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Exemplar theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exemplar theory of concepts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Represents concepts as stored sets of individual exemplars (specific experienced instances); categorization is performed by comparing new stimuli to these exemplars via similarity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>exemplar theory</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Conceptual knowledge is functionally stored as concrete exemplars—memory traces of specific category members—each represented as a point in a multidimensional (conceptual) space; categorization is done by computing similarity between the stimulus and stored exemplars and making decisions based on nearest neighbors or aggregated similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>instance-based / geometric (conceptual spaces)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Categorization, recognition, learning from examples, typicality where exemplars explain exceptions and atypical members</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Empirical literature and cited studies indicate some human subjects use exemplars for categorization and exemplar accounts explain many classification phenomena; Dual-PECCS implements exemplars as points in conceptual spaces and gives them priority when a stored exemplar is sufficiently similar to the input (threshold rule). System results: exemplar preference produces many proxyfication errors where an exemplar is returned instead of expected prototype (21.0% without IE; 28.8% with IE), indicating exemplars strongly drive S1 outputs when matches exist.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Contrasted with prototypes: exemplars represent concrete stored instances and can capture atypical members and fine-grained distinctions, and are favored when specific similarity is available; contrasted with symbolic/classical formats: exemplars enable non-monotonic, similarity-based inference rather than rule-based deduction. Dual-PECCS explicitly combines exemplar and prototype strategies, preferring exemplars when similarity threshold met (consistent with Medin & Schaffer).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Requires storage of many exemplars (scalability/coverage concerns); may incorrectly dominate categorization for very general descriptions (as observed in the system evaluations), producing counterintuitive exemplar-for-prototype choices; depends on quality and coverage of stored exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Exemplars function as fast, associative S1 resources that supply detailed similarity-based evidence for categorization and can override prototype-based generalizations when close matches are present; they play a complementary role in a heterogeneous representation ensemble to support exception handling and fine-grained classification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual PECCS: a cognitive system for conceptual representation and categorization', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8886.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8886.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Classical theory / Ontological (symbolic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Classical (Aristotelian) concept representation implemented via ontologies / Description Logics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Represents concepts as symbolic definitions composed of necessary and sufficient conditions (logical descriptions); in the system this is operationalized via a formal ontology (OpenCyc) used for deductive, monotonic categorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>classical symbolic (ontological) representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Concepts are encoded as explicit symbolic definitions or logical descriptions (class expressions) consisting of necessary and sufficient conditions; membership and inference are determined by rule-based, monotonic deduction implemented by Description Logics and ontological reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic / rule-based</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Deductive categorization, taxonomic reasoning, consistency checking, tasks requiring rule following and necessary/sufficient condition evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Classical representations are unsuitable to capture typicality phenomena highlighted by Rosch (1975) but are required for precise, taxonomic consistency checks; Dual-PECCS uses an OpenCyc-based S2 module to perform deliberative consistency checks on S1 outputs. In examples the S2 ontological check can reject S1's top candidate (e.g., 'whale' as fish) and force selection of a consistent category ('whale-shark'), demonstrating complementary roles.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Symbolic/ontological formats provide rigorous deductive power absent in prototype/exemplar formats but cannot easily represent graded typicality or defeasible knowledge; Dual-PECCS reconciles them by letting S1 (prototype/exemplar) propose candidates and S2 (ontology) verify or reject them.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Cannot deal naturally with typicality, graded membership, or defeasible defaults; non-monotonic extensions to DL exist but are often computationally expensive and intractable for practical systems (as noted in paper), motivating the hybrid approach.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Functionally, ontological/symbolic representations provide slower, deliberative Type-2 processes that enforce logical consistency and supply necessary-rule-based classification, serving as a corrective/verification stage after fast S1 heuristic categorization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual PECCS: a cognitive system for conceptual representation and categorization', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8886.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8886.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Proxytype</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proxytype theory (proxytypes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concepts are represented as elements (proxies) in long-term memory that can be tokenized into working memory to 'go proxy' for a category; originally conceived as prototype-like but extensible to heterogeneous composition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>proxytype</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Functionally, a proxytype is any long-term-memory element corresponding to a category that can be tokenized in working memory to stand in for that category during processing; proxytypes serve as temporary constructs activated for identification, recognition, or retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>hybrid (pointer/proxy bridging LTM and WM)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Concept activation, retrieval, recognition, categorization and working-memory tokenization phenomena</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prinz's proxytype notion explains activation/retrieval dynamics; Dual-PECCS adopts and extends proxytypes to a heterogeneous set, operationalizing tokenization (proxyfication) where S1 returns a typicality-based representation which is then proxyfied into working memory and checked by S2. Empirically, proxyfication errors in the system (mis-selection of exemplar vs prototype) reveal functional dynamics and heuristics of proxy selection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Proxytypes are not a separate content format but a functional mechanism that can point to prototype, exemplar or symbolic representations; the paper contrasts original monolithic proxytype (prototype-like) with a heterogeneous proxytype that aggregates multiple representational kinds.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Original (monolithic) proxytype view is limited if proxytypes are only prototypes; system-level evidence motivates heterogeneous proxytypes because different tasks call for different representations. The proxyfication process can produce errors (incorrectly tokenizing exemplars for general descriptions) indicating the need for refined heuristics and contextual control.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Proxytypes provide a functional account of how different long-term representations are selected and instantiated into working memory for use; extending to heterogeneous proxytypes allows multiple representational forms to coexist and be selectively tokenized according to task demands and similarity heuristics, integrating Type-1 (fast) and Type-2 (deliberative) processes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual PECCS: a cognitive system for conceptual representation and categorization', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8886.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8886.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heterogeneous proxytypes / hybrid KB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Heterogeneous proxytypes (heterogeneous proxytype hypothesis)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional hypothesis that multiple representational types (prototypes, exemplars, classical definitions) co-exist as interlinked bodies of knowledge for the same concept, each with specific access procedures and roles in different cognitive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>heterogeneous proxytypes (hybrid knowledge base)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Conceptual knowledge for each category is stored as multiple heterogeneous representations (prototype, exemplar, classical/ontological) in LTM; during processing the system activates only the contextually relevant representation(s) (proxyfication) and employs associated retrieval and reasoning procedures (S1 fast similarity-based vs S2 rule-based).</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>hybrid / multi-representational</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Categorization, recognition, retrieval, typicality judgments, inconsistency detection, naming-from-definition, learning with exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The heterogeneous proxytypes architecture motivated Dual-PECCS implementation combining conceptual spaces (prototypes/exemplars) and an ontology (classical knowledge). Experimental evaluation with 112 linguistic descriptions and human gold-standard data shows the hybrid system attains high conceptual categorization accuracy (89.3% without IE) and reveals characteristic error patterns (e.g., exemplar-over-prototype proxyfication) that illuminate functional interactions among representations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Argues for coexistence rather than exclusivity of prototype, exemplar, and classical representations; compared to single-format systems, the hybrid approach can better model a wider range of empirical phenomena (both typicality effects and taxonomic/deductive tasks) by allocating tasks to appropriate representation/reasoning mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Requires alignment and mapping across heterogeneous resources (conceptual spaces ↔ ontologies), which is nontrivial and currently approximate (string matching, WordNet linking); context-sensitive selection heuristics and similarity weighting are underdeveloped in current implementation, causing proxyfication errors; scalability concerns for storing many exemplars and mapping tens of thousands of classes.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Functionally, heterogeneous proxytypes support a division of labor: fast S1 processes operate on similarity-based representations (prototypes/exemplars) to generate candidate classifications while S2 verifies with symbolic knowledge; this architecture provides a plausible cognitive-level account for how multiple representational formats coexist and are dynamically selected according to task/context.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual PECCS: a cognitive system for conceptual representation and categorization', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8886.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8886.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conceptual spaces</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conceptual spaces framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A geometric, metric framework where concepts are regions in a multidimensional space of quality dimensions; prototypes are centroids and exemplars are points, enabling similarity and typicality to be computed by distance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>conceptual spaces</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Functional representation using quality dimensions (domains like color, size, shape) with geometric structure; concepts correspond to convex regions and prototypes to region centres (centroids); instances/exemplars are points; similarity and typicality are computed via metric distances (Euclidean, Manhattan, cosine with weights), optionally modulated by context weights.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>geometric / continuous / hybrid (can interface with symbolic)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Similarity judgments, typicality ratings, categorization, property-based matching, mapping between perception and conceptual representations</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Conceptual spaces naturally encode prototypes and exemplars and provide computational similarity measures used by Dual-PECCS S1 processes; the system implements domains (size, shape, color L*a*b*, location, locomotion, etc.) and uses weighted Euclidean / Manhattan and cosine-based metrics for similarity. Practical evaluation shows this representation supports effective prototype/exemplar retrieval in the hybrid system; however context-weighted similarity and tunable metrics were not implemented and are noted as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Compared with symbolic ontologies, conceptual spaces represent graded, continuous information and similarity relations better; compared with pure feature lists, they provide principled geometry and metrics. The paper advocates conceptual spaces as the functional substrate for prototype/exemplar representations integrated with symbolic S2 representations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Current implementation lacks contextually weighted dimensions and tunable metrics (acknowledged limitations); mapping between conceptual spaces and symbolic ontologies requires laborious linking (WordNet/BabelNet/OpenCyc) and suffers from granularity mismatches; practical coverage of conceptual spaces in system is limited (~13k descriptions mapped).</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Conceptual spaces functionally enable computation of similarity and typicality needed for fast S1 categorization and offer a principled geometric account bridging perception-like continuous representations and higher-level symbolic reasoning when combined via proxyfication.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual PECCS: a cognitive system for conceptual representation and categorization', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8886.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8886.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic pointers (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic pointer concept (semantic pointers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The idea that different bodies of knowledge can act like semantic pointers toward the same conceptual entity (a representational linking mechanism), mentioned as analogous but distinct from the paper's heterogeneous proxytypes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>semantic pointers (as a linking/abstraction mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Functionally, semantic pointers are compact referential tokens that link distributed or lower-level representations to higher-level symbolic content; in the paper this idea is mentioned to characterize how different representational bodies may point to the same concept.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>pointer / hybrid / representational interface</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Cross-representation linking, semantic anchoring, integration of multimodal information</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper mentions semantic pointers (Blouw et al., Eliasmith) as related work and notes similarity is limited: semantic pointers focus on information channel heterogeneity while the paper emphasizes content heterogeneity. No experimental usage in Dual-PECCS beyond conceptual analogy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Discussed as similar in spirit to the proxy/semantic-pointer idea but differentiated: Dual-PECCS emphasizes heterogeneity in content across representations rather than only heterogeneity in information channels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Not implemented in the system as a formal mechanism; discussed only as a conceptual analogy, so no empirical evidence from this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Suggests that pointer-like mechanisms can mediate between heterogeneous representations, but Dual-PECCS argues for a content-level heterogeneity that goes beyond the channel-level heterogeneity emphasized by semantic pointer proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual PECCS: a cognitive system for conceptual representation and categorization', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Conceptual Spaces: The Geometry of Thought <em>(Rating: 2)</em></li>
                <li>Categories of Thought: Rosch's Work on Prototypes (Rosch 1975) <em>(Rating: 2)</em></li>
                <li>Prinz: Furnishing the Mind — Proxytypes (Prinz 2002) <em>(Rating: 2)</em></li>
                <li>Barsalou: Perceptual Symbol Systems (Barsalou 1999) <em>(Rating: 2)</em></li>
                <li>Medin & Schaffer: Context theory / exemplar preference (Medin & Schaffer 1978) <em>(Rating: 2)</em></li>
                <li>Machery: Doing Without Concepts (Machery 2009) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8886",
    "paper_id": "paper-de7839351ccd3b4d0ed9fe97a058adb2e967524c",
    "extraction_schema_id": "extraction-schema-156",
    "extracted_data": [
        {
            "name_short": "Prototype theory",
            "name_full": "Prototype theory of concepts",
            "brief_description": "Represents concepts functionally as prototypical summaries (a 'best' or central instance) typically encoded as weighted feature lists or as centroids of convex regions in a metric conceptual space; categorization is driven by similarity to this prototype.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "prototype theory",
            "representational_format_description": "Conceptual knowledge is represented by a prototype: an abstracted central tendency (centroid) of observed instances, which can be encoded as a weighted list of typical features or as the geometric center of a convex region in a conceptual space; membership and typicality are graded by distance/similarity to the prototype.",
            "format_type": "feature-based / geometric (conceptual spaces)",
            "cognitive_task_or_phenomenon": "Categorization (naming from definition), typicality effects, similarity judgments, recognition",
            "key_findings": "Psychological literature (Rosch and later work) shows prototypes explain typicality phenomena and many categorization behaviors; in Dual-PECCS prototypes are encoded in conceptual spaces as centroids and used by S1 processes for fast, defeasible categorization; system-level evaluation shows prototypes retrieve expected categories but exemplars are often preferred when exemplars matching the input exist. System quantitative results relevant to prototype functioning: overall concept-categorization accuracy 89.3% (no IE) / 77.7% (with IE); proxyfication (correctly returning prototype vs exemplar) shows 21% of cases where exemplar was returned instead of expected prototype (28.8% with IE).",
            "comparison_with_other_formats": "Compared with exemplar representations, prototypes are more abstract and tend to be selected for general descriptions; however empirical and system evidence indicates humans and the implemented system often favor exemplars if a close exemplar exists (algorithmic preference follows Medin & Schaffer 1978). Compared with classical symbolic representations, prototypes support non-monotonic, heuristic categorization and graded typicality whereas symbolic representations support deductive, rule-based membership.",
            "limitations_or_counter_evidence": "Fails to capture cases where categorization relies on specific stored instances (exemplar-driven judgments) or when necessary/sufficient conditions are required; in Dual-PECCS prototypes can be overridden by exemplars even for high-level/general descriptions, producing proxyfication errors; contextual weighting and tunable similarity metrics are not fully implemented, limiting handling of context-dependent typicality.",
            "theoretical_claims_or_implications": "Functionally, prototypes serve as fast, associative S1 representations enabling approximate, defeasible categorization and typicality judgments; when combined with exemplar and symbolic knowledge in a heterogeneous proxytype architecture they support complementary roles for fast heuristic inference and later verification by deliberative processes.",
            "uuid": "e8886.0",
            "source_info": {
                "paper_title": "Dual PECCS: a cognitive system for conceptual representation and categorization",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "Exemplar theory",
            "name_full": "Exemplar theory of concepts",
            "brief_description": "Represents concepts as stored sets of individual exemplars (specific experienced instances); categorization is performed by comparing new stimuli to these exemplars via similarity metrics.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "exemplar theory",
            "representational_format_description": "Conceptual knowledge is functionally stored as concrete exemplars—memory traces of specific category members—each represented as a point in a multidimensional (conceptual) space; categorization is done by computing similarity between the stimulus and stored exemplars and making decisions based on nearest neighbors or aggregated similarity.",
            "format_type": "instance-based / geometric (conceptual spaces)",
            "cognitive_task_or_phenomenon": "Categorization, recognition, learning from examples, typicality where exemplars explain exceptions and atypical members",
            "key_findings": "Empirical literature and cited studies indicate some human subjects use exemplars for categorization and exemplar accounts explain many classification phenomena; Dual-PECCS implements exemplars as points in conceptual spaces and gives them priority when a stored exemplar is sufficiently similar to the input (threshold rule). System results: exemplar preference produces many proxyfication errors where an exemplar is returned instead of expected prototype (21.0% without IE; 28.8% with IE), indicating exemplars strongly drive S1 outputs when matches exist.",
            "comparison_with_other_formats": "Contrasted with prototypes: exemplars represent concrete stored instances and can capture atypical members and fine-grained distinctions, and are favored when specific similarity is available; contrasted with symbolic/classical formats: exemplars enable non-monotonic, similarity-based inference rather than rule-based deduction. Dual-PECCS explicitly combines exemplar and prototype strategies, preferring exemplars when similarity threshold met (consistent with Medin & Schaffer).",
            "limitations_or_counter_evidence": "Requires storage of many exemplars (scalability/coverage concerns); may incorrectly dominate categorization for very general descriptions (as observed in the system evaluations), producing counterintuitive exemplar-for-prototype choices; depends on quality and coverage of stored exemplars.",
            "theoretical_claims_or_implications": "Exemplars function as fast, associative S1 resources that supply detailed similarity-based evidence for categorization and can override prototype-based generalizations when close matches are present; they play a complementary role in a heterogeneous representation ensemble to support exception handling and fine-grained classification.",
            "uuid": "e8886.1",
            "source_info": {
                "paper_title": "Dual PECCS: a cognitive system for conceptual representation and categorization",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "Classical theory / Ontological (symbolic)",
            "name_full": "Classical (Aristotelian) concept representation implemented via ontologies / Description Logics",
            "brief_description": "Represents concepts as symbolic definitions composed of necessary and sufficient conditions (logical descriptions); in the system this is operationalized via a formal ontology (OpenCyc) used for deductive, monotonic categorization.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "classical symbolic (ontological) representation",
            "representational_format_description": "Concepts are encoded as explicit symbolic definitions or logical descriptions (class expressions) consisting of necessary and sufficient conditions; membership and inference are determined by rule-based, monotonic deduction implemented by Description Logics and ontological reasoning.",
            "format_type": "symbolic / rule-based",
            "cognitive_task_or_phenomenon": "Deductive categorization, taxonomic reasoning, consistency checking, tasks requiring rule following and necessary/sufficient condition evaluation",
            "key_findings": "Classical representations are unsuitable to capture typicality phenomena highlighted by Rosch (1975) but are required for precise, taxonomic consistency checks; Dual-PECCS uses an OpenCyc-based S2 module to perform deliberative consistency checks on S1 outputs. In examples the S2 ontological check can reject S1's top candidate (e.g., 'whale' as fish) and force selection of a consistent category ('whale-shark'), demonstrating complementary roles.",
            "comparison_with_other_formats": "Symbolic/ontological formats provide rigorous deductive power absent in prototype/exemplar formats but cannot easily represent graded typicality or defeasible knowledge; Dual-PECCS reconciles them by letting S1 (prototype/exemplar) propose candidates and S2 (ontology) verify or reject them.",
            "limitations_or_counter_evidence": "Cannot deal naturally with typicality, graded membership, or defeasible defaults; non-monotonic extensions to DL exist but are often computationally expensive and intractable for practical systems (as noted in paper), motivating the hybrid approach.",
            "theoretical_claims_or_implications": "Functionally, ontological/symbolic representations provide slower, deliberative Type-2 processes that enforce logical consistency and supply necessary-rule-based classification, serving as a corrective/verification stage after fast S1 heuristic categorization.",
            "uuid": "e8886.2",
            "source_info": {
                "paper_title": "Dual PECCS: a cognitive system for conceptual representation and categorization",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "Proxytype",
            "name_full": "Proxytype theory (proxytypes)",
            "brief_description": "Concepts are represented as elements (proxies) in long-term memory that can be tokenized into working memory to 'go proxy' for a category; originally conceived as prototype-like but extensible to heterogeneous composition.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "proxytype",
            "representational_format_description": "Functionally, a proxytype is any long-term-memory element corresponding to a category that can be tokenized in working memory to stand in for that category during processing; proxytypes serve as temporary constructs activated for identification, recognition, or retrieval.",
            "format_type": "hybrid (pointer/proxy bridging LTM and WM)",
            "cognitive_task_or_phenomenon": "Concept activation, retrieval, recognition, categorization and working-memory tokenization phenomena",
            "key_findings": "Prinz's proxytype notion explains activation/retrieval dynamics; Dual-PECCS adopts and extends proxytypes to a heterogeneous set, operationalizing tokenization (proxyfication) where S1 returns a typicality-based representation which is then proxyfied into working memory and checked by S2. Empirically, proxyfication errors in the system (mis-selection of exemplar vs prototype) reveal functional dynamics and heuristics of proxy selection.",
            "comparison_with_other_formats": "Proxytypes are not a separate content format but a functional mechanism that can point to prototype, exemplar or symbolic representations; the paper contrasts original monolithic proxytype (prototype-like) with a heterogeneous proxytype that aggregates multiple representational kinds.",
            "limitations_or_counter_evidence": "Original (monolithic) proxytype view is limited if proxytypes are only prototypes; system-level evidence motivates heterogeneous proxytypes because different tasks call for different representations. The proxyfication process can produce errors (incorrectly tokenizing exemplars for general descriptions) indicating the need for refined heuristics and contextual control.",
            "theoretical_claims_or_implications": "Proxytypes provide a functional account of how different long-term representations are selected and instantiated into working memory for use; extending to heterogeneous proxytypes allows multiple representational forms to coexist and be selectively tokenized according to task demands and similarity heuristics, integrating Type-1 (fast) and Type-2 (deliberative) processes.",
            "uuid": "e8886.3",
            "source_info": {
                "paper_title": "Dual PECCS: a cognitive system for conceptual representation and categorization",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "Heterogeneous proxytypes / hybrid KB",
            "name_full": "Heterogeneous proxytypes (heterogeneous proxytype hypothesis)",
            "brief_description": "A functional hypothesis that multiple representational types (prototypes, exemplars, classical definitions) co-exist as interlinked bodies of knowledge for the same concept, each with specific access procedures and roles in different cognitive tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "heterogeneous proxytypes (hybrid knowledge base)",
            "representational_format_description": "Conceptual knowledge for each category is stored as multiple heterogeneous representations (prototype, exemplar, classical/ontological) in LTM; during processing the system activates only the contextually relevant representation(s) (proxyfication) and employs associated retrieval and reasoning procedures (S1 fast similarity-based vs S2 rule-based).",
            "format_type": "hybrid / multi-representational",
            "cognitive_task_or_phenomenon": "Categorization, recognition, retrieval, typicality judgments, inconsistency detection, naming-from-definition, learning with exemplars",
            "key_findings": "The heterogeneous proxytypes architecture motivated Dual-PECCS implementation combining conceptual spaces (prototypes/exemplars) and an ontology (classical knowledge). Experimental evaluation with 112 linguistic descriptions and human gold-standard data shows the hybrid system attains high conceptual categorization accuracy (89.3% without IE) and reveals characteristic error patterns (e.g., exemplar-over-prototype proxyfication) that illuminate functional interactions among representations.",
            "comparison_with_other_formats": "Argues for coexistence rather than exclusivity of prototype, exemplar, and classical representations; compared to single-format systems, the hybrid approach can better model a wider range of empirical phenomena (both typicality effects and taxonomic/deductive tasks) by allocating tasks to appropriate representation/reasoning mechanisms.",
            "limitations_or_counter_evidence": "Requires alignment and mapping across heterogeneous resources (conceptual spaces ↔ ontologies), which is nontrivial and currently approximate (string matching, WordNet linking); context-sensitive selection heuristics and similarity weighting are underdeveloped in current implementation, causing proxyfication errors; scalability concerns for storing many exemplars and mapping tens of thousands of classes.",
            "theoretical_claims_or_implications": "Functionally, heterogeneous proxytypes support a division of labor: fast S1 processes operate on similarity-based representations (prototypes/exemplars) to generate candidate classifications while S2 verifies with symbolic knowledge; this architecture provides a plausible cognitive-level account for how multiple representational formats coexist and are dynamically selected according to task/context.",
            "uuid": "e8886.4",
            "source_info": {
                "paper_title": "Dual PECCS: a cognitive system for conceptual representation and categorization",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "Conceptual spaces",
            "name_full": "Conceptual spaces framework",
            "brief_description": "A geometric, metric framework where concepts are regions in a multidimensional space of quality dimensions; prototypes are centroids and exemplars are points, enabling similarity and typicality to be computed by distance metrics.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "conceptual spaces",
            "representational_format_description": "Functional representation using quality dimensions (domains like color, size, shape) with geometric structure; concepts correspond to convex regions and prototypes to region centres (centroids); instances/exemplars are points; similarity and typicality are computed via metric distances (Euclidean, Manhattan, cosine with weights), optionally modulated by context weights.",
            "format_type": "geometric / continuous / hybrid (can interface with symbolic)",
            "cognitive_task_or_phenomenon": "Similarity judgments, typicality ratings, categorization, property-based matching, mapping between perception and conceptual representations",
            "key_findings": "Conceptual spaces naturally encode prototypes and exemplars and provide computational similarity measures used by Dual-PECCS S1 processes; the system implements domains (size, shape, color L*a*b*, location, locomotion, etc.) and uses weighted Euclidean / Manhattan and cosine-based metrics for similarity. Practical evaluation shows this representation supports effective prototype/exemplar retrieval in the hybrid system; however context-weighted similarity and tunable metrics were not implemented and are noted as future work.",
            "comparison_with_other_formats": "Compared with symbolic ontologies, conceptual spaces represent graded, continuous information and similarity relations better; compared with pure feature lists, they provide principled geometry and metrics. The paper advocates conceptual spaces as the functional substrate for prototype/exemplar representations integrated with symbolic S2 representations.",
            "limitations_or_counter_evidence": "Current implementation lacks contextually weighted dimensions and tunable metrics (acknowledged limitations); mapping between conceptual spaces and symbolic ontologies requires laborious linking (WordNet/BabelNet/OpenCyc) and suffers from granularity mismatches; practical coverage of conceptual spaces in system is limited (~13k descriptions mapped).",
            "theoretical_claims_or_implications": "Conceptual spaces functionally enable computation of similarity and typicality needed for fast S1 categorization and offer a principled geometric account bridging perception-like continuous representations and higher-level symbolic reasoning when combined via proxyfication.",
            "uuid": "e8886.5",
            "source_info": {
                "paper_title": "Dual PECCS: a cognitive system for conceptual representation and categorization",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "Semantic pointers (mention)",
            "name_full": "Semantic pointer concept (semantic pointers)",
            "brief_description": "The idea that different bodies of knowledge can act like semantic pointers toward the same conceptual entity (a representational linking mechanism), mentioned as analogous but distinct from the paper's heterogeneous proxytypes.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representational_format_name": "semantic pointers (as a linking/abstraction mechanism)",
            "representational_format_description": "Functionally, semantic pointers are compact referential tokens that link distributed or lower-level representations to higher-level symbolic content; in the paper this idea is mentioned to characterize how different representational bodies may point to the same concept.",
            "format_type": "pointer / hybrid / representational interface",
            "cognitive_task_or_phenomenon": "Cross-representation linking, semantic anchoring, integration of multimodal information",
            "key_findings": "Paper mentions semantic pointers (Blouw et al., Eliasmith) as related work and notes similarity is limited: semantic pointers focus on information channel heterogeneity while the paper emphasizes content heterogeneity. No experimental usage in Dual-PECCS beyond conceptual analogy.",
            "comparison_with_other_formats": "Discussed as similar in spirit to the proxy/semantic-pointer idea but differentiated: Dual-PECCS emphasizes heterogeneity in content across representations rather than only heterogeneity in information channels.",
            "limitations_or_counter_evidence": "Not implemented in the system as a formal mechanism; discussed only as a conceptual analogy, so no empirical evidence from this paper.",
            "theoretical_claims_or_implications": "Suggests that pointer-like mechanisms can mediate between heterogeneous representations, but Dual-PECCS argues for a content-level heterogeneity that goes beyond the channel-level heterogeneity emphasized by semantic pointer proposals.",
            "uuid": "e8886.6",
            "source_info": {
                "paper_title": "Dual PECCS: a cognitive system for conceptual representation and categorization",
                "publication_date_yy_mm": "2017-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Conceptual Spaces: The Geometry of Thought",
            "rating": 2
        },
        {
            "paper_title": "Categories of Thought: Rosch's Work on Prototypes (Rosch 1975)",
            "rating": 2
        },
        {
            "paper_title": "Prinz: Furnishing the Mind — Proxytypes (Prinz 2002)",
            "rating": 2
        },
        {
            "paper_title": "Barsalou: Perceptual Symbol Systems (Barsalou 1999)",
            "rating": 2
        },
        {
            "paper_title": "Medin & Schaffer: Context theory / exemplar preference (Medin & Schaffer 1978)",
            "rating": 2
        },
        {
            "paper_title": "Machery: Doing Without Concepts (Machery 2009)",
            "rating": 1
        }
    ],
    "cost": 0.014824249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>IHIS AperTO</h1>
<h2>UNIVERSITÀ DEGLI STUDI DI TORINO</h2>
<p>AperTO - Archivio Istituzionale Open Access dell'Università di Torino</p>
<h2>Dual PECCS: a cognitive system for conceptual representation and categorization</h2>
<h2>This is a pre print version of the following article:</h2>
<p>Original Citation:</p>
<h2>Availability:</h2>
<p>This version is available http://hdl.handle.net/2318/1603656 since 2016-10-18T10:12:04Z</p>
<p>Published version:
DOI:10.1080/0952813X.2016.1198934
Terms of use:
Open Access
Anyone can freely access the full text of works made available as "Open Access". Works made available under a Creative Commons license can be used according to the terms and conditions of said license. Use of all other works requires consent of the right holder (author or publisher) if not exempted from copyright protection by the applicable law.</p>
<h1>Dual PECCS: A Cognitive System for Conceptual Representation and Categorization</h1>
<p>Antonio Lieto ${ }^{\mathrm{a}, \mathrm{b} <em>}$ Daniele P. Radicioni ${ }^{\mathrm{a} </em>}$ and Valentina Rho ${ }^{\mathrm{a} *}$<br>${ }^{a}$ University of Turin, Dipartimento di Informatica, 10149 Corso Svizzera 185, Turin, Italy;<br>${ }^{b}$ ICAR-CNR, 90128 Viale delle Scienze 12 Ed.11, Palermo, Italy</p>
<p>(Received 05 July 2015; accepted 02 June 2016)</p>
<h4>Abstract</h4>
<p>In this article we present an advanced version of Dual-PECCS, a cognitively-inspired knowledge representation and reasoning system aimed at extending the capabilities of artificial systems in conceptual categorization tasks. It combines different sorts of common-sense categorization (prototypical and exemplars-based categorization) with standard monotonic categorization procedures. These different types of inferential procedures are reconciled according to the tenets coming from the dual process theory of reasoning. On the other hand, from a representational perspective, the system relies on the hypothesis of conceptual structures represented as heterogeneous proxytypes. Dual-PECCS has been experimentally assessed in a task of conceptual categorization where a target concept illustrated by a simple common-sense linguistic description had to be identified by resorting to a mix of categorization strategies, and its output has been compared to human responses. The obtained results suggest that our approach can be beneficial to improve the representational and reasoning conceptual capabilities of standard cognitive artificial systems, and -in addition- that it may be plausibly applied to different general computational models of cognition. The current version of the system, in fact, extends our previous work, in that Dual-PECCS is now integrated and tested into two cognitive architectures, ACT-R and CLARION, implementing different assumptions on the underlying invariant structures governing human cognition. Such integration allowed us to extend our previous evaluation.</p>
<p>Keywords: Knowledge Representation, Categorization, Conceptual Spaces, Cognitive Architectures, Heterogeneous Proxytypes, ACT-R, Prototypes, Exemplars, Common-sense Reasoning, CLARION.</p>
<h2>1. Introduction</h2>
<p>In this work we present the extended version of an integrated knowledge representation system aimed at performing conceptual categorization tasks. It is named Dual-PECCS (after Prototypes and Exemplars-based Conceptual Categorization System), since it relies on two different sorts of cognitively-inspired common-sense categorization: prototypical and exemplars-based categorization. In addition, it is grounded on the theoretical tenets coming from the dual process theory of mind, and on the hypothesis of "heterogeneous proxytypes" developed in the area of the biologically inspired cognitive architectures (BICA).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The system aims at providing a unified framework for the conceptual categorization simulating some of the common-sense heuristic strategies exploited by humans in categorization tasks. More specifically, it integrates strategies based on prototypes and exemplars-based reasoning, as suggested by the psychological results coming form the area of experimental Cognitive Science. The current version of Dual-PECCS has been integrated and tested in the ACT-R and CLARION cognitive architectures to investigate its compatibility with the models of cognition therein implemented (Anderson et al., 2004; Langley et al., 2009; Sun, 2006). ${ }^{1}$</p>
<p>While existing systems and architectures allow to selectively perform either prototype or exemplar-based categorization rather than autonomously adapting their strategy to the input being categorized (Anderson and Betz, 2001), conversely, Dual-PECCS addresses this issue. In addition to the deployment of such common-sense categorization strategies, Dual-PECCS also provides an integration of such types of non-monotonic reasoning with the classical categorization based on standard, deductive, processes. The flow and the interaction of such diverse reasoning mechanisms have been devised based on the tenets coming from the dual process theory of reasoning, and compared to the answers provided by human subjects. In this respect, the main goal of the current system is proposing a cognitively-inspired framework for conceptual representation and categorization. Nonetheless, some preliminary results (see (Lieto et al., 2015a)) show that the resulting system also provides advances in performing common-sense categorization of linguistic descriptions compared with state-of-the-art question-answering systems (including Bing, Google and Wolfram-Alpha). However, a deeper cross-systems comparison is out of the scope of our present contribution, and is ongoing work.</p>
<p>This work is organized as follows: in Section 2 we introduce the theoretical bases inspiring our system, coming from the research in Cognitive Science. In Section 3 we start by outlining the overall architecture and describe the heterogeneous Knowledge Base adopted (Section 3.1), we then illustrate the pipeline going all throughout from the input textual description to the category in output (Section 3.2) and provide the detailed algorithms devised to implement the categorization task (Section 3.3). In Section 4 we show how the hybrid system for the conceptual categorization was integrated into ACTR and CLARION, and in Section 5 we describe the evaluation of the resulting system, by introducing the experimental setting adopted and discussing the obtained results. Finally, we elaborate on future work.</p>
<h1>2. Prototypes, Exemplars and Proxytypes</h1>
<p>In Cognitive Science different theories about the nature of concepts have been proposed. According to the traditional view, known as "classical" or Aristotelian theory, concepts can be simply defined in terms of sets of necessary and sufficient conditions. Such theory was dominant until the mid '70s of the last Century, when Rosch's experimental results demonstrated the inadequacy of such a theory for ordinary -or common-sense- concepts (Rosch, 1975). Rosch's results seemed to suggest, on the other hand, that ordinary concepts are characterized and organized in our mind in terms of prototypes. Since then, different theories of concepts have been proposed to explain different representational and reasoning aspects concerning the problem of typicality: we focus here on the proto-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>type theory and on the exemplars theory. ${ }^{2}$ According to the prototype view, knowledge about categories is stored in terms of prototypes, i.e., in terms of some representation of the "best" instance of the category. In this view, the concept bird should coincide with a representation of a typical bird (e.g., a robin). In the simpler versions of this approach, prototypes are represented as (possibly weighted) lists of typical features. According to the exemplar view, a given category is mentally represented as set of specific exemplars explicitly stored in memory: the mental representation of the concept bird is a set containing the representation of (some of) the birds we encountered during our past experience.</p>
<p>Although these approaches have been largely considered as competing ones (since they propose different models and predictions about how we organize and reason on conceptual information), they turned out to be not mutually exclusive (Malt, 1989). Rather, they seem to succeed in explaining different classes of cognitive phenomena, such as the fact that human subjects use different representations to categorize concepts: some use exemplars, a few rely on prototypes, and often both exemplars and prototypes are employed (Smith and Minda, 1998). This distinction also has neural plausibility, as witnessed by empirical research (Squire and Knowlton, 1995). Such experimental evidences led to the development of the so called "heterogeneous hypothesis" about the nature of concepts: this approach assumes that concepts do not constitute a unitary phenomenon, and hypothesizes that different types of conceptual representations may co-exist: prototypes, exemplars, classical representations, and so on (Machery, 2009). All such representations, in this view, constitute different bodies of knowledge and contain different types of information associated to the the same conceptual entity. Furthermore, each body of conceptual knowledge is featured by specific processes in which such representations are involved (e.g., in cognitive tasks like recognition, learning, categorization, etc.). In particular prototypes and exemplars-based representations are associated with the possibility of dealing with non-monotonic strategies of reasoning and categorization, while the classical representations (i.e. that ones based on necessary and/or sufficient conditions) are associated with standard deductive mechanism of reasoning. ${ }^{3}$</p>
<p>In recent years an alternative theory of concepts has been proposed: the proxytype theory. It postulates a biological localization and interaction between different brain areas for dealing with conceptual structures, that have a direct counterpart in the distinction between long term and working memory (Prinz, 2002). Such characterization is partic-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>ularly interesting for the explanation of phenomena such as, for example, the activation (and the retrieval) of conceptual information. In this setting, concepts are seen as proxytypes.</p>
<p>Definition 1 (Proxytypes): A proxytype is any element of a complex representational network stored in long-term memory corresponding to a particular category that can be tokenized in working memory to 'go proxy' for that category (Prinz, 2002).</p>
<p>In other terms, the proxytype theory, inspired by the work of Barsalou (Barsalou, 1999), considers concepts as temporary constructs of a given category, activated (tokenized) in working memory as a result of conceptual processing activities, such as concept identification, recognition and retrieval.</p>
<p>In its original formulation, however, proxytypes are depicted as monolithic conceptual structures, primarily intended as prototypes (Prinz, 2002). A revised view of this approach has been recently proposed in the area of BICA, hypothesizing the availability of a wider range of representation types than just prototypes (Lieto, 2014). They correspond to the kinds of representations hypothesized by the above mentioned heterogeneous approach to concepts. In this sense, proxytypes are assumed to be heterogeneous in nature (i.e., they are composed by heterogeneous networks of conceptual representations and not only by a monolithic one).</p>
<p>Definition 2 (Heterogeneous Proxytypes): Heterogeneous representations (such as prototypes, exemplars, etc.) for each conceptual category are stored in long-term memory. They can be activated and accessed by resorting to different categorization strategies. In this view, each representation has its associated accessing procedures.</p>
<p>In the design of our system we followed the approach based on heterogeneous proxytypes ${ }^{4}$ for both the representational level (that is, we devised a hybrid knowledge base composed of heterogeneous representations, each endowed with specific reasoning mechanisms) and for the 'proxyfication' (that is, the set of procedures implementing the tokenization of the different representations in working memory).</p>
<h1>3. Dual Process Architecture for Conceptual Representation and Processing</h1>
<p>As earlier mentioned, the Dual-PECCS relies -at the representational level- on the heterogeneous proxytypes approach, and it is also inspired by the dual process theory of reasoning and rationality. More in detail, Dual-PECCS is equipped with a hybrid knowledge base composed of heterogeneous representations of the same conceptual entities: that is, the hybrid knowledge base includes prototypes, exemplars and classical representations for the same concept. Such different bodies of knowledge act like semantic pointers towards the same conceptual entity (Blouw et al., 2015; Eliasmith et al., 2012; Thagard, 2012). ${ }^{5}$</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Both prototypes and exemplars are represented by adopting the conceptual spaces framework (see Section 3.1), while classical information is represented through standard symbolic formalisms (i.e., by means of a formal ontology).</p>
<p>From a reasoning perspective, instead, the retrieval of such representations is driven by different process types. In particular, prototype and exemplar-based retrieval is based on a fast and approximate kind of categorization, and benefits from defeasible, commonsense information associated to concepts. On the other hand, the retrieval of classical representation of concepts is featured by explicit rule following, and makes no use of defeasible, common-sense information. These two differing categorization strategies have been widely studied in psychology of reasoning in the frame of the dual process theory, that postulates the co-existence of two different types of cognitive systems (Evans and Frankish, 2009; Kahneman, 2011). The systems of the first type (type 1) are phylogenetically older, unconscious, automatic, associative, parallel and fast. The systems of the second type (type 2) are more recent, conscious, sequential and slow, and featured by explicit rule following. We assume that both systems can be composed in their turn by many sub-systems and processes. According to the hypotheses in (Frixione and Lieto, 2012, 2014), the conceptual representation of our system includes two main sorts of components, based on these two sorts of processes. Type 1 processes have been designed to deal with prototypes- and exemplar-based retrieval, while Type 2 processes have been designed to deal with deductive inference.</p>
<p>The two sorts of system processes interact (Algorithm 1), since Type 1 processes are executed first and their results are then refined by Type 2 processes. In the implemented system the typical representational and reasoning functions are assigned to the System 1 (hereafter $\mathcal{S} 1$ ), which executes processes of Type 1, and is associated to the conceptual spaces framework (Gärdenfors, 2000, 2014). On the other hand, the classical representational and reasoning functions are assigned to the System 2 (hereafter $\mathcal{S} 2$ ) to execute processes of Type 2, and are associated to a standard Description Logics based ontological representation.</p>
<p>Figure 1 shows the heterogeneous representation for the concept tiger, with prototypical and exemplar-based representations semantically pointing to the same conceptual entity. In this example, the exemplar and prototype-based representations make use of non classical (or typical) information. Namely, the prototypical representation grasps information such as that tigers are wild animals, their fur has yellow and black stripes, etc.; the exemplar-based representations grasp information on individuals. For example, in Fig. 1 is represented an individual of white-tiger, which is a particular type of tiger with white fur. ${ }^{6}$ Both sorts of representations activate Type 1 processes. On the other hand, the classical body of knowledge is filled with necessary and sufficient information to characterize the concept (representing, for example, the taxonomic information that a tiger is a mammal and a carnivore), and activates Type 2 processes. For the sake of readability the information in Figure 1 is visualized with a uniform format, even though the different representations are actually encoded in different formalisms.</p>
<p>In the following we introduce the two representational and reasoning frameworks adopted in our system, by focusing $i$ ) on how typicality information (including both prototypes and exemplars) and their corresponding non-monotonic reasoning procedures</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1.: Heterogeneous representation of the tiger concept in the hybrid knowledge base.
can be encoded through conceptual spaces; and $i i$ ) on how classical information can be naturally encoded in terms of formal ontologies.</p>
<h1>3.1. $\mathcal{S} 1-\mathcal{S} 2$ : Conceptual Spaces and Ontologies</h1>
<p>Conceptual spaces (CS) are a representational framework where knowledge is represented as a set of quality dimensions, and where a geometrical structure is associated to each quality dimension. ${ }^{7}$ In this setting, concepts correspond to convex regions, and regions with different geometrical properties correspond to different sorts of concepts (Gärdenfors, 2000). In addition, concepts are characterized in terms of domains; a domain is "a set of integral dimensions that are separable from all other dimensions" (Gärdenfors, 2014). Typical domain examples are color, size, shape, texture. In turn, domain information can be specified along some dimensions: e.g., in the case of the color domain, relevant dimensions are hue, chromaticity, and brightness.</p>
<p>Prototypes are geometrically interpreted in conceptual spaces, in that they correspond to the geometrical centre of a convex region. This can be thought of as a centroid, that is the mean position of all the points in all dimensions. This representation also allows</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>us, given a convex region, to associate each point to a certain centrality degree, that can be interpreted as a measure of its typicality. Instances can be represented as points in a multidimensional space, and their similarity can be computed as the intervening distance between each two points, based on some suitable metrics. ${ }^{8}$ This framework has been employed also to represent exemplars, that are modelled as points in the multidimensional space. In general, conceptual spaces can be used to compute the proximity between any two entities, and between entities and prototypes. In order to compute the distance between two points $p_{1}, p_{2}$ we use Euclidean metrics to calculate within-domain distance, while for dimensions from different domains we use the Manhattan distance metrics, as suggested in (Adams and Raubal, 2009; Gärdenfors, 2000). The weighted Euclidean distance $\operatorname{dist}_{E}$ is computed as follows</p>
<p>$$
\operatorname{dist}<em 1="1">{E}\left(p</em>
$$}, p_{2}\right)=\sqrt{\sum_{i=1}^{n} w_{i}\left(p_{1, i}-p_{2, i}\right)^{2}</p>
<p>where $i$ varies over the $n$ domain dimensions and $w_{i}$ are dimension weights.
In our implementation of conceptual spaces we represent points as vectors (with as many dimensions as required by the considered domain), whose components correspond to the point coordinates, so that a natural metrics to compute the similarity between them is cosine similarity. In this perspective two vectors with same orientation have a cosine similarity 1 , while two orthogonal vectors have cosine similarity 0 . The normalized version of cosine similarity ( $\hat{cs}$ ), also accounting for the above weights $w_{i}$ is computed as</p>
<p>$$
\hat{c s}\left(p_{1}, p_{2}\right)=\frac{\sum_{i=1}^{n} w_{i}\left(p_{1, i} \times p_{2, i}\right)}{\sqrt{\sum_{i=1}^{n} w_{i}\left(p_{1, i}\right)^{2}} \times \sqrt{\sum_{i=1}^{n} w_{i}\left(p_{2, i}\right)^{2}}}
$$</p>
<p>In the metric space we have defined, the distance between an individual and prototypes is computed with the Manhattan distance metrics. The distance between two concepts can be computed as the distance between two regions: namely, we can compute the distance between their prototypes, or the minimal distance between their individuals ${ }^{9}$, or we can apply more sophisticated algorithms. Further details about technical issues can be found in (Ghignone et al., 2013).</p>
<p>Optionally, a context $k$ can be defined as a set of weights, to grade the relative relevance of the considered dimensions -thus resulting in the following formulas: $\operatorname{dist}<em 1="1">{E}\left(p</em>, k\right)$ from Eq. 1 and 2, respectively-, and to adapt the computation to a variety of settings, such as, e.g., default values vs. known values, explicitly asserted values $v s$. computed values and/or inherited, etc.. Although it is widely accepted that context plays a major role in considering similarity issues (both in human judgements and in computational models), presently our similarity metrics does not involve contextually}, p_{2}, k\right)$ and $\hat{c s}\left(p_{1}, p_{2</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>weighted dimensions, nor tunable metrics, such as those devised in (Aisbett and Gibbon, 1994). These aspects represent an interesting enhancement that we defer to future work. However, it is worth noting that linguistic context, also known as discourse context which is the focus of the present work- is implicitly accounted for by Dual-PECCS categorization strategy: in our setting we consider as contextually relevant elements only those explicitly expressed in the input description. The explicit elements of the discourse structure are therefore at the base of our notion of context: this implies that we compute similarity (between the stimulus and the representations in the hybrid knowledge base, see Figure 1) by considering only dimensions whose values are explicitly expressed in the linguistic descriptions.</p>
<p>The internal representation format for handling conceptual spaces has been introduced in (Lieto et al., 2014); we briefly recall it for the sake of self-containedness. Although the format has been developed by attempting to keep it as general as possible (so to extend its usage to further domains), the current implementation has been devised based on specific representational needs described in Section 5. The basic representational structure processed by the system is named genericDescription; it is actually a super-domain that hosts information about both physical and non physical features, arranged into nine further domains, such as size, shape, color, location, feeding, locomotion, hasPart, partOf, manRelationship.</p>
<p>To give an example of the conceptual spaces representation, let us consider the size domain: the size of entities is expressed through the three Euclidean dimensions; the shape allows expressing that an object has circular, square, spherical, cubic, etc., shape. The color space maps object's features onto the three dimensional $\mathrm{L}^{<em>} \mathrm{a}^{</em>} \mathrm{~b}^{<em>}$ color space, in the same spirit as drawn by Gärdenfors in (Gärdenfors, 2000): in particular, in $\mathrm{L}^{</em>} \mathrm{a}^{<em>} \mathrm{~b}^{</em>}$ terms $\mathrm{L}^{<em>}(0 \leq \mathrm{L} \leq 100)$ is the correlate of lightness, $\mathrm{a}^{</em>}(-128 \leq \mathrm{a} \leq 127)$ is the chromaticity axis ranging from green to red, and $\mathrm{b}^{*}(-128 \leq \mathrm{b} \leq 127)$ is the chromaticity axis ranging from blue to yellow. The location domain indicates the place or the environment where the object being modeled can be typically found. It actually results from the combination of five dimensions, and namely: humidity, indicated as a percentage; temperature, ranging in $\left[-40^{\circ}, 50^{\circ}\right]$; altitude, ranging in $[-11000,8848]$; vegetation, ranging in $[0,100]$; time. In turn, time contains a partitioning of the hours of the day into sunrise (4-6 AM), morning (6-12 AM), afternoon (12-5 PM), evening (5-10 PM) and night (10 PM-4 AM). The locomotion domain combines two dimensions: the former dimension is used to account for the type of movement (1: swim, 2: dig, 3: crawl, 4: walk, 5: run, 6: roll, 7: jump, 8: fly), and the latter one is used to account for the speed, expressed in km/h (Bejan and Marden, 2006). The hasPart and partOf domains are used to complement the analogous ontological properties: in particular, we collected information about the following dimensions: name, number, and partSize, partColor that are intended to specialize the above illustrated spaces.</p>
<p>A simplified example of the lion prototype information is reported below.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">&lt;object</span><span class="w"> </span><span class="na">name=</span><span class="s">&quot;lion&quot;</span><span class="nt">&gt;</span>
<span class="w">    </span><span class="nt">&lt;genericPhysicalDescription&gt;</span>
<span class="w">    </span><span class="nt">&lt;size</span><span class="w"> </span><span class="na">name=</span><span class="s">&quot;lion_size&quot;</span><span class="nt">&gt;</span>
<span class="w">        </span><span class="nt">&lt;x&gt;</span>70<span class="nt">&lt;/x&gt;</span>
<span class="w">        </span><span class="nt">&lt;y&gt;</span>120<span class="nt">&lt;/y&gt;</span>
<span class="w">        </span><span class="nt">&lt;z&gt;</span>200<span class="nt">&lt;/z&gt;</span>
<span class="w">    </span><span class="nt">&lt;/size&gt;</span>
<span class="w">    </span><span class="nt">&lt;color</span><span class="w"> </span><span class="na">name=</span><span class="s">&quot;beige&quot;</span><span class="nt">&gt;</span>
<span class="w">        </span><span class="nt">&lt;l&gt;</span>63<span class="nt">&lt;/l&gt;</span>
<span class="w">        </span><span class="nt">&lt;a&gt;</span>13<span class="nt">&lt;/a&gt;</span>
<span class="w">        </span><span class="nt">&lt;b&gt;</span>32<span class="nt">&lt;/b&gt;</span>
<span class="w">    </span><span class="nt">&lt;/color&gt;</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nt">&lt;location</span><span class="w"> </span><span class="na">name=</span><span class="s">&quot;savanna&quot;</span><span class="nt">&gt;</span>
<span class="w">    </span><span class="nt">&lt;humidity&gt;</span>50<span class="nt">&lt;/humidity&gt;</span>
<span class="w">    </span><span class="nt">&lt;temperature&gt;</span>40<span class="nt">&lt;/temperature&gt;</span>
<span class="w">    </span><span class="nt">&lt;altitude&gt;</span>100<span class="nt">&lt;/altitude&gt;</span>
<span class="w">    </span><span class="nt">&lt;vegetation&gt;</span>50<span class="nt">&lt;/vegetation&gt;</span>
<span class="nt">&lt;/location&gt;</span>
<span class="nt">&lt;locomotion</span><span class="w"> </span><span class="na">name=</span><span class="s">&quot;walk&quot;</span><span class="nt">&gt;</span>
<span class="w">    </span><span class="nt">&lt;movement&gt;</span>4<span class="nt">&lt;/movement&gt;</span>
<span class="w">    </span><span class="nt">&lt;speed&gt;</span>10<span class="nt">&lt;/speed&gt;</span>
<span class="nt">&lt;/locomotion&gt;</span>
<span class="nt">&lt;locomotion</span><span class="w"> </span><span class="na">name=</span><span class="s">&quot;run&quot;</span><span class="nt">&gt;</span>
<span class="w">    </span><span class="nt">&lt;movement&gt;</span>5<span class="nt">&lt;/movement&gt;</span>
<span class="w">    </span><span class="nt">&lt;speed&gt;</span>40<span class="nt">&lt;/speed&gt;</span>
<span class="nt">&lt;/locomotion&gt;</span>
<span class="nt">&lt;/genericPhysicalDescription&gt;</span>
<span class="nt">&lt;/object&gt;</span>
</code></pre></div>

<p>On the other hand, the representation of the classical information regarding a given concept is demanded to classical ontological formalizations. Although relevant efforts have been made in the Description Logics community to enhance the ontological representation and reasoning capacities with the design of formalisms endowing types of typical representation and non-monotonic inference (Giordano et al., 2013), nonetheless the problem of representing and reasoning on typicality remains computationally expensive and practically intractable, and therefore of limited interest for concrete applications (Frixione and Lieto, 2012). ${ }^{10}$ Formal ontologies, then, provide the characterization of concepts in terms of necessary and sufficient conditions (if these conditions exists: as mentioned, most common-sense concepts cannot be characterized in these terms). In our implementation, the ontological representation adopted by the $\mathcal{S} 2$ component is grounded on the OpenCyc ontology, one of the widest ontological resources currently available containing more than 230,000 concepts.</p>
<h1>3.1.1. On the Entity Mapping in Heterogeneous Resources</h1>
<p>A relevant issue we face is aligning knowledge resources based on different sorts of representational formalisms. Although many efforts have been recently invested in the ontology mapping task (Euzenat et al., 2007; Shvaiko and Euzenat, 2013), in the present setting a different effort is required by knowledge bases that are as diverse as conceptual spaces and ontologies.</p>
<p>Under an architectural perspective, $\mathcal{S} 1$ and $\mathcal{S} 2$ rely on knowledge bases encoded in different ways, which need to be connected and mapped onto a shared and uniform representation of meaning in order to allow the ACT-R and CLARION layers to operate them. A rather common approach is to provide entities with sense identifiers, such as WordNet or BabelNet identifiers. WordNet (WN) is a lexical database for the English language (Miller, 1995). Different from common dictionaries -organizing terms alphabetically, but possibly scattering senses- it relies on the idea of grouping terms into synonyms sets (called synsets), that are equipped with short definitions and usage examples. Such sets are represented as nodes of a large semantic network, where the intervening edges represent a number of semantic relations among synset elements (such as hyponymy, hypernymy, antonymy, meronymy, holonymy). BabelNet is a widecoverage multilingual semantic network resulting from the integration of lexicographic</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">TOTAL</th>
<th style="text-align: center;">MAPPED</th>
<th style="text-align: center;">$\%$ SUCCESS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Wikipedia links</td>
<td style="text-align: center;">19,876</td>
<td style="text-align: center;">7,698</td>
<td style="text-align: center;">38.7</td>
</tr>
<tr>
<td style="text-align: left;">WordNet 2.1 links</td>
<td style="text-align: center;">8,779</td>
<td style="text-align: center;">5,197</td>
<td style="text-align: center;">59.2</td>
</tr>
<tr>
<td style="text-align: left;">total num of classes</td>
<td style="text-align: center;">27,823</td>
<td style="text-align: center;">12,723</td>
<td style="text-align: center;">45.7</td>
</tr>
</tbody>
</table>
<p>Table 1.: Summary of the automatic mapping of OpenCyc classes onto WordNet 3.0 synset IDs.
and encyclopedic knowledge from WordNet and Wikipedia (Navigli and Ponzetto, 2010); it extends the constructive rationale of WN -based on sets of synonyms- through the structure of Wikipedia composed of redirect pages, disambiguation pages, internal links, inter-language links, and categorical information. More on the algorithm used to build BabelNet can be found in (Navigli and Ponzetto, 2012).</p>
<p>Entities represented in the conceptual spaces domains, processed by $\mathcal{S} 1$, are provided with WordNet synset IDs; on the other side, classes in $\mathcal{S} 2$ are featured by Cyc IDs. The linking between such identifiers is presently managed through a mapping table. It turns out, then, that the mapping process is a crucial one in order to practically handle the considered resources; a more detailed account on such anchoring aspects is provided in Section 3.1.2. Our mapping proceeded by considering that about $20 \%$ classes in OpenCyc contain references to external knowledge bases (KBs) such as WordNet 2.1 and Wikipedia. The automatically extracted mappings have been computed by following the references in OpenCyc towards BabelNet and WordNet, which produced as output about 13, 000 new associations. Namely, linking information in OpenCyc can be arranged in two classes:</p>
<ul>
<li>Wikipedia Links. A subset of classes in OpenCyc are provided with references to Wikipedia pages. Such references can be directly mapped onto a BabelSynset (in turn providing a pointer to the WordNet synset ID); each link from a BabelNet ID to a WordNet ID has been added to our mapping table. The steps to perform this kind of mapping were: [OpenCyc Class] $\rightarrow$ [BabelSynset ID] $\rightarrow$ [WordNet 3.0 ID].</li>
<li>WordNet 2.1 Links. A subset of classes in OpenCyc contain a link to the online WordNet 2.1 APIs, whose IDs can be directly converted into WN 3.0 IDs through the SenseMap library. ${ }^{11}$ The synset name has then been converted into a numeric ID through the online WordNet 3.0 APIs, and added to the mapping table. The steps to perform this kind of mapping were: [OpenCyc Class] $\rightarrow$ [WordNet 2.1 SynsetName] $\rightarrow$ [WordNet 3.0 SynsetName] $\rightarrow$ [WordNet 3.0 ID].</li>
</ul>
<p>Table 1 illustrates some figures about the mapping of Wikipedia links and WordNet links, by showing how many links were found in OpenCyc, and how many of them could be mapped onto WN synset IDs. We observed that $i$ ) sometimes Cyc classes are linked to several WN synsets; and $i i$ ) in some cases such links are not precise, mostly due to the differing granularity of the information contained in the KB involved in the mapping process.</p>
<p>At the current stage of development, although the entities described through the conceptual spaces used by $\mathcal{S} 1$ module require being substantially enriched, the mapping procedure allows handling some $13 K$ descriptions.</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h4>3.1.2. On Mapping Conceptual Spaces and Ontological Representations</h4>
<p>As above mentioned, an important aspect related to the proposed artificial architecture for conceptual systems regards the way in which the different kinds of representations (i.e., the symbolic structures of the ontological representation and the corresponding underlying conceptual space representations) are mapped one to another. We dealt with this problem by anchoring both the representations at the level of the general concept enveloping them. The heterogeneous proxytypes approach, in fact, requires the existence of co-referring representational structures where the different bodies of knowledge are assumed to semantically point to the main reference conceptual container (see Figure 1). In our approach such a container has been automatically provided with a WordNet synset identifier. In addition, also the corresponding pointing representations (conveying the different types of conceptual knowledge) have been equipped with the same WordNet synset ID.</p>
<p>The anchoring mechanism between the representations follows two different ways. The first one corresponds to the mapping between the general concept and its related ontological component (S2 mapping). This mapping is already provided in the OpenCyc ontology, which is sometimes equipped with the information regarding the corresponding WordNet synset ID. On the other hand, the anchoring between the conceptual space representations and the corresponding general concept (S1 mapping) is obtained as follows: the linguistic elements extracted through an Information Extraction step (please refer to Section 3.2) are automatically equipped with the corresponding WordNet synset ID, if any. After this process, DUAL-PECCS inspects the Conceptual Spaces Knowledge Base searching for the elements corresponding to the extracted linguistic entities. This process is presently based on a simple string-matching between the names of the extracted linguistic entities and the names of the concepts available in the Conceptual Spaces. Once this mapping is provided, the WordNet synset ID assigned to the extracted linguistic entities is transferred to the corresponding representation in the Conceptual Spaces. Finally, the ID obtained in Conceptual Spaces is linked to the WN synset ID already associated to the general concept.</p>
<p>Interestingly, this approach could be easily extended to visual categorization systems by adding, to the whole networks of heterogeneous representations, the ImageNet identifiers (Deng et al., 2009). In the context of artificial vision and robotic applications there are proposals similar to our approach. In particular, the works in (Chella et al., 1997, 2003) also provides a mapping between different levels of representations, including conceptual spaces and symbolic representations. In this case, anchoring is performed for static and dynamic visual scenes. In particular, in the case of dynamic scenes it is based on an anchoring function defined from time t to couple assertions (formulas for the Situation Calculus) and Conceptual Space structures by using look-up tables. The main similarity with their approach by (Chella et al., 1997) is that also our model is primarily focused on the representational issue of the anchoring problem, and does not consider the procedural one (on these aspects, please refer to (Coradeschi and Saffiotti, 2000)). Conversely, the main difference w.r.t. their approach is that we adopt externally developed and well known resources for providing the linguistic anchoring (such as WordNet). This allows our system a good level of compatibility and interoperability with other language technologies adopting the same resources. In the next sections we present the details regarding the categorization strategies adopted in the DUAL-PECCS.</p>
<p>Data: Linguistic description $d$
Result: A class assignment, as computed by $\mathcal{S} 1$ and $\mathcal{S} 2$
trialCounter $\leftarrow 0$;
closed ${ }^{\mathcal{S} 1}={\emptyset}$
while trialCounter $&lt;$ maxTrials do
// conceptual spaces output
$\mathrm{c} \leftarrow \mathcal{S} 1\left(d\right.$, closed $\left.{ }^{\mathcal{S} 1}\right)$
if trialCounter $==0$ then $\mathrm{c}^{<em>} \leftarrow \mathrm{c}$;
// ontology based consistency check
$\mathrm{cc} \leftarrow \mathcal{S} 2(d$, conceptPointedBy(c));
if cc equals(conceptPointedBy(c)) then
return $\left\langle\mathrm{c}^{</em>}, \mathrm{cc}\right\rangle$
else
closed ${ }^{\mathcal{S} 1}$ add(conceptPointedBy(c))
end
++ trialCounter ;
end
$\mathrm{cc} \leftarrow \mathcal{S} 2(\langle d$, Thing $\rangle)$;
return $\left\langle\mathrm{c}^{*}, \mathrm{cc}\right\rangle$
Algorithm 1: The $\mathcal{S} 1-\mathcal{S} 2$ categorization process.</p>
<h1>3.2. Categorization Pipeline of the Dual-PECCS</h1>
<p>The whole categorization pipeline implemented by Dual-PECCS works as follows. The input to the system is a simple linguistic description, like 'The animal that eats bananas', and the expected output is a given category evoked by the description (e.g., the category monkey in this case). After an Information Extraction (IE) step, the input information is encoded into an internal format devised to store conceptual spaces information, which is then used as input in the categorization task by adopting the strategies that will be described below.</p>
<p>A shallow IE approach has been devised, where the morphological information computed from input sentences has been used to devise a simple finite-state automaton describing the input sentences' structure (more on the input descriptions in Section 5). This approach would not scale to handle more complex sentences; its limitations are due to using morphological information, and in general are inherent in finite-state machines (which prevents us from dealing with parenthetical clauses, like relative clauses). We defer to future work the adoption of richer language models. Despite these limitations, however, it allowed us to complete the automatization of the software pipeline going all throughout from the simple linguistic input description used for the evaluation (that will be described later) to its final conceptual categorization.</p>
<h3>3.3. Dual Process Categorization</h3>
<p>The overall control strategy implemented by Dual-PECCS governs the flow of interaction between the $\mathcal{S} 1$ and $\mathcal{S} 2$ systems (it is thus referred to as $\mathcal{S} 1-\mathcal{S} 2$ categorization). Its underlying rationale is to assess the approximate categorization results obtained by Type 1 processes in $\mathcal{S} 1$ with the ontological information and the deliberative processes of reasoning implemented by $\mathcal{S} 2$. The $\mathcal{S} 1-\mathcal{S} 2$ categorization process can be summarized as follows (Algorithm 1). The system takes in input a textual description $d$ and produces in</p>
<p>Data: Linguistic description: $d$; list of inconsistent concepts: closed ${ }^{\mathcal{S} 1}$.
Result: A typicality based representation of a category.
$1 \mathcal{S} 1_{\mathrm{EX}} \leftarrow$ categorizeExemplars $(d)$;
2 if firstOf $\left(\mathcal{S} 1_{\mathrm{EX}}\right.$, closed $\left.{ }^{\mathcal{S} 1}\right)$.distance $(d)&lt;$ similarityThreshold then
3 return firstOf $\left(\mathcal{S} 1_{\mathrm{EX}}\right.$, closed $\left.{ }^{\mathcal{S} 1}\right)$;
4 else
$5 \mathcal{S} 1_{\mathrm{PR}} \leftarrow$ categorizePrototypes $(d)$;
// in case of equal distance prefer exemplars
6 typicalityCategorization $\leftarrow$ sortResults $\left(\mathcal{S} 1_{\mathrm{EX}}, \mathcal{S} 1_{\mathrm{PR}}\right)$;
7 return firstOf (typicalityCategorization, closed ${ }^{\mathcal{S} 1}$ );
8 end
Algorithm 2: $\mathcal{S} 1$ categorization with prototypes and exemplars implementing the instruction in Algorithm 1: line 4.
output a pair $\left\langle\mathrm{c}^{<em>}, \mathrm{cc}\right\rangle$, the output of $\mathcal{S} 1$ and $\mathcal{S} 2$, respectively. If the categorization result provided by $\mathcal{S} 1$ (based on the similarity calculation between the input and $\mathcal{S} 1$ representations) is consistent with the ontology, then the categorization succeeded and the category provided by $\mathcal{S} 2$ (cc) is returned along with $\mathrm{c}^{</em>}$, the top scoring class returned by $\mathcal{S} 1$. Otherwise the system evaluates a fixed amount (maxTrials) of $\mathcal{S} 1$ candidates, meantime keeping track of the inconsistent elements that are added to the closed ${ }^{\mathcal{S} 1}$ list. In case all the $\mathcal{S} 1$ candidates are inconsistent w.r.t. the ontology in $\mathcal{S} 2$, the output of $\mathcal{S} 2$, computed independently of $\mathcal{S} 1$, is provided along with $\mathrm{c}^{*}$. The control strategy implements a tradeoff between ontological inference and the output of $\mathcal{S} 1$, which is more informative but also formally less reliable. ${ }^{12}$</p>
<p>The $\mathcal{S} 1$ categorization algorithm implements the instruction in Algorithm 1, line 4 by determining which kind of $\mathcal{S} 1$ output must be selected and then checked against the deliberative $\mathcal{S} 2$ module (Algorithm 2). In particular, the algorithm is designed to activate either the prototypical-based or the exemplar-based representation, according to the actual input description. The implemented procedure works as follows: when the input stimulus -in our case a simple linguistic description- is similar enough to an exemplar representation (a threshold has been fixed to these ends), the corresponding exemplar of a given category is retrieved. Otherwise, the prototypical representations are also scanned and the representation (prototype or exemplar) that is closest to the input is returned. By following a preference that has been experimentally observed in human cognition (Medin and Schaffer, 1978), this algorithm favors the results of the exemplars-based categorization if the knowledge-base stores any exemplars similar to the input being categorized.</p>
<h1>4. Integrating Dual-PECCS into ACT-R and CLARION</h1>
<p>The proposed system has been integrated into two of the most widely known cognitive architectures: ACT-R (Anderson et al., 2004) and CLARION (Sun, 2006). The underlying rationale behind such integration efforts is to investigate whether our approach is</p>
<p><sup id="fnref10:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>compatible with architectures implementing different cognitive theories of mind; in this case, it can be considered a candidate general framework for representing and reasoning on conceptual information, and eventually tested with even further architectures.</p>
<p>One main difference between the two architectures is that CLARION natively assumes the perspective of the dual process theory; ACT-R, on the other hand, is not natively dual process based. Therefore, in the latter architecture, the dual mechanisms of reasoning need to be explicitly designed and implemented instantiated within an already existing general framework. In particular, in ACT-R cognitive mechanisms emerge from the interaction of two types of knowledge: the declarative knowledge, that encodes explicit facts that the system knows, and the procedural knowledge, that encodes rules for processing declarative knowledge. The declarative module is used to store and retrieve pieces of information called chunks, that are featured by a type and a set of attributevalue pairs, similar to frame slots. Finally, the central production system connects these modules by using a set of IF-THEN production rules.</p>
<p>Differently, in CLARION, cognitive processes are mainly subject to the activity of two sub-systems, called Action Centered Sub-system (ACS) and the Non-Action Centered Sub-system (NACS). Both sub-systems store information using a two-layered architecture, i.e., they both include an explicit and an implicit level of representation. The working memory, acting as temporary storage for decision making, is a part of the ACS, which also maintains the active behavior strategies. To hold general knowledge, the NACS provides a semantic memory consisting of both a rule-based layer that encodes explicit, symbolic knowledge, and of an underlying distributed layer with implicit, subsymbolic representations. A rule in CLARION connects a condition, encoded as a chunk, with an action, encoded as another chunk. For both architectures we mainly focused on the Declarative Memory and Working Memory, and on the corresponding retrieval mechanisms (Figure 2).</p>
<p>Besides, the dual process strategies of concept categorization have been integrated into the ACT-R and CLARION processes and connected to the retrieval request executed in the Working Memory. More in detail: in the Extended Declarative Memory (equivalent to its counterpart, NACS, in CLARION) every concept is represented as an empty chunk (that is, a chunk having no associated information, except for its WordNet synset ID and a human readable name), referred to by the external bodies of knowledge (prototypes and exemplars) acting like semantic pointers. The novel dual process-based categorization mechanism triggers both the $\mathcal{S} 1$ categorization and the $\mathcal{S} 1-\mathcal{S} 2$ categorization procedures. In this setting, when the categorization result of $\mathcal{S} 1$ is returned, the representation activated in the Extended Declarative Long-Term Memory is proxyfied (i.e., recalled to the working memory, see Algorithm 1, line 4) in order to perform the $\mathcal{S} 2$ consistency check (Algorithm 1, line 6), in the dual process perspective.</p>
<p>As regards as the ACT-R implementation, we have integrated our hybrid knowledge base directly into the declarative memory, differently from other approaches that have extended the knowledge capabilities of ACT-R based on the introduction of a new, $a d$-hoc, external module of declarative memory (Oltramari and Lebiere, 2012; Salvucci, 2014). We designed a novel retrieval request implementing the $\mathcal{S} 1-\mathcal{S} 2$ categorization mechanism by extending the repertoire of the retrieval buffer through a new action (symbolized by the operator $\$$ ). Such action allows a direct access to the heterogeneous information represented by the $\mathcal{S} 1-\mathcal{S} 2$ external bodies of knowledge. ${ }^{13}$ We designed two types of $\$$</p>
<p><sup id="fnref11:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2.: ACT-R Architecture with the used memory slots in dotted frames (top), adapted from (Anderson et al., 2004); CLARION cognitive architecture with the working memory and the declarative memory emphasized through shaded frames (bottom), adapted from (Sun, 2007).</p>
<p>Requests that are executed according to the specific type of request received from the exception of some slots—that need to be explicitly indicated by the programmer—for which it is possible to exploit values similarly); conversely, our approach implements a similarity-based retrieval over all the considered dimensions, which is aimed at providing as result the element minimizing the distance from the input query.</p>
<p>retrieval buffer: the approximate categorization request and the consistency request. The approximate categorization request is activated when the retrieval request is generic: i.e., the request chunk lacks of the concept type (the concept_id slot set to nil), that in this kind of request needs to be retrieved. This task is similar to the open request that is possible to execute in ACT-R (Thomson et al., 2014), like in the following example.</p>
<div class="codehilite"><pre><span></span><code>$retrieval&gt;
    concept_id nil
    family_1_name felin
    haspart_1_name fur
    haspart_1_color_1_name yellow
    haspart_1_color_1_1 80.0
    haspart_1_color_1_a -20.0
    haspart_1_color_1_b 94.0
    haspart_2_name stripe
    haspart_2_color_1_name black
    haspart_2_color_1_1 0.0
    haspart_2_color_1_a 0.0
    haspart_2_color_1_b 0.0
    [...]
</code></pre></div>

<p>This kind of request triggers the $\mathcal{S} 1$ retrieval system, and its output is a chunk-like translation of the exemplar or the prototype resulting from the execution of the $\mathcal{S} 1$ retrieval on the typicality-based knowledge. We used the conceptual finsts, by building on the notion of declarative finsts (Pylyshyn, 1989) delivered in ACT-R, to keep track of the representations that have been recently retrieved by the system $\mathcal{S} 1$. In our implementation, conceptual finsts allow $\mathcal{S} 1$ to exclude the elements already inspected and found inconsistent, by adding them to the closed ${ }^{\mathcal{S} 1}$ list. On the other hand, if the $\$$ request specifies the concept to which it refers (i.e., the $\$$ request chunk contains a filler for the concept_id slot), then we are dealing with a consistency check request, to be sent to the $\mathcal{S} 2$ system: in this case, we convert the request and redirect it to the $\mathcal{S} 2$ system that checks whether the features of the chunk are compatible with the proposed classification. The output of this request is a chunk where the slot concept_id is filled with the conceptual representation resulting from $\mathcal{S} 2$.</p>
<p>The integration at the representational and reasoning level in CLARION followed the same rationale indicated in ACT-R, but has been adapted to the specific requirements of the architecture. In particular, we adopted both implicit and explicit representational layers provided by the NACS in order to create a direct mapping with our hybrid architecture: $\mathcal{S} 1$ (and its typicality-based information represented with conceptual spaces) has been mapped onto the implicit layer, while $\mathcal{S} 2$ (the classical, ontology-based representation), has been mapped onto the explicit one. The mapping between the sub-symbolic module of CLARION and the dimension-based representations of the conceptual spaces has been favored since such architecture also synthesizes the implicit information in terms of dimensions-values slots. The dual process based categorization mechanisms have been implemented based on the following procedure: every request is encoded in working memory as a particular type of instance (instance chunk). The dimensions and values of every instance chunk are filled through an update of the implicit module with the information extracted from the external stimulus (in the present case a linguistic description). After building the chunk request, a retrieval request is executed on the $\mathcal{S} 1$ knowledge base with the aim at retrieving an exemplar or a prototype-based representation. Such result is stored in working memory, and checked, as previously illustrated, with the knowledge of the external $\mathcal{S} 2$ knowledge base (the Cyc ontology in our case). More specifically, the</p>
<p>categorization process starts in CLARION when an instance chunk is built and added to the working memory; such process is executed in the ACS module, and it is arranged as a series of rounds, each producing a query to the implicit $\mathcal{S} 1$ component and to the explicit $\mathcal{S} 2$ module. The ACS module initializes the input layer of the $\mathcal{S} 1$ module, based on the instance chunk being considered. This initialization requires to handle external stimuli (the world) along with internal information, at disposal of CLARION agents. Let us start from the sensory input space: this space represents the agent's percepts, and is encoded as a set of pairs $\langle$ dimension,value $\rangle$. Populating the sensory input space (and therefore building the instance chunk for the request to be sent to the NACS declarative memory) involves adding the appropriate set of $\langle$ dimension,value $\rangle$ pairs (when such information, extracted from the input stimuli, is available). Filling a value of a dimension in CLARION is based on the sub-symbolic activation of that dimension when the external input is processed.</p>
<p>In our implementation the available dimensions that a chunk can assume is based on the set of dimensions defined in (Lieto et al., 2015b) for the encoding of conceptual spaces (therefore the internal information that CLARION can process is fixed). It is worth nothing that the activated chunk can lack of some information (i.e., a dimension not filled with its corresponding value), since by definition percepts include noisy or partially missing information.</p>
<p>When the instance chunk is formed a request to the implicit $\mathcal{S} 1$ component is called. The output will be either a chunk representing a prototype or an exemplar representation, corresponding to the prototype/exemplar returned by $\mathcal{S} 1$. Such intermediate result is then added to the working memory for the consistency check, which is performed by the explicit module. In turn, the explicit module tests the consistency of the instance chunk recently added to the Working Memory: if the instance chunk is consistent with the ontological knowledge base, then the explicit module returns the concept chunk corresponding to the output of $\mathcal{S} 1$, thus implementing the notion of proxyfication; otherwise, a special chunk is returned to indicate that an inconsistency was detected. In this case the explicit module evaluates the next result returned by the implicit module, and the whole $\mathcal{S} 1-\mathcal{S} 2$ process is iterated until a representation consistent with the KB is found (as illustrated in Algorithm 1: line 3), or by exiting with a failure in case the maximal number of rounds is reached (Algorithm 1: line 14).</p>
<p>In the next sections we present and discuss some results obtained by the system.</p>
<h1>5. Evaluation</h1>
<p>A dataset composed of 112 descriptions (corresponding to very simple riddles), was collected and given in input to the implemented system: namely, we selected 56 descriptions for which an exemplar-based representation was expected to be retrieved, and 56 descriptions for which a prototype-based representation was expected to be retrieved. This experimentation extends a previous one, presented in (Lieto et al., 2015c). These stimuli have been built by a multidisciplinary team composed of neuropsychologists, linguists and philosophers in the frame of a project aimed at investigating the brain activation of visual areas in tasks of lexical processing even for very abstract concepts. An example of such descriptions is "The big carnivore with yellow fur and black stripes", where the expected category to be retrieved was tiger, and in particular its representation corresponding to the "prototype of tiger"; conversely, a description such as "The big carnivore with white</p>
<p>fur and black stripes" was expected to lead as answer to "exemplar of white_tiger". ${ }^{14}$
The expected categorical targets represent a gold standard, since they correspond to the results provided by human subjects in a twofold psychological experimentation. In the first experiment, 30 healthy volunteers ( 16 females and 14 males) were recruited for the experiment from the Computer Science and Psychology Departments of the University of Turin. Participants were all naïve to the experimental procedure and to the aims of the study. Participants were asked to perform an inferential task "Naming from definition": the subjects were presented the stimuli and asked to overtly name, as accurately and as fast as possible, the target concept corresponding to the definition, using a microphone connected to a response box. The stimuli were presented through the E-Prime software, which was also used to record data on accuracy and reaction times; a richer account of this experimentation is provided in (Radicioni et al., 2015). An additional experimentation has been run on a dataset also including exemplar-based representations along with prototype-based representations. In this case, 10 subjects ( 4 females and 6 males) were recruited among PhD students and Postdoc fellows from the Computer Science Department of the University of Turin. As for the previous experiment, participants were all naïve to the experimental procedure and to the aims of the study. The experimental design was borrowed from the third experiment described in (Malt, 1989): subjects underwent a training phase where exemplar information was also explicitly taught. For example, for the concept tiger they were presented an exemplar of siberian_tiger, an exemplar of malaysian_tiger and a prototypical tiger. After the training phase they were asked to perform the same inferential task "Naming from definition", by naming not only the target concept corresponding to the definition, but also the specific representation (either exemplar or prototype) activated by the linguistic definition.</p>
<p>In both experimentations the recorded answers were consistent with our assumptions and more in general with the literature concerning prototypes and exemplar-based retrieval, which allows us to consider the target concepts like the expected categories, and the target representations as expected proxyfied representations.</p>
<p>To assess the DuAL-PECCS system we have considered a twofold experimental setting. In the first case we tested the whole pipeline, where the salient information is extracted by starting from the linguistic description, the corresponding representation is retrieved, proxyfied and loaded in working memory according to the dual process approach. The information extraction of the linguistic input is not implemented in ACT-R, but it relies on the CoreNLP Stanford Parser (Manning et al., 2014), which is used to convert the textual description into a chunk request. This measure is intended to assess the robustness of the overall approach, from the input parsing to the final categorization. In the second case we tested the heterogeneous proxytypes approach by directly starting with a manual encoding of the linguistic stimulus in terms of chunk request: this measure is intended to assess the accuracy in the categorization task of the hybrid system, featured by dual process approach, heterogeneous representation and reasoning, proxyfication, integration in the ACT-R architecture, but no information extraction.</p>
<p>In both cases (all linguistic pipeline vs. 'clean' input) we recorded two distinct metrics: i) Concept-categorization accuracy (CC-ACC metrics) This metrics was designed to evaluate the final categorization, that is the accuracy in retrieving the expected concept (in this case, the wrong proxyfication did not count as error).
ii) Proxyfication accuracy (P-ACC metrics) This metrics was designed to evaluate whether</p>
<p><sup id="fnref12:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2.: The accuracy results (Table 2-a) and the analysis of the proxyfication errors (Table 2-b).
a. Accuracy rates obtained for the conceptual categorization accuracy (CC-ACC) and proxyfication accuracy (P-ACC) metrics.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">test</th>
<th style="text-align: center;">CC-ACC</th>
<th style="text-align: center;">P-ACC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">with no IE</td>
<td style="text-align: center;">$89.3 \%(100 / 112)$</td>
<td style="text-align: center;">$79.0 \%(79 / 100)$</td>
</tr>
<tr>
<td style="text-align: center;">with IE</td>
<td style="text-align: center;">$77.7 \%(87 / 112)$</td>
<td style="text-align: center;">$71.3 \%(62 / 87)$</td>
</tr>
</tbody>
</table>
<p>b. Analysis of the errors in the proxyfication (P-ACC metrics).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">test</th>
<th style="text-align: right;">Proxyfication error</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: right;">Ex-Proto</td>
<td style="text-align: right;">Proto-Ex</td>
<td style="text-align: right;">Ex-Ex</td>
</tr>
<tr>
<td style="text-align: center;">with no IE</td>
<td style="text-align: right;">$21.0 \%(21 / 100)$</td>
<td style="text-align: right;">$0.0 \%(0 / 100)$</td>
<td style="text-align: right;">$0.0 \%(0 / 100)$</td>
</tr>
<tr>
<td style="text-align: center;">with IE</td>
<td style="text-align: right;">$28.8 \%(26 / 87)$</td>
<td style="text-align: right;">$0.0 \%(0 / 87)$</td>
<td style="text-align: right;">$5.8 \%(5 / 87)$</td>
</tr>
</tbody>
</table>
<p>given in input a description evoking a given concept, the expected proxyfied representation was retrieved. In this case the confusion between prototype and exemplar (or between exemplars) is scored as error even if the expected category is returned.</p>
<h1>5.1. Results and Discussion</h1>
<p>The system obtained an accuracy of $89.3 \%$ in conceptual retrieval (that reduces to $77.7 \%$ when performing the IE step), and $79 \%$ in the proxyfication task ( $71.3 \%$ in the setting with the IE). These figures are reported in Table 2.</p>
<p>The results in the conceptual categorization are in line with those previously reported in (Lieto et al., 2015a), although the dataset was presently more diverse by including exemplars, as well. The results of the whole pipeline (Table 2-a, second row) provide a baseline for future implementations of the IE step; we observe that, although producing $11.6 \%$ error either in the POS tagging or in the IE step proper, this result is also in line with those reported in (Lieto et al., 2015b). This fact shows that the approach -devised to match the simple linguistic structures in the considered stimuli, and which was expected not to generalize to handle further linguistic descriptions- maintains its performance when dealing with a broader dataset. The IE step significantly affects also the P-ACC metrics: that is, if we restrict to considering cases where the concept was categorized as expected, the proxyfication step is performed correctly in $79.0 \%$ of descriptions with 'clean' input, and only in $71.3 \%$ of cases with the Information Extraction step.</p>
<p>Table 2-b reports the detailed errors committed in the proxyfication phase; here we distinguish three cases. Provided that proxyfication errors occur only when the concept has been correctly categorized, three kinds of proxyfication errors were recorded: an exemplar returned in place of an expected prototype (column Ex-Proto); a prototype returned in place of an expected exemplar (column Proto-Ex), or by retrieving a wrong exemplar (e.g., an individual of siberian_tiger in place of an individual of malaysian_tiger, column Ex-Ex). Notably, the vast majority of errors are due to confusion between exemplars and prototypes: in particular, in the $21 \%$ of the considered stimuli an exemplar-proxyfied representation has been returned by the system in spite of the expected prototype. This sort of error raises to $28.8 \%$ in the implementation including the IE task. This error was caused</p>
<p>by the fact that in case exemplar based representations in the KB are equipped with the typical information matching the linguistic description being categorized, then such representations are always favored w.r.t. their prototypical counterpart (see Section 3.1, Algorithm 2). However this fact, although in line with psychological experimental evidence where exemplars -if available- are mostly returned as first choice, is counterintuitive for very general descriptions situated at a high level of abstraction. For example, given the input description "The animal that eats bananas", the Dual-PECCS retrieves the representation of the exemplar associated to roloway_monkey, while the expected output, based on the answers provided by human subjects, is a generic prototype of monkey since the provided description is quite general. This particular class of errors deserves additional clarification in future works: albeit emerged experimenting the computational model, it also suggests that additional analysis is needed in the theoretical debate about exemplars and prototypes. Our results seem to demand deepening the heuristics that steer our categorical choices when the stimulus at hand spans different levels of abstraction. Therefore also from an epistemological perspective this result is interesting, since it shows how cognitively-inspired computational models of cognition taking into account a structuralist perspective, can fruitfully provide insights to the theoretical counterparts that they implement, in a continuous cycle of interaction between theoretical and experimental settings (see e.g. (Lieto and Radicioni, 2016) for the distinction between structuralist and functionalist artificial models of cognition).</p>
<p>The rest of errors are less relevant, except for the type Ex-Ex, where we observe a $5.8 \%$ error rate, which is mostly due to the noisy features extracted from the linguistic descriptions.</p>
<h1>6. Conclusions</h1>
<p>This article has illustrated two main advancements, in that Dual-PECCS provides the heterogeneous proxyfication approach-governed by the dual process theory-with a working implementation; the resulting system is able to autonomously perform prototypical and exemplar-based categorization. Additionally, it has been integrated into ACT-R and CLARION, thus showing a good level of compatibility with two general cognitive systems making different theoretical assumptions about the architecture of human cognition. Although there is room for both refining the theory and tuning the implemented system, the obtained experimental results are encouraging. Our proposal, in addition, seems to suggest, in perspective, a suitable way to deal with both the size and the heterogeneity problems affecting the knowledge level in cognitive architectures (Lieto, 2015).</p>
<p>As mentioned, an aspect that emerged from the experimentation deserves further investigation: namely, it regards the phenomenon of the proxyfication errors ${ }^{15}$ of exemplarbased representations, even for stimuli representing quite general typical descriptions. Such descriptions, in fact, determine in humans the activation of prototypes.</p>
<p>As a future work, we plan to extend the current integrated $\mathcal{S} 1-\mathcal{S} 2$ knowledge base (in particular the $\mathcal{S} 1$ component) by recurring to both semi-automatic enrichment of conceptual spaces through the use of existing resources such as ConceptNet (Liu and Singh, 2004), as well as to manual annotation by exploiting also crowdsourcing and gamification systems.</p>
<p><sup id="fnref13:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{15}$ We remark that we considered as errors results differing from the answers provided by the human subjects interviewed.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref10:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref11:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref12:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref13:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8098 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8098</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8098</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-781b9a445d1878ee4744546f2b8c7466e3cbbd1a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/781b9a445d1878ee4744546f2b8c7466e3cbbd1a" target="_blank">SummEval: Re-evaluating Summarization Evaluation</a></p>
                <p><strong>Paper Venue:</strong> Transactions of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd-sourced human annotations and implements and shares a toolkit that provides an extensible and unified API for evaluating summarization models across a broad range of automatic metrics.</p>
                <p><strong>Paper Abstract:</strong> Abstract The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continue to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd-sourced human annotations; 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics; 3) we assemble the largest collection of summaries generated by models trained on the CNN/DailyMail news dataset and share it in a unified format; 4) we implement and share a toolkit that provides an extensible and unified API for evaluating summarization models across a broad range of automatic metrics; and 5) we assemble and share the largest and most diverse, in terms of model types, collection of human judgments of model-generated summaries on the CNN/Daily Mail dataset annotated by both expert judges and crowd-source workers. We hope that this work will help promote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgments.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8098.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8098.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BertScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BertScore</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contextualized-token-alignment automatic evaluation metric that computes similarity between generated and reference text using BERT token embeddings and greedy alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bertscore: Evaluating text generation with bert</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>SummEval: Re-evaluating Summarization Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>summarization</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CNN/DailyMail (test split; multi-reference setting with 11 references per example)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>BERT (used for contextualized token embeddings in BertScore)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>BERT-based contextualized token embeddings (model size/version not specified in this paper; uses BertScore implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>expert annotators (3 experts; correlations computed on expert annotations; crowd annotations also collected but not used for metric correlations)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's tau (system-level)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.6618</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>low sensitivity to inter-sentential coherence; poorer correlation on subjective relevance judgments; variability across BertScore precision/recall/f1 variants</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>BertScore (particularly recall variant) shows relatively high system-level correlation with expert judgments on consistency and good correlation on fluency, but weaker on coherence and relevance; different BertScore variants behave differently across dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Leveraging contextual embeddings yields stronger semantic matching than purely lexical metrics; higher correlation on factual consistency and fluency dimensions; scalable and automatable.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>System-level Kendall's tau computed between automatic metric scores and averaged expert annotator scores across models; metrics computed in multi-reference setting (original reference + 10 additional references = 11).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SummEval: Re-evaluating Summarization Evaluation', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8098.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8098.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SummaQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SummaQA (QA-based summarization metric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-based question-answering metric that generates cloze-style questions from source documents and evaluates whether answers can be recovered from generated summaries, reporting F1 overlap and QA-model confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Answers unite! unsupervised metrics for reinforced summarization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>SummEval: Re-evaluating Summarization Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>summarization</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CNN/DailyMail (test split; multi-reference setting with 11 references per example)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>BERT-based QA model (used to answer cloze-style questions)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>BERT QA model used to answer questions generated from source documents (exact QA model variant not specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>expert annotators (3 experts; correlations computed on expert annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's tau (system-level)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.6029</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>reduced sensitivity to coherence; potential bias favoring extractive summaries (higher scores for extractive outputs); limited ability to judge subjective relevance</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>SummaQA achieves relatively high correlation with experts on the consistency (factual alignment) dimension, but is less correlated with coherence and relevance; tends to favor extractive models.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Directly targets factual consistency through QA, enabling stronger agreement with expert judgments on consistency; reference-less/QA-driven evaluation can detect factual alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>System-level Kendall's tau between SummaQA scores (F1/confidence) and averaged expert annotations, multi-reference evaluation with 11 references per example.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SummEval: Re-evaluating Summarization Evaluation', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8098.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8098.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLANC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLANC</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reference-less metric that measures the performance gain of a pre-trained language model on downstream language understanding tasks when given access to a candidate summary, intended as a proxy for summary usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fill in the BLANC: human-free quality estimation of document summaries</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>SummEval: Re-evaluating Summarization Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>summarization</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CNN/DailyMail (test split; multi-reference setting with 11 references per example)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>pre-trained language model (unspecified in this paper; BLANC method measures LM performance delta)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>BLANC uses a pre-trained language model and measures performance gains with/without summary context; the exact LM variant is not specified in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>expert annotators (3 experts; correlations computed on expert annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's tau (system-level)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.5588</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>relatively weak sensitivity to coherence; may favor longer summaries (noted length correlations for some metrics); limited detection of subtle factual hallucinations</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>BLANC correlates moderately with expert judgments on consistency and fluency but performs poorly on coherence; as a reference-less metric it behaves differently than overlap-based metrics and shows only weak correlations with many other metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Reference-less evaluation; leverages LM capabilities to assess summary usefulness relative to source, enabling scalable evaluation without gold references.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>System-level Kendall's tau between BLANC scores and averaged expert annotations; experiments used 11 references per example for multi-reference metrics but BLANC itself is reference-less; correlations computed on expert judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SummEval: Re-evaluating Summarization Evaluation', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8098.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8098.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoverScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MoverScore</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semantic similarity metric that computes a Word Mover's Distance-like measure over contextualized n-gram embeddings (pooled from BERT) to estimate closeness between candidate and reference summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>SummEval: Re-evaluating Summarization Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>summarization</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CNN/DailyMail (test split; multi-reference setting with 11 references per example)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>BERT (used to produce contextualized n-gram embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Uses BERT-derived contextualized embeddings pooled into n-gram representations; exact BERT variant not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>expert annotators (3 experts; correlations computed on expert annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's tau (system-level)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.2941</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>weaker correlation with expert judgments on consistency compared to other embedding-based metrics; limited sensitivity to inter-sentence coherence and higher-level discourse structure</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>MoverScore shows modest correlation with expert judgments (best scores on relevance dimension among some lexical metrics), but is substantially weaker than some BERT-based and QA-based metrics on consistency and fluency.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Captures semantic matching beyond surface n-gram overlap; more robust to paraphrase than lexical metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>System-level Kendall's tau between MoverScore and averaged expert annotations; multi-reference evaluation used for metrics that require references (11 references per example).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SummEval: Re-evaluating Summarization Evaluation', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8098.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8098.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sentence Mover's Similarity (SMS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of the Word Mover's Distance to sentence-level: computes distance between bags of sentence embeddings (and a joint sentence+word variant) to assess similarity between summaries and references.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sentence mover's similarity: Automatic evaluation for multi-sentence texts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>SummEval: Re-evaluating Summarization Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>summarization</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CNN/DailyMail (test split; multi-reference setting with 11 references per example)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>sentence embedding model (BERT-based sentence embeddings in original SMS work; exact variant not specified here)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>SMS uses sentence embeddings (as described in Clark et al., 2019) to compute a sentence-level Earth Mover's Distance; the exact encoder variant used in SummEval is not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>expert annotators (3 experts; correlations computed on expert annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's tau (system-level)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.5588</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>reduced sensitivity to coherence measured across sentences; may favor extractive outputs (some metrics including SMS favored extractive models)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>SMS correlates reasonably well with expert judgments on consistency and fluency but, like many metrics, struggles to capture coherence and subjective relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Sentence-level embedding comparison better captures multi-sentence structure than token n-gram overlaps; useful for multi-sentence summary evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>System-level Kendall's tau between SMS scores and averaged expert annotations; multi-reference evaluation (11 references) used for metrics requiring references.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SummEval: Re-evaluating Summarization Evaluation', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8098.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8098.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-family as judges</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large autoregressive LLMs (e.g., GPT-2/3/4) used as human-like judges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large generative language models used as direct evaluators (prompted to score or compare outputs) — note: this paper did not employ GPT-style LLMs as evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>SummEval: Re-evaluating Summarization Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>summarization</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CNN/DailyMail (test split)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Not used (no GPT-3/GPT-4-style LLM-as-judge experiments reported)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>expert annotators (3 experts) and crowdworkers (5 each) were used instead of GPT-style LLM judges</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>N/A in this paper — the paper does not report using large GPT-style LLMs as judges and therefore does not report their failure modes here</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>The study compares automatic (including BERT-based and model-based) metrics to expert human judgments, but does not include experiments where modern large LLMs (e.g., GPT-3/4) act as the primary judge; therefore no direct LLM-as-judge vs human comparison for GPT-family is available in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>N/A in this paper (not evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>No prompted LLM-as-judge experiments reported; correlations and evaluations are between automatic metrics (including BERT-based methods) and expert human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SummEval: Re-evaluating Summarization Evaluation', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bertscore: Evaluating text generation with bert <em>(Rating: 2)</em></li>
                <li>Answers unite! unsupervised metrics for reinforced summarization <em>(Rating: 2)</em></li>
                <li>Fill in the BLANC: human-free quality estimation of document summaries <em>(Rating: 2)</em></li>
                <li>MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance <em>(Rating: 2)</em></li>
                <li>Sentence mover's similarity: Automatic evaluation for multi-sentence texts <em>(Rating: 2)</em></li>
                <li>Studying summarization evaluation metrics in the appropriate scoring range <em>(Rating: 2)</em></li>
                <li>The price of debiasing automatic metrics in natural language evaluation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8098",
    "paper_id": "paper-781b9a445d1878ee4744546f2b8c7466e3cbbd1a",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "BertScore",
            "name_full": "BertScore",
            "brief_description": "A contextualized-token-alignment automatic evaluation metric that computes similarity between generated and reference text using BERT token embeddings and greedy alignment.",
            "citation_title": "Bertscore: Evaluating text generation with bert",
            "mention_or_use": "use",
            "paper_title": "SummEval: Re-evaluating Summarization Evaluation",
            "evaluation_task": "summarization",
            "dataset_name": "CNN/DailyMail (test split; multi-reference setting with 11 references per example)",
            "judge_model_name": "BERT (used for contextualized token embeddings in BertScore)",
            "judge_model_details": "BERT-based contextualized token embeddings (model size/version not specified in this paper; uses BertScore implementation)",
            "human_evaluator_type": "expert annotators (3 experts; correlations computed on expert annotations; crowd annotations also collected but not used for metric correlations)",
            "agreement_metric": "Kendall's tau (system-level)",
            "agreement_score": 0.6618,
            "reported_loss_aspects": "low sensitivity to inter-sentential coherence; poorer correlation on subjective relevance judgments; variability across BertScore precision/recall/f1 variants",
            "qualitative_findings": "BertScore (particularly recall variant) shows relatively high system-level correlation with expert judgments on consistency and good correlation on fluency, but weaker on coherence and relevance; different BertScore variants behave differently across dimensions.",
            "advantages_of_llm_judge": "Leveraging contextual embeddings yields stronger semantic matching than purely lexical metrics; higher correlation on factual consistency and fluency dimensions; scalable and automatable.",
            "experimental_setting": "System-level Kendall's tau computed between automatic metric scores and averaged expert annotator scores across models; metrics computed in multi-reference setting (original reference + 10 additional references = 11).",
            "uuid": "e8098.0",
            "source_info": {
                "paper_title": "SummEval: Re-evaluating Summarization Evaluation",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "SummaQA",
            "name_full": "SummaQA (QA-based summarization metric)",
            "brief_description": "A BERT-based question-answering metric that generates cloze-style questions from source documents and evaluates whether answers can be recovered from generated summaries, reporting F1 overlap and QA-model confidence.",
            "citation_title": "Answers unite! unsupervised metrics for reinforced summarization",
            "mention_or_use": "use",
            "paper_title": "SummEval: Re-evaluating Summarization Evaluation",
            "evaluation_task": "summarization",
            "dataset_name": "CNN/DailyMail (test split; multi-reference setting with 11 references per example)",
            "judge_model_name": "BERT-based QA model (used to answer cloze-style questions)",
            "judge_model_details": "BERT QA model used to answer questions generated from source documents (exact QA model variant not specified in this paper)",
            "human_evaluator_type": "expert annotators (3 experts; correlations computed on expert annotations)",
            "agreement_metric": "Kendall's tau (system-level)",
            "agreement_score": 0.6029,
            "reported_loss_aspects": "reduced sensitivity to coherence; potential bias favoring extractive summaries (higher scores for extractive outputs); limited ability to judge subjective relevance",
            "qualitative_findings": "SummaQA achieves relatively high correlation with experts on the consistency (factual alignment) dimension, but is less correlated with coherence and relevance; tends to favor extractive models.",
            "advantages_of_llm_judge": "Directly targets factual consistency through QA, enabling stronger agreement with expert judgments on consistency; reference-less/QA-driven evaluation can detect factual alignment.",
            "experimental_setting": "System-level Kendall's tau between SummaQA scores (F1/confidence) and averaged expert annotations, multi-reference evaluation with 11 references per example.",
            "uuid": "e8098.1",
            "source_info": {
                "paper_title": "SummEval: Re-evaluating Summarization Evaluation",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "BLANC",
            "name_full": "BLANC",
            "brief_description": "A reference-less metric that measures the performance gain of a pre-trained language model on downstream language understanding tasks when given access to a candidate summary, intended as a proxy for summary usefulness.",
            "citation_title": "Fill in the BLANC: human-free quality estimation of document summaries",
            "mention_or_use": "use",
            "paper_title": "SummEval: Re-evaluating Summarization Evaluation",
            "evaluation_task": "summarization",
            "dataset_name": "CNN/DailyMail (test split; multi-reference setting with 11 references per example)",
            "judge_model_name": "pre-trained language model (unspecified in this paper; BLANC method measures LM performance delta)",
            "judge_model_details": "BLANC uses a pre-trained language model and measures performance gains with/without summary context; the exact LM variant is not specified in this study.",
            "human_evaluator_type": "expert annotators (3 experts; correlations computed on expert annotations)",
            "agreement_metric": "Kendall's tau (system-level)",
            "agreement_score": 0.5588,
            "reported_loss_aspects": "relatively weak sensitivity to coherence; may favor longer summaries (noted length correlations for some metrics); limited detection of subtle factual hallucinations",
            "qualitative_findings": "BLANC correlates moderately with expert judgments on consistency and fluency but performs poorly on coherence; as a reference-less metric it behaves differently than overlap-based metrics and shows only weak correlations with many other metrics.",
            "advantages_of_llm_judge": "Reference-less evaluation; leverages LM capabilities to assess summary usefulness relative to source, enabling scalable evaluation without gold references.",
            "experimental_setting": "System-level Kendall's tau between BLANC scores and averaged expert annotations; experiments used 11 references per example for multi-reference metrics but BLANC itself is reference-less; correlations computed on expert judgments.",
            "uuid": "e8098.2",
            "source_info": {
                "paper_title": "SummEval: Re-evaluating Summarization Evaluation",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "MoverScore",
            "name_full": "MoverScore",
            "brief_description": "A semantic similarity metric that computes a Word Mover's Distance-like measure over contextualized n-gram embeddings (pooled from BERT) to estimate closeness between candidate and reference summaries.",
            "citation_title": "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance",
            "mention_or_use": "use",
            "paper_title": "SummEval: Re-evaluating Summarization Evaluation",
            "evaluation_task": "summarization",
            "dataset_name": "CNN/DailyMail (test split; multi-reference setting with 11 references per example)",
            "judge_model_name": "BERT (used to produce contextualized n-gram embeddings)",
            "judge_model_details": "Uses BERT-derived contextualized embeddings pooled into n-gram representations; exact BERT variant not specified in this paper.",
            "human_evaluator_type": "expert annotators (3 experts; correlations computed on expert annotations)",
            "agreement_metric": "Kendall's tau (system-level)",
            "agreement_score": 0.2941,
            "reported_loss_aspects": "weaker correlation with expert judgments on consistency compared to other embedding-based metrics; limited sensitivity to inter-sentence coherence and higher-level discourse structure",
            "qualitative_findings": "MoverScore shows modest correlation with expert judgments (best scores on relevance dimension among some lexical metrics), but is substantially weaker than some BERT-based and QA-based metrics on consistency and fluency.",
            "advantages_of_llm_judge": "Captures semantic matching beyond surface n-gram overlap; more robust to paraphrase than lexical metrics.",
            "experimental_setting": "System-level Kendall's tau between MoverScore and averaged expert annotations; multi-reference evaluation used for metrics that require references (11 references per example).",
            "uuid": "e8098.3",
            "source_info": {
                "paper_title": "SummEval: Re-evaluating Summarization Evaluation",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "SMS",
            "name_full": "Sentence Mover's Similarity (SMS)",
            "brief_description": "An extension of the Word Mover's Distance to sentence-level: computes distance between bags of sentence embeddings (and a joint sentence+word variant) to assess similarity between summaries and references.",
            "citation_title": "Sentence mover's similarity: Automatic evaluation for multi-sentence texts",
            "mention_or_use": "use",
            "paper_title": "SummEval: Re-evaluating Summarization Evaluation",
            "evaluation_task": "summarization",
            "dataset_name": "CNN/DailyMail (test split; multi-reference setting with 11 references per example)",
            "judge_model_name": "sentence embedding model (BERT-based sentence embeddings in original SMS work; exact variant not specified here)",
            "judge_model_details": "SMS uses sentence embeddings (as described in Clark et al., 2019) to compute a sentence-level Earth Mover's Distance; the exact encoder variant used in SummEval is not detailed.",
            "human_evaluator_type": "expert annotators (3 experts; correlations computed on expert annotations)",
            "agreement_metric": "Kendall's tau (system-level)",
            "agreement_score": 0.5588,
            "reported_loss_aspects": "reduced sensitivity to coherence measured across sentences; may favor extractive outputs (some metrics including SMS favored extractive models)",
            "qualitative_findings": "SMS correlates reasonably well with expert judgments on consistency and fluency but, like many metrics, struggles to capture coherence and subjective relevance.",
            "advantages_of_llm_judge": "Sentence-level embedding comparison better captures multi-sentence structure than token n-gram overlaps; useful for multi-sentence summary evaluation.",
            "experimental_setting": "System-level Kendall's tau between SMS scores and averaged expert annotations; multi-reference evaluation (11 references) used for metrics requiring references.",
            "uuid": "e8098.4",
            "source_info": {
                "paper_title": "SummEval: Re-evaluating Summarization Evaluation",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "GPT-family as judges",
            "name_full": "Large autoregressive LLMs (e.g., GPT-2/3/4) used as human-like judges",
            "brief_description": "Large generative language models used as direct evaluators (prompted to score or compare outputs) — note: this paper did not employ GPT-style LLMs as evaluators.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "SummEval: Re-evaluating Summarization Evaluation",
            "evaluation_task": "summarization",
            "dataset_name": "CNN/DailyMail (test split)",
            "judge_model_name": "Not used (no GPT-3/GPT-4-style LLM-as-judge experiments reported)",
            "judge_model_details": null,
            "human_evaluator_type": "expert annotators (3 experts) and crowdworkers (5 each) were used instead of GPT-style LLM judges",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "N/A in this paper — the paper does not report using large GPT-style LLMs as judges and therefore does not report their failure modes here",
            "qualitative_findings": "The study compares automatic (including BERT-based and model-based) metrics to expert human judgments, but does not include experiments where modern large LLMs (e.g., GPT-3/4) act as the primary judge; therefore no direct LLM-as-judge vs human comparison for GPT-family is available in this work.",
            "advantages_of_llm_judge": "N/A in this paper (not evaluated)",
            "experimental_setting": "No prompted LLM-as-judge experiments reported; correlations and evaluations are between automatic metrics (including BERT-based methods) and expert human annotations.",
            "uuid": "e8098.5",
            "source_info": {
                "paper_title": "SummEval: Re-evaluating Summarization Evaluation",
                "publication_date_yy_mm": "2020-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Bertscore: Evaluating text generation with bert",
            "rating": 2
        },
        {
            "paper_title": "Answers unite! unsupervised metrics for reinforced summarization",
            "rating": 2
        },
        {
            "paper_title": "Fill in the BLANC: human-free quality estimation of document summaries",
            "rating": 2
        },
        {
            "paper_title": "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance",
            "rating": 2
        },
        {
            "paper_title": "Sentence mover's similarity: Automatic evaluation for multi-sentence texts",
            "rating": 2
        },
        {
            "paper_title": "Studying summarization evaluation metrics in the appropriate scoring range",
            "rating": 2
        },
        {
            "paper_title": "The price of debiasing automatic metrics in natural language evaluation",
            "rating": 1
        }
    ],
    "cost": 0.0184125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SummEval: Re-evaluating Summarization Evaluation</h1>
<p>Alexander R. Fabbri ${ }^{\dagger <em>}$ Wojciech Kryściński ${ }^{\ddagger}$ </em><br>Bryan McCann ${ }^{\ddagger}$ Caiming Xiong ${ }^{\ddagger}$ Richard Socher ${ }^{\ddagger}$ Dragomir Radev ${ }^{\dagger \ddagger}$<br>${ }^{\dagger}$ Yale University ${ }^{\ddagger}$ Salesforce Research<br>{alexander.fabbri,dragomir.radev}@yale.edu,<br>{kryscinski,bmccann,cxiong,rsocher}@salesforce.com</p>
<h4>Abstract</h4>
<p>The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continue to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we reevaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd-sourced human annotations, 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics, 3) we assemble the largest collection of summaries generated by models trained on the CNN/DailyMail news dataset and share it in a unified format, 4) we implement and share a toolkit that provides an extensible and unified API for evaluating summarization models across a broad range of automatic metrics, 5) we assemble and share the largest and most diverse, in terms of model types, collection of human judgments of model-generated summaries on the CNN/Daily Mail dataset annotated by both expert judges and crowd-source workers. We hope that this work will help promote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgments.</p>
<h2>1 Introduction</h2>
<p>Text summarization aims to compress long document(s) into a short, fluent, and human-readable form that preserves the most salient information from the source document.</p>
<p>The field has benefited from advances in neural network architectures (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2015; Vaswani</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>et al., 2017) as well as the availability of largescale datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018). Recent advances in pretrained language models, such as BERT (Devlin et al., 2019), have motivated a corresponding shift to pretraining methods in summarization (Liu and Lapata, 2019; Zhang et al., 2019b; Dong et al., 2019; Ziegler et al., 2019; Raffel et al., 2019; Lewis et al., 2019).</p>
<p>A standard dataset for training summarization models is the CNN/DailyMail corpus (Hermann et al., 2015), originally a question answering task, which was repurposed for summarization by Nallapati et al. (2016). The dataset consists of news articles and associated human-created bullet-point summaries. The ROUGE (Lin, 2004b) metric, which measures lexical overlap between generated and target summaries, is then typically used together with crowd-sourced human annotations for model evaluation. While the current setup has become standardized, we believe several factors prevent a more complete comparison of models, thus negatively impacting the progress of the field.</p>
<p>As noted by Hardy et al. (2019), recent papers vastly differ in their evaluation protocol. Existing work often limits model comparisons to only a few baselines and offers human evaluations which are largely inconsistent with prior work. Additionally, despite problems associated with ROUGE when used outside of its original setting (Liu and Liu, 2008; Cohan and Goharian, 2016) as well as the introduction of many variations on ROUGE (Zhou et al., 2006; Ng and Abrecht, 2015; Ganesan, 2015; ShafieiBavani et al., 2018) and other text generation metrics (Peyrard, 2019; Zhao et al., 2019; Zhang* et al., 2020; Scialom et al., 2019; Clark et al., 2019), ROUGE has remained the default automatic evaluation metric. We believe that the shortcomings of the current evaluation protocol are partially caused by the lack of easy-to-use resources for evaluation, both in the form of sim-</p>
<p>plified evaluation toolkits and large collections of model outputs.</p>
<p>In parallel, there is an issue with how evaluation metrics are evaluated themselves. Many of the currently used metrics were developed and assessed using the Document Understanding Conference (DUC) and Text Analysis Conference (TAC) shared-tasks datasets (Dang and Owczarzak, 2008, 2009). However, it has recently been shown that the mentioned datasets contain human judgments for model outputs scoring on a lower scale compared to current summarization systems putting into question the true performance of those metrics in the new setting (Peyrard, 2019).</p>
<p>We address these gaps in complementary ways: 1) We re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using outputs from recent neural summarization models along with expert and crowd-sourced human annotations, 2) We consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics, 3) We release aligned summarization model outputs from 23 papers (44 model outputs) published between 2017 and 2019 trained on the CNN/DailyMail dataset to allow for large-scale comparisons of recent summarization models, 4) We release a toolkit of 14 evaluation metrics with an extensible and unified API to promote the reporting of additional metrics in papers, 5) We collect and release expert, as well as crowd-sourced, human judgments for 16 model outputs on 100 articles over 4 dimensions to further research into human-correlated evaluation metrics. Code and data associated with this work is available at https://github.com/ Yale-LILY/SummEval.</p>
<h2>2 Related Work</h2>
<p>Previous work examining the research setup of text summarization can be broadly categorized into three groups, based on the subject of analysis: evaluation metrics, datasets, and models.</p>
<p>Dealing with evaluation methods, Lin (2004a) examined the effectiveness of the ROUGE metric in various DUC tasks. The authors concluded that evaluating against multiple references results in higher correlation scores with human judgments, however, a single-reference setting is sufficient for the metric to be effective. Owczarzak et al. (2012) studied the effects of inconsistencies in human annotations on the rankings of evaluated sum-
marization systems. Results showed that systemlevel rankings were robust against annotation inconsistencies, however, summary-level rankings were not stable in such settings and largely benefit from improving annotator consistency. Rankel et al. (2013) analyzed the performance of different variants of the ROUGE metric using TAC datasets. The authors found that higher-order and less commonly reported ROUGE settings showed a higher correlation with human judgments. In a similar line of work, Graham (2015) conducted a large-scale study of the effectiveness of different ROUGE metric variants and compared it against the BLEU metric on the DUC datasets. Its results highlighted several superior, non-standard ROUGE settings that achieved strong correlations with human judgments on model-generated summaries. In (Chaganty et al., 2018) the authors investigated using an automatic metric to reduce the cost of human evaluation without introducing bias. Together with the study, the authors released a set of human judgments over several model outputs, limited to a small set of model types. Peyrard (2019) showed that standard metrics are in agreement when dealing with summaries in the scoring range found in TAC summaries, but vastly differ in the higher-scoring range found in current models. The authors reported that additional human annotations on modern model outputs are necessary to conduct a conclusive study of evaluation metrics. Hardy et al. (2019) underscore the differences in approaches to human summary evaluation while proposing a highlight-based reference-less evaluation metric. Other work has examined the problems with applying ROUGE in settings such as meeting summarization (Liu and Liu, 2008) and summarization of scientific articles (Cohan and Goharian, 2016). We build upon this line of research by examining the performance of several automatic evaluation methods, including ROUGE and its variants, against the performance of expert human annotators.</p>
<p>In relation to datasets, Dernoncourt et al. (2018) presented a detailed taxonomy of existing summarization datasets. The authors highlighted the differences in formats of available corpora and called for creating a unified data standard. In a similar line of research, Grusky et al. (2018) offered a thorough analysis of existing corpora, focusing their efforts on news summarization datasets. The authors also introduced several metrics for</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Generated Summaries</th>
<th style="text-align: center;">Expert scores (avg.)</th>
<th style="text-align: center;">Crowd-worker scores (avg.)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">the queen's guard was left red-faced after he slipped on a manhole cover he lost his footing and slid sideways, knocking his bearskin on the side . the embarrassed soldier quickly scrambled to his feet as his colleagues marched past as if nothing had happened . tourist david meadwell recorded the unscheduled manouevre outside buckingham palace on thursday afternoon .</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Coh: } 5.0 \ &amp; \text { Con: } 5.0 \ &amp; \text { Flu: } 5.0 \ &amp; \text { Rel: } 5.0 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Coh: } 3.4 \ &amp; \text { Con: } 3.8 \ &amp; \text { Flu: } 3.4 \ &amp; \text { Rel: } 3.8 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">holidaymaker david meadwell recorded the unscheduled manouevre outside buckingham palace . he lost his footing and slid sideways , knocking bearskin on the side of the box . queen 's guard was left red-faced after he slipped on manhole cover . the entire incident was caught on a manhole cover . the embarrassed soldier quickly scrambled to his feet as his colleagues marched past .</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Coh: } 2.7 \ &amp; \text { Con: } 2.0 \ &amp; \text { Flu: } 4.7 \ &amp; \text { Rel: } 3.7 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Coh: } 3.2 \ &amp; \text { Con: } 3.4 \ &amp; \text { Flu: } 3.4 \ &amp; \text { Rel: } 4.0 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">buckingham palace guard slipped on manhole cover in front of hundreds of horrified tourists . the queen 's guard was left red-faced after he slipped on a manhole cover . he lost his footing and dropped his rifle on the side of the box and dropping his rifle . the incident was caught on camera camera camera . the guard is thought to have slipped because of metal shutters nailed to the soles of his boots .</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Coh: } 3.3 \ &amp; \text { Con: } 5.0 \ &amp; \text { Flu: } 1.7 \ &amp; \text { Rel: } 4.3 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Coh: } 3.0 \ &amp; \text { Con: } 3.2 \ &amp; \text { Flu: } 2.8 \ &amp; \text { Rel: } 3.2 \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>(a) Generated summary examples illustrate common problems found in model outputs, such as ambiguous pronouns, incorrect references, and repetitive content.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Reference Summaries</th>
<th style="text-align: center;">Expert scores (avg.)</th>
<th style="text-align: center;">Crowd-worker scores (avg.)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">river plate admit they ' dream' of manchester united striker radamel falcao . the colombia international spent eight years with the argentine club . falcao has managed just four goals in 19 premier league appearances . read : falcao still ' has faith ' that he could continue at man utd next season . click here for the latest manchester united news .</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Coh: } 3.0 \ &amp; \text { Con: } 2.0 \ &amp; \text { Flu: } 5.0 \ &amp; \text { Rel: } 2.3 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Coh: } 3.0 \ &amp; \text { Con: } 3.6 \ &amp; \text { Flu: } 3.0 \ &amp; \text { Rel: } 4.4 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">the incident occurred on april 7 north of poland in the baltic sea . u.s. says plane was in international airspace . russia says it had transponder turned off and was flying toward russia</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Coh: } 2.0 \ &amp; \text { Con: } 1.7 \ &amp; \text { Flu: } 3.0 \ &amp; \text { Rel: } 2.3 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Coh: } 4.0 \ &amp; \text { Con: } 3.4 \ &amp; \text { Flu: } 4.2 \ &amp; \text { Rel: } 3.6 \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>(b) Reference summaries highlight issues found in the CNN/DailyMail dataset, such as click-baits and references to other articles as well as unreferenced dates and low coherence caused by concatenating bullet-point summaries.</p>
<p>Table 1: Example summaries with the corresponding averaged expert and crowd-sourced annotations for coherence, consistency, fluency, and relevance. Expert annotations better differentiate coherence, consistency, and fluency among the examples when compared to the crowd-sourced annotations.
evaluating the extractiveness of summaries which are included in the toolkit implemented as part of this work. Kryściński et al. (2020) showed that news-related summarization datasets, such as CNN/DailyMail, contain strong layout biases. The authors revealed that datasets in the current format, where each news article is associated with a single reference summary, leave the task of summarization underconstrained. The paper also highlighted the problem of noisy, low-quality data in automatically-collected news datasets.</p>
<p>Looking into models, Zhang et al. (2018a) analyzed the level of abstraction of several recent abstractive summarization models. The authors showed that word-level extractive models achieved a similar level of abstraction to fully abstractive models. In (Kedzie et al., 2018) the authors examined the influence of various model components on the quality of content selection. The study revealed that in the current setting the training signal is dominated by biases present in summarization datasets preventing models from learning accurate content selection. Kryściński et al. (2020) investigate the problem of factual correctness of text summarization models. The authors concluded that the issue of hallucinating facts touches up to $30 \%$ of generated summaries and list common types of errors made by generative models. Closely related to that work, Maynez et al. (2020) conducted a large-scale study of abstractive summarizers from the perspective of faithfulness. The authors reached similar conclusions, stating that improving factual faithfulness is a critical issue in summarization. The results also showed that currently available evaluation methods, such as ROUGE and BertScore, are not sufficient to study the problem at hand. Durmus et al. (2020) and Wang et al. (2020) similarly examine faithfulness evaluation, both proposing question answering frameworks as a means of evaluating</p>
<p>factual consistency.
Insights and contributions coming from our work are complementary to the conclusions of previous efforts described in this section. To the best of our knowledge, this is the first work in neural text summarization to offer a large-scale, consistent, side-by-side re-evaluation of summarization model outputs and evaluation methods. We also share resources that we hope will prove useful for future work in analyzing and improving summarization models and metrics.</p>
<p>Shortly before publishing this manuscript a library for developing summarization metrics was released by Deutsch and Roth (2020). Our toolkit is complementary to their work as their toolkit includes only 3 of our 12 evaluation metrics.</p>
<h2>3 Evaluation Metrics and Summarization Models</h2>
<p>We briefly introduce metrics included in our evaluation toolkit as well as the summarization models for which outputs were collected at the time of releasing this manuscript.</p>
<h3>3.1 Evaluation Metrics</h3>
<p>Our selection of evaluation methods includes several recently introduced metrics that have been applied to both text generation and summarization, standard machine translation metrics, and other miscellaneous performance statistics.
ROUGE (Lin, 2004b), (Recall-Oriented Understudy for Gisting Evaluation), measures the number of overlapping textual units (n-grams, word sequences) between the generated summary and a set of gold reference summaries.
ROUGE-WE ( Ng and Abrecht, 2015) extends ROUGE by using soft lexical matching based on the cosine similarity of Word2Vec (Mikolov et al., 2013) embeddings.
$\boldsymbol{S}^{\mathbf{3}}$ (Peyrard et al., 2017) is a model-based metric that uses previously proposed evaluation metrics, such as ROUGE, JS-divergence, and ROUGEWE, as input features for predicting the evaluation score. The model is trained on human judgment datasets from TAC conferences.
BertScore (Zhang* et al., 2020) computes similarity scores by aligning generated and reference summaries on a token-level. Token alignments are computed greedily to maximize the cosine similarity between contextualized token embeddings from BERT.</p>
<p>MoverScore (Zhao et al., 2019) measures the semantic distance between a summary and reference text by making use of the Word Mover's Distance (Kusner et al., 2015) operating over n-gram embeddings pooled from BERT representations.
Sentence Mover's Similarity (SMS) (Clark et al., 2019) extends Word Mover's Distance to view documents as a bag of sentence embeddings as well as a variation which represents documents as both a bag of sentences and a bag of words.
SummaQA (Scialom et al., 2019) applies a BERT-based question-answering model to answer cloze-style questions using generated summaries. Questions are generated by masking named entities in source documents associated with evaluated summaries. The metric reports both the F1 overlap score and QA-model confidence.
BLANC (Vasilyev et al., 2020) is a reference-less metric which measures the performance gains of a pre-trained language model given access to a document summary while carrying out language understanding tasks on the source document's text.
SUPERT (Gao et al., 2020) is a reference-less metric, originally designed for multi-document summarization, which measures the semantic similarity of model outputs with pseudo-reference summaries created by extracting salient sentences from the source documents, using soft token alignment techniques.
BLEU (Papineni et al., 2002) is a corpus-level precision-focused metric which calculates n-gram overlap between a candidate and reference utterance and includes a brevity penalty. It is the primary evaluation metric for machine translation.
CHRF (Popović, 2015) calculates character-based n-gram overlap between model outputs and reference documents.
METEOR (Lavie and Agarwal, 2007) computes an alignment between candidate and reference sentences by mapping unigrams in the generated summary to 0 or 1 unigrams in the reference, based on stemming, synonyms, and paraphrastic matches. Precision and recall are computed and reported as a harmonic mean.
CIDEr (Vedantam et al., 2015) computes ${1-4}$ gram co-occurrences between the candidate and reference texts, down-weighting common n-grams and calculating cosine similarity between the ngrams of the candidate and reference texts.
Data Statistics: Grusky et al. (2018) define three measures of the extractiveness of a dataset. Ex-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;">Consistency</th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;">Relevance</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ROUGE-1</td>
<td style="text-align: center;">0.2500</td>
<td style="text-align: center;">0.5294</td>
<td style="text-align: center;">$\mathbf{0 . 5 2 4 0}$</td>
<td style="text-align: center;">0.4118</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-2</td>
<td style="text-align: center;">0.1618</td>
<td style="text-align: center;">0.5882</td>
<td style="text-align: center;">0.4797</td>
<td style="text-align: center;">0.2941</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-3</td>
<td style="text-align: center;">0.2206</td>
<td style="text-align: center;">$\mathbf{0 . 7 0 5 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 0 9 2}$</td>
<td style="text-align: center;">0.3529</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-4</td>
<td style="text-align: center;">$\mathbf{0 . 3 0 8 8}$</td>
<td style="text-align: center;">0.5882</td>
<td style="text-align: center;">$\mathbf{0 . 5 5 3 5}$</td>
<td style="text-align: center;">0.4118</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-L</td>
<td style="text-align: center;">0.0735</td>
<td style="text-align: center;">0.1471</td>
<td style="text-align: center;">0.2583</td>
<td style="text-align: center;">0.2353</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-su*</td>
<td style="text-align: center;">0.1912</td>
<td style="text-align: center;">0.2941</td>
<td style="text-align: center;">0.4354</td>
<td style="text-align: center;">0.3235</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-w</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.3971</td>
<td style="text-align: center;">0.3764</td>
<td style="text-align: center;">0.1618</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-we-1</td>
<td style="text-align: center;">$\mathbf{0 . 2 6 4 7}$</td>
<td style="text-align: center;">0.4559</td>
<td style="text-align: center;">$\mathbf{0 . 5 0 9 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2 6 5}$</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-we-2</td>
<td style="text-align: center;">-0.0147</td>
<td style="text-align: center;">0.5000</td>
<td style="text-align: center;">0.3026</td>
<td style="text-align: center;">0.1176</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-we-3</td>
<td style="text-align: center;">0.0294</td>
<td style="text-align: center;">0.3676</td>
<td style="text-align: center;">0.3026</td>
<td style="text-align: center;">0.1912</td>
</tr>
<tr>
<td style="text-align: left;">$\boldsymbol{S}^{\mathbf{2}}$-pyr</td>
<td style="text-align: center;">-0.0294</td>
<td style="text-align: center;">0.5147</td>
<td style="text-align: center;">0.3173</td>
<td style="text-align: center;">0.1324</td>
</tr>
<tr>
<td style="text-align: left;">$\boldsymbol{S}^{\mathbf{3}}$-resp</td>
<td style="text-align: center;">-0.0147</td>
<td style="text-align: center;">0.5000</td>
<td style="text-align: center;">0.3321</td>
<td style="text-align: center;">0.1471</td>
</tr>
<tr>
<td style="text-align: left;">BertScore-p</td>
<td style="text-align: center;">0.0588</td>
<td style="text-align: center;">-0.1912</td>
<td style="text-align: center;">0.0074</td>
<td style="text-align: center;">0.1618</td>
</tr>
<tr>
<td style="text-align: left;">BertScore-r</td>
<td style="text-align: center;">0.1471</td>
<td style="text-align: center;">$\mathbf{0 . 6 6 1 8}$</td>
<td style="text-align: center;">0.4945</td>
<td style="text-align: center;">0.3088</td>
</tr>
<tr>
<td style="text-align: left;">BertScore-f</td>
<td style="text-align: center;">0.2059</td>
<td style="text-align: center;">0.0441</td>
<td style="text-align: center;">0.2435</td>
<td style="text-align: center;">$\mathbf{0 . 4 2 6 5}$</td>
</tr>
<tr>
<td style="text-align: left;">MovefScore</td>
<td style="text-align: center;">0.1912</td>
<td style="text-align: center;">-0.0294</td>
<td style="text-align: center;">0.2583</td>
<td style="text-align: center;">0.2941</td>
</tr>
<tr>
<td style="text-align: left;">SMS</td>
<td style="text-align: center;">0.1618</td>
<td style="text-align: center;">0.5588</td>
<td style="text-align: center;">0.3616</td>
<td style="text-align: center;">0.2353</td>
</tr>
<tr>
<td style="text-align: left;">SummaQA*</td>
<td style="text-align: center;">0.1176</td>
<td style="text-align: center;">$\mathbf{0 . 6 0 2 9}$</td>
<td style="text-align: center;">0.4059</td>
<td style="text-align: center;">0.2206</td>
</tr>
<tr>
<td style="text-align: left;">BLANC*</td>
<td style="text-align: center;">0.0735</td>
<td style="text-align: center;">0.5588</td>
<td style="text-align: center;">0.3616</td>
<td style="text-align: center;">0.2647</td>
</tr>
<tr>
<td style="text-align: left;">SUPERT*</td>
<td style="text-align: center;">0.1029</td>
<td style="text-align: center;">0.5882</td>
<td style="text-align: center;">0.4207</td>
<td style="text-align: center;">0.2353</td>
</tr>
<tr>
<td style="text-align: left;">BLEU</td>
<td style="text-align: center;">0.1176</td>
<td style="text-align: center;">0.0735</td>
<td style="text-align: center;">0.3321</td>
<td style="text-align: center;">0.2206</td>
</tr>
<tr>
<td style="text-align: left;">CHRF</td>
<td style="text-align: center;">$\mathbf{0 . 3 9 7 1}$</td>
<td style="text-align: center;">0.5294</td>
<td style="text-align: center;">0.4649</td>
<td style="text-align: center;">$\mathbf{0 . 5 8 8 2}$</td>
</tr>
<tr>
<td style="text-align: left;">CIDEr</td>
<td style="text-align: center;">0.1176</td>
<td style="text-align: center;">-0.1912</td>
<td style="text-align: center;">-0.0221</td>
<td style="text-align: center;">0.1912</td>
</tr>
<tr>
<td style="text-align: left;">METEOR</td>
<td style="text-align: center;">0.2353</td>
<td style="text-align: center;">$\mathbf{0 . 6 3 2 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 1 2 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2 6 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Length*</td>
<td style="text-align: center;">-0.0294</td>
<td style="text-align: center;">0.4265</td>
<td style="text-align: center;">0.2583</td>
<td style="text-align: center;">0.1618</td>
</tr>
<tr>
<td style="text-align: left;">Novel unigram*</td>
<td style="text-align: center;">0.1471</td>
<td style="text-align: center;">-0.2206</td>
<td style="text-align: center;">-0.1402</td>
<td style="text-align: center;">0.1029</td>
</tr>
<tr>
<td style="text-align: left;">Novel bi-gram*</td>
<td style="text-align: center;">0.0294</td>
<td style="text-align: center;">-0.5441</td>
<td style="text-align: center;">-0.3469</td>
<td style="text-align: center;">-0.1029</td>
</tr>
<tr>
<td style="text-align: left;">Novel tri-gram*</td>
<td style="text-align: center;">0.0294</td>
<td style="text-align: center;">-0.5735</td>
<td style="text-align: center;">-0.3469</td>
<td style="text-align: center;">-0.1324</td>
</tr>
<tr>
<td style="text-align: left;">Repeated unigram*</td>
<td style="text-align: center;">$-\mathbf{0 . 3 8 2 4}$</td>
<td style="text-align: center;">0.1029</td>
<td style="text-align: center;">-0.0664</td>
<td style="text-align: center;">-0.3676</td>
</tr>
<tr>
<td style="text-align: left;">Repeated bi-gram*</td>
<td style="text-align: center;">$-\mathbf{0 . 3 8 2 4}$</td>
<td style="text-align: center;">-0.0147</td>
<td style="text-align: center;">-0.2435</td>
<td style="text-align: center;">$-\mathbf{0 . 4 5 5 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Repeated tri-gram*</td>
<td style="text-align: center;">-0.2206</td>
<td style="text-align: center;">0.1471</td>
<td style="text-align: center;">-0.0221</td>
<td style="text-align: center;">-0.2647</td>
</tr>
<tr>
<td style="text-align: left;">Stats-coverage*</td>
<td style="text-align: center;">-0.1324</td>
<td style="text-align: center;">0.3529</td>
<td style="text-align: center;">0.1550</td>
<td style="text-align: center;">-0.0294</td>
</tr>
<tr>
<td style="text-align: left;">Stats-compression*</td>
<td style="text-align: center;">0.1176</td>
<td style="text-align: center;">-0.4265</td>
<td style="text-align: center;">-0.2288</td>
<td style="text-align: center;">-0.0147</td>
</tr>
<tr>
<td style="text-align: left;">Stats-density*</td>
<td style="text-align: center;">0.1618</td>
<td style="text-align: center;">$\mathbf{0 . 6 4 7 1}$</td>
<td style="text-align: center;">0.3911</td>
<td style="text-align: center;">0.2941</td>
</tr>
</tbody>
</table>
<p>Table 2: Kendall's tau correlation coefficients of expert annotations computed on a system-level along four quality dimensions with automatic metrics using 11 reference summaries per example. ${ }^{*}$ denotes metrics which use the source document. The five most-correlated metrics in each column are bolded.
tractive fragment coverage is the percentage of words in the summary that are from the source article, measuring the extent to which a summary is a derivative of a text. Density is defined as the average length of the extractive fragment to which each summary word belongs. Compression ratio is defined as the word ratio between the articles and its summaries: In addition to these measures, we also include the percentage of n-grams in the summary not found in the input document as a novelty score and the percentage of n-grams in the summary which repeat as a score of redundancy. For a comprehensive explanation of each metric, please refer to the corresponding paper.</p>
<h3>3.2 Summarization models</h3>
<p>We broadly categorize the models included in this study into extractive and abstractive approaches. For each model, we provide a model code ( $\mathrm{M}^{*}$ ) as well as a descriptive model name which will allow for easy matching with the released data.</p>
<h2>Extractive Methods</h2>
<p>M1 - NEUSUM (Zhou et al., 2018) jointly scores and selects sentences by first building a hierarchical representation of a document and considering the partially outputted summary at each time step. M2 - BanditSum (Dong et al., 2018) treats extractive summarization as a contextual bandit problem where the document is the context and the sequence of sentences to include in the summary is the action.
M3 - LATENT (Zhang et al., 2018b) propose a latent variable extractive model which views relevance labels of sentences in a document as binarylatent variables
M4 - REFRESH (Narayan et al., 2018) propose using REINFORCE (Williams, 1992) to extract summaries, approximating the search space during training by limiting to combinations of individually high-scoring sentences.
M5 - RNES (Wu and Hu, 2018) propose a coherence model to capture cross-sentence coherence, combining output from the coherence model and ROUGE scores as a reward in a REINFORCE framework.
M6 - JECS (Xu and Durrett, 2019) first extracts sentences from a document and then scores possible constituency-based compressed units to produce the final compressed summary.
M7 - STRASS (Bouscarrat et al., 2019) extracts a summary by selecting the sentences with the closest embeddings to the document embedding, learning a transformation to maximize the similarity between the summary and the ground truth reference.</p>
<h2>Abstractive Methods</h2>
<p>M8 - Pointer Generator (See et al., 2017) propose a variation of encoder-decoder models, the Pointer Generator Network, where the decoder can choose to generate a word from the vocabulary or copy a word from the input. A coverage mechanism is also proposed to prevent repeatedly attending to the same part of the source document.
M9 - Fast-abs-rl (Chen and Bansal, 2018) propose a model which first extracts salient sentences with a Pointer Network and rewrites these sentences with a Pointer Generator Network. In addition to maximum likelihood training, a ROUGE-L</p>
<p>reward is used to update the extractor via REINFORCE (Williams, 1992).
M10 - Bottom-Up (Gehrmann et al., 2018) introduce a bottom-up approach whereby a content selection model restricts the copy attention distribution of a pretrained Pointer Generator Network during inference.
M11 - Improve-abs (Kryściński et al., 2018) extend the model of Paulus et al. (2017) by augmenting the decoder with an external LSTM language model and add a novelty RL-based objective during training.
M12 - Unified-ext-abs (Hsu et al., 2018) propose to use the probability output of an extractive model as sentence-level attention to modify word-level attention scores of an abstractive model, introducing an inconsistency loss to encourage consistency between these two levels of attention.
M13 - ROUGESal (Pasunuru and Bansal, 2018) propose a keyphrase-based salience reward as well as an entailment-based reward in addition to using a ROUGE-based reward in a REINFORCE setting, optimizing rewards simultaneously in alternate mini-batches.
M14 - Multi-task (Ent + QG ) (Guo et al., 2018) propose question generation and entailment generation as auxiliary tasks in a multi-task framework along with a corresponding multi-task architecture.
M15 - Closed book decoder (Jiang and Bansal, 2018) build upon a Pointer Generator Network by adding copy-less and attention-less decoder during training time to force the encoder to be more selective in encoding salient content.
M16 - SENECA (Sharma et al., 2019) propose to use entity-aware content selection module and an abstractive generation module to generate the final summary.
M17 - T5 (Raffel et al., 2019) perform a systematic study of transfer learning techniques and apply their insights to a set of tasks all framed as text-input to text-output generation tasks, including summarization.
M18 - NeuralTD (Böhm et al., 2019) learn a reward function from 2,500 human judgments which is used in a reinforcement learning setting.
M19 - BertSum-abs (Liu and Lapata, 2019) introduce a novel document-level encoder on top of BERT (Devlin et al., 2019), over which they introduce both an extractive and an abstractive model.
M20 - GPT-2 (Ziegler et al., 2019) build off of</p>
<p>GPT-2 (Radford et al., 2019) and fine-tune the model by using human labels of which of four sampled summaries is the best to direct fine-tuning in a reinforcement learning framework.
M21 - UniLM (Dong et al., 2019) introduce a model pretrained on three language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. It is thus applicable to natural language understanding tasks and generation tasks such as abstractive summarization.
M22 - BART (Lewis et al., 2019) introduce a denoising autoencoder for pretraining sequence to sequence tasks which is applicable to both natural language understanding and generation tasks.
M23 - Pegasus (Zhang et al., 2019a) introduce a model pretrained with a novel objective function designed for summarization by which important sentences are removed from an input document and then generated from the remaining sentences.</p>
<h2>4 Resources</h2>
<p>We now describe the resources collected and released together with this manuscript.</p>
<h3>4.1 Model Outputs</h3>
<p>The model output collection contains summaries associated with 23 recent papers on neural text summarization described in Section 3.2. We obtained a total of 44 model outputs, as many papers include variations of the main model. All models were trained on the CNN/DailyMail news corpus and the collected summaries were generated using the test split of the dataset without constraints limiting the output length. Outputs were solicited from the authors of papers to ensure comparability between results presented in this paper with those in the original works. They are shared publicly with the consent of the authors.</p>
<p>Model outputs were transformed into a unified format and are shared with IDs of the original CNN/DailyMail examples so that generated summaries can be matched with corresponding source articles. Pairing model outputs with original articles was done using a heuristic approach that relied on aligning reference summaries. The pairing process revealed that 38 examples in the CNN/DailyMail test split contained duplicate reference summaries preventing those examples to be correctly aligned. However, this problem involves only $0.3 \%$ of the available data and should not have a significant impact on downstream results.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Pairwise Kendall's Tau correlations for all automatic evaluation metrics.</p>
<p>IDs of duplicate examples are provided together with the data.</p>
<h3>4.2 Evaluation Toolkit</h3>
<p>The evaluation toolkit contains 14 automatic evaluation metrics described in Section 3.1 consolidated into a Python package. The package provides a high-level, easy-to-use interface unifying all of the underlying metrics. For each metric, we implement both evaluate_example and evaluate_batch functions that return the metric's score on example- and corpus-levels accordingly. Function inputs and outputs are also unified across all metrics to streamline multi-metric evaluation and result processing. The toolkit comes with a standard configuration resembling the most popular settings for each of the metrics to enable easy, out-of-the-box use. However, each metric can be further configured using external gin configuration files. We also provide a command-line tool to evaluate a summarization model with several metrics in parallel.</p>
<h3>4.3 Human Annotations</h3>
<p>The collection of human annotations contains summary evaluations of 16 recent neural summarization models solicited from crowd-sourced and expert judges. Annotations were collected for 100 articles randomly picked from the CNN/DailyMail test set. To ensure high quality of annotations, each summary was scored by 5 crowd-sourced and 3 expert workers, amounting to 12800 summary-level annotations. Model outputs were evaluated along the following four dimensions, as in Kryściński et al. (2019):</p>
<p><strong>Coherence</strong> - the collective quality of all sentences. We align this dimension with the DUC quality question (Dang, 2005) of structure and coherence whereby "the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic."</p>
<p><strong>Consistency</strong> - the factual alignment between the summary and the summarized source. A factually consistent summary contains only statements that are entailed by the source document. Annotators were also asked to penalize summaries that contained hallucinated facts.</p>
<p><strong>Fluency</strong> - the quality of individual sentences. Drawing again from the DUC quality guidelines, sentences in the summary "should have no formatting problems, capitalization errors or obviously ungrammatical sentences (e.g., fragments, missing components) that make the text difficult to read."</p>
<p><strong>Relevance</strong> - selection of important content from the source. The summary should include only important information from the source document.</p>
<p>Annotators were instructed to penalize summaries which contained redundancies and excess information.</p>
<p>The data collection interface provided judges with the source article and associated summaries grouped in sets of 5 . Each group of summaries contained the reference summary associated with the source article to establish a common point of reference between groups. Summary grouping and order within groups were randomized for each annotator. Judges were asked to rate the summaries on a Likert scale from 1 to 5 (higher better) along the four mentioned dimensions.</p>
<p>Crowd-sourced annotators were hired through the Amazon Mechanical Turk platform. The hiring criteria were set to a minimum of 10000 approved HITs and an approval rate of $97 \%$ or higher. Geographic constraints for workers were set to United States, United Kingdom, and Australia to ensure that summaries were evaluated by native English speakers. Compensation was carefully calculated to ensure an average wage of 12 USD per hour.</p>
<p>Gillick and Liu (2010) showed that summary judgments obtained through non-experts may differ greatly from expert annotations and could exhibit worse inter-annotator agreement. As a result, in addition to the hired crowd-sourced workers, we enlisted three expert annotators who have written papers on summarization either for academic conferences (2) or as part of a senior thesis (1). The expert annotators were asked to evaluate the same set of summaries under the same instructions as the hired crowd-sourced workers. For expert judgments, we proceeded with two rounds of annotation to correct any obvious mistakes as well as to confirm judgments and ensure a higher quality of annotations. In the second round, annotators were asked to check all examples for which their score of a dimension differed from another annotator by more than 2 points and where the other annotators were within 1 point of each other. In cases where a score differed by more than 2 points for which such a pattern did not exist, all annotators examined the annotation. When re-evaluating examples, judges were allowed to see scores assigned by other expert annotators in the first round of annotations. While such a setting could undermine the wisdom of the crowd and shift the re-assigned scores towards the average judgment from the first round, we encouraged experts to remain critical</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;">Consistency</th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;">Relevance</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CNN/DM Reference Summary</td>
<td style="text-align: center;">3.26</td>
<td style="text-align: center;">4.47</td>
<td style="text-align: center;">4.79</td>
<td style="text-align: center;">3.77</td>
</tr>
<tr>
<td style="text-align: center;">Extractive Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">M0 - LEAD-3</td>
<td style="text-align: center;">4.16</td>
<td style="text-align: center;">4.98</td>
<td style="text-align: center;">4.94</td>
<td style="text-align: center;">4.14</td>
</tr>
<tr>
<td style="text-align: center;">M1 - NEUSUM</td>
<td style="text-align: center;">3.22</td>
<td style="text-align: center;">4.98</td>
<td style="text-align: center;">4.90</td>
<td style="text-align: center;">3.82</td>
</tr>
<tr>
<td style="text-align: center;">M2 - Randb9sum</td>
<td style="text-align: center;">3.28</td>
<td style="text-align: center;">4.99</td>
<td style="text-align: center;">4.83</td>
<td style="text-align: center;">3.81</td>
</tr>
<tr>
<td style="text-align: center;">M5 - RNES</td>
<td style="text-align: center;">3.71</td>
<td style="text-align: center;">4.97</td>
<td style="text-align: center;">4.81</td>
<td style="text-align: center;">4.06</td>
</tr>
<tr>
<td style="text-align: center;">Alternative Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">M8 - Pointer Generator</td>
<td style="text-align: center;">3.29</td>
<td style="text-align: center;">4.65</td>
<td style="text-align: center;">4.79</td>
<td style="text-align: center;">3.55</td>
</tr>
<tr>
<td style="text-align: center;">M9 - Fast-abs-r1</td>
<td style="text-align: center;">2.38</td>
<td style="text-align: center;">4.67</td>
<td style="text-align: center;">4.50</td>
<td style="text-align: center;">3.52</td>
</tr>
<tr>
<td style="text-align: center;">M10 - Bottom-Up</td>
<td style="text-align: center;">2.73</td>
<td style="text-align: center;">4.25</td>
<td style="text-align: center;">4.42</td>
<td style="text-align: center;">3.38</td>
</tr>
<tr>
<td style="text-align: center;">M11 - Improve-abs</td>
<td style="text-align: center;">2.28</td>
<td style="text-align: center;">3.27</td>
<td style="text-align: center;">3.65</td>
<td style="text-align: center;">3.15</td>
</tr>
<tr>
<td style="text-align: center;">M12 - Unified-ext-abs</td>
<td style="text-align: center;">3.60</td>
<td style="text-align: center;">4.96</td>
<td style="text-align: center;">4.85</td>
<td style="text-align: center;">3.85</td>
</tr>
<tr>
<td style="text-align: center;">M13 - ROUGESal</td>
<td style="text-align: center;">3.44</td>
<td style="text-align: center;">4.82</td>
<td style="text-align: center;">4.86</td>
<td style="text-align: center;">3.83</td>
</tr>
<tr>
<td style="text-align: center;">M14 - Multi-task (Eat + QG )</td>
<td style="text-align: center;">3.20</td>
<td style="text-align: center;">4.90</td>
<td style="text-align: center;">4.74</td>
<td style="text-align: center;">3.63</td>
</tr>
<tr>
<td style="text-align: center;">M15 - Closed book decoder</td>
<td style="text-align: center;">3.35</td>
<td style="text-align: center;">4.95</td>
<td style="text-align: center;">4.80</td>
<td style="text-align: center;">3.67</td>
</tr>
<tr>
<td style="text-align: center;">M17 - T5</td>
<td style="text-align: center;">4.00</td>
<td style="text-align: center;">4.93</td>
<td style="text-align: center;">4.93</td>
<td style="text-align: center;">4.23</td>
</tr>
<tr>
<td style="text-align: center;">M20 - GPT-2 (zero-shot) ${ }^{1}$</td>
<td style="text-align: center;">3.63</td>
<td style="text-align: center;">3.40</td>
<td style="text-align: center;">3.97</td>
<td style="text-align: center;">3.30</td>
</tr>
<tr>
<td style="text-align: center;">M22 - BART</td>
<td style="text-align: center;">4.18</td>
<td style="text-align: center;">4.94</td>
<td style="text-align: center;">4.90</td>
<td style="text-align: center;">4.25</td>
</tr>
<tr>
<td style="text-align: center;">M23 - Pegasus (C4)</td>
<td style="text-align: center;">4.16</td>
<td style="text-align: center;">4.91</td>
<td style="text-align: center;">4.80</td>
<td style="text-align: center;">4.26</td>
</tr>
<tr>
<td style="text-align: center;">M23 - Pegasus (dynamic mix)</td>
<td style="text-align: center;">4.09</td>
<td style="text-align: center;">4.85</td>
<td style="text-align: center;">4.79</td>
<td style="text-align: center;">4.27</td>
</tr>
</tbody>
</table>
<p>Table 3: Human ratings of summaries along four evaluation dimensions, averaged over three expert annotators, broken down by extractive and abstractive models. The $\mathrm{M}^{*}$ codes follow the notation described in Section 3.2. The three highestrated models in each column are in bold.
and discuss contested examples when necessary. For completeness, the data collection user interface and additional details regarding the data collection process are presented in the Appendix.</p>
<h2>5 Metric Re-evaluation</h2>
<h3>5.1 Human Annotations</h3>
<p>Considering the concerns raised in previous work (Gillick and Liu, 2010) about the quality differences between crowd-sourced and expert annotations we study this issue using the human annotations collected as part of this work.</p>
<p>To evaluate the inter-annotator agreement of collected crowd-sourced and expert annotations we computed the Krippendorff's alpha coefficient (Krippendorff, 2011). We found the interannotator interval kappa to be below an acceptable range - 0.4920 and 0.4132 for the crowd-sourced workers and the first round of expert annotations accordingly. However, the second round of expert annotations improved the inter-annotator agreement achieving a kappa coefficient of 0.7127 . For further insights, we computed standard deviations of annotator scores within the respective groups and present histograms of those statistics in Figure 2. Plots of crowd-sourced annotations show strong similarities across all evaluated dimensions. Such an effect could be caused by an insufficient distinction made by the annotators between</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Histogram of standard deviations of inter-annotator scores between: crowd-sourced annotations, first round expert annotations, second round expert annotations, respectively.</p>
<p>The 4 scored axes, where the overall quality of a summary biased scores of the individual dimensions. The histograms also show that while the second round of expert annotations lowered the standard deviation of scores and substantially increased inter-annotator agreement, relevance and coherence remained the most disagreed on dimensions between experts. This could be attributed to the subjective nature of relevance and coherence as an evaluation dimensions (kryściński2020).</p>
<p>To assess the similarity of annotations between the crowd-sourced and expert annotators we averaged the assigned scores per example within the respective annotator groups and computed Pearson's correlation coefficient. The statistic returned a value close to 0, indicating no correlation between expert and crowd-sourced judges.</p>
<p>We also manually inspected the human annotations and present examples of annotated summaries, both generated and reference, as well as the differences in human judgments in Table 1a. The first row shows a well written, comprehensive summary. The high quality of the summary is reflected by top scores assigned by expert annotators, while being rated as average by crowd-sourced workers. The second row shows a summary with ambiguous pronoun usage and factual inconsistencies. The errors result in a decrease in coherence, consistency, and relevance scores in the expert annotations, but do not see a corresponding decrease in crowd-worker annotations. The third row presents a factually correct summary that contains token and phrase repetitions. The errors were caught by the expert annotators resulting in a low fluency score, while crowd-sourced annotators incorrectly classified them as issues with factual consistency. These examples again illustrate the disparities in the understanding of evaluated dimensions between judges and underscore our observation above about the uniformity of crowd-sourced annotations; the crowd-sourced annotations tend to be similar across quality dimensions even when distinctions exist, which are captured in the expert annotations.</p>
<p>Results presented in this section highlight the difficulties of crowd-sourcing high-quality annotations and the necessity for protocols for improving human evaluation in text summarization.</p>
<h3>5.2 Automatic Metrics</h3>
<p>Many automatic metrics have been proposed for evaluating both summarization and other text generation models. However, the field lacks a comprehensive study that would offer a consistent side-by-side comparison of their performance. We address this issue with the following experiments.</p>
<p>In Table 2 we show Kendall's tau rank correlations between automatic metrics and human judgments calculated on a system-level following Louis and Nenkova (2013). The statistics were computed using the available expert annotations to avoid possible quality problems associated with crowd-sourced ratings, as highlighted in the previous subsection. Automatic metrics were computed in a multi-reference setting, using the original reference summary included in the CNN/DailyMail dataset and 10 additional summaries coming from kryściński2020, and the length of model outputs was not constrained. We report correlations without differentiating between abstractive and extractive models, as most metrics did not exhibit large differences in correlation when reported separately.</p>
<p>Correlation results show several trends. We find that most metrics have the lowest correlation within the coherence dimension, where the correlation strength can be classified as weak or</p>
<p>moderate. This finding follows intuition as the majority of metrics rely on hard or soft subsequence alignments, which do not measure well the interdependence between consecutive sentences. Low and moderate correlation scores were also found for the relevance dimension. As discussed in the previous subsection, such trends could result from the inherent subjectiveness of the dimension and the difficulty of collecting consistent human annotations. Model correlations increase considerably across the consistency and fluency dimensions. While unexpected, the strong correlation with consistency could be attributed to the low abstractiveness of most neural models, which could increase the effectiveness of metrics using higher-order n-gram overlap, such as ROUGE-3 or Extractive Density. Referring back to the previous subsection, both of the mentioned dimensions achieved high inter-annotator agreement between expert judges which could also positively affect the correlation scores. Additionally, the results show a substantially higher correlation between all evaluated dimensions and ROUGE scores computed for higher-order n-grams in comparison to ROUGE-L, which corroborates with findings of Rankel et al. (2013).</p>
<p>To examine the dependencies between different metrics we computed Kendall's tau rank correlation coefficients, pairwise, between all metrics. Results are presented as a correlation matrix in Figure 1. Following intuition, we observe a strong correlation between all metrics that compute, implicitly or explicitly, the lexical overlap between generated and reference summaries. Metrics measuring the n-gram novelty and repetitiveness show a weak negative correlation with all ROUGErelated metrics. Length as a feature is weakly correlated with most metrics apart from $S^{3}$, BLANC, and SuPERT which might suggest the mentioned metrics favor longer summaries. Worth noting is also the weak correlation of reference-less SummaQA, BLANC, and SuPERT metrics with most other evaluated metrics.</p>
<p>Results presented in this section highlight the evaluation dimensions that are not reliably covered by currently available metrics and pave the way for future work in model evaluation.</p>
<h2>6 Model Re-evaluation</h2>
<p>We now turn to an analysis of model scores across human evaluations and automatic metrics. The
evaluated models were released between 2017 and 2019, represent different approaches to summarization: abstractive, extractive, and hybrid, and their architectures reflect the trends in summarization research. Although in many cases we obtained multiple variants of the same model, in the study we focus on the versions with the highest ROUGE-L scores.</p>
<p>Table 3 contains the results of human evaluation across the four dimensions described in Section 4.3. Scores for ground truth summaries are included as a point of reference. We find that pretrained models such as Pegasus, BART, and T5 consistently performed best on most dimensions. Notably, the mentioned models scored highest on consistency and fluency while obtaining lower scores for relevance and coherence. Scores for extractive models highlight the known shortcomings of such approaches, which are lack of coherence of summaries and issues with selecting relevant content. Abstractive model ratings show an increasing trend with respect to the date of publication. This is a promising result as it suggests that the quality of models is improving with time. Worth noting is also the fact that reference summaries did not score well on consistency, coherence, and relevance. Upon examination of the annotations, we found that the reference summaries often contained extraneous information, such as hyperlinks and click-bait descriptions of other articles. As this information was not present in the source documents nor relevant for the summaries, the annotators interpreted it as hallucinations and assigned lower consistency and relevance scores. Additionally, many reference summaries in the CNN/DailyMail dataset were constructed by naively concatenating bullet-point summaries into contiguous sequences. Such processing steps negatively affected the coherence of examples. Similar trends in human studies of reference summaries were reported by Stiennon et al. (2020). Examples of noisy reference summaries are shown in Table 1b.</p>
<p>Table 4 show scores for model outputs across all automatic evaluation metrics. Parameters of metrics used in this study can be found in the evaluation toolkit repository listed in Section 1. The results align with insights coming from the human evaluation of models. We found that for most metrics, the highest scores were assigned to large models pretrained on vast quantities of data.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">ROUGE-1/2/3/4/LowVar</th>
<th style="text-align: center;">ROUGE-WE+1/2/3</th>
<th style="text-align: center;">$\boldsymbol{S}^{\mathbf{3}}$ (geology)</th>
<th style="text-align: center;">Bottlomer</th>
<th style="text-align: center;">Mover/boom</th>
<th style="text-align: center;">SummaQA</th>
<th style="text-align: center;">SMS</th>
<th style="text-align: center;">BLANC</th>
<th style="text-align: center;">SUPPORT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">KEGE LEAD-3</td>
<td style="text-align: center;">0.3904 / 0.1746 / 0.0990 / 0.0647 / 0.3606 / 0.1377 / 0.2072</td>
<td style="text-align: center;">0.4069 / 0.1249 / 0.2172</td>
<td style="text-align: center;">0.5395 / 0.6328</td>
<td style="text-align: center;">0.3742</td>
<td style="text-align: center;">0.1679</td>
<td style="text-align: center;">0.1652</td>
<td style="text-align: center;">0.1059</td>
<td style="text-align: center;">0.0480</td>
<td style="text-align: center;">0.7259</td>
</tr>
<tr>
<td style="text-align: center;">M1 - NEGSUM</td>
<td style="text-align: center;">0.4130 / 0.1893 / 0.1109 / 0.0742 / 0.3768 / 0.1495 / 0.2196</td>
<td style="text-align: center;">0.4180 / 0.2402 / 0.2316</td>
<td style="text-align: center;">0.5562 / 0.6589</td>
<td style="text-align: center;">0.3955</td>
<td style="text-align: center;">0.1839</td>
<td style="text-align: center;">0.1700</td>
<td style="text-align: center;">0.1062</td>
<td style="text-align: center;">0.1087</td>
<td style="text-align: center;">0.7010</td>
</tr>
<tr>
<td style="text-align: center;">M2 - BanditSum</td>
<td style="text-align: center;">0.4137 / 0.1668 / 0.1086 / 0.0721 / 0.3759 / 0.1513 / 0.2139</td>
<td style="text-align: center;">0.4165 / 0.2343 / 0.2300</td>
<td style="text-align: center;">0.5339 / 0.6304</td>
<td style="text-align: center;">0.3938</td>
<td style="text-align: center;">0.1615</td>
<td style="text-align: center;">0.1712</td>
<td style="text-align: center;">0.1058</td>
<td style="text-align: center;">0.0409</td>
<td style="text-align: center;">0.7018</td>
</tr>
<tr>
<td style="text-align: center;">M1 - LATENT</td>
<td style="text-align: center;">0.4136 / 0.1867 / 0.1085 / 0.0721 / 0.3757 / 0.1512 / 0.2138</td>
<td style="text-align: center;">0.4194 / 0.2384 / 0.2299</td>
<td style="text-align: center;">0.5337 / 0.6305</td>
<td style="text-align: center;">0.3936</td>
<td style="text-align: center;">0.1814</td>
<td style="text-align: center;">0.1645</td>
<td style="text-align: center;">0.1058</td>
<td style="text-align: center;">0.0502</td>
<td style="text-align: center;">0.7026</td>
</tr>
<tr>
<td style="text-align: center;">M4 - REFRESH</td>
<td style="text-align: center;">0.3972 / 0.1607 / 0.1042 / 0.0690 / 0.3621 / 0.1240 / 0.2129</td>
<td style="text-align: center;">0.4023 / 0.2314 / 0.2230</td>
<td style="text-align: center;">0.6395 / 0.7124</td>
<td style="text-align: center;">0.3903</td>
<td style="text-align: center;">0.1720</td>
<td style="text-align: center;">0.1544</td>
<td style="text-align: center;">0.1080</td>
<td style="text-align: center;">0.1496</td>
<td style="text-align: center;">0.7526</td>
</tr>
<tr>
<td style="text-align: center;">M5 - RMES</td>
<td style="text-align: center;">0.4068 / 0.1878 / 0.1102 / 0.0736 / 0.3719 / 0.1446 / 0.2165</td>
<td style="text-align: center;">0.4153 / 0.2395 / 0.2317</td>
<td style="text-align: center;">0.6082 / 0.6894</td>
<td style="text-align: center;">0.3997</td>
<td style="text-align: center;">0.1652</td>
<td style="text-align: center;">0.1794</td>
<td style="text-align: center;">0.1107</td>
<td style="text-align: center;">0.1232</td>
<td style="text-align: center;">0.7434</td>
</tr>
<tr>
<td style="text-align: center;">M6 - JECS</td>
<td style="text-align: center;">0.4144 / 0.1846 / 0.1063 / 0.0699 / 0.3760 / 0.1485 / 0.2135</td>
<td style="text-align: center;">0.4200 / 0.2371 / 0.2283</td>
<td style="text-align: center;">0.5327 / 0.6284</td>
<td style="text-align: center;">0.3925</td>
<td style="text-align: center;">0.1805</td>
<td style="text-align: center;">0.1644</td>
<td style="text-align: center;">0.1045</td>
<td style="text-align: center;">0.1044</td>
<td style="text-align: center;">0.6946</td>
</tr>
<tr>
<td style="text-align: center;">M7 - STRASS</td>
<td style="text-align: center;">0.3377 / 0.1237 / 0.0650 / 0.0416 / 0.2790 / 0.1052 / 0.1559</td>
<td style="text-align: center;">0.3477 / 0.1757 / 0.1656</td>
<td style="text-align: center;">0.3632 / 0.4939</td>
<td style="text-align: center;">0.3080</td>
<td style="text-align: center;">0.1079</td>
<td style="text-align: center;">0.1367</td>
<td style="text-align: center;">0.1023</td>
<td style="text-align: center;">0.1042</td>
<td style="text-align: center;">0.6566</td>
</tr>
<tr>
<td style="text-align: center;">kEch - KEGE (geology)</td>
<td style="text-align: center;">0.3921 / 0.1723 / 0.1003 / 0.0674 / 0.3590 / 0.1435 / 0.1990</td>
<td style="text-align: center;">0.3990 / 0.2226 / 0.2128</td>
<td style="text-align: center;">0.4328 / 0.5567</td>
<td style="text-align: center;">0.3948</td>
<td style="text-align: center;">0.1645</td>
<td style="text-align: center;">0.1596</td>
<td style="text-align: center;">0.0974</td>
<td style="text-align: center;">0.0784</td>
<td style="text-align: center;">0.6901</td>
</tr>
<tr>
<td style="text-align: center;">M9 - Fast-abs-rl</td>
<td style="text-align: center;">0.4037 / 0.1774 / 0.0975 / 0.0616 / 0.3806 / 0.1439 / 0.2112</td>
<td style="text-align: center;">0.4123 / 0.2302 / 0.2184</td>
<td style="text-align: center;">0.4818 / 0.5865</td>
<td style="text-align: center;">0.3918</td>
<td style="text-align: center;">0.1748</td>
<td style="text-align: center;">0.1431</td>
<td style="text-align: center;">0.0847</td>
<td style="text-align: center;">0.0855</td>
<td style="text-align: center;">0.6125</td>
</tr>
<tr>
<td style="text-align: center;">M10 - Bottom-Up</td>
<td style="text-align: center;">0.4124 / 0.1870 / 0.1064 / 0.0693 / 0.3815 / 0.1563 / 0.2084</td>
<td style="text-align: center;">0.4192 / 0.2406 / 0.2313</td>
<td style="text-align: center;">0.4458 / 0.5655</td>
<td style="text-align: center;">0.3964</td>
<td style="text-align: center;">0.1645</td>
<td style="text-align: center;">0.1408</td>
<td style="text-align: center;">0.0925</td>
<td style="text-align: center;">0.0576</td>
<td style="text-align: center;">0.6092</td>
</tr>
<tr>
<td style="text-align: center;">M11 - Improve-abs</td>
<td style="text-align: center;">0.3985 / 0.1720 / 0.0927 / 0.0567 / 0.3739 / 0.1431 / 0.2073</td>
<td style="text-align: center;">0.4045 / 0.2300 / 0.2228</td>
<td style="text-align: center;">0.4899 / 0.5897</td>
<td style="text-align: center;">0.3826</td>
<td style="text-align: center;">0.1652</td>
<td style="text-align: center;">0.1341</td>
<td style="text-align: center;">0.0816</td>
<td style="text-align: center;">0.0777</td>
<td style="text-align: center;">0.5972</td>
</tr>
<tr>
<td style="text-align: center;">M12 - Unified-scr-abs</td>
<td style="text-align: center;">0.4038 / 0.1790 / 0.1039 / 0.0693 / 0.3675 / 0.1484 / 0.2074</td>
<td style="text-align: center;">0.4007 / 0.2299 / 0.2206</td>
<td style="text-align: center;">0.4936 / 0.5995</td>
<td style="text-align: center;">0.3832</td>
<td style="text-align: center;">0.1730</td>
<td style="text-align: center;">0.1530</td>
<td style="text-align: center;">0.1038</td>
<td style="text-align: center;">0.0962</td>
<td style="text-align: center;">0.6826</td>
</tr>
<tr>
<td style="text-align: center;">M13 - BOULESd</td>
<td style="text-align: center;">0.4016 / 0.1797 / 0.1053 / 0.0709 / 0.3679 / 0.1497 / 0.2058</td>
<td style="text-align: center;">0.4078 / 0.2294 / 0.2190</td>
<td style="text-align: center;">0.4643 / 0.5795</td>
<td style="text-align: center;">0.3837</td>
<td style="text-align: center;">0.1722</td>
<td style="text-align: center;">0.1475</td>
<td style="text-align: center;">0.1009</td>
<td style="text-align: center;">0.0882</td>
<td style="text-align: center;">0.6570</td>
</tr>
<tr>
<td style="text-align: center;">M14 - Multi-task (Est + QG )</td>
<td style="text-align: center;">0.3952 / 0.1758 / 0.1037 / 0.0703 / 0.3625 / 0.1476 / 0.2007</td>
<td style="text-align: center;">0.4015 / 0.2253 / 0.2149</td>
<td style="text-align: center;">0.4246 / 0.5515</td>
<td style="text-align: center;">0.3759</td>
<td style="text-align: center;">0.1670</td>
<td style="text-align: center;">0.1360</td>
<td style="text-align: center;">0.0982</td>
<td style="text-align: center;">0.0648</td>
<td style="text-align: center;">0.6380</td>
</tr>
<tr>
<td style="text-align: center;">M15 - Closed-look-deceder</td>
<td style="text-align: center;">0.3976 / 0.1760 / 0.1031 / 0.0686 / 0.3636 / 0.1472 / 0.2033</td>
<td style="text-align: center;">0.4039 / 0.2263 / 0.2160</td>
<td style="text-align: center;">0.4591 / 0.5757</td>
<td style="text-align: center;">0.3783</td>
<td style="text-align: center;">0.1699</td>
<td style="text-align: center;">0.1456</td>
<td style="text-align: center;">0.1009</td>
<td style="text-align: center;">0.0896</td>
<td style="text-align: center;">0.6612</td>
</tr>
<tr>
<td style="text-align: center;">M16 - SENECA</td>
<td style="text-align: center;">0.4151 / 0.1836 / 0.1052 / 0.0681 / 0.3806 / 0.1520 / 0.2112</td>
<td style="text-align: center;">0.4211 / 0.2369 / 0.2282</td>
<td style="text-align: center;">0.4735 / 0.5836</td>
<td style="text-align: center;">0.3907</td>
<td style="text-align: center;">0.1811</td>
<td style="text-align: center;">0.1466</td>
<td style="text-align: center;">0.1005</td>
<td style="text-align: center;">0.0892</td>
<td style="text-align: center;">0.6518</td>
</tr>
<tr>
<td style="text-align: center;">M17 - T5</td>
<td style="text-align: center;">0.4479 / 0.2205 / 0.1336 / 0.0920 / 0.4172 / 0.1879 / 0.2291</td>
<td style="text-align: center;">0.4543 / 0.2723 / 0.2631</td>
<td style="text-align: center;">0.5168 / 0.6294</td>
<td style="text-align: center;">0.4458</td>
<td style="text-align: center;">0.2376</td>
<td style="text-align: center;">0.1437</td>
<td style="text-align: center;">0.1046</td>
<td style="text-align: center;">0.0773</td>
<td style="text-align: center;">0.6059</td>
</tr>
<tr>
<td style="text-align: center;">M18 - NeuralTD</td>
<td style="text-align: center;">0.4004 / 0.1762 / 0.1000 / 0.0630 / 0.3723 / 0.1452 / 0.2085</td>
<td style="text-align: center;">0.4063 / 0.2277 / 0.2187</td>
<td style="text-align: center;">0.4546 / 0.5975</td>
<td style="text-align: center;">0.3949</td>
<td style="text-align: center;">0.1697</td>
<td style="text-align: center;">0.1440</td>
<td style="text-align: center;">0.0916</td>
<td style="text-align: center;">0.0859</td>
<td style="text-align: center;">0.6290</td>
</tr>
<tr>
<td style="text-align: center;">M19 - Berthan-abs</td>
<td style="text-align: center;">0.4163 / 0.1944 / 0.1156 / 0.0785 / 0.3554 / 0.1625 / 0.1979</td>
<td style="text-align: center;">0.4230 / 0.2454 / 0.2351</td>
<td style="text-align: center;">0.4664 / 0.5855</td>
<td style="text-align: center;">0.3855</td>
<td style="text-align: center;">0.1859</td>
<td style="text-align: center;">0.1585</td>
<td style="text-align: center;">0.1071</td>
<td style="text-align: center;">0.0815</td>
<td style="text-align: center;">0.6116</td>
</tr>
<tr>
<td style="text-align: center;">M20 - GPT-2 (supervised)</td>
<td style="text-align: center;">0.3981 / 0.1738 / 0.0993 / 0.0649 / 0.3674 / 0.1470 / 0.2006</td>
<td style="text-align: center;">0.4048 / 0.2268 / 0.2179</td>
<td style="text-align: center;">0.4889 / 0.5373</td>
<td style="text-align: center;">0.3915</td>
<td style="text-align: center;">0.1750</td>
<td style="text-align: center;">0.1299</td>
<td style="text-align: center;">0.0930</td>
<td style="text-align: center;">0.0705</td>
<td style="text-align: center;">0.6053</td>
</tr>
<tr>
<td style="text-align: center;">M21 - CoILM</td>
<td style="text-align: center;">0.4306 / 0.2044 / 0.1218 / 0.0824 / 0.4013 / 0.1714 / 0.2228</td>
<td style="text-align: center;">0.4369 / 0.2567 / 0.2483</td>
<td style="text-align: center;">0.5143 / 0.6210</td>
<td style="text-align: center;">0.4122</td>
<td style="text-align: center;">0.2112</td>
<td style="text-align: center;">0.1855</td>
<td style="text-align: center;">0.0937</td>
<td style="text-align: center;">0.0841</td>
<td style="text-align: center;">0.6100</td>
</tr>
<tr>
<td style="text-align: center;">M22 - BART</td>
<td style="text-align: center;">0.4416 / 0.2128 / 0.1205 / 0.0800 / 0.4100 / 0.1818 / 0.2266</td>
<td style="text-align: center;">0.4472 / 0.2646 / 0.2556</td>
<td style="text-align: center;">0.5116 / 0.6215</td>
<td style="text-align: center;">0.4264</td>
<td style="text-align: center;">0.2259</td>
<td style="text-align: center;">0.1457</td>
<td style="text-align: center;">0.1037</td>
<td style="text-align: center;">0.0822</td>
<td style="text-align: center;">0.6184</td>
</tr>
<tr>
<td style="text-align: center;">M23 - Pegasus (dynamic mix)</td>
<td style="text-align: center;">0.4407 / 0.2155 / 0.1307 / 0.0901 / 0.4101 / 0.1825 / 0.2269</td>
<td style="text-align: center;">0.4471 / 0.2668 / 0.2575</td>
<td style="text-align: center;">0.5099 / 0.6235</td>
<td style="text-align: center;">0.4369</td>
<td style="text-align: center;">0.2283</td>
<td style="text-align: center;">0.1422</td>
<td style="text-align: center;">0.1040</td>
<td style="text-align: center;">0.0797</td>
<td style="text-align: center;">0.6046</td>
</tr>
<tr>
<td style="text-align: center;">M23 - Pegasus (large news)</td>
<td style="text-align: center;">0.4408 / 0.2147 / 0.1295 / 0.0889 / 0.4103 / 0.1821 / 0.2273</td>
<td style="text-align: center;">0.4473 / 0.2663 / 0.2568</td>
<td style="text-align: center;">0.5293 / 0.6372</td>
<td style="text-align: center;">0.4377</td>
<td style="text-align: center;">0.2286</td>
<td style="text-align: center;">0.1497</td>
<td style="text-align: center;">0.1049</td>
<td style="text-align: center;">0.0845</td>
<td style="text-align: center;">0.6148</td>
</tr>
</tbody>
</table>
<p>(a) Model scores from summarization-specific evaluation metrics.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">CHRF</th>
<th style="text-align: center;">CIDEs</th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;">Length</th>
<th style="text-align: center;">Stats (cov/compr/den)</th>
<th style="text-align: center;">Repeated-11/2/3)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Extractive Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">M0 - LEAD-3</td>
<td style="text-align: center;">11.4270</td>
<td style="text-align: center;">0.3892</td>
<td style="text-align: center;">0.2125</td>
<td style="text-align: center;">0.2141</td>
<td style="text-align: center;">87.4475</td>
<td style="text-align: center;">0.9825 / 9.6262 / 57.8001</td>
<td style="text-align: center;">0.2086 / 0.0310 / 0.0310</td>
</tr>
<tr>
<td style="text-align: center;">M1 - NEGSUM</td>
<td style="text-align: center;">12.7784</td>
<td style="text-align: center;">0.3946</td>
<td style="text-align: center;">0.2832</td>
<td style="text-align: center;">0.2183</td>
<td style="text-align: center;">84.4075</td>
<td style="text-align: center;">0.9819 / 9.8047 / 52.8574</td>
<td style="text-align: center;">0.2325 / 0.0531 / 0.0531</td>
</tr>
<tr>
<td style="text-align: center;">M2 - BanditSum</td>
<td style="text-align: center;">12.9761</td>
<td style="text-align: center;">0.3897</td>
<td style="text-align: center;">0.3305</td>
<td style="text-align: center;">0.2124</td>
<td style="text-align: center;">78.5279</td>
<td style="text-align: center;">0.9836 / 10.2810 / 40.4265</td>
<td style="text-align: center;">0.2384 / 0.0573 / 0.0573</td>
</tr>
<tr>
<td style="text-align: center;">M1 - LATENT</td>
<td style="text-align: center;">12.9715</td>
<td style="text-align: center;">0.3897</td>
<td style="text-align: center;">0.3305</td>
<td style="text-align: center;">0.2123</td>
<td style="text-align: center;">78.5279</td>
<td style="text-align: center;">0.9834 / 10.2809 / 40.4095</td>
<td style="text-align: center;">0.2384 / 0.0573 / 0.0573</td>
</tr>
<tr>
<td style="text-align: center;">M4 - REFRESH</td>
<td style="text-align: center;">10.6568</td>
<td style="text-align: center;">0.4526</td>
<td style="text-align: center;">0.0677</td>
<td style="text-align: center;">0.2395</td>
<td style="text-align: center;">114.5684</td>
<td style="text-align: center;">0.9850 / 7.1059 / 53.1928</td>
<td style="text-align: center;">0.2127 / 0.0289 / 0.0289</td>
</tr>
<tr>
<td style="text-align: center;">M5 - RMES</td>
<td style="text-align: center;">11.2203</td>
<td style="text-align: center;">0.4062</td>
<td style="text-align: center;">0.1559</td>
<td style="text-align: center;">0.2300</td>
<td style="text-align: center;">99.9199</td>
<td style="text-align: center;">0.9938 / 7.9032 / 67.7089</td>
<td style="text-align: center;">0.2451 / 0.0540 / 0.0540</td>
</tr>
<tr>
<td style="text-align: center;">M6 - JECS</td>
<td style="text-align: center;">12.5659</td>
<td style="text-align: center;">0.4310</td>
<td style="text-align: center;">0.3060</td>
<td style="text-align: center;">0.2122</td>
<td style="text-align: center;">79.7797</td>
<td style="text-align: center;">0.9874 / 10.1111 / 26.6943</td>
<td style="text-align: center;">0.2041 / 0.0327 / 0.0327</td>
</tr>
<tr>
<td style="text-align: center;">M7 - STRASS</td>
<td style="text-align: center;">7.8330</td>
<td style="text-align: center;">0.3330</td>
<td style="text-align: center;">0.2945</td>
<td style="text-align: center;">0.1607</td>
<td style="text-align: center;">76.4859</td>
<td style="text-align: center;">0.9969 / 12.7835 / 59.9498</td>
<td style="text-align: center;">0.1864 / 0.0343 / 0.0343</td>
</tr>
<tr>
<td style="text-align: center;">Abstractive Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">M8 - Pointer Generator</td>
<td style="text-align: center;">13.8247</td>
<td style="text-align: center;">0.3567</td>
<td style="text-align: center;">0.5065</td>
<td style="text-align: center;">0.1860</td>
<td style="text-align: center;">63.5211</td>
<td style="text-align: center;">0.9957 / 13.1940 / 26.0880</td>
<td style="text-align: center;">0.2015 / 0.0375 / 0.0375</td>
</tr>
<tr>
<td style="text-align: center;">M9 - Fast-abs-rl</td>
<td style="text-align: center;">12.9812</td>
<td style="text-align: center;">0.3778</td>
<td style="text-align: center;">0.4329</td>
<td style="text-align: center;">0.2014</td>
<td style="text-align: center;">70.8660</td>
<td style="text-align: center;">0.9860 / 11.0141 / 9.9859</td>
<td style="text-align: center;">0.2157 / 0.0370 / 0.0370</td>
</tr>
<tr>
<td style="text-align: center;">M10 - Bottom-Up</td>
<td style="text-align: center;">15.1255</td>
<td style="text-align: center;">0.3523</td>
<td style="text-align: center;">0.6176</td>
<td style="text-align: center;">0.1887</td>
<td style="text-align: center;">56.5715</td>
<td style="text-align: center;">0.9811 / 14.7771 / 12.6181</td>
<td style="text-align: center;">0.1856 / 0.0211 / 0.0211</td>
</tr>
<tr>
<td style="text-align: center;">M11 - Improve-abs</td>
<td style="text-align: center;">11.9816</td>
<td style="text-align: center;">0.3715</td>
<td style="text-align: center;">0.3356</td>
<td style="text-align: center;">0.2005</td>
<td style="text-align: center;">75.9512</td>
<td style="text-align: center;">0.9674 / 10.6043 / 8.9755</td>
<td style="text-align: center;">0.2499 / 0.0542 / 0.0542</td>
</tr>
<tr>
<td style="text-align: center;">M12 - Unified-scr-abs</td>
<td style="text-align: center;">12.8457</td>
<td style="text-align: center;">0.3786</td>
<td style="text-align: center;">0.3851</td>
<td style="text-align: center;">0.2017</td>
<td style="text-align: center;">74.4663</td>
<td style="text-align: center;">0.9868 / 10.7510 / 33.1106</td>
<td style="text-align: center;">0.2177 / 0.0493 / 0.0493</td>
</tr>
<tr>
<td style="text-align: center;">M13 - BOULESd</td>
<td style="text-align: center;">13.8882</td>
<td style="text-align: center;">0.3668</td>
<td style="text-align: center;">0.4746</td>
<td style="text-align: center;">0.1936</td>
<td style="text-align: center;">66.5575</td>
<td style="text-align: center;">0.9853 / 13.0369 / 25.2893</td>
<td style="text-align: center;">0.2102 / 0.0458 / 0.0458</td>
</tr>
<tr>
<td style="text-align: center;">M14 - Multi-task (Est + QG )</td>
<td style="text-align: center;">14.5276</td>
<td style="text-align: center;">0.3539</td>
<td style="text-align: center;">0.5749</td>
<td style="text-align: center;">0.1831</td>
<td style="text-align: center;">60.0294</td>
<td style="text-align: center;">0.9853 / 14.1828 / 22.2296</td>
<td style="text-align: center;">0.1985 / 0.0411 / 0.0411</td>
</tr>
<tr>
<td style="text-align: center;">M15 - Closed-look-deceder</td>
<td style="text-align: center;">13.4158</td>
<td style="text-align: center;">0.3675</td>
<td style="text-align: center;">0.4648</td>
<td style="text-align: center;">0.1925</td>
<td style="text-align: center;">68.2858</td>
<td style="text-align: center;">0.9866 / 12.0588 / 27.3686</td>
<td style="text-align: center;">0.2074 / 0.0444 / 0.0444</td>
</tr>
<tr>
<td style="text-align: center;">M16 - SENECA</td>
<td style="text-align: center;">13.7676</td>
<td style="text-align: center;">0.3660</td>
<td style="text-align: center;">0.5233</td>
<td style="text-align: center;">0.1966</td>
<td style="text-align: center;">64.9710</td>
<td style="text-align: center;">0.9880 / 12.3610 / 16.7640</td>
<td style="text-align: center;">0.2146 / 0.0303 / 0.0303</td>
</tr>
<tr>
<td style="text-align: center;">M17 - T5</td>
<td style="text-align: center;">19.3891</td>
<td style="text-align: center;">0.3833</td>
<td style="text-align: center;">0.7763</td>
<td style="text-align: center;">0.2140</td>
<td style="text-align: center;">59.5288</td>
<td style="text-align: center;">0.9775 / 14.2002 / 12.9565</td>
<td style="text-align: center;">0.1810 / 0.0209 / 0.0209</td>
</tr>
<tr>
<td style="text-align: center;">M18 - NeuralTD</td>
<td style="text-align: center;">12.9241</td>
<td style="text-align: center;">0.3783</td>
<td style="text-align: center;">0.3543</td>
<td style="text-align: center;">0.2038</td>
<td style="text-align: center;">74.4633</td>
<td style="text-align: center;">0.9830 / 10.7768 / 12.4443</td>
<td style="text-align: center;">0.2045 / 0.0981 / 0.0901</td>
</tr>
<tr>
<td style="text-align: center;">M19 - Berthan-abs</td>
<td style="text-align: center;">14.9525</td>
<td style="text-align: center;">0.3649</td>
<td style="text-align: center;">0.6240</td>
<td style="text-align: center;">0.1876</td>
<td style="text-align: center;">60.8893</td>
<td style="text-align: center;">0.9517 / 13.9197 / 12.3254</td>
<td style="text-align: center;">0.1697 / 0.0156 / 0.0156</td>
</tr>
<tr>
<td style="text-align: center;">M20 - GPT-2 (supervised)</td>
<td style="text-align: center;">13.9364</td>
<td style="text-align: center;">0.3678</td>
<td style="text-align: center;">0.5787</td>
<td style="text-align: center;">0.1759</td>
<td style="text-align: center;">51.8352</td>
<td style="text-align: center;">0.9791 / 15.9839 / 15.4999</td>
<td style="text-align: center;">0.1875 / 0.0362 / 0.0362</td>
</tr>
<tr>
<td style="text-align: center;">M21 - CoILM</td>
<td style="text-align: center;">15.5736</td>
<td style="text-align: center;">0.4230</td>
<td style="text-align: center;">0.5594</td>
<td style="text-align: center;">0.2084</td>
<td style="text-align: center;">67.3960</td>
<td style="text-align: center;">0.9685 / 11.3672 / 11.7960</td>
<td style="text-align: center;">0.1722 / 0.0180 / 0.0180</td>
</tr>
<tr>
<td style="text-align: center;">M22 - BART</td>
<td style="text-align: center;">17.1005</td>
<td style="text-align: center;">0.4271</td>
<td style="text-align: center;">0.7573</td>
<td style="text-align: center;">0.2105</td>
<td style="text-align: center;">62.2989</td>
<td style="text-align: center;">0.9771 / 12.8811 / 15.2999</td>
<td style="text-align: center;">0.1627 / 0.0127 / 0.0127</td>
</tr>
<tr>
<td style="text-align: center;">M23 - Pegasus (dynamic mix)</td>
<td style="text-align: center;">18.6517</td>
<td style="text-align: center;">0.4261</td>
<td style="text-align: center;">0.7280</td>
<td style="text-align: center;">0.2131</td>
<td style="text-align: center;">64.1348</td>
<td style="text-align: center;">0.9438 / 13.7208 / 11.6003</td>
<td style="text-align: center;">0.1855 / 0.0355 / 0.0091</td>
</tr>
<tr>
<td style="text-align: center;">M23 - Pegasus (large news)</td>
<td style="text-align: center;">17.8102</td>
<td style="text-align: center;">0.3912</td>
<td style="text-align: center;">0.6595</td>
<td style="text-align: center;">0.2189</td>
<td style="text-align: center;">66.7559</td>
<td style="text-align: center;">0.9814/12.9473/14.9850</td>
<td style="text-align: center;">0.1883/0.0251/0.0251</td>
</tr>
</tbody>
</table>
<p>(b) Model scores from other text generation evaluation metrics.</p>
<p>Table 4: Model scores from automatic evaluation metrics available in the evaluation toolkit. The five highest scores for each metric (and lowest for Length and Repeated-1/2/3) are bolded.</p>
<p>However, several metrics, such as $\boldsymbol{S}^{\mathbf{3}}$, SummaQA, SMS, CHRF, and METEOR tended to favor extractive models, assigning the highest scores to their outputs.</p>
<p>Presented results provide a comprehensive perspective on the current state of the field and highlight directions for future modeling work.</p>
<h2>7 Conclusions</h2>
<p>We introduced SummEval, a set of resources for summarization model and evaluation research that include: a collection of summaries generated by recent summarization models on the CNN/DailyMail dataset, an extensible and unified toolkit for summarization model evaluation, and a
diverse collection of human annotations of model outputs collected from the crowd-source and expert annotators. Using the accumulated resources we re-evaluated a broad selection of current models and evaluation metrics in a consistent and comprehensive manner. We hope that this work will prove to be a valuable resource for future research on text summarization evaluation and models. We also encourage the research community to join our efforts by contributing model outputs and extending the evaluation toolkit with new metrics.</p>
<h2>8 Acknowledgements</h2>
<p>We thank all authors for sharing model outputs and Tony Wong for assistance with annotations.</p>
<h2>References</h2>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</p>
<p>Florian Böhm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych. 2019. Better rewards yield better summaries: Learning to summarise without references. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3110-3120, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Léo Bouscarrat, Antoine Bonnefoy, Thomas Peel, and Cécile Pereira. 2019. STRASS: A light and effective method for extractive summarization based on sentence embeddings. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 243-252, Florence, Italy. Association for Computational Linguistics.</p>
<p>Arun Chaganty, Stephen Mussmann, and Percy Liang. 2018. The price of debiasing automatic metrics in natural language evalaution. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 643-653, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Yen-Chun Chen and Mohit Bansal. 2018. Fast abstractive summarization with reinforce-selected sentence rewriting. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 675-686, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Elizabeth Clark, Asli Celikyilmaz, and Noah A. Smith. 2019. Sentence mover's similarity: Automatic evaluation for multi-sentence texts. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2748-2760, Florence, Italy. Association for Computational Linguistics.</p>
<p>Arman Cohan and Nazli Goharian. 2016. Revisiting summarization evaluation for scientific articles. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 806-813, Portorož, Slovenia. European Language Resources Association (ELRA).</p>
<p>Hoa Trang Dang. 2005. Overview of duc 2005. In Proceedings of the document understanding conference, volume 2005, pages 1-12.</p>
<p>Hoa Trang Dang and Karolina Owczarzak. 2008. Overview of the tac 2008 update summarization task. In TAC.</p>
<p>Hoa Trang Dang and Karolina Owczarzak. 2009. Overview of the tac 2009 summarization track. In proceedings of the Text Analysis Conference.</p>
<p>Franck Dernoncourt, Mohammad Ghassemi, and Walter Chang. 2018. A repository of corpora for summarization. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).</p>
<p>Daniel Deutsch and Dan Roth. 2020. SacreROUGE: An Open-Source Library for Using and Developing Summarization Evaluation Metrics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. In Advances in Neural Information Processing Systems, pages 13042-13054.</p>
<p>Yue Dong, Yikang Shen, Eric Crawford, Herke van Hoof, and Jackie Chi Kit Cheung. 2018.</p>
<p>BanditSum: Extractive summarization as a contextual bandit. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3739-3748, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Esin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5055-5070, Online. Association for Computational Linguistics.</p>
<p>Kavita Ganesan. 2015. Rouge 2.0: Updated and improved measures for evaluation of summarization tasks.</p>
<p>Yang Gao, Wei Zhao, and Steffen Eger. 2020. SUPERT: towards new frontiers in unsupervised evaluation metrics for multi-document summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 1347-1354. Association for Computational Linguistics.</p>
<p>Sebastian Gehrmann, Yuntian Deng, and Alexander Rush. 2018. Bottom-up abstractive summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4098-4109, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Dan Gillick and Yang Liu. 2010. Non-expert evaluation of summarization systems is risky. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk, pages 148151, Los Angeles. Association for Computational Linguistics.</p>
<p>Yvette Graham. 2015. Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 128-137, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A dataset of 1.3 million summaries
with diverse extractive strategies. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 708-719, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Han Guo, Ramakanth Pasunuru, and Mohit Bansal. 2018. Soft layer-specific multi-task summarization with entailment and question generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 687-697, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Hardy Hardy, Shashi Narayan, and Andreas Vlachos. 2019. HighRES: Highlight-based reference-less evaluation of summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3381-3392, Florence, Italy. Association for Computational Linguistics.</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in neural information processing systems, pages $1693-1701$.</p>
<p>Wan-Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui Min, Jing Tang, and Min Sun. 2018. A unified model for extractive and abstractive summarization using inconsistency loss. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 132-141, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Yichen Jiang and Mohit Bansal. 2018. Closedbook training to improve summarization encoder memory. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4067-4077, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Chris Kedzie, Kathleen McKeown, and Hal Daumé III. 2018. Content selection in deep learning models of summarization. In Proceedings of the 2018 Conference on Empiri-</p>
<p>cal Methods in Natural Language Processing, pages 1818-1828, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Klaus Krippendorff. 2011. Computing krippendorff's alpha-reliability.</p>
<p>Wojciech Kryściński, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Neural text summarization: A critical evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 540-551, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Wojciech Kryściński, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332-9346, Online. Association for Computational Linguistics.</p>
<p>Wojciech Kryściński, Romain Paulus, Caiming Xiong, and Richard Socher. 2018. Improving abstraction in text summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1808-1817, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. 2015. From word embeddings to document distances. In International conference on machine learning, pages 957-966.</p>
<p>Alon Lavie and Abhaya Agarwal. 2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 228231, Prague, Czech Republic. Association for Computational Linguistics.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.</p>
<p>Chin-Yew Lin. 2004a. Looking for a few good metrics: Automatic summarization evaluationhow many samples are enough? In NTCIR.</p>
<p>Chin-Yew Lin. 2004b. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Feifan Liu and Yang Liu. 2008. Correlation between ROUGE and human evaluation of extractive meeting summaries. In Proceedings of ACL-08: HLT, Short Papers, pages 201204, Columbus, Ohio. Association for Computational Linguistics.</p>
<p>Yang Liu and Mirella Lapata. 2019. Text summarization with pretrained encoders. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3730-3740, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Annie Louis and Ani Nenkova. 2013. Automatically assessing machine summary content without a gold standard. Computational Linguistics, 39(2):267-300.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan T. McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 1906-1919. Association for Computational Linguistics.</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111-3119. Curran Associates, Inc.</p>
<p>Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. 2016. Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023.</p>
<p>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Ranking sentences for extractive summarization with reinforcement learning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1747-1759, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Jun-Ping Ng and Viktoria Abrecht. 2015. Better summarization evaluation with word embeddings for ROUGE. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1925-1930, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Karolina Owczarzak, Peter A. Rankel, Hoa Trang Dang, and John M. Conroy. 2012. Assessing the effect of inconsistent assessors on summarization evaluation. In The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, July 814, 2012, Jeju Island, Korea - Volume 2: Short Papers, pages 359-362. The Association for Computer Linguistics.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>Ramakanth Pasunuru and Mohit Bansal. 2018. Multi-reward reinforced summarization with saliency and entailment. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 646-653, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Romain Paulus, Caiming Xiong, and Richard Socher. 2017. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304.</p>
<p>Maxime Peyrard. 2019. Studying summarization evaluation metrics in the appropriate scoring
range. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5093-5100, Florence, Italy. Association for Computational Linguistics.</p>
<p>Maxime Peyrard, Teresa Botschen, and Iryna Gurevych. 2017. Learning to score system summaries for better content selection evaluation. In Proceedings of the Workshop on New Frontiers in Summarization, pages 74-84, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Maja Popović. 2015. chrF: character n-gram fscore for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392-395, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints.</p>
<p>Peter A. Rankel, John M. Conroy, Hoa Trang Dang, and Ani Nenkova. 2013. A decade of automatic content evaluation of news summaries: Reassessing the state of the art. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 131-136, Sofia, Bulgaria. Association for Computational Linguistics.</p>
<p>Evan Sandhaus. 2008. The New York Times annotated corpus. Linguistic Data Consortium, Philadelphia, 6(12):e26752.</p>
<p>Thomas Scialom, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. 2019. Answers unite! unsupervised metrics for reinforced summarization models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3246-3256, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073-1083, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Elaheh ShafieiBavani, Mohammad Ebrahimi, Raymond Wong, and Fang Chen. 2018. A graph-theoretic summary evaluation for ROUGE. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 762-767, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Eva Sharma, Luyang Huang, Zhe Hu, and Lu Wang. 2019. An entity-driven framework for abstractive summarization. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3280-3291, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. Learning to summarize from human feedback. CoRR, abs/2009.01325.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104-3112.</p>
<p>Oleg V. Vasilyev, Vedant Dhamidharka, and John Bohannon. 2020. Fill in the BLANC: humanfree quality estimation of document summaries. CoRR, abs/2002.09836.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 59986008 .</p>
<p>Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image description evaluation. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pages 4566-4575.</p>
<p>Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In Advances in neural information processing systems, pages 2692-2700.</p>
<p>Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008-5020, Online. Association for Computational Linguistics.</p>
<p>Ronald J Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(34):229-256.</p>
<p>Yuxiang Wu and Baotian Hu. 2018. Learning to extract coherent summary via deep reinforcement learning. In Thirty-Second AAAI Conference on Artificial Intelligence.</p>
<p>Jiacheng Xu and Greg Durrett. 2019. Neural extractive text summarization with syntactic compression. In EMNLP-IJCNLP 2019, pages 3292-3303, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Fangfang Zhang, Jin-ge Yao, and Rui Yan. 2018a. On the abstractiveness of neural document summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 785-790, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J Liu. 2019a. Pegasus: Pretraining with extracted gap-sentences for abstractive summarization. arXiv preprint arXiv:1912.08777.</p>
<p>Tianyi Zhang<em>, Varsha Kishore</em>, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.</p>
<p>Xingxing Zhang, Mirella Lapata, Furu Wei, and Ming Zhou. 2018b. Neural latent extractive document summarization. In Proceedings of</p>
<p>the 2018 Conference on Empirical Methods in Natural Language Processing, pages 779-784, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Xingxing Zhang, Furu Wei, and Ming Zhou. 2019b. HIBERT: Document level pre-training of hierarchical bidirectional transformers for document summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5059-5069, Florence, Italy. Association for Computational Linguistics.</p>
<p>Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In EMNLP-IJCNLP 2019, pages 563578, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu, and Eduard Hovy. 2006. ParaEval: Using paraphrases to evaluate summaries automatically. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 447-454, New York City, USA. Association for Computational Linguistics.</p>
<p>Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang, Ming Zhou, and Tiejun Zhao. 2018. Neural document summarization by jointly learning to score and select sentences. In ACL 2018, pages 654-663, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593.</p>
<h1>9 Appendix</h1>
<p>Data Collection The data collection interface used by both crowd-source and expert annotators is presented in Figure 3. In the annotation process, judges were first asked to carefully read the content of the source article and next proceed to evaluating the associated summaries along four axes: relevance, consistency, fluency, and coherence.</p>
<h2>Instructions</h2>
<p>In this task you will evaluate the quality of summaries written for a news article.
To correctly solve this task, follow these steps:</p>
<ol>
<li>Carefully read the news article, be aware of the information it contains.</li>
<li>Read the proposed summaries A-F (B in total).</li>
<li>Rate each summary on a scale from 1 (worst) to 5 (best) by its relevance, consistency, fluency, and coherence.</li>
</ol>
<h2>Definitions</h2>
<h2>Relevance:</h2>
<p>The rating measures how well the summary captures the key points of the article.
Consider whether all and only the important aspects are contained in the summary.</p>
<h2>Consistency:</h2>
<p>The rating measures whether the facts in the summary are consistent with the facts in the original article.
Consider whether the summary does reproduce all facts accurately and does not make up untrue information.</p>
<h2>Fluency</h2>
<p>This rating measures the quality of individual sentences, are they well-written and grammatically correct.
Consider the quality of individual sentences.</p>
<h2>Coherence:</h2>
<p>The rating measures the quality of all sentences collectively, to the fit together and sound naturally.
Consider the quality of the summary as a whole.</p>
<h2>Article</h2>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Example of the data collection interface used by crowd-source and expert annotators.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ The zero-shot model was used for evaluation.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
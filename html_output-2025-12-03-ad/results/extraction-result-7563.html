<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7563 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7563</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7563</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-6b026776cb0588c82cdb696291664966aaa32675</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6b026776cb0588c82cdb696291664966aaa32675" target="_blank">Language Models Meet Anomaly Detection for Better Interpretability and Generalizability</a></p>
                <p><strong>Paper Venue:</strong> LDTM/MMMI/ML4MHD/ML-CDS@MICCAI</p>
                <p><strong>Paper Abstract:</strong> This research explores the integration of language models and unsupervised anomaly detection in medical imaging, addressing two key questions: (1) Can language models enhance the interpretability of anomaly detection maps? and (2) Can anomaly maps improve the generalizability of language models in open-set anomaly detection tasks? To investigate these questions, we introduce a new dataset for multi-image visual question-answering on brain magnetic resonance images encompassing multiple conditions. We propose KQ-Former (Knowledge Querying Transformer), which is designed to optimally align visual and textual information in limited-sample contexts. Our model achieves a 60.81% accuracy on closed questions, covering disease classification and severity across 15 different classes. For open questions, KQ-Former demonstrates a 70% improvement over the baseline with a BLEU-4 score of 0.41, and achieves the highest entailment ratios (up to 71.9%) and lowest contradiction ratios (down to 10.0%) among various natural language inference models. Furthermore, integrating anomaly maps results in an 18% accuracy increase in detecting open-set anomalies, thereby enhancing the language model's generalizability to previously unseen medical conditions. The code and dataset are available at https://github.com/compai-lab/miccai-2024-junli?tab=readme-ov-file</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7563",
    "paper_id": "paper-6b026776cb0588c82cdb696291664966aaa32675",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00315825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Language Models Meet Anomaly Detection for Better Interpretability and Generalizability</h1>
<p>Jun $\mathrm{Li}^{1,2}$, Su Hwan Kim ${ }^{4}$, Philip Müller ${ }^{1}$, Lina Felsner ${ }^{1}$, Daniel Rueckert ${ }^{1,2,4,5}$,<br>Benedikt Wiestler ${ }^{1,4}$, Julia A.Schnabel ${ }^{<em> 1,2,3,6(\boxtimes)}$, and Cosmin I. Bercea</em>1,3(✉) ${ }^{1}$<br>1 Technical University of Munich, Germany<br>${ }^{2}$ Munich Center for Machine Learning, Germany<br>${ }^{3}$ Helmholtz AI and Helmholtz Munich, Germany<br>${ }^{4}$ Klinikum Rechts der Isar, Munich, Germany<br>${ }^{5}$ Imperial College London, UK<br>${ }^{6}$ King's College London, UK<br>{june.li, julia.schnabel, cosmin.bercea}@tum.de</p>
<h4>Abstract</h4>
<p>This research explores the integration of language models and unsupervised anomaly detection in medical imaging, addressing two key questions: (1) Can language models enhance the interpretability of anomaly detection maps? and (2) Can anomaly maps improve the generalizability of language models in open-set anomaly detection tasks? To investigate these questions, we introduce a new dataset for multi-image visual question-answering on brain magnetic resonance images encompassing multiple conditions. We propose KQ-Former (Knowledge Querying Transformer), which is designed to optimally align visual and textual information in limited-sample contexts. Our model achieves a $60.81 \%$ accuracy on closed questions, covering disease classification and severity across 15 different classes. For open questions, KQ-Former demonstrates a $70 \%$ improvement over the baseline with a BLEU-4 score of 0.41 , and achieves the highest entailment ratios (up to $71.9 \%$ ) and lowest contradiction ratios (down to $10.0 \%$ ) among various natural language inference models. Furthermore, integrating anomaly maps results in an $18 \%$ accuracy increase in detecting open-set anomalies, thereby enhancing the language model's generalizability to previously unseen medical conditions. The code and dataset are available at: https://github.com/ compai-lab/miccai-2024-junli?tab=readme-ov-file.</p>
<p>Keywords: Multimodal Learning$\cdot$ Vision-Language Models $\cdot$ VQA</p>
<h2>1 Introduction</h2>
<p>Unsupervised Anomaly Detection (UAD) plays a vital role in early disease diagnosis by identifying deviations from normal patterns. Common UAD methods in medical imaging utilize auto-encoders [7,40], generative adversarial networks [1,28], or diffusion models [3,4,37] and are typically trained on data from</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Our framework is designed to process questions in conjunction with results from anomaly detection methods aiming to provide clinicians with clear, interpretable responses that render anomaly map analyses more intuitive and clinically actionable.
healthy subjects. When applied to pathological inputs, they generate counterfactual "pseudo-healthy" (PH) images that normalize anomalous features to resemble healthy tissues [6]. By comparing the pathological inputs with the generated PH images, anomaly maps can be derived, highlighting regions of interest for clinicians. However, the interpretability of UAD findings is inherently limited due to the unsupervised nature of these methods. Clinicians often lack explicit explanations of the detected anomalies, which can hinder effective decision-making.</p>
<p>To provide interpretable text descriptions for clinicians, we integrate language models with UAD as shown in Figure 1. Recent advancements in language models have achieved human-like performance in tasks such as question answering, summarizing, reasoning, and knowledge retrieval $[8,25,32]$. Notably, these models have demonstrated the capability to pass the United States medical licensing examination [13], showcasing their potential in the medical field $[23,30,33]$.</p>
<p>However, integrating language models with UAD shifts the task from a typical single-image analysis $[2,20,34]$ to a more complex multi-image visual question answering (VQA) challenge. While recent studies have explored generating radiology reports from frontal and lateral views of X-rays $[16,31,38]$, the broader application of multi-image VQA remains largely unexplored. To address this gap, we propose a framework for multi-image VQA in UAD, analyzing various feature fusion strategies to effectively combine original images, anomaly maps, and PH reconstructions. Furthermore, adapting language models for multi-modal tasks introduces additional challenges due to the scarcity and high costs associated with large annotated medical datasets required for fine-tuning [17, 18, 24, 39]. To tackle these challenges, we introduce the KQ-Former, a novel module designed to improve the alignment between visual and textual features, even in settings with limited data availability. In this work, we propose, to the best of our knowledge, the first multi-image question answering application for unsupervised anomaly detection (VQA-UAD). Our main contributions are as follows:</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. An overview of our novel framework for VQA-UAD: (a) the multi-image VQA baseline; (b) multi-image feature fusion strategies; (c) the $K Q$-former module.</p>
<ul>
<li>We have developed a specialized multi-image VQA dataset for UAD, featuring brain Magnetic Resonance Imaging scans. This dataset is meticulously annotated by medical experts and covers a wide range of medical conditions.</li>
<li>We introduce a model-agnostic baseline tailored for multi-image VQA-UAD, alongside a comprehensive analysis of various image fusion strategies.</li>
<li>We introduce the $K Q$-Former, an innovative module designed to enhance the extraction of knowledge-related visual features, thereby improving the alignment between visual and textual information.</li>
<li>Our experimental results demonstrate that language models not only render anomaly maps interpretable but can also leverage these anomaly maps to bolster the accuracy of responses in VQA. This proved particularly effective in scenarios involving previously unseen anomalies.</li>
</ul>
<h1>2 Methods</h1>
<p>Figure 1 shows our VQA-UAD framework, which leverages multiple imaging modalities and language models to enhance diagnostic accuracy. The goal of VQA-UAD is to generate precise answers $\left(A_{i}\right)$ from a set of three images - original image $\left(I_{i}^{o}\right)$, anomaly map $\left(I_{i}^{a}\right)$, and PH reconstruction $\left(I_{i}^{r}\right)$-and a question $\left(Q_{i}\right)$. Section 2.1 introduces our baseline for multi-image VQA, setting the foundation for this application. Section 2.2 introduces the novel $K Q$-Former, designed to enhance both UAD and VQA through improved visual-textual alignment.</p>
<h3>2.1 Multi-Image VQA Baseline</h3>
<p>Figure 1a provides an overview of our multi-image VQA baseline, which incorporates a visual encoder and a language decoder. Figure 1b illustrates three feature</p>
<p>fusion methods within our module.
The visual encoder processes the image triple $\mathcal{I}=\left(I_{i}^{o}, I_{i}^{a}, I_{i}^{r}\right)$, where $I_{i}$ is a tensor in $\mathbb{R}^{H \times W \times C}$ representing height, width, and channels, respectively. It transforms these images into visual embeddings $\mathcal{V}=\left(V_{i}^{o}, V_{i}^{a}, V_{i}^{r}\right)$ through the operation $\mathcal{V}=\mathcal{F}<em i="i">{v}(\mathcal{I})$, where each $V</em>$, with $n$ indicating the number of patches, and $d$ the dimension of embeddings. Here, we implement the encoder based on two different backbones: ViT-B/16 [9] and ResNet50 [10].}$ is an array in $\mathbb{R}^{n \times d</p>
<p>The different fusion strategies are depicted in Figure 1b. The first strategy averages the image features by computing $V_{i}^{\prime}=\frac{1}{3} \sum_{j \in{o, o, r}} V_{i}^{j}$. The second strategy concatenates the visual features into $C_{i}=\left[V_{i}^{o} ; V_{i}^{a} ; V_{i}^{r}\right]$, where $C_{i}$ resides in $\mathbb{R}^{3 n \times d}$. Subsequently, a trainable projection model $\Phi(\cdot)$ is employed to reduce the dimension of $C_{i}$ into $V_{i}^{\prime}=\Phi\left(C_{i}\right)$, where $\Phi(\cdot)$ consists of a two-layer multilayer perception. The final fusion strategy converts each component of the image triple $\mathcal{I}$ into single-channel grayscale images. These are then concatenated channel-wise to form a combined three-channel image $\hat{I}<em i="i">{i}=\left[I</em>}^{o}, I_{i}^{a}, I_{i}^{r}\right] \in \mathbb{R}^{H \times W \times 3}$. This transformation simplifies the multi-image VQA challenge into a single-image format, with the final integrated visual features expressed as $V_{i}^{\prime}=\mathcal{F<em i="i">{v}\left(\hat{I}</em>\right)$.</p>
<p>Language Decoder. Unlike existing methods that primarily treat VQA as a classification task [20, 24], our framework approaches it as a natural language generation challenge, drawing inspiration from recent advances [16, 31, 38]. Our language decoder, here GPT-2 small [27], processes a question $Q_{i}$ and the corresponding merged image features $V_{i}^{r}$ to generate the answer $A_{i}$, producing tokens sequentially. At each decoding step $t$, it calculates a probability distribution $p_{\theta}\left(A_{i}^{t}\right)$ over the vocabulary. Consequently, the full answer distribution for $A_{i}$ with length $T$ is defined as $p_{\theta}\left(A_{i}\right)=\prod_{t=1}^{T} p_{\theta}\left(A_{i}^{t} \mid Q_{i}, V_{i}^{r}, A_{i}^{1}, \ldots, A_{i}^{t-1}\right)$. During training, the goal is to minimize the negative log-likelihood for $N$ samples:</p>
<p>$$
\mathcal{L}\left(\theta^{*}\right)=\arg \min <em i="1">{\theta} \sum</em>\right)
$$}^{N}-\log p_{\theta}\left(A_{i} \mid Q_{i}, V_{i}^{\prime</p>
<h1>2.2 Knowledge Q-Former</h1>
<p>We introduce the $K Q$-Former as an enhancement to our baseline multi-image VQA, specifically addressing the challenge of effectively aligning visual and textual features in contexts where datasets are typically limited. Figure 1c illustrates the design and integration of the $K Q$-Former within our VQA framework. The novel aspect of the $K Q$-Former lies in its ability to leverage knowledge embeddings that integrate pre-trained medical information, enriching its capability to understand and interpret complex medical images.</p>
<p>Formally, the $K Q$-Former operates by taking an input of learnable queries $L_{i} \in \mathbb{R}^{32 \times 768}$ and visual features $V_{i}$ and processes these through a dynamic crossattention mechanism. This interaction enables the $K Q$-Former to dynamically</p>
<p>merge the embedded medical knowledge with the visual information, resulting in enhanced visual tokens $K_{i} \in \mathbb{R}^{32 \times 768}$ that carry detailed visual data alongside relevant medical insights. These tokens are then aggregated following the strategies outlined in Section 2.1 and fed directly into the language decoder.</p>
<p>Network Architecture. The architecture of the KQ-Former is transformerbased, inspired by the Q-Former design [17,35] but modified to consolidate image and text processing into a single transformer unit. This simplification is crucial in medical applications, where data sets are often limited to a few hundred samples. Additionally, the KQ-Former is initialized with BioBERT [14], enhancing its ability to incorporate deep medical knowledge.</p>
<h1>3 Experiments</h1>
<h2>Dataset.</h2>
<p>We retrieved 440 T1-weighted MRI 2D mid-axial brain images from the fastMRI dataset [12], including 253 healthy and 187 unhealthy samples, featuring 13 distinct types of anomalies. For our main experiment, we focused on seven types, while the remaining six types were used to test open-set anomaly detection capabilities (refer to supplementary material for category distribution). We generated the anomaly maps and PH reconstructions using the publicly available method in [5]. Nevertheless, our framework is complementary to UAD research and can benefit from advances in this field.</p>
<p>We created and released VQA labels to facilitate further research. The dataset, annotated by two senior neuroradiologists with both closed and open question types as shown in Figure 3, is organized into question-answer pairs. We divide them patient-wise into training, validation, and test sets in a 7:1:2 ratio, containing 1078,154 , and 308 samples respectively, ensuring a diverse representation of disease and question types across all sets without overlap.</p>
<p>Evaluation Metrics. We evaluated the performance on the closed questions using Accuracy (ACC) and F1 scores. For the open questions, we employed two types of metrics. Firstly, we used standard language evaluation metrics such as BLEU scores [26], ROUGE-L [19], and CIDEr [36] to assess the similarity between the predicted answers and the ground truths. However, since these metrics primarily measure similarity without confirming factual accuracy, we supplemented them with a second type of evaluation. We utilized four Natural Language Inference (NLI) models-BART [15], DEBERTA [11], mDeBERTa [29], and ROBERTA [21]-to determine the logical relationship between the predicted answers and the ground truths. The NLI model categorizes whether the given predicted sentence and the ground truth answer logically imply (entailment) or oppose (contradiction) each other, or are indeterminate (neutral) to each other.</p>
<p>Experimental Setup. Our experiments focus on two main areas. The first evaluates how well our proposed methods explain anomaly maps. The second investi-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Left: Distribution of anomaly categories. Right: Definitions of closed and open questions. For the closed questions, the blue text indicates the answer type, with the count of each type in parentheses. For more details, please refer to the supplementary material. Some questions are simplified here due to space constraints.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Visualization examples of the KQ-Former module with concatenation strategy. Each example includes from left to right: the original image, anomaly map, and PH reconstruction. CQ and OQ represent closed and open questions, respectively.
gates whether anomaly maps can enhance the generalizability of language models in real-world clinical scenarios, which include predominantly healthy data and some previously unseen anomalies. We trained the models on a single NVIDIA RTX A6000 for 40 epochs, using early stopping with patience of 10 . We utilized the AdamW optimizer [22] with a learning rate of $1.5 e^{-5}$ and a weight decay of 0.05 . We used beam search with a width of 5 during the generation phase.</p>
<h1>4 Results</h1>
<h3>4.1 Language Models Enhance the Explainability of Anomaly Maps</h3>
<p>This section assesses the impact of language models on the explainability of anomaly maps. Figure 4 displays examples of the KQ-Former addressing radiologist queries. Instances (a, c, e) show the KQ-Former effectively describes anomaly maps. In cases (b, d, f) predictions do not fully align with expected</p>
<p>Table 1. Performance of the proposed Multi-Image baseline (MI) and KQ-Former (KQF). We experiment with different backbones: ViT and ResNet, and using different feature fusion strategies. B1 to B4 denote BLEU-1 to BLEU-4, while RL and Cr denote ROUGE-L and CIDEr. The best two performances are shown in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Fusion</th>
<th style="text-align: center;">Closed</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Open</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ACC $\uparrow$</td>
<td style="text-align: center;">F1 $\uparrow$</td>
<td style="text-align: center;">B1 $\uparrow$</td>
<td style="text-align: center;">B2 $\uparrow$</td>
<td style="text-align: center;">B3 $\uparrow$</td>
<td style="text-align: center;">B4 $\uparrow$</td>
<td style="text-align: center;">RL $\uparrow$</td>
<td style="text-align: center;">Cr $\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;"><img alt="img-4.jpeg" src="img-4.jpeg" /></td>
<td style="text-align: center;">average</td>
<td style="text-align: center;">56.76</td>
<td style="text-align: center;">49.30</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">1.94</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">concat</td>
<td style="text-align: center;">57.43</td>
<td style="text-align: center;">50.14</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">1.79</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">channel</td>
<td style="text-align: center;">53.38</td>
<td style="text-align: center;">48.33</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">1.77</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">concat</td>
<td style="text-align: center;">60.14</td>
<td style="text-align: center;">56.92</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">2.84</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">channel</td>
<td style="text-align: center;">60.81</td>
<td style="text-align: center;">55.93</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">2.50</td>
</tr>
<tr>
<td style="text-align: center;"><img alt="img-5.jpeg" src="img-5.jpeg" /></td>
<td style="text-align: center;">average</td>
<td style="text-align: center;">36.49</td>
<td style="text-align: center;">38.89</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">1.82</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">concat</td>
<td style="text-align: center;">40.54</td>
<td style="text-align: center;">43.89</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">1.48</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">channel</td>
<td style="text-align: center;">36.49</td>
<td style="text-align: center;">38.89</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">1.69</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">concat</td>
<td style="text-align: center;">54.05</td>
<td style="text-align: center;">47.27</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">1.97</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">channel</td>
<td style="text-align: center;">47.97</td>
<td style="text-align: center;">41.54</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">1.90</td>
</tr>
</tbody>
</table>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 5. Evaluation results on open questions by different NLI models show that the KQ-Former consistently achieves the highest entailment ratios and lower contradiction ratios compared to the baseline models across various tests.
outcomes. However, the language model still interprets these questions effectively and offers contextually relevant responses, demonstrating its capability to enhance the understanding of anomaly detection.</p>
<p>Table 1 summarizes the performance metrics. Independent of the backbone architecture or fusion strategy employed, the KQ-Former (KQF) consistently outperforms the multi-image VQA baseline (MI) in all performance metrics. For instance, it improves accuracy by $5 \%$ for closed questions and increases the BLEU-4 score by $71 \%$ for open questions for the best variants. Among the</p>
<p>Table 2. Performance in anomaly detection for known and unknown anomalies. The utilization of anomaly maps enhances performance in anomaly detection, particularly improving the VQA model's ability to generalize to previously unobserved pathologies.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Known <br> Overall <br> ACC $\uparrow$ F1 $\uparrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Overall <br> ACC $\uparrow$ F1 $\uparrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Unknown <br> Unhealthy (17\%) <br> ACC $\uparrow$ F1 $\uparrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Healthy (83\%) <br> ACC $\uparrow$ F1 $\uparrow$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">w/o Ano</td>
<td style="text-align: center;">85.29</td>
<td style="text-align: center;">85.29</td>
<td style="text-align: center;">84.13</td>
<td style="text-align: center;">87.50</td>
<td style="text-align: center;">69.67</td>
<td style="text-align: center;">80.00</td>
<td style="text-align: center;">98.70</td>
<td style="text-align: center;">95.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">w Ano.</td>
<td style="text-align: center;">88.24</td>
<td style="text-align: center;">88.19</td>
<td style="text-align: center;">89.37</td>
<td style="text-align: center;">89.37</td>
<td style="text-align: center;">82.35 A $18 \%$</td>
<td style="text-align: center;">82.35 A $3 \%$</td>
<td style="text-align: center;">96.39</td>
<td style="text-align: center;">96.39</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">w/o Ano</td>
<td style="text-align: center;">89.71</td>
<td style="text-align: center;">89.69</td>
<td style="text-align: center;">84.45</td>
<td style="text-align: center;">87.00</td>
<td style="text-align: center;">71.43</td>
<td style="text-align: center;">78.95</td>
<td style="text-align: center;">97.47</td>
<td style="text-align: center;">95.06</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">w Ano.</td>
<td style="text-align: center;">91.18</td>
<td style="text-align: center;">91.15</td>
<td style="text-align: center;">85.72</td>
<td style="text-align: center;">88.85</td>
<td style="text-align: center;">72.73 A $2 \%$</td>
<td style="text-align: center;">82.05 A $4 \%$</td>
<td style="text-align: center;">98.72</td>
<td style="text-align: center;">95.65</td>
</tr>
</tbody>
</table>
<p>different backbone architectures tested, the Vision Transformer (ViT) consistently outperforms the ResNet50. Specifically, switching to ViT boosts the KQFormer's accuracy by $11.27 \%$ for closed questions and improves its BLEU-4 score by up to $26.77 \%$ for open questions. Regarding fusion strategies, the concatenation approach generally yields the highest improvements in both methods. We observe that $K Q F$ is more robust across different fusion strategies, likely due to its enhanced ability to utilize visual features from multiple images effectively. Additionally, the NLI model results depicted in Figure 5 further validate the robustness of the KQ-Former. The model demonstrates a higher entailment ratio and a lower contradiction ratio across different configurations, indicating that its answers are not only contextually appropriate but also more aligned with the factual content of the ground truth.</p>
<h1>4.2 Anomaly Maps Improve Generalizability of Language Models</h1>
<p>In this section, we investigate how anomaly maps enhance the generalizability of language models, particularly in detecting unknown anomalies. We utilized the top-performing KQF method with a ViT backbone for this experiment. Table 2 shows that using anomaly maps with both concatenation and channel fusion strategies leads to better detection of known anomalies, with accuracy improvements of $3 \%$ and $2 \%$, respectively. More significantly, anomaly maps greatly improve performance on previously unseen anomalies. For instance, including anomaly maps in the concatenation strategy raised overall accuracy from $69.67 \%$ to $82.35 \%$, marking an $18 \%$ improvement in identifying open-set anomalous data. These findings underscore the substantial role of anomaly maps in boosting the adaptability of language models.</p>
<h2>5 Conclusion</h2>
<p>In this work, we integrated language models with unsupervised anomaly detection and introduced the first multi-image Visual Question Answering benchmark for anomaly detection (VQA-UAD). We established multi-image VQA baselines</p>
<p>and analyzed various feature fusion strategies. We then proposed the Knowledge Querying Transformer (KQF) module, which considerably enhanced the extraction of knowledge-related visual features when fine-tuned on a small dataset. Our findings demonstrated mutual benefits: language models provided interpretability to anomaly maps, improving clinical insights, while anomaly maps enhanced the generalizability of language models, particularly for detecting previously unseen anomalies.</p>
<p>Future work will explore larger language models trained on extensive medical knowledge and expand the diversity and size of our dataset. This will further enhance the generalizability and robustness of our anomaly detection framework across diverse healthcare settings. We believe our research will open new avenues for combining language models with unsupervised anomaly detection, driving innovations in this field.</p>
<p>Acknowledgments. C.I.B. is funded via the EVUK program ("Next-generation Al for Integrated Diagnostics") of the Free State of Bavaria and partially supported by the Helmholtz Association under the joint research school 'Munich School for Data Science'.</p>
<h1>References</h1>
<ol>
<li>Akcay, S., Atapour-Abarghouei, A., Breckon, T.P.: Ganomaly: Semi-supervised anomaly detection via adversarial training. In: Computer Vision-ACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 2-6, 2018, Revised Selected Papers, Part III 14. pp. 622-637. Springer (2019)</li>
<li>Bai, L., Islam, M., Ren, H.: Cat-vil: Co-attention gated vision-language embedding for visual question localized-answering in robotic surgery. In: Medical Image Computing and Computer-Assisted Intervention. pp. 397-407. Springer (2023)</li>
<li>Behrendt, F., Bhattacharya, D., Krüger, J., Opfer, R., Schlaefer, A.: Patched diffusion models for unsupervised anomaly detection in brain mri. International Conference on Medical Imaging with Deep Learning (2023)</li>
<li>Bercea, C.I., Neumayr, M., Rueckert, D., Schnabel, J.A.: Mask, stitch, and resample: Enhancing robustness and generalizability in anomaly detection through automatic diffusion models. ICML 3rd Workshop on Interpretable Machine Learning in Healthcare (2023)</li>
<li>Bercea, C.I., Wiestler, B., Rueckert, D., Schnabel, J.A.: Generalizing unsupervised anomaly detection: Towards unbiased pathology screening. In: Medical Imaging with Deep Learning (2023)</li>
<li>Bercea, C.I., Wiestler, B., Rueckert, D., Schnabel, J.A.: Reversing the abnormal: Pseudo-healthy generative networks for anomaly detection. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 293-303. Springer (2023)</li>
<li>Chen, X., You, S., Tezcan, K.C., Konukoglu, E.: Unsupervised lesion detection via image restoration with a normative prior. Medical Image Analysis 64 (2020)</li>
<li>
<p>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., et al.: Palm: Scaling language modeling with pathways. Journal of Machine Learning Research 24(240), 1-113 (2023)</p>
</li>
<li>
<p>Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. In: International Conference on Learning Representations (2021)</p>
</li>
<li>He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. $770-778(2016)$</li>
<li>He, P., Gao, J., Chen, W.: Debertav3: Improving debería using electra-style pretraining with gradient-disentangled embedding sharing. In: The Eleventh International Conference on Learning Representations (2022)</li>
<li>Knoll, F., Zbontar, J., Sriram, A., Muckley, M.J., Bruno, M., Defazio, A., Parente, M., Geras, K.J., Katsnelson, J., et al.: fastmri: A publicly available raw k-space and dicom dataset of knee images for accelerated mr image reconstruction using machine learning. Radiology: Artificial Intelligence 2(1), e190007 (2020)</li>
<li>Kung, T.H., Cheatham, M., Medenilla, A., Sillos, C., De Leon, L., Elepaño, C., et al.: Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. PLoS digital health 2(2), e0000198 (2023)</li>
<li>Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C.H., Kang, J.: Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics 36(4), 1234-1240 (2020)</li>
<li>Lewis, M., Liu, Y., Goyal, N., et al.: BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In: Proceedings of the Association for Computational Linguistics. pp. 7871-7880 (2020)</li>
<li>Li, J., Li, S., Hu, Y., Tao, H.: A self-guided framework for radiology report generation. In: Medical Image Computing and Computer-Assisted Intervention. pp. 588-598. Springer (2022)</li>
<li>Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In: International Conference on Machine Learning. pp. 12888-12900. PMLR (2022)</li>
<li>Li, P., Liu, G., He, J., Zhao, Z., Zhong, S.: Masked vision and language pre-training with unimodal and multimodal contrastive losses for medical visual question answering. In: Medical Image Computing and Computer-Assisted Intervention. pp. $374-383$. Springer (2023)</li>
<li>Lin, C.Y.: Rouge: A package for automatic evaluation of summaries. In: Text summarization branches out. pp. 74-81 (2004)</li>
<li>Liu, B., Zhan, L.M., Wu, X.M.: Contrastive pre-training and representation distillation for medical visual question answering based on radiology images. In: Medical Image Computing and Computer Assisted Intervention. pp. 210-220. Springer (2021)</li>
<li>Liu, Y., Ott, M., Goyal, N., Du, J., et al.: Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019)</li>
<li>Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International Conference on Learning Representations (2018)</li>
<li>Luo, R., Sun, L., Xia, Y., Qin, T., Zhang, S., Poon, H., Liu, T.Y.: Biogpt: generative pre-trained transformer for biomedical text generation and mining. Briefings in bioinformatics 23(6), bbac409 (2022)</li>
<li>Nguyen, B.D., Do, T.T., Nguyen, B.X., Do, T., Tjiputra, E., Tran, Q.D.: Overcoming data limitation in medical visual question answering. In: Medical Image Computing and Computer Assisted Intervention. pp. 522-530. Springer (2019)</li>
<li>
<p>OpenAI: Introducing ChatGPT (2023), https://openai.com/blog/chatgpt/, November 30, 2022</p>
</li>
<li>
<p>Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic evaluation of machine translation. In: Proceedings of the 40th annual meeting of the Association for Computational Linguistics. pp. 311-318 (2002)</p>
</li>
<li>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019)</li>
<li>Schlegl, T., Seeböck, P., Waldstein, S.M., Langs, G., Schmidt-Erfurth, U.: fAnoGAN: Fast unsupervised anomaly detection with generative adversarial networks. Medical Image Analysis 54, 30-44 (2019)</li>
<li>Sileo, D.: Tasksource: Structured dataset preprocessing annotations for frictionless extreme multi-task learning and evaluation. arXiv preprint arXiv:2301.05948 (2023)</li>
<li>Singhal, K., Azizi, S., Tu, T., Mahdavi, S.S., Wei, J., Chung, H.W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S., et al.: Large language models encode clinical knowledge. Nature 620(7972), 172-180 (2023)</li>
<li>Tanida, T., Müller, P., Kaissis, G., Rueckert, D.: Interactive and explainable regionguided radiology report generation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 7433-7442 (June 2023)</li>
<li>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023)</li>
<li>Tu, T., Palepu, A., Schaekermann, M., Saab, K., Freyberg, J., Tanno, R., Wang, A., Li, B., Amin, M., Tomasev, N., et al.: Towards conversational diagnostic ai. arXiv preprint arXiv:2401.05654 (2024)</li>
<li>Van Sonsbeek, T., Derakhshani, M.M., Najdenkoska, I., Snoek, C.G., Worring, M.: Open-ended medical visual question answering through prefix tuning of language models. In: Medical Image Computing and Computer Assisted Intervention. pp. 726-736. Springer (2023)</li>
<li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. In: Advances in Neural Information Processing Systems. vol. 30 (2017)</li>
<li>Vedantam, R., Lawrence Zitnick, C., Parikh, D.: Cider: Consensus-based image description evaluation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4566-4575 (2015)</li>
<li>Wolleb, J., Bieder, F., Sandkühler, R., Cattin, P.C.: Diffusion models for medical anomaly detection. Medical Image Computing and Computer Assisted Intervention pp. $35-45(2022)$</li>
<li>Wu, X., Yang, S., Qiu, Z., Ge, S., Yan, Y., Wu, X., Zheng, Y., Zhou, S.K., Xiao, L.: Deltanet: Conditional medical report generation for covid-19 diagnosis. In: Proceedings of the 29th International Conference on Computational Linguistics. pp. $2952-2961(2022)$</li>
<li>Zhang, X., Wu, C., Zhao, Z., Lin, W.o.: Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415 (2023)</li>
<li>Zimmerer, D., Isensee, F., Petersen, J., Kohl, S., Maier-Hein, K.: Unsupervised anomaly localization using variational auto-encoders. Medical Image Computing and Computer Assisted Intervention pp. 289-297 (2019)</li>
</ol>
<h1>Supplementary Material for Language Models Meet Anomaly Detection for Better Interpretability and Generalizability</h1>
<p>Jun $\mathrm{Li}^{1,2}$, Su Hwan Kim ${ }^{4}$, Philip Müller ${ }^{1}$, Lina Felsner ${ }^{1}$, Daniel Rueckert ${ }^{1,2,4,5}$, Benedikt Wiestler ${ }^{1,4}$, Julia A.Schnabel<em>1,2,3,6(SS), and Cosmin I. Bercea</em>1,3(SS)<br>${ }^{1}$ Technical University of Munich, Germany<br>${ }^{2}$ Munich Center for Machine Learning, Germany<br>${ }^{3}$ Helmholtz AI and Helmholtz Munich, Germany<br>${ }^{4}$ Klinikum Rechts der Isar, Munich, Germany<br>${ }^{5}$ Imperial College London, UK<br>${ }^{6}$ King's College London, UK<br>{june.li, julia.schnabel, cosmin.bercea}@tum.de</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 1. Category distribution of unseen anomalies. These unseen anomalies are dural thickening, white matter lesion, sinus opacification, encephalomalacia, intraventricular substance, and absent septum pellucidum.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Framework: KQF-channel-ViT</th>
<th style="text-align: left;">BART: Neutral</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Question: Can you describe the differences highlighted between anomaly maps and <br> origin image and why it is the healthy region?</td>
<td style="text-align: left;">DEBERTA: Neutral</td>
</tr>
<tr>
<td style="text-align: left;">Ground Truth: Width of left lateral ventricle is within normal range. <br> Predict Answer: Sulci are unremarkable in size.</td>
<td style="text-align: left;">mDeBERTa: Entailment</td>
</tr>
<tr>
<td style="text-align: left;">Framework: KQF-concate-ViT</td>
<td style="text-align: left;">ROBERTA: Entailment</td>
</tr>
<tr>
<td style="text-align: left;">Question: Is the pseudo-healthy reconstruction a plausible restoration of the input to <br> a healthy state?</td>
<td style="text-align: left;">BART: Entailment</td>
</tr>
<tr>
<td style="text-align: left;">Ground Truth: No (there is an unnatural shape in the left lateral horn). <br> Predict Answer: No (there is an unnatural shape in the left lateral horn).</td>
<td style="text-align: left;">DEBERTA : Entailment</td>
</tr>
<tr>
<td style="text-align: left;">Framework: KQF-concate-ViT</td>
<td style="text-align: left;">mDeBERTa: Entailment</td>
</tr>
<tr>
<td style="text-align: left;">Question: Do the anomaly maps accurately reflect the selected disease? <br> Ground Truth: Yes (however, only partial marking of anomaly). <br> Predict Answer: Yes.</td>
<td style="text-align: left;">ROBERTA: Entailment</td>
</tr>
</tbody>
</table>
<p>Fig. 2. Visualization examples from different NLI models. In certain instances, different models may have different judgments, indicating that the results may still exhibit some deviations from human recognition. For example, in the first case, the KQF framework predicts "Sulci are unremarkable in size" and the ground truth is "Width of left lateral ventricle is within normal range". The BART and DEBERTA models classify as "Neutral", while mDeBERTa and ROBERTA predict as "Entailment".</p>
<p>Table 1. Definitions and illustrative examples of questions and responses. Responses to open questions vary widely in format, so a single example is provided for visualization.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Question Definition</th>
<th style="text-align: left;">Response Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Closed</td>
<td style="text-align: left;">Is the case normal?</td>
<td style="text-align: left;">Yes. / No.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Please describe the condition of the <br> brain.</td>
<td style="text-align: left;">It's + category.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Can you comment on the severity of <br> the pathology?</td>
<td style="text-align: left;">Clinically irrelevant. / Potentially <br> clinically relevant. / Clinically <br> relevant. / Not applicable.</td>
</tr>
<tr>
<td style="text-align: left;">Open</td>
<td style="text-align: left;">Are there areas in the anomaly maps <br> that highlight a normal variation of <br> the healthy, rather than pathological <br> areas (false positives)?</td>
<td style="text-align: left;">Yes. Anomalies are observed in the <br> left frontal and right occipital sulci.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Is the pseudo-healthy reconstruction a <br> plausible restoration of the input to a <br> healthy state?</td>
<td style="text-align: left;">No (there is a midline shift to the <br> right).</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Do the anomaly maps accurately <br> reflect the selected disease?</td>
<td style="text-align: left;">No (not marked).</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Can you describe the differences <br> highlighted between anomaly maps <br> and the original image and why it is <br> the healthy region?</td>
<td style="text-align: left;">Ventricles probably appear narrow <br> because of the height of the slice.</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Equal contribution.</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
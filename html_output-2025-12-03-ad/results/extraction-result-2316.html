<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2316 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2316</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2316</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-268681120</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.15989v2.pdf" target="_blank">Knowledge-guided Machine Learning: Current Trends and Future Prospects</a></p>
                <p><strong>Paper Abstract:</strong> This paper presents an overview of scientific modeling and discusses the complementary strengths and weaknesses of ML methods for scientific modeling in comparison to process-based models. It also provides an introduction to the current state of research in the emerging field of scientific knowledge-guided machine learning (KGML) that aims to use both scientific knowledge and data in ML frameworks to achieve better generalizability, scientific consistency, and explainability of results. We discuss different facets of KGML research in terms of the type of scientific knowledge used, the form of knowledge-ML integration explored, and the method for incorporating scientific knowledge in ML. We also discuss some of the common categories of use cases in environmental sciences where KGML methods are being developed, using illustrative examples in each category.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2316.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2316.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KGML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-guided Machine Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of ML approaches that integrate scientific knowledge (equations, constraints, heuristics, simulations) with data-driven models to improve generalizability, scientific consistency, and interpretability in scientific modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Environmental & Earth sciences (general scientific modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Replace or augment process-based models and pure data-driven (black-box) ML to predict or simulate responses of physical/environmental systems (e.g., lake temperature, streamflow, climate fields) while respecting scientific knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Often limited to moderate; many scientific domains are transitioning from data-poor to data-rich but observational coverage is heterogeneous (well-observed systems vs. many sparsely-observed systems); simulated data from process-based models frequently available but can be biased.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Spatio-temporal data (gridded rasters, time series), graphs/networks (river/stream networks), multimodal (sensor, satellite, simulation outputs); often structured but irregular in space/time and sparse for some targets.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: multi-scale dynamics, nonlinear interactions, high dimensionality (spatio-temporal fields), partial observability, unknown confounders, computationally expensive forward models.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature in the sense of well-established process-based models and physical laws, but many sub-processes are imperfectly known (partial/imperfect knowledge); domain expertise and legacy models exist.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - scientific applications require mechanistic/physically consistent outputs and interpretability; black-box predictions alone are often insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Knowledge-guided machine learning (KGML) — hybrid physics-ML approaches</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>KGML encompasses methods that inject scientific knowledge via loss functions (constraints/regularizers), architecture (baked-in symmetries, modular decomposition), and pretraining (simulation-based or self-supervised tasks). Implementations include physics-guided losses, physics-informed networks, physics-aware architectures, pretraining on simulated data, and hybrid residual/HPD models.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Hybrid / physics-informed ML / supervised & self-supervised learning (depending on instantiation)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable across environmental scientific modeling problems where (partial) domain knowledge exists and observations are limited; requires careful balancing of knowledge vs data (weights/tolerances) and may need bespoke formulation per problem.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported to improve generalizability and scientific consistency relative to pure data-driven models; enables better out-of-distribution performance and interpretability when knowledge is appropriately incorporated.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — can reduce computational costs (surrogates), improve predictions in data-sparse regimes, enable scientific insight consistent with theory, and facilitate transfer across locations/systems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>KGML is positioned between process-based models (mechanistic, generalize via theory but expensive/biased) and black-box ML (flexible but poor OOD generalization); KGML aims to combine strengths of both.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of partial or full scientific knowledge, availability of simulated data, careful formulation of knowledge constraints (hard vs soft), modular architectures that reflect process decomposition, and pretraining strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Blending scientific knowledge with data-driven ML improves generalization and interpretability in scientific modeling, especially in data-sparse or out-of-distribution settings where pure ML fails and pure process models are imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-guided Machine Learning: Current Trends and Future Prospects', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2316.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2316.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PGNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-Guided Neural Network (PGNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ML approach that adds physical/monotonic constraints as loss terms to neural network training to enforce scientifically consistent outputs (original application: lake temperature modeling).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Physics-guided neural networks (pgnn): An application in lake temperature modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Lake temperature modeling (freshwater ecosystem modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict water temperature profiles from drivers (weather, depth, etc.) while ensuring outputs obey domain physics (e.g., density-depth monotonicity).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Observational data limited for many lakes; some lakes have high-frequency sensor data, but many are sparsely observed — motivates physics guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Time series and profiles across depth (spatio-temporal, regularly sampled in depth/time for monitored lakes).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate-to-high: nonlinear thermodynamic processes, depth-resolved dynamics, intermediate latent heat fluxes; limited labeled targets.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Process-based lake models exist (GLM, GLM-AED), but biogeochemical processes are less well understood; substantial domain expertise available.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — models must respect physical relationships (e.g., density-depth) for scientific credibility.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Physics-guided neural networks (PGNN)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Feedforward neural networks trained with augmented loss functions that include penalties for violating known physical constraints (e.g., monotonicity, conservation laws), sometimes using process simulations as auxiliary inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning with physics-informed regularization</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate where partial/soft constraints exist and observations are limited; constraints help steer learned mappings to physically plausible regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Improves physical consistency of predictions compared to unconstrained NN; used successfully in lake temperature tasks to enforce monotonic relationships and reduce nonsensical outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Moderate-to-high for operational environmental prediction where physical plausibility is essential; reduces risk of spurious predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to pure ML, PGNN yields more physically consistent outputs; compared to pure process models, PGNN can better fit observations where process models are biased or misspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Existence of clear, enforceable physical constraints (e.g., monotonic relations), ability to compute constraint residuals for loss, and careful weighting of physics vs. data loss.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Embedding even simple physics as loss constraints can substantially increase the scientific plausibility and generalizability of ML predictions in domains with limited data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-guided Machine Learning: Current Trends and Future Prospects', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2316.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2316.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PGRNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-Guided Recurrent Neural Network (PGRNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent neural network variant that incorporates energy conservation and intermediate heat-flux estimation into the loss/architecture for time-resolved lake temperature prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Physics guided rnns for modeling dynamical systems: A case study in simulating lake temperature profiles.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Time-series forward modeling of lake temperature profiles (freshwater ecosystems)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict temporal evolution of vertical temperature profiles in lakes by combining recurrent architectures with physics (heat transfer, energy conservation).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Limited for many lakes — high-frequency sensor data for some lakes but many are sparsely observed; simulated data available for pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Spatio-temporal time series (depth × time), sequential data suitable for RNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High temporal complexity (heat flux estimation, latent intermediate states), nonlinearity, need to model hidden fluxes and conservation constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established process knowledge for thermodynamics exists but parameterizations and ecological components are imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — model incorporates mechanistic energy balance to ensure physically plausible dynamics and to aid interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Physics-guided recurrent neural network (PGRNN)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>RNN architecture that estimates intermediate physical fluxes and applies physics-based constraints (energy conservation) as part of the training objective; may use pretraining on simulated data.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning / physics-informed recurrent networks</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited to temporal dynamical systems with known conservation laws and limited data; imposes mechanistic structure on sequence models.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported (per Read et al.) to significantly outperform both standard ML models and process-based models on out-of-distribution years/seasons with different weather compared to training data, demonstrating improved OOD generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for forecasting ecological state variables, enabling better generalization across seasons/years and across lakes with limited observations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms unconstrained ML and some process-based models on OOD tests; combines advantages of data fit (ML) with physical consistency (process knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Explicit incorporation of conservation laws, estimating intermediate latent fluxes rather than only mapping inputs to outputs, and use of simulated pretraining to initialize parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Incorporating conservation laws into recurrent architectures yields models that generalize better across distributional shifts than black-box ML or imperfect process models alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-guided Machine Learning: Current Trends and Future Prospects', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2316.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2316.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PINN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-Informed Neural Networks (PINNs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural network framework that incorporates PDEs and boundary/initial conditions into training by penalizing PDE residuals in the loss, enabling solution of forward and inverse PDE problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Partial differential equations across physics and engineering (fluid dynamics, climate, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Solve PDEs (forward) or infer PDE parameters/fields (inverse) without relying solely on numerical discretization, by training neural networks whose outputs satisfy governing equations.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Can operate with scarce labeled data because PDE residuals provide supervisory signal; when data exist they can be combined with physics losses.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Continuous function inputs/outputs (spatio-temporal fields) — can be treated as coordinate-to-solution mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: solving nonlinear PDEs, multi-dimensional continuous domains, complex boundary conditions; optimization landscapes can be challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Well-established PDE theory; PINNs are a relatively recent ML approach (since ~2017) built for domains with strong mechanistic equations.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — exact PDE form is assumed known and enforced during training for mechanistic fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Physics-informed neural networks (PINNs)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Feedforward neural networks trained with composite loss including data misfit and PDE residuals evaluated at collocation points; solves for continuous solutions and can incorporate boundary/initial conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Physics-informed ML / supervised with equation constraints</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate when governing equations are known (perfect/complete knowledge) and one wants mesh-free PDE solvers or to infer unknown PDE coefficients; less suited when equations are highly imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>PINNs have enabled solving single-instance PDEs and inverse PDE problems, but face challenges (optimization, scalability) for complex/high-dimensional PDEs; many extensions (neural operators) seek to address limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for providing mesh-free, differentiable PDE solvers and for inverse problems; potential computational gains and ability to incorporate data and physics jointly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to conventional numerical solvers: PINNs are flexible and mesh-free but may be slower to train and face optimization issues; compared to neural operators, PINNs typically solve single PDE instances rather than families of PDE solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Accurate specification of PDEs/boundary conditions, careful sampling of collocation points, loss weighting between data and physics, and architectural/training improvements to mitigate optimization pathologies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>When governing equations are known, embedding them directly into NN training yields physics-consistent solutions and enables inverse estimation, but computational/training challenges limit straightforward application to large complex PDEs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-guided Machine Learning: Current Trends and Future Prospects', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2316.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2316.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeuralOperator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Operators (Fourier Neural Operator / DeepONet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Operator-learning neural architectures that learn mappings between function spaces (e.g., PDE initial/boundary conditions to solutions) enabling fast inference across a family of PDE instances and arbitrary resolution outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fourier neural operator for parametric partial differential equations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>PDE solving, surrogate modeling for physics (fluid dynamics, climate downscaling, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn the solution operator mapping input functions (initial/boundary conditions, forcing) to output solution fields for families of PDEs to enable rapid, possibly zero-shot, evaluation at arbitrary spatial resolutions.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires datasets of input–solution pairs (often from numerical solvers) — simulated data can be generated, so data can be abundant for training; however high-fidelity simulations are costly to produce.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional spatio-temporal fields; structured gridded data often used, but variants handle general geometries.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High-dimensional function-to-function mapping; capturing multi-scale behavior and broad spectral content is challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Relatively recent but rapidly maturing; neural operator literature (FNO, DeepONet, variants) is active and growing.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Varies — neural operators can be purely data-driven or augmented with physics (PINO, physics-informed operator learning) to enforce constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Neural operator learning (FNO, DeepONet, PINO, variants)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Architectures (e.g., FNO) transform fields into Fourier/frequency domains, learn kernel mappings via NNs, and invert transform to spatial domain; can be trained on families of PDE solutions to generalize across initial/boundary/parameter variations and produce arbitrary-resolution outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised operator learning / physics-enhanced operator learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly suitable for surrogate PDE solvers, fast emulation of expensive models, and zero-shot/few-shot downscaling; needs representative simulated datasets covering variability of interest.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Neural operators can produce accurate PDE solutions much faster at inference time and enable arbitrary-resolution outputs; issues include neglecting low-amplitude spectral components (addressed by PDE-Refiner) and need for physics constraints for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — enables scalable surrogate modeling, rapid scenario evaluation, and potential zero-shot downscaling of climate/PDE solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to PINNs: neural operators learn families of solutions and are more efficient for many-query settings; compared to numerical solvers: orders-of-magnitude speedup at inference but reliant on training data quality.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of diverse, representative simulated training data, architectural choices (Fourier layers, learned deformations), and physics-regularization/refinement techniques (e.g., PINO, PDE-Refiner) to recover full spectral fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Learning the operator mapping between function spaces allows fast, generalizable surrogate solutions to PDE families and supports arbitrary-resolution predictions when trained on sufficient simulated variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-guided Machine Learning: Current Trends and Future Prospects', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2316.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2316.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PGA-LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-Guided Architecture LSTM (PGA-LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent LSTM architecture modified to bake in physics (density-depth relations) in the internal node connections to guarantee physically consistent temperature profiles and produce uncertainty estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Physics-guided architecture (pga) of neural networks for quantifying uncertainty in lake temperature modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Lake temperature prediction (vertical profiles)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Model vertical temperature profiles ensuring each predicted realization respects density-depth physics and produce meaningful uncertainty quantification.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Limited observational data for many lakes; some lakes have sensors enabling training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Time series across depth (spatio-temporal sequences applicable to LSTM).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate: need to respect inter-layer (depth) constraints and capture temporal dynamics with uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established physics for density-depth; process models exist but are computationally heavy and sometimes inaccurate in biogeochemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — requirement that each realization respects physical laws motivates architecture-level constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Physics-guided architecture in LSTM (PGA-LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Modify LSTM connections/structure to encode density-depth physics so the architecture inherently enforces monotonic/physical relations, enabling physically valid samples and uncertainty estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Architecture-level physics-informed supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate when specific structural physics can be encoded in network connectivity; useful for enforcing hard constraints across model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Shown to produce uncertainty estimates and sample realizations guaranteed to respect density-depth physics; improves interpretability and scientific plausibility of predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Moderate — improves trustworthiness of ML outputs in scientific settings by guaranteeing physics-compliant realizations and providing uncertainty quantification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Unlike loss-based PGNN, PGA-LSTM bakes constraints into architecture guaranteeing adherence for every sample rather than penalizing violations, reducing reliance on loss weighting.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Clear mapping of physics to network structure, ability to redesign recurrent connections to reflect process interactions, and problem formulations where hard constraints are known.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Embedding physics directly into network architecture can guarantee physically valid outputs and reliable uncertainty estimates, avoiding tradeoffs inherent in soft-constraint loss formulations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-guided Machine Learning: Current Trends and Future Prospects', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2316.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2316.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pretraining(Simulated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-Guided Pretraining Using Simulated Data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretraining ML models on simulated outputs from process-based models (possibly with varied parameterizations) to initialize weights so that fine-tuning on limited real observations yields better generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Physics guided rnns for modeling dynamical systems: A case study in simulating lake temperature profiles.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Environmental modeling (lakes, streams, hydrology, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Leverage abundant simulated data to pretrain ML models so they capture physics-rooted patterns and then adapt to scarce observational data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Simulated data from process-based models are abundant (can be generated), while observational data are often sparse; simulated data may be biased due to imperfect parameterizations.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Simulated spatio-temporal fields or time series; similar structure to observational targets.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High due to mismatch/bias between simulation and reality and domain heterogeneity; requires strategies to diversify simulated parameterizations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Process models mature and available, enabling generation of simulated datasets for pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium-to-high — pretraining uses mechanistic simulations to encode physics but final model must still adapt to real-world deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Simulation-based pretraining / meta-learning on simulated datasets</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Pretrain on large collections of simulated runs (possibly varying physical parameters) to learn robust features; may use meta-learning to initialize models for quick adaptation; then fine-tune on limited observations, possibly with regularization to avoid catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Pretraining / transfer learning / meta-learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited when process-based simulators exist and can produce varied datasets; must address simulator bias and catastrophic forgetting during fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported improvements in data-sparse and out-of-sample scenarios (Jia et al.); benefits depend on diversity/realism of simulated training data and adaptation strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — reduces observational data requirements for ML deployment and enables transfer across poorly monitored systems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms naive training on scarce observations by providing physics-informed initialization; requires mitigation of simulator bias unlike purely observational fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Diversity of simulated parameterizations, meta-learning strategies to learn transferable initializations, careful fine-tuning protocols to prevent catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Pretraining on diverse simulated datasets encodes physics into ML models and can substantially improve performance in data-sparse, out-of-distribution settings if simulator bias and forgetting are managed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-guided Machine Learning: Current Trends and Future Prospects', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2316.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2316.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SurrogateModeling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ML-based Surrogate Modeling for Process-based Simulators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use ML (including neural operators and other surrogate models) trained on simulator outputs to emulate expensive process-based models, enabling many-query scenarios and faster inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Climate modeling, hydrology, fluid dynamics, environmental impact assessment</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Replace or accelerate expensive numerical simulations (PDE solvers, high-fidelity models) with learned surrogates for tasks like uncertainty quantification, optimization, and rapid forecasting.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Training requires simulated input-output pairs from high-fidelity models — data generation can be expensive but controllable; observational data may be used to correct bias.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional spatio-temporal fields, parameter-to-field mappings; structured gridded data common.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High computational complexity in original simulator; learned surrogate must capture multi-scale dynamics and produce stable long-rollout predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Process-based simulators are mature; surrogate modeling with ML is an active research area with diverse methods.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — surrogate must preserve essential mechanistic behavior; physics-guided constraints often applied to ensure consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Surrogate ML models (neural operators, CNNs, Koopman approaches, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train ML models on simulated outputs to approximate forward simulator mapping; can include physics constraints, multi-step refinement (e.g., PDE-Refiner), and operator-learning to generalize across parameter spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning / operator learning / physics-informed surrogates</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable when many-query or real-time inference is needed and when simulators can provide training data; requires attention to spectral fidelity and long-horizon accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Surrogates give large inference speedups and can match accuracy of numerical solvers in many cases; refinement techniques (diffusion-based refiners) can address missing spectral components.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — enables scalable uncertainty quantification, ensemble forecasting, and operational uses of expensive models that were previously infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Surrogates are faster than numerical solvers but may require corrective refinements to match fine-scale spectral content; operator-learning offers advantages for families of PDEs over single-instance PINNs.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Sufficient diversity and fidelity of simulated training data, operator architectures that capture global structure, and refinement/physics-regularization to recover lost components.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>ML surrogates can dramatically reduce computational costs for simulation tasks if trained on representative simulated data and combined with physics-aware refinement to preserve essential dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-guided Machine Learning: Current Trends and Future Prospects', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2316.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2316.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InverseModeling(dPL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable Parameter Learning (dPL) / ML-driven Inverse Modeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use differentiable, learnable process models or ML models (including invertible NNs) to infer latent physical parameters or states from observed inputs/outputs, improving calibration and state estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Differentiable, learnable, regionalized process-based models with multiphysical outputs can approach state-of-the-art hydrologic prediction accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Hydrology, seismic imaging, geothermal systems, photonics (inverse PDE problems)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Estimate static or slowly-varying physical parameters (e.g., soil, hydraulic conductivity) or latent states by learning inverse mappings from observations, or by differentiable calibration of process-based models using gradient-based optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Often sparse: direct measurements of parameters are expensive or infeasible; observational data of inputs/outputs used for calibration/inference.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Time series, spatio-temporal observations; often irregular and incomplete.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High nonlinearity and ill-posed inverse problems, multimodal parameter spaces, computational cost from multiple forward runs mitigated by differentiable models.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Traditional inverse methods mature (Bayesian, grid search); differentiable parameter learning is an emerging ML-enabled approach for geosciences.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — inferred parameters are interpretable and used within mechanistic simulators; scientific plausibility is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Differentiable parameter learning / invertible neural networks for inverse modeling</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Embed ML modules inside differentiable process-based simulators or use invertible NN architectures to learn inverse maps; use automatic differentiation to compute gradients and perform parameter inference efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised/unsupervised inverse learning / differentiable programming</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate when forward models can be implemented differentiably or when invertible architectures exist; speeds calibration and improves parameter estimation in data-limited contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>dPL and related approaches allow efficient gradient-based calibration and can approach state-of-the-art predictive accuracy in hydrology; they enable learning spatially varying parameters modeled by separate NN modules.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for automating calibration, improving state estimation, and informing model parameters that are otherwise hard to measure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to grid/Bayesian search, differentiable approaches are more efficient (gradient-based) and can scale better; interpretability retains advantage over black-box inversions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of differentiable implementations of forward models, good initialization, and incorporation of constraints or priors to regularize ill-posed inversions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Making process-based models differentiable and coupling them with ML enables efficient inverse parameter learning that retains interpretability while leveraging gradient-based optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-guided Machine Learning: Current Trends and Future Prospects', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2316.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2316.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PhysicsReg-Generative</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-Regularized Generative Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generative ML models (GANs, diffusion models, VAEs) augmented with physical constraints or conservation laws to produce physically consistent synthetic simulations (e.g., turbulent flows, precipitation fields).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Fluid dynamics (turbulence), climate nowcasting (precipitation), materials science</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate synthetic spatio-temporal fields consistent with physical laws for data augmentation, downscaling, or initial condition refinement while avoiding spurious artifacts common in unconstrained generative models.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Training data (simulations, observations) can be scarce for high-fidelity regimes; generative models often require many examples but physics-regularization can compensate.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional spatio-temporal fields, images, or volumetric data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High nonlinearity and chaotic dynamics; generative fidelity requires preserving statistical and physical invariants across scales.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Generative modeling mature in vision/NLP; physics-regularized generative approaches are emergent in scientific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — outputs must satisfy conservation laws or statistical constraints for scientific validity.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Physics-regularized GANs and diffusion models</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Add physics-based loss terms (e.g., conservation penalties), constraint enforcement, or physics-informed denoising schedules (for diffusion models) to generative training; can also refine outputs of operator surrogates (PDE-Refiner).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Generative modeling with physics-informed regularization</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Useful for producing realistic simulations, downscaling, and refining ML-PDE solvers; effective when physics constraints are known and can be computed on generated samples.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Physics-regularized generative models reduce spurious patterns and yield more physically realistic outputs in turbulent flow simulations and precipitation nowcasting; diffusion-based refiners improve neglected spectral components.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for generating realistic synthetic datasets, improving surrogate predictions, and enabling data augmentation for scarce regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms unconstrained generative models in physical fidelity; complements operator-learning surrogates by refining spectral content.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Explicit physically-computable constraints, ability to evaluate physics residuals on synthetic samples, and architectures/training schemes (e.g., diffusion refiners) suited for multi-scale dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Enforcing physical constraints during generative training materially improves fidelity and scientific usefulness of synthetic simulations for chaotic physical systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-guided Machine Learning: Current Trends and Future Prospects', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2316.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2316.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FNO-Downscaling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fourier Neural Operator for Downscaling / Super-resolution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use FNO (and variants) to perform arbitrary-resolution downscaling (zero-shot or few-shot) of PDE solutions or coarse-grained climate fields by learning global frequency-domain mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fourier neural operators for arbitrary resolution climate data downscaling.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Climate and weather downscaling, super-resolution of spatio-temporal PDE solutions</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Convert coarse-scale simulation or observational fields to higher-resolution fields efficiently while preserving physical structure and multi-scale dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Training typically uses paired coarse–fine simulated datasets or uses operator learning on PDE solution families; fine-scale observational labels are often scarce.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Gridded spatio-temporal fields (images/rasters); sometimes irregular geometries with adapted variants.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High due to multi-scale interactions and need to predict small-scale features not present in coarse inputs; zero-shot downscaling imposes additional generalization demands.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging area; FNO and variants demonstrate promising results for climate downscaling and super-resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — physical consistency desirable; physics-informed operator variants and refiners help maintain fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Fourier Neural Operator (FNO) and variants for downscaling</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Learn global mappings in Fourier space from coarse inputs to fine outputs; can generate outputs at arbitrary spatial resolution and be combined with physics-informed training or refinement (PDE-Refiner).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Operator learning / supervised / zero-shot downscaling</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for gridded climate variables where operator mappings exist; requires representative training across regimes for zero-shot capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>FNO shows encouraging results for zero-shot or few-shot downscaling and arbitrary resolution predictions; refinement steps improve accuracy on neglected spectral components.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — enables scalable high-resolution predictions for climate applications without full high-resolution simulations, facilitating regional analyses and decision support.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms naive interpolation and many purely data-driven SR methods by leveraging operator structure and frequency-domain learning; needs physics-regularization for extreme fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Training on diverse PDE solutions or coarse–fine pairs, architectural choices that capture global structure (Fourier layers), and post-training refinement to recover low-amplitude spectral features.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Operator learning in frequency domain enables generalizable, arbitrary-resolution downscaling, but physics-aware refinement is often needed to restore full spectral fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-guided Machine Learning: Current Trends and Future Prospects', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2316.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2316.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FoundationModels-Env</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Foundation Models for Environmental Science (e.g., ClimaX)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large pre-trained models (transformer or related architectures) trained on broad environmental datasets to enable few-shot/zero-shot transfer across climate/weather/environmental tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Climax: A foundation model for weather and climate.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Climate and weather modeling, multi-task environmental prediction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Build large-scale models that learn general representations from diverse environmental datasets to be adapted to downstream predictive tasks with limited labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Environmental data are heterogeneous and often underrepresented in generic foundation model pretraining; but domain-specific datasets exist (reanalysis, satellite, model outputs) for targeted pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Multimodal spatio-temporal data (satellite imagery, reanalysis fields, station time series), heterogeneous scales and resolutions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Very high: multi-scale physical processes, non-stationarity, broad range of tasks and modalities, and computational cost of large model training.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Foundation models are mature in NLP/vision but nascent in environmental science; some dedicated models (ClimaX, FNO-based foundation-like models) have emerged.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High for scientific use — users require interpretability and mechanistic insights often lacking in generic LLMs; integrating scientific knowledge is essential.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Foundation model pretraining (transformers, large neural architectures) specialized to environmental data</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train large transformer-like architectures on wide collections of environmental data (reanalysis, simulations, observations) to learn general spatio-temporal representations; can be adapted via fine-tuning or prompting to specific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Foundation models / transfer learning / self-supervised pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Promising for multi-task and few-shot tasks across environmental domains, but constrained by data coverage, interpretability needs, and computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Early results (ClimaX, others) show promise in climate/weather tasks, but major challenges remain in interpreting predictions scientifically and in fine-tuning with limited observational data.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High if models can be made interpretable and effectively fine-tuned — could enable unified modeling across variables and scales and few-shot adaptation to poorly observed regions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Potentially more generalizable than task-specific ML models but more expensive and currently less interpretable; KGML approaches that inject physics into foundation models could combine strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of large, diverse environmental pretraining datasets, physics-informed training/prompting, efficient fine-tuning methods to avoid catastrophic forgetting, and cost-effective deployment strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Foundation-style pretraining on environmental data has great potential for transfer across tasks, but must be combined with scientific knowledge and careful fine-tuning to meet scientific interpretability and data-scarcity constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-guided Machine Learning: Current Trends and Future Prospects', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Physics-guided neural networks (pgnn): An application in lake temperature modeling. <em>(Rating: 2)</em></li>
                <li>Physics guided rnns for modeling dynamical systems: A case study in simulating lake temperature profiles. <em>(Rating: 2)</em></li>
                <li>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. <em>(Rating: 2)</em></li>
                <li>Fourier neural operator for parametric partial differential equations. <em>(Rating: 2)</em></li>
                <li>Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. <em>(Rating: 2)</em></li>
                <li>Physics-informed neural operator for learning partial differential equations. <em>(Rating: 1)</em></li>
                <li>Pde-refiner: Achieving accurate long rollouts with neural pde solvers. <em>(Rating: 1)</em></li>
                <li>Climax: A foundation model for weather and climate. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2316",
    "paper_id": "paper-268681120",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "KGML",
            "name_full": "Knowledge-guided Machine Learning",
            "brief_description": "A family of ML approaches that integrate scientific knowledge (equations, constraints, heuristics, simulations) with data-driven models to improve generalizability, scientific consistency, and interpretability in scientific modeling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Environmental & Earth sciences (general scientific modeling)",
            "problem_description": "Replace or augment process-based models and pure data-driven (black-box) ML to predict or simulate responses of physical/environmental systems (e.g., lake temperature, streamflow, climate fields) while respecting scientific knowledge.",
            "data_availability": "Often limited to moderate; many scientific domains are transitioning from data-poor to data-rich but observational coverage is heterogeneous (well-observed systems vs. many sparsely-observed systems); simulated data from process-based models frequently available but can be biased.",
            "data_structure": "Spatio-temporal data (gridded rasters, time series), graphs/networks (river/stream networks), multimodal (sensor, satellite, simulation outputs); often structured but irregular in space/time and sparse for some targets.",
            "problem_complexity": "High: multi-scale dynamics, nonlinear interactions, high dimensionality (spatio-temporal fields), partial observability, unknown confounders, computationally expensive forward models.",
            "domain_maturity": "Mature in the sense of well-established process-based models and physical laws, but many sub-processes are imperfectly known (partial/imperfect knowledge); domain expertise and legacy models exist.",
            "mechanistic_understanding_requirements": "High - scientific applications require mechanistic/physically consistent outputs and interpretability; black-box predictions alone are often insufficient.",
            "ai_methodology_name": "Knowledge-guided machine learning (KGML) — hybrid physics-ML approaches",
            "ai_methodology_description": "KGML encompasses methods that inject scientific knowledge via loss functions (constraints/regularizers), architecture (baked-in symmetries, modular decomposition), and pretraining (simulation-based or self-supervised tasks). Implementations include physics-guided losses, physics-informed networks, physics-aware architectures, pretraining on simulated data, and hybrid residual/HPD models.",
            "ai_methodology_category": "Hybrid / physics-informed ML / supervised & self-supervised learning (depending on instantiation)",
            "applicability": "Highly applicable across environmental scientific modeling problems where (partial) domain knowledge exists and observations are limited; requires careful balancing of knowledge vs data (weights/tolerances) and may need bespoke formulation per problem.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported to improve generalizability and scientific consistency relative to pure data-driven models; enables better out-of-distribution performance and interpretability when knowledge is appropriately incorporated.",
            "impact_potential": "High — can reduce computational costs (surrogates), improve predictions in data-sparse regimes, enable scientific insight consistent with theory, and facilitate transfer across locations/systems.",
            "comparison_to_alternatives": "KGML is positioned between process-based models (mechanistic, generalize via theory but expensive/biased) and black-box ML (flexible but poor OOD generalization); KGML aims to combine strengths of both.",
            "success_factors": "Availability of partial or full scientific knowledge, availability of simulated data, careful formulation of knowledge constraints (hard vs soft), modular architectures that reflect process decomposition, and pretraining strategies.",
            "key_insight": "Blending scientific knowledge with data-driven ML improves generalization and interpretability in scientific modeling, especially in data-sparse or out-of-distribution settings where pure ML fails and pure process models are imperfect.",
            "uuid": "e2316.0",
            "source_info": {
                "paper_title": "Knowledge-guided Machine Learning: Current Trends and Future Prospects",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "PGNN",
            "name_full": "Physics-Guided Neural Network (PGNN)",
            "brief_description": "An ML approach that adds physical/monotonic constraints as loss terms to neural network training to enforce scientifically consistent outputs (original application: lake temperature modeling).",
            "citation_title": "Physics-guided neural networks (pgnn): An application in lake temperature modeling.",
            "mention_or_use": "use",
            "scientific_problem_domain": "Lake temperature modeling (freshwater ecosystem modeling)",
            "problem_description": "Predict water temperature profiles from drivers (weather, depth, etc.) while ensuring outputs obey domain physics (e.g., density-depth monotonicity).",
            "data_availability": "Observational data limited for many lakes; some lakes have high-frequency sensor data, but many are sparsely observed — motivates physics guidance.",
            "data_structure": "Time series and profiles across depth (spatio-temporal, regularly sampled in depth/time for monitored lakes).",
            "problem_complexity": "Moderate-to-high: nonlinear thermodynamic processes, depth-resolved dynamics, intermediate latent heat fluxes; limited labeled targets.",
            "domain_maturity": "Process-based lake models exist (GLM, GLM-AED), but biogeochemical processes are less well understood; substantial domain expertise available.",
            "mechanistic_understanding_requirements": "High — models must respect physical relationships (e.g., density-depth) for scientific credibility.",
            "ai_methodology_name": "Physics-guided neural networks (PGNN)",
            "ai_methodology_description": "Feedforward neural networks trained with augmented loss functions that include penalties for violating known physical constraints (e.g., monotonicity, conservation laws), sometimes using process simulations as auxiliary inputs.",
            "ai_methodology_category": "Supervised learning with physics-informed regularization",
            "applicability": "Appropriate where partial/soft constraints exist and observations are limited; constraints help steer learned mappings to physically plausible regimes.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Improves physical consistency of predictions compared to unconstrained NN; used successfully in lake temperature tasks to enforce monotonic relationships and reduce nonsensical outputs.",
            "impact_potential": "Moderate-to-high for operational environmental prediction where physical plausibility is essential; reduces risk of spurious predictions.",
            "comparison_to_alternatives": "Compared to pure ML, PGNN yields more physically consistent outputs; compared to pure process models, PGNN can better fit observations where process models are biased or misspecified.",
            "success_factors": "Existence of clear, enforceable physical constraints (e.g., monotonic relations), ability to compute constraint residuals for loss, and careful weighting of physics vs. data loss.",
            "key_insight": "Embedding even simple physics as loss constraints can substantially increase the scientific plausibility and generalizability of ML predictions in domains with limited data.",
            "uuid": "e2316.1",
            "source_info": {
                "paper_title": "Knowledge-guided Machine Learning: Current Trends and Future Prospects",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "PGRNN",
            "name_full": "Physics-Guided Recurrent Neural Network (PGRNN)",
            "brief_description": "A recurrent neural network variant that incorporates energy conservation and intermediate heat-flux estimation into the loss/architecture for time-resolved lake temperature prediction.",
            "citation_title": "Physics guided rnns for modeling dynamical systems: A case study in simulating lake temperature profiles.",
            "mention_or_use": "use",
            "scientific_problem_domain": "Time-series forward modeling of lake temperature profiles (freshwater ecosystems)",
            "problem_description": "Predict temporal evolution of vertical temperature profiles in lakes by combining recurrent architectures with physics (heat transfer, energy conservation).",
            "data_availability": "Limited for many lakes — high-frequency sensor data for some lakes but many are sparsely observed; simulated data available for pretraining.",
            "data_structure": "Spatio-temporal time series (depth × time), sequential data suitable for RNNs.",
            "problem_complexity": "High temporal complexity (heat flux estimation, latent intermediate states), nonlinearity, need to model hidden fluxes and conservation constraints.",
            "domain_maturity": "Established process knowledge for thermodynamics exists but parameterizations and ecological components are imperfect.",
            "mechanistic_understanding_requirements": "High — model incorporates mechanistic energy balance to ensure physically plausible dynamics and to aid interpretability.",
            "ai_methodology_name": "Physics-guided recurrent neural network (PGRNN)",
            "ai_methodology_description": "RNN architecture that estimates intermediate physical fluxes and applies physics-based constraints (energy conservation) as part of the training objective; may use pretraining on simulated data.",
            "ai_methodology_category": "Supervised learning / physics-informed recurrent networks",
            "applicability": "Well-suited to temporal dynamical systems with known conservation laws and limited data; imposes mechanistic structure on sequence models.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported (per Read et al.) to significantly outperform both standard ML models and process-based models on out-of-distribution years/seasons with different weather compared to training data, demonstrating improved OOD generalization.",
            "impact_potential": "High for forecasting ecological state variables, enabling better generalization across seasons/years and across lakes with limited observations.",
            "comparison_to_alternatives": "Outperforms unconstrained ML and some process-based models on OOD tests; combines advantages of data fit (ML) with physical consistency (process knowledge).",
            "success_factors": "Explicit incorporation of conservation laws, estimating intermediate latent fluxes rather than only mapping inputs to outputs, and use of simulated pretraining to initialize parameters.",
            "key_insight": "Incorporating conservation laws into recurrent architectures yields models that generalize better across distributional shifts than black-box ML or imperfect process models alone.",
            "uuid": "e2316.2",
            "source_info": {
                "paper_title": "Knowledge-guided Machine Learning: Current Trends and Future Prospects",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "PINN",
            "name_full": "Physics-Informed Neural Networks (PINNs)",
            "brief_description": "Neural network framework that incorporates PDEs and boundary/initial conditions into training by penalizing PDE residuals in the loss, enabling solution of forward and inverse PDE problems.",
            "citation_title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Partial differential equations across physics and engineering (fluid dynamics, climate, etc.)",
            "problem_description": "Solve PDEs (forward) or infer PDE parameters/fields (inverse) without relying solely on numerical discretization, by training neural networks whose outputs satisfy governing equations.",
            "data_availability": "Can operate with scarce labeled data because PDE residuals provide supervisory signal; when data exist they can be combined with physics losses.",
            "data_structure": "Continuous function inputs/outputs (spatio-temporal fields) — can be treated as coordinate-to-solution mappings.",
            "problem_complexity": "High: solving nonlinear PDEs, multi-dimensional continuous domains, complex boundary conditions; optimization landscapes can be challenging.",
            "domain_maturity": "Well-established PDE theory; PINNs are a relatively recent ML approach (since ~2017) built for domains with strong mechanistic equations.",
            "mechanistic_understanding_requirements": "High — exact PDE form is assumed known and enforced during training for mechanistic fidelity.",
            "ai_methodology_name": "Physics-informed neural networks (PINNs)",
            "ai_methodology_description": "Feedforward neural networks trained with composite loss including data misfit and PDE residuals evaluated at collocation points; solves for continuous solutions and can incorporate boundary/initial conditions.",
            "ai_methodology_category": "Physics-informed ML / supervised with equation constraints",
            "applicability": "Appropriate when governing equations are known (perfect/complete knowledge) and one wants mesh-free PDE solvers or to infer unknown PDE coefficients; less suited when equations are highly imperfect.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "PINNs have enabled solving single-instance PDEs and inverse PDE problems, but face challenges (optimization, scalability) for complex/high-dimensional PDEs; many extensions (neural operators) seek to address limitations.",
            "impact_potential": "High for providing mesh-free, differentiable PDE solvers and for inverse problems; potential computational gains and ability to incorporate data and physics jointly.",
            "comparison_to_alternatives": "Compared to conventional numerical solvers: PINNs are flexible and mesh-free but may be slower to train and face optimization issues; compared to neural operators, PINNs typically solve single PDE instances rather than families of PDE solutions.",
            "success_factors": "Accurate specification of PDEs/boundary conditions, careful sampling of collocation points, loss weighting between data and physics, and architectural/training improvements to mitigate optimization pathologies.",
            "key_insight": "When governing equations are known, embedding them directly into NN training yields physics-consistent solutions and enables inverse estimation, but computational/training challenges limit straightforward application to large complex PDEs.",
            "uuid": "e2316.3",
            "source_info": {
                "paper_title": "Knowledge-guided Machine Learning: Current Trends and Future Prospects",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "NeuralOperator",
            "name_full": "Neural Operators (Fourier Neural Operator / DeepONet)",
            "brief_description": "Operator-learning neural architectures that learn mappings between function spaces (e.g., PDE initial/boundary conditions to solutions) enabling fast inference across a family of PDE instances and arbitrary resolution outputs.",
            "citation_title": "Fourier neural operator for parametric partial differential equations.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "PDE solving, surrogate modeling for physics (fluid dynamics, climate downscaling, etc.)",
            "problem_description": "Learn the solution operator mapping input functions (initial/boundary conditions, forcing) to output solution fields for families of PDEs to enable rapid, possibly zero-shot, evaluation at arbitrary spatial resolutions.",
            "data_availability": "Requires datasets of input–solution pairs (often from numerical solvers) — simulated data can be generated, so data can be abundant for training; however high-fidelity simulations are costly to produce.",
            "data_structure": "High-dimensional spatio-temporal fields; structured gridded data often used, but variants handle general geometries.",
            "problem_complexity": "High-dimensional function-to-function mapping; capturing multi-scale behavior and broad spectral content is challenging.",
            "domain_maturity": "Relatively recent but rapidly maturing; neural operator literature (FNO, DeepONet, variants) is active and growing.",
            "mechanistic_understanding_requirements": "Varies — neural operators can be purely data-driven or augmented with physics (PINO, physics-informed operator learning) to enforce constraints.",
            "ai_methodology_name": "Neural operator learning (FNO, DeepONet, PINO, variants)",
            "ai_methodology_description": "Architectures (e.g., FNO) transform fields into Fourier/frequency domains, learn kernel mappings via NNs, and invert transform to spatial domain; can be trained on families of PDE solutions to generalize across initial/boundary/parameter variations and produce arbitrary-resolution outputs.",
            "ai_methodology_category": "Supervised operator learning / physics-enhanced operator learning",
            "applicability": "Highly suitable for surrogate PDE solvers, fast emulation of expensive models, and zero-shot/few-shot downscaling; needs representative simulated datasets covering variability of interest.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Neural operators can produce accurate PDE solutions much faster at inference time and enable arbitrary-resolution outputs; issues include neglecting low-amplitude spectral components (addressed by PDE-Refiner) and need for physics constraints for robustness.",
            "impact_potential": "High — enables scalable surrogate modeling, rapid scenario evaluation, and potential zero-shot downscaling of climate/PDE solutions.",
            "comparison_to_alternatives": "Compared to PINNs: neural operators learn families of solutions and are more efficient for many-query settings; compared to numerical solvers: orders-of-magnitude speedup at inference but reliant on training data quality.",
            "success_factors": "Availability of diverse, representative simulated training data, architectural choices (Fourier layers, learned deformations), and physics-regularization/refinement techniques (e.g., PINO, PDE-Refiner) to recover full spectral fidelity.",
            "key_insight": "Learning the operator mapping between function spaces allows fast, generalizable surrogate solutions to PDE families and supports arbitrary-resolution predictions when trained on sufficient simulated variability.",
            "uuid": "e2316.4",
            "source_info": {
                "paper_title": "Knowledge-guided Machine Learning: Current Trends and Future Prospects",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "PGA-LSTM",
            "name_full": "Physics-Guided Architecture LSTM (PGA-LSTM)",
            "brief_description": "A recurrent LSTM architecture modified to bake in physics (density-depth relations) in the internal node connections to guarantee physically consistent temperature profiles and produce uncertainty estimates.",
            "citation_title": "Physics-guided architecture (pga) of neural networks for quantifying uncertainty in lake temperature modeling.",
            "mention_or_use": "use",
            "scientific_problem_domain": "Lake temperature prediction (vertical profiles)",
            "problem_description": "Model vertical temperature profiles ensuring each predicted realization respects density-depth physics and produce meaningful uncertainty quantification.",
            "data_availability": "Limited observational data for many lakes; some lakes have sensors enabling training and evaluation.",
            "data_structure": "Time series across depth (spatio-temporal sequences applicable to LSTM).",
            "problem_complexity": "Moderate: need to respect inter-layer (depth) constraints and capture temporal dynamics with uncertainty.",
            "domain_maturity": "Established physics for density-depth; process models exist but are computationally heavy and sometimes inaccurate in biogeochemistry.",
            "mechanistic_understanding_requirements": "High — requirement that each realization respects physical laws motivates architecture-level constraints.",
            "ai_methodology_name": "Physics-guided architecture in LSTM (PGA-LSTM)",
            "ai_methodology_description": "Modify LSTM connections/structure to encode density-depth physics so the architecture inherently enforces monotonic/physical relations, enabling physically valid samples and uncertainty estimates.",
            "ai_methodology_category": "Architecture-level physics-informed supervised learning",
            "applicability": "Appropriate when specific structural physics can be encoded in network connectivity; useful for enforcing hard constraints across model outputs.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Shown to produce uncertainty estimates and sample realizations guaranteed to respect density-depth physics; improves interpretability and scientific plausibility of predictions.",
            "impact_potential": "Moderate — improves trustworthiness of ML outputs in scientific settings by guaranteeing physics-compliant realizations and providing uncertainty quantification.",
            "comparison_to_alternatives": "Unlike loss-based PGNN, PGA-LSTM bakes constraints into architecture guaranteeing adherence for every sample rather than penalizing violations, reducing reliance on loss weighting.",
            "success_factors": "Clear mapping of physics to network structure, ability to redesign recurrent connections to reflect process interactions, and problem formulations where hard constraints are known.",
            "key_insight": "Embedding physics directly into network architecture can guarantee physically valid outputs and reliable uncertainty estimates, avoiding tradeoffs inherent in soft-constraint loss formulations.",
            "uuid": "e2316.5",
            "source_info": {
                "paper_title": "Knowledge-guided Machine Learning: Current Trends and Future Prospects",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Pretraining(Simulated)",
            "name_full": "Knowledge-Guided Pretraining Using Simulated Data",
            "brief_description": "Pretraining ML models on simulated outputs from process-based models (possibly with varied parameterizations) to initialize weights so that fine-tuning on limited real observations yields better generalization.",
            "citation_title": "Physics guided rnns for modeling dynamical systems: A case study in simulating lake temperature profiles.",
            "mention_or_use": "use",
            "scientific_problem_domain": "Environmental modeling (lakes, streams, hydrology, etc.)",
            "problem_description": "Leverage abundant simulated data to pretrain ML models so they capture physics-rooted patterns and then adapt to scarce observational data.",
            "data_availability": "Simulated data from process-based models are abundant (can be generated), while observational data are often sparse; simulated data may be biased due to imperfect parameterizations.",
            "data_structure": "Simulated spatio-temporal fields or time series; similar structure to observational targets.",
            "problem_complexity": "High due to mismatch/bias between simulation and reality and domain heterogeneity; requires strategies to diversify simulated parameterizations.",
            "domain_maturity": "Process models mature and available, enabling generation of simulated datasets for pretraining.",
            "mechanistic_understanding_requirements": "Medium-to-high — pretraining uses mechanistic simulations to encode physics but final model must still adapt to real-world deviations.",
            "ai_methodology_name": "Simulation-based pretraining / meta-learning on simulated datasets",
            "ai_methodology_description": "Pretrain on large collections of simulated runs (possibly varying physical parameters) to learn robust features; may use meta-learning to initialize models for quick adaptation; then fine-tune on limited observations, possibly with regularization to avoid catastrophic forgetting.",
            "ai_methodology_category": "Pretraining / transfer learning / meta-learning",
            "applicability": "Well-suited when process-based simulators exist and can produce varied datasets; must address simulator bias and catastrophic forgetting during fine-tuning.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported improvements in data-sparse and out-of-sample scenarios (Jia et al.); benefits depend on diversity/realism of simulated training data and adaptation strategy.",
            "impact_potential": "High — reduces observational data requirements for ML deployment and enables transfer across poorly monitored systems.",
            "comparison_to_alternatives": "Outperforms naive training on scarce observations by providing physics-informed initialization; requires mitigation of simulator bias unlike purely observational fine-tuning.",
            "success_factors": "Diversity of simulated parameterizations, meta-learning strategies to learn transferable initializations, careful fine-tuning protocols to prevent catastrophic forgetting.",
            "key_insight": "Pretraining on diverse simulated datasets encodes physics into ML models and can substantially improve performance in data-sparse, out-of-distribution settings if simulator bias and forgetting are managed.",
            "uuid": "e2316.6",
            "source_info": {
                "paper_title": "Knowledge-guided Machine Learning: Current Trends and Future Prospects",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "SurrogateModeling",
            "name_full": "ML-based Surrogate Modeling for Process-based Simulators",
            "brief_description": "Use ML (including neural operators and other surrogate models) trained on simulator outputs to emulate expensive process-based models, enabling many-query scenarios and faster inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Climate modeling, hydrology, fluid dynamics, environmental impact assessment",
            "problem_description": "Replace or accelerate expensive numerical simulations (PDE solvers, high-fidelity models) with learned surrogates for tasks like uncertainty quantification, optimization, and rapid forecasting.",
            "data_availability": "Training requires simulated input-output pairs from high-fidelity models — data generation can be expensive but controllable; observational data may be used to correct bias.",
            "data_structure": "High-dimensional spatio-temporal fields, parameter-to-field mappings; structured gridded data common.",
            "problem_complexity": "High computational complexity in original simulator; learned surrogate must capture multi-scale dynamics and produce stable long-rollout predictions.",
            "domain_maturity": "Process-based simulators are mature; surrogate modeling with ML is an active research area with diverse methods.",
            "mechanistic_understanding_requirements": "Medium — surrogate must preserve essential mechanistic behavior; physics-guided constraints often applied to ensure consistency.",
            "ai_methodology_name": "Surrogate ML models (neural operators, CNNs, Koopman approaches, etc.)",
            "ai_methodology_description": "Train ML models on simulated outputs to approximate forward simulator mapping; can include physics constraints, multi-step refinement (e.g., PDE-Refiner), and operator-learning to generalize across parameter spaces.",
            "ai_methodology_category": "Supervised learning / operator learning / physics-informed surrogates",
            "applicability": "Highly applicable when many-query or real-time inference is needed and when simulators can provide training data; requires attention to spectral fidelity and long-horizon accuracy.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Surrogates give large inference speedups and can match accuracy of numerical solvers in many cases; refinement techniques (diffusion-based refiners) can address missing spectral components.",
            "impact_potential": "High — enables scalable uncertainty quantification, ensemble forecasting, and operational uses of expensive models that were previously infeasible.",
            "comparison_to_alternatives": "Surrogates are faster than numerical solvers but may require corrective refinements to match fine-scale spectral content; operator-learning offers advantages for families of PDEs over single-instance PINNs.",
            "success_factors": "Sufficient diversity and fidelity of simulated training data, operator architectures that capture global structure, and refinement/physics-regularization to recover lost components.",
            "key_insight": "ML surrogates can dramatically reduce computational costs for simulation tasks if trained on representative simulated data and combined with physics-aware refinement to preserve essential dynamics.",
            "uuid": "e2316.7",
            "source_info": {
                "paper_title": "Knowledge-guided Machine Learning: Current Trends and Future Prospects",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "InverseModeling(dPL)",
            "name_full": "Differentiable Parameter Learning (dPL) / ML-driven Inverse Modeling",
            "brief_description": "Use differentiable, learnable process models or ML models (including invertible NNs) to infer latent physical parameters or states from observed inputs/outputs, improving calibration and state estimation.",
            "citation_title": "Differentiable, learnable, regionalized process-based models with multiphysical outputs can approach state-of-the-art hydrologic prediction accuracy.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Hydrology, seismic imaging, geothermal systems, photonics (inverse PDE problems)",
            "problem_description": "Estimate static or slowly-varying physical parameters (e.g., soil, hydraulic conductivity) or latent states by learning inverse mappings from observations, or by differentiable calibration of process-based models using gradient-based optimization.",
            "data_availability": "Often sparse: direct measurements of parameters are expensive or infeasible; observational data of inputs/outputs used for calibration/inference.",
            "data_structure": "Time series, spatio-temporal observations; often irregular and incomplete.",
            "problem_complexity": "High nonlinearity and ill-posed inverse problems, multimodal parameter spaces, computational cost from multiple forward runs mitigated by differentiable models.",
            "domain_maturity": "Traditional inverse methods mature (Bayesian, grid search); differentiable parameter learning is an emerging ML-enabled approach for geosciences.",
            "mechanistic_understanding_requirements": "High — inferred parameters are interpretable and used within mechanistic simulators; scientific plausibility is critical.",
            "ai_methodology_name": "Differentiable parameter learning / invertible neural networks for inverse modeling",
            "ai_methodology_description": "Embed ML modules inside differentiable process-based simulators or use invertible NN architectures to learn inverse maps; use automatic differentiation to compute gradients and perform parameter inference efficiently.",
            "ai_methodology_category": "Supervised/unsupervised inverse learning / differentiable programming",
            "applicability": "Appropriate when forward models can be implemented differentiably or when invertible architectures exist; speeds calibration and improves parameter estimation in data-limited contexts.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "dPL and related approaches allow efficient gradient-based calibration and can approach state-of-the-art predictive accuracy in hydrology; they enable learning spatially varying parameters modeled by separate NN modules.",
            "impact_potential": "High for automating calibration, improving state estimation, and informing model parameters that are otherwise hard to measure.",
            "comparison_to_alternatives": "Compared to grid/Bayesian search, differentiable approaches are more efficient (gradient-based) and can scale better; interpretability retains advantage over black-box inversions.",
            "success_factors": "Availability of differentiable implementations of forward models, good initialization, and incorporation of constraints or priors to regularize ill-posed inversions.",
            "key_insight": "Making process-based models differentiable and coupling them with ML enables efficient inverse parameter learning that retains interpretability while leveraging gradient-based optimization.",
            "uuid": "e2316.8",
            "source_info": {
                "paper_title": "Knowledge-guided Machine Learning: Current Trends and Future Prospects",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "PhysicsReg-Generative",
            "name_full": "Physics-Regularized Generative Models",
            "brief_description": "Generative ML models (GANs, diffusion models, VAEs) augmented with physical constraints or conservation laws to produce physically consistent synthetic simulations (e.g., turbulent flows, precipitation fields).",
            "citation_title": "here",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Fluid dynamics (turbulence), climate nowcasting (precipitation), materials science",
            "problem_description": "Generate synthetic spatio-temporal fields consistent with physical laws for data augmentation, downscaling, or initial condition refinement while avoiding spurious artifacts common in unconstrained generative models.",
            "data_availability": "Training data (simulations, observations) can be scarce for high-fidelity regimes; generative models often require many examples but physics-regularization can compensate.",
            "data_structure": "High-dimensional spatio-temporal fields, images, or volumetric data.",
            "problem_complexity": "High nonlinearity and chaotic dynamics; generative fidelity requires preserving statistical and physical invariants across scales.",
            "domain_maturity": "Generative modeling mature in vision/NLP; physics-regularized generative approaches are emergent in scientific domains.",
            "mechanistic_understanding_requirements": "High — outputs must satisfy conservation laws or statistical constraints for scientific validity.",
            "ai_methodology_name": "Physics-regularized GANs and diffusion models",
            "ai_methodology_description": "Add physics-based loss terms (e.g., conservation penalties), constraint enforcement, or physics-informed denoising schedules (for diffusion models) to generative training; can also refine outputs of operator surrogates (PDE-Refiner).",
            "ai_methodology_category": "Generative modeling with physics-informed regularization",
            "applicability": "Useful for producing realistic simulations, downscaling, and refining ML-PDE solvers; effective when physics constraints are known and can be computed on generated samples.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Physics-regularized generative models reduce spurious patterns and yield more physically realistic outputs in turbulent flow simulations and precipitation nowcasting; diffusion-based refiners improve neglected spectral components.",
            "impact_potential": "High for generating realistic synthetic datasets, improving surrogate predictions, and enabling data augmentation for scarce regimes.",
            "comparison_to_alternatives": "Outperforms unconstrained generative models in physical fidelity; complements operator-learning surrogates by refining spectral content.",
            "success_factors": "Explicit physically-computable constraints, ability to evaluate physics residuals on synthetic samples, and architectures/training schemes (e.g., diffusion refiners) suited for multi-scale dynamics.",
            "key_insight": "Enforcing physical constraints during generative training materially improves fidelity and scientific usefulness of synthetic simulations for chaotic physical systems.",
            "uuid": "e2316.9",
            "source_info": {
                "paper_title": "Knowledge-guided Machine Learning: Current Trends and Future Prospects",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "FNO-Downscaling",
            "name_full": "Fourier Neural Operator for Downscaling / Super-resolution",
            "brief_description": "Use FNO (and variants) to perform arbitrary-resolution downscaling (zero-shot or few-shot) of PDE solutions or coarse-grained climate fields by learning global frequency-domain mappings.",
            "citation_title": "Fourier neural operators for arbitrary resolution climate data downscaling.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Climate and weather downscaling, super-resolution of spatio-temporal PDE solutions",
            "problem_description": "Convert coarse-scale simulation or observational fields to higher-resolution fields efficiently while preserving physical structure and multi-scale dynamics.",
            "data_availability": "Training typically uses paired coarse–fine simulated datasets or uses operator learning on PDE solution families; fine-scale observational labels are often scarce.",
            "data_structure": "Gridded spatio-temporal fields (images/rasters); sometimes irregular geometries with adapted variants.",
            "problem_complexity": "High due to multi-scale interactions and need to predict small-scale features not present in coarse inputs; zero-shot downscaling imposes additional generalization demands.",
            "domain_maturity": "Emerging area; FNO and variants demonstrate promising results for climate downscaling and super-resolution.",
            "mechanistic_understanding_requirements": "Medium — physical consistency desirable; physics-informed operator variants and refiners help maintain fidelity.",
            "ai_methodology_name": "Fourier Neural Operator (FNO) and variants for downscaling",
            "ai_methodology_description": "Learn global mappings in Fourier space from coarse inputs to fine outputs; can generate outputs at arbitrary spatial resolution and be combined with physics-informed training or refinement (PDE-Refiner).",
            "ai_methodology_category": "Operator learning / supervised / zero-shot downscaling",
            "applicability": "Appropriate for gridded climate variables where operator mappings exist; requires representative training across regimes for zero-shot capabilities.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "FNO shows encouraging results for zero-shot or few-shot downscaling and arbitrary resolution predictions; refinement steps improve accuracy on neglected spectral components.",
            "impact_potential": "High — enables scalable high-resolution predictions for climate applications without full high-resolution simulations, facilitating regional analyses and decision support.",
            "comparison_to_alternatives": "Outperforms naive interpolation and many purely data-driven SR methods by leveraging operator structure and frequency-domain learning; needs physics-regularization for extreme fidelity.",
            "success_factors": "Training on diverse PDE solutions or coarse–fine pairs, architectural choices that capture global structure (Fourier layers), and post-training refinement to recover low-amplitude spectral features.",
            "key_insight": "Operator learning in frequency domain enables generalizable, arbitrary-resolution downscaling, but physics-aware refinement is often needed to restore full spectral fidelity.",
            "uuid": "e2316.10",
            "source_info": {
                "paper_title": "Knowledge-guided Machine Learning: Current Trends and Future Prospects",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "FoundationModels-Env",
            "name_full": "Foundation Models for Environmental Science (e.g., ClimaX)",
            "brief_description": "Large pre-trained models (transformer or related architectures) trained on broad environmental datasets to enable few-shot/zero-shot transfer across climate/weather/environmental tasks.",
            "citation_title": "Climax: A foundation model for weather and climate.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Climate and weather modeling, multi-task environmental prediction",
            "problem_description": "Build large-scale models that learn general representations from diverse environmental datasets to be adapted to downstream predictive tasks with limited labeled data.",
            "data_availability": "Environmental data are heterogeneous and often underrepresented in generic foundation model pretraining; but domain-specific datasets exist (reanalysis, satellite, model outputs) for targeted pretraining.",
            "data_structure": "Multimodal spatio-temporal data (satellite imagery, reanalysis fields, station time series), heterogeneous scales and resolutions.",
            "problem_complexity": "Very high: multi-scale physical processes, non-stationarity, broad range of tasks and modalities, and computational cost of large model training.",
            "domain_maturity": "Foundation models are mature in NLP/vision but nascent in environmental science; some dedicated models (ClimaX, FNO-based foundation-like models) have emerged.",
            "mechanistic_understanding_requirements": "High for scientific use — users require interpretability and mechanistic insights often lacking in generic LLMs; integrating scientific knowledge is essential.",
            "ai_methodology_name": "Foundation model pretraining (transformers, large neural architectures) specialized to environmental data",
            "ai_methodology_description": "Train large transformer-like architectures on wide collections of environmental data (reanalysis, simulations, observations) to learn general spatio-temporal representations; can be adapted via fine-tuning or prompting to specific tasks.",
            "ai_methodology_category": "Foundation models / transfer learning / self-supervised pretraining",
            "applicability": "Promising for multi-task and few-shot tasks across environmental domains, but constrained by data coverage, interpretability needs, and computational cost.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Early results (ClimaX, others) show promise in climate/weather tasks, but major challenges remain in interpreting predictions scientifically and in fine-tuning with limited observational data.",
            "impact_potential": "High if models can be made interpretable and effectively fine-tuned — could enable unified modeling across variables and scales and few-shot adaptation to poorly observed regions.",
            "comparison_to_alternatives": "Potentially more generalizable than task-specific ML models but more expensive and currently less interpretable; KGML approaches that inject physics into foundation models could combine strengths.",
            "success_factors": "Availability of large, diverse environmental pretraining datasets, physics-informed training/prompting, efficient fine-tuning methods to avoid catastrophic forgetting, and cost-effective deployment strategies.",
            "key_insight": "Foundation-style pretraining on environmental data has great potential for transfer across tasks, but must be combined with scientific knowledge and careful fine-tuning to meet scientific interpretability and data-scarcity constraints.",
            "uuid": "e2316.11",
            "source_info": {
                "paper_title": "Knowledge-guided Machine Learning: Current Trends and Future Prospects",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Physics-guided neural networks (pgnn): An application in lake temperature modeling.",
            "rating": 2,
            "sanitized_title": "physicsguided_neural_networks_pgnn_an_application_in_lake_temperature_modeling"
        },
        {
            "paper_title": "Physics guided rnns for modeling dynamical systems: A case study in simulating lake temperature profiles.",
            "rating": 2,
            "sanitized_title": "physics_guided_rnns_for_modeling_dynamical_systems_a_case_study_in_simulating_lake_temperature_profiles"
        },
        {
            "paper_title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.",
            "rating": 2,
            "sanitized_title": "physicsinformed_neural_networks_a_deep_learning_framework_for_solving_forward_and_inverse_problems_involving_nonlinear_partial_differential_equations"
        },
        {
            "paper_title": "Fourier neural operator for parametric partial differential equations.",
            "rating": 2,
            "sanitized_title": "fourier_neural_operator_for_parametric_partial_differential_equations"
        },
        {
            "paper_title": "Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators.",
            "rating": 2,
            "sanitized_title": "deeponet_learning_nonlinear_operators_for_identifying_differential_equations_based_on_the_universal_approximation_theorem_of_operators"
        },
        {
            "paper_title": "Physics-informed neural operator for learning partial differential equations.",
            "rating": 1,
            "sanitized_title": "physicsinformed_neural_operator_for_learning_partial_differential_equations"
        },
        {
            "paper_title": "Pde-refiner: Achieving accurate long rollouts with neural pde solvers.",
            "rating": 1,
            "sanitized_title": "pderefiner_achieving_accurate_long_rollouts_with_neural_pde_solvers"
        },
        {
            "paper_title": "Climax: A foundation model for weather and climate.",
            "rating": 1,
            "sanitized_title": "climax_a_foundation_model_for_weather_and_climate"
        }
    ],
    "cost": 0.026120499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Knowledge-guided Machine Learning: Current Trends and Future Prospects
1 May 2024</p>
<p>Anuj Karpatne karpatne@vt.edu 
Department of Computer Science
Virginia Tech</p>
<p>Xiaowei Jia xiaowei@pitt.edu 
Department of Computer Science
University of Pittsburgh</p>
<p>Vipin Kumar kumar001@umn.edu 
Department of Computer Science and Engineering
University of Minnesota</p>
<p>Knowledge-guided Machine Learning: Current Trends and Future Prospects
1 May 2024DCC55F675783847F1853A0592B5720A1arXiv:2403.15989v2[cs.LG]
This paper presents an overview of scientific modeling and discusses the complementary strengths and weaknesses of ML methods for scientific modeling in comparison to process-based models.It also provides an introduction to the current state of research in the emerging field of scientific knowledge-guided machine learning (KGML) that aims to use both scientific knowledge and data in ML frameworks to achieve better generalizability, scientific consistency, and explainability of results.We discuss different facets of KGML research in terms of the type of scientific knowledge used, the form of knowledge-ML integration explored, and the method for incorporating scientific knowledge in ML.We also discuss some of the common categories of use cases in environmental sciences where KGML methods are being developed, using illustrative examples in each category.</p>
<p>Introduction</p>
<p>As advances in artificial intelligence (AI) and machine learning (ML) continue to revolutionize mainstream applications in commercial problems, there is a growing excitement in the scientific community to harness the power of ML for accelerating scientific discovery [1,86,151,181].This is especially true in environmental sciences that are rapidly transitioning from being data-poor to data-rich, e.g., with the ever-increasing volumes of environmental data being collected by Earth observing satellites, in-situ sensors, and those generated by model simulations (e.g., climate model runs [113]).Similar to how recent developments in ML has transformed how we interact with the information on the Internet, it is befitting to ask how ML advances can enable Earth system scientists to transform a fundamental goal in science, which is to build better models of physical, biological, and environmental systems.</p>
<p>The conventional approach for modeling relationships between input drivers and response variables is to use process-based models rooted in scientific equations.Despite their ability to leverage the mechanistic understanding of scientific phenomena, process-based models suffer from several shortcomings limiting their adoption in complex real-world settings, e.g., due to imperfections in model formulations (or modeling bias), incorrect choices of parameter values in equations, and high computational costs in running high-fidelity simulations.In response to these challenges, ML methods offer a promising alternative to capture statistical relationships between inputs and outputs directly from data.However, "black-box" ML models, that solely rely on the supervision contained in data, show limited generalizability in scientific problems, especially when applied to out-of-distribution data.One of the reasons for this lack of generalizability is the limited scale of data in scientific disciplines in contrast to mainstream applications of AI and ML where large-scale datasets in computer vision and natural language modeling have been instrumental in the success of state-of-the-art AI/ML models.Another fundamental deficiency in black-box ML models is their tendency to produce results that are inconsistent with existing scientific theories and their inability to provide a mechanistic understanding of discovered patterns and relationships from data, limiting their usefulness in science.In contrast to black-box ML, a new generation of ML methods is being developed that leverage the information contained in data without ignoring the wealth of knowledge available in scientific disciplines.This emerging field of research in ML is referred to as scientific knowledge-guided machine learning (KGML) [77] where scientific theories (e.g., the principle of mass and energy conservation) are used to guide the construction and training of ML models for improved generalizability and scientific consistency.Even though the idea of incorporating scientific knowledge in ML is relatively recent [37,77,129], this area has seen a surge in interest that is reflected in the large volume of papers published just in the last few years on topics related to KGML in a wide range of physical, biological, and environmental applications (see a recent book [78] and survey articles [76,148,163]).</p>
<p>This paper aims to accomplish the following goals.First, it provides the ML community with a gentle introduction to the field of scientific modeling and discusses complementary strengths and weaknesses of process-based and ML models (Section 2).Second, this paper provides an easy entry point to the field of KGML for a broad research audience, by summarizing prior research in the field in a simple and accessible format using a three-dimensional view, namely the type of scientific knowledge used, the form of knowledge-ML integration deployed, and the method for incorporating scientific knowledge in ML (Section 3).Third, it provides a detailed view of three major research methods for integrating scientific knowledge in ML frameworks (Section 4).Fourth, it describes research in KGML from the context of four primary use-cases in scientific disciplines: forward modeling, inverse modeling, generative modeling, and downscaling (Section 5).Finally, it highlights emerging opportunities for KGML research in building foundation models in environmental science (Section 6), and provides concluding remarks and future directions of research in this field (Section 7).</p>
<p>Background on Scientific Modeling: Opportunities and Challenges</p>
<p>One of the fundamental pursuits in science is to understand the nature of the physical world by building computational models that are capable of predicting (or simulating) the response of physical/environmental systems.Figure 1 provides a brief overview of the generic framework of scientific modeling, where the goal is to model relationships between input drivers and response variables.For example, consider the problem of modeling lake water quality.Here, we are interested in capturing relationships between input drivers such as the temperature of air above the lake and the net flow of nutrients to the lake (e.g., through agricultural discharge), and response variables of the lake such as the temperature of the water and concentrations of chemical and biological constituents such as dissolved oxygen and chlorophyll content.Note that both the input drivers and response variables can have spatial and temporal structures and can be represented as spatio-temporal rasters (available on regular grids), time-varying graphs (available on irregular grids), or time series.Environmental data for both driver and response variables are generally collected through ground-based sensors, satellites, or simulation runs of other process-based models.Further, while we expect the input drivers to be relatively wellobserved, the response variables are generally hard to measure directly and thus have a limited number of observations at desired spatial and temporal scales.By modeling relationships between input drivers and response variables, an important end-goal of scientific modeling is to improve our understanding of scientific systems in novel (or unseen) testing scenarios, where the distributions of input and response variables are different than those observed while building the model.</p>
<p>Process-based Models</p>
<p>The conventional approach for scientific modeling is to use process-based models (also referred to as physics-based or science-based models), where the solution structure of relationships between inputs and response variables is rooted in scientific equations (e.g., laws of energy and mass conservation).In particular, process-based models make use of scientific equations to infer the evolution of time-varying latent or hidden variables of the system, also referred to as system states (see Figure 1).Additionally, process-based models often involve model parameters that need to be specified or calibrated (often with the help of observational data) for approximating real-world phenomena.For example, state-ofthe-art models in lake modeling such as the general lake model (GLM) [62] involve parameters such as water clarity that can dramatically change the nature of relationships between inputs and outputs and have to be tuned for every lake based on observations of input-output pairs.Taken together, the conventional paradigm of scientific modeling-developed over centuries of systematic research-forms the foundation of our present-day understanding of scientific systems across a wide spectrum of problems in environmental sciences.</p>
<p>A key feature of process-based models is their ability to provide a mechanistic understanding of the cause-effect mechanisms between input and output variables that can be used as a building block for advancing scientific knowledge.As a result, process-based models are continually updated and improved by the scientific community to fill knowledge gaps in current modeling standards and discover new theories and formulations of scientific equations that better match with observations and are scientifically meaningful and explainable.Since process-based models are rooted in scientific equations that are assumed to hold true in any testing scenario, they are also expected to easily generalize even outside the data used during model building.For example, process-based models can be made to extrapolate in space (e.g., over different geographic regions), in time (e.g., forecasting the future of a system under varying forcings of input drivers), or in scale (e.g., discovering emergent properties of larger-scale systems using models of smaller-scale system components).</p>
<p>Limitations of Process-based Models</p>
<p>Despite the widespread use of process-based models in almost every domain of science, they suffer from certain research gaps limiting their effectiveness in many real-world scientific applications.First, process-based models often involve several parameters that have to be calibrated for every new system using observations specific to the system.For example, in the problem of modeling the quality of water in lakes, there are several hundreds of parameters in process-based models such as GLM-Aquatic EcoDynamics (GLM-AED) [61] capturing different aspects of lake thermodynamics and chemical/biological processes that need to be calibrated for every lake using observational data.While it is still possible to calibrate model parameters in highly-observed systems with a sufficient number of data samples, process-based models are difficult to generalize in sparsely-observed systems where the underlying model parameters can be difficult to estimate using limited (or sometimes non-existent) observations.For example, we have a sufficiently good number of observations in a few highly-monitored lakes but for the vast population of lakes in the U.S. and across the world, we do not have dedicated programs for high-resolution monitoring of lake water quality, making it challenging to calibrate process-based models with little to no data.</p>
<p>Second, process-based models frequently suffer from several imperfections (or bias) in their solution structure (e.g., due to incomplete understanding of the system or approximations introduced in the model to speed up model runs), leading to poor generalizability of model outputs.For example, while state-of-the-art models in lake modeling such as the GLM [62] adequately capture the thermodynamic processes responsible for mass and energy balance in lakes, the processes governing the biogeochemistry of lake nutrients such as dissolved oxygen, organic matter, and phytoplankton counts are less wellunderstood.Such processes are generally approximated using over-parameterized forms of weaklyinformed equations, introducing large modeling errors and uncertainties.Furthermore, the introduced over-parametrizations are often difficult to understand and relate to physically meaningful quantities, making them fall short in serving the goal of explainability.</p>
<p>Third, process-based models are often computationally expensive to be run at the required scales in operational settings.In particular, in many scientific problems, learning the 'forward' mapping from input drivers to response variables (also referred to as the problem of forward modeling) requires finding solutions to complicated scientific equations often involving complex numerical operations.On the other hand, learning the 'inverse mapping' from observed inputs and outputs to model parameters (referred to as the problem of inverse modeling) is also a computationally demanding process as it requires multiple runs of the forward model for every round of model inversion [10,106,142].For example, in the problem of lake modeling, custom-calibrating the GLM model is both labor-and computation-intensive, and hence there is a trade-off between increasing the accuracy of GLM estimates and expanding the feasibility of study to a large number of lakes.</p>
<p>Opportunities for ML in Scientific Modeling</p>
<p>To address the research gaps of process-based models in scientific modeling, ML methods offer a promising alternative to capture relationships between input drivers and output response variables directly from data.ML models are similar to process-based models in many ways.For example, both ML and process-based models pursue the common goal of mapping inputs to outputs.Just like the parameters of process-based models, many ML models also involve several learnable parameters (e.g., the weights and biases of deep neural networks) that have to be inferred from data often using computationally efficient optimization techniques such as first-order gradient descent.</p>
<p>However, ML models also show fundamental differences from process-based models endowing them with complementary advantages.First, while the functional forms of process-based models may have imperfections that are difficult to fix using data, the solution structure of ML models (e.g., deep learning models) use generic architectures that are highly expressive and capable of capturing arbitrarily complex relationships between inputs and outputs provided they are fed with adequate training data.Second, ML methods (e.g., deep learning methods) are very fast at making forward inferences using simple matrix operations that are amenable to modern GPU architectures.As a result, ML models, once trained, can offer significant computational gains in modeling complicated scientific systems in comparison to process-based models employing complex numerical operations.Third, a unique advantage of many ML frameworks (e.g., deep learning) is that the parameters of ML models trained over a certain system can be used as a starting point to initialize the learning of parameters in a new system.A number of research strategies have been developed over the years in ML in the fields of transfer learning and meta learning to enable the transfer of learning from source tasks with a sufficient number of data samples to target tasks with little to no data, possibly even with shifts in distributions of input and output variables.Building upon the idea of transfer learning in mainstream ML applications of computer vision and natural language understanding, there is a growing trend to create Foundation models [13] such as chatGPT [121] that have been pre-trained on a wide variety of tasks to learn universal feature representations of data, which can be fine-tuned on a new task with very few data.This inherent transferability of learning in ML models provides a novel opportunity in environmental sciences to build broad-scale models that leverage knowledge from a large number of well-observed systems and can be fine-tuned on new less-observed systems with very few (or even no) observations.</p>
<p>Challenges for Black-box ML</p>
<p>Despite their promise, black-box ML models-that are designed, trained, and deployed agnostic to scientific theories-have led to spectacular failures in scientific problems <a href="e.g., the rise and fall of Meta AI's Galactica [57] and Google Flu Trends [92]">16,92,116</a>.This is primarily because black-box ML models solely rely on the supervision contained in data and hence are only as good as the data they are trained with.In problems where we have limited coverage of data distributions in the training set, black-box ML models can fail to generalize on out-of-distribution samples and produce results that are not only inaccurate but also nonsensical and incoherent with existing scientific theories while being highly overconfident.This is in contrast to process-based models that are designed to produce robust and scientifically consistent predictions even if they are inaccurate due to poor calibration of parameters or modeling imperfections.One strategy to improve the generalization capability of black-box ML models is to scale the size and diversity of training data along with increasing the size of models, which is the approach pursued in the creation of Foundation models [13].While this is possible in mainstream applications of ML where we have Internet-scale data with millions [32] or even trillions [120] of labeled samples, the nature of scientific data is very different that makes it infeasible to match the scales of labeled data possible in commercial domains.In particular, scientific systems exhibit rich heterogeneity in the distributions of input and output variables over space and time, with complex interactions of processes operating at multiple scales with varying memory footprints, often involving unknown confounding factors.This makes it difficult to cover the full spectrum of possible data distributions that one may encounter when deploying ML models in scientific applications.</p>
<p>Another fundamental limitation of black-box ML models is their inability to generate explainable hypotheses of the relationships between input and output variables in a manner consistent with existing scientific theories.Since one of the primary goals in science is to explain the cause-effect mechanisms of phenomena observed in the physical world, a black-box model that achieves somewhat good predictive performance but fails to deliver a mechanistic understanding of the underlying processes is unfit for being used as a basis for subsequent scientific developments.There is a growing interest in machine learning to incorporate the tools of causal inference [124,125] to move beyond learning associations in data (which is what most predictive models do) to designing interventions and eventually reasoning with counterfactuals.Scientific problems provide a unique opportunity to study causal reasoning rooted in existing theories of cause-effect mechanisms available as process-based models.</p>
<p>Knowledge-guided Machine Learning (KGML): An Overview</p>
<p>Despite the challenges in using black-box ML, modeling of environmental systems also presents novel opportunities for developing new advances in ML that leverage the unique characteristics of data encountered in scientific domains.For example, data collected in environmental sciences capture valuable information about a variety of Earth system processes operating at different spatial and temporal scales.While a given data set may only provide a partial view of the complex nexus of physical processes evolving over space and time, they represent real-world phenomena grounded in core scientific theories (e.g., the principle of mass and energy conservation).As a result, there is a growing interest in scientific and ML communities to leverage scientific knowledge in ML frameworks to produce explainable and scientifically grounded solutions that are likely to generalize even on out-of-distribution samples with limited training data.Research in this emerging field is referred to as scientific knowledge-guided machine learning (KGML) [77,78] where both scientific knowledge and data are used at an equal footing as complementary sources of supervision in ML frameworks, in contrast to first-principle process-based models and data-only (black-box) models (see Figure 2(a)).KGML is a rapidly growing research community that has seen several notable examples of promising research in recent years.One of the first major efforts to present the roadmap for research in KGML includes a 2017 perspective article [77] that laid the foundation of several research methods for combining scientific knowledge in ML framework.This consequently sparked ample interest in the ML and scientific communities to pursue research in KGML, as evidenced by the large volume of literature published in the last few years including survey articles [76,148,163], perspective papers [134,172], and a recent book on KGML [78].</p>
<p>A Multi-dimensional View of KGML Research</p>
<p>KGML has quickly grown to become a thriving area of research that is reflected in the diversity of research communities that have emerged to leverage KGML in the context of varied scientific problems.As a result, research in KGML is also referred to by many names including 'theory-guided data science', 'physics-guided machine learning', 'science-guided machine learning', and 'physics-informed machine learning'.While all of these terms refer to the common goal of integrating scientific knowledge with ML, it is important to understand the differentiating aspects of KGML research that lead to its rich diversity.We present these differentiating aspects in the form of a three-dimensional view of prior research in KGML: type of scientific knowledge (ranging from perfect and complete to imperfect and partial), form of knowledge-ML integration (ranging from process-centric to ML-centric), and method for incorporating scientific knowledge in ML (with choices including knowledge-guided learning, architecture, or pretraining).See Figure 2(b) for a schematic depiction of these three dimensions, which represent different steps or decisions one must take to take to select a KGML formulation best-suited for a scientific problem.They also help in categorizing previous research in KGML and exposing commonalities between research directions explored in diverse scientific communities.We briefly describe each of these three dimensions in the following.</p>
<p>Type of Scientific Knowledge</p>
<p>The first aspect that one must decide when working on a KGML problem is the type of scientific knowledge available in the problem.Scientific knowledge is available in diverse formats in various disciplines, including first-principle equations, domain-specific laws, rules, heuristics, or ontological relationships.In many scientific applications, we only have partial knowledge of the system (i.e., we may not have complete knowledge of all system processes) or we have to deal with approximate terms in scientific equations.We refer to such systems as having partial and/or imperfect scientific knowledge.</p>
<p>A seminal line of work in incorporating partial and imperfect scientific knowledge in the learning of ML models is the framework of physics-guided neural networks (PGNNs) [79] that was originally developed in 2017 for modeling the temperature of water in lakes using monotonic constraints of density and depth.This has led to a growing body of research in KGML for incorporating diverse forms of scientific knowledge including monotonic constraints [118], eigenvalue equations [35,46], energy balance [71,72,133], and scaling laws [54].For example, Jia et al. developed the framework of physicsguided recurrent neural networks (PGRNNs) [71] that accounted for the conservation of energy across time for predicting lake temperatures, while allowing for error tolerance in the equation for energy balance involving approximate terms.</p>
<p>In other applications, scientific knowledge can be assumed to be perfect and complete, i.e., all of the underlying processes of a scientific system can be fully described by a set of scientific equations that are exactly known with no incorrect or missing terms.For example, in many applications, scientific knowledge is expressed in the form of idealized partial differential equations (PDEs), where the goal is to solve these equations in a computationally efficient manner.Many techniques in KGML have been developed for problems where scientific knowledge can be assumed to be perfect and complete.In such problems, ML methods can serve as a promising alternative to numerical methods for solving scientific equations with significant computational gains.One of the seminal works in this direction is the framework of Physics-Informed Neural Networks (PINNs) [128,129,130] that were developed in 2017 to solve PDEs using feed-forward neural networks.This has opened an entirely new line of research on solving PDEs using advanced neural architectures such as neural operators [50,97,155].There is thus a continuum of research directions in KGML based on the type of scientific knowledge, which can range from perfect and complete to imperfect and partial.</p>
<p>Form of Knowledge-ML Integration</p>
<p>There is also a continuum of research in KGML depending on the exact form of knowledge-ML integration, ranging from ML-centric approaches to process-centric approaches.In ML-centric approaches, the mapping of inputs to response variables is primarily driven by ML methods while scientific knowledge plays the role of guiding ML algorithms to scientifically consistent solutions.We describe the different strategies for incorporating scientific knowledge in ML frameworks in detail in Section 4. On the other hand, in process-centric approaches, the entire modeling exercise is primarily driven by process knowledge whereas ML methods are only used to inform or augment certain components of process-based models.In these approaches, ML is embedded inside the process-based model and the final predictions of response variables are made by the process-based model.For example, the role of ML can be limited to identifying (or calibrating) latent parameters in process-based models from data (e.g., in a recent line of work on differential parameter learning for hydrology [140]), which results in better match between process-based model outputs with observations of response variables.Another example of processcentric KGML formulations is the area of subgrid parameterization in climate science [12], where ML methods are used to learn corrective terms in science-based equations running at coarse resolutions of space (e.g., at 1km grids), to capture the effect of subgrid processes at finer resolutions not accounted by the equations.This is related to the general direction of using ML to augment certain components in process-based models that are known to be imperfect, e.g., in the field of turbulence modeling to close the gap between high-fidelity and low-fidelity simulations [34] and in geosciences where MLaugmented equations are used to model hydrological processes [40].There is also a range of approaches in KGML to create hybrid combinations of ML-based and process-based components, referred to as the field of hybrid modeling [77,78,163].Approaches in this field include residual modeling strategies [145], where ML is simply used to learn the residuals of process-based models and the final predictions are equally contributed by both ML and process-based model outputs.Another strategy is to use the outputs of process-based models as additional inputs in ML models along with drivers to predict response variables, referred to as the framework of hybrid-physics-data (HPD) models [79].This style of hybrid modeling is closer to being ML-centric as the final predictions of response variables are determined by the ML model, and the process-based simulations are only used as inputs that serve as initial estimates.</p>
<p>Method for Incorporating Scientific Knowledge in ML</p>
<p>When it comes to using ML components in KGML approaches (either process-centric or ML-centric), an important aspect is the method used for incorporating scientific knowledge in ML frameworks.One of the primary ways of doing this is by modifying the learning algorithm of ML models (e.g., deep learning models) to favor the selection of models that are consistent with scientific equations (e.g., using knowledge-guided loss functions), so as to steer the training trajectory toward generalizable solutions.This method is adopted in the frameworks of PGNN and PGRNN for lake temperature modeling and the framework of PINNs for solving PDEs.Another approach is to "hard-code" or "bake-in" scientific knowledge directly in the solution structure of deep learning models, resulting in knowledgeguided neural network architectures.Some examples of research in this area include the framework of physics-guided architecture (PGA) of LSTM models (PGA-LSTM) [31] and approaches for capturing symmetries and invariances of dynamical systems in the architecture of neural networks [152].The basic goal of these methods is to improve the scientific explainability of features learned at the hidden layers of neural networks, grounded in scientific theories.A third approach is to initialize neural network parameters using weights informed by scientific knowledge, e.g., by pretraining the model on simulations of science-based models before fine-tuning on gold-standard observations [71,72].See Figure 3 for a pictorial depiction of these three methods.Note that these three methods are also not mutually exclusive; they can be combined together in various ways depending on the context of application.</p>
<p>Table 1 provides a categorization of a few example research works in KGML from the perspective of the three dimensions.They are described in more detail in later sections.</p>
<p>Research Methods in KGML</p>
<p>In this section, we expand upon the three primary ways of integrating scientific knowledge in ML that we introduced in the previous section, namely knowledge-guided learning algorithms (e.g., using loss functions), knowledge-guided architecture of ML models, and knowledge-guided pretraining or initialization of ML models.</p>
<p>Knowledge-guided Learning</p>
<p>Directly building ML models from limited observational data often leads to the learning of spurious patterns that cannot be generalized to new scenarios, which necessitates the awareness of underlying physical knowledge in the learning of ML models.The predominant method for integrating scientific knowledge into ML is to directly introduce physical principles into the training objective of ML models [20,71,78,79,128].For example, the framework of physics-guided neural network (PGNN) [79] was one of the first works to add physical constraints (in particular, the density-depth relationship) as loss functions to guide the training of neural networks to physically consistent solutions in the context of predicting lake water temperature.The PGRNN model [71] further extended this idea to capture the heat transfer process as physics-guided loss functions in the prediction of water temperature dynamics in lakes.This method represents a pioneering effort in integrating the energy conservation law into</p>
<p>KGML Research</p>
<p>Type of Knowledge</p>
<p>Knowledge-ML Integration</p>
<p>KGML Method Additional Comments</p>
<p>PINN [128] Perfect &amp; Complete ML-centric Learning Solves a single PDE at any arbitrary point using feedforward networks Neural Operators (FNO [96], DeepONet [109])
Perfect &amp; Complete ML-centric Learning (optional)
Solves a family of PDEs at arbitrary resolutions using feedforward networks PGNN [30] Imperfect &amp; Partial ML-centric Learning</p>
<p>Uses monotonic constraints (partial) and process simulations (imperfect) in feedforward networks PGRNN [70] Imperfect ML training.In particular, this method estimates intermediate heat fluxes in the heat transfer process and then defines the energy conservation over these fluxes and the predicted water temperature over the water column.Read et al. [133], for the first time, systematically evaluated the generalizability of the PGRNN approach on out-of-distribution samples.They demonstrated that the PGRNN model significantly outperforms both standard ML models and process-based models when tested on years or seasons with very different weather conditions compared to training data.</p>
<p>The idea of knowledge-guided learning can be utilized to integrate scientific knowledge in different forms as loss functions in the training of ML models.For example, scientific theories are commonly represented as equations, particularly differential equations, which offer a direct way to embed known scientific laws into models [111] such as the approach of physics-informed neural networks (PINN) [128] and the physics-informed neural operator (PINO) [97].Integrating scientific equations into the loss functions of ML models can be especially useful when the goal is to solve a known target equation, e.g., in the problem of solving PDEs [4,14,176].Similarly, heuristics, rules, and structured relationships in ontologies and knowledge graphs can also be encoded and added to the loss function [19,42,165,166,167].By penalizing deviations from these established knowledge bases, the training algorithm has a higher chance of learning physically consistent data patterns and thus achieving improved generalizability.</p>
<p>However, there are challenges in defining and utilizing knowledge-guided loss functions when dealing with imperfect knowledge, e.g., equations not perfectly aligned with true observations.In such cases, we can adjust the weight of regularization terms in the loss functions, enabling a balance between minimizing training errors and satisfying domain constraints [76,97,128].Moreover, incorporating knowledge-guided loss functions can introduce potential conflict between data-driven and domainguided loss functions, which can lead to the challenge of competing objectives during ML training [35].Additionally, the optimization landscape could suffer from the presence of poor local minima due to the added complexity of knowledge constraints, making convergence to an optimal solution more elusive [29,153,154,156].These issues can be potentially addressed by multi-objective optimization strategies [52,78,88], which offer a solution by exploiting trade-offs between conflicting goals.ML models are essentially parameterized architectures that need to be trained using observational data.The internal structures of ML models often involve common components such as convolutional layers, recurrent layers, and graph layers for capturing complex spatio-temporal data dependencies.However, these data-driven components are prone to overfitting when the available observations are limited or not representative of the data distribution in larger regions or different time periods.There is a general interest in integrating scientific knowledge into specific components in ML architectures [2,3,7,33,71,73,100].These methods can be broadly grouped into three categories: (1) incorporating knowledge to modify components in existing deep learning models (e.g., convolutional and recurrent layers, graph structures), (2) decomposing ML models guided by the interactions of different processes, and (3) encoding physical properties (e.g., invariance or equivariance) into the architecture.</p>
<p>Knowledge-guided Architecture</p>
<p>Prior work has explored different ways to enhance the deep learning architecture by integrating scientific knowledge into certain modeling components [2,6,7,31,71,73,100].In the context of water temperature prediction in freshwater ecosystems, prior works have incorporated the energy conservation law in modeling the hidden information being transferred across time in the RNN structure [71] and across stream segments in the graph convolutional process [73].Other works have explored directly using known PDEs to inform certain model components.For example, Bao et al. [7] built dynamic graph structures across stream segments based on the heat transfer PDE [33].Lienen et al. [100] developed a graph neural network by incorporating physics knowledge on the unknown PDE to improve sea surface temperature and gas flow predictions.Airphynet [58] also utilized multiple graphs to capture the diffusion and advection effect in the air pollutant concentration.In the work by Daw et al. [31], a physics-guided architecture of LSTM models (PGA-LSTM) was developed to explicitly encode the density-depth physics in the connections among LSTM nodes for the problem of lake water temperature modeling.PGA-LSTM was shown to produce meaningful uncertainty estimates of water temperature profiles across depth in lakes, where every realization of temperature profile generated by the model was guaranteed to respect density-depth physics.</p>
<p>Another thread of research is in decomposing the model architecture into multiple modules responsible for unique physical processes, with interactions among processes captured by module dependencies.</p>
<p>For example, the agroecosystem consists of diverse types of physical processes, e.g., weather, soil conditions, plant, and respiration, which jointly form the cycling of energy, water, and carbon.These processes are simulated by existing physics-based models, which are often built with a modular structure.Fig. 4 shows a general modular structure of many hydrology models for simulating streamflow.Here each module is responsible for converting input variables to specific output variables based on known or modeled physical processes, and these variables can be either observable or unobservable.Each module is implemented as a series of mathematical computations, which can be either explicit (e.g., following mathematical equations) or implicit (e.g., solving a differential equation).Conventional ML largely ignores the modular structure and try to directly create a mapping between input drivers (e.g., weather) and the response variable (e.g., streamflow).To fully capture the dynamics of variables and their interactions in scientific systems, previous methods developed modularized NN architectures, which are consistent with the processes defined in physics-based models [56,84,103,104].This entails ascribing physical meanings to the output of each NN module while also maintaining the interrelationships amongst these modules as defined in existing physics-based models.This architecture decomposes the modeling of different variables and thus offers the flexibility for selecting different NN structures (e.g., recurrent and convolutional networks) for each module that best fit the nature of each variable that may evolve and interact at different scales.For example, Liu et al. [103] created a hierarchical NN structure using multiple GRUs for predicting different intermediate physical variables in agroecosystems, such as CO 2 fluxes and soil NH + 4 .Feng and Shen et al. [40,140] further extended this idea by replacing certain process-based modules by NN structures while implementing other time-discrete processes directly on PyTorch to allow automatic differentiation.They also model the spatial variation of physical parameters (in the process-based model) using a separate NN model (e.g., LSTM) [147], which takes the forcing data and attributes to predict the static parameters or dynamic parameters of the process-based model, and feeds them to the main predictive model.</p>
<p>In addition, the ML architecture can be designed to explicitly capture desired physical properties, e.g., the equivariance property in a dynamical particle system.While the equivariance can also be partially achieved through data augmentation, directly encoding it in model architecture can reduce the number of model parameters, and thus mitigate overfitting and improve the generalizability.Most existing equivariance models (e.g., G-CNN [26], Steerable CNN [27]) are developed by extending the translation equivariance of the standard CNN model to group equivariance.The EGNN model [137] further extends this to higher dimensional space on a graph structure.It is also noteworthy that the development of many commonly used ML architectures was initially inspired by important physical properties.For example, the convolutional layers in the standard CNN are designed to preserve translational equivariance in image recognition.</p>
<p>Knowledge-guided Pre-training</p>
<p>Overparametrized ML models are prone to overfitting due to the large parameter space and the instability inherent in their optimization process.Scientific knowledge can be leveraged to initialize the model parameters so less data is needed to refine the model parameters to learn generalizable data patterns.Existing KGML pre-training methods can be conducted by utilizing simulated data or self-defined proxy learning tasks (self-supervised learning), as discussed in the following.</p>
<p>Use of Simulated data: Simulated data generated by process-based models (even generic uncalibrated models) can be used to pre-train ML models.Such simulated data encode fundamental physical principles governing the complex systems, and thus can provide valuable information in training ML models, especially when observations are sparse.Use of such pre-training is one of the easiest ways of leveraging the knowledge about scientific principles governing complex environmental systems into ML [71,72,173].Jia et al. [71] first utilized simulated data from a generic process-based model to pre-train the ML model and demonstrated the improvement in the performance under data-sparse and out-of-sample scenarios.</p>
<p>One limitation of this approach is that the simulated data are often biased due to the parameterization used in physics-based models, which makes the pre-training less helpful when we adapt (or fine-tune) the model to the target system.In particular, physics-based models simulate different physical processes with preset parameterizations, e.g., the setting of soil moisture and watershed characteristics when simulating streamflow and water temperature, but these preset parameter values can be very different in the target ecosystems.Several approaches have been developed to address this issue [21,23,67,69].For example, Jia et al. [69] proposed a meta-learning algorithm to initialize an ML model using multiple sets of simulated data created by varying the parameterizations of the physics-based model.This process results in a diverse dataset that closely emulates real-world hydrological data, allowing the ML model to learn and generalize effectively to real-world scenarios.McCabe et al. [117] also explored embedding data from multiple physical systems into a shared representation space so as to create a generalizable pretrained model.In another work, Chen et al. [23] utilized multiple physical equations to simulate baseflow in river basins, and selectively used simulated values to regularize the ML model in its initialization and adaptation phases.</p>
<p>Another issue with simulated data is that they are often generated independently by physics-based models without considering the effect of other real-world factors, e.g., human infrastructures.For example, most physics-based stream models do not explicitly capture the effect of dams, reservoirs, and transportation systems, which introduces bias to the simulated streamflow and water quality measures.Such biased simulated data can be less helpful in initializing ML models as they present only physical processes that are in ideal scenarios but could behave very differently from true observations.One interesting direction is to combine simulated datasets independently created over different systems and use data-driven approaches to model their interactions.Recently, there was a data release from the U.S. Geological Survey on stream temperature simulations that combines two types of physics-based models, the SNTemp model [8] for streams and the GLM model for reservoirs [61].Such a combination is through a linear function between each stream and its upstream reservoir, and the weight of the reservoir is exponentially decayed over a longer distance.Jia et al. [68] further extended this idea by using a graph neural network to combine the simulations on different types of nodes, and then use the obtained composite simulations to pre-train the graph model for predicting stream water temperature.</p>
<p>Self-supervised learning: In the field of self-supervised learning (SSL), deep neural networks are trained to learn useful feature representations using pseudo labels created from pre-defined pretext tasks, e.g., colorization [91] and inpainting [123] in computer vision.These pretext tasks are designed in a way such that solving these pretext tasks requires the extraction of complex data patterns needed for the target prediction task.For the self-supervised methods to be effective in environmental science problems, one promising direction is to create pretext tasks that can reflect knowledge about underlying physical processes.In general, the self-supervised learning paradigm can be broadly categorized as sample-wise similarity learning and predictive learning, described in the following.</p>
<p>The core concept of sample-wise similarity learning involves learning representations for individual data samples, aiming to precisely reflect the inherent similarities between them [149,174].This approach primarily concentrates on deciphering the inherent relationships within the data, considering each data point in relation to every other point in the sample space.Specifically, a similarity is estimated between each pair of samples based on their characteristics in scientific systems, e.g., depth and surface area in lake systems [23,24,164].Next, ML models are pre-trained to map data samples to a representation space, in which similar samples are pulled closer and dissimilar ones are pushed further apart [47,161].This can be commonly achieved by contrastive learning [93].For example, Ghosh et al. [47] adopted this in streamflow prediction.They developed a representation learning method to embed basin-specific characteristics, which is trained via contrastive learning using positive (similar) sample pairs from the same basin and negative sample pairs from different basins.</p>
<p>As an alternative method, predictive SSL learning aims to pre-train ML models by predicting certain state variables in the target system.The idea is to enforce the learning of key state variables related to the target variables.For example, an ML model for crop yield prediction can be pre-trained to predict variables related to its carbon and water cycles [99].The training can be conducted using simulated values or proxy values for these state variables.Prior works further extended this idea to pre-training a knowledge-guided ML architecture, which contains multiple ML modules to represent different processes [103].They leveraged simulated data for intermediate modular outputs to pre-train different ML modules.</p>
<p>The Challenge of Catastrophic forgetting: The pre-training enables the ML model to learn the representation of physics from simulated data.Next, the model can be fine-tuned with true observations from the target system [139].One potential issue is that the fine-tuning could distort the representation learned from pre-training, leading to degraded generalizability [87].There is an opportunity to explore different fine-tuning techniques, e.g., tuning a subset of parameters at a time while fixing other parameters [69].The objective is to preserve the generalizable patterns extracted from simulated data when adapting the pre-trained model to the target system.</p>
<p>Use Cases in Environmental Sciences</p>
<p>Figure 5 illustrates many prominent use-cases for using KGML methodologies in scientific problems.First, KGML methods can be used to learn the forward mapping from input drivers to response variables better than state-of-the-art (or sometimes, non-existent) science-based forward models in terms of accuracy and/or computational efficiency.A second use-case is to learn the inverse mapping from observations of inputs and outputs to the parameters of science-based models using ML algorithms, which can help provide useful information of the system states and aid in calibrating physics-based models.A third use-case of KGML in scientific domains is to use generative modeling approaches to create digital twins of scientific systems, where ML methods are used to generate synthetic distributions of variables of interest (e.g., spatio-temporal variations of Earth's surface temperature), optionally conditioned on a few parameters.This is useful for the purpose of data generation, which can then be fed into downstream tasks of forward modeling.A fourth use-case of KGML in scientific domains is to convert coarse-scale information of physical variables to higher resolutions in space and/or time, referred to as the problem of downscaling (also referred to as the problem of super-resolution in the field of computer vision).This can help reduce the expensive computational cost of creating simulations at fine scales.</p>
<p>Forward Modeling</p>
<p>Process-based models consist of a series of mathematical equations to simulate the forward mapping of underlying processes from input drivers to response or target variables.Running these models often requires substantial computational costs due to the need to solve internal equations (e.g., complex PDEs).This poses a major impediment for simulating scientific processes at fine spatial resolutions and high time frequency required in many real-world settings.Moreover, existing process-based models often use parameterizations and approximations due to incomplete knowledge or excessive complexity in modeling certain processes, leading to degraded predictive performance.ML approaches are increasingly being utilized to build forward models, but standard ML approaches are not designed for capturing complex dynamics and interactions of scientific processes, especially when training data are limited.</p>
<p>KGML methods can be used in the problem of forward modeling in two scenarios: (1) surrogate modeling: when target variables can be assumed to be perfectly described by process-based models, e.g., simulated data generated by solving PDEs, KGML methods can be utilized to build computationally efficient surrogate models, and (2) improved forward modeling: when we have observational data for target variables and process-based models are known to be imperfect (or approximate) and incomplete representations of reality, KGML methods can be used to improve both computational efficiency and predictive accuracy of forward modeling.We describe both these scenarios in the following.</p>
<p>Surrogate Modeling Solving PDEs:</p>
<p>Traditional numerical solvers for PDEs often use the Finite Elements Method or the Finite Difference Method [41], and can be prohibitively expensive for many simulation tasks, e.g., simulating turbulent flow with a large Reynolds number.Recently, neural network (NN) models have been used to approximate the solution of PDEs through the neural network (NN) forward process, which significantly reduces the computational cost compared to conventional numerical solvers.NN-based PDE solvers have shown promise in approximating PDE solutions with reasonable accuracy in a number of applications.Some popular approaches include physics-informed neural networks (PINN) [76] that are designed to solve a single instance of a PDE, and neural operator learning methods that can learn a family of PDEs including the Fourier Neural Operator (FNO) [96] and DeepONet [108].Operator learning methods such as FNO and DeepONet can be further enhanced by adding physical constraints in the training process [97,155].</p>
<p>One area of the recent focus in the problem of solving PDE is in using FNO to approximate the PDE solutions through a transformation in a Fourier space.The intuition is to approximate the Green functions by kernels, which are parameterized by neural networks in the Fourier space.Specifically, the FNO approach initially transforms the input function, representing the initial or boundary condition of a PDE, from the spatial domain to the frequency domain using the Fourier transformation, which allows the representation of the function to capture the global information effectively.A neural network then learns the mapping between the Fourier coefficients of the input and output representation in the frequency domain.After this learning process, the inverse Fourier transform is applied to convert the learned representation back to the spatial domain, yielding the approximated solution to the PDE.Based on such network design, FNO is able to combine the global information of the entire field embedded through the Fourier transformation and the expressive power of neural networks, enabling the learning and approximation of high-dimensional and complex PDE operators directly from data.An additional benefit of FNO is its ability to create the PDE solution at arbitrary spatial resolutions.This has the potential to achieve zero-shot or few-shot downscaling in complex dynamical systems.Several variants of FNO have already emerged [83,112,146,182] and have also been used to address the scientific challenges in environmental science [74,95,176].The PICL approach [107] was introduced to further extend FNO for simultaneously modeling multiple governing PDEs by using contrastive learning to enforce the distance amongst samples from different PDE systems.Furthermore, Lippe et al. [101] introduced the diffusion model [162] as PDE-Refiner into FNO for more accurate modeling of PDE dynamics via a multi-step refinement process.</p>
<p>Surrogate Modeling for Other Systems: Existing process-based models rely on intensive computations to simulate complex physical processes.Surrogate modeling addresses this challenge by creating data-driven models that approximate the behavior of physical models and predict the outputs of these complex systems with much less computational effort.This makes them extremely valuable in scenarios where rapid decision-making is crucial, such as emergency response planning, environmental impact assessments, or policy development [11,38,60,102,132,179,180,184].Surrogate models are typically developed using advanced machine learning tools [11,60,144,158,179,184] (e.g., Koopman operator [15]) or statistical methods [43], and are trained on simulated data generated by complex physical models.To fully capture the essential patterns and relationships inherent in environmental processes, scientific knowledge can also be leveraged to enhance the ML-based surrogate models.</p>
<p>Improved Forward Modeling</p>
<p>Utilizing scientific knowledge can further enhance the ability of process-based models to better align with ground-truth observations.Building upon the traditional approach of residual modeling where ML or statistical methods are used to learn the residual of a process-based model w.r.t.groundtruth data [145], researchers have investigated the combination of simulated and observational data through a pretrain-finetune pipeline [53,98,103].This method leverages the knowledge embedded in comprehensive simulated data while further improving the prediction on observational data through parameter refinement.Another KGML approach for improved forward modeling is to incorporate domain knowledge into the ML model's architecture [103].This integration ensures that the model adheres to established scientific principles, while the model parameters can be further adjusted to match true observations.Advancements also be made through feature selection, which utilizes domain knowledge to select or extract the most relevant features for the target task [171].This enhances the model's ability to focus on the most informative aspects of the data, thereby improving its predictive performance.Moreover, the ML model can be constrained by known physical relationships.This can be achieved by either adding physical laws as constraints during the model training process [80,175] or adjusting model outputs in the post-processing phase to maintain physical consistency [19].</p>
<p>Inverse Modeling</p>
<p>Standard forward models aim to create a mapping  from input drivers   to target variables   .The mapping   →   can be different for every system (e.g., different basins or farmlands) due to the variation of physical characteristics (e.g., groundwater, hydraulic conductivity, soil, and vegetation conditions) [48].Hence, forward models often need to include physical characteristics  as additional input, i.e.,   =  (  , ).Most physical characteristics  are considered static or evolving slowly.Due to the technical difficulties or prohibitively expensive cost in measuring these characteristics, they are often modeled as physical parameters and need to be calibrated using real observations {, }.Traditional calibration of physics-based models commonly uses grid search or Bayesian approaches to find combinations of parameter values leading to the best match with observations or measurements, which can be highly time-consuming and require extensive domain expertise to select the range for each variable [39,59,115].This process can also degrade the predictive performance using the obtained parameters due to the limited computational resources available to explore all the possible parameter combinations, especially in scenarios involving complex interactions among variables and parameters.</p>
<p>Advances in ML and deep learning have brought opportunities to estimate physical characteristics directly using data-driven approaches.Given some available data samples {  ,   }, the characteristics (or parameters)  can be estimated by learning the inverse mapping  = (  ,   ).A key motivation behind inferring the parameters of process-based models is to recover critical information about the state of scientific processes that are difficult to observe directly.Another motivation is to identify useful settings of model parameters that when fed inside process-based forward models result in simulated outputs that best match with observations, referred to as the problem of model calibration.</p>
<p>Building upon traditional inverse modeling approaches based on regularized regression [36], researchers are now building ML models for the inverse modeling of characteristics in hydrology [45], photonics [126], land surface temperature [159], among many others.For example, in the problem of seismic imaging, ML methods have been developed to learn the inverse mapping from observations of waveform amplitudes to the velocity profile of wave propagation through different layers of the Earth's subsurface [75].Some existing works on discovering PDEs can also be viewed as special cases of data-driven inverse modeling through the estimation of PDE coefficients from available simulations [127,135].Note that inverse modeling can also be performed with partial knowledge of the underlying system without explicitly using a process-based forward model, as explored in a recent work in the domain of hydrology [47].Another recent line of work in KGML for inverse modeling includes the framework of differentiable parameter learning (dPL) [147] for geoscience applications, where deep learning methods are used to infer the parameters of differentiable physics-based models using gradient descent algorithms leveraging automatic differentiation of the models.</p>
<p>Due to the intrinsic complexity of the inverse mapping, training inverse models can be challenging for some systems with sparse observations.Prior work has applied physical constraints to NN-based inverse models by regularizing the predicted latent state variables from observations [65].Another promising direction is to build an inverse loop for the forward model (e.g., using an invertible NN structure [143]), which allows natural incorporation of scientific knowledge in the forward process.</p>
<p>Generative Modeling</p>
<p>Generative models have found great success in computer vision [28,157] and natural language processing [64].Some popular models include generative adversarial networks [49], variational autoencoder [81], normalizing flow-based modes [82], and diffusion models [63].These models aim to capture the underlying data distribution and then create synthetic samples that look similar to the real observations.Given their success in generating images, speech, and text data, the expectation is rising for using these models to generate virtual physical simulations under specified conditions.</p>
<p>Despite the power of these models, they are often found to generate spurious data patterns or artifacts when directly applied to complex applications in fluid dynamics and environmental sciences, especially when training data are scarce (e.g., simulations at high resolutions).To address this challenge, prior work has explored regularizing the generative models by additional physical constraints, which have shown improvement in the quality of generated data in fluid dynamics [22,168,177], material science [17], and climate science [44].For example, conservation laws have been utilized to regularize the GAN-based for generating the simulation of turbulent flows [22].Similarly, Gao et al. also include the anticipated precipitation intensity for regularizing the denoising process of the diffusion model, and show the promise of this method for precipitation nowcasting and the detection of extreme cases such as rainstorms and droughts [44].The generative modeling approaches have also been used for refining the initial predictions.For example, Lipple et al. [101] found that data-driven PDE solvers (e.g., FNOs) often neglect components of the spatial frequency spectrum that have low amplitude.To overcome this issue, they developed PDE-Refiner, which employs diffusion models to refine the prediction by considering information from all frequency components at different amplitude levels.</p>
<p>Downscaling</p>
<p>In environmental science problems, complex physics-based models are used to capture the details of physical reality by incorporating diverse components that account for numerous processes at fine spatial or temporal scales.These models are often restricted by computational expenses and model complexity.Consequently, many models are operated at a coarser resolution than necessary to accurately represent the underlying physical phenomena.For instance, cloud-resolving models (CRMs) effectively capture boundary layer eddies and low clouds with sub-kilometer horizontal resolution [131].However, running global climate models at such fine resolutions, even with anticipated advancements in computing power, remains unfeasible.</p>
<p>Handling downscaling problems in environmental science using machine learning involves deriving fine-scale information from coarser-scale data.Note that the coarse-scale inputs can either be obtained through observations or through simulations of low-fidelity models operating at coarser scales.In particular, machine learning models have been widely used for automatically projecting the coarserscale or lower-resolution environmental data into fine-scale or higher-resolution ones, which is especially popular in the domains of climate science, hydrology, and ecology [9,51,66,80,105,122,136,138,163,178].For example, Wang et al. [150] adopted super-resolution methods to generate higher-resolution predictions (e.g.temperature, precipitation, etc) in different locations and times at the local scale from coarse spatial resolutions.The authors further extended this work by transferring the trained model in one region to downscale the precipitation in another region under a different environment.</p>
<p>In many environmental science problems, building data-driven downscaling methods is challenging due to the often limited availability of fine-scale observational data.Few-shot and zero-shot learning could be potential solutions to such problems.Few-shot learning [160] and zero-short learning [55,74,176] can leverage models trained from data-sufficient regions and adapt them to regions that are poorly observed or completely unobserved after moderate fine-tuning.Despite the promise of these approaches, using these approaches for improving scientific data downscaling remains largely underexplored.Recently, FNO has shown encouraging results in performing zero-shot downscaling of PDE solutions to arbitrary specified resolutions [96].Another limitation of existing data-driven downscaling approaches is their ability to harness irregularly structured data.Existing downscaling techniques in environmental science are often applied to gridded data, such as remote sensing imagery.There are plenty of machine learning models suitable for these kinds of data, e.g., Convolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs) are commonly used in super-resolution (SR) methods for refining coarse spatial datasets [5,90].More recently, researchers have started exploring downscaling approaches for irregularly structured data, such as river networks [18].In prior work, a graph network model is used to represent and predict intricate network dynamics, while a long short-term memory (LSTM) model is used to capture flow sequences along river paths [141].These methodologies hold promise for enhancing fine-scale environmental predictions from coarser datasets.</p>
<p>Foundation models in environmental science</p>
<p>Existing process-based and data-driven models are often built for specific target problems in environmental science.The coupling of multiple models is often non-trivial, making it difficult to capture the relationships or share the information across different problems.For example, the prediction of water quality variables (e.g., water temperature, concentration of nutrients) and water quantity variables (e.g., streamflow) are often separately modeled even though they involve some common processes.Foundation models, which aim to establish a general paradigm to tackle a diverse set of tasks, have found immense success in natural language processing (NLP) [13,183].Utilizing the textual understanding capabilities acquired from extensive text data in pre-training, these models have been shown to provide reasonable predictive performance on different tasks under zero-shot or few-shot settings.The intricate structure of the large-scale model also allows the information to be forwarded through numerous branches of routes in the model, facilitating the encoding of both shared and distinct information essential for various tasks.This enables learning simultaneously over diverse tasks and then generating predictions for any task specified by a user.</p>
<p>Recently, researchers started exploring using foundation models (e.g., LLMs) in environmental applications [1,114,170].When used in environmental science applications, foundation models provide new opportunities for performing predictions for different physical variables and at different scales, or over different sets of locations and different time periods.The expectation is also rising for using foundation models to understand the complex nature of the environment from limited observation data.Additionally, prior works have shown that LLMs are effective in harnessing missing or inconsistent input data (e.g., weather data, soil properties, and other modeled variables) across space and assimilating new observations in environmental applications [110].</p>
<p>Despite the promise, several major challenges could prevent existing foundation models from reaching the same level of success as they did in the text and vision domains.Environmental data are often not included in training existing foundation models.This omission can lead to a knowledge gap when the model is directly deployed for query tasks involving complex environmental processes.Although some open-sourced foundation models can be fine-tuned toward the downstream task, the size of observation data in many environmental applications is often far smaller than what is needed for tuning large-scale models.Moreover, existing foundation models remain limited in interpreting model predictions from a scientific perspective, e.g., the simulation of intermediate processes that lead to the dynamics of final predicted variables.Third, running large-scale foundation models requires high computational cost and financial cost, which makes them unsuitable for prediction over long periods and large regions.To address these challenges, one promising direction is to better leverage scientific knowledge for enhancing the foundation models, e.g., through prompting, reasoning, and tuning, to improve their generalizability and interpretability.New research is also urgently needed for selectively running foundation models and existing data-driven or KGML models in a hybrid manner to reduce the cost.</p>
<p>Alternatively, researchers also investigated building foundation models directly on environmental applications without referring to existing foundation models in vision and text domains.For example, the ClimaX model [119] has been built using climate datasets and a Transformer-based architecture.It was reported to achieve good predictive performance and generalizability on different climate and weather tasks.Several previous studies in scientific modeling can be considered as foundation models.For example, FNO [95] has been shown to be able to perform PDE solving tasks for PDEs with different settings (e.g., different initial conditions).FNO and implicit neural representation [25] can be used to downsample data to an arbitrary resolution after they are trained on a specific spatial resolution.Existing works on building heterogeneity-aware models over a large number of different locations can also be viewed as simplified foundation models that are specifically designed for spatial generalization [85,94,169].For example, Xie et al. [169] create a general pipeline for transforming deep learning models to predict land covers for locations with statistically different distributions.These models could be further extended to address the problems of different spatial scales.</p>
<p>Another thread in building foundation models is based on the concept of transfer learning [185].By using this approach, a model is first trained on a broad dataset that encompasses multiple scientific topics [89].This initial training allows the model to learn general trends and behaviors.After this broad training, the same model can be fine-tuned or adjusted on a more specific dataset related to a particular environmental task [119].Consider an example in hydrological problems.A foundation model trained using broad environmental features like climate data, satellite imagery, and general topography could be fine-tuned using river basin-specific data.This specialized model can predict river flow dynamics or water quality, optimizing its performance by leveraging both the broad knowledge from the global pre-training and the specific nuances from the fine-tuning phase.However, several challenges arise in this approach.Data in different scientific problems often vary in scale, resolution, and quality, making the integration a complex procedure.Moreover, the intricate interactions in environmental systems, like hydrology with its dependencies on soil, plants, and weather, make it challenging to capture all nuances.</p>
<p>Concluding Remarks and Future Directions</p>
<p>This paper provided a summary of prior research in KGML and contextualized it using a threedimensional view.KGML is indeed a rapidly growing field of research, as illustrated by the diversity of research methods and scientific use-cases that are being explored in a variety of application domains.There are also several directions for future work in KGML.First, along with serving the goal of better predictive accuracy, novel advances in KGML are needed to solve the end-goal of discovering new scientific knowledge from data.While there is a growing focus in mainstream ML on explainability, trustworthiness, and fairness of ML results, scientific problems may require entirely new definitions of explainability and related concepts.This is because in scientific applications, we need to go beyond learning input attributions to make sure that the patterns, insights, or rules discovered from data are consistent with existing scientific theories and are meaningful to domain scientists.We also need to go beyond modeling associations to understand the causality of scientific systems, e.g., using counterfactuals, in conjunction with existing scientific knowledge.Another advance that is needed in the field of KGML is to design better tools for uncertainty quantification in scientific applications of ML.For example, we need to improve our understanding of the limits of large-scale ML models (e.g., Foundation models) and predict in advance when ML models will fail and by how much given the characteristics of a new dataset.</p>
<p>Figure 1 .
1
Figure 1.Generic framework for scientific modeling using process-based models.</p>
<p>Figure 2 .
2
Figure 2. (a) Pictorial depiction of how KGML models are different from data-only (black-box) ML models and science-only models.(b) A three-dimensional view of KGML research where different settings of these dimensions correspond to various research directions in KGML.</p>
<p>Figure 3 .
3
Figure 3. Research methods for incorporating scientific knowledge in ML frameworks.</p>
<p>Figure 4 .
4
Figure 4.The hierarchical physical variables simulated in the hydrological systems.The variables shown on each module is the output of the module. .</p>
<p>Figure 5 .
5
Figure 5. Schematic diagrams of four primary use-cases of KGML in scientific disciplines .</p>
<p>Table 1 .
1
Categorization of previous research in KGML in terms of the three-dimensional view of KGML research.
Uses energy conservationPartial&amp;ML-centricLearning &amp; Pretraining(imperfect and partial) in RNN with processsimulations (imperfect)PGA-LSTM [31]Perfect &amp; PartialML-centricArchitectureUses monotonic constraints (perfect and partial) in LSTM architectureEquivariant-Nets [152]Perfect &amp; PartialML-centricArchitectureUses symmetries (perfect and partial) in CNNdPL [140]Perfect &amp; CompleteProcess-centric-Learns parameters in differentiable process modelSubgrid Parameterization [12]Imperfect &amp; CompleteProcess-centric-Uses ML to correct effects of subgrid processes
AcknowledgementsWe thank Shengyu Chen for his valuable contributions to summarizing the existing literature on KGML research methods.We also thank Runlong Yu and Yue Wan for their very helpful review on an earlier version of the manuscript.This work was supported in part by National Science Foundation (NSF) awards IIS-2239328, IIS-2313174, IIS-2239175, IIS-2147195, IIS-2316305, OAC-1934721, OAC-2203581, and DEB-2213550.This work was also supported by USDA National Institute of Food and Agriculture (NIFA) and the National Science Foundation (NSF) National AI Research Institutes Competitive Award # 2023-67021-39829.
arXiv:2311.07361Microsoft Research AI4Science and Microsoft Azure Quantum. The impact of large language models on scientific discovery: a preliminary study using gpt-4. 2023arXiv preprint</p>
<p>Airphynet: Harnessing physics-guided neural networks for air quality prediction. Submitted to The Twelfth International Conference on Learning Representations. 2023under review</p>
<p>ClimODE: Climate forecasting with physics-informed neural ODEs. Submitted to The Twelfth International Conference on Learning Representations. 2023under review</p>
<p>A computational approach to a mathematical model of climate change using heat sources and diffusion. Muhammad Shoaib Arif, Kamaleldin Abodayeh, Yasir Nawaz, Civil Engineering Journal. 872022</p>
<p>Contrastive learning for climate model bias correction and super-resolution. Tristan Ballard, Gopal Erinjippurath, arXiv:2211.075552022arXiv preprint</p>
<p>Physics guided neural networks for spatio-temporal super-resolution of turbulent flows. Tianshu Bao, Shengyu Chen, Taylor T Johnson, Peyman Givi, Shervin Sammak, Xiaowei Jia, Uncertainty in Artificial Intelligence. PMLR2022</p>
<p>Partial differential equation driven dynamic graph networks for predicting stream water temperature. Tianshu Bao, Xiaowei Jia, Jacob Zwart, Jeffrey Sadler, Alison Appling, Samantha Oliver, Taylor T Johnson, 2021 IEEE International Conference on Data Mining (ICDM). IEEE2021</p>
<p>Stream network and stream segment temperature models software. John Bartholow, US Geological Survey. 2010Technical report</p>
<p>Characterization of extreme hydroclimate events in earth system models using ml/ai. Katrina E Bennett, System Predictability (AI4ESP . .Satish Karra, System Predictability (AI4ESP . .Velimir, System Predictability (AI4ESP . .Vesselinov, System Predictability (AI4ESP . .Artificial Intelligence for Earth. 2021Technical report</p>
<p>The future of distributed models: model calibration and uncertainty prediction. Hydrological processes. Keith Beven, Andrew Binley, 19926</p>
<p>Surrogate modeling for the climate sciences dynamics with machine learning and data assimilation. Marc Bocquet, Frontiers in Applied Mathematics and Statistics. 911332262023</p>
<p>Applications of deep learning to ocean data inference and subgrid parameterization. Thomas Bolton, Laure Zanna, Journal of Advances in Modeling Earth Systems. 1112019</p>
<p>On the opportunities and risks of foundation models. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney Von Arx, Jeannette Michael S Bernstein, Antoine Bohg, Emma Bosselut, Brunskill, arXiv:2108.072582021arXiv preprint</p>
<p>Magnet: Mesh agnostic neural pde solver. Oussama Boussif, Yoshua Bengio, Loubna Benabbou, Dan Assouline, Advances in Neural Information Processing Systems. 202235</p>
<p>Koopman operator theory: Past, present, and future. Steven Brunton, Eurika Kaiser, Nathan Kutz, APS Division of Fluid Dynamics Meeting Abstracts. 2017</p>
<p>Statistical significance of climate sensitivity predictors obtained by data mining. P Caldwell, Geophysical Research Letters. 4152014</p>
<p>Improving direct physical properties prediction of heterogeneous materials from imaging data via convolutional neural network and a morphology-aware generative model. Ruĳin Cang, Hechao Li, Hope Yao, Yang Jiao, Yi Ren, Computational Materials Science. 1502018</p>
<p>Heterogeneous stream-reservoir graph networks with data assimilation. Shengyu Chen, Alison Appling, Samantha Oliver, Hayley Corson-Dosch, Jordan Read, Jeffrey Sadler, Jacob Zwart, Xiaowei Jia, 2021 IEEE International Conference on Data Mining (ICDM). IEEE2021</p>
<p>Reconstructing turbulent flows using spatio-temporal physical dynamics. Shengyu Chen, Tianshu Bao, Peyman Givi, Can Zheng, Xiaowei Jia, ACM Transactions on Intelligent Systems and Technology. 1512024</p>
<p>Hossnet: An efficient physics-guided neural network for simulating micro-crack propagation. Shengyu Chen, Shihang Feng, Yao Huang, Zhou Lei, Xiaowei Jia, Youzuo Lin, Estaben Rougier, Computational Materials Science. 2361128462024</p>
<p>Physics-guided machine learning from simulated data with different physical parameters. Shengyu Chen, Nasrin Kalanat, Yiqun Xie, Sheng Li, Jacob A Zwart, Jeffrey M Sadler, Alison P Appling, Samantha K Oliver, Jordan S Read, Xiaowei Jia, Knowledge and Information Systems. 6582023</p>
<p>Reconstructing high-resolution turbulent flows using physics-guided neural networks. Shengyu Chen, Shervin Sammak, Peyman Givi, Joseph P Yurko, Xiaowei Jia, 2021 IEEE International Conference on Big Data (Big Data). IEEE2021</p>
<p>Physics-guided meta-learning method in baseflow prediction over large regions. Shengyu Chen, Yiqun Xie, Xiang Li, Xu Liang, Xiaowei Jia, Proceedings of the 2023 SIAM International Conference on Data Mining (SDM). the 2023 SIAM International Conference on Data Mining (SDM)SIAM2023</p>
<p>Physics-guided graph meta learning for predicting water temperature and streamflow in stream networks. Shengyu Chen, Jacob A Zwart, Xiaowei Jia, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>Videoinr: Learning video implicit neural representation for continuous space-time super-resolution. Zeyuan Chen, Yinbo Chen, Jingwen Liu, Xingqian Xu, Vidit Goel, Zhangyang Wang, Humphrey Shi, Xiaolong Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Group equivariant convolutional networks. Taco Cohen, Max Welling, International conference on machine learning. PMLR2016</p>
<p>. S Taco, Max Cohen, Welling, arXiv:1612.084982016Steerable cnns. arXiv preprint</p>
<p>Diffusion models in vision: A survey. Florinel-Alin, Vlad Croitoru, Radu Hondru, Tudor Ionescu, Mubarak Shah, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2023</p>
<p>Mitigating propagation failures in physics-informed neural networks using retain-resample-release (r3) sampling. Arka Daw, Jie Bu, Sifan Wang, Paris Perdikaris, Anuj Karpatne, ICML'23. JMLR.orgProceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>Physicsguided neural networks (pgnn): An application in lake temperature modeling. Arka Daw, Anuj Karpatne, William D Watkins, Jordan S Read, Vipin Kumar, Knowledge Guided Machine Learning. Chapman and Hall/CRC2022</p>
<p>Physics-guided architecture (pga) of neural networks for quantifying uncertainty in lake temperature modeling. Arka Daw, Quinn Thomas, Cayelan C Carey, Jordan S Read, Alison P Appling, Anuj Karpatne, Proceedings of the 2020 SIAM International Conference on Data Mining. the 2020 SIAM International Conference on Data MiningSIAM2020</p>
<p>ImageNet: A Large-Scale Hierarchical Image Database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, 200909</p>
<p>River temperature modelling: A review of process-based approaches and future directions. David M Stephen J Dugdale, Iain A Hannah, Malcolm, Earth-Science Reviews. 1752017</p>
<p>Turbulence modeling in the age of data. Karthik Duraisamy, Gianluca Iaccarino, Heng Xiao, Annual review of fluid mechanics. 512019</p>
<p>Cophy-pgnn: Learning physics-guided neural networks with competing loss functions for solving eigenvalue problems. Mohannad Elhamod, Jie Bu, Christopher Singh, Matthew Redell, Abantika Ghosh, Viktor Podolskiy, Wei-Cheng Lee, Anuj Karpatne, ACM Transactions on Intelligent Systems and Technology. 1362022</p>
<p>Regularization of inverse problems. Heinz Werner Engl, Martin Hanke, Andreas Neubauer, 1996Springer Science &amp; Business Media375</p>
<p>Theory-guided data science for climate change. J H Faghmous, A Banerjee, S Shekhar, M Steinbach, V Kumar, A R Ganguly, N Samatova, Computer. 47112014</p>
<p>Optimal design of groundwater pollution monitoring network based on the svr surrogate model under uncertainty. Wenxi Yue Fan, Tiansheng Lu, Yongkai Miao, Jiuhui An, Jiannan Li, Luo, Environmental Science and Pollution Research. 272020</p>
<p>An overview of current applications, challenges, and future trends in distributed process-based models in hydrology. Simone Fatichi, Enrique R Vivoni, Fred L Ogden, Y Valeriy, Benjamin Ivanov, David Mirus, Charles W Gochis, Matteo Downer, Jason H Camporese, Brian Davison, Ebel, Journal of Hydrology. 5372016</p>
<p>Differentiable, learnable, regionalized process-based models with multiphysical outputs can approach state-of-the-art hydrologic prediction accuracy. Dapeng Feng, Jiangtao Liu, Kathryn Lawson, Chaopeng Shen, Water Resources Research. 58102022</p>
<p>A first course in finite elements. J Fish, Belytschko, 2007Wiley</p>
<p>Opening up new geographical ontologies around adapting to climate change. Susannah Fisher, Tĳdschrift voor economische en sociale geografie. 11422023</p>
<p>Surrogate models in rock and soil mechanics: Integrating numerical modeling and machine learning. Rock Mechanics and Rock Engineering. Jk Furtney, Thielsen, Romain Le Fu, Goc, 2022</p>
<p>Prediff: Precipitation nowcasting with latent diffusion models. Zhihan Gao, Xingjian Shi, Boran Han, Hao Wang, Xiaoyong Jin, Danielle Maddix, Yi Zhu, Mu Li, Yuyang Wang, arXiv:2307.104222023arXiv preprint</p>
<p>Recent developments in fast and scalable inverse modeling and data assimilation methods in hydrology. Hojat Ghorbanidehno, Amalia Kokkinaki, Jonghyun Lee, Eric Darve, Journal of Hydrology. 5911252662020</p>
<p>Physics-informed machine learning for optical modes in composites. Abantika Ghosh, Mohannad Elhamod, Jie Bu, Wei-Cheng Lee, Anuj Karpatne, Viktor, Podolskiy, Advanced Photonics Research. 31122000732022</p>
<p>Robust inverse framework using knowledgeguided self-supervised learning: An application to hydrology. Rahul Ghosh, Arvind Renganathan, Kshitĳ Tayal, Xiang Li, Ankush Khandelwal, Xiaowei Jia, Christopher Duffy, John Nieber, Vipin Kumar, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>Entity aware modelling: A survey. Rahul Ghosh, Haoyu Yang, Ankush Khandelwal, Erhu He, Arvind Renganathan, Somya Sharma, Xiaowei Jia, Vipin Kumar, arXiv:2302.084062023arXiv preprint</p>
<p>Generative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, NIPS. 2014</p>
<p>Somdatta Goswami, Aniruddha Bora, Yue Yu, George Em Karniadakis, arXiv:2207.05748Physics-informed neural operators. 2022arXiv preprint</p>
<p>Downscaling satellite soil moisture using geomorphometry and machine learning. Mario Guevara, Rodrigo Vargas, PloS One. 149e02196392019</p>
<p>A knowledge guided transfer strategy for evolutionary dynamic multiobjective optimization. Yinan Guo, Guoyu Chen, Min Jiang, Dunwei Gong, Jing Liang, IEEE Transactions on Evolutionary Computation. 2022</p>
<p>Deeporyza: A knowledge guided machine learning model for rice growth simulation. Jingye Han, Liangsheng Shi, Christos Pylianidis, Qi Yang, Ioannis N Athanasiadis, 2nd AAAI Workshop on AI for Agriculture and Food Systems. 2023</p>
<p>Predicting lake surface water phosphorus dynamics using process-guided machine learning. Aviah B Paul C Hanson, Xiaowei Stillman, Anuj Jia, Hilary A Karpatne, Cayelan C Dugan, Joseph Carey, Nicole K Stachelek, Yu Ward, Jordan S Zhang, Read, Ecological Modelling. 4301091362020</p>
<p>Augmented convolutional lstms for generation of high-resolution climate change projections. Nidhin Harilal, Mayank Singh, Udit Bhatia, IEEE Access. 92021</p>
<p>Physics guided neural networks for time-aware fairness: an application in crop yield prediction. Erhu He, Yiqun Xie, Licheng Liu, Weiye Chen, Zhenong Jin, Xiaowei Jia, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Why Meta's latest large language model survived only three days online. Will Douglas Heaven, MIT Technology Review. 2022</p>
<p>Airphynet: Harnessing physics-guided neural networks for air quality prediction. Kethmi Hirushini Hettige, Jiahao Ji, Shili Xiang, Cheng Long, Gao Cong, Jingyuan Wang, arXiv:2402.037842024arXiv preprint</p>
<p>Effective groundwater model calibration: with analysis of data, sensitivities, predictions, and uncertainty. C Mary, Claire R Hill, Tiedeman, 2006John Wiley &amp; Sons</p>
<p>Accurate machine-learning atmospheric retrieval via a neural-network surrogate model for radiative transfer. Joseph Michael D Himes, Adam D Harrington, Atılım Cobb, Frank Güneş Baydin, Soboczenski, D Molly, Simone O'beirne, David C Zorzan, Zacchaeus Wright, Shawn D Scheffer, Domagal-Goldman, The Planetary Science Journal. 34912022</p>
<p>A general lake model (glm 3.0) for linking with high-frequency sensor data from the global lake ecological observatory network (gleon). Louise C Matthew R Hipsey, Casper Bruce, Brendan Boon, Busch, C Cayelan, David P Carey, Paul C Hamilton, Jordan S Hanson, Eduardo Read, Michael De Sousa, Weber, 2019Geoscientific Model Development12</p>
<p>Glm-general lake model: Model overview and user information. Mr Hipsey, Bruce, Hamilton, 2014Perth (AustraliaUniversity of Western Australia Technical Manual</p>
<p>Advances in neural information processing systems. Jonathan Ho, Ajay Jain, Pieter Abbeel, 202033Denoising diffusion probabilistic models</p>
<p>The survey: Text generation models in deep learning. Touseef Iqbal, Shaima Qureshi, Journal of King Saud University-Computer and Information Sciences. 3462022</p>
<p>Physics-informed neural network for inverse modeling of natural-state geothermal systems. Kazuya Ishitsuka, Weiren Lin, Applied Energy. 3371208552023</p>
<p>Understanding climate change with statistical downscaling and machine learning. Julie Jebeile, Vincent Lam, Tim Räz, Synthese. 1992021</p>
<p>Modeling reservoir release using pseudo-prospective learning and physical simulations to predict water temperature. Xiaowei Jia, Shengyu Chen, Yiqun Xie, Haoyu Yang, Alison Appling, Samantha Oliver, Zhe Jiang, Proceedings of the 2022 SIAM International Conference on Data Mining (SDM). the 2022 SIAM International Conference on Data Mining (SDM)SIAM2022</p>
<p>Physicsguided graph diffusion network for combining heterogeneous simulated data: An application in predicting stream water temperature. Xiaowei Jia, Shengyu Chen, Can Zheng, Yiqun Xie, Zhe Jiang, Nasrin Kalanat, Proceedings of the 2023 SIAM International Conference on Data Mining (SDM). the 2023 SIAM International Conference on Data Mining (SDM)SIAM2023</p>
<p>Physics-guided machine learning from simulation data: an application in modeling lake and river systems. Xiaowei Jia, ICDM. 2021</p>
<p>Recurrent generative networks for multi-resolution satellite data: An application in cropland monitoring. Xiaowei Jia, Mengdie Wang, Ankush Khandelwal, Anuj Karpatne, Vipin Kumar, ĲCAI2019</p>
<p>Physics guided rnns for modeling dynamical systems: A case study in simulating lake temperature profiles. Xiaowei Jia, Jared Willard, Anuj Karpatne, Jordan Read, Jacob Zwart, Michael Steinbach, Vipin Kumar, Proceedings of the 2019 SIAM International Conference on Data Mining. the 2019 SIAM International Conference on Data MiningSIAM2019</p>
<p>Physics-guided machine learning for scientific discovery: An application in simulating lake temperature profiles. Xiaowei Jia, Jared Willard, Anuj Karpatne, Jordan S Read, Jacob A Zwart, Michael Steinbach, Vipin Kumar, ACM/IMS Transactions on Data Science. 232021</p>
<p>Physics-guided recurrent graph model for predicting flow and temperature in river networks. Xiaowei Jia, Jacob Zwart, Jeffrey Sadler, Alison Appling, Samantha Oliver, Steven Markstrom, Jared Willard, Shaoming Xu, Michael Steinbach, Jordan Read, Proceedings of the 2021 SIAM International Conference on Data Mining (SDM). the 2021 SIAM International Conference on Data Mining (SDM)SIAM2021</p>
<p>Efficient super-resolution of near-surface climate modeling using the fourier neural operator. Peishi Jiang, Zhao Yang, Jiali Wang, Chenfu Huang, Pengfei Xue, Xingyuan Chakraborty, Yun Chen, Qian, Journal of Advances in Modeling Earth Systems. 1572023</p>
<p>Unsupervised learning of full-waveform inversion: Connecting cnn and partial differential equation in a loop. Peng Jin, Xitong Zhang, Yinpeng Chen, Sharon Xiaolei Huang, Zicheng Liu, Youzuo Lin, arXiv:2110.075842021arXiv preprint</p>
<p>Physics-informed machine learning. George Em Karniadakis, G Ioannis, Lu Kevrekidis, Paris Lu, Sifan Perdikaris, Liu Wang, Yang, Nature Reviews Physics. 362021</p>
<p>Theory-guided data science: A new paradigm for scientific discovery from data. Anuj Karpatne, Gowtham Atluri, James H Faghmous, Michael Steinbach, Arindam Banerjee, Auroop Ganguly, Shashi Shekhar, Nagiza Samatova, Vipin Kumar, IEEE Transactions on Knowledge and Data Engineering. 29102017</p>
<p>Knowledge Guided Machine Learning: Accelerating Discovery Using Scientific Knowledge and Data. Anuj Karpatne, Ramakrishnan Kannan, Vipin Kumar, 2022CRC Press</p>
<p>Anuj Karpatne, William Watkins, Jordan Read, Vipin Kumar, arXiv:1710.11431Physics-guided neural networks (pgnn): An application in lake temperature modeling. 2017arXiv preprint</p>
<p>Physics-informed machine learning: case studies for weather and climate modelling. Kashinath, Mustafa, Albert, Wu, Jiang, Esmaeilzadeh, Azizzadenesheli, Wang, Chattopadhyay, Singh, Philosophical Transactions of the Royal Society A. 379202000932194. 2021</p>
<p>. P Diederik, Max Kingma, Welling, arXiv:1312.61142013Auto-encoding variational bayes. arXiv preprint</p>
<p>Normalizing flows: An introduction and review of current methods. Ivan Kobyzev, Simon Jd Prince, Marcus A Brubaker, IEEE transactions on pattern analysis and machine intelligence. 43112020</p>
<p>Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar, arXiv:2108.08481Neural operator: Learning maps between function spaces. 2021arXiv preprint</p>
<p>Towards hybrid modeling of the global hydrological cycle. Basil Kraft, Martin Jung, Marco Körner, Sujan Koirala, Markus Reichstein, Hydrology and Earth System Sciences. 2662022</p>
<p>Towards learning universal, regional, and local hydrological behaviors via machine learning applied to large-sample datasets. Frederik Kratzert, Daniel Klotz, Guy Shalev, Günter Klambauer, Sepp Hochreiter, Grey Nearing, Hydrology and Earth System Sciences. 23122019</p>
<p>On scientific understanding with artificial intelligence. Mario Krenn, Robert Pollice, Si Yue Guo, Matteo Aldeghi, Alba Cervera-Lierta, Pascal Friederich, Gabriel Dos Passos, Florian Gomes, Adrian Häse, Akshatkumar Jinich, Nigam, Nature Reviews Physics. 4122022</p>
<p>Finetuning can distort pretrained features and underperform out-of-distribution. Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, Percy Liang, arXiv:2202.100542022arXiv preprint</p>
<p>Advanced machine learning techniques to improve hydrological prediction: A comparative analysis of streamflow prediction models. Vĳendra Kumar, Naresh Kedam, Kul Vaibhav Sharma, Darshan J Mehta, Tommaso Caloiero, 2023Water152572</p>
<p>Geo-bench: Toward foundation models for earth monitoring. Alexandre Lacoste, Nils Lehmann, Pau Rodriguez, Evan David Sherwin, Hannah Kerner, Björn Lütjens, Jeremy Andrew Irvin, David Dao, Hamed Alemohammad, Alexandre Drouin, arXiv:2306.038312023arXiv preprint</p>
<p>Super-resolution of sea surface temperature satellite images. Devyani Lambhate, Deepak, Subramani, Global Oceans 2020: Singapore-US Gulf Coast. IEEE2020</p>
<p>Colorization as a proxy task for visual understanding. Gustav Larsson, Maire Michael, Gregory Shakhnarovich, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2017</p>
<p>The Parable of Google Flu: Traps in Big Data Analysis. David Lazer, Ryan Kennedy, Gary King, Alessandro Vespignani, Science. 6176March 2014</p>
<p>Contrastive representation learning: A framework and review. Graham Phuc H Le-Khac, Alan F Healy, Smeaton, Ieee Access. 82020</p>
<p>Regionalization in a global hydrologic deep learning model: from physical descriptors to random vectors. Xiang Li, Ankush Khandelwal, Xiaowei Jia, Kelly Cutler, Rahul Ghosh, Arvind Renganathan, Shaoming Xu, Kshitĳ Tayal, John Nieber, Christopher Duffy, Water Resources Research. 5882022</p>
<p>Fourier neural operator with learned deformations for pdes on general geometries. Zongyi Li, Daniel Zhengyu Huang, Burigede Liu, Anima Anandkumar, arXiv:2207.052092022arXiv preprint</p>
<p>Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar, arXiv:2010.08895Fourier neural operator for parametric partial differential equations. 2020arXiv preprint</p>
<p>Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar Azizzadenesheli, Anima Anandkumar, arXiv:2111.03794Physics-informed neural operator for learning partial differential equations. 2021arXiv preprint</p>
<p>Knowledge-based artificial intelligence for agroecosystem carbon budget and crop yield estimation. Wang Liu Licheng, Kaiyu Zhou, Bin Guan, Chongya Peng, Jinyun Jiang, Sheng Tang, Robert Wang, Symon Grant, Xiaowei Mezbahuddin, Jia, 2023Authorea Preprints</p>
<p>Estimating the autotrophic and heterotrophic respiration in the us crop fields using knowledge guided machine learning. Wang Liu Licheng, Zhenong Zhou, Jinyun Jin, Xiaowei Tang, Chongya Jia, Kaiyu Jiang, Bin Guan, Shaoming Peng, Yufeng Xu, Yang, 2022Authorea Preprints</p>
<p>Learning the dynamics of physical systems from sparse observations with finite element networks. Marten Lienen, Stephan Günnemann, arXiv:2203.088522022arXiv preprint</p>
<p>Phillip Lippe, S Bastiaan, Paris Veeling, Richard E Perdikaris, Johannes Turner, Brandstetter, arXiv:2308.05732Pde-refiner: Achieving accurate long rollouts with neural pde solvers. 2023arXiv preprint</p>
<p>Study of water quality response to water transfer patterns in the receiving basin and surrogate model. Lei Liu, Xue-Yi You, Environmental Science and Pollution Research. 2021</p>
<p>Kgml-ag: a modeling framework of knowledge-guided machine learning to simulate agroecosystems: a case study of estimating n 2 o emission using data from mesocosm experiments. Licheng Liu, Shaoming Xu, Jinyun Tang, Kaiyu Guan, Timothy J Griffis, Matthew D Erickson, Alexander L Frie, Xiaowei Jia, Taegon Kim, Lee T Miller, 2022Geoscientific Model Development15</p>
<p>Knowledge-guided machine learning can improve carbon cycle quantification in agroecosystems. Licheng Liu, Wang Zhou, Kaiyu Guan, Bin Peng, Shaoming Xu, Jinyun Tang, Qing Zhu, Jessica Till, Xiaowei Jia, Chongya Jiang, Nature Communications. 1513572024</p>
<p>Downscaling satellite retrieved soil moisture using regression tree-based machine learning algorithms over southwest france. Yangxiaoyue Liu, Xiaolin Xia, Ling Yao, Wenlong Jing, Chenghu Zhou, Wumeng Huang, Yong Li, Ji Yang, Earth and Space Science. 7102020</p>
<p>Lennart Ljung. System identification. Signal analysis and prediction. Springer1998</p>
<p>Cooper Lorsung, Amir Barati, Farimani , arXiv:2401.16327Picl: Physics informed contrastive learning for partial differential equations. 2024arXiv preprint</p>
<p>Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. Lu Lu, Pengzhan Jin, George Em Karniadakis, arXiv:1910.031932019arXiv preprint</p>
<p>Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, George Em Karniadakis, Nature machine intelligence. 332021</p>
<p>Free: The foundational semantic recognition for modeling environmental ecosystems. Shiyuan Luo, Juntong Ni, Shengyu Chen, Runlong Yu, Yiqun Xie, Licheng Liu, Zhenong Jin, Huaxiu Yao, Xiaowei Jia, arXiv:2311.102552023arXiv preprint</p>
<p>Physics-guided discovery of highly nonlinear parametric partial differential equations. Yingtao Luo, Qiang Liu, Yuntian Chen, Wenbo Hu, Tian Tian, Jun Zhu, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>Björn Lütjens, Catherine H Crawford, Christopher Campbell D Watson, Dava Hill, Newman, arXiv:2207.11417Multiscale neural operator: Learning fast and grid-independent pde solvers. 2022arXiv preprint</p>
<p>The origins of computer weather prediction and climate modeling. Peter Lynch, Journal of computational physics. 22772008</p>
<p>On the opportunities and challenges of foundation models for geospatial artificial intelligence. Gengchen Mai, Weiming Huang, Jin Sun, Suhang Song, Deepak Mishra, Ninghao Liu, Song Gao, Tianming Liu, Gao Cong, Yingjie Hu, arXiv:2304.067982023arXiv preprint</p>
<p>Process-based models for forest ecosystem management: current state of the art and challenges for practical implementation. Annikki Mäkelä, Joe Landsberg, Alan R Ek, Thomas E Burk, Michael Ter-Mikaelian, Chadwick D Göran I Ågren, Pasi Oliver, Puttonen, Tree physiology. 205-62000</p>
<p>Eight (no, nine!) problems with big data. Gary Marcus, Ernest Davis, The New York Times. 60420142014</p>
<p>Multiple physics pretraining for physical surrogate models. Michael Mccabe, Bruno Régaldo-Saint Blancard, Liam Holden Parker, Ruben Ohana, Miles Cranmer, Alberto Bietti, Michael Eickenberg, Siavash Golkar, Geraud Krawezik, Francois Lanusse, arXiv:2310.029942023arXiv preprint</p>
<p>Incorporating prior domain knowledge into deep neural networks. Nikhil Muralidhar, Mohammad Raihanul Islam, Manish Marwah, Anuj Karpatne, Naren Ramakrishnan, IEEE International Conference on Big Data (Big Data). IEEE2018. 2018</p>
<p>Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K Gupta, Aditya Grover, arXiv:2301.10343Climax: A foundation model for weather and climate. 2023arXiv preprint</p>
<p>Natural language corpus data. Beautiful data. Peter Norvig, 2009</p>
<p>Openai, arxiv 2303.08774Gpt-4 technical report. 20232</p>
<p>Downscaling earth system models with deep learning. Sungwon Park, Karandeep Singh, Arjun Nellikkattil, Elke Zeller, Tung Duong Mai, Meeyoung Cha, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>Context encoders: Feature learning by inpainting. Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A Efros, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>The seven tools of causal inference, with reflections on machine learning. Judea Pearl, Communications of the ACM. 6232019</p>
<p>The book of why: the new science of cause and effect. Basic books. Judea Pearl, Dana Mackenzie, 2018</p>
<p>Machine learning inverse problem for topological photonics. Laura Pilozzi, Francis A Farrelly, Giulia Marcucci, Claudio Conti, Communications Physics. 112018</p>
<p>Deep hidden physics models: Deep learning of nonlinear partial differential equations. Maziar Raissi, The Journal of Machine Learning Research. 1912018</p>
<p>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Maziar Raissi, Paris Perdikaris, George E Karniadakis, Journal of Computational Physics. 3782019</p>
<p>Maziar Raissi, Paris Perdikaris, George Em Karniadakis, arXiv:1711.10561Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential equations. 2017arXiv preprint</p>
<p>Physics informed deep learning (part ii): Data-driven discovery of nonlinear partial differential equations. Maziar Raissi, Paris Perdikaris, George Em Karniadakis, arXiv:1711.105662017arXiv preprint</p>
<p>Deep learning to represent subgrid processes in climate models. Stephan Rasp, Michael S Pritchard, Pierre Gentine, Proceedings of the National Academy of Sciences. 115392018</p>
<p>Review of surrogate modeling in water resources. Saman Razavi, Bryan A Tolson, Donald H Burn, Water Resources Research. 4872012</p>
<p>Process-guided deep learning predictions of lake water temperature. Xiaowei Jordan S Read, Jared Jia, Alison P Willard, Jacob A Appling, Samantha K Zwart, Anuj Oliver, Gretchen Ja Karpatne, Paul C Hansen, William Hanson, Watkins, Water Resources Research. 55112019</p>
<p>Deep learning and process understanding for data-driven earth system science. Markus Reichstein, Gustau Camps-Valls, Bjorn Stevens, Martin Jung, Joachim Denzler, Nuno Carvalhais, Nature. 56677432019</p>
<p>Data-driven discovery of partial differential equations. Steven L Samuel H Rudy, Joshua L Brunton, Nathan Proctor, Kutz, Science advances. 34e16026142017</p>
<p>Enhancing spatial resolution of grace-derived groundwater storage anomalies in urmia catchment using machine learning downscaling methods. Sabzehee, S Amiri-Simkooei, Iran-Pour, Vishwakarma, Kerachian, Journal of Environmental Management. 3301171802023</p>
<p>E (n) equivariant graph neural networks. Garcia Vıctor, Emiel Satorras, Max Hoogeboom, Welling, International conference on machine learning. PMLR2021</p>
<p>Machine-learning-based downscaling of modelled climate change impacts on groundwater table depth. Raphael Schneider, Julian Koch, Lars Troldborg, Hans Jørgen Henriksen, Simon Stisen, Hydrology and Earth System Sciences. 26222022</p>
<p>Knowledge guided representation learning and causal structure learning in soil science. Somya Sharma, Swati Sharma, Licheng Liu, Rishabh Tushir, Andy Neal, Robert Ness, John Crawford, Emre Kiciman, Ranveer Chandra, arXiv:2306.093022023arXiv preprint</p>
<p>Differentiable modelling to unify machine learning and physical models for geosciences. Chaopeng Shen, Alison P Appling, Pierre Gentine, Toshiyuki Bandai, Hoshin Gupta, Alexandre Tartakovsky, Marco Baity-Jesi, Fabrizio Fenicia, Daniel Kifer, Li Li, Nature Reviews Earth &amp; Environment. 482023</p>
<p>Spatial downscaling of streamflow data with attention based spatio-temporal graph convolutional networks. Muhammed Sit, Bekir Zahit Demiray, Ibrahim Demir, 2023</p>
<p>Inverse problem theory and methods for model parameter estimation. Albert Tarantola, 2005SIAM</p>
<p>Invertibility aware integration of static and time-series data: An application to lake temperature modeling. Kshitĳ Tayal, Xiaowei Jia, Rahul Ghosh, Jared Willard, Jordan Read, Vipin Kumar, Proceedings of the 2022 SIAM International Conference on Data Mining (SDM). the 2022 SIAM International Conference on Data Mining (SDM)SIAM2022</p>
<p>Koopman invertible autoencoder: Leveraging forward and backward dynamics for temporal modeling. Kshitĳ Tayal, Arvind Renganathan, Rahul Ghosh, Xiaowei Jia, Vipin Kumar, 2023 IEEE International Conference on Data Mining (ICDM). IEEE2023</p>
<p>Modeling chemical processes using prior knowledge and neural networks. L Michael, Mark A Thompson, Kramer, AIChE Journal. 4081994</p>
<p>Alasdair Tran, Alexander Mathews, Lexing Xie, Soon Cheng, Ong, arXiv:2111.13802Factorized fourier neural operators. 2021arXiv preprint</p>
<p>From calibration to parameter learning: Harnessing the scaling effects of big data in geoscientific modeling. Wen-Ping Tsai, Dapeng Feng, Ming Pan, Hylke Beck, Kathryn Lawson, Yuan Yang, Jiangtao Liu, Chaopeng Shen, Nature communications. 12159882021</p>
<p>Informed machine learning-a taxonomy and survey of integrating knowledge into learning systems. Sebastian Laura Von Rueden, Katharina Mayer, Bogdan Beckh, Sven Georgiev, Raoul Giesselbach, Birgit Heese, Julius Kirsch, Annika Pfrommer, Rajkumar Pick, Ramamurthy, arXiv:1903.123942019arXiv preprint</p>
<p>From molecules to scaffolds to functional groups: building context-dependent molecular representation via multi-channel learning. Jialu Yue Wan, Tingjun Wu, Chang-Yu Hou, Xiaowei Hsieh, Jia, arXiv:2311.027982023arXiv preprint</p>
<p>Deep learning for daily precipitation and temperature downscaling. Fang Wang, Di Tian, Lisa Lowe, Latif Kalin, John Lehrter, Water Resources Research. 5742021</p>
<p>Scientific discovery in the age of artificial intelligence. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Nature. 62079722023</p>
<p>Incorporating symmetry into deep dynamics models for improved generalization. Rui Wang, Robin Walters, Rose Yu, arXiv:2002.030612020arXiv preprint</p>
<p>An expert's guide to training physics-informed neural networks. Sifan Wang, Shyam Sankaran, Hanwen Wang, Paris Perdikaris, arXiv:2308.084682023arXiv preprint</p>
<p>Understanding and mitigating gradient flow pathologies in physics-informed neural networks. Sifan Wang, Yujun Teng, Paris Perdikaris, SIAM Journal on Scientific Computing. 4352021</p>
<p>Learning the solution operator of parametric partial differential equations with physics-informed deeponets. Sifan Wang, Hanwen Wang, Paris Perdikaris, Science advances. 74086052021</p>
<p>Improved architectures and training algorithms for deep operator networks. Sifan Wang, Hanwen Wang, Paris Perdikaris, Journal of Scientific Computing. 922352022</p>
<p>Generative adversarial networks in computer vision: A survey and taxonomy. Zhengwei Wang, Qi She, Tomas E Ward, ACM Computing Surveys (CSUR). 5422021</p>
<p>High-fidelity deep approximation of ecosystem simulation over long-term at large scale. Zhihao Wang, Yiqun Xie, Xiaowei Jia, Lei Ma, George Hurtt, Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems. the 31st ACM International Conference on Advances in Geographic Information Systems2023</p>
<p>Simfair: Physics-guided fairness-aware learning with simulation models. Zhihao Wang, Yiqun Xie, Zhili Li, Xiaowei Jia, Zhe Jiang, Aolin Jia, Shuo Xu, arXiv:2401.152702024arXiv preprint</p>
<p>Mapping large-scale plateau forest in sanjiangyuan using high-resolution satellite imagery and few-shot learning. Zhihao Wei, Kebin Jia, Xiaowei Jia, Pengyu Liu, Ying Ma, Ting Chen, Guilian Feng, Remote Sensing. 1423882022</p>
<p>Large-scale river mapping using contrastive learning and multi-source satellite imagery. Zhihao Wei, Kebin Jia, Pengyu Liu, Xiaowei Jia, Yiqun Xie, Zhe Jiang, Remote Sensing. 131528932021</p>
<p>The solution-diffusion model: a review. G Johannes, Richard W Wĳmans, Baker, Journal of membrane science. 1071-21995</p>
<p>Integrating scientific knowledge with machine learning for engineering and environmental systems. Jared Willard, Xiaowei Jia, Shaoming Xu, Michael Steinbach, Vipin Kumar, ACM Computing Surveys. 5542022</p>
<p>Predicting water temperature dynamics of unmonitored lakes with meta-transfer learning. Jared D Willard, Jordan S Read, Alison P Appling, Samantha K Oliver, Xiaowei Jia, Vipin Kumar, Water Resources Research. 5772021</p>
<p>An ontology model for climatic data analysis. Jiantao Wu, Fabrizio Orlandi, O' Declan, Soumyabrata Sullivan, Dev, 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS. IEEE2021</p>
<p>Linkclimate: An interoperable knowledge graph platform for climate data. Jiantao Wu, Fabrizio Orlandi, O' Declan, Soumyabrata Sullivan, Dev, Computers &amp; Geosciences. 1691052152022</p>
<p>Improving tourism analytics from climate data using knowledge graphs. Jiantao Wu, Jarrett Pierse, Fabrizio Orlandi, O' Declan, Soumyabrata Sullivan, Dev, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. 162023</p>
<p>Enforcing statistical constraints in generative adversarial networks for modeling chaotic dynamical systems. Jin-Long Wu, Karthik Kashinath, Adrian Albert, Dragos Chirila, Heng Xiao, Journal of Computational Physics. 4061092092020</p>
<p>Statistically-robust clustering techniques for mapping spatial hotspots: A survey. Yiqun Xie, Shashi Shekhar, Yan Li, ACM Computing Surveys (CSUR). 5522022</p>
<p>Geo-foundation models: Reality, gaps and opportunities. Yiqun Xie, Zhaonan Wang, Gengchen Mai, Yanhua Li, Xiaowei Jia, Song Gao, Shaowen Wang, Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems. the 31st ACM International Conference on Advances in Geographic Information Systems2023</p>
<p>Knowledge-guided machine learning reveals pivotal drivers for gas-to-particle conversion of atmospheric nitrate. Bo Xu, Haofei Yu, Zongbo Shi, Jinxing Liu, Yuting Wei, Zhongcheng Zhang, Yanqi Huangfu, Han Xu, Yue Li, Linlin Zhang, Environmental Science and Ecotechnology. 191003332024</p>
<p>Physicsaware machine learning revolutionizes scientific paradigm for machine learning and processbased hydrology. Qingsong Xu, Yilei Shi, Jonathan Bamber, Ye Tuo, Ralf Ludwig, Xiao Xiang Zhu, arXiv:2310.052272023arXiv preprint</p>
<p>Deep transfer learning based on transformer for flood forecasting in data-sparse basins. Yuanhao Xu, Kairong Lin, Caihong Hu, Shuli Wang, Qiang Wu, Li Zhang, Guang Ran, Journal of Hydrology. 6251299562023</p>
<p>Adaptive hierarchical similarity metric learning with noisy labels. Jiexi Yan, Lei Luo, Cheng Deng, Heng Huang, IEEE Transactions on Image Processing. 322023</p>
<p>A flexible and efficient knowledge-guided machine learning data assimilation (kgml-da) framework for agroecosystem prediction in the us midwest. Qi Yang, Licheng Liu, Junxiong Zhou, Rahul Ghosh, Bin Peng, Kaiyu Guan, Jinyun Tang, Wang Zhou, Vipin Kumar, Zhenong Jin, 2023Remote Sensing of Environment299113880</p>
<p>Fourier neural operators for arbitrary resolution climate data downscaling. Qidong Yang, Alex Hernandez-Garcia, Paula Harder, Venkatesh Ramesh, Prasanna Sattegeri, Daniela Szwarcman, David Campbell D Watson, Rolnick, arXiv:2305.144522023arXiv preprint</p>
<p>Enforcing deterministic constraints on generative adversarial networks for emulating physical systems. Zeng Yang, Jin-Long Wu, Heng Xiao, arXiv:1911.066712019arXiv preprint</p>
<p>Machine learning as a downscaling approach for prediction of wind characteristics under future climate change scenarios. Abbas Yeganeh-Bakhtiary, Hossein Eyvazoghli, Naser Shabakhty, Bahareh Kamranzad, Soroush Abolfathi, Complexity. 2022. 2022</p>
<p>Predicting combined tidal and pluvial flood inundation using a machine learning surrogate model. T Faria, Jonathan L Zahura, Goodall, Journal of Hydrology: Regional Studies. 411010872022</p>
<p>Prediction of the outflow temperature of large-scale hydropower using theory-guided machine learning surrogate models of a high-fidelity hydrodynamics model. Di Zhang, Dongsheng Wang, Qidong Peng, Junqiang Lin, Tiantian Jin, Tiantian Yang, Soroosh Sorooshian, Yi Liu, Journal of Hydrology. 6061274272022</p>
<p>Artificial intelligence for science in quantum, atomistic, and continuum systems. Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, arXiv:2307.084232023arXiv preprint</p>
<p>Jiawei Zhao, Robert Joseph George, Yifei Zhang, Zongyi Li, Anima Anandkumar, arXiv:2211.15188Incremental fourier neural operator. 2022arXiv preprint</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Building a machine learning surrogate model for wildfire activities within a global earth system model. Qing Zhu, Fa Li, William J Riley, Li Xu, Lei Zhao, Kunxiaojia Yuan, Huayi Wu, Jianya Gong, James Randerson, Geoscientific Model Development. 202215</p>
<p>A comprehensive survey on transfer learning. Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, Qing He, Proceedings of the IEEE. 10912020</p>            </div>
        </div>

    </div>
</body>
</html>
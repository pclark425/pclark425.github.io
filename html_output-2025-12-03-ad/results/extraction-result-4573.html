<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4573 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4573</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4573</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-269790814</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.09783v1.pdf" target="_blank">LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models have recently gained significant attention in scientific discovery for their extensive knowledge and advanced reasoning capabilities. However, they encounter challenges in effectively simulating observational feedback and grounding it with language to propel advancements in physical scientific discovery. Conversely, human scientists undertake scientific discovery by formulating hypotheses, conducting experiments, and revising theories through observational analysis. Inspired by this, we propose to enhance the knowledge-driven, abstract reasoning abilities of LLMs with the computational strength of simulations. We introduce Scientific Generative Agent (SGA), a bilevel optimization framework: LLMs act as knowledgeable and versatile thinkers, proposing scientific hypotheses and reason about discrete components, such as physics equations or molecule structures; meanwhile, simulations function as experimental platforms, providing observational feedback and optimizing via differentiability for continuous parts, such as physical parameters. We conduct extensive experiments to demonstrate our framework's efficacy in constitutive law discovery and molecular design, unveiling novel solutions that differ from conventional human expectations yet remain coherent upon analysis.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4573.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4573.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SGA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scientific Generative Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bilevel-optimization framework that couples large language models as outer-loop proposers of discrete symbolic hypotheses (equations, SMILES, code) with differentiable physical simulators as inner-loop gradient-based optimizers for continuous parameters; iterates proposals, simulation feedback, and revision to discover constitutive laws and molecular designs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Scientific Generative Agent (SGA)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>SGA runs an iterative bilevel loop: (1) an LLM (prompted with system + format + history) proposes a discrete expression E (e.g., PyTorch code implementing a constitutive law or a SMILES string) and a continuous parameterization Θ; (2) a differentiable simulator Φ executes the candidate, returning simulated observables y and compact feedback z; (3) an inner gradient-based optimizer (via differentiable simulation / PyTorch) optimizes continuous parameters θ ∈ Θ producing intermediate results o (loss curves, trajectories); (4) the LLM consumes {L(yk), zk, ok, Ek, Θk}k∈[K] from a top-K heap and generates refined proposals. The outer LLM search uses an exploit/explore split controlled by decoding temperature (two temperatures, T_l and T_h) to balance conservative and exploratory proposals. The pipeline is applied to constitutive law discovery (using differentiable MPM simulation) and molecular design (SMILES + 3D coordinates with ETKGD/MMFF and UniMol property estimators). Prompts include strict code/format templates to ensure simulatable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>gpt-4-turbo-preview (backbone reported); also evaluated with Claude-3-Sonnet and Mixtral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science (constitutive law discovery), computational chemistry / molecular design</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>symbolic constitutive equations (material stress–strain relations), molecular structure → quantum-property relationships (e.g. HOMO-LUMO gap), numeric material parameters (Young's modulus, Poisson ratio etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>symbolic expressions / PyTorch code implementing equations, SMILES strings + 3D coordinates, numerical parameter vectors (continuous optimized values)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>fit-to-ground-truth via differentiable simulation (loss on particle trajectories or target molecular property), comparison to baselines (symbolic regression baselines and molecule-design baselines), and expert inspection of discovered solutions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported strong empirical performance: in symbolic-regression-style comparisons 'Ours' achieved R2 = 0.9990, MSE = 1.7e4, MAE = 86.4 (Table reported values); molecule-design task losses reported (Table 3) for Ours: (e) 1.3e-4, (f) 1.1e-1, (g) 5.4e-1, (h) 3.6e-5. The paper states SGA significantly outperforms LLM-only baselines and several symbolic-regression baselines by orders of magnitude on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against LLM-only baselines (FunSearch, Eureka, OPRO-like methods with temperature=1.0), classical symbolic-regression methods (FFX, DSO, Operon, etc.), and a population-based molecular design method (GhemGE). SGA reports substantially lower loss / higher R2 versus these baselines (example: Ours R2=0.9990 vs top SR baselines ~0.996–0.998 in reported tables; molecule-design losses lower than GhemGE in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Limitations reported include: potential lack of interpretability of LLM-generated code/solutions; safety risk from executing LLM-generated code without filtering; dependence on differentiability of generated code (though zero-order optimizers are suggested as alternatives); sensitivity to LLM inference cost (paper reports ~USD 10 per task with GPT-4 in their setup); need for careful prompt/format engineering; and the method reuses top-K historical context which may interact with LLM KV-caching considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4573.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4573.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FunSearch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mathematical discoveries from program search with large language models (FunSearch)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that pairs large language models with program evaluators to search program space and discover novel mathematical constructions or proofs, exceeding known results in some combinatorial problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mathematical discoveries from program search with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>FunSearch (LLM + evaluator program search)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>LLMs generate candidate programs/hypotheses; specialized evaluators test these candidates on mathematical problem instances and return signals used to guide further generation/search. The method has been applied to extremal combinatorics and online bin packing to discover constructions that exceed prior known results.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mathematics (combinatorics, theoretical CS)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>mathematical constructions/conjectures and provable bounds (combinatorial solutions)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>programs/code representing mathematical constructions and proofs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>evaluator-based testing against problem instances and comparison to previously known bounds/solutions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Reported to exceed known results in extremal combinatorics and online bin packing; specific numeric comparisons not provided in this paper's text.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4573.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4573.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaGeometry</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaGeometry</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based system that solves high-difficulty geometry problems (olympiad level) without human demonstrations, demonstrating that LLMs can discover complex geometric reasoning solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Solving olympiad geometry without human demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AlphaGeometry (LLM-driven problem solving)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses LLM reasoning and search strategies (no human demonstrations) to produce step-by-step geometric solutions that solve olympiad-level problems; demonstrates automated discovery/derivation of mathematical relationships from problem statements.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mathematics (geometry)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>mathematical theorems / geometric derivations</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>natural-language proofs and constructive geometry steps (structured derivations)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Solution verification against problem correctness (benchmark problems)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4573.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4573.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SymbolicGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SymbolicGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative transformer approach that produces symbolic expressions for symbolic regression tasks by generating formulae directly rather than via genetic programming.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Symbolicgpt: A generative transformer model for symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SymbolicGPT (generative-transformer symbolic regression)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train or use a transformer to generate symbolic expressions (formulas) for regression tasks; expressions are then evaluated on data to select good candidates. Emphasizes generative modeling of symbolic formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>transformer-based generative model (paper-cited model)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>symbolic regression / equation discovery</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>symbolic mathematical expressions (closed-form equations)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>symbolic expressions (strings representing formulas)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluation on benchmark datasets (SRBench) comparing R2 / MSE / MAE</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>In the paper's comparisons SymbolicGPT was listed among symbolic regression methods and did not outperform top SR methods (paper reports it but shows lower scores than top SR methods).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>In practice limited by variable-count scaling and less competitive than state-of-the-art SR methods on some benchmarks (noted in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4573.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4573.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Feynman 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic regression system that discovers analytic expressions from data by exploiting modularity and multiple regression strategies (not LLM-based but a prominent algorithmic baseline for equation discovery).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AI Feynman 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Combines physics-inspired decompositions, dimensional analysis, and regression search to find closed-form expressions that explain data; used as a strong symbolic-regression baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>symbolic regression / physics-inspired equation discovery</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>closed-form symbolic equations</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>symbolic expressions (analytic formulas)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Benchmarks on symbolic regression tasks (SRBench); comparison of R2 / MSE / MAE</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Listed among SR baselines in the paper; not top-performing versus the SGA approach in the reported tables.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4573.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4573.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eureka</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eureka (Human-level reward design via coding large language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses coding-capable LLMs to design reward functions and related code artifacts; cited as an example of LLMs being used as coders/optimizers in scientific/engineering workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-level reward design via coding large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Eureka</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses LLMs to generate code (reward functions / programmatic artifacts) and iteratively refines them via evaluation; in this paper, Eureka is used as a baseline where its LLM-temperature settings correspond to exploration-only behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>RL / automated reward design / coding for scientific workflows</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>programmatic reward specifications and code artifacts (not direct numerical laws)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>generated code / reward functions</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluation via downstream task performance (as-cited baseline behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used as a baseline in experiments; SGA with bilevel optimization outperformed Eureka in the tasks reported.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4573.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4573.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GhemGE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GhemGE (grammar-evolution population-based molecule design)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A population-based molecule design algorithm using grammatical evolution that serves as a traditional baseline for de novo molecule generation and optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Population-based de novo molecule generation, using grammatical evolution</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>GhemGE</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Population/genetic algorithm using grammar-based operators to generate molecular SMILES and evolve them for desired properties; used as a baseline for molecule design tasks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computational chemistry / molecule design</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>structure–property optimization (not explicit symbolic laws)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>SMILES strings and scored molecules</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Property evaluation and comparison of achieved objective values</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In Table 3, GhemGE reported higher losses on molecule-design tasks vs SGA (example numbers shown in paper: GhemGE (e) 4.8e-3 vs Ours (e) 1.3e-4, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>SGA outperformed GhemGE on the molecular-design benchmarks reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mathematical discoveries from program search with large language models <em>(Rating: 2)</em></li>
                <li>Solving olympiad geometry without human demonstrations <em>(Rating: 2)</em></li>
                <li>Symbolicgpt: A generative transformer model for symbolic regression <em>(Rating: 2)</em></li>
                <li>AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity <em>(Rating: 2)</em></li>
                <li>Human-level reward design via coding large language models <em>(Rating: 2)</em></li>
                <li>Population-based de novo molecule generation, using grammatical evolution <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4573",
    "paper_id": "paper-269790814",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "SGA",
            "name_full": "Scientific Generative Agent",
            "brief_description": "A bilevel-optimization framework that couples large language models as outer-loop proposers of discrete symbolic hypotheses (equations, SMILES, code) with differentiable physical simulators as inner-loop gradient-based optimizers for continuous parameters; iterates proposals, simulation feedback, and revision to discover constitutive laws and molecular designs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Scientific Generative Agent (SGA)",
            "method_description": "SGA runs an iterative bilevel loop: (1) an LLM (prompted with system + format + history) proposes a discrete expression E (e.g., PyTorch code implementing a constitutive law or a SMILES string) and a continuous parameterization Θ; (2) a differentiable simulator Φ executes the candidate, returning simulated observables y and compact feedback z; (3) an inner gradient-based optimizer (via differentiable simulation / PyTorch) optimizes continuous parameters θ ∈ Θ producing intermediate results o (loss curves, trajectories); (4) the LLM consumes {L(yk), zk, ok, Ek, Θk}k∈[K] from a top-K heap and generates refined proposals. The outer LLM search uses an exploit/explore split controlled by decoding temperature (two temperatures, T_l and T_h) to balance conservative and exploratory proposals. The pipeline is applied to constitutive law discovery (using differentiable MPM simulation) and molecular design (SMILES + 3D coordinates with ETKGD/MMFF and UniMol property estimators). Prompts include strict code/format templates to ensure simulatable outputs.",
            "llm_model_used": "gpt-4-turbo-preview (backbone reported); also evaluated with Claude-3-Sonnet and Mixtral-8x7B",
            "scientific_domain": "materials science (constitutive law discovery), computational chemistry / molecular design",
            "number_of_papers": null,
            "type_of_quantitative_law": "symbolic constitutive equations (material stress–strain relations), molecular structure → quantum-property relationships (e.g. HOMO-LUMO gap), numeric material parameters (Young's modulus, Poisson ratio etc.)",
            "extraction_output_format": "symbolic expressions / PyTorch code implementing equations, SMILES strings + 3D coordinates, numerical parameter vectors (continuous optimized values)",
            "validation_method": "fit-to-ground-truth via differentiable simulation (loss on particle trajectories or target molecular property), comparison to baselines (symbolic regression baselines and molecule-design baselines), and expert inspection of discovered solutions",
            "performance_metrics": "Reported strong empirical performance: in symbolic-regression-style comparisons 'Ours' achieved R2 = 0.9990, MSE = 1.7e4, MAE = 86.4 (Table reported values); molecule-design task losses reported (Table 3) for Ours: (e) 1.3e-4, (f) 1.1e-1, (g) 5.4e-1, (h) 3.6e-5. The paper states SGA significantly outperforms LLM-only baselines and several symbolic-regression baselines by orders of magnitude on some tasks.",
            "baseline_comparison": "Compared against LLM-only baselines (FunSearch, Eureka, OPRO-like methods with temperature=1.0), classical symbolic-regression methods (FFX, DSO, Operon, etc.), and a population-based molecular design method (GhemGE). SGA reports substantially lower loss / higher R2 versus these baselines (example: Ours R2=0.9990 vs top SR baselines ~0.996–0.998 in reported tables; molecule-design losses lower than GhemGE in Table 3).",
            "challenges_limitations": "Limitations reported include: potential lack of interpretability of LLM-generated code/solutions; safety risk from executing LLM-generated code without filtering; dependence on differentiability of generated code (though zero-order optimizers are suggested as alternatives); sensitivity to LLM inference cost (paper reports ~USD 10 per task with GPT-4 in their setup); need for careful prompt/format engineering; and the method reuses top-K historical context which may interact with LLM KV-caching considerations.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4573.0",
            "source_info": {
                "paper_title": "LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "FunSearch",
            "name_full": "Mathematical discoveries from program search with large language models (FunSearch)",
            "brief_description": "An approach that pairs large language models with program evaluators to search program space and discover novel mathematical constructions or proofs, exceeding known results in some combinatorial problems.",
            "citation_title": "Mathematical discoveries from program search with large language models",
            "mention_or_use": "mention",
            "method_name": "FunSearch (LLM + evaluator program search)",
            "method_description": "LLMs generate candidate programs/hypotheses; specialized evaluators test these candidates on mathematical problem instances and return signals used to guide further generation/search. The method has been applied to extremal combinatorics and online bin packing to discover constructions that exceed prior known results.",
            "llm_model_used": null,
            "scientific_domain": "mathematics (combinatorics, theoretical CS)",
            "number_of_papers": null,
            "type_of_quantitative_law": "mathematical constructions/conjectures and provable bounds (combinatorial solutions)",
            "extraction_output_format": "programs/code representing mathematical constructions and proofs",
            "validation_method": "evaluator-based testing against problem instances and comparison to previously known bounds/solutions",
            "performance_metrics": null,
            "baseline_comparison": "Reported to exceed known results in extremal combinatorics and online bin packing; specific numeric comparisons not provided in this paper's text.",
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4573.1",
            "source_info": {
                "paper_title": "LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "AlphaGeometry",
            "name_full": "AlphaGeometry",
            "brief_description": "An LLM-based system that solves high-difficulty geometry problems (olympiad level) without human demonstrations, demonstrating that LLMs can discover complex geometric reasoning solutions.",
            "citation_title": "Solving olympiad geometry without human demonstrations",
            "mention_or_use": "mention",
            "method_name": "AlphaGeometry (LLM-driven problem solving)",
            "method_description": "Uses LLM reasoning and search strategies (no human demonstrations) to produce step-by-step geometric solutions that solve olympiad-level problems; demonstrates automated discovery/derivation of mathematical relationships from problem statements.",
            "llm_model_used": null,
            "scientific_domain": "mathematics (geometry)",
            "number_of_papers": null,
            "type_of_quantitative_law": "mathematical theorems / geometric derivations",
            "extraction_output_format": "natural-language proofs and constructive geometry steps (structured derivations)",
            "validation_method": "Solution verification against problem correctness (benchmark problems)",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4573.2",
            "source_info": {
                "paper_title": "LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "SymbolicGPT",
            "name_full": "SymbolicGPT",
            "brief_description": "A generative transformer approach that produces symbolic expressions for symbolic regression tasks by generating formulae directly rather than via genetic programming.",
            "citation_title": "Symbolicgpt: A generative transformer model for symbolic regression",
            "mention_or_use": "mention",
            "method_name": "SymbolicGPT (generative-transformer symbolic regression)",
            "method_description": "Train or use a transformer to generate symbolic expressions (formulas) for regression tasks; expressions are then evaluated on data to select good candidates. Emphasizes generative modeling of symbolic formulas.",
            "llm_model_used": "transformer-based generative model (paper-cited model)",
            "scientific_domain": "symbolic regression / equation discovery",
            "number_of_papers": null,
            "type_of_quantitative_law": "symbolic mathematical expressions (closed-form equations)",
            "extraction_output_format": "symbolic expressions (strings representing formulas)",
            "validation_method": "Evaluation on benchmark datasets (SRBench) comparing R2 / MSE / MAE",
            "performance_metrics": null,
            "baseline_comparison": "In the paper's comparisons SymbolicGPT was listed among symbolic regression methods and did not outperform top SR methods (paper reports it but shows lower scores than top SR methods).",
            "challenges_limitations": "In practice limited by variable-count scaling and less competitive than state-of-the-art SR methods on some benchmarks (noted in the paper).",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4573.3",
            "source_info": {
                "paper_title": "LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "AI Feynman 2.0",
            "name_full": "AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity",
            "brief_description": "A symbolic regression system that discovers analytic expressions from data by exploiting modularity and multiple regression strategies (not LLM-based but a prominent algorithmic baseline for equation discovery).",
            "citation_title": "AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity",
            "mention_or_use": "mention",
            "method_name": "AI Feynman 2.0",
            "method_description": "Combines physics-inspired decompositions, dimensional analysis, and regression search to find closed-form expressions that explain data; used as a strong symbolic-regression baseline in comparisons.",
            "llm_model_used": null,
            "scientific_domain": "symbolic regression / physics-inspired equation discovery",
            "number_of_papers": null,
            "type_of_quantitative_law": "closed-form symbolic equations",
            "extraction_output_format": "symbolic expressions (analytic formulas)",
            "validation_method": "Benchmarks on symbolic regression tasks (SRBench); comparison of R2 / MSE / MAE",
            "performance_metrics": null,
            "baseline_comparison": "Listed among SR baselines in the paper; not top-performing versus the SGA approach in the reported tables.",
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4573.4",
            "source_info": {
                "paper_title": "LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Eureka",
            "name_full": "Eureka (Human-level reward design via coding large language models)",
            "brief_description": "A method that uses coding-capable LLMs to design reward functions and related code artifacts; cited as an example of LLMs being used as coders/optimizers in scientific/engineering workflows.",
            "citation_title": "Human-level reward design via coding large language models",
            "mention_or_use": "mention",
            "method_name": "Eureka",
            "method_description": "Uses LLMs to generate code (reward functions / programmatic artifacts) and iteratively refines them via evaluation; in this paper, Eureka is used as a baseline where its LLM-temperature settings correspond to exploration-only behavior.",
            "llm_model_used": null,
            "scientific_domain": "RL / automated reward design / coding for scientific workflows",
            "number_of_papers": null,
            "type_of_quantitative_law": "programmatic reward specifications and code artifacts (not direct numerical laws)",
            "extraction_output_format": "generated code / reward functions",
            "validation_method": "Evaluation via downstream task performance (as-cited baseline behavior)",
            "performance_metrics": null,
            "baseline_comparison": "Used as a baseline in experiments; SGA with bilevel optimization outperformed Eureka in the tasks reported.",
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4573.5",
            "source_info": {
                "paper_title": "LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GhemGE",
            "name_full": "GhemGE (grammar-evolution population-based molecule design)",
            "brief_description": "A population-based molecule design algorithm using grammatical evolution that serves as a traditional baseline for de novo molecule generation and optimization.",
            "citation_title": "Population-based de novo molecule generation, using grammatical evolution",
            "mention_or_use": "mention",
            "method_name": "GhemGE",
            "method_description": "Population/genetic algorithm using grammar-based operators to generate molecular SMILES and evolve them for desired properties; used as a baseline for molecule design tasks in this paper.",
            "llm_model_used": null,
            "scientific_domain": "computational chemistry / molecule design",
            "number_of_papers": null,
            "type_of_quantitative_law": "structure–property optimization (not explicit symbolic laws)",
            "extraction_output_format": "SMILES strings and scored molecules",
            "validation_method": "Property evaluation and comparison of achieved objective values",
            "performance_metrics": "In Table 3, GhemGE reported higher losses on molecule-design tasks vs SGA (example numbers shown in paper: GhemGE (e) 4.8e-3 vs Ours (e) 1.3e-4, etc.).",
            "baseline_comparison": "SGA outperformed GhemGE on the molecular-design benchmarks reported in the paper.",
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4573.6",
            "source_info": {
                "paper_title": "LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mathematical discoveries from program search with large language models",
            "rating": 2
        },
        {
            "paper_title": "Solving olympiad geometry without human demonstrations",
            "rating": 2
        },
        {
            "paper_title": "Symbolicgpt: A generative transformer model for symbolic regression",
            "rating": 2
        },
        {
            "paper_title": "AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity",
            "rating": 2
        },
        {
            "paper_title": "Human-level reward design via coding large language models",
            "rating": 2
        },
        {
            "paper_title": "Population-based de novo molecule generation, using grammatical evolution",
            "rating": 2
        }
    ],
    "cost": 0.019861749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery
16 May 2024</p>
<p>Pingchuan Ma 
Tsun-Hsuan Wang 
Minghao Guo 
Zhiqing Sun 
Joshua B Tenenbaum 
Daniela Rus 
Chuang Gan 
Wojciech Matusik 
LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery
16 May 202415229B47DC78FCEE17B1442BD7EB3912arXiv:2405.09783v1[cs.LG]
Large Language Models have recently gained significant attention in scientific discovery for their extensive knowledge and advanced reasoning capabilities.However, they encounter challenges in effectively simulating observational feedback and grounding it with language to propel advancements in physical scientific discovery.Conversely, human scientists undertake scientific discovery by formulating hypotheses, conducting experiments, and revising theories through observational analysis.Inspired by this, we propose to enhance the knowledge-driven, abstract reasoning abilities of LLMs with the computational strength of simulations.We introduce Scientific Generative Agent (SGA), a bilevel optimization framework: LLMs act as knowledgeable and versatile thinkers, proposing scientific hypotheses and reason about discrete components, such as physics equations or molecule structures; meanwhile, simulations function as experimental platforms, providing observational feedback and optimizing via differentiability for continuous parts, such as physical parameters.We conduct extensive experiments to demonstrate our framework's efficacy in constitutive law discovery and molecular design, unveiling novel solutions that differ from conventional human expectations yet remain coherent upon analysis.</p>
<p>Introduction</p>
<p>In physical science, spanning physics, chemistry, pharmacology, etc., various research streams aim to automate and speed up scientific discovery (Wang et al., 2023).Each stream innovates within its field, creating methods tailored to its specific challenges and nuances.However, this approach often misses a universally applicable philosophy (Popper, 2005;Fortunato et al., 2018), which can be pivotal to democratizing access to advanced research tools, standardizing scientific practices, and enhancing efficiency across disciplines.Our goal aims to transcend specific domains, offering a unified approach to physical science.</p>
<p>As an inspiration, we observe how human scientists conduct scientific discovery experiments and conclude a few key experiences: (i) iteratively propose a hypothesis and make observations from experimentation to correct theories (Popper, 2005), (ii) divide the solutions into discrete components, such as physics equations or molecule structures, and continuous components, such as parameters for physics and molecule properties (Wang et al., 2023), (iii) exploit the existing knowledge while occasionally explore novel ideas aggressively in pursuits of breakthrough (Wuestman et al., 2020), (iv) follow a generic, universal principle for all types of physical scientific discovery yet with specific nuance of each discipline (Rosenberg &amp; McIntyre, 2019).</p>
<p>Standing out as generalist tools with an extensive repository of knowledge (AI4Science &amp; Quantum, 2023), large language models (LLMs) have recently risen to prominence in scientific discovery for their expansive knowledge bases, advanced reasoning capabilities, and human-friendly natural language interface.One line of research focuses on fine-tuning LLMs with domain-specific data to align natural language with scientific information, such as chemical (Chithrananda et al., 2020) or drug (Liu et al., 2021) structures; however, these methods are domain-bound and demand extensive data for broader application.Another research direction seeks to leverage the innate capabilities of pre-trained LLMs, augmented by external resources like the internet, programming, or documentation.LLMs serve as optimizers or agents (Huang et al., 2023) for mathematical problem-solving (Romera-Paredes et al., 2023), conducting chemical experiments (Boiko et al., 2023), and advancing molecular (Li et al., 2023) and drug discovery (Sharma &amp; Thakur, 2023).Nevertheless, these approaches are confined to the computational capability of LLMs, a crucial factor Taking the constitutive law searching problem as an example, the input is an initial guess (a purely elastic material), and the output is another constitutive law optimized towards the ground-truth (weakly compressible fluid).The initial guess first initialize a top-K heap for storing the solutions.In the outer-level optimization, an LLM takes in top-K previously proposed solutions and generates a better one upon them with modified continuous parameterization Θ and discrete expression E. In the inner-level optimization, a gradient-based optimization solves for optimal Θ via simulation and appends these optimized solutions in the heap.After a few iterations of bilevel optimization, the heap returns the top-1 solutions as the final solution.</p>
<p>in physical science for tasks like calculating numerical results based on physics law hypotheses to predict natural phenomena.To address this limitation, we propose to augment LLMs with physical simulation, hereby merging the knowledge-driven, abstract reasoning abilities of LLMs with the computational structures and accuracy of simulations.</p>
<p>To this end, inspired by the overarching philosophy of human scientists, we introduce Scientific Generative Agent (SGA), a bilevel optimization approach wherein the outerlevel engages LLMs as knowledgeable and versatile thinkers for generating and revising scientific hypothesis, while the inner-level involves simulations as experimental platforms for providing observational feedback.First, we employ LLMs to generate hypotheses, which then guide the execution of simulations.These simulations, in turn, yield observational feedback that helps refine and improve the proposed hypotheses.Secondly, we introduce a bilevel optimization framework: one level performs search-based optimization on discrete, symbolic variables like physics laws or molecule structures via LLMs; the other level performs gradient-based optimization via differentiable simulation for continuous parameters like material stiffness or molecule coordinates.Thirdly, we devise an exploit-and-explore strategy for the hypothesis proposal by adjusting LLM's generation temperature.Lastly, we demonstrate our pipeline is generally applicable across scientific disciplines, with only minimal modification such as altering the prompts.</p>
<p>For the empirical study, we focus on (i) molecular design that aims to discover molecular structure and atoms' coordinates based on its conformation and quantum mechanical properties and (ii) constitutive law discovery that aims to discover material constitutive equations and its corresponding mechanical properties directly from a recorded motion trajectory.To provide a concrete example, let's assume that we initially have simply the code for a purely linear material.We then task our model to uncover a more complex representation by optimizing its code to fit a highly nonlinear trajectory.In this task, our method capitalizes on the strengths of bilevel optimization: the outer-level utilizes LLMs to identify the correct symbolic material constitutive equations and formulates a proposition for potentially beneficial continuous parameterization (e.g., Young's modulus and Poisson's ratio); and the inner-level refines the proposed material parameters and provides informative feedback using differentiable simulation.Generally, our method can discover the desired molecules and constitutive laws, outperforming other LLM-based baselines; more interestingly, it can propose well-performing solutions that are beyond human expectation yet sensible under analysis by domain experts.Overall, our contributions are concluded as:</p>
<p>• We present a generic framework for physical scientific discovery that combines LLMs with physical simulations.</p>
<p>• We propose a bilevel optimization with LLMs for discretespace search-based optimization and differentiable simulations for continuous-space gradient-based optimization.</p>
<p>• We conduct extensive experiments to demonstrate the effectiveness and generality of the proposed framework in physics law discovery and molecular design; moreover, we showcase novel molecules or constitutive laws, while unexpected from a conventional perspective, are deemed reasonable upon examination by domain experts.</p>
<p>Scientific Generative Agent</p>
<p>SGA is a bilevel optimization framework where the upper level features LLMs as proposers of scientific solutions, and the lower level utilizes simulations as experimental platforms for validation.In Sec.2.1, we describe a formal definition of the bilevel optimization, followed by Sec.2.2 for outer optimization and Sec.2.3 for inner optimization.</p>
<p>Bilevel Optimization Pipeline</p>
<p>We formally describe the pipeline of our method, including the input/output of the system and the underlying submodules, and the overall optimization formulation.Suppose we are given a metric to evaluate a physical phenomenon y (e.g., a configuration of deformation) for a scientific problem L (y) (e.g., reconstruction of a mechanistic behavior).First, we describe the simulation (as an experimental platform) as,
y, z = Φ (θ; E) ,(1)
where Φ is a simulator that takes in scientific expression E (e.g., constitutive equations) and continuous components θ (e.g., material parameters) as inputs and gives simulated physical phenomenon y and additional observational feedback z (e.g., particles' trajectories) as outputs.Next, the LLM is prompted to act as a thinker to propose expressions E based on past experimental results from simulation,
E, Θ = LLM {L (y k ) , z k , o k , E k , Θ k } k∈[K] ; P ,(2)
where G (•) ≤ 0 refers to the validity of the simulation (i.e., whether an expression E is simulatable).The outer optimization searches for (i) an expression E that defines what experiments to be conducted Φ (•; E) and (ii) continuous parametrization Θ that defines the search space of the inner continuous optimization min θ∈Θ .With the dependencies on the outer-level variables (E, Θ), the inner optimization searches for the optimal continuous parameters θ given the proposed expression via differentiable simulation.</p>
<p>LLM-Driven Outer-Level Search</p>
<p>We dive deeper into how we use LLMs (Eq.2) and their interaction with the simulation for outer-level search (Eq.3).Interfacing with Simulation The primary challenge in integrating LLMs with simulation lies in devising a protocol that enables efficient, structured, yet adaptable communication between the two modules.We observe that physical scientific solutions are often represented as mathematical expressions or structured entities.Hereby, from LLMs to simulation, we consider two settings: equation searching and entity searching, both unified as the abstraction (E, Θ) in Eq. 2. In equation searching, LLMs are allowed to propose equations E along with the search space of the inner-level continuous optimization Θ; for practitioners, an example using PyTorch can be Θ as init that defines continuous parameters via nn.Parameter and E as forward that defines computation of equations (see Fig. 1).In entity searching, LLMs propose descriptions of structures E (e.g., how atoms are connected to form a molecule) with Θ simply reduced to constant (e.g., every atom has its 3D coordinates to be optimized) and omitted from the optimization Eq. 3a as decision variables.On the other hand, from simulation to LLMs, we leverage domain experts' knowledge to craft functions for extracting compact, relevant information z as observational feedback; this process is akin to an experienced scientist offering guidance to a junior colleague on how to document experimental findings effectively.For instance, human experts often monitor the movements of specific body regions to derive constitutive laws.Therefore, to aid in this process, we include a function in the simulation that records the particle trajectories.Lastly, the subsequent section Sec.2.3 will provide an in-depth explanation of the inner optimization results denoted as o.These results serve as feedback from the simulation to the LLMs.</p>
<p>LLM-driven Optimization</p>
<p>Exploitation and Exploration</p>
<p>Inspired by human scientists achieving breakthroughs by skillfully balancing careful progression with bold exploration, we devise an exploitand-explore strategy by tuning the LLMs' decoding temperature (Yang et al., 2024).When generating offspring {E m , Θ m } m∈[M ] in Eq. 3a, we divide them into two groups: one (m ∈ M exploit ) consists of cautious followers that keep the "gradient" and conservatively trails previous solutions, while the other (m ∈ M explore ) comprises daring adventurers that take risks and suggest unique solutions.Empirically, we observed that (i) M exploit often contains repetitive solutions from previous iterations, and (ii) M explore tends to yield solutions too random to be informative for guiding optimization, or invalid (i.e., violating Eq. 3b), thus providing little feedback signal.As a rule of thumb, we have found that a 1:3 ratio between M exploit and M explore is effective.</p>
<p>Differentiable Inner-Level Optimization</p>
<p>Under the search space Θ and expression for simulation E from the outer level, inner optimization (Eq.3c) involves a gradient-based optimization that solves for optimal continuous parameters θ ∈ Θ via differentiable simulation (Eq.1).</p>
<p>Essentially, the domain-specific knowledge is distilled via gradients ∇ θ Φ(θ; E) from the simulation to the intermediate optimization results o (like loss curve).The (ŷ, o) are then fed back to LLMs for revising solutions.Note that o may involve the loss curve toward the target metric L and other auxiliary recordings throughout optimization, carrying information of how to improve solutions in various aspects; for example, with L as displacement of position, o may include velocities across the inner optimization iterations.</p>
<p>Experiments</p>
<p>Problem Definitions</p>
<p>Constitutive Law Discovery Identifying the constitutive law from motion observations stands as one of the most difficult challenges in fields such as physics, material science, and mechanical engineering.Here we follow the recent advances in physical simulation and formulate the constitutive law discovery task as an optimization problem (Ma et al., 2023) using differentiable Material Point Method (MPM) simulators (Sulsky et al., 1995;Jiang et al., 2016).Note that our method is not specifically tailored to MPM simulators and applies to any physical simulation.The objective of this task is to identify both the discrete expression and continuous parameters in a constitutive law, specifically the symbolic material models φ (•) and their corresponding material parameters θ, from a ground-truth trajectory of particle positions Xt∈[1,...,T ] where T denotes the number of steps.</p>
<p>In this problem, we consider two types of constitutive laws, φ E (•; θ E ) and φ P (•; θ P ), for modeling elastic and plastic Table 1.Benchmark.We compare our method against 4 baselines and 2 variations of our method, while also noting the difference in architecture or hyper-parameters.We use column #Iter.as the number of iterations, #Hist.as the K value for the top-k retrieval in the historical optimization steps, #Exploit #Explore as the number of offspring for exploitation versus exploration, Bilevel as if bilevel optimization is enabled.Our experiments encompass 8 different tasks, which are divided into constitutive law search (a-d) and molecule design (e-h).A lower loss value is preferable across all tasks.The best method with the lowest loss is highlighted in bold text.materials respectively, and they are formally defined as:
Method #Iter. #Hist. #Exploit #Explore Bilevel Constitutive Law Search Molecule Design (a) ↓ (b) ↓ (c) ↓ (d) ↓ (e) ↓ (f) ↓ (g) ↓ (h) ↓ CoT 1 5 N/A ✗ 298φ E (F; θ E ) → τ (4a) φ P (F; θ P ) → F corrected ,(4b)
where
F ∈ R 3×3 is the deformation gradient, τ ∈ R 3×3 is the Kirchhoff stress tensor, F corrected ∈ R 3×3
is the deformation gradient after plastic return-mapping correction, and θ E and θ P are the continuous material parameters for elastic and plastic constitutive laws respectively.Given a specific constitutive law, we input it to the differentiable simulation and yields a particle position trajectory:
X t∈[1,...,T ] = sim (φ (•; θ)) ,(5)
and we optimize the constitutive law by fitting the output trajectory to the ground truth Xt∈[1,...,T ] .</p>
<p>Molecule Design In this study, we focus on a prevalent task in molecule design: discovering molecules with specific quantum mechanical properties.Our objective is to determine the optimal molecular structure and its 3D conformation to match a predefined target quantum mechanical property.The design process involves both the discrete expression -the molecular structure represented by SMILES strings (Weininger, 1988), and the continuous parameters -the 3D coordinates of each atom in the molecule.The methodology comprises two loops: In the outer loop, the LLM generates the initial molecular structure as a SMILES string, along with a preliminary guess for the 3D atom coordinates.The inner loop involves simultaneous optimization of both the molecule's 3D conformation and quantum mechanical properties, both determined by 3D atom positions.</p>
<p>For the generation of 3D conformations, we utilize the ETKGD algorithm (Riniker &amp; Landrum, 2015) followed by optimization using the Merck Molecular Force Field (MMFF) (Halgren, 1996), both implemented within the RD-Kit (Landrum et al., 2013).To get the quantum mechanical property values, we employ UniMol (Zhou et   We choose gpt-4-turbo-preview as the backbone model for LLM and tentatively set the exploiting temperature T l = 0.5 and exploring temperature T h = 1.0.</p>
<p>Physical Scientific Discovery</p>
<p>We consider 6 strong baselines for evaluation: (i) We set the hyperparameters to be equal to Eureka except for the number of historical optimization steps.In all these works (i-iv), we notice the temperatures for LLM inference are all 1.0, which is equal to the exploring temperature in our method, so we denote them with 0 exploiter.We also consider 2 variants of our method: (v) Ours (no bilevel) We present our experiments against the 8 designed tasks and show the results in Table 1.Compared to baselines (i-iv), our method is significantly better by a number of magnitudes.When the bilevel optimization is removed from our method, the performance drops dramatically, but still statistically better than baselines (i-iv), indicating the choice of hyperparameters and the integration of exploitation is helpful for the task.When we remove the exploitation but restore the bilevel optimization, we notice the performance grows back.It has comparable performance compared to our method in (d) or even better results in (h).However, in some tasks, especially hard ones (e.g., (b) and (f)) that we care more in reality, the performance gap is over 50%, indicating the effectiveness of our exploit-and-explore strategy.We also present the loss trend in task (a) in Figure 2, our method outstands with a much lower loss and a converging trend.</p>
<p>We also compare our method with traditional methods in each specific area to demonstrate the generalizability of our method.First, we reformulate our constitutive law search task (a) into a symbolic regression task by (i) capture the ground-truth output (the stress tensors) as the supervision, and (ii) separate the 9 output dimension into 9 independent problems and ensemble them for evaluation.Note that these modifications dramatically simplified the original task: we removed back-propagation through time (BPTT) and directly discover the constitutive law without surrogate loss.We evaluate 14 traditional baselines in SRBench (La Cava et al., 2021) and 3 data-driven pre-trained baselines.We select the top few baselines in Table 2 and show the rest in the Appendix C.1.As shown the table, our method topped on this task even with a much more challenging setting.Also, since our method depends on the in-context learning ability of LLMs, it has little constraint in the num- Figure 3. Ablation on bilevel optimization.We denote the optimization trajectory with and without out bilevel optimization with red dot and orange triangle respectively.We visualize the intermediate step of our method before the inner-level optimization using orange cross.We also highlight the outer LLM optimization and inner simulation optimization using orange and red arrows.ber of variables than the data-driven pre-trained baselines.For moledule design tasks, we also compare our method with GhemGE (Yoshikawa et al., 2018), which employs a population-based molecule design algorithm.As shown in Table 3, our method presents a much lower loss, demonstrating the general effectiveness of our method.</p>
<p>Ablation Study</p>
<p>Generalization or Memorization In order to figure out if the improvement introduced by our method is merely because the LLM saw the solutions in its training phase, we design an experiment ablating it by making it invent an imaginary constitutive law that does not exist on the earth.We mix the constitutive law of von Mises plasticity, granular material, and weakly compressible fluid by 50%, 30%, and 20%, so that the new constitutive law represents an imaginary material whose behavior is extremely complex.We repeat our experiment setup as in Figure 1.We compare our method against the baselines and report the performances in Table 4.As shown in the table, our method can still discover the constitutive law with a low quantitative loss.From our observation, there is very little visual difference between the ground-truth material and the optimized constitutive law.We show the discovered constitutive law in Appendix D.9.</p>
<p>Bilevel Optimization is the Key Here we evaluate the importance of bilevel optimization in Figure 3 using the task (h).Comparing the blue triangle curve and the red dot curve, which represent the LLM-driven outer-level optimization and the simulation-driven inner-level optimization, it is easy to conclude that the loss performance with bilevel optimization is better.Nevertheless, we are also interested in how bilevel optimization works inside each optimization step and how much LLMs and simulations help respectively.As shown as a zigzag curve, we found that LLMs and simulations help each other over all optimization steps: the next proposal from LLMs will be better with simulationoptimized results, and vice versa.We argue that LLMs and simulations have different expertise: LLMs are generalist scientists who have cross-discipline knowledge, while simulations are domain experts who have specialized knowledge.</p>
<p>LLM Backbone In addition to GPT-4 (OpenAI, 2023), we repeat the experiments in Table 1 4. Indicated by the largest area, GPT-4, as our choice, statistically outperforms the other methods.Interestingly, we found Claude-3-Sonnet is the second top method on most of constitutive law search task, while Mixtral-8x7B even tops on 2 molecule design tasks.As a result, our workflow also works for other LLMs, however, our suggestion for practitioners is to try GPT-4 as the first choice but also consider open-source model (e.g., Mixtral-8x7B) for budget or customizability.</p>
<p>Exploitation v.s. Exploration</p>
<p>We visualize the statistics of the simulation execution status in Figure 5 (a) using the task (b), which is one of the most challenging tasks in our experiments.When the exploitation is removed, the error rate dramatically increases, as shown by a decrease in green bars.It leads to a degeneration in the performance of the methods with exploitation as shown in Figure 5 (b).However, even though the success rate remains high, when exploration is removed, the optimization result is still worse than keeping them both.We argue that exploration is significant when the optimization problem is challenging, especially in our case, where the search space is highly non-linear and unstructured and resulting in numerous local optimum.</p>
<p>Case Study</p>
<p>Constitutive Law Search We provide a trimmed snippet of our searched constitutive law in Figure 6 (a) for task (a) where a highly non-linear material is provided as the trajectory to fit.We reformat the code slightly to fit into the text, where the complete example can be found in the Appendix.Starting from a linear material, our method is able to automatically generate the constitutive law with a quadratic deviatoric term.Note that our method also provides a concrete implementation of init function that defines the continuous parameters in the computational graph for later inner-level optimization.</p>
<p>Molecule Design When comparing the two molecules with respect to their HOMO-LUMO energy gap based on optimized results from the LLM as shown in Figure 6 (b), we observe distinct characteristics in each: (i) Molecule A (gap-0) includes sulfur and chlorine atoms attached to a ring, coupled with a trifluoromethyl group, introducing electron-withdrawing effects, and (ii) Molecule B (gap-2) includes oxygen (notably in ethers) and sulfur within the ring structures introducing localized non-bonding electron pairs.Furthermore, the overall structure of Molecule B is more complex than that of Molecule A, containing multiple rings.An intriguing aspect of Molecule B, which might initially defy expectations, is the presence of a single fluorine atom.The high electronegativity of fluorine typically leads to electron density withdrawal, influencing the gap value.However, due to the complexity of Molecule B's structure, the impact of the fluorine atom is somewhat localized, thereby not significantly altering the gap value.This multifaceted process unfolds through two synergistically linked stages: hypothesis formation and the collection and analysis of experimental data.The integration of automated systems not only augments the scientific inquiry process but also streamlines the discovery pipeline, from conceptualization to empirical validation.This paper places a particular emphasis on, but is not limited to, constitutive law discovery and molecular design.These areas exemplify the profound impact of automation in unraveling complex material behaviors and in the innovative design of molecules with tailored properties.Automatic identification of constitutive material models has been a long-standing problem and recent works utilizes differentiable simulation (Du et al., 2021;Ma et al., 2023;2021) to address it as a system identification problem.Leveraging machine learning and artificial intelligence, researchers are able to predict molecular be-havior, optimize chemical structures for specific functions, and thus, rapidly accelerate the development of new drugs, materials, and chemicals (Jin et al., 2018;Zhou et al., 2019;Schneider, 2018).</p>
<p>Related Work</p>
<p>Large Language Models and Agents</p>
<p>The advancement of Large Language Models (LLMs) such as ChatGPT and GPT-4 has sparked considerable interest in their potential as autonomous agents (Brown et al., 2020;OpenAI, 2022;2023).Recent developments have shown that LLMs can be enhanced to solve complex problems by creating and utilizing their own tools, as demonstrated in the LATM framework (Sumers et al., 2024), and by acting as optimizers in the absence of gradients, as seen in the OPRO methodology (Yang et al., 2024).These approaches signify a shift towards more independent and versatile LLM-based agents capable of generating solutions through self-crafted tools and optimization techniques (Cai et al., 2024;Yao et al., 2023b;a), showcasing their evolving problem-solving capabilities.In the realm of scientific discovery, LLMs have begun to make significant contributions, particularly in mathematics and computational problems.The FunSearch method (Romera-Paredes et al., 2023) pairs LLMs with evaluators to exceed known results in extremal combinatorics and online bin packing, illustrating LLMs' ability to discover new solutions to established problems.Similarly, AlphaGeometry's success (Trinh et al., 2024) in solving olympiad-level geometry problems without human demonstrations highlights the potential of LLMs in automating complex reasoning tasks.These examples underline the transformative impact of LLMs in pushing the boundaries of scientific inquiry and automated reasoning.</p>
<p>Bilevel Optimization</p>
<p>Bilevel optimization involves a hierarchical structure with two levels of optimization problems, where the solution to the upper-level problem is contingent upon the outcome of the lower-level problem (Colson et al., 2007).Bilevel optimization problems are inherently more complex than their single-level counterparts due to the nested nature of the optimization tasks and the intricate interdependencies between them.Recent advancements have focused on developing efficient algorithms, including evolutionary algorithms (Sinha et al., 2017b), gradient-based approaches (Liu et al., 2022), and approximation techniques (Sinha et al., 2017a), to tackle the computational challenges presented by the non-convex and non-differentiable characteristics of many bilevel problems.Among a wide span of application domains of bilevel optimization, neural architecture search (NAS) (Liu et al., 2019;Bender et al., 2018;Cai et al., 2019;Xue et al., 2021) is prominent and close to the problem setting in this paper: the upper level optimizes the discrete neural network architecture while the lower level optimizes the continu-ous weights of the neural network.However, typical NAS methods require a predefined search space, constraining the exploration of discrete network architectures to manually specified boundaries.Our framework distinguishes itself by employing LLM encoded with general knowledge and gets rid of the limitations imposed by manual design constraints.</p>
<p>Conclusion</p>
<p>We consider a few limitations and future directions.(i) Although we prompt the LLM to generate pseudo-code plans and comments, it is generally hard to ensure the interpretability of LLM-generated solutions.(ii) Since the LLM-generated codes are executed directly without any filtering in our application, there exists potential AI safety risk that hazards the operating system.(iii) Our method only utilizes the internal knowledge of LLMs as the prior, where in reality people design manual constraints and rule to regularize and improve the optimization (Udrescu et al., 2020).We leave these domain-specific applications and human feedback-based regularization methods as our future work.(iv) The performance our method highly depends on the differentiablity of the generated code.However, Zeroorder optimizers (Hansen, 2006) should also shine since the number of continuous parameters is relatively limited.(v) LLM inference requires large computational resources and thus increases expense.For example, it spends around $10 for our method to complete one task using GPT-4, which will be increasingly inacceptable when the number of iteration grows.(vi) Due to the reuse of previously generated solutions in our proposed top-k heap, the KV cache in LLM will be highly similar between neighbor iterations.It opens a gate for recent KV cache optimization methods (Zheng et al., 2023) to speedup our method by KV cache reusing.</p>
<p>In conclution, we present Scientific Generative Agent, a bilevel optimization framework: LLMs serve as knowledgeable and adaptable thinkers, formulating scientific solutions like physics equations or molecule structures; concurrently, simulations operate as platforms for experimentation, offering observational feedback and optimizing continuous components like physical parameters.We focused on two scientific problems: constitutive law search and molecular design.Our approach outperforms other LLM-based benchmark methods, delivering consistent, robust, and nearly monotonic improvement.Furthermore, it shows exceptional ability in identifying unknown, true constitutive laws and molecular structures.Remarkably, our system generates innovative solutions that, despite being unconventional, are deemed reasonable after being thoroughly analyzed by experts in their respective domains.We view our process as a trailblazer, establishing a new paradigm for utilizing LLMs and simulations as bilevel optimization to further advancements in physical scientific discoveries.</p>
<p>A. Full Prompts</p>
<p>System prompt for constitutive law discovery:</p>
<p>You are an intelligent AI assistant for coding, physical simulation, and scientific discovery.Follow the user's requirements carefully and make sure you understand them.Your expertise is strictly limited to physical simulation, material science, mathematics, and coding.Keep your answers short and to the point.Do not provide any information that is not requested.Always document your code as comments to explain the reason behind them.Use Markdown to format your solution.You are very familiar with Python and PyTorch.Do not use any external libraries other than the libraries used in the examples.</p>
<p>System prompt for molecule design:</p>
<p>You are an intelligent AI assistant for coding, molecule design, and scientific discovery.Follow the user's requirements carefully and make sure you understand them.Your expertise is strictly limited to physical simulation, material science, chemistry, molecule design, mathematics, and coding.Keep your answers short and to the point.Do not provide any information that is not requested.Always document your code as comments to explain the reason behind them.Use Markdown to format your solution.You are very familiar with PyTorch.Your are very familiar with the SMILES notation (Simplified Molecular-Input Line-Entry System).Do not use any external libraries other than the libraries used in the examples.</p>
<p>Coding format prompt for elastic constitutive law discovery:</p>
<h2>Format Requirements ### PyTorch Tips 1.When element-wise multiplying two matrix, make sure their number of dimensions match before the operation.For example, when multiplying 'J' (B,) and 'I' (B, 3, 3), you should do 'J.view(-1, 1, 1)' before the operation.Similarly, '(J -1)' should also be reshaped to '(J -1).view(-1, 1, 1)'.If you are not sure, write down every component in the expression one by one and annotate its dimension in the comment for verification.2. When computing the trace of a tensor A (B, 3, 3), use 'A.diagonal(dim1=1, dim2=2).sum(dim=1).view(-1, 1, 1)'.Avoid using 'torch.trace'or 'Tensor.trace'since they only support 2D matrix.### Code Requirements 1.The programming language is always python. 2. Annotate the size of the tensor as comment after each tensor operation.For example, '# (B, 3, 3)'.3. The only library allowed is PyTorch.Follow the examples provided by the user and check the PyTorch documentation to learn how to use PyTorch.4. Separate the code into continuous physical parameters that can be tuned with differentiable optimization and the symbolic constitutive law represented by PyTorch code.Define them respectively in the '<strong>init</strong>' function and the 'forward' function.5.The first output of the 'forward' function is the updated deformation gradient.Always remember the second output of the 'forward' function is Kirchhoff stress tensor, which is defined by the matrix multiplication between the first Piola-Kirchhoff stress tensor and the transpose of the deformation gradient tensor.Formally, 'tau = P @ FˆT', where tau is the Kirchhoff stress tensor, P is the first Piola-Kirchhoff stress tensor, and F is the deformation gradient tensor.Do not directly return any other type of stress tensor other than Kirchhoff stress tensor.Compute Kirchhoff stress tensor using the equation: 'tau = P @ FˆT'.Returns: kirchhoff_stress (torch.Tensor): Kirchhoff stress tensor (B, 3, 3).""" return kirchhoff_stress ''' ### Solution Requirements 2. Annotate the size of the tensor as comment after each tensor operation.For example, '# (B, 3, 3)'.3. Separate the code into: (1) python string 'SMILES': the SMILES string describing the molecular topology structure and atomic types, and (2) matrix 'coordinates' the 3D coordinates of all atoms.These representations should not include hydrogens.4. The SMILES string should be valid.Use your knowledge about Simplified Molecular-Input Line-Entry System to help you design a valid one.5.The number of atoms in the SMILES string should be no less than 8, which means the number of atoms should be &gt;= 8. Try to generate molecule with diverse atoms.6.The 3D coordinates of the atoms should not be overlapping with each other.In another word, every row in the matrix 'coordinates' should be distinct from each other.7. The 'coordinates' matrix is of shape '(N, 3)' where 'N' stands for the number of atoms in the molecule.It should be identical to the number of atoms that the proposed SMILES string represents.State out the shape of any matrix defined in the comment as shown in the following example.State out the number of atoms that the SMILES string represents in the comment as shown in the following example.8.The discrete SMILES string is critical in this problem since it defines the structure and cannot be tuned using differentiable optimization.Please propose different SMILES string from all examples or iterations above to discover and evaluate more structure.This is very important.3. Output the code in a single code block "'''python ... '''" with detailed comments in the code block.After the SMILES string, compute the number of atoms in it by counting.Remember that the number of atoms in the SMILES string should be no less than 8, which means the number of atoms should be &gt;= 8. Try to generate molecule with diverse atoms.Do not add any trailing comments before or after the code block.Start this section with "### Code".</h2>
<p>B. More Explanations B.1. Data Workflow</p>
<p>The full input to LLM has 3 main parts: (i) system prompt, (ii) iteration information, and (iii) format prompt.For the system prompt, we insert it into the LLM at the beginning or input it as a special instruction depending on the type of LLM.For the iteration information, we first concatenate the code and its feedback and then simply stack the top K solutions.Finally, we append the format prompt at the end of the prompt to regularize the expected output.From our experiments, it is important to keep the order of prompts to ensure the performance and the successful parsing.More precisely, we show this process in the following python-like code:</p>
<p>B.2. Differences to Symbolic Regression Task</p>
<p>• Our problem focuses on loss-guided general scientific discovery, which is a super-set of regular regression problems.</p>
<p>In the constitutive law search tasks, we do not directly feed the input/output pair to our method.Instead, we consider a much more challenging task: apply the generated constitutive law recursively and use the overall loss as the performance metric.Concretely, a classic SR methods solve arg min f ∥f (X) − y∥ given &lt; X, y &gt; pairs, whereas our method solves arg min f ∥g(f (X))∥ given &lt; X, g(f (X)) &gt; pairs and g is a complex function like physical simulation.It is easy to construct g to cover the former case using the later formulation, proving the generality of our problem setup.We formulate our problem as such to reflect a more realistic scenario in scientific discovery, where direct supervision is extremely sparse.</p>
<p>• Our method supports arbitrary number of input variables and output features, where most of SR methods (Valipour et al., 2021) have limitation on the number of input and output.The input limitation strongly caps the complexity of tasks they can solve, and the output limitation forces them ignore the structural correlation between each output dimension.In a comparison, our method supports arbitrary problem settings thanks to the code-based representation, which enables multi-dimensional arrays and tensor operations.</p>
<p>• Our model adapts to multi-discipline application easily, while traditional SR methods typically incorporate with domainexperts' priors via hard-coded constraints and heuristic (Udrescu et al., 2020), which is limited, domain-specific, and difficult to customize.Our method is built upon LLMs pre-trained on internet-level data that contains multi-discipline natural languages, mathematical expressions, and codes.As a result, it is easy for users to customize it and adapt to their own diverse applications via natural language guidance.</p>
<p>C. More Experiments</p>
<p>C.1. Symbolic Regression</p>
<p>We present the full results of the comparison to symbolic regression methods in Table 5</p>
<p>C.2. Longer Iteration</p>
<p>In order to further investigate the potential of our method and ablate the hyper-parameters for practitioners, we add a new study in terms of the number of iterations (question-answering cycles).We repeat our experiment in Table 1 with a prolonged number of iterations to 20 and report the performance in Table 6.As shown in the table, the number of iterations turns out to be a determining hyper-parameter with significant impart on the performance.While it has little affect on relatively easier tasks, it dramatically improves the performance of the most challenging tasks including (b) and (c).For practitioners, the number of iteration should be first considered as the most important hyper-parameter when adapting our method to their own tasks.</p>
<p>D. More Results</p>
<p>D.1. Constitutive Law Discovery (a)</p>
<p>The best solution on task (a) optimized by our method:</p>
<p>Proceedings of the41 st International Conference on Machine Learning, Vienna, Austria.PMLR 235, 2024.Copyright 2024 by the author(s).</p>
<p>Figure 1 .
1
Figure1.The overall pipeline of Scientific Generative Agent (SGA).Taking the constitutive law searching problem as an example, the input is an initial guess (a purely elastic material), and the output is another constitutive law optimized towards the ground-truth (weakly compressible fluid).The initial guess first initialize a top-K heap for storing the solutions.In the outer-level optimization, an LLM takes in top-K previously proposed solutions and generates a better one upon them with modified continuous parameterization Θ and discrete expression E. In the inner-level optimization, a gradient-based optimization solves for optimal Θ via simulation and appends these optimized solutions in the heap.After a few iterations of bilevel optimization, the heap returns the top-1 solutions as the final solution.</p>
<p>Figure 2 .
2
Figure 2. Loss trends comparison.Loss of the best solution averaged across seeds at different iterations of LLM-driven optimization, where the shading shows the min/max value.</p>
<p>w/ bilevel (pre-inner-optim) w/ bilevel (ours)</p>
<p>Figure 4 .
4
Figure 4. Ablation on the backbone LLM.We compare the performances of 4 selected backbone LLMs and report the rank of them.A outer curve indicates a better performance.</p>
<p>Figure 5 .
5
Figure 5. Ablation on exploration-exploitation.(a) Histogram of solutions that are valid for simulation (Eq.3b) across iterations.(b) Loss (L in Sec.2.1) of the best solution averaged across seeds at different iterations, where the shading indicates the min/max values.</p>
<p>Figure 6 .
6
Figure 6.Case Study.(a) We give a concrete example of the searched constitutive law.(b) We provide 2 novel molecules optimized for different objectives with their SMILES stings.</p>
<p>6 .
6
The proposed code should strictly follow the structure and function signatures below: '''python import torch import torch.nnas nn class Physics(nn.Module): def <strong>init</strong>(self, param: float = DEFAULT_VALUE): """ Define trainable continuous physical parameters for differentiable optimization.Tentatively initialize the parameters with the default values in args.Args: param (float): the physical meaning of the parameter.""" super().<strong>init</strong>()self.param= nn.Parameter(torch.tensor(param))def forward(self, F: torch.Tensor) -&gt; torch.Tensor: """ Compute Kirchhoff stress tensor from deformation gradient tensor.Args: F (torch.Tensor): deformation gradient tensor (B, 3, 3).</p>
<p>9 .
9
The proposed code should strictly follow the structure and function signatures below: '''python SMILES: str # N atoms coordinates: list[list[float]] # (N, 3) ''' ### Solution Requirements 1. Analyze step-by-step what the potential problem is in the previous iterations based on the feedback.Think about why the results from previous molecule structure mismatched with the ground truth.Do not give advice about how to optimize.Focus on the formulation of the SMILES string.Start this section with "### Analysis".Analyze all iterations individually, and start the subsection for each iteration with "#### Iteration N", where N stands for the index.Remember to analyze every iteration in the history.2. Think step-by-step what you need to do in this iteration.Think about how to separate your algorithm into a continuous 3D coordinate system part and a discrete SMILES string part.Remember the SMILES string proposed should always be different from previous iterations.After propose the new SMILES string, compute and count step-by-step how many atoms it contains.The continuous parameter should follow the number of atoms in the SMILES string.Describe your plan in pseudo-code, written out in great detail.Start this section with "### Step-by-Step Plan".</p>
<p>19 """ 20 #
1920
Compute the determinant of the deformation gradient (volumetric change) 21 J = torch.det(F)# (B,) 22 23 # Compute the volumetric part of the deformation gradient: Jˆ(1/3) * I 24 I = torch.eye(3).to(F.device)# (3, 3) 25 # Expand the identity matrix to the entire batch 26 I = I.view(1, 3, 3).expand(F.size(0),-1, -1) # (B, 3, 3) 27 # Calculate volumetric part 28 vol_deform = torch.pow(J,1.0 / 3.0).view(-1, 1, 1) * I # (B, 3, 3) 29 30 # Calculate the deviatoric part of F: divide F by Jˆ(1/3) 31 dev_deform = F / torch.pow(J,1.0 / 3.0).view(-1, 1, 1) # (B, 3, 3) 32 33 # Modulate correction by self.param and construct the correction term 34 correction = self.param* (I -dev_deform) # (B, 3</p>
<p>Num of exploiting M l , Num of exploring M h , Exploiting temperature T l , Exploring temperature T h 1: # Store ranked (solution,param) by heap 2: H ← heap()
3: # Continuous optimization4: θ ← optim(E,θ;Φ)5: H.append((E, θ))6: for i = 1, . . . , N do7: # Generate M l solutions from LLM8: (E,Θ)[:M l ] ← LLM(H.topk(K),T l )9: # Generate M h solutions from LLM10:(E,Θ)[M l :M l +M h ] ← LLM(H.topk(K),T h )11:for m = 1, . . . , M l + M h do12:# Continuous optimization13:θ ← optim(E,θ ∈ Θ;Φ)14:H.append((E, θ))15:end for16: end forOutput: H.topk(1) # Return the bestwhere the set [K] summarizes the pointers to the past sim-ulation results containing an evaluation of the scientificproblem L(y k ), other physical feedback (z k , o k ), and pastproposals (E k , Θ k ); o k summarizes the intermediate resultsof the inner optimization (later detailed in Sec. 2.3); Θ de-termines the continuous parameterization for the decisionvariables of the inner optimization (e.g., which variablesto be optimized within a proposed equation); P is prompt.With these, we define the bilevel optimization problem as,min E,ΘL y E, Θ, θ; Φ(3a)s.t. G (E, Θ; Φ) ≤ 0(3b)θ ∈ arg minL (y (θ; Φ, E)) ,θ∈Θ
Algorithm 1 Scientific Generative AgentInput: Discrete expression and continuous param (E,θ ∈ Θ),</p>
<p>Table 2 .
2
Comparison with symbolic regression.We compare our method against 5 most performant methods in SRBench (La Cava et al., 2021) and 3 pre-trained symbolic regression methods.Sym.denotes whether the result is symbolic or not.
MethodR2 ↑MSE ↓ MAE ↓ Sym.FFX0.9824 4.5e+5 3.7e+2✓MLP0.9876 3.2e+5 3.4e+2✗FEAT0.9964 9.2e+4 1.7e+2✓DSO0.9968 8.2e+4 9.2e+1✓Operon0.9988 2.8e+4 9.8e+1✓SymbolicGPT 0.5233 6.9e+6 1.7e+3✓NeSymReSN/A to &gt;3 variables✓T-JSLN/A to &gt;2 variables✓Ours0.9990 1.7e+4 8.6e+1✓Implementation Details We run all our experiments 5times with different random seeds following previous prac-
(Kingma &amp; Ba, 2015)9)).Due to the complexity of the task, we provide a simple bootstrapping example of a valid design to ensure the success rate.We use warp(Macklin, 2022)for the differentiable MPM simulation, and we develop our inner-level optimization upon PyTorch(Paszke et al., 2019).In all our experiments, we use mean square error as the criteria and Adam optimizer(Kingma &amp; Ba, 2015).</p>
<p>Table 3 .
3
(Yoshikawa et al., 2018)on-based molecule design.We compare our method against a traditional population-based molecule design method GhemGE(Yoshikawa et al., 2018)and report the results of molecule design tasks (e-h).
Method(e) ↓(f) ↓(g) ↓(h) ↓GhemGE 4.8e-31.81.59.8e-5Ours1.3e-4 1.1e-1 5.4e-1 3.6e-5</p>
<p>Table 4 .
4
Experiment in imaginary constitutive law.We construct an imaginary constitutive law to keep LLM from cheating by memorization and report the results of our method and baselines.Ours (no exploit) removes the exploitation by setting the temperature to 1.0 all the time.
Method FunSearch Eureka OPRO OursLoss105.089.198.01.3e-3removes the bilevel optimization by only searching withLLM. (vi)</p>
<p>Table 5 .
5
Symbolic Regression
MethodR2 ↑MSE ↓MAE ↓ SymbolicAIFeynman (Udrescu et al., 2020)0.05105 22814675.8 2520.0✓DSR (Petersen et al., 2020)0.57527 10966411.0 2045.0✓BSR (Jin et al., 2019)0.66526 8642965.01938.6✓AdaBoost (Schapire, 2003)0.75058 6439962.91777.7✗GP-GOMEA (Virgolin et al., 2021)0.77734 5749076.41580.1✓SBP-GP (Virgolin et al., 2019)0.81773 4706077.01367.5✓LightGBM (Ke et al., 2017)0.83368 4294433.71129.9✗XGBoost (Chen &amp; Guestrin, 2016)0.87775 3156500.51109.2✗MRGP (Arnaldo et al., 2014)0.91074 2304682.5950.5✓EPLEX (La Cava et al., 2019)0.91851 2104070.1122.2✓FFX (McConaghy, 2011)0.93124 1775263.7801.7✓MLP0.98240454461.5366.3✗FEAT (Cava et al., 2019)0.98761319800.6336.1✓DSO (Mundhenk et al., 2021)0.9964292374.9168.6✓Operon (Kommenda et al., 2020)0.9968481577.992.4✓SymbolicGPT (Valipour et al., 2021) 0.52333 6862154.71680.7✓NeSymReS (Biggio et al., 2021)N/A to &gt;3 variables✓T-JSL (Li et al., 2022)N/A to &gt;2 variables✓Ours0.9990117424.686.4✓</p>
<p>Table 6 .
6
Longer Iteration</p>
<h1>Iterations(a) ↓(b) ↓(c) ↓(d) ↓(e) ↓(f) ↓(g) ↓(h) ↓55.2e-52.1e-16.0e-21.4e-12 1.3e-41.1e-15.4e-13.6e-5204.2e-64.0e-42.5e-31.4e-12 1.3e-46.5e-21.2e-15.6e-6Improvement +1138.1% +52400.0% +2300.0%0.0%0.0% +69.2% +350.0% +542.9%</h1>
<p>The best solution on task (b) optimized by our method:The best solution on task (d) optimized by our method:
D.4. Constitutive Law Discovery (d)1 import torch2 import torch.nn as nn34 class Physics(nn.Module):56DEFAULT_VALUE = 0.0 # Best guess based on previous behavior78def <strong>init</strong>(self, param: float = DEFAULT_VALUE):9 1 import torch """ 10 Define trainable continuous physical parameters for differentiable optimization. 2 import torch.nn as nn 11 The parameter modulates corrections towards nearly isochoric behavior. 3 12 """ 4 class Physics(nn.Module): 5 13 super().<strong>init</strong>() # Best values from the training curves 6 14 self.param = nn.Parameter(torch.tensor([param])) # Scalar modulation parameter DEFAULT_YOUNGS_MODULUS_LOG = 13.03 7 15 DEFAULT_POISSONS_RATIO_SIGMOID = -1.99 16 def forward(self, F: torch.Tensor) -&gt; torch.Tensor: 8 9 17 """ def <strong>init</strong>(self, youngs_modulus_log: float = DEFAULT_YOUNGS_MODULUS_LOG, poissons_ratio_sigmoid: float = DEFAULT_POISSONS_RATIO_SIGMOID): 18 Symbolic deformation gradient correction model.10"""11Define trainable continuous physical parameters for differentiable optimization.12Initialize the parameters with the best values from previous feedback.13"""14super().<strong>init</strong>()15# Initialize the parameters as trainable parameters16self.youngs_modulus_log = nn.Parameter(torch.tensor(youngs_modulus_log)) # Log of Young's modulus17self.poissons_ratio_sigmoid = nn.Parameter(torch.tensor(poissons_ratio_sigmoid)) # Sigmoid of Poisson's ratio1819def forward(self, F: torch.Tensor) -&gt; torch.Tensor:20"""21Compute Kirchhoff stress tensor from deformation gradient tensor.2223Args:24F (torch.Tensor): Deformation gradient tensor (B, 3, 3).2526Returns:27kirchhoff_stress (torch.Tensor): Kirchhoff stress tensor (B, 3, 3).28"""29# Convert the parameters to their actual values30youngs_modulus = self.youngs_modulus_log.exp() # (1,)31poissons_ratio = torch.sigmoid(self.poissons_ratio_sigmoid) * 0.49 # (1,)3233# Lame parameters34mu = youngs_modulus / (2 * (1 + poissons_ratio)) # Shear modulus (1,)35lam = youngs_modulus * poissons_ratio / ((1 + poissons_ratio) * (1 -2 * poissons_ratio)) # First Lame parameter(1,)3637# Deformation gradient determinant J and its reshape for operations (B,)38J = F.det().view(-1, 1, 1)3940# Inverse transpose of F for stress computation (B, 3, 3)41F_invT = F.inverse().transpose(1, 2)4243# Compute first Piola-Kirchhoff stress tensor P (B, 3, 3)44# Volumetric part45P_vol = lam * (J -1) * F_invT4647# Deviatoric part combining neo-Hookean behavior48# This accounts for the near incompressible nature of the material49P_dev = mu * (F -(1 / J) * F_invT)5051# Compute Kirchhoff stress tensor tau by multiplying the first Piola-Kirchhoff with the transpose of F (B, 3, 3)52kirchhoff_stress = P_vol + P_dev @ F.transpose(1, 2)5354return kirchhoff_stressD.2. Constitutive Law Discovery (b)1 import torch2 import torch.nn as nn34 class Physics(nn.Module):56def <strong>init</strong>(self, gamma: float = -0.07): # Based on best value from iteration 57"""8Initialize gamma as a trainable parameter which will be used for scaling the soft9deformation correction.1011Args:12gamma (float): scaling factor for the deformation correction.13"""14super().<strong>init</strong>()15self.gamma = nn.Parameter(torch.tensor(gamma)) # Initialize gamma, (1,)
 MIT CSAIL<br />
CMU LTI
 MIT BCS<br />
Center for Brains, Minds and Machines
UMass Amherst
MIT-IBM Watson AI Lab. Correspondence to: Pingchuan Ma <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#112;&#99;&#109;&#97;&#64;&#99;&#115;&#97;&#105;&#108;&#46;&#109;&#105;&#116;&#46;&#101;&#100;&#117;">&#112;&#99;&#109;&#97;&#64;&#99;&#115;&#97;&#105;&#108;&#46;&#109;&#105;&#116;&#46;&#101;&#100;&#117;</a>.
Scientific Generative Agent
AcknowledgementsWe would like to thank Bohan Wang, Ziming Liu, Zhuoran Yang, Liane Makatura, Megan Tjandrasuwita, and Michael Sun for the valuable discussion.The mesh "Stanford Bunny" in Figure1is from The Stanford 3D Scanning Repository.This work is supported by MIT-IBM Watson AI Lab.Impact StatementThis paper presents work whose goal is to advance the field of Machine Learning.There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.
The impact of large language models on scientific discovery: a preliminary study using gpt-4. M R References Ai4science, M A Quantum, arXiv:2311.073612023. 2024arXiv preprintAnthropic. Introducing the next generation of claude</p>
<p>Multiple regression genetic programming. I Arnaldo, K Krawiec, O' Reilly, U.-M , Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation. the 2014 Annual Conference on Genetic and Evolutionary Computation2014</p>
<p>Understanding and simplifying one-shot architecture search. G Bender, P.-J Kindermans, B Zoph, V Vasudevan, Q Le, International conference on machine learning. PMLR2018</p>
<p>Neural symbolic regression that scales. L Biggio, T Bendinelli, A Neitz, A Lucchi, G Parascandolo, International Conference on Machine Learning. Pmlr2021</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 62479922023</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>ProxylessNAS: Direct neural architecture search on target task and hardware. H Cai, L Zhu, S Han, T Cai, X Wang, T Ma, X Chen, D Zhou, International Conference on Learning Representations. 2019. 2024International Conference on Learning Representations</p>
<p>Learning concise representations for regression by evolving networks of trees. W L Cava, T R Singh, J Taggart, S Suri, J Moore, International Conference on Learning Representations. 2019</p>
<p>Xgboost: A scalable tree boosting system. T Chen, C Guestrin, Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. the 22nd acm sigkdd international conference on knowledge discovery and data mining2016</p>
<p>Chemberta: large-scale self-supervised pretraining for molecular property prediction. S Chithrananda, G Grand, B Ramsundar, arXiv:2010.098852020arXiv preprint</p>
<p>An overview of bilevel optimization. B Colson, P Marcotte, G Savard, Annals of operations research. 1532007</p>
<p>T Du, K Wu, P Ma, S Wah, A Spielberg, D Rus, W Matusik, Diffpd, Differentiable projective dynamics. 202141</p>
<p>Geometry-enhanced molecular representation learning for property prediction. X Fang, L Liu, J Lei, D He, S Zhang, J Zhou, F Wang, H Wu, H Wang, Nature Machine Intelligence. 422022</p>
<p>. S Fortunato, C T Bergstrom, K Börner, J A Evans, D Helbing, S Milojević, A M Petersen, F Radicchi, R Sinatra, B Uzzi, Science of science. 35963791852018Science</p>
<p>Merck molecular force field. i. basis, form, scope, parameterization, and performance of mmff94. T A Halgren, Journal of computational chemistry. 175-61996</p>
<p>Towards a new evolutionary computation: Advances in the estimation of distribution algorithms. N Hansen, 2006The cma evolution strategy: a comparing review</p>
<p>Q Huang, J Vora, P Liang, J Leskovec, arXiv:2310.03302Benchmarking large language models as ai research agents. 2023arXiv preprint</p>
<p>A Q Jiang, A Sablayrolles, A Roux, A Mensch, B Savary, C Bamford, D S Chaplot, D Casas, E B Hanna, F Bressand, arXiv:2401.04088Mixtral of experts. 2024arXiv preprint</p>
<p>The material point method for simulating continuum materials. C Jiang, C Schroeder, J Teran, A Stomakhin, Selle , A , Acm siggraph 2016 courses. 2016</p>
<p>Junction tree variational autoencoder for molecular graph generation. W Jin, R Barzilay, T Jaakkola, International conference on machine learning. PMLR2018</p>
<p>Y Jin, W Fu, J Kang, J Guo, J Guo, arXiv:1910.08892Bayesian symbolic regression. 2019arXiv preprint</p>
<p>Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems. G Ke, Q Meng, T Finley, T Wang, W Chen, W Ma, Q Ye, T.-Y Liu, 201730</p>
<p>A method for stochastic optimization. D Kingma, J Ba, Adam, International Conference on Learning Representations. San Diega, CA, USA2015</p>
<p>Parameter identification for symbolic regression using nonlinear least squares. M Kommenda, B Burlacu, G Kronberger, M Affenzeller, Genetic Programming and Evolvable Machines. 2132020</p>
<p>Automated scientific discovery: From equation discovery to autonomous discovery systems. S Kramer, M Cerrato, S Džeroski, R King, arXiv:2305.022512023arXiv preprint</p>
<p>A probabilistic and multi-objective analysis of lexicase selection and ε-lexicase selection. La Cava, W Helmuth, T Spector, L Moore, J H , Evolutionary Computation. 2732019</p>
<p>Rdkit: A software suite for cheminformatics, computational chemistry, and predictive modeling. La Cava, W Orzechowski, P Burlacu, B De Franca, F Virgolin, M Jin, Y Kommenda, M Moore, J Landrum, G , Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. the Neural Information Processing Systems Track on Datasets and Benchmarks2021. 2013831Contemporary symbolic regression methods and their relative performance</p>
<p>Empowering molecule discovery for moleculecaption translation with large language models: A chatgpt perspective. J Li, Y Liu, W Fan, X.-Y Wei, H Liu, J Tang, Q Li, arXiv:2306.066152023arXiv preprint</p>
<p>Transformer-based model for symbolic regression via joint supervised learning. W Li, W Li, L Sun, M Wu, L Yu, J Liu, Y Li, S Tian, The Eleventh International Conference on Learning Representations. 2022</p>
<p>DARTS: Differentiable architecture search. H Liu, K Simonyan, Y Yang, International Conference on Learning Representations. 2019</p>
<p>A general descent aggregation framework for gradient-based bi-level optimization. R Liu, P Mu, X Yuan, S Zeng, J Zhang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4512022</p>
<p>Ai-based language models powering drug discovery and development. Z Liu, R A Roberts, M Lal-Nag, X Chen, R Huang, W Tong, Drug Discovery Today. 26112021</p>
<p>Risp: Rendering-invariant state predictor with differentiable simulation and rendering for cross-domain parameter estimation. P Ma, T Du, J B Tenenbaum, W Matusik, C Gan, International Conference on Learning Representations. 2021</p>
<p>Learning neural constitutive laws from motion observations for generalizable pde dynamics. P Ma, P Y Chen, B Deng, J B Tenenbaum, T Du, C Gan, W Matusik, International Conference on Machine Learning. PMLR2023</p>
<p>Human-level reward design via coding large language models. Y J Ma, W Liang, G Wang, D.-A Huang, O Bastani, D Jayaraman, Y Zhu, L Fan, A Anandkumar, Eureka, International Conference on Learning Representations. 2024</p>
<p>Warp: A high-performance python framework for gpu simulation and graphics. M Macklin, NVIDIA GPU Technology Conference. March 2022</p>
<p>Ffx: Fast, scalable, deterministic symbolic regression technology. Genetic Programming Theory and Practice IX. T Mcconaghy, 2011</p>
<p>Symbolic regression via neural-guided genetic programming population seeding. T N Mundhenk, M Landajuela, R Glatt, C P Santiago, D M Faissol, B K Petersen, Advances in Neural Information Processing Systems. 2021</p>
<p>OpenAI: Introducing ChatGPT. 2022OpenAI</p>
<p>Openai, Openai, GPT-4. 2023</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in neural information processing systems. 202235</p>
<p>Pytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, Advances in neural information processing systems. 201932</p>
<p>Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. B K Petersen, M L Larma, T N Mundhenk, C P Santiago, S K Kim, J T Kim, International Conference on Learning Representations. 2020</p>
<p>Quantum chemistry structures and properties of 134 kilo molecules. K Popper, R Ramakrishnan, P O Dral, M Rupp, Von Lilienfeld, O A , Scientific data. 112005. 2014RoutledgeThe logic of scientific discovery</p>
<p>Better informed distance geometry: using what we know to improve conformation generation. S Riniker, G A Landrum, Journal of chemical information and modeling. 55122015</p>
<p>Mathematical discoveries from program search with large language models. B Romera-Paredes, M Barekatain, A Novikov, M Balog, M P Kumar, E Dupont, F J Ruiz, J S Ellenberg, P Wang, O Fawzi, Nature. 2023</p>
<p>Philosophy of science: A contemporary introduction. A Rosenberg, L Mcintyre, 2019Routledge</p>
<p>The boosting approach to machine learning: An overview. Nonlinear estimation and classification. R E Schapire, 2003</p>
<p>Automating drug discovery. G Schneider, Nature reviews drug discovery. 1722018</p>
<p>Chatgpt in drug discovery. G Sharma, A Thakur, 2023</p>
<p>Evolutionary algorithm for bilevel optimization using approximations of the lower level optimal solution mapping. A Sinha, P Malo, Deb , K , European Journal of Operational Research. 25722017a</p>
<p>A review on bilevel optimization: From classical to evolutionary approaches and applications. A Sinha, P Malo, Deb , K , IEEE Transactions on Evolutionary Computation. 2222017b</p>
<p>Application of a particle-in-cell method to solid mechanics. D Sulsky, S.-J Zhou, H L Schreyer, Computer physics communications. 871-21995</p>
<p>Cognitive architectures for language agents. T Sumers, S Yao, K Narasimhan, T Griffiths, Transactions on Machine Learning Research. 2835-88562024Survey Certification</p>
<p>Solving olympiad geometry without human demonstrations. T H Trinh, Y Wu, Q V Le, H He, T Luong, Nature. 62579952024</p>
<p>Ai feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity. S.-M Udrescu, A Tan, J Feng, O Neto, T Wu, M Tegmark, Advances in Neural Information Processing Systems. 202033</p>
<p>M Valipour, B You, M Panju, A Ghodsi, arXiv:2106.14131Symbolicgpt: A generative transformer model for symbolic regression. 2021arXiv preprint</p>
<p>Linear scaling with and within semantic backpropagation-based genetic programming for symbolic regression. M Virgolin, T Alderliesten, P A Bosman, Proceedings of the genetic and evolutionary computation conference. the genetic and evolutionary computation conference2019</p>
<p>Improving model-based genetic programming for symbolic regression of small expressions. M Virgolin, T Alderliesten, C Witteveen, P A Bosman, Evolutionary computation. 2922021</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, T Fu, Y Du, W Gao, K Huang, Z Liu, P Chandak, S Liu, P Van Katwyk, A Deac, Nature. 62079722023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. D Weininger, Journal of chemical information and computer sciences. 2811988</p>
<p>A typology of scientific breakthroughs. M Wuestman, J Hoekman, K Frenken, Quantitative Science Studies. 132020</p>
<p>Rethinking bi-level optimization in neural architecture search: A gibbs sampling perspective. C Xue, X Wang, J Yan, Y Hu, X Yang, K Sun, AAAI Conference on Artificial Intelligence. 202135</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. C Yang, X Wang, Y Lu, H Liu, Q V Le, D Zhou, X Chen, S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K R Narasimhan, International Conference on Learning Representations. 2024. 2023aConference on Neural Information Processing Systems</p>
<p>ReAct: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, International Conference on Learning Representations. 2023b</p>
<p>Population-based de novo molecule generation, using grammatical evolution. N Yoshikawa, K Terayama, M Sumita, T Homma, K Oono, K Tsuda, Chemistry Letters. 47112018</p>
<p>Efficiently programming large language models using sglang. L Zheng, L Yin, Z Xie, J Huang, C Sun, C H Yu, S Cao, C Kozyrakis, I Stoica, J E Gonzalez, arXiv:2312.071042023arXiv preprint</p>
<p>Uni-mol: A universal 3d molecular representation learning framework. G Zhou, Z Gao, Q Ding, H Zheng, H Xu, Z Wei, L Zhang, G Ke, International Conference on Learning Representations. 2023</p>
<p>Optimization of molecules via deep reinforcement learning. Z Zhou, S Kearnes, L Li, R N Zare, P Riley, Scientific reports. 91107522019</p>
<p>Analyze step-by-step what the potential problem is in the previous iterations based on the feedback. Think about why the results from previous constitutive laws mismatched with the ground truth. Do not give advice about how to optimize. Focus on the formulation of the constitutive law. Start this section with "### Analysis. Analyze all iterations individually, and start the subsection for each iteration with "#### Iteration N. where N stands for the index. Remember to analyze every iteration in the history</p>
<p>Think step-by-step what you need to do in this iteration. Think about how to separate your algorithm into a continuous physical parameter part and a symbolic constitutive law part. Describe your plan in pseudo-code, written out in great detail. Remember to update the default values of the trainable physical parameters based on previous optimizations. Start this section with. ### Step-by-Step Plan. </p>
<p>Coding format prompt for plastic constitutive law discovery: ## Format Requirements ### PyTorch Tips 1. When element-wise multiplying two matrix, make sure their number of dimensions match before the operation. For example, when multiplying 'J' (B,) and 'I' (B, 3, 3), you should do 'J.view(-1, 1, 1)' before the operation. Similarly, '(J -1)' should also be reshaped to '(J -1).view(-1, 1, 1)'. If you are not sure. Do not add any trailing comments before or after the code block. write down every component in the expression one by one and annotate its dimension in the comment for verification</p>
<p>When computing the trace of a tensor A (B, 3, 3), use 'A.diagonal(dim1=1, dim2=2).sum(dim=1).view(-1, 1, 1)'. Avoid using 'torch.trace' or 'Tensor.trace' since they only support 2D matrix. ### Code Requirements 1. The programming language is always python. </p>
<p>Annotate the size of the tensor as comment after each tensor operation. For example. 3</p>
<p>The only library allowed is PyTorch. Follow the examples provided by the user and check the PyTorch documentation to learn how to use PyTorch. </p>
<p>Separate the code into continuous physical parameters that can be tuned with differentiable optimization and the symbolic deformation gradient correction model represented by PyTorch code. Define them respectively in the '<strong>init</strong>' function and the 'forward' function. </p>
<p>The proposed code should strictly follow the structure and function signatures below: '''python import torch import torch.nn as nn class Physics(nn.Module): def <strong>init</strong>(self, param: float = DEFAULT_VALUE). Define trainable continuous physical parameters for differentiable optimization. Tentatively initialize the parameters with the default values in args. Args: param (float): the physical meaning of the parameter</p>
<p>Parameter(torch.tensor(param)) def forward(self, F: torch.Tensor) -&gt; torch.Tensor: """ Compute corrected deformation gradient from deformation gradient tensor. Args: F (torch.Tensor): deformation gradient tensor. 3= nn</p>
<p>Returns: F_corrected (torch.Tensor): corrected deformation gradient tensor. 3</p>
<p>Analyze step-by-step what the potential problem is in the previous iterations based on the feedback. Think about why the results from previous constitutive laws mismatched with the ground truth. Do not give advice about how to optimize. Focus on the formulation of the constitutive law. Start this section with "### Analysis. Analyze all iterations individually, and start the subsection for each iteration with "#### Iteration N. where N stands for the index. Remember to analyze every iteration in the history</p>
<p>Remember that plasticity is not necessary. If your analysis supports plasticity, think about how to update deformation gradient using plasticity. Think about how to separate your algorithm into a continuous physical parameter part and a symbolic deformation gradient correction model part. Describe your plan in pseudo-code, written out in great detail. Think step-by-step what you need to do in this iteration. Think about if the plasticity is needed to improve performance. Remember to update the default values of the trainable physical parameters based on previous optimizations. Start this section with "### Step-by-Step Plan</p>
<p>Do not add any trailing comments before or after the code block. Start this section with "### Code. Coding format prompt for molecule design: ## Format Requirements ### Code Requirements 1. Output the code in a single code block. The programming language is always python</p>
<p>Tensor: 18 """ 19 Compute corrected deformation gradient tensor F, by applying a soft correction 20 proportional to the deviation of its determinant from 1, effectively guiding the 21 gradient towards physically realistic states. 17 def forward(self, F: torch.Tensor) -&gt; torch. </p>
<p>23 Args: 24 F (torch.Tensor): deformation gradient tensor. 3</p>
<p>26 Returns: 27 F_corrected (torch.Tensor): corrected deformation gradient tensor. 3</p>
<p>29 # Compute determinant of F and create a condition based on its value. B,)</p>
<p>. J = torch.det(F) #. </p>
<p>32 # Apply a smooth step function as a deviation condition. B,)</p>
<p>. J_deviation_condition = torch.tanh(J -1) # (B. 34</p>
<p>Prepare for correction, taking into account the batch dimension. B,)</p>
<p>36 gamma_correction = self.gamma * J_deviation_condition.view. 371, 1, 1) # (B, 1, 1</p>
<h1>Identity matrix. expanded for batch size (B, 3, 3</h1>
<p>eye(3, device=F.device).repeat(F.size(0), 1, 1) # (B. I = Torch, 340</p>
<h1>Correct F by pulling towards identity matrix when determinant deviates from 1. 3</h1>
<p>return F_corrected D.3. Constitutive Law Discovery (c) The best solution on task (c) optimized by our method: 1 import torch 2 import torch. F_corrected = F -gamma_correction * (F -I) # (B33</p>
<p>The default value for elastic_limit is set to the best from the last iteration, and 5 # we initialize a new parameter for capturing the hardening effect 6. DEFAULT_ELASTIC_LIMIT = 0.92</p>
<p>def <strong>init</strong>(self, elastic_limit: float = DEFAULT_ELASTIC_LIMIT, 12 hardening_factor: float = DEFAULT_HARDENING_FACTOR): 13. DEFAULT_HARDENING_FACTOR = 0.1 8 9 class Physics(nn.Module): 10 1114 Define trainable continuous physical parameters for differentiable optimization</p>
<p>21 self.elastic_limit = nn.Parameter(torch.tensor(elastic_limit)) # () 22 self.hardening_factor = nn.Parameter(torch.tensor(hardening_factor)) # (. Tensor: 25def forward(self, F: torch.Tensor) -&gt; torch. 232416 Args: 17 elastic_limit (float): the parameter determining the initial yield strength. 26 Compute corrected deformation gradient from deformation gradient tensor</p>
<p>28 Args: 29 F (torch.Tensor): deformation gradient tensor. 3</p>
<p>31 Returns: 32 F_corrected (torch.Tensor): corrected deformation gradient tensor. 3</p>
<p>34 # Obtain the polar decomposed rotation (R) and stretch (S). </p>
<p>. U , S , V = Torch, svd(F) # U: (B, 3, 3), S: (B, 3), V: (B, 3, 3</p>
<p>. R = U @ V , 337transpose(-2, -1) # R: (B</p>
<p>Correct the S tensor with hardening 39 # Assuming hardening affects the elastic limit linearly with accumulated plastic strain 40 plastic_strain = torch.relu(S -self.elastic_limit) # Presumed plastic strain 41 hardening_adjustment = 1. 420 + (self.hardening_factor * plastic_strain</p>
<p>S_clamped = torch.min(S, self.elastic_limit * hardening_adjustment) # Clamp S with hardening 44 S_corrected = torch.diag_embed(S_clamped) # S_corrected: (B. 345</p>
<p>3) Corrected deformation gradient tensor 47 48 # Ensure volume preservation. = R @ F_Corrected, # S_Corrected, B , 3</p>
<p>det(F).view(-1, 1, 1) # (B, 1, 1) Determinant of the input F for. J = Torch, J_corrected = torch.det(F_corrected).view(-1, 1, 1) # (B, 1, 1) Determinant of the corrected F. 501/3</p>
<p>. F_Corrected = F_Corrected * Volume_Ratio # (b, 3F_corrected 53 54 return F_corrected D.5. Molecule Design (e) The Top-20 solution on task (e) optimized by our method: 1. C1=CC=C(Br</p>
<p>C1=CC=C(I). </p>
<p>C1=CC=C(C=C1)C2=NC(Cl). C2</p>
<p>C1=CC=C(I)C=C1C2=NC(C(F)(F)F). C2</p>
<p>C1N2C3C4OC(C5). </p>
<p>. C1=cc=c2n=cc=c(br, </p>
<p>C=CC1C(=O)NC(=S). N1C</p>
<p>. C1=cc=cs1c2=nc=cc(cl, C2</p>
<p>C1OC2C(O)C3C(N). </p>
<p>. O=c, 1</p>
<p>C1=CC=C2C(=C1). </p>
<p>C1=CC=C(C=C1). </p>
<p>. C1=cc=c, C2</p>
<p>C1=CC=C2C(=C1)C(=NN2). </p>
<p>C1=CC=C2N=C(C(=O)NC2=C1). </p>
<p>C1=CC=C(C=C1). C2</p>
<p>C1=CSC(=C1)NNC(=O)CF. </p>
<p>C1CSC(Cl)C(N). C1C</p>
<p>. C1csc(c, C1F</p>
<p>C1=CC(=CS1)C2=CC=C(Br)C(F). C2</p>
<p>O=S(=O)(N)C1=CC=C(Br). C1</p>
<p>. C1csc2c3cc(cl, </p>
<p>C1=CC(=O)N(C2=CS1). C2</p>
<p>C1=CC=C(N)C(Cl). C1C</p>
<p>. C1scc2c1c1=c, C=o, </p>
<p>C1=CC2=C(N=C(I)C=C2). C1</p>
<p>. C1=cc=nc2=c1c, =o, C2</p>
<p>Molecule Design (g) The Top-20 solution on task (g) optimized by our method: 1. c1cc(sc1Cl). C1=CC=C2C(=C1)C(=CS2)Cl D.7. C(C(F)(F)F)N</p>
<p>C , C , Cl)Cl)C1=CSC(N. N1</p>
<p>C1OC2C3NOC1C3C2 5. C1C(Cl). </p>
<p>. C#n)s , C1(=CC(=C</p>
<p>. C1coc2c3cc4, 2O1</p>
<p>c1cc(c(c(c1)Cl). </p>
<p>C1CC2NCC(C1). C2O</p>
<p>C1CC2SCC(C1). N2</p>
<p>C1=NC2=CS(=O)(=O). C12</p>
<p>C1OC2C3CC(S). </p>
<p>C1C2C(NC=O)C(Cl). </p>
<p>C1C=CC(O1)(F)N2C=CC(Br). C2C</p>
<p>Molecule Design (h) The Top-20 solution on task (h) optimized by our method: 1. D , CC(NC(=O)C(Cl)C(=O)O)CSC</p>
<p>C1=CC(=CC=C1). </p>
<p>. Fc(f, </p>
<p>. O=c1nc, =o, </p>
<p>C1NOC2C1SC1C2N1Cl 6. C1CSCC(N)C1N=C(O). </p>
<p>C1C2CC(NC1=O)C(Cl). C2I</p>
<p>C1OC2SC3C4OC(F). </p>
<p>SC(Cl)(Cl). </p>
<p>C1=NOC(=C1)C2C(C(=O)NC2=O)Cl. </p>
<p>C1C2C(N(C1Cl)C2=O)F. </p>
<p>OC1C2C(O1)N=CS(=O). C2</p>
<p>. C1csc, 2N1</p>
<p>C1CSCC(N1). </p>
<p>C1CC2(CNC2)C(O1). </p>
<p>O=C1SCCN(C1). </p>
<p>Imaginary Constitutive Law The best solution on the imaginary constitutive law invention task optimized by our method: 1 import torch 2 import torch. C1=CC=NC2=C1C(=O)N=C(S2)Cl D.93</p>
<p>Default values for the physical parameters based on previous iterations 5 DEFAULT_KAPPA. = 0.08</p>
<p>def <strong>init</strong>(self, kappa: float = DEFAULT_KAPPA, mu: float = DEFAULT_MU): 10 """ 11 Initialize the continuous physical parameters kappa and mu for differentiable optimization. 12. DEFAULT_MU = 0.28 7 8 class Physics(nn.Module): 914 self.kappa = nn.Parameter(torch.tensor(kappa)) # Bulk modulus correction factor (scalar) 15 self.mu = nn. Parameter(torch.tensor(mu</p>
<p>Shear modulus correction factor (scalar) 16 17 def forward(self, F: torch.Tensor) -&gt; torch.Tensor: 18 """ 19 Compute the corrected deformation gradient from the deformation gradient tensor. . 20</p>
<p>size(0) # Batch size (scalar). B , F , </p>
<p>eye(3, device=F.device).unsqueeze(0).expand(B, -1, -1) # Identity matrix. I = Torch, 3</p>
<p>det(F).view(-1, 1, 1) # Jacobian determinant. J = Torch, 24B, 1, 1</p>
<p>Volume correction factor (B, 1, 1) 26 vol_correction_factor = torch.clamp(self.kappa * (J -1), min=0.0, max=1.0) 27 vol_correction = vol_correction_factor * I # Volume correction term. 328</p>
<p>Compute trace of F for shape correction (B, 1, 1) 30 trace_F = F.diagonal(dim1=1, dim2=2).sum(dim=1).view(-1, 1, 1) 31 dev_F = F -(trace_F / 3) * I # Deviatoric part of F (B, 3, 3) 32 33 # Shape correction factor (scalar) 34 shape_correction_factor = torch.clamp(self.mu, min=0.0, max=1.0) 35 shape_correction = shape_correction_factor * dev_F # Shape correction term. 336</p>
<p>F_corrected = F -vol_correction -shape_correction # Corrected deformation gradient (B, 3, 3) 38 return F_corrected. </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1662 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1662</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1662</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-227162315</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2011.12421v1.pdf" target="_blank">Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents</a></p>
                <p><strong>Paper Abstract:</strong> Deep reinforcement learning models are notoriously data hungry, yet real-world data is expensive and time consuming to obtain. The solution that many have turned to is to use simulation for training before deploying the robot in a real environment. Simulation offers the ability to train large numbers of robots in parallel, and offers an abundance of data. However, no simulation is perfect, and robots trained solely in simulation fail to generalize to the real-world, resulting in a"sim-vs-real gap". How can we overcome the trade-off between the abundance of less accurate, artificial data from simulators and the scarcity of reliable, real-world data? In this paper, we propose Bi-directional Domain Adaptation (BDA), a novel approach to bridge the sim-vs-real gap in both directions -- real2sim to bridge the visual domain gap, and sim2real to bridge the dynamics domain gap. We demonstrate the benefits of BDA on the task of PointGoal Navigation. BDA with only 5k real-world (state, action, next-state) samples matches the performance of a policy fine-tuned with ~600k samples, resulting in a speed-up of ~120x.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1662.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1662.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BDA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bi-directional Domain Adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-module approach for sim-to-real transfer that (1) adapts real sensory observations to look like simulation (real2sim) and (2) adapts simulator transition dynamics to better match reality (sim2real) so policies can be trained cheaply in an augmented simulator and deployed with an observation adaptor.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>LoCoBot (modeled)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A small differential-drive mobile robot (modeled dimensions: circular base radius 0.175 m, height 0.61 m) equipped with an egocentric RGB-D camera and GPS+Compass for localization; action set = {turn-left 30°, turn-right 30°, forward 0.25 m, STOP}.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>embodied robotic navigation (PointGoal Navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Habitat-Sim (Gibson dataset); Matterport-scanned LAB (virtualized)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Photorealistic 3D simulation environment (Habitat-Sim) using Gibson and Matterport scans; simulates rendering (RGB and depth), discrete motion actions, and supports adding sensor and actuation noise models and physics for navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Photorealistic rendering with approximate dynamics (high visual fidelity; approximate physics/actuation models and configurable noise models).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Photorealistic RGB rendering from Matterport/Gibson scans; depth sensing (clipped to 10 m); configurable RGB and depth sensor noise models; actuation noise modeled via truncated 2D Gaussian translational noise and 1D Gaussian rotational noise; ability to reset simulator states arbitrarily.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Simulator does not model all real-world effects such as wear-and-tear, battery drainage, exact hardware irregularities, some unmodeled contact phenomena; default simulator 'sliding' behavior was disabled to better match reality; localization in sim used GPS+Compass (assumed more accurate than real SLAM in some cases).</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>A physical LoCoBot-like lab (LAB) was virtualized by Matterport 3D scans and used as target testing environment; in experiments a set of target simulators with added observation and dynamics noise stand in for reality (real robot RGB-D images from prior data also used for OA evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>PointGoal Navigation: navigating from randomized starts to a goal specified in relative coordinates using egocentric RGB-D observations and issuing discrete navigation actions (including STOP).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (DD-PPO) to train base navigation policies in simulation; OA trained with unpaired image-to-image translation (CycleGAN); DA trained as a supervised regression (3-layer MLP) predicting residuals between sim and real state transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Success rate (SUCC) and Success weighted by (normalized inverse) Path Length (SPL).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Example: π_100M trained in Gibson no-noise tested in LAB (no-noise) achieved SPL = 0.84 (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Representative degraded performances when testing π_100M Gibson no-noise in noisy targets: LAB with dynamics noise (D) SPL = 0.56; LAB with observation noise (O) SPL = 0.04; LAB with both (O+D) SPL = 0.06. BDA (π_BDA-5k) using 5k target samples matched the performance of an oracle policy fine-tuned in-target (π_1M) within ~5% on average; BDA reported to match oracle SPL while using 117× less target data in one reported comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Visual domain gap (camera noise, textures, lighting), dynamics/actuation mismatch (wheel slip, controller differences), sensor noise (RGB, depth), unmodeled environment/robot effects (wear-and-tear, battery), simulator-specific behaviors (e.g. sliding), and inaccurate or simplified dynamics models.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Training a real2sim observation adaptor (CycleGAN) to map real images to the simulator visual distribution; training a sim2real dynamics adaptor (residual MLP) to predict real transition residuals and augment simulator state during training; collecting a small number (≈1k–5k) of real (state, action, next-state) samples to fit DA and OA; disabling simulator sliding to better match real-world contacts; decoupling sensing (OA) from acting (policy) to avoid retraining policy when sensor changes.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Empirical findings indicate high visual fidelity matters: adding modest observation noise caused SPL to drop dramatically (e.g., 0.84→0.04); dynamics mismatches also degrade performance but less extremely (0.84→0.56). Paper identifies that modeling RGB-D sensor noise and actuation noise (translational/rotational) is important and that small amounts of real data to adapt simulator dynamics and observation distribution suffice (1k–5k samples). No strict numeric thresholds (e.g., percent physics accuracy) are given.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Instead of fine-tuning the policy in reality, the method collects limited target-domain samples (100–5,000 state/action/next-state samples) to train OA (CycleGAN, 200 epochs) and DA (3-layer MLP with weighted MSE) and then fine-tunes the policy in the DA-augmented simulator (Sim_DA) – e.g., π_BDA-5k used 5,000 target samples (estimated 7 hours of real robot data collection).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Comparisons across target noise settings show large sensitivity: policies trained in noise-free sim suffer huge drops when observation noise is added (SPL 0.84→0.04) and moderate drops with dynamics noise (0.84→0.56). BDA reduces required target data dramatically (example speedups: 36×, 117×, average 61× across settings) to reach the same SPL as direct fine-tuning in-target.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Separately adapting observations (real2sim) and dynamics (sim2real) is an effective, sample-efficient strategy for sim-to-real transfer in embodied navigation. 2) Visual (sensor) mismatch is especially damaging to zero-shot transfer; modest observation noise can collapse performance. 3) Using 1k–5k real (state,action,next-state) samples to train OA and DA and then training policies in the DA-augmented simulator matches the performance of direct in-target fine-tuning requiring hundreds of thousands of target samples (empirical speed-ups reported up to ~117×; average ~61×). 4) Decoupling sensing and acting (real2sim for observations) enables reusing policies across sensor changes without retraining the policy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1662.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1662.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OA (real2sim)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Observation Adaptor (real→sim) — CycleGAN-based</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CycleGAN-based real2sim image translation module that maps real RGB-D observations to the simulator image distribution at test time, trained with unpaired images and cycle consistency to close the visual domain gap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>LoCoBot (modeled)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>See BDA entry: mobile robot with RGB-D camera, GPS+Compass, discrete navigation actions for PointGoal Navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>embodied robotic navigation (visual domain adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Habitat-Sim (Gibson/Matterport images used for paired/unpaired datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Photorealistic rendering environment providing simulation images used as the target visual distribution for the adaptor.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>High visual fidelity rendering (images at up to 640×360 used in experiments); image-level realism emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>RGB pixel distributions, depth image appearance/statistics; trained to remove/add visual noise and smooth textures to match simulator distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Does not alter physics/dynamics; translation happens at pixel level and may not preserve exact geometric/sensor calibration effects; relies on unpaired image mapping rather than paired ground-truth correspondences.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real robot RGB-D images collected pre-COVID and simulated noisy images from target simulators used to train/validate OA.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Visual appearance adaptation (so policy trained on simulator images can accept adapted real images at test time).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Unpaired image-to-image translation using CycleGAN with adversarial discriminators and cycle-consistency loss; OA trained on sets of unpaired images collected by a behavior policy in sim and target.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Fréchet Inception Distance (FID) for visual similarity; downstream navigation SPL/SUCC when OA is used with policy in target.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>N/A (OA is not a policy); however image FID between I_real and I_sim was 100.74, and OA(I_real) to I_sim FID reduced to 83.05 (improved).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Image-level differences (lighting, textures, sensor-specific noise patterns, depth artifacts); unpaired data distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Training with unpaired images collected from the same behavior policy in both domains; cycle-consistency to preserve content while altering appearance; training OA in parallel with policy training so sensor changes don’t require policy retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper shows decreasing FID and qualitative improvement in depth smoothing and noise removal improves downstream performance; no strict numeric thresholds beyond FID improvements reported.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>OA trained from datasets of unpaired images collected by the behavior policy; in reported experiments OA trained for 200 epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>OA reduces perceptual visual gap (FID improvement from 100.74 → 83.05 in real→sim mapping); using OA improved downstream navigation relative to raw noisy observations (qualitative and quantitative improvements reported in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A real2sim observation adaptor implemented with CycleGAN materially reduces the visual domain gap (measured by FID) and is a practical way to decouple sensor differences from policy learning, enabling reuse of simulation-trained policies without policy retraining for many sensor changes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1662.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1662.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DA (sim2real)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamics Adaptor (sim→real residual model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural residual dynamics adaptor (3-layer MLP) trained on (state, action, next-state) samples from the target domain to predict residual errors between simulator transitions and real transitions, used to augment simulator rollouts during policy finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>LoCoBot (modeled)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>See BDA entry: mobile robot with discrete navigation actions, used for PointGoal Navigation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>embodied robotic navigation (dynamics adaptation / model-based residual learning)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Habitat-Sim (simulator dynamics augmented by DA)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulator providing nominal transitions; DA predicts residual ∆s_real to correct simulated next-states to more closely match expected real next-states.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Approximate dynamics with neural residual correction (simulator provides base physics; DA models residuals to approximate real dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Translational and rotational residuals between simulated and target transitions; DA learned to predict change in (x, y, θ) (with higher weight on positional errors).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>DA is a learned residual model (MLP) limited to predicting state deltas; may not capture complex contact/coulomb/friction dynamics beyond residual mapping; trained on limited samples so may not generalize to very different conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Target domain rollouts (sim2sim target or real robot data) producing (s_t, a_t, s_{t+1}) used to supervise DA training; in paper these are simulated target environments with noise models and a small set of collected real RGB-D sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Compensating actuation/dynamics mismatch so policies trained in augmented simulator produce trajectories closer to those expected in reality.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised regression (3-layer MLP) trained on collected (state,action,next-state) tuples using weighted MSE (higher weight on positional components than orientation).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Downstream navigation SPL/SUCC when policies are trained in DA-augmented simulator and deployed in target; also training loss (weighted MSE) on residual predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Actuation noise, controller differences, wheel slip, dynamics mismatches, unmodeled contact interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Collecting modest amounts of real (state,action,next-state) data (100–5,000 samples) to learn residuals; using DA to reset simulator states during training to the DA-predicted real next-state so the policy experiences realistic trajectories; weighting positional error higher during DA training.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper shows that learning residual dynamics with as few as 5k samples enables policies to be trained in augmented simulator and match in-target fine-tuning performance (empirical results), indicating that accurate modeling of positional residuals is especially important for navigation transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>DA trained on collected target samples (100–5,000); once trained DA augments simulator and policy is fine-tuned in Sim_DA. DA uses weighted MSE emphasizing position components (twice weight on position vs orientation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>DA-augmented training yields substantial sample-efficiency gains compared to direct policy fine-tuning in-target: e.g., matching target fine-tuning SPL with 5k samples rather than ~585k–1M samples (reported speedups up to 117× in specific comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A learned residual dynamics adaptor can effectively close dynamics mismatch using modest target-domain data; when combined with real2sim observation adaptation and policy training in the augmented simulator, it produces large sample-efficiency gains versus direct in-target fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real transfer with neural-augmented robot simulation <em>(Rating: 2)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 2)</em></li>
                <li>Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks <em>(Rating: 2)</em></li>
                <li>Vr-goggles for robots: Real-to-sim domain adaptation for visual control <em>(Rating: 2)</em></li>
                <li>Sim2real predictivity: Does evaluation in simulation predict real-world performance <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 1)</em></li>
                <li>Residual reinforcement learning for robot control <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1662",
    "paper_id": "paper-227162315",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "BDA",
            "name_full": "Bi-directional Domain Adaptation",
            "brief_description": "A two-module approach for sim-to-real transfer that (1) adapts real sensory observations to look like simulation (real2sim) and (2) adapts simulator transition dynamics to better match reality (sim2real) so policies can be trained cheaply in an augmented simulator and deployed with an observation adaptor.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "LoCoBot (modeled)",
            "agent_system_description": "A small differential-drive mobile robot (modeled dimensions: circular base radius 0.175 m, height 0.61 m) equipped with an egocentric RGB-D camera and GPS+Compass for localization; action set = {turn-left 30°, turn-right 30°, forward 0.25 m, STOP}.",
            "domain": "embodied robotic navigation (PointGoal Navigation)",
            "virtual_environment_name": "Habitat-Sim (Gibson dataset); Matterport-scanned LAB (virtualized)",
            "virtual_environment_description": "Photorealistic 3D simulation environment (Habitat-Sim) using Gibson and Matterport scans; simulates rendering (RGB and depth), discrete motion actions, and supports adding sensor and actuation noise models and physics for navigation.",
            "simulation_fidelity_level": "Photorealistic rendering with approximate dynamics (high visual fidelity; approximate physics/actuation models and configurable noise models).",
            "fidelity_aspects_modeled": "Photorealistic RGB rendering from Matterport/Gibson scans; depth sensing (clipped to 10 m); configurable RGB and depth sensor noise models; actuation noise modeled via truncated 2D Gaussian translational noise and 1D Gaussian rotational noise; ability to reset simulator states arbitrarily.",
            "fidelity_aspects_simplified": "Simulator does not model all real-world effects such as wear-and-tear, battery drainage, exact hardware irregularities, some unmodeled contact phenomena; default simulator 'sliding' behavior was disabled to better match reality; localization in sim used GPS+Compass (assumed more accurate than real SLAM in some cases).",
            "real_environment_description": "A physical LoCoBot-like lab (LAB) was virtualized by Matterport 3D scans and used as target testing environment; in experiments a set of target simulators with added observation and dynamics noise stand in for reality (real robot RGB-D images from prior data also used for OA evaluation).",
            "task_or_skill_transferred": "PointGoal Navigation: navigating from randomized starts to a goal specified in relative coordinates using egocentric RGB-D observations and issuing discrete navigation actions (including STOP).",
            "training_method": "Reinforcement learning (DD-PPO) to train base navigation policies in simulation; OA trained with unpaired image-to-image translation (CycleGAN); DA trained as a supervised regression (3-layer MLP) predicting residuals between sim and real state transitions.",
            "transfer_success_metric": "Success rate (SUCC) and Success weighted by (normalized inverse) Path Length (SPL).",
            "transfer_performance_sim": "Example: π_100M trained in Gibson no-noise tested in LAB (no-noise) achieved SPL = 0.84 (reported).",
            "transfer_performance_real": "Representative degraded performances when testing π_100M Gibson no-noise in noisy targets: LAB with dynamics noise (D) SPL = 0.56; LAB with observation noise (O) SPL = 0.04; LAB with both (O+D) SPL = 0.06. BDA (π_BDA-5k) using 5k target samples matched the performance of an oracle policy fine-tuned in-target (π_1M) within ~5% on average; BDA reported to match oracle SPL while using 117× less target data in one reported comparison.",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Visual domain gap (camera noise, textures, lighting), dynamics/actuation mismatch (wheel slip, controller differences), sensor noise (RGB, depth), unmodeled environment/robot effects (wear-and-tear, battery), simulator-specific behaviors (e.g. sliding), and inaccurate or simplified dynamics models.",
            "transfer_enabling_conditions": "Training a real2sim observation adaptor (CycleGAN) to map real images to the simulator visual distribution; training a sim2real dynamics adaptor (residual MLP) to predict real transition residuals and augment simulator state during training; collecting a small number (≈1k–5k) of real (state, action, next-state) samples to fit DA and OA; disabling simulator sliding to better match real-world contacts; decoupling sensing (OA) from acting (policy) to avoid retraining policy when sensor changes.",
            "fidelity_requirements_identified": "Empirical findings indicate high visual fidelity matters: adding modest observation noise caused SPL to drop dramatically (e.g., 0.84→0.04); dynamics mismatches also degrade performance but less extremely (0.84→0.56). Paper identifies that modeling RGB-D sensor noise and actuation noise (translational/rotational) is important and that small amounts of real data to adapt simulator dynamics and observation distribution suffice (1k–5k samples). No strict numeric thresholds (e.g., percent physics accuracy) are given.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": "Instead of fine-tuning the policy in reality, the method collects limited target-domain samples (100–5,000 state/action/next-state samples) to train OA (CycleGAN, 200 epochs) and DA (3-layer MLP with weighted MSE) and then fine-tunes the policy in the DA-augmented simulator (Sim_DA) – e.g., π_BDA-5k used 5,000 target samples (estimated 7 hours of real robot data collection).",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Comparisons across target noise settings show large sensitivity: policies trained in noise-free sim suffer huge drops when observation noise is added (SPL 0.84→0.04) and moderate drops with dynamics noise (0.84→0.56). BDA reduces required target data dramatically (example speedups: 36×, 117×, average 61× across settings) to reach the same SPL as direct fine-tuning in-target.",
            "key_findings": "1) Separately adapting observations (real2sim) and dynamics (sim2real) is an effective, sample-efficient strategy for sim-to-real transfer in embodied navigation. 2) Visual (sensor) mismatch is especially damaging to zero-shot transfer; modest observation noise can collapse performance. 3) Using 1k–5k real (state,action,next-state) samples to train OA and DA and then training policies in the DA-augmented simulator matches the performance of direct in-target fine-tuning requiring hundreds of thousands of target samples (empirical speed-ups reported up to ~117×; average ~61×). 4) Decoupling sensing and acting (real2sim for observations) enables reusing policies across sensor changes without retraining the policy.",
            "uuid": "e1662.0",
            "source_info": {
                "paper_title": "Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "OA (real2sim)",
            "name_full": "Observation Adaptor (real→sim) — CycleGAN-based",
            "brief_description": "A CycleGAN-based real2sim image translation module that maps real RGB-D observations to the simulator image distribution at test time, trained with unpaired images and cycle consistency to close the visual domain gap.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "LoCoBot (modeled)",
            "agent_system_description": "See BDA entry: mobile robot with RGB-D camera, GPS+Compass, discrete navigation actions for PointGoal Navigation.",
            "domain": "embodied robotic navigation (visual domain adaptation)",
            "virtual_environment_name": "Habitat-Sim (Gibson/Matterport images used for paired/unpaired datasets)",
            "virtual_environment_description": "Photorealistic rendering environment providing simulation images used as the target visual distribution for the adaptor.",
            "simulation_fidelity_level": "High visual fidelity rendering (images at up to 640×360 used in experiments); image-level realism emphasized.",
            "fidelity_aspects_modeled": "RGB pixel distributions, depth image appearance/statistics; trained to remove/add visual noise and smooth textures to match simulator distribution.",
            "fidelity_aspects_simplified": "Does not alter physics/dynamics; translation happens at pixel level and may not preserve exact geometric/sensor calibration effects; relies on unpaired image mapping rather than paired ground-truth correspondences.",
            "real_environment_description": "Real robot RGB-D images collected pre-COVID and simulated noisy images from target simulators used to train/validate OA.",
            "task_or_skill_transferred": "Visual appearance adaptation (so policy trained on simulator images can accept adapted real images at test time).",
            "training_method": "Unpaired image-to-image translation using CycleGAN with adversarial discriminators and cycle-consistency loss; OA trained on sets of unpaired images collected by a behavior policy in sim and target.",
            "transfer_success_metric": "Fréchet Inception Distance (FID) for visual similarity; downstream navigation SPL/SUCC when OA is used with policy in target.",
            "transfer_performance_sim": "N/A (OA is not a policy); however image FID between I_real and I_sim was 100.74, and OA(I_real) to I_sim FID reduced to 83.05 (improved).",
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Image-level differences (lighting, textures, sensor-specific noise patterns, depth artifacts); unpaired data distributions.",
            "transfer_enabling_conditions": "Training with unpaired images collected from the same behavior policy in both domains; cycle-consistency to preserve content while altering appearance; training OA in parallel with policy training so sensor changes don’t require policy retraining.",
            "fidelity_requirements_identified": "Paper shows decreasing FID and qualitative improvement in depth smoothing and noise removal improves downstream performance; no strict numeric thresholds beyond FID improvements reported.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": "OA trained from datasets of unpaired images collected by the behavior policy; in reported experiments OA trained for 200 epochs.",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "OA reduces perceptual visual gap (FID improvement from 100.74 → 83.05 in real→sim mapping); using OA improved downstream navigation relative to raw noisy observations (qualitative and quantitative improvements reported in experiments).",
            "key_findings": "A real2sim observation adaptor implemented with CycleGAN materially reduces the visual domain gap (measured by FID) and is a practical way to decouple sensor differences from policy learning, enabling reuse of simulation-trained policies without policy retraining for many sensor changes.",
            "uuid": "e1662.1",
            "source_info": {
                "paper_title": "Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "DA (sim2real)",
            "name_full": "Dynamics Adaptor (sim→real residual model)",
            "brief_description": "A neural residual dynamics adaptor (3-layer MLP) trained on (state, action, next-state) samples from the target domain to predict residual errors between simulator transitions and real transitions, used to augment simulator rollouts during policy finetuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "LoCoBot (modeled)",
            "agent_system_description": "See BDA entry: mobile robot with discrete navigation actions, used for PointGoal Navigation experiments.",
            "domain": "embodied robotic navigation (dynamics adaptation / model-based residual learning)",
            "virtual_environment_name": "Habitat-Sim (simulator dynamics augmented by DA)",
            "virtual_environment_description": "Simulator providing nominal transitions; DA predicts residual ∆s_real to correct simulated next-states to more closely match expected real next-states.",
            "simulation_fidelity_level": "Approximate dynamics with neural residual correction (simulator provides base physics; DA models residuals to approximate real dynamics).",
            "fidelity_aspects_modeled": "Translational and rotational residuals between simulated and target transitions; DA learned to predict change in (x, y, θ) (with higher weight on positional errors).",
            "fidelity_aspects_simplified": "DA is a learned residual model (MLP) limited to predicting state deltas; may not capture complex contact/coulomb/friction dynamics beyond residual mapping; trained on limited samples so may not generalize to very different conditions.",
            "real_environment_description": "Target domain rollouts (sim2sim target or real robot data) producing (s_t, a_t, s_{t+1}) used to supervise DA training; in paper these are simulated target environments with noise models and a small set of collected real RGB-D sequences.",
            "task_or_skill_transferred": "Compensating actuation/dynamics mismatch so policies trained in augmented simulator produce trajectories closer to those expected in reality.",
            "training_method": "Supervised regression (3-layer MLP) trained on collected (state,action,next-state) tuples using weighted MSE (higher weight on positional components than orientation).",
            "transfer_success_metric": "Downstream navigation SPL/SUCC when policies are trained in DA-augmented simulator and deployed in target; also training loss (weighted MSE) on residual predictions.",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Actuation noise, controller differences, wheel slip, dynamics mismatches, unmodeled contact interactions.",
            "transfer_enabling_conditions": "Collecting modest amounts of real (state,action,next-state) data (100–5,000 samples) to learn residuals; using DA to reset simulator states during training to the DA-predicted real next-state so the policy experiences realistic trajectories; weighting positional error higher during DA training.",
            "fidelity_requirements_identified": "Paper shows that learning residual dynamics with as few as 5k samples enables policies to be trained in augmented simulator and match in-target fine-tuning performance (empirical results), indicating that accurate modeling of positional residuals is especially important for navigation transfer.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": "DA trained on collected target samples (100–5,000); once trained DA augments simulator and policy is fine-tuned in Sim_DA. DA uses weighted MSE emphasizing position components (twice weight on position vs orientation).",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "DA-augmented training yields substantial sample-efficiency gains compared to direct policy fine-tuning in-target: e.g., matching target fine-tuning SPL with 5k samples rather than ~585k–1M samples (reported speedups up to 117× in specific comparisons).",
            "key_findings": "A learned residual dynamics adaptor can effectively close dynamics mismatch using modest target-domain data; when combined with real2sim observation adaptation and policy training in the augmented simulator, it produces large sample-efficiency gains versus direct in-target fine-tuning.",
            "uuid": "e1662.2",
            "source_info": {
                "paper_title": "Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents",
                "publication_date_yy_mm": "2020-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real transfer with neural-augmented robot simulation",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_with_neuralaugmented_robot_simulation"
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 2,
            "sanitized_title": "closing_the_simtoreal_loop_adapting_simulation_randomization_with_real_world_experience"
        },
        {
            "paper_title": "Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks",
            "rating": 2,
            "sanitized_title": "simtoreal_via_simtosim_dataefficient_robotic_grasping_via_randomizedtocanonical_adaptation_networks"
        },
        {
            "paper_title": "Vr-goggles for robots: Real-to-sim domain adaptation for visual control",
            "rating": 2,
            "sanitized_title": "vrgoggles_for_robots_realtosim_domain_adaptation_for_visual_control"
        },
        {
            "paper_title": "Sim2real predictivity: Does evaluation in simulation predict real-world performance",
            "rating": 2,
            "sanitized_title": "sim2real_predictivity_does_evaluation_in_simulation_predict_realworld_performance"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 1,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Residual reinforcement learning for robot control",
            "rating": 1,
            "sanitized_title": "residual_reinforcement_learning_for_robot_control"
        }
    ],
    "cost": 0.014043249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents</p>
<p>Joanne Truong 
Sonia Chernova 
Dhruv Batra 
Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents</p>
<p>Deep reinforcement learning models are notoriously data hungry, yet real-world data is expensive and time consuming to obtain. The solution that many have turned to is to use simulation for training before deploying the robot in a real environment. Simulation offers the ability to train large numbers of robots in parallel, and offers an abundance of data. However, no simulation is perfect, and robots trained solely in simulation fail to generalize to the real-world, resulting in a "sim-vs-real gap". How can we overcome the tradeoff between the abundance of less accurate, artificial data from simulators and the scarcity of reliable, real-world data? In this paper, we propose Bi-directional Domain Adaptation (BDA), a novel approach to bridge the sim-vs-real gap in both directions-real2sim to bridge the visual domain gap, and sim2real to bridge the dynamics domain gap. We demonstrate the benefits of BDA on the task of PointGoal Navigation. BDA with only 5k real-world (state, action, next-state) samples matches the performance of a policy fine-tuned with ∼600k samples, resulting in a speed-up of ∼120×.</p>
<p>I. INTRODUCTION</p>
<p>Deep reinforcement learning (RL) methods have made tremendous progress in many high-dimensional tasks, such as navigation [23], manipulation [4], and locomotion [9]. Since RL algorithms are data hungry, and training robots in the real-world is slow, expensive, and difficult to reproduce, these methods are typically trained in simulation (where gathering experience is scalable, safe, cheap, and reproducible) before being deployed in the real-world.</p>
<p>However, no simulator perfectly replicates reality. Simulators fail to model many aspects of the robot and the environment (noisy dynamics, sensor noise, wear and-tear, battery drainage, etc.). In addition, RL algorithms are prone to overfitting -i.e., they learn to achieve strong performance in the environments they were trained in, but fail to generalize to novel environments. On the other hand, humans are able to quickly adapt to small changes in their environment. The ability to quickly adapt and transfer skills is a key aspect of intelligence that we hope to reproduce in artificial agents.</p>
<p>This raises a fundamental question -How can we leverage imperfect but useful simulators to train robots while ensuring that the learned skills generalize to reality? This question is studied under the umbrella of 'sim2real transfer' and has been a topic of much interest in the community [5], [8], [11], [16], [22], [25], [26].</p>
<p>In this work, we first reframe the sim2real transfer problem into the following question -given a cheap abundant lowfidelity data generator (a simulator) and an expensive scarce 1 Georgia Institute of Technology, {truong.j, chernova, dbatra}@gatech.edu 2 Facebook AI Research high-fidelity data source (reality), how should we best leverage the two to maximize performance of an agent in the expensive domain (reality)? The status quo in machine learning is to pre-train a policy in simulation using large amounts of simulation data (potentially with domain randomization [22]) and then fine-tune this policy on the robot using the small amount of real data. Can we do better?</p>
<p>We contend that the small amount of expensive, highfidelity data from reality is better utilized to adapt the simulator (and reduce the sim-vs-real gap) than to directly adapt the policy. Concretely, we propose Bi-directional Domain Adaptation (BDA) between simulation and reality to answer this question. BDA reduces the sim-vs-real gap in two different directions (shown in Fig. 1).</p>
<p>First, for sensory observations (e.g. an RGB-D camera image I) we train a real2sim observation adaptation module OA : I real → I sim . This can be thought of as 'goggles' [26], [24] that the agent puts on at deployment time to make real observations 'look' like the ones seen during training in simulation. At first glance, this choice may appear counterintuitive (or the 'wrong' direction). We choose real2sim observation adaption instead of sim2real because this decouples sensing and acting. If the sensor characteristics in reality change but the dynamics remain the same (e.g. same robot different camera), the policy does not need to be retrained, but only equipped with a re-trained observation adaptor. In contrast, changing a sim2real observation adaptor results in the generated observations being out of distribution for the policy, requiring expensive re-training of the policy. Our real2sim observation adaptor is based on CycleGAN [27], and thus does not require any sort of alignment or pairing between sim and real observations, which can be prohibitive.</p>
<p>Second, for transition dynamics T : P r(s t+1 | s t , a t ) (the probably of transitioning from state s t to s t+1 upon taking action a t ), we train a sim2real dynamics adaptation module DA : T sim → T real . This can be thought of as a neural-augmented simulator [8] or a specific kind of boosted ensembling method [19] -where a simulator first makes predictions about state transitions and then a learned neural network predicts the residual between the simulator predictions and the state transitions observed in reality. At each time t during training in simulation, DA resets the simulator state from s sim t+1 (where the simulator believes the agent should reach at time t+1) toŝ real t+1 (where DA predicts the agent will reach in reality), thus exposing the policy to trajectories expected in reality. We choose sim2real dynamics adaptation instead of real2sim because this nicely exploits the fundamental asymmetry between the two domains - We learn a sim2real dynamics adaptation module to predict residual errors between state transitions in simulation and reality. Right: We learn a real2sim observation adaptation module to translate images the robot sees in the real-world at test time to images that more closely align with what the robot has seen in simulation during training. (b) Using BDA, we achieve the same SPL as a policy finetuned directly in reality while using 117× less real-world data.</p>
<p>simulators can (typically) be reset to arbitrary states, reality (typically) cannot. Once an agent acts in the real-world, it doesn't matter what corresponding state it would have reached in simulator, reality cannot be reset to it.</p>
<p>Once the two modules are trained, BDA trains a policy in a simulator augmented with the dynamics adaptor (DA) and deploys the policy augmented with the observation adaptor (OA) to reality. This process is illustrated in Fig. 1a, left showing policy training in simulation and right showing its deployment in reality.</p>
<p>We instantiate and demonstrate the benefits of BDA on the task of PointGoal Navigation (PointNav) [3], which involves an agent navigating in a previously unseen environment from a randomized initial starting location to a goal location specified in relative coordinates. For controlled experimentation, and due to COVID-19 restrictions, we use Sim2Sim transfer of PointNav policies as a stand-in for Sim2Real transfer. We conduct experiments in photo-realistic 3D simulation environments using Habitat-Sim [18], which prior work [13] has found to have high sim2real predictivity, meaning that inferences drawn in simulation experiments have a high likelihood of holding in reality on Locobot mobile robot [2].</p>
<p>In our experiments, we find that BDA is significantly more sample-efficient than the baseline of fine-tuning a policy. Specifically, BDA trained on as few as 5,000 samples (state, action, next-state) from reality (equivalent of 7 hours to collect data in reality) is able to match the performance of baseline trained on 585,000 samples from reality (equivalent of 836 hours to collect data in reality, or 3.5 months at 8 working hours per day), a speed-up of 117× (Fig. 1b).</p>
<p>While our experiments are conducted on the PointNav task, we believe our findings, and the core idea of Bidirectional Domain Adaptation, is broadly applicable to a number of problems in robotics and reinforcement learning.</p>
<p>II. BI-DIRECTIONAL DOMAIN ADAPTATION (BDA)</p>
<p>We now describe the two key components of Bi-directional Domain Adaptation (BDA) in detail -(1) real2sim observation adaptation module OA to close the visual domain gap, and (2) sim2real dynamics adaptation module DA to close the dynamics domain gap.</p>
<p>Preliminaries and Notation. We formulate our problem by representing both the source and target domain as a Markov Decision Process (MDP). A MDP is defined by the tuple (S, A, T , R, γ), where s ∈ S denotes states, a ∈ A denotes actions, T (s, a, s ) = P r(s | s, a) is the transition probability, R : S × A → R is the reward function, and γ is a discount factor. In RL, the goal is to learn a policy π : S → A to maximize expected reward.</p>
<p>A. System Architecture</p>
<p>Algorithm 1: Bi-directional Domain Adaptation 1 Train behavior policy π sim in Sim 2 for t = 0, ..., N steps do 3 Collect I sim t ∼ Sim rollout (π sim ) 4 Collect I real t , s real t , a real t ∼ Real rollout (π sim )
5 Train OA ({I sim N i=1 }, {I real N i=1 }) 6 Train DA ({s real N i=1 }, {a real N i=1 }) 7
Sim DA ← Augment Source with DA 8 for j = 0, ..., K steps do 9 π Sim DA ← Finetune π sim in Sim DA 10 π Sim OA+DA ← Apply OA at test-time 11 Test π Sim OA+DA in Real Observation Adaptation. We consider a real2sim domain adaptation approach to deal with the visual domain gap.</p>
<p>We leverage CycleGAN [27], a pixel-level image-toimage translation technique that uses a cycle-consistency loss function with unpaired images. We start by using a behavior policy π sim trained in simulation to sample rollouts in simulation and reality to collect RGB-D images I sim
t and I real t at time t (line 3). The dataset of N unpaired images {I sim N i=1 } and {I real N i=1 } is used to train OA, to translate {I sim N i=1 } → {I real N i=1 } (line 5).
OA learns a mapping G sim : I real → I sim , an inverse mapping G real : I sim → I real , and adversarial discriminators D real , D sim . Although our method focuses on adaptation from real2sim, learning both mappings encourages the generative models to remain cycle-consistent, i.e., forward cycle: I real → G sim (I real ) → G real (G sim (I real )) ≈ I real and backwards cycle: I sim → G real (I sim ) → G sim (G real (I sim )) ≈ I real . The ability to learn mappings from unpaired images from both domains is important because it is difficult to accurately collect paired images between simulation and reality.</p>
<p>A real2sim approach for adapting the visual domain offers many advantages over a sim2real approach because it disentangles the sensor adaptation module from our policy training. This enables us to remove an additional bottleneck during the RL policy training process; we can train OA in parallel with the RL policy, thus reducing the overall training time needed. In addition, if the sensor observation noise in the environment changes, the base policy can be kept frozen, and only OA will have to be retrained. Dynamics Adaptation. To close the dynamics domain gap, we follow a sim2real approach.</p>
<p>Starting with the behavior policy π sim , we collect stateaction pairs (s real t , a real t ) in the real-world (line 4). The stateaction pairs are used to train DA, a 3 layer multilayer perceptron regression network, that learns the residual error between the state transitions in simulation and reality T sim → T real (line 7). Specifically, DA learns to estimate the change in position and orientation ∆s real : (s real t+1 − s real t ). We use a weighted MSE loss function, 1
n N n=1 w (∆s real n − ∆ŝ real n ) 2 . For our experiments, the state s real t = (x real t , y real t , θ real t )
, is represented by the position and orientation of the robot at timestep t. We placed twice as much weight on the prediction terms for the robot's position than for its orientation because getting the position correct is more important for our performance metric. Once trained, DA is used to augment the source environment (line 7). We finetune π sim in the augmented simulator, Sim DA (lines 8-9). Our hypothesis (which we validate in our experiments) is using real-world data to adapt the simulator via our DA model pays off because we can then train RL policies in this DA-augmented simulator for large amounts of experience cheaply. We use OA at test time (line 10). Finally, we test our policy trained with BDA in the real-world (line 11).</p>
<p>To recap, BDA has a number of advantages over the status quo (of directly using real data to fine-tune a simulation trained policy) that we demonstrate in our experiments: (1) Decouples sensing and acting, (2) Does not require paired training data, (3) The data to train both modules can be collected jointly (by gathering experience from a behavior policy in reality) but the two can be trained in parallel independently of each other, (4) Similar to model-based RL [21], reducing the sim-vs-real gap is made significantly more sample-efficient than directly fine-tuning the policy.</p>
<p>III. EXPERIMENTAL SETUP: SIM2SIM TRANSFER FOR POINT-GOAL NAVIGATION</p>
<p>Our goal in this work is to enable sample efficient Sim2Real transfer for the task of PointGoal Navigation (PointNav) [3]. However, for controlled experiments and due to COVID-19 restrictions, we study Sim2Sim transfer as a stand-in for Sim2Real. Specifically, we train policies in a "source" simulator (which serves as 'Sim' in 'Sim2Real') and transfer it to a "target" simulator (which serves as 'Real' in 'Sim2Real'). We add observation and dynamics noise to the target simulator to mimic the noise observed in reality. Notice that these noise models are purely for the purpose of conducting controlled experiments and are not avaliable to the agent (which must adapt and learn from samples of state and observations). Since no noise model is perfect (just like no simulator is perfect), we experiment with a range of noise models and report results with multiple target simulators. Our results show consistent improvements regardless of the noise model used, thus providing increased confidence in our experimental setup. For clarity, in the text below we present our approach from the perspective of "transfer from a source to target domain," with the assumption that obtaining data in the target domain is always expensive, regardless of whether it is a simulated or real-world environment. All of our experiments are conducted in Habitat [18].</p>
<p>A. Task: PointGoal Navigation</p>
<p>In PointNav, a robot is initialized in an unseen environment and asked to navigate to a goal location specified in relative coordinates purely from egocentric RGB-D observations without a map, in a limited time budget. An episode is considered successful if the robot issues the STOP command within 0.2m of the goal location. In order to increase confidence that our simulation settings will translate to the real-world, we limit episodes to 200 steps, limit number of collisions allowed (before deeming the episode a failure) to 40, and turn sliding off-specifications found by [13] to have high sim2real predictivity (how well evaluation in simulation predicts real-world performance). Sliding is a behavior enabled by default in many physics simulators that allows agents to slide along obstacles when the agent takes an action that would result in a collision. Turning sliding off ensures that the agent cannot cheat in simulation by sliding along obstacles. We use success rate (SUCC), and Success weighted by (normalized inverse) Path Length (SPL) [3] as metrics for evaluation.</p>
<p>B. Robot in Simulation</p>
<p>Body. The robot has a circular base with a radius of 0.175m and a height of 0.61m. These dimensions correspond to the base width and camera height of the LoCoBot robot [2]. Sensors. The robot has access to an egocentric RGB and Depth sensor, and accurate localization and heading through a GPS+Compass sensor. real-world robot experiments from [13] used Hector SLAM [14] with a Hokuyo UTM-30LX LIDAR sensor and found that localization errors were approximately 7cm (much lower than the 20cm PointNav success criterion). This gives us confidence that our results will generalize to reality, despite the lack of precise localization. We match the specifications of the Intel D435 camera on the LoCoBot, and set the camera field of view to 70. To match the maximum range on the depth camera, we clip the simulated depth readings to 10m. Sensor Noise. To simulate noisy sensor observations of the real-world, we add RGB and Depth sensor noise models to  [7]. Fig. 2 shows a comparison between noise free RGB-D images and RGB-D images with the different noise models and multipliers we use.</p>
<p>Actions. The action space for the robot is turn-left 30 • turn-right 30 • , forward 0.25m, and STOP. In the source simulator, these actions are executed deterministically and accurately. However, actions in the real-world are never deterministic -identical actions can lead to vastly different final locations due to the actuation noise (wheel slippage, battery power drainage, etc.) typically found on a real robot. To simulate the noisy actuation that occurs in the real-world, we leverage the real-world translational and rotational actuation noise models characterized by [15]. A Vicon motion capture was used to measure the difference in commanded state and achieved state on LoCoBot for 3 different positional controllers: Proportional Controller, Dynamic Window Approach Controller from Movebase, and Linear Quadratic Regulator (ILQR). These are controllers typically used on a mobile robot. From a state (x, y, θ) and given a particular action, we add translational noise sampled from a truncated 2D Gaussian, and rotational noise from a 1D Gaussian to calculate the next state.</p>
<p>C. Testing Environment</p>
<p>We virtualize a 6.5m by 10m real lab environment (LAB) to use as our testing environment, using a Matterport Pro2 3D camera. To model the space, we placed the Matterport camera at various locations in the room, and collected 360 • scans of the environment. We used the scans to create 3D meshes of the environment, and directly imported the 3D meshes into Habitat to create a photorealistic replica of LAB Fig. 3b. We vary the number of obstacles in LAB to create 3 room configurations with varying levels of difficulty. Fig. 3 shows one of our room configurations with 5 obstacles. We perform testing over the 3 different room configurations, each with 5 start and end waypoints for navigation episodes, and 10 independent trials, for a total of 150 runs. We report the average success rate and SPL over the 150 runs. Our models were trained entirely in the Gibson dataset [24], and have never seen LAB during training. The Gibson dataset contains 3D models of 572 cluttered indoor environments (homes, hospitals, offices, museums, etc.). In this work, we used the 72 Gibson environments that were rated 4+ in quality in [18].</p>
<p>D. Experimental Protocol</p>
<p>Recall that our objective is to improve the ability for RL agents to generalize to new environments using little real-world data. To do this, we define our source environment as Gibson without any sensor or actuation noise (Gibson no noise ). We create 10 target environments with noise settings described in Table I. We use the notation O to represent an environment afflicted with only RGBD observation noise (rows 2, 5, or 8), D to represent an environment afflicted with only dynamics noise (rows 3, 6, or 9), and O + D to represent an environment afflicted with RGBD observation noise and dynamics noise (rows 4, 7, or 10).</p>
<p>E. RL Navigation Models</p>
<p>We train learning-based navigation policies, π, for Point-Goal in Habitat using environments from the Gibson dataset. Policies were trained from scratch with reinforcement learning using DD-PPO [23], a decentralized, distributed variant of the proximal policy optimization (PPO) algorithm, that allows for large-scale training in GPU-intensive simulation environments. We follow the navigation policy described in [23], which is composed of a ResNet50 visual encoder, and a 2-layer LSTM. Each policy is trained using 64 Tesla V100s. Base policies are trained for 100 million steps (π 100M ) to ensure convergence.</p>
<p>IV. EXPERIMENTS</p>
<p>Our experiments aim to answer the following: (1) How large is the sim2real gap? (2) Does our method improve generalization to target domains? (3) How does our method compare to directly training (or fine-tuning) in the target environment? (4) How much real-world data do we need?</p>
<p>How large is the sim2real gap? First, we show that RL policies fail to generalize to new environments. We train a policy without any noise (π 100M Gibson no noise ), and a policy with observation and dynamics noise (π 100M Gibson O+D ). We test these policies in LAB with 4 different noise settings: LAB no noise , LAB O , LAB D , LAB O+D , and average across the noise settings. For each noise setting, we conduct 3 sets of runs, each containing 150 episodes in the target environments. We see that π 100M Gibson no noise tested in LAB no noise exhibits good transfer across environments -0.84 SPL (in contrast, the Habitat 2019 challenge winner was at 0.95 SPL [1]). [23] showed that near-perfect performance is possible when the policy is trained out for significantly longer (2.5B frames), but for the sake of multiple experiments, we limit our analysis to 100M frames of training and compare all models across the same number.</p>
<p>From Fig. 4, we see that when dynamics noise is introduced (π 100M Gibson no noise tested in LAB D ), SPL drops from 0.84 to 0.56 (relative drop of 28%). More significantly, when sensor noise is introduced (π 100M Gibson no noise tested in LAB O ), SPL drops to 0.04 (relative drop of 81%), and when both sensor and dynamics noise are present, (π 100M Gibson no noise tested in LAB O+D ), SPL drops to 0.06 (relative drop of 78%). Thus, in the absence of noise, generalization across scenes (Gibson to LAB) is good, but in the presence of noise, the generalization suffers. We also notice that the converse is true: policies trained from scratch in Gibson O+D environments fail to generalize to LAB no noise and LAB D environments. Gibson no noise and π 100M Gibson O+D tested in LAB with different combinations of observation and dynamics noise. We see that SPL drops when a policy is tested in an environment with noise different from what it was trained in.</p>
<p>These results show us that RL agents are highly sensitive to what might be considered perceptually minor changes to visual inputs. To the best of our knowledge, no prior work in embodied navigation appears to have considered this question of sensitivity to noise; hopefully our results will encourage others to consider this as well.</p>
<p>How well does OA do? Following Alg. 1 described in Sec. II-A, we train OA from scratch for 200 epochs. In Fig. 5, we see that the model learns to remove the Gaussian noise placed on the RGB image, and learns to smooth out textures in the depth image.
I no noise I O+D OA (I O+D ) RGB Depth (a) (b) (c)
In addition, we have RGB-D images of LAB collected from a real robot, pre-COVID, and results using our real2sim OA module. While no GAN metric is perfect (user studies are typically conducted for evaluation as done in [27]), we calculated the Fréchet Inception Distance (FID) [10] score (lower is better) to provide quantitative results. We find that the FID comparing I real and I sim is 100.74, and the FID from OA (I real ) to I sim images is 83.05. We also calculated FID comparing simulation images afflicted with Gaussian noise, I Gaussian , to noise-free simulation images I no noise to be 98.73, and FID between OA (I Gaussian ) to I no noise images to be 88.44. To put things in context, the FID score comparing images from CIFAR10 to our simulation images is 317.61. This shows that perceptually, the distribution of our adapted images more closely resembles images taken directly from simulation, and that real2sim OA is not far off from our sim2sim OA experiments. While our architecture has changed since this initial data collection (initial images are 256 × 256, compared to our current architecture which uses 640 × 360 images), these results will serve as a good indication that our approach will generalize to reality.</p>
<p>How does our method compare to fine-tuning? Next, we evaluate our policy finetuned using BDA with 5,000 data samples collected in the target environment (π BDA−5k Gibson OA+DA ). We compare this to directly finetuning in the target environment (π 1M Gibson O+D ), which serves as an oracle baseline. Both π BDA−5k Gibson OA+DA and π 1M Gibson O+D are initialized with π 100M Gibson no noise , and both are re-trained for each target O + D setting.</p>
<p>Our results in Table II show the benefit in finetuning using data from target environments. π 1M Gibson O+D demonstrates robustness in all combinations of sensor and actuation noise. We also observe that using BDA to learn the observation and dynamics noise models with 5,000 samples from the target environment is capable of nearly matching performance of π 1M</p>
<p>Gibson O+D . In fact, we only see, on average, a 5% difference between π 1M</p>
<p>Gibson O+D and π BDA−5k Gibson OA+DA (rows 4,8,12), while the former is directly trained in the target environment which is not possible in reality, as it requires 1M samples from the target environment.</p>
<p>From these results, we notice in certain environments our method performs worse than the oracle baseline if no or only observation noise is present (rows 1, 5, 6, 10), but performs on the level of the oracle baseline when dynamics Gibson no noise is a policy trained solely in simulation. π BDA−5k Gibson OA+DA and π 1M Gibson O+D are initialized with π 100M Gibson no noise . π BDA−5k Gibson OA+DA is fine-tuned with BDA using 5k samples from the target environment. π 1M</p>
<p>Gibson O+D is fine-tuned directly in the target environment for 1M steps of experience, and serves as an oracle baseline. While π 1M</p>
<p>Gibson O+D and π BDA−5k Gibson OA+DA achieve in strong performance across environments with varying noises (rows 4,8,12) is added (rows 3,4,7,8,11,12). We believe it's due to 'sliding', a default behavior in 3D simulators allowing agents to slide along obstacles rather than stopping immediately on contact. Following the findings and recommendations of [13], we disabled sliding to make our simulation results more predictive of real-world experiments. We find that one common failure mode in the absence of sliding is that agents get stuck on obstacles. In the presence of dynamics noise, the slight amount of actuation noise allows the agent to free itself from obstacles, similar to how it would in reality. Without dynamics noise, the agents continue to stay stuck.</p>
<p>Sample Efficiency. We repeat our experiments, varying the amounts of data collected from the target environment. We re-train OA and DA using 100, 250, 500, 1,000, and 5,000 steps of experience in the target environment, and re-evaluate performance. We compare this to directly finetuning in the target environment for varying amounts of data. In Fig. 7, the x-axis represents the number of samples collected in the target environment. From previous experiments, we estimate 1 episode in the real-world to last on average 6 minutes, in which the robot will take approximately 70 steps to reach the goal. We use this as a conversion factor, and add an additional x-axis to show the number of hours needed for collecting the required samples from the target environment. The y-axis shows the SPL in the target environment. We see that the majority of our success comes from our first 1,000 samples from the target environment, and after 5,000 samples, π BDA−5k Gibson OA+DA is able to match the performance from π 1M Gibson O+D . Collecting 5,000 samples of data from a target environment to train our method would have taken 7 hours. In comparison, Fig. 7b shows that we would have to finetune the base policy for approximately 585,000 steps in the target environment (836 hours to collect data from target environment) to reach the same SPL. Comparing the amount of data needed to reach the same SPL, we see that BDA reduces the amount of data needed from the target environment by 36× in Fig. 7a, 117×  in Fig. 7c, for an average speed up of 61×. These results give us confidence in the importance of our approach, as we wish to limit the amount of data needed from a target environment (i.e. real-world).</p>
<p>V. RELATED WORK</p>
<p>Bi-directional Domain Adaptation is related to literature on domain and dynamics randomization, domain adaptation, and residual policy learning.</p>
<p>Domain and Dynamics Randomization. Borrowing ideas from data augmentation commonly used in computer vision, domain randomization is a technique to train robust policies by exposing the agent to a wide variety of simulation environments with randomized visual properties such as lighting, texture, camera position, etc. Similarly, dynamics randomization is a process that randomizes physical properties in the simulator such as friction, mass, damping, etc. [17] applied randomization to textures to learn real indoor flight by training solely in simulation. [6] used real-world roll outs to learn a distribution of simulation dynamics parameters to randomize over. [4] randomized both physical and visual parameters to train a robotic hand to perform in hand manipulation. However, finding the right distribution to randomize parameters over is difficult, and may require expert knowledge. If the distribution chosen to randomize parameters over is too large, the task becomes much harder for the policy to learn. On the other hand, if the distribution is too small, then the reality gap remains large, and the policy will fail to generalize.</p>
<p>Domain Adaptation. To bridge the simulation to reality gap, many works have used domain adaptation, a technique in which data from a source domain is adapted to more closely resemble data from a target domain. Prior works have used domain adaptation techniques for adapting visionbased models to translate images from sim-to-real during training for manipulation tasks [5], [11], and real-to-sim during testing for navigation tasks [26]. Other works have focused on adapting policies for dynamic changes [8], [25]. In our work, we seek to use domain adaptation to close the gap for both the visual and the dynamics domain.</p>
<p>Residual Policy</p>
<p>Learning. An alternative to typical transfer learning techniques is to directly improve the underlying policy itself. Instead of re-training an agent from scratch when policies perform sub-optimally, the sub-optimal policy can be used as prior knowledge in RL to speed up training. This is the main idea behind residual policy learning, in which a residual policy is used to augment an initial policy to correct for changes in the environment. [12], [20] demonstrated that combining residual policy learning with conventional robotic control improves the robot's ability to adapt to variations in the environment for manipulation tasks. Our method builds on this line of research by augmenting the simulator using a neural network that learns the residual error between simulation and reality.</p>
<p>VI. CONCLUSION</p>
<p>We introduce Bi-directional Domain Adaptation (BDA), a method to utilize the differences between simulation and reality to accelerate learning and improve generalization of RL policies. We use domain adaptation techniques to transfer images from real2sim to close the visual domain gap, and learn the residual error in dynamics from sim2real to close the dynamics domain gap. We find that our method consistently improves performance of the initial policy π while remaining sample efficient.</p>
<p>Fig. 1: (a) Left: We learn a sim2real dynamics adaptation module to predict residual errors between state transitions in simulation and reality. Right: We learn a real2sim observation adaptation module to translate images the robot sees in the real-world at test time to images that more closely align with what the robot has seen in simulation during training. (b) Using BDA, we achieve the same SPL as a policy finetuned directly in reality while using 117× less real-world data.</p>
<p>Fig. 3 :
3(a) Top-down view of one of our testing environments. White boxes are obstacles. The robot navigates sequentially through thewaypoints A → B → C → D → E → A.Figure taken from [13] (b) 3D visualization of the robot navigating in one of our testing environments in simulation. RGB and Depth observations are shown on the right. TABLE I: Definition of the 10 different noise settings we use for training and testing. Row 1 indicates the 'source' environment with no observation or actuation noise present.</p>
<p>Fig. 4 :
4Zero-shot transfer of π 100M</p>
<p>Fig. 5 :
5(a) LAB with no RGB or Depth sensor noise (b) LAB with 0.1 Gaussian RGB noise and 1.0 Redwood Depth noise. (c) By adapting images from real2sim, we now have images that closely resemble (a).</p>
<p>Fig. 6 :
6(a) LAB real (b) We adapt from real2sim to obtain images that closely resemble RGB-D images from simulation.</p>
<p>Fig. 7 :
7Performance of BDA compared to directly finetuning a policy in the target environment: Plots (a), (b) and (c) represent LAB environments with different noise settings we test in. On average, BDA requires 61× less data from the target environment to achieve the same SPL as finetuning directly in the target environment.</p>
<p>TABLE II :
IISuccess rate and SPL of three policies with RGB-D observations. π 100M</p>
<p>, BDA requires 200× fewer samples from the target environment.# 
RGB Obs 
Depth Obs Actuation 
π 100M </p>
<p>Gibson no noise </p>
<p>π BDA−5k </p>
<p>Gibson OA+DA </p>
<p>π 1M </p>
<p>Gibson O+D </p>
<p>Noise 
Noise 
Noise 
SUCC 
SPL 
SUCC 
SPL 
SUCC 
SPL </p>
<h2>1</h2>
<h2>-</h2>
<p>1.0 
0.84 
0.80 
0.61 
0.99 
0.84 
2 
Gaus. 0.1 
Red. 1.0 
-
0.10 
0.04 
0.97 
0.80 
0.99 
0.87 
3 
-
-
Prop. 1.0 
0.89 
0.57 
0.99 
0.65 
1.00 
0.64 
4 
Gaus. 0.1 
Red. 1.0 
Prop. 1.0 
0.32 
0.11 
1.00 
0.62 
1.00 
0.65 </p>
<h2>5</h2>
<h2>-</h2>
<p>1.0 
0.84 
0.85 
0.68 
0.97 
0.80 
6 
Speck. 0.1 
Red. 1.5 
-
0.11 
0.05 
0.70 
0.54 
0.99 
0.81 
7 
-
-
MB 1.0 
0.71 
0.42 
0.97 
0.58 
1.00 
0.59 
8 
Speck. 0.1 
Red. 1.5 
MB 1.0 
0.08 
0.03 
0.99 
0.60 
1.00 
0.62 </p>
<h2>9</h2>
<h2>-</h2>
<p>1.0 
0.85 
1.00 
0.87 
1.00 
0.83 
10 
Pois. 1.0 
Red. 2.0 
-
0.07 
0.04 
0.96 
0.69 
1.00 
0.87 
11 
-
-
ILQR 1.0 
0.93 
0.68 
1.00 
0.76 
0.99 
0.73 
12 
Pois. 1.0 
Red. 2.0 
ILQR 1.0 
0.14 
0.05 
0.99 
0.63 
1.00 
0.73 </p>
<p>Habitat Challenge 2019 @ Habitat Embodied Agents Workshop. CVPR 2019. Habitat Challenge 2019 @ Habitat Embodied Agents Workshop. CVPR 2019. https://aihabitat.org/challenge/2019/.</p>
<p>Locobot: An open source low cost robot. Locobot: An open source low cost robot. https: //locobot-website.netlify.com/.</p>
<p>Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, arXiv:1807.06757Manolis Savva, et al. On Evaluation of Embodied Navigation Agents. arXiv preprintPeter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Doso- vitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. On Evaluation of Embodied Navigation Agents. arXiv preprint arXiv:1807.06757, 2018.</p>
<p>Learning dexterous inhand manipulation. Bowen Openai: Marcin Andrychowicz, Maciek Baker, Rafal Chociej, Bob Jozefowicz, Jakub Mcgrew, Arthur Pachocki, Matthias Petron, Glenn Plappert, Alex Powell, Ray, The International Journal of Robotics Research. 391OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in- hand manipulation. The International Journal of Robotics Research, 39(1):3-20, 2020.</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, 2018 IEEE international conference on robotics and automation (ICRA). IEEEKonstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, et al. Using simulation and domain adaptation to improve efficiency of deep robotic grasping. In 2018 IEEE international conference on robotics and automation (ICRA), pages 4243-4250. IEEE, 2018.</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, Dieter Fox, Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, and Dieter Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. pages 8973-8979, 05 2019.</p>
<p>Robust reconstruction of indoor scenes. Sungjoon Choi, Qian-Yi Zhou, Vladlen Koltun, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Sungjoon Choi, Qian-Yi Zhou, and Vladlen Koltun. Robust recon- struction of indoor scenes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.</p>
<p>Sim-to-real transfer with neural-augmented robot simulation. Florian Golemo, Adrien Ali Taiga, Aaron Courville, Pierre-Yves Oudeyer, Conference on Robot Learning. Florian Golemo, Adrien Ali Taiga, Aaron Courville, and Pierre-Yves Oudeyer. Sim-to-real transfer with neural-augmented robot simulation. In Conference on Robot Learning, pages 817-828, 2018.</p>
<p>Learning to walk via deep reinforcement learning. Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, Sergey Levine, arXiv:1812.11103arXiv preprintTuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, and Sergey Levine. Learning to walk via deep reinforcement learning. arXiv preprint arXiv:1812.11103, 2018.</p>
<p>Gans trained by a two time-scale update rule converge to a local nash equilibrium. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter, Advances in neural information processing systems. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in neural information processing systems, pages 6626-6637, 2017.</p>
<p>Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks. Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalashnikov, Alex Irpan, Julian Ibarz, Sergey Levine, Raia Hadsell, Konstantinos Bousmalis, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionStephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalash- nikov, Alex Irpan, Julian Ibarz, Sergey Levine, Raia Hadsell, and Konstantinos Bousmalis. Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 12627-12637, 2019.</p>
<p>Residual reinforcement learning for robot control. T Johannink, S Bahl, A Nair, J Luo, A Kumar, M Loskyll, J A Ojea, E Solowjow, S Levine, 2019 International Conference on Robotics and Automation (ICRA). T. Johannink, S. Bahl, A. Nair, J. Luo, A. Kumar, M. Loskyll, J. A. Ojea, E. Solowjow, and S. Levine. Residual reinforcement learning for robot control. In 2019 International Conference on Robotics and Automation (ICRA), pages 6023-6029, 2019.</p>
<p>Sim2real predictivity: Does evaluation in simulation predict real-world performance. Abhishek Kadian, Joanne Truong, Aaron Gokaslan, Alexander Clegg, Erik Wijmans, Stefan Lee, Manolis Savva, Sonia Chernova, Dhruv Batra, IEEE Robotics and Automation Letters. Abhishek Kadian, Joanne Truong, Aaron Gokaslan, Alexander Clegg, Erik Wijmans, Stefan Lee, Manolis Savva, Sonia Chernova, and Dhruv Batra. Sim2real predictivity: Does evaluation in simulation predict real-world performance. IEEE Robotics and Automation Letters, 2020.</p>
<p>A flexible and scalable slam system with full 3d motion estimation. S Kohlbrecher, J Meyer, O Stryk, U Klingauf, SSRR. IEEE. S. Kohlbrecher, J. Meyer, O. von Stryk, and U. Klingauf. A flexible and scalable slam system with full 3d motion estimation. In SSRR. IEEE, November 2011.</p>
<p>Adithyavairavan Murali, Tao Chen, Kalyan Vasudev Alwala, Dhiraj Gandhi, Lerrel Pinto, Saurabh Gupta, Abhinav Gupta, arXiv:1906.08236Pyrobot: An open-source robotics framework for research and benchmarking. arXiv preprintAdithyavairavan Murali, Tao Chen, Kalyan Vasudev Alwala, Dhiraj Gandhi, Lerrel Pinto, Saurabh Gupta, and Abhinav Gupta. Pyrobot: An open-source robotics framework for research and benchmarking. arXiv preprint arXiv:1906.08236, 2019.</p>
<p>Sim-to-real transfer of robotic control with dynamics randomization. Marcin Xue Bin Peng, Wojciech Andrychowicz, Pieter Zaremba, Abbeel, 2018 IEEE international conference on robotics and automation (ICRA). IEEEXue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of robotic control with dynamics ran- domization. In 2018 IEEE international conference on robotics and automation (ICRA), pages 1-8. IEEE, 2018.</p>
<p>CAD2RL: real single-image flight without a single real image. Fereshteh Sadeghi, Sergey Levine, Robotics: Science and Systems XIII, Massachusetts Institute of Technology. Cambridge, Massachusetts, USAFereshteh Sadeghi and Sergey Levine. CAD2RL: real single-image flight without a single real image. In Robotics: Science and Sys- tems XIII, Massachusetts Institute of Technology, Cambridge, Mas- sachusetts, USA, July 12-16, 2017, 2017.</p>
<p>Habitat: A Platform for Embodied AI Research. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, Dhruv Batra, ICCV. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A Platform for Embodied AI Research. In ICCV, 2019.</p>
<p>A brief introduction to boosting. Robert E Schapire, Proceedings of the 16th International Joint Conference on Artificial Intelligence. the 16th International Joint Conference on Artificial Intelligence299Robert E. Schapire. A brief introduction to boosting. In Proceedings of the 16th International Joint Conference on Artificial Intelligence - Volume 2, IJCAI'99, 1999.</p>
<p>. Tom Silver, Kelsey R Allen, Joshua B Tenenbaum, Leslie Pack Kaelbling, Residual policy learning. ArXiv, abs/1812.06298Tom Silver, Kelsey R. Allen, Joshua B. Tenenbaum, and Leslie Pack Kaelbling. Residual policy learning. ArXiv, abs/1812.06298, 2018.</p>
<p>Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, MIT pressRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, Pieter Abbeel, Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017</p>
<p>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 23-30. IEEE, 2017.</p>
<p>DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames. Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra, ICLR. Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra. DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames. In ICLR, 2020.</p>
<p>Gibson env: Real-world perception for embodied agents. Fei Xia, Zhiyang Amir R Zamir, Alexander He, Jitendra Sax, Silvio Malik, Savarese, CVPR. Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In CVPR, 2018.</p>
<p>Learning fast adaptation with meta strategy optimization. Wenhao Yu, Jie Tan, Yunfei Bai, Erwin Coumans, Sehoon Ha, IEEE Robotics and Automation Letters. 52Wenhao Yu, Jie Tan, Yunfei Bai, Erwin Coumans, and Sehoon Ha. Learning fast adaptation with meta strategy optimization. IEEE Robotics and Automation Letters, 5(2):2950-2957, 2020.</p>
<p>Vr-goggles for robots: Real-to-sim domain adaptation for visual control. Jingwei Zhang, Lei Tai, Peng Yun, Yufeng Xiong, Ming Liu, Joschka Boedecker, Wolfram Burgard, IEEE Robotics and Automation Letters. 42Jingwei Zhang, Lei Tai, Peng Yun, Yufeng Xiong, Ming Liu, Joschka Boedecker, and Wolfram Burgard. Vr-goggles for robots: Real-to-sim domain adaptation for visual control. IEEE Robotics and Automation Letters, 4(2):1148-1155, 2019.</p>
<p>Unpaired image-to-image translation using cycle-consistent adversarial networks. Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A Efros, Computer Vision (ICCV. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Un- paired image-to-image translation using cycle-consistent adversarial networks. In Computer Vision (ICCV), 2017 IEEE International Conference on, 2017.</p>            </div>
        </div>

    </div>
</body>
</html>
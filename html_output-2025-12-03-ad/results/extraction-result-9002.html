<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9002 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9002</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9002</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-278959353</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.22112v1.pdf" target="_blank">Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test</a></p>
                <p><strong>Paper Abstract:</strong> Cognitive flexibility has been extensively studied in human cognition but remains relatively unexplored in the context of Visual Large Language Models (VLLMs). This study assesses the cognitive flexibility of state-of-the-art VLLMs (GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet) using the Wisconsin Card Sorting Test (WCST), a classic measure of set-shifting ability. Our results reveal that VLLMs achieve or surpass human-level set-shifting capabilities under chain-of-thought prompting with text-based inputs. However, their abilities are highly influenced by both input modality and prompting strategy. In addition, we find that through role-playing, VLLMs can simulate various functional deficits aligned with patients having impairments in cognitive flexibility, suggesting that VLLMs may possess a cognitive architecture, at least regarding the ability of set-shifting, similar to the brain. This study reveals the fact that VLLMs have already approached the human level on a key component underlying our higher cognition, and highlights the potential to use them to emulate complex brain processes.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9002.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9002.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based multimodal large language model by OpenAI with integrated visual processing; supports adaptive image processing modes (low- and high-resolution) and was evaluated here as a Visual Large Language Model (VLLM) on set-shifting tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based multimodal model from OpenAI with adaptive image processing (low-res 512x512 representation, high-res via tiled crops), supports text and visual inputs; used via API in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Wisconsin Card Sorting Test (WCST-64)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Standard WCST-64: 64 trials where each trial requires matching a response card to one of four stimulus cards according to an undisclosed rule (color, shape, or number) inferred from feedback; rule switches after 10 consecutive correct responses; assesses cognitive flexibility / set-shifting, perseveration, conceptual understanding, and related executive functions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Across 4 experimental conditions (STA-VI, STA-TI, CoT-VI, CoT-TI; 10 repetitions each): STA-VI near-chance (mean reported range across models 0.02-0.04 standardized CC); STA-TI modest improvement (example: Claude reported µ=0.10 σ=0.10); CoT-VI substantial improvement; CoT-TI peak performance. Specifically for GPT-4o: CoT-TI standardized Categories Completed (CC) µ = 0.92 (σ = 0.16). Detailed raw CC under CoT-TI in Table IV: CC = 4.60 (σ = 0.84) categories completed (out of 5 possible). Other metric highlights under normal CoT-TI: Perseverative Errors (PE) = 7.60 (σ = 1.84), Non-Perseverative Errors (NPE) = 2.10 (σ = 0.88), Trials to First Category (TFC) = 12.60 (σ = 2.46), Conceptual Level Responses (CLR) = 63.28% (σ = 5.86%), Failure to Maintain Set (FMS) = 0.10 (σ = 0.32). Under simulated impairments (role-play) CC declined modestly to 3.50–4.30 depending on impairment type.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Standardized CC human baseline µ = 0.95 (σ = 0.09). Human Trials-to-First-Category baseline reported TFC µ = 12.93 (σ = 1.62). (Paper uses standardized CC scale where human baseline is 0.95.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4o achieves near-human performance under Chain-of-Thought prompting with textual inputs (CoT-TI), slightly below the human baseline on standardized CC (0.92 vs human µ=0.95) but comparable on many auxiliary metrics (TFC, CLR). Under STA and/or visual-input conditions performance is substantially below human level.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>2x2 factorial design manipulating input modality (Visual Input VI / Textual Input TI) and prompting strategy (Straight-to-Answer STA / Chain-of-Thought CoT). Each model tested independently across STA-VI, STA-TI, CoT-VI, CoT-TI with 10 randomized repetitions per condition, 64 trials per run. Responses recorded verbatim via API. Test variant 'ALIEN Task' (same logic, different surface features) used to control for memorization; patterns persisted. Evaluation metrics: Categories Completed (CC, standardized 0-1), Perseverative Errors (PE), Non-Perseverative Errors (NPE), Trials to First Category (TFC), Conceptual Level Responses (CLR), Failure to Maintain Set (FMS). Visual feature recognition accuracy also assessed (card count, color, shape, number).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Performance strongly depends on prompting strategy and input modality: CoT + textual inputs produce best results; visual-input conditions show degraded/variable performance due in part to occasional visual misrecognitions (e.g., counting errors) that cascade into rule application failures. Comparisons use a standardized CC measure anchored to human baseline but may not generalize to more ambiguous or unconstrained real-world scenarios. Model size and training data specifics not provided. Simulated impairments use role-playing prompts and may reflect stereotyped representations rather than true mechanistic analogs of human neuropathology.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9002.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9002.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-1.5Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-1.5Pro (Google)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Google Mixture-of-Experts multimodal VLLM with standardized image tokenization, evaluated on WCST-64 to probe cognitive flexibility across visual/textual inputs and prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.5Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multimodal Mixture-of-Experts architecture from Google; for image processing each image treated as 258 tokens regardless of size (images scaled to preserve aspect ratio within defined bounds). Proprietary architecture details; strong multimodal performance reported in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Wisconsin Card Sorting Test (WCST-64); ALIEN Task (novel variant)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>WCST-64 as above; ALIEN Task is a logically equivalent variant that replaces surface features (shape→orbit type, color→atmosphere composition, number→moon count) to control for memorization and test generalization of rule learning/set-shifting.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Gemini performance varied strongly by condition: STA-VI near-chance (standardized CC approx. 0.02–0.04). Under CoT-TI: standardized CC µ ≈ 0.96 (σ = 0.08) as reported for Gemini in aggregate (CoT-TI peak performance). Raw CC under CoT-TI (Table IV): CC = 4.80 (σ = 0.42). Other CoT-TI metrics: PE = 6.80 (σ = 1.55), NPE = 3.50 (σ = 1.72), TFC = 13.30 (σ = 1.95), CLR = 63.12% (σ = 4.67%), FMS = 0.10 (σ = 0.32). Under STA-TI PE were highest (example: Claude had high PE in STA-TI) and STA conditions generally poor. On the ALIEN Task, pattern of failures in STA and high performance in CoT-TI persisted, arguing against simple memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Standardized CC human baseline µ = 0.95 (σ = 0.09). For TFC human baseline µ = 12.93 (σ = 1.62).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Gemini-1.5Pro reaches near-human to human-equivalent performance under CoT-TI (standardized CC ≈ 0.96; raw CC ≈ 4.8/5), but performs poorly under STA prompting and especially under visual STA-VI. Sensitive to removal of explicit rule exclusivity: CC dropped from 4.8 to 2.6 without explicit exclusivity information.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Same WCST-64 procedure and 2x2 design as other models; 10 randomized repetitions per condition; ALIEN Task used for memorization control (STA-TI and CoT-TI). The study also manipulated presence/absence of explicit rule exclusivity statements in CoT-TI and ran simulated impairment role-play prompts under CoT-TI to model goal maintenance, inhibitory control, and adaptive updating deficits. Visual feature recognition assessed separately (Gemini showed some decline on number/count recognition relative to Claude).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Gemini displayed high sensitivity to instruction specificity: removal of explicit exclusivity degraded performance markedly, indicating reliance on explicit constraints. Visual-input performance suffered from miscounts/feature recognition errors which cascaded to higher-level failures. Variability (σ) increased in some CoT conditions indicating occasional instability. Model size and training data are proprietary and not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9002.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9002.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.5Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3.5 Sonnet (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based multimodal model from Anthropic with enhanced visual understanding and image-token heuristics; in this study it achieved the strongest WCST performance, in some conditions surpassing human baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3.5Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic's Claude-3.5Sonnet: transformer-based multimodal LLM optimized for visual inputs; computes image token usage from image dimensions and emphasizes image clarity; balances multi-image processing by resizing very large images.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Wisconsin Card Sorting Test (WCST-64); ALIEN Task (novel variant)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>WCST-64 standard set-shifting task measuring cognitive flexibility (color/shape/number sorting inferred from feedback; rule switches after 10 correct). ALIEN Task is a surface-novel variant used to rule out memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Claude-3.5Sonnet produced the best overall performance in this study. Standardized CC by condition: STA-VI very low (0.02–0.04 range across models), STA-TI modest (Claude STA-TI µ = 0.10 σ = 0.10), CoT-VI strong (µ = 0.96 σ = 0.08), CoT-TI perfect in reported aggregate (µ = 1.00 σ = 0.00). Raw CC under CoT-TI (Table IV): CC = 5.00 (σ = 0.00) categories completed (perfect across repetitions). Other CoT-TI metrics: PE = 6.30 (σ = 0.82), NPE = 2.00 (σ = 0.82), TFC = 12.00 (σ = 0.94) (note TFC outperformed human baseline µ=12.93), CLR = 67.50% (σ = 2.74%), FMS = 0.00. On the ALIEN Task the pattern of high CoT-TI performance persisted, supporting generalization beyond memorized WCST wording.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Standardized CC human baseline µ = 0.95 (σ = 0.09). Human Trials-to-First-Category baseline TFC µ = 12.93 (σ = 1.62).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Claude-3.5Sonnet matched or exceeded the human baseline in the best condition (CoT-TI) — perfect standardized CC (1.00) and raw CC = 5.0/5; TFC slightly better than human baseline. Under STA and/or visual input conditions performance was substantially lower. Claude also showed greatest robustness to removal of explicit rule exclusivity (small decline only).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Same 2x2 design (STA/CoT × VI/TI), 10 repetitions per condition, 64 trials, API-based data collection. The study additionally removed explicit rule-exclusivity statements in CoT-TI to test robustness and performed role-playing simulations of PFC impairments (goal maintenance, inhibitory control, adaptive updating) under CoT-TI. Visual feature recognition testing showed Claude had near-perfect visual feature identification.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Although Claude achieved perfect performance in CoT-TI in this dataset, performance dropped or became more variable when explicit rule exclusivity was removed (though decline was small relative to other models). High performance depends critically on chain-of-thought prompting and precise instruction framing; visual-input conditions still show vulnerability to perceptual errors in other models, though Claude's visual accuracy was near-perfect. Role-played impairments are simulated via prompts and may not reflect true mechanistic equivalence to human neuropsychology; potential for stereotyped or biased simulations exists. Model internals and training specifics not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Challenging chatgpt'intelligence'with human tools: a neuropsychological investigation on prefrontal functioning of a large language model <em>(Rating: 2)</em></li>
                <li>Testing theory of mind in large language models and humans <em>(Rating: 2)</em></li>
                <li>Using large language models to simulate multiple humans and replicate human subject studies <em>(Rating: 1)</em></li>
                <li>Large language models are visual reasoning coordinators <em>(Rating: 1)</em></li>
                <li>Vision language models are blind <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9002",
    "paper_id": "paper-278959353",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (OpenAI)",
            "brief_description": "A transformer-based multimodal large language model by OpenAI with integrated visual processing; supports adaptive image processing modes (low- and high-resolution) and was evaluated here as a Visual Large Language Model (VLLM) on set-shifting tasks.",
            "citation_title": "Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Transformer-based multimodal model from OpenAI with adaptive image processing (low-res 512x512 representation, high-res via tiled crops), supports text and visual inputs; used via API in this study.",
            "model_size": null,
            "test_battery_name": "Wisconsin Card Sorting Test (WCST-64)",
            "test_description": "Standard WCST-64: 64 trials where each trial requires matching a response card to one of four stimulus cards according to an undisclosed rule (color, shape, or number) inferred from feedback; rule switches after 10 consecutive correct responses; assesses cognitive flexibility / set-shifting, perseveration, conceptual understanding, and related executive functions.",
            "llm_performance": "Across 4 experimental conditions (STA-VI, STA-TI, CoT-VI, CoT-TI; 10 repetitions each): STA-VI near-chance (mean reported range across models 0.02-0.04 standardized CC); STA-TI modest improvement (example: Claude reported µ=0.10 σ=0.10); CoT-VI substantial improvement; CoT-TI peak performance. Specifically for GPT-4o: CoT-TI standardized Categories Completed (CC) µ = 0.92 (σ = 0.16). Detailed raw CC under CoT-TI in Table IV: CC = 4.60 (σ = 0.84) categories completed (out of 5 possible). Other metric highlights under normal CoT-TI: Perseverative Errors (PE) = 7.60 (σ = 1.84), Non-Perseverative Errors (NPE) = 2.10 (σ = 0.88), Trials to First Category (TFC) = 12.60 (σ = 2.46), Conceptual Level Responses (CLR) = 63.28% (σ = 5.86%), Failure to Maintain Set (FMS) = 0.10 (σ = 0.32). Under simulated impairments (role-play) CC declined modestly to 3.50–4.30 depending on impairment type.",
            "human_baseline_performance": "Standardized CC human baseline µ = 0.95 (σ = 0.09). Human Trials-to-First-Category baseline reported TFC µ = 12.93 (σ = 1.62). (Paper uses standardized CC scale where human baseline is 0.95.)",
            "performance_comparison": "GPT-4o achieves near-human performance under Chain-of-Thought prompting with textual inputs (CoT-TI), slightly below the human baseline on standardized CC (0.92 vs human µ=0.95) but comparable on many auxiliary metrics (TFC, CLR). Under STA and/or visual-input conditions performance is substantially below human level.",
            "experimental_details": "2x2 factorial design manipulating input modality (Visual Input VI / Textual Input TI) and prompting strategy (Straight-to-Answer STA / Chain-of-Thought CoT). Each model tested independently across STA-VI, STA-TI, CoT-VI, CoT-TI with 10 randomized repetitions per condition, 64 trials per run. Responses recorded verbatim via API. Test variant 'ALIEN Task' (same logic, different surface features) used to control for memorization; patterns persisted. Evaluation metrics: Categories Completed (CC, standardized 0-1), Perseverative Errors (PE), Non-Perseverative Errors (NPE), Trials to First Category (TFC), Conceptual Level Responses (CLR), Failure to Maintain Set (FMS). Visual feature recognition accuracy also assessed (card count, color, shape, number).",
            "limitations_or_caveats": "Performance strongly depends on prompting strategy and input modality: CoT + textual inputs produce best results; visual-input conditions show degraded/variable performance due in part to occasional visual misrecognitions (e.g., counting errors) that cascade into rule application failures. Comparisons use a standardized CC measure anchored to human baseline but may not generalize to more ambiguous or unconstrained real-world scenarios. Model size and training data specifics not provided. Simulated impairments use role-playing prompts and may reflect stereotyped representations rather than true mechanistic analogs of human neuropathology.",
            "uuid": "e9002.0",
            "source_info": {
                "paper_title": "Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Gemini-1.5Pro",
            "name_full": "Gemini-1.5Pro (Google)",
            "brief_description": "A Google Mixture-of-Experts multimodal VLLM with standardized image tokenization, evaluated on WCST-64 to probe cognitive flexibility across visual/textual inputs and prompting strategies.",
            "citation_title": "Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test",
            "mention_or_use": "use",
            "model_name": "Gemini-1.5Pro",
            "model_description": "A multimodal Mixture-of-Experts architecture from Google; for image processing each image treated as 258 tokens regardless of size (images scaled to preserve aspect ratio within defined bounds). Proprietary architecture details; strong multimodal performance reported in paper.",
            "model_size": null,
            "test_battery_name": "Wisconsin Card Sorting Test (WCST-64); ALIEN Task (novel variant)",
            "test_description": "WCST-64 as above; ALIEN Task is a logically equivalent variant that replaces surface features (shape→orbit type, color→atmosphere composition, number→moon count) to control for memorization and test generalization of rule learning/set-shifting.",
            "llm_performance": "Gemini performance varied strongly by condition: STA-VI near-chance (standardized CC approx. 0.02–0.04). Under CoT-TI: standardized CC µ ≈ 0.96 (σ = 0.08) as reported for Gemini in aggregate (CoT-TI peak performance). Raw CC under CoT-TI (Table IV): CC = 4.80 (σ = 0.42). Other CoT-TI metrics: PE = 6.80 (σ = 1.55), NPE = 3.50 (σ = 1.72), TFC = 13.30 (σ = 1.95), CLR = 63.12% (σ = 4.67%), FMS = 0.10 (σ = 0.32). Under STA-TI PE were highest (example: Claude had high PE in STA-TI) and STA conditions generally poor. On the ALIEN Task, pattern of failures in STA and high performance in CoT-TI persisted, arguing against simple memorization.",
            "human_baseline_performance": "Standardized CC human baseline µ = 0.95 (σ = 0.09). For TFC human baseline µ = 12.93 (σ = 1.62).",
            "performance_comparison": "Gemini-1.5Pro reaches near-human to human-equivalent performance under CoT-TI (standardized CC ≈ 0.96; raw CC ≈ 4.8/5), but performs poorly under STA prompting and especially under visual STA-VI. Sensitive to removal of explicit rule exclusivity: CC dropped from 4.8 to 2.6 without explicit exclusivity information.",
            "experimental_details": "Same WCST-64 procedure and 2x2 design as other models; 10 randomized repetitions per condition; ALIEN Task used for memorization control (STA-TI and CoT-TI). The study also manipulated presence/absence of explicit rule exclusivity statements in CoT-TI and ran simulated impairment role-play prompts under CoT-TI to model goal maintenance, inhibitory control, and adaptive updating deficits. Visual feature recognition assessed separately (Gemini showed some decline on number/count recognition relative to Claude).",
            "limitations_or_caveats": "Gemini displayed high sensitivity to instruction specificity: removal of explicit exclusivity degraded performance markedly, indicating reliance on explicit constraints. Visual-input performance suffered from miscounts/feature recognition errors which cascaded to higher-level failures. Variability (σ) increased in some CoT conditions indicating occasional instability. Model size and training data are proprietary and not specified.",
            "uuid": "e9002.1",
            "source_info": {
                "paper_title": "Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Claude-3.5Sonnet",
            "name_full": "Claude-3.5 Sonnet (Anthropic)",
            "brief_description": "A transformer-based multimodal model from Anthropic with enhanced visual understanding and image-token heuristics; in this study it achieved the strongest WCST performance, in some conditions surpassing human baselines.",
            "citation_title": "Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test",
            "mention_or_use": "use",
            "model_name": "Claude-3.5Sonnet",
            "model_description": "Anthropic's Claude-3.5Sonnet: transformer-based multimodal LLM optimized for visual inputs; computes image token usage from image dimensions and emphasizes image clarity; balances multi-image processing by resizing very large images.",
            "model_size": null,
            "test_battery_name": "Wisconsin Card Sorting Test (WCST-64); ALIEN Task (novel variant)",
            "test_description": "WCST-64 standard set-shifting task measuring cognitive flexibility (color/shape/number sorting inferred from feedback; rule switches after 10 correct). ALIEN Task is a surface-novel variant used to rule out memorization.",
            "llm_performance": "Claude-3.5Sonnet produced the best overall performance in this study. Standardized CC by condition: STA-VI very low (0.02–0.04 range across models), STA-TI modest (Claude STA-TI µ = 0.10 σ = 0.10), CoT-VI strong (µ = 0.96 σ = 0.08), CoT-TI perfect in reported aggregate (µ = 1.00 σ = 0.00). Raw CC under CoT-TI (Table IV): CC = 5.00 (σ = 0.00) categories completed (perfect across repetitions). Other CoT-TI metrics: PE = 6.30 (σ = 0.82), NPE = 2.00 (σ = 0.82), TFC = 12.00 (σ = 0.94) (note TFC outperformed human baseline µ=12.93), CLR = 67.50% (σ = 2.74%), FMS = 0.00. On the ALIEN Task the pattern of high CoT-TI performance persisted, supporting generalization beyond memorized WCST wording.",
            "human_baseline_performance": "Standardized CC human baseline µ = 0.95 (σ = 0.09). Human Trials-to-First-Category baseline TFC µ = 12.93 (σ = 1.62).",
            "performance_comparison": "Claude-3.5Sonnet matched or exceeded the human baseline in the best condition (CoT-TI) — perfect standardized CC (1.00) and raw CC = 5.0/5; TFC slightly better than human baseline. Under STA and/or visual input conditions performance was substantially lower. Claude also showed greatest robustness to removal of explicit rule exclusivity (small decline only).",
            "experimental_details": "Same 2x2 design (STA/CoT × VI/TI), 10 repetitions per condition, 64 trials, API-based data collection. The study additionally removed explicit rule-exclusivity statements in CoT-TI to test robustness and performed role-playing simulations of PFC impairments (goal maintenance, inhibitory control, adaptive updating) under CoT-TI. Visual feature recognition testing showed Claude had near-perfect visual feature identification.",
            "limitations_or_caveats": "Although Claude achieved perfect performance in CoT-TI in this dataset, performance dropped or became more variable when explicit rule exclusivity was removed (though decline was small relative to other models). High performance depends critically on chain-of-thought prompting and precise instruction framing; visual-input conditions still show vulnerability to perceptual errors in other models, though Claude's visual accuracy was near-perfect. Role-played impairments are simulated via prompts and may not reflect true mechanistic equivalence to human neuropsychology; potential for stereotyped or biased simulations exists. Model internals and training specifics not provided.",
            "uuid": "e9002.2",
            "source_info": {
                "paper_title": "Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Challenging chatgpt'intelligence'with human tools: a neuropsychological investigation on prefrontal functioning of a large language model",
            "rating": 2,
            "sanitized_title": "challenging_chatgptintelligencewith_human_tools_a_neuropsychological_investigation_on_prefrontal_functioning_of_a_large_language_model"
        },
        {
            "paper_title": "Testing theory of mind in large language models and humans",
            "rating": 2,
            "sanitized_title": "testing_theory_of_mind_in_large_language_models_and_humans"
        },
        {
            "paper_title": "Using large language models to simulate multiple humans and replicate human subject studies",
            "rating": 1,
            "sanitized_title": "using_large_language_models_to_simulate_multiple_humans_and_replicate_human_subject_studies"
        },
        {
            "paper_title": "Large language models are visual reasoning coordinators",
            "rating": 1,
            "sanitized_title": "large_language_models_are_visual_reasoning_coordinators"
        },
        {
            "paper_title": "Vision language models are blind",
            "rating": 1,
            "sanitized_title": "vision_language_models_are_blind"
        }
    ],
    "cost": 0.0121715,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test</p>
<p>Guangfu Hao 
Institute of Automation
Laboratory of Brain Atlas and Brain-inspired Intelligence
Chinese Academy of Sciences
BeijingChina</p>
<p>School of Artificial Intelligence
University of Chinese Academy of Sciences
BeijingChina</p>
<p>Frederic Alexandre 
Inria centre at the university of Bordeaux
Neurodegenerative Diseases Institute
BordeauxFrance</p>
<p>Shan Yu 
Institute of Automation
Laboratory of Brain Atlas and Brain-inspired Intelligence
Chinese Academy of Sciences
BeijingChina</p>
<p>School of Artificial Intelligence
University of Chinese Academy of Sciences
BeijingChina</p>
<p>Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test
00F0FF021D0530AC9F7C72446A059035Cognitive flexibilityWisconsin Card Sorting TestVisual large language modelsPrefrontal cortexPrompting strategyCognitive impairment simulation
Cognitive flexibility has been extensively studied in human cognition but remains relatively unexplored in the context of Visual Large Language Models (VLLMs).This study assesses the cognitive flexibility of state-of-the-art VLLMs (GPT-4o, Gemini-1.5Pro, and Claude-3.5Sonnet) using the Wisconsin Card Sorting Test (WCST), a classic measure of set-shifting ability.Our results reveal that VLLMs achieve or surpass humanlevel set-shifting capabilities under chain-of-thought prompting with text-based inputs.However, their abilities are highly influenced by both input modality and prompting strategy.In addition, we find that through role-playing, VLLMs can simulate various functional deficits aligned with patients having impairments in cognitive flexibility, suggesting that VLLMs may possess a cognitive architecture, at least regarding the ability of set-shifting, similar to the brain.This study reveals the fact that VLLMs have already approached the human level on a key component underlying our higher cognition, and highlights the potential to use them to emulate complex brain processes.</p>
<p>I. INTRODUCTION</p>
<p>C OGNITIVE flexibility, a key component of executive function, is fundamental to human adaptability and problem-solving [1], [2].This ability to shift between mental sets or strategies in response to changing environmental demands is crucial for everyday functioning and has been extensively studied in cognitive psychology research [3].The prefrontal cortex (PFC) is known to be central to this cognitive process [4], [5], facilitating goal-directed behavior and controlled processing.</p>
<p>Recent advancements in artificial intelligence (AI), particularly in visual large language models (VLLMs) [6], have sparked a growing need to assess these systems' cognitive abilities using paradigms analogous to human cognitive assessment [7]- [9].State-of-the-art VLLMs such as GPT-4o [10], Gemini-1.5Pro [11], and Claude-3.5Sonnet [12] have demonstrated remarkable capabilities in processing and interpreting both textual and visual information, excelling in tasks that demand complex reasoning and contextual understanding.</p>
<p>Despite these achievements, the extent to which VLLMs exhibit cognitive flexibility, especially in tasks requiring setshifting and adaptation to changing rules, remains largely *Corresponding author: shan.yu@nlpr.ia.ac.cn unexplored.While these models have demonstrated impressive performance across diverse tasks, their ability to flexibly adapt to changing environmental demands has not been systematically evaluated.This gap in our understanding is particularly significant given the increasing integration of VLLMs into complex, dynamic real-world environments where adaptability is crucial.</p>
<p>The Wisconsin Card Sorting Test (WCST), developed in the 1940s [13] and refined over decades, has emerged as the gold standard for assessing cognitive flexibility in both clinical and research settings [14], [15].Originally developed to evaluate PFC function, it requires participants to discover sorting rules based on feedback and then flexibly shift to new rules when the criteria change (see Figure 1).The test's sensitivity to PFC function has been consistently demonstrated through lesion studies [16], neuroimaging research [17], and clinical observations, cementing its status as a crucial tool in understanding the cognitive flexibility.While other measures of cognitive flexibility exist, the WCST's established validity make it a core benchmark for evaluating this fundamental cognitive capacity.</p>
<p>This research aims to evaluate the cognitive flexibility in VLLMs using the WCST paradigm and investigate how different input modalities (image-based vs. text-based) ,prompting strategy (direct vs. chain-of-thought reasoning) and rule description specificity affect their performance.Additionally, we explore the potential of VLLMs to simulate specific patterns of cognitive impairment through role-playing, which enables us to understand human cognitive architecture.By comparing VLLM performance across varied conditions, we aim to elucidate their cognitive flexibility and inherent limitations.This investigation not only advances our understanding of VLLMs but also offers insights into the nature of cognitive flexibility itself.</p>
<p>II. RELATED WORK</p>
<p>A. Cognitive Flexibility and Assessment Methods</p>
<p>Neuroimaging studies have consistently implicated the PFC in cognitive flexibility tasks.The dorsolateral prefrontal cortex (DLPFC) and anterior cingulate cortex (ACC) play critical roles in set-shifting, with the DLPFC maintaining and updating task rules, and the ACC involved in conflict monitoring and  1. WCST Procedure and Sample Stimuli.The WCST consists of matching response cards to four stimulus cards based on a sorting rule (color, shape, or number) that changes periodically.Participants receive feedback on the accuracy of each match but are not explicitly told the sorting rule or when it changes.</p>
<p>error detection [18].The fronto-parietal network, encompassing these regions, dynamically reconfigures during flexibilitydemanding tasks [19].Cognitive flexibility is closely interrelated with other executive functions: working memory maintains task-relevant information and goals [3], while inhibitory control suppresses previous cognitive sets when rules change [20].</p>
<p>Several tasks have been developed to assess cognitive flexibility in humans, with the WCST being a widely recognized measure of set-shifting ability [21].The WCST's sensitivity to PFC dysfunction has been extensively validated [22].Complementary assessments include the Dimensional Change Card Sort (DCCS) task for children [23] and the computerized Intra-Extra Dimensional Set Shift (IED) subtest of the Cambridge Neuropsychological Test Automated Battery (CANTAB) [24], offering targeted measures across different populations and modalities.</p>
<p>B. Multifaceted Evaluation of LLMs</p>
<p>Recent studies have employed diverse assessments to evaluate large language models (LLMs) and VLLMs across various domains and tasks.Models like GPT-4 demonstrated human-level or superior performance on most theory of mind tests [25].Similarly, research on human creativity found that ChatGPT-assisted ideas were more creative compared to those generated without LLM assistance [26].However, challenges persist in other areas.The Test of Time (ToT) benchmark exposed difficulties with complex temporal reasoning tasks, particularly those requiring multi-fact integration and intricate arithmetic operations [27].Despite strong performance on high-level vision tasks, state-of-the-art VLLMs struggled with basic geometric tasks that are straightforward for humans [28].A neuropsychological investigation revealed a discontinuous profile in ChatGPT's prefrontal functioning, with performance ranging from superior to impaired across different cognitive tasks [29].To address the multifaceted nature of artificial intelligence, researchers have proposed new evaluation frameworks.A comprehensive framework for artificial general intelligence (AGI) tests inspired by cognitive science emphasizes the need for multidimensional intelligence assessment [9].Additionally, the concept of Turing Experiments (TEs) was introduced as a method for evaluating LLMs' ability to simulate human behavior in experimental settings [30].</p>
<p>C. Prompting Strategies</p>
<p>Prompting strategies significantly influence the performance of LLMs [31].The simplest approach, "Straight-to-Answer" (STA), directly queries the model without additional context.While effective for straightforward tasks, STA often falters on complex problems requiring multi-step reasoning.Chain-of-Thought (CoT) prompting encourages step-by-step reasoning [32], substantially improving performance on complex reason-ing tasks [33].Variations such as zero-shot CoT [34] and selfconsistency CoT [35] have further refined this approach, adapting it to scenarios with limited or no task-specific examples.In multimodal contexts, visual CoT have extended these concepts to VLLMs [36], demonstrating the potential for improved reasoning in tasks that combine textual and visual information.Other task-specific strategies, such as least-to-most prompting address challenges of easy-to-hard generalization [37], while meta-prompting and automatic prompt engineering techniques aim to optimize the prompts themselves [38], [39].</p>
<p>III. METHOD</p>
<p>A. Models and Experimental Procedure</p>
<p>This study focuses on three state-of-the-art VLLMs: GPT-4o, Gemini-1.5Pro, and Claude-3.5Sonnet.These models represent the current pinnacle of multimodal LLMs capabilities, demonstrating proficiency in processing both textual and visual inputs (see Table V for details).We employs a standard version of the WCST-64 [15] to assess the cognitive flexibility of VLLMs.Our experimental design incorporates a 2x2 factorial structure, manipulating input modality (Visual Input (VI) / Textual Input (TI)) and prompting strategy (Straight to Answer (STA) / Chain of Thought (CoT)) to comprehensively evaluate VLLMs performance.This design resulted in four experimental conditions: STA-VI, STA-TI, CoT-VI, and CoT-TI.Each VLLM was tested independently across all four conditions, with 10 repetitions per condition.The arrangement of stimulus cards and the sequence of sorting rules were randomized for each repetition.</p>
<p>Algorithm 1 WCST for VLLMs  5).The interface presented cards sequentially and allowed participants to indicate their sorting choices via button presses.The language used in instructions was carefully adapted to be more intuitive for human subjects while maintaining the essential structure of the task.</p>
<p>We adapted the WCST for use with VLLMs while maintaining its core principles (see Algorithm 1 for the implementation).The test consists of a series of virtual cards, each featuring shapes (circle, cross, triangle, or star) in varying colors (red, green, yellow, or blue) and quantities (one to four).The models are tasked with sorting these cards according to an undisclosed rule (color, shape, or number), which changes periodically without explicit notification.The sorting rule changed after ten consecutive correct categorizations.The assessment concluded after 64 trials.Detailed descriptions of the task instructions are provided in supplementary Data collection was fully automated using API calls to each VLLM.Model responses were recorded verbatim for each trial.Human participant data was collected through the webbased interface.All participants provided informed consent, and the study was approved by the institutional review board.</p>
<p>To address potential concerns regarding model memorization of the classic WCST paradigm, we also designed a novel variant called the ALIEN Task that preserves the logical structure of the WCST while using entirely different terminology, stimuli, and framing.In this variant, participants explore an alien civilization by categorizing extraterrestrial astronomical systems.The original WCST dimensions were replaced with thematically distinct alternatives: shape was replaced with planetary orbit types (spiral, elliptical, circular, Z-shaped); color was replaced with atmospheric composition (hydrogen/blue, helium/yellow, nitrogen/purple, oxygen/green); and count was replaced with number of moons (1,2,3,4).The underlying logical structure and rule-switching mechanisms remained identical to the WCST.We implemented this variant under STA-TI and CoT-TI conditions to examine whether performance patterns would persist with entirely novel surface features.The specific instructions and example stimuli for the ALIEN Task are detailed in supplementary Figure s-4.</p>
<p>B. Evaluation Metrics</p>
<p>Performance was primarily assessed using the following metrics which were chosen for their ability to quantify different aspects of cognitive flexibility:</p>
<p>Categories Completed (CC): The number of categories (sets of 10 consecutive correct sorts) completed.
CC = n i=1 I(c i = 10)(1)
where n is the total number of trials, c i is the number of consecutive correct sorts at trial i, and I(•) is the indicator function.</p>
<p>Perseverative Errors (PE): The number of errors where the model persisted with a previously correct but currently incorrect rule.
P E = n i=1 I(r i = r prev ∧ r i ̸ = r current )(2)
where r i is the rule used by the model at trial i, r prev is the previously correct rule, and r current is the current correct rule.Non-Perseverative Errors (NPE): All errors that are not perseverative.
N P E = Total Errors − P E(3)
NPE captures non-perseverative errors, potentially indicating exploration or random mistakes.Trials to First Category (TFC): The number of trials required to complete the first category, indicating how quickly the model can deduce and consistently apply the first sorting rule.
T F C = min{i : c i = 10} (4)
where i is the trial number and c i is as defined in CC.</p>
<p>Conceptual Level Responses (CLR):</p>
<p>The percentage of responses occurring in runs of three or more correct sorts, indicating conceptual understanding.
CLR = n i=1 I(c i ≥ 3) n × 100%(5)
where I(c i ≥ 3) is an indicator function that equals 1 if the number of consecutive correct sorts up to and including trial i is 3 or more, and 0 otherwise.Failure to Maintain Set (FMS): The number of times the model makes an error after five or more consecutive correct sorts but before completing a category.
F M S = n−1 i=1 I(5 ≤ c i &lt; 10) • I(c i+1 = 0) (6)
where i is the trial number and c i is as defined in CC.These metrics collectively provide a comprehensive view of the VLLMs' cognitive flexibility [40], capturing various aspects such as rule learning, set-shifting, perseveration, and conceptual understanding.</p>
<p>IV. RESULTS</p>
<p>A. WCST Task Performance Across Models and Conditions</p>
<p>The WCST performance of GPT-4o, Gemini-1.5Pro, and Claude-3.5Sonnet exhibited marked variations across the four conditions (Figure 2).Their cognitive flexibility was measured using the CC metric, standardized on a 0-1 scale, with the human baseline(µ = 0.95, σ = 0.09).The CoT-TI condition consistently yielded superior outcomes across all VLLMs, followed by CoT-VI, STA-TI, and STA-VI, respectively, underscoring the critical influence of both prompting strategies and input modalities on VLLMs' set-shifting capabilities.</p>
<p>In the STA-VI condition, all VLLMs struggled significantly, with mean performances ranging from 0.02 to 0.04.The transition to STA-TI yielded modest improvements, particularly for Claude-3.5Sonnet (µ = 0.10, σ = 0.10).However, the introduction of CoT prompting precipitated a dramatic performance surge across all models.In the CoT-VI condition, Claude-3.5Sonnet exhibited remarkable improvement (µ = 0.96, σ = 0.08), while Gemini-1.5Pro and GPT-4o also showed substantial gains.This stark contrast between STA and CoT conditions illuminates the pivotal role of explicit reasoning in augmenting VLLMs' cognitive flexibility.</p>
<p>The CoT-TI condition elicited peak performances, with Claude-3.5Sonnet achieving perfection (µ = 1.00, σ = 0.00), surpassing even the human baseline.Gemini-1.5Pro (µ = 0.96, σ = 0.08) and GPT-4o (µ = 0.92, σ = 0.16) also demonstrated near-human or human-equivalent performance in this setting.Notably, the performance variability (σ) was generally higher in CoT conditions for Gemini-1.5Pro and GPT-4o, indicating potential instability in their cognitive processes.The consistent superiority of TI over VI across all conditions suggests a potential advantage in processing textual over visual inputs.</p>
<p>The observed performance gradient, from near-chance levels in STA-VI to human-surpassing in CoT-TI, demonstrates the potential of VLLMs to exhibit human-like cognitive flexibility under appropriate conditions, while also highlighting the critical impact of prompting strategies and input modalities on their performance in tasks requiring set-shifting and rule adaptation.</p>
<p>To investigate whether VLLMs' performance might be influenced by memorization of similar tasks in their training data, we evaluated all three models on our novel ALIEN Task variant under both STA-TI and CoT-TI conditions.The results, presented in Figure s-1 and Table t-1, demonstrate performance patterns remarkably consistent with those observed in the original WCST.Under the STA-TI condition, all models struggled to complete the ALIEN Task, while under the CoT-TI condition, all models demonstrated high performance levels that closely mirrored their achievements on the standard WCST.This consistent performance strongly suggests that the models' capabilities reflect genuine cognitive flexibility rather than memorization of specific task patterns.</p>
<p>B. Detailed Analysis by Evaluation Metric</p>
<p>To offer a comprehensive assessment of the VLLMs' performance on the WCST, we analyzed six key metrics outlined in the previous section.Table I presents the mean scores and standard deviations across all evaluation metrics for each VLLM and condition.This analysis reveals distinct patterns in cognitive flexibility and set-shifting abilities among the models.</p>
<p>PE were most prevalent in the STA-TI condition for all models, with Claude-3.5Sonnet showing the highest number of errors in this condition (µ = 15.90, σ = 17.53).In the STA-VI condition, PE were relatively low for all models, as they largely failed to follow the rules at all.However, the transition to CoT conditions reduced PE, with Claude-3.5Sonnet demonstrated the lowest number of PE in the CoT-TI condition (µ = 6.30, σ = 0.82).This suggests that Claude-3.5Sonnet may surpass human performance in adapting to changing rules, especially when provided with explicit reasoning prompts and textual descriptions.</p>
<p>NPE showed a dramatic reduction from STA to CoT conditions across all models, with improvements observed in the   All models required the fewest trials to complete the first category in the CoT-TI condition, with Claude-3.5Sonnet performing best (µ = 12.00, σ = 0.94), followed closely by GPT-4o (µ = 12.60, σ = 2.46) and Gemini-1.5Pro (µ = 13.30,σ = 1.95).Notably, Claude-3.5Sonnet outperformed the human baseline (µ = 12.93, σ = 1.62).</p>
<p>CLR patterns showed substantial improvement from STA to</p>
<p>CoT conditions for all VLLMs, with the highest percentages observed in the CoT-TI condition.Claude-3.5Sonnet achieved the highest CLR in this condition (µ = 67.50%,σ = 2.74%), followed by GPT-4o (µ = 63.28%,σ = 5.86%) and Gemini-1.5Pro (µ = 63.12%,σ = 4.67%).This indicates that under CoT-TI conditions, VLLMs can maintain conceptual understanding at a level comparable to or exceeding human capability.</p>
<p>FMS were generally low in STA conditions, but this reflects the models' overall poor performance rather than true set maintenance.The transition to CoT conditions led to increased FMS in the VI condition, suggesting that improved overall performance paradoxically led to more instances of set loss after initial successful rule application.However, in the CoT-TI condition, Claude-3.5Sonnet achieved perfect set maintenance (FMS = 0.00), outperforming the human baseline.This indicates that VLLMs can maintain exceptional consistency in rule application, potentially surpassing human capabilities in this aspect of cognitive flexibility.</p>
<p>These detailed metrics collectively reinforce the finding that CoT prompting, particularly when combined with textual inputs, substantially enhances VLLMs' cognitive flexibility as measured by WCST performance.While all models showed similar patterns across conditions, Claude-3.5Sonnet consistently demonstrated superhuman cognitive flexibility in CoT-TI condition.The consistent pattern across all six metrics highlights the robustness of the effects of prompting strategy and input modality, while also revealing subtle differences in the cognitive capabilities of these VLLMs.</p>
<p>C. Analysis of Input modality</p>
<p>To investigate the performance difference between visual and textual input conditions, we conducted a detailed analysis of each model's ability to accurately perceive and interpret the WCST card features.This analysis aimed to determine whether the performance gap was due to limitations in visual processing or differences in cognitive flexibility across modalities.We evaluated the models' accuracy in identifying the three key features of WCST cards: color, shape, and number, comparing their descriptions against actual card features for each trial (detailed in Appendix -E).</p>
<p>The results indicate that all three models demonstrated high accuracy in visual feature recognition (Table II).Claude-3.5Sonnet demonstrated perfect accuracy across all features, while Gemini-1.5Pro and GPT-4o showed a decline in visual capabilities, particularly when recognizing how many cards were present in the image and the number of shapes on each card.Notably, GPT-4o almost always misidentified 5 cards as 6 cards.These findings suggest that the performance gap between VI and TI conditions is not solely attributable to limitations in visual feature extraction, but rather to the cascading effects of occasional visual misinterpretations on higher-order cognitive processes.In the VI condition, visual recognition errors can disrupt the model's ability to consistently apply a rule, necessitating re-exploration of the problem space.This phenomenon explains the increased variance observed in model performance under the CoT-VI condition compared to CoT-TI.The textual input's inherent precision eliminates this source of variability, allowing models to demonstrate more consistent cognitive flexibility.These results reveal the complex interplay between visual perception and executive function in VLLMs, highlighting the need for more robust visual processing pipelines.</p>
<p>D. Impact of Explicit Rule Exclusivity</p>
<p>All results in previous analyses were obtained under conditions that included both a general rule statement specifying " The correct answer depends on a rule, which will be based solely on either the number of shapes, the color of the shapes, or the shape type itself " and an explicit rule exclusivity constraint stating " There will be no combination of these characteristics to define the rule ".To further investigate the robustness of VLLMs' cognitive flexibility, we conducted an additional study examining performance when explicit rule exclusivity was removed.We examined this under the CoT-TI condition, which had previously demonstrated near-human or superior cognitive flexibility for all models.The results are in Table III.Gemini-1.5Pro exhibited the most pronounced sensitivity to the absence of explicit rule exclusivity, with mean CC decreasing from 4.8 (σ = 0.42) with the constraint to 2.6 (σ = 2.01) without it, representing a 2.2 decline.GPT-4o demonstrated moderate sensitivity, with performance dropping from 4.6 (σ = 0.84) to 3.5 (σ = 1.27) categories.Claude-3.5Sonnet showed the most robust performance, maintaining high functionality even without the explicit exclusivity statement, with only a marginal decline from perfect performance (µ = 5.0, σ = 0.00) to near-perfect (µ = 4.7, σ = 0.67).The observed increases in standard deviations across all models when the exclusivity constraint was removed indicate that explicit rule exclusivity not only enhances performance but also promotes more consistent cognitive flexibility.</p>
<p>The differential declines observed among models reflect disparities in their ability to maintain simple rule structures in the absence of explicit constraints against more complex possibilities.Claude-3.5Sonnet's robust performance suggests a superior ability to infer and adhere to simpler rule structures, even when the possibility of more complex rules is not explicitly excluded.</p>
<p>E. Simulating Cognitive Impairment</p>
<p>To explore the potential of VLLMs in modeling human cognitive impairment without modifying the models, we employed role-playing prompts to simulate three key aspects of the PFC function commonly impaired in various neurological conditions [41], [42]: goal maintenance, inhibitory control, and adaptive updating.This method leverages the models' ability to imagine and simulate different cognitive states, allowing us to study how they conceptualize and perform under various impairment conditions without modifying the underlying model architecture.</p>
<p>We focused on the CoT-TI condition, as it consistently yielded the best performance across all models in our previous  IV, reveal that all three VLLMs demonstrated significant performance decrements under simulated impairment conditions, with patterns that align with neuropsychological observations of patients with prefrontal dysfunction.Gemini-1.5Pro exhibited the highest sensitivity to simulated impairments, with substantial declines in CC across all conditions.The most severe impact was observed under inhibitory control impairment (CC reduced from 4.80 to 1.70), accompanied by a marked increase in NPE from 3.50 to 20.10.</p>
<p>GPT-4o demonstrated greater resilience, maintaining relatively stable performance across impairment conditions.The model's CC decreased modestly from 4.60 to 3.50-4.30,with PE showing consistent increases across conditions.Notably, NPE remained stable, indicating a robust ability to maintain overall response consistency even under simulated impairments.This stability suggests that GPT-4o's decision-making processes may be more resistant to perturbation.Claude-3.5Sonnet, despite showing the highest baseline performance (CC = 5.00), exhibited significant vulnerability to simulated impairments.The model showed increases in both PE and NPE under impairment conditions, particularly for inhibitory control (PE: 12.80, NPE: 18.70).This pattern suggests that Claude-3.5Sonnet's high baseline performance may rely on finely tuned cognitive processes that are more susceptible to disruption when specific aspects of executive function are impaired.</p>
<p>Across all models, inhibitory control impairment consistently produced the most severe performance decrements and led to increased NPE and FMS, aligning with observations in patients with orbitofrontal damage [43].Models frequently mentioned irrelevant card features, simulating distraction and impulsivity.Goal maintenance impairment primarily affected CLR and FMS, reflecting difficulties in consistently applying rules.This pattern is consistent with observations in patients with dorsolateral prefrontal cortex lesions [44].Adaptive updating impairment had a more moderate impact, mainly affecting CC and CLR, while having less effect on FMS, consistent with difficulties in switching to new rules [45].These distinct patterns of impairment across models suggest that while VLLMs can simulate aspects of cognitive dysfunction, the underlying mechanisms of their decision-making processes may differ.</p>
<p>V. DISCUSSION AND CONCLUSION</p>
<p>This study demonstrates that state-of-the-art VLLMs can achieve, and in some cases surpass, human-level cognitive flexibility as measured by the WCST, suggesting a potential for emulating and exceeding human set-shifting abilities in specific contexts.The observed performance gradient underscores the complex interplay between input modalities and prompting strategies.The consistent performance across both the standard WCST and our novel ALIEN Task variant provides compelling evidence that these models demonstrate genuine cognitive flexibility rather than relying on memorized patterns.Despite the complete transformation of surface features, the models exhibited virtually identical performance patterns across conditions, suggesting engagement with the underlying logical structure of set-shifting tasks rather than merely recalling similar examples from their training data.The performance gap between VI and TI conditions indicates that current VLLMs may rely more heavily on language-based reasoning pathways, even when processing visual information.Explicit reasoning prompts enable VLLMs to maintain more stable internal representations of task rules.</p>
<p>Our analysis of explicit rule exclusivity reveals a critical dependence of VLLMs on precise task instructions.The significant performance decline observed when specific rule constraints were removed highlights the models' reliance on explicit information to guide their decision-making processes.This finding suggests that VLLMs' impressive performance in structured tasks may not fully generalize to more ambiguous real-world scenarios without careful consideration of instruction design.</p>
<p>The simulation of cognitive impairments through roleplaying prompts demonstrates the potential of VLLMs to model complex behavioral patterns of executive dysfunction.The distinct performance profiles observed under simulated goal maintenance, inhibitory control, and adaptive updating impairments closely mirror behavioral patterns seen in human neuropsychological research.However, as recent research has shown that language models can form biased associations that mirror societal stereotypes , it is important to acknowledge that VLLMs' simulations of cognitive impairments may reflect stereotypical representations of clinical populations rather than authentic mechanisms underlying the disorders themselves.</p>
<p>Future research should focus on elucidating the underlying mechanisms that enable VLLMs to perform set-shifting tasks and investigating the generalizability of these abilities to other domains of executive function.Additionally, the development of more sophisticated visual processing capabilities and the integration of multimodal information processing warrant further exploration.The potential of VLLMs to simulate specific patterns of cognitive impairment also opens up new possibilities for creating realistic models of neuropsychological conditions, which could have applications in both clinical research and AI safety.By analyzing VLLMs' internal representations during simulated impairments, we could potentially decode the computational principles underlying various cognitive functions, complementing traditional neuroscience methods.</p>
<p>In conclusion, this study provides insights into our understanding of cognitive flexibility in VLLMs, revealing capabilities that match or exceed human performance and important limitations that depend on task framing and input modalities.As these models continue to evolve, a deeper understanding of their cognitive processes will be crucial for harnessing their potential while addressing their constraints, ultimately leading to more adaptive and robust AI systems that can flexibly navigate complex, real-world environments.</p>
<p>MODEL DETAILS</p>
<p>This study employs three state-of-the-art visual language models: Gemini-1.5Pro, GPT-4o, and Claude-3.5Sonnet.Table A1 presents a comparative overview of these models' key characteristics.</p>
<p>A. Gemini-1.5Pro Gemini-1.5Pro, created by Google, employs a Mixture of Experts architecture, allowing for efficient processing of both textual and visual inputs.While specific architectural details are proprietary, the model demonstrates strong performance across various tasks.For image processing, Gemini-1.5Pro uses a standardized approach where each image is equivalent to 258 tokens, regardless of size.Large images are scaled down to a maximum of 3072x3072 pixels, while small images are scaled up to 768x768 pixels, both preserving aspect ratio.</p>
<p>B. GPT-4o</p>
<p>GPT-4o, developed by OpenAI, represents an advanced iteration of the GPT series.It utilizes a transformer-based architecture and incorporates visual processing capabilities.GPT-4o offers adaptive image processing with low and high resolution modes, allowing for a balance between processing speed and detail level.In low resolution mode, it processes a 512px x 512px version of the image, representing it with 85 tokens.The high resolution mode initially processes a lowres image, then creates detailed 512px x 512px crops, each represented by 170 tokens.</p>
<p>PROMPTS AND TOKEN USAGE</p>
<p>D. Detailed prompts</p>
<p>The WCST setup consists of four stimulus cards, each featuring unique combinations of color (red, green, yellow, blue), shape (triangle, star, cross, circle), and number (one,
I(f ij = f * ij ) 64 * 5 × 100%(8)
where f eature ∈ color, shape, number, f ij is the model's identification of the feature for card j in trial i, and f * ij is the correct feature.</p>
<p>The Overall Accuracy (ACC overall ) was computed as a composite score, incorporating all correct identifications while applying a penalty for overcounting cards.First, we define a penalty function P for overcounting:
P = 64 i=1 0.5 × max(0, c i − 5) 64 × 100%(9)
where c i is the number of cards counted by the model in trial i.This penalty deducts 0.5 points for each card counted beyond the correct number of 5 in any given trial.</p>
<p>The Overall Accuracy is then calculated as:
ACC overall = ACC count + ACC f eature − P(10)
This assessment of the VLLMs' visual processing capabilities enables detailed comparisons across models and features.By evaluating multiple aspects of visual perception, from basic counting to complex feature recognition, the system offered insights into the strengths and limitations of each model's visual cognition in the context of the WCST.</p>
<p>F. Token Usage</p>
<p>To provide insight into the computational resources required, we list token usage across models and conditions (Table VI).Across all models, VI conditions consistently required more tokens than TI conditions, reflecting the additional computational demand of processing visual information.CoT conditions consumed significantly more tokens than STA conditions, indicating the increased computational cost of explicit reasoning processes.Among the models, Claude-3.5Sonnet showed the highest token usage across all conditions, suggesting a more computationally intensive approach to task processing.These token usage patterns provide valuable insights into the relative efficiency and resource requirements of different VLLMs and experimental conditions in cognitive flexibility tasks.</p>
<p>Figure s- 2 .
2
Example stimuli for VI and TI conditions, and prompt templates for STA and CoT strategies are provided in supplementary Figure s-3.</p>
<p>Fig. 2 .
2
Fig. 2. WCST Task Performance Across Models and Conditions.The distribution of standardized Categories Completed (CC) scores for GPT-4o, Gemini-1.5Pro, and Claude-3.5Sonnet under four experimental conditions: STA-VI (Straight to Answer -Visual Input), STA-TI (Straight to Answer -Textual Input), CoT-VI (Chain of Thought -Visual Input), and CoT-TI (Chain of Thought -Textual Input), with the human baseline performance.</p>
<p>C. Claude- 3 . 5 Sonnet
35
Developed by Anthropic, Claude-3.5Sonnet builds upon previous Claude models, incorporating enhanced visual understanding capabilities.The model utilizes a transformer-based architecture optimized for multimodal inputs.Claude-3.5Sonnet balances multi-image processing by resizing images that exceed 1568 pixels on the long edge or approximately 1,600 tokens.It calculates token usage based on image dimensions (tokens = (width px * height px)/750) and emphasizes image clarity and text legibility for optimal performance.</p>
<p>TABLE IV WCST
IV
PERFORMANCE UNDER NORMAL AND SIMULATED IMPAIRMENT CONDITIONS (COT-TI)
ModelConditionCCPENPETFCCLR (%)FMSNormal4.80 (0.42)6.80 (1.55)3.50 (1.72)13.30 (1.95)63.12 (4.67)0.10 (0.32)Gemini-1.5 ProGoal Maint. (↓) Inhib. Ctrl. (↓)1.90 (1.60) 1.70 (1.49)8.60 (7.83) 6.40 (5.66)12.00 (12.38) 20.10 (14.04)16.86 (-) 32.00 (-)37.03 (22.39) 30.63 (18.91) 0.90 (0.99) 0.60 (0.70)Adapt. Upd. (↓) 3.90 (0.74)8.30 (2.71)6.70 (4.57)17.50 (6.20)56.09 (9.53)0.00 (0.00)Normal4.60 (0.84)7.60 (1.84)2.10 (0.88)12.60 (2.46)63.28 (5.86)0.10 (0.32)GPT-4oGoal Maint. (↓) Inhib. Ctrl. (↓)3.50 (1.65) 4.20 (1.03)9.80 (2.66) 10.10 (6.66)4.50 (3.78) 3.70 (2.41)18.10 (9.46) 14.00 (3.50)52.34 (16.49) 57.97 (11.66) 0.30 (0.48) 0.80 (1.03)Adapt. Upd. (↓) 4.30 (0.82)8.30 (3.13)2.80 (2.49)13.10 (1.79)61.56 (9.24)0.10 (0.32)Normal5.00 (0.00)6.30 (0.82)2.00 (0.82)12.00 (0.94)67.50 (2.74)0.00 (0.00)Claude-3.5 SonnetGoal Maint. (↓) Inhib. Ctrl. (↓)3.20 (1.40) 1.50 (1.65) 12.80 (13.82) 18.70 (19.82) 12.50 (5.64) 5.50 (4.79)17.50 (7.20) 18.83 (-)47.19 (17.44) 23.59 (20.75) 0.40 (0.52) 0.60 (0.84)Adapt. Upd. (↓) 3.60 (1.26)8.60 (4.93)7.50 (9.35)20.60 (12.55) 51.56 (14.91)0.00 (0.00)experiments. The specific role-playing prompts and analysismethods for this component are detailed in supplementaryFigure s-6. The results presented in Table
ACKNOWLEDGMENTSThis work was supported in part by the Strategic Priority Research Program of the Chinese Academy of Sciences (CAS)(XDB1010302), CAS Project for Young Scientists in Basic Research, Grant No. YSBR-041 and the International Partnership Program of the Chinese Academy of Sciences (CAS) (173211KYSB20200021).CODE AVAILABILITY The complete code and experimental data are available at: https://drive.google.com/file/d/1uUyPn6fP4JDI50zRcdMeGJKwRaI-JTUs/view?usp= sharing.two, three, four) of symbols.A series of 64 response cards is used, each sharing properties with the stimulus cards but in different combinations.The sorting rules are based on three possible categories: color, shape, or number.In our implementation, each trial is presented to the VLLMs as a single image containing two rows.The top row displays the four stimulus cards, while the bottom row shows one response card.This format is consistent across all 64 trials, providing a standardized visual input for the models to process.For the text-based conditions (TI), detailed descriptions of these images are provided instead.The test procedure begins with the VLLM being instructed to match each response card to one of the stimulus cards based on a rule that it must deduce from feedback.After each match attempt, the VLLM receives feedback (correct or incorrect) without explicit mention of the current sorting rule.The sorting rule changes after 10 consecutive correct matches, without notification to the VLLM.The test concludes after all 64 cards have been presented.To explore the VLLMs' capacity to simulate cognitive impairments, we introduced role-playing scenarios as described in supplementary Figures-6.This figure outlines the specific instructions given to models for simulating various prefrontal cortex dysfunctions, including impaired goal maintenance, inhibitory control deficits, and adaptive updating impairments.These scenarios were carefully designed to mimic specific cognitive deficits commonly observed in neurological conditions, allowing us to assess the models' ability to flexibly adapt their behavior to simulate human-like cognitive impairments.Supplementary Figure s-7 provides a detailed example of a typical VLLM interaction during the WCST.This example illustrates how models process the presented cards, articulate their reasoning (in CoT-TI conditions), and make decisions.Supplementary Figures-8showcases the visual processing capabilities of VLLMs.E. Visual Accuracy CalculationThe visual accuracy of VLLMs was assessed using a comprehensive scoring system that evaluated their ability to correctly identify key features of the WCST cards across 64 trials.The system encompassed five distinct measures: Card Count Accuracy, Color Accuracy, Shape Accuracy, Number Accuracy, and Overall Accuracy.For each trial, models were evaluated on their ability to correctly identify the presence of five cards and accurately describe the color, shape, and number of symbols on each card.Detailed descriptions of the Visual instructions are provided in supplementary Count Accuracy was calculated as the proportion of trials where the model correctly identified the presence of five cards:where I(c i = 5) is an indicator function that equals 1 if the model correctly counted 5 cards in trial i, and 0 otherwise.Color Accuracy , Shape Accuracy , and Number Accuracy were calculated similarly, assessing the model's performance across all cards in all trials:
Demystifying cognitive flexibility: Implications for clinical and developmental neuroscience. D R Dajani, L Q Uddin, Trends in neurosciences. 3892015</p>
<p>Cognitive and behavioural flexibility: neural mechanisms and clinical considerations. L Q Uddin, Nature Reviews Neuroscience. 2232021</p>
<p>Exploring the nature of cognitive flexibility. T Ionescu, New ideas in psychology. 3022012</p>
<p>Prefrontal deep projection neurons enable cognitive flexibility via persistent feedback monitoring. T Spellman, M Svei, J Kaminsky, G Manzano-Nieves, C Liston, Cell. 184102021</p>
<p>Prefrontal cortex and neural mechanisms of executive function. S Funahashi, J M Andreau, Journal of Physiology-Paris. 10762013</p>
<p>Large language models are visual reasoning coordinators. L Chen, B Li, S Shen, J Yang, C Li, K Keutzer, T Darrell, Z Liu, Advances in Neural Information Processing Systems. 202436</p>
<p>Evaluating cognitive maps and planning in large language models with cogeval. I Momennejad, H Hasanbeig, F Vieira Frujeri, H Sharma, N Jojic, H Palangi, R Ness, J Larson, Advances in Neural Information Processing Systems. 202436</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, ACM Transactions on Intelligent Systems and Technology. 1532024</p>
<p>Integration of cognitive tasks into artificial general intelligence test for large models. Y Qu, C Wei, P Du, W Che, C Zhang, W Ouyang, Y Bian, F Xu, B Hu, K Du, Iscience. 2742024</p>
<p>Hello gpt-4o. Openai, 2024</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. M Reid, N Savinov, D Teplyashin, D Lepikhin, T Lillicrap, J -B. Alayrac, R Soricut, A Lazaridou, O Firat, J Schrittwieser, arXiv:2403.055302024arXiv preprint</p>
<p>Anthropic, Announcements: Claude 3.5 sonnet. 2024</p>
<p>A simple objective technique for measuring flexibility in thinking. E A Berg, The Journal of general psychology. 3911948</p>
<p>Considerations for using the wisconsin card sorting test to assess cognitive flexibility. S Miles, C A Howlett, C Berryman, M Nedeljkovic, G L Moseley, A Phillipou, Behavior research methods. 5352021</p>
<p>The wcst-64: A standardized short-form of the wisconsin card sorting test. K W Greve, The Clinical Neuropsychologist. 1522001</p>
<p>Wisconsin card sorting test as a measure of executive function impairments in stroke patients. K Jodzio, D Biechowska, Applied neuropsychology. 1742010</p>
<p>Using fmri to decompose the neural processes underlying the wisconsin card sorting test. C.-H Lie, K Specht, J C Marshall, G R Fink, Neuroimage. 3032006</p>
<p>Common and distinct mechanisms of cognitive flexibility in prefrontal cortex. C Kim, N F Johnson, S E Cilles, B T Gold, Journal of Neuroscience. 31132011</p>
<p>Flexible adjustment of the effective connectivity between the fronto-parietal and visual regions supports cognitive flexibility. L Qiao, M Xu, X Luo, L Zhang, H Li, A Chen, NeuroImage. 2201171582020</p>
<p>Executive functions. A Diamond, Annual Review of Psychology. 6412013</p>
<p>On the historical and conceptual background of the wisconsin card sorting test. P Eling, K Derckx, R Maes, Brain and cognition. 6732008</p>
<p>Adult clinical neuropsychology: lessons from studies of the frontal lobes. D T Stuss, B Levine, Annual review of psychology. 5312002</p>
<p>The dimensional change card sort (dccs): A method of assessing executive function in children. P D Zelazo, Nature protocols. 112006</p>
<p>Emotional processing and executive functions in major depressive disorder: dorsal prefrontal activity correlates with performance in the intra-extra dimensional set shift. A Heinzel, G Northoff, H Boeker, P Boesiger, S Grimm, Acta neuropsychiatrica. 2262010</p>
<p>Testing theory of mind in large language models and humans. J W Strachan, D Albergo, G Borghini, O Pansardi, E Scaliti, S Gupta, K Saxena, A Rufo, S Panzeri, G Manzi, Nature Human Behaviour. 2024</p>
<p>An empirical investigation of the impact of chatgpt on creativity. B C Lee, J Chung, Nature Human Behaviour. 2024</p>
<p>B Fatemi, M Kazemi, A Tsitsulin, K Malkan, J Yim, J Palowitch, S Seo, J Halcrow, B Perozzi, arXiv:2406.09170Test of time: A benchmark for evaluating llms on temporal reasoning. 2024arXiv preprint</p>
<p>Vision language models are blind. P Rahmanzadehgervi, L Bolton, M R Taesiri, A T Nguyen, arXiv:2407.065812024arXiv preprint</p>
<p>Challenging chatgpt'intelligence'with human tools: a neuropsychological investigation on prefrontal functioning of a large language model. R Loconte, G Orru, M Tribastone, P Pietrini, G Sartori, Intelligence. 2023</p>
<p>Using large language models to simulate multiple humans and replicate human subject studies. G V Aher, R I Arriaga, A T Kalai, International Conference on Machine Learning. PMLR2023</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, ACM Computing Surveys. 5592023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>A survey of chain of thought reasoning: Advances, frontiers and future. Z Chu, J Chen, Q Chen, W Yu, T He, H Wang, W Peng, M Liu, B Qin, T Liu, arXiv:2309.154022023arXiv preprint</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, S Narang, A Chowdhery, D Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>M 3 cot: A novel benchmark for multi-domain multi-step multi-modal chain-ofthought. Q Chen, L Qin, J Zhang, Z Chen, X Xu, W Che, arXiv:2405.164732024arXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q Le, arXiv:2205.106252022arXiv preprint</p>
<p>Automatic prompt optimization with" gradient descent" and beam search. R Pryzant, D Iter, J Li, Y T Lee, C Zhu, M Zeng, arXiv:2305.034952023arXiv preprint</p>
<p>Large language models are human-level prompt engineers. Y Zhou, A I Muresanu, Z Han, K Paster, S Pitis, H Chan, J Ba, arXiv:2211.019102022arXiv preprint</p>
<p>D J Schretlen, Modified Wisconsin Card Sorting Test®: M-WCST. </p>
<p>. Professional Manual. PAR. 2010</p>
<p>An integrative theory of prefrontal cortex function. E K Miller, J D Cohen, Annual review of neuroscience. 2412001</p>
<p>Is there a dysexecutive syndrome?. D T Stuss, M P Alexander, Philosophical Transactions of the Royal Society B: Biological Sciences. 36214812007</p>
<p>The involvement of orbitofrontal cerebrum in cognitive tasks. D T Stuss, D Benson, W Weir, M Naeser, I Lieberman, D Ferrill, Neuropsychologia. 2131983</p>
<p>Wisconsin card sorting test performance in patients with focal frontal and posterior brain damage: effects of lesion location and test structure on separable cognitive processes. D Stuss, B Levine, M Alexander, J Hong, C Palumbo, L Hamer, K Murphy, D Izukawa, Neuropsychologia. 3842000</p>
<p>Effects of different brain lesions on card sorting: The role of the frontal lobes. B Milner, Archives of neurology. 911963</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6680 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6680</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6680</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-128.html">extraction-schema-128</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <p><strong>Paper ID:</strong> paper-d60f0bc0be7b8ae8e049ceb906530ba7744229c5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d60f0bc0be7b8ae8e049ceb906530ba7744229c5" target="_blank">Deep Reinforcement Learning with Stacked Hierarchical Attention for Text-based Games</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a stacked hierarchical attention mechanism to construct an explicit representation of the reasoning process by exploiting the structure of the knowledge graph for decision making in reinforcement learning.</p>
                <p><strong>Paper Abstract:</strong> We study reinforcement learning (RL) for text-based games, which are interactive simulations in the context of natural language. While different methods have been developed to represent the environment information and language actions, existing RL agents are not empowered with any reasoning capabilities to deal with textual games. In this work, we aim to conduct explicit reasoning with knowledge graphs for decision making, so that the actions of an agent are generated and supported by an interpretable inference procedure. We propose a stacked hierarchical attention mechanism to construct an explicit representation of the reasoning process by exploiting the structure of the knowledge graph. We extensively evaluate our method on a number of man-made benchmark games, and the experimental results demonstrate that our method performs better than existing text-based agents.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6680.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6680.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SHA-KG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stacked Hierarchical Attention with Knowledge Graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL agent that constructs and maintains a knowledge graph from textual observations, splits the KG into relational/temporal sub-graphs, encodes graphs with GATs and uses a two-level (text->KG) stacked hierarchical attention mechanism to form state representations for A2C-based policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Deep Reinforcement Learning with Stacked Hierarchical Attention for Text-based Games</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SHA-KG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Maintains an explicit Knowledge Graph (KG) that records triples extracted from text observations; divides the KG into multiple sub-graphs (connectivity, objects in current room, inventory, graph-without-you) to capture relational and temporal awareness; encodes full KG and sub-graphs with Graph Attention Networks (GATs); forms a high-level query from full KG+score to attend over textual components, updates the query, then uses a low-level attention over sub-graphs to produce a final state vector; actions decoded via template- and object-GRUs and trained with A2C plus a supervised 'valid action prediction' auxiliary loss.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho (man-made interactive fiction games; examples include Zork1, detective, ztuu, pentari, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>structured Knowledge Graph (explicit episodic/semantic memory split into sub-graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Triples (Subject, Relation, Object) extracted from textual observations via OpenIE plus simple domain rules; sub-graphs store room connectivity, objects in current room, objects in inventory, and a graph without 'you' links (history-containing graphs vs present-only graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Rule-based GraphUpdate: newly extracted triples from current textual observation are merged into the previous KG; two handcrafted rules (linking inventory items to 'you' or room; inferring room connectivity from navigational actions) are applied; graph partitioning into sub-graphs is done by predefined rules.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>GAT-based graph encoding to obtain node and graph embeddings plus stacked hierarchical attention: (1) encode full KG via GATs -> build high-level query that attends over textual components (multiple group/channel attention); (2) updated query attends over encoded sub-graphs (low-level attention) to select which sub-graph(s) to use; node-level attention from GATs used to highlight important nodes within a chosen sub-graph; a graph mask from full KG is also used to constrain action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (Advantage Actor Critic, A2C) with an auxiliary supervised 'valid action prediction' loss; trained across 32 parallel environments for 1e6 interactions per game.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Raw (game) score reported per game (average over five independent runs; for training curves average raw score over last 100 finished episodes is reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>SHA-KG (with its KG + sub-graph memory and stacked hierarchical attention) achieves improved raw scores over baselines on many games (Table 1). Examples from Table 1: 'detective' SHA-KG = 308.0 (new SOTA), 'pentari' = 51.3, 'zork1' = 34.5, 'ztuu' = 25.2; the paper states SHA-KG achieves new state-of-the-art in 8 games and is equal to best in 7 more.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>The paper includes ablations removing historical information ('w/o history') or other KG-related components; results vary by game and are presented qualitatively and as learning curves rather than a single numeric aggregate. Removing history or sub-graph structure can reduce performance or sample efficiency in some games, but the effect is game-dependent (no single numeric overall decline reported in the main text).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Several ablations target the memory and attention design: (1) 'w/o low-level' (no sub-graph attention) uses full KG only and shows reduced sample efficiency in some games; (2) 'w/o high-level' (no full KG guiding textual attention) sometimes improves sample efficiency on some games but fails on others — effects are game-dependent; (3) 'w/o GroupAttn' (single attention per channel instead of multiple groups) performs similarly but worse than SHA-KG, indicating multi-group/channel attention captures finer-grained info; (4) sub-graph type ablations ('w/o relational-awareness', 'w/o temporal-awareness', 'w/o history') show differing impacts across games, implying the combination of relational and temporal sub-graphs is beneficial overall but contributions vary per game. The authors report improved interpretability using top-25-sum aggregation of attention values.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Graph partitioning into sub-graphs is rule-based (handcrafted) and left for future work to automate; attention-based explanations have known controversies (authors acknowledge attention is not a guaranteed explanation); no unified numerical summary of how much historical memory alone contributes across all games — effects are task-dependent. Computational cost considerations (reduced embedding sizes to save GPU) are mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Use an explicit KG as structured memory for partial observability; partition the KG into relational and temporal sub-graphs to capture different semantics; encode graphs with GATs and use a stacked hierarchical attention (text-guided then sub-graph-guided) to retrieve relevant memory; compute multiple groups of channel-wise attention (group attention) for finer-grained focus; aggregate multi-head/group attention via top-25-sum for interpretability; include a supervised 'valid action prediction' auxiliary loss to stabilize RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning with Stacked Hierarchical Attention for Text-based Games', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6680.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6680.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Constrained Reinforcement Learning for Natural Language Action Spaces (KG-A2C)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A template-based RL agent that constructs a single dynamic Knowledge Graph from observations, encodes it with GATs and uses it as part of the state for A2C policy learning, with a supervised valid-action prediction auxiliary task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph constrained reinforcement learning for natural language action spaces</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Deep Reinforcement Learning with Stacked Hierarchical Attention for Text-based Games</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Constructs a single Knowledge Graph from OpenIE-extracted triples plus simple rules (inventory links to 'you', infer room connectivity from movement), encodes the graph with Graph Attention Networks to obtain graph embeddings, uses these embeddings together with textual observations and scores as state for an A2C agent; applies an auxiliary 'valid action prediction' supervised task.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho (man-made interactive fiction games)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>single Knowledge Graph (dynamic structured memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Triples (Subject, Relation, Object) extracted from each text observation using OpenIE plus two domain rules (link inventory items to 'you'; infer room connectivity from navigation actions). The KG records objects, room connectivity and history (single graph).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Rule-based GraphUpdate: newly extracted triples are merged into the previous KG; room connectivity inferred from navigation; inventory items linked by rule.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>GAT-based graph encoding to obtain graph/node embeddings; full KG embedding used as part of the state and a graph-derived mask ('graph mask') is used to constrain/score template/object decoding. No sub-graph attention decomposition (single KG encoding).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (A2C) with a supervised 'valid action prediction' auxiliary loss.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Raw game score per game (averaged over runs), reported in Table 1 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported per-game scores in Table 1. Representative examples from the paper: 'detective' KG-A2C = 207.9, 'pentari' = 50.7, 'zork1' = 34.0, 'ztuu' = 9.2. KG-A2C is used as the backbone baseline for SHA-KG.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>The paper compares KG-A2C to baselines that do not use an explicit KG (e.g., TDQN, DRRN, NAIL). Those baselines sometimes outperform KG-A2C on particular games, but KG-A2C generally benefits from the KG for partial observability and action masking. No direct ablation in this paper removes the KG from KG-A2C itself, but the paper's SHA-KG ablations compare single full-KG usage vs sub-graph decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Within this paper, KG-A2C is compared to SHA-KG and its variants; results indicate that using a single KG (as in KG-A2C) is improved upon by SHA-KG's sub-graph decomposition and stacked hierarchical attention. The authors note that sub-graph division and hierarchical attention provide complementary improvements (better sample efficiency and higher final scores on many games).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Single KG representation conflates relational and temporal information and may make fine-grained distinctions (e.g., newness/oldness of facts, relation types) hard to preserve; prior KG-A2C lacks an explicit reasoning procedure over sub-graphs which SHA-KG aims to remedy.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Use a dynamic KG to handle partial observability and to build graph-based action masks; combine KG embeddings with textual observations; include auxiliary supervised valid-action prediction to aid RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning with Stacked Hierarchical Attention for Text-based Games', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Building dynamic knowledge graphs from text-based games <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Enhancing text-based reinforcement learning agents with commonsense knowledge <em>(Rating: 2)</em></li>
                <li>Learning dynamic knowledge graphs to generalize on text-based games <em>(Rating: 2)</em></li>
                <li>Interactive fiction games: A colossal adventure <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6680",
    "paper_id": "paper-d60f0bc0be7b8ae8e049ceb906530ba7744229c5",
    "extraction_schema_id": "extraction-schema-128",
    "extracted_data": [
        {
            "name_short": "SHA-KG",
            "name_full": "Stacked Hierarchical Attention with Knowledge Graphs",
            "brief_description": "An RL agent that constructs and maintains a knowledge graph from textual observations, splits the KG into relational/temporal sub-graphs, encodes graphs with GATs and uses a two-level (text-&gt;KG) stacked hierarchical attention mechanism to form state representations for A2C-based policy learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Deep Reinforcement Learning with Stacked Hierarchical Attention for Text-based Games",
            "agent_name": "SHA-KG",
            "agent_description": "Maintains an explicit Knowledge Graph (KG) that records triples extracted from text observations; divides the KG into multiple sub-graphs (connectivity, objects in current room, inventory, graph-without-you) to capture relational and temporal awareness; encodes full KG and sub-graphs with Graph Attention Networks (GATs); forms a high-level query from full KG+score to attend over textual components, updates the query, then uses a low-level attention over sub-graphs to produce a final state vector; actions decoded via template- and object-GRUs and trained with A2C plus a supervised 'valid action prediction' auxiliary loss.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Jericho (man-made interactive fiction games; examples include Zork1, detective, ztuu, pentari, etc.)",
            "memory_used": true,
            "memory_type": "structured Knowledge Graph (explicit episodic/semantic memory split into sub-graphs)",
            "memory_representation": "Triples (Subject, Relation, Object) extracted from textual observations via OpenIE plus simple domain rules; sub-graphs store room connectivity, objects in current room, objects in inventory, and a graph without 'you' links (history-containing graphs vs present-only graphs)",
            "memory_update_mechanism": "Rule-based GraphUpdate: newly extracted triples from current textual observation are merged into the previous KG; two handcrafted rules (linking inventory items to 'you' or room; inferring room connectivity from navigational actions) are applied; graph partitioning into sub-graphs is done by predefined rules.",
            "memory_retrieval_method": "GAT-based graph encoding to obtain node and graph embeddings plus stacked hierarchical attention: (1) encode full KG via GATs -&gt; build high-level query that attends over textual components (multiple group/channel attention); (2) updated query attends over encoded sub-graphs (low-level attention) to select which sub-graph(s) to use; node-level attention from GATs used to highlight important nodes within a chosen sub-graph; a graph mask from full KG is also used to constrain action selection.",
            "training_method": "Reinforcement learning (Advantage Actor Critic, A2C) with an auxiliary supervised 'valid action prediction' loss; trained across 32 parallel environments for 1e6 interactions per game.",
            "evaluation_metric": "Raw (game) score reported per game (average over five independent runs; for training curves average raw score over last 100 finished episodes is reported).",
            "performance_with_memory": "SHA-KG (with its KG + sub-graph memory and stacked hierarchical attention) achieves improved raw scores over baselines on many games (Table 1). Examples from Table 1: 'detective' SHA-KG = 308.0 (new SOTA), 'pentari' = 51.3, 'zork1' = 34.5, 'ztuu' = 25.2; the paper states SHA-KG achieves new state-of-the-art in 8 games and is equal to best in 7 more.",
            "performance_without_memory": "The paper includes ablations removing historical information ('w/o history') or other KG-related components; results vary by game and are presented qualitatively and as learning curves rather than a single numeric aggregate. Removing history or sub-graph structure can reduce performance or sample efficiency in some games, but the effect is game-dependent (no single numeric overall decline reported in the main text).",
            "has_comparative_results": true,
            "ablation_findings": "Several ablations target the memory and attention design: (1) 'w/o low-level' (no sub-graph attention) uses full KG only and shows reduced sample efficiency in some games; (2) 'w/o high-level' (no full KG guiding textual attention) sometimes improves sample efficiency on some games but fails on others — effects are game-dependent; (3) 'w/o GroupAttn' (single attention per channel instead of multiple groups) performs similarly but worse than SHA-KG, indicating multi-group/channel attention captures finer-grained info; (4) sub-graph type ablations ('w/o relational-awareness', 'w/o temporal-awareness', 'w/o history') show differing impacts across games, implying the combination of relational and temporal sub-graphs is beneficial overall but contributions vary per game. The authors report improved interpretability using top-25-sum aggregation of attention values.",
            "reported_limitations": "Graph partitioning into sub-graphs is rule-based (handcrafted) and left for future work to automate; attention-based explanations have known controversies (authors acknowledge attention is not a guaranteed explanation); no unified numerical summary of how much historical memory alone contributes across all games — effects are task-dependent. Computational cost considerations (reduced embedding sizes to save GPU) are mentioned.",
            "best_practices_recommendations": "Use an explicit KG as structured memory for partial observability; partition the KG into relational and temporal sub-graphs to capture different semantics; encode graphs with GATs and use a stacked hierarchical attention (text-guided then sub-graph-guided) to retrieve relevant memory; compute multiple groups of channel-wise attention (group attention) for finer-grained focus; aggregate multi-head/group attention via top-25-sum for interpretability; include a supervised 'valid action prediction' auxiliary loss to stabilize RL training.",
            "uuid": "e6680.0",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning with Stacked Hierarchical Attention for Text-based Games",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "KG-A2C",
            "name_full": "Graph Constrained Reinforcement Learning for Natural Language Action Spaces (KG-A2C)",
            "brief_description": "A template-based RL agent that constructs a single dynamic Knowledge Graph from observations, encodes it with GATs and uses it as part of the state for A2C policy learning, with a supervised valid-action prediction auxiliary task.",
            "citation_title": "Graph constrained reinforcement learning for natural language action spaces",
            "mention_or_use": "use",
            "paper_title": "Deep Reinforcement Learning with Stacked Hierarchical Attention for Text-based Games",
            "agent_name": "KG-A2C",
            "agent_description": "Constructs a single Knowledge Graph from OpenIE-extracted triples plus simple rules (inventory links to 'you', infer room connectivity from movement), encodes the graph with Graph Attention Networks to obtain graph embeddings, uses these embeddings together with textual observations and scores as state for an A2C agent; applies an auxiliary 'valid action prediction' supervised task.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Jericho (man-made interactive fiction games)",
            "memory_used": true,
            "memory_type": "single Knowledge Graph (dynamic structured memory)",
            "memory_representation": "Triples (Subject, Relation, Object) extracted from each text observation using OpenIE plus two domain rules (link inventory items to 'you'; infer room connectivity from navigation actions). The KG records objects, room connectivity and history (single graph).",
            "memory_update_mechanism": "Rule-based GraphUpdate: newly extracted triples are merged into the previous KG; room connectivity inferred from navigation; inventory items linked by rule.",
            "memory_retrieval_method": "GAT-based graph encoding to obtain graph/node embeddings; full KG embedding used as part of the state and a graph-derived mask ('graph mask') is used to constrain/score template/object decoding. No sub-graph attention decomposition (single KG encoding).",
            "training_method": "Reinforcement learning (A2C) with a supervised 'valid action prediction' auxiliary loss.",
            "evaluation_metric": "Raw game score per game (averaged over runs), reported in Table 1 of the paper.",
            "performance_with_memory": "Reported per-game scores in Table 1. Representative examples from the paper: 'detective' KG-A2C = 207.9, 'pentari' = 50.7, 'zork1' = 34.0, 'ztuu' = 9.2. KG-A2C is used as the backbone baseline for SHA-KG.",
            "performance_without_memory": "The paper compares KG-A2C to baselines that do not use an explicit KG (e.g., TDQN, DRRN, NAIL). Those baselines sometimes outperform KG-A2C on particular games, but KG-A2C generally benefits from the KG for partial observability and action masking. No direct ablation in this paper removes the KG from KG-A2C itself, but the paper's SHA-KG ablations compare single full-KG usage vs sub-graph decomposition.",
            "has_comparative_results": true,
            "ablation_findings": "Within this paper, KG-A2C is compared to SHA-KG and its variants; results indicate that using a single KG (as in KG-A2C) is improved upon by SHA-KG's sub-graph decomposition and stacked hierarchical attention. The authors note that sub-graph division and hierarchical attention provide complementary improvements (better sample efficiency and higher final scores on many games).",
            "reported_limitations": "Single KG representation conflates relational and temporal information and may make fine-grained distinctions (e.g., newness/oldness of facts, relation types) hard to preserve; prior KG-A2C lacks an explicit reasoning procedure over sub-graphs which SHA-KG aims to remedy.",
            "best_practices_recommendations": "Use a dynamic KG to handle partial observability and to build graph-based action masks; combine KG embeddings with textual observations; include auxiliary supervised valid-action prediction to aid RL training.",
            "uuid": "e6680.1",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning with Stacked Hierarchical Attention for Text-based Games",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Building dynamic knowledge graphs from text-based games",
            "rating": 2
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Enhancing text-based reinforcement learning agents with commonsense knowledge",
            "rating": 2
        },
        {
            "paper_title": "Learning dynamic knowledge graphs to generalize on text-based games",
            "rating": 2
        },
        {
            "paper_title": "Interactive fiction games: A colossal adventure",
            "rating": 1
        }
    ],
    "cost": 0.01482625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Deep Reinforcement Learning with Stacked Hierarchical Attention for Text-based Games</h1>
<p>Yunqiu Xu*<br>University of Technology Sydney<br>Yunqiu.Xu@student.uts.edu.au</p>
<h2>Ling Chen</h2>
<p>University of Technology Sydney
Ling.Chen@uts.edu.au</p>
<p>Joey Tianyi Zhou
IHPC A*STAR
zhouty@ihpc.a-star.edu.sg</p>
<p>Meng Fang*<br>Tencent Robotics X mfang@tencent.com</p>
<p>Yali Du<br>University College London<br>yali.du@ucl.ac.uk</p>
<p>Chengqi Zhang<br>University of Technology Sydney<br>Chengqi.Zhang@uts.edu.au</p>
<h4>Abstract</h4>
<p>We study reinforcement learning (RL) for text-based games, which are interactive simulations in the context of natural language. While different methods have been developed to represent the environment information and language actions, existing RL agents are not empowered with any reasoning capabilities to deal with textual games. In this work, we aim to conduct explicit reasoning with knowledge graphs for decision making, so that the actions of an agent are generated and supported by an interpretable inference procedure. We propose a stacked hierarchical attention mechanism to construct an explicit representation of the reasoning process by exploiting the structure of the knowledge graph. We extensively evaluate our method on a number of man-made benchmark games, and the experimental results demonstrate that our method performs better than existing text-based agents.</p>
<h2>1 Introduction</h2>
<p>Language plays a core role in human intelligence and cognition [14, 43]. Text-based games [13, 20], where both the states and actions are described by textual descriptions, are suitable simulation environments for studying the language-informed decision making process. These games can be regarded as an intersection of natural language processing (NLP) and reinforcement learning (RL) tasks [35]. To solve text-based games via RL, the agent has to tackle many challenges, such as learning representation from text [42], making decisions based on partial observations [4], handling combinatorial action space [57] and sparse rewards [56]. Generally, existing agents for text-based games can be classified as rule-based agents and learning-based agents. Rule-based agents, such as NAIL [21], solve the games based on pre-defined rules, engineering tricks, and pre-trained language models. By heavily relying on prior knowledge of the games, these agents lack flexibility and adaptability. With the progress of deep reinforcement learning [38, 39], learning-based agents such as LSTM-DRQN [42] become increasingly popular since they learn purely from interaction without requiring expensive human knowledge as prior. Recently, considering the rich information that can be maintained by its structural memory, knowledge graphs (KGs) have been incorporated into RL agents to facilitate solving text-based games $[1,4,3]$.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>While a lot of studies have been conducted on representing useful information from text observations [3, 4, 42] and reducing action spaces [20, 57], few RL agent addresses the reasoning process for text-based games. Going beyond mapping a question to an answer, human beings have the ability of reasoning - they can reuse the knowledge [50], or compose the supporting facts (e.g., the relation between objects in the scene) from the question and the knowledge base to interpret the answer [10, 30]. We believe that RL agents empowered with reasoning capabilities will be better mimicking human decisions in solving text-based games and achieving enhanced performance. In terms of RL agents, we consider enhancing the reasoning capability of the agent by exploiting KGs. While existing studies [3, 4, 58] treat KGs as a part of the observation to handle partial observability, they ignore the potential of KGs for reasoning [12, 27]. Furthermore, the effectiveness of reasoning is constrained by two problems. Firstly, existing KG-based agents construct one single KG, so that fine-grained information (e.g., the types of object relationship, the newness/oldness of information) is hard to be maintained. Secondly, the multi-modal inputs, such as textual observations and KGs, are aggregated via simple concatenation so that their respective benefits cannot be sufficiently exploited.
We believe that an intelligent agent should have the ability to conduct explicit reasoning with relational and temporal awareness being taken into consideration to make decisions. In this paper, our goal is to design an enhanced RL agent with a reasoning process for text-based games. We propose a new method, named as Stacked Hierarchical Attention with Knowledge Graphs (SHA-KG) , to enable the agent to perform multi-step reasoning via a hierarchical architecture on playing games. Briefly, to leverage the structure information of a KG that maintains the agent's knowledge about the game environment, we first consider the sub-graphs of the KG with different semantic meanings so that relational and temporal awareness will be taken into account. Secondly, a stacked hierarchical attention module is devised to build effective state representation from multi-modal inputs, so that their respective importance will be considered.</p>
<p>Our contributions include four aspects. Firstly, our work is a first step in pursuing reasoning in solving text-based games. Secondly, we propose to incorporate sub-graphs of the KG into decision making to introduce the reasoning process. Thirdly, we propose a new stacked hierarchical attention mechanism for RL approach featured by multi-level and multi-modal reasoning. Fourthly, we extensively evaluate our method on a wide range of text-based benchmark games, achieving favorable results compared with the state-of-the-art methods.</p>
<h1>2 Related work</h1>
<p>Agents for text-based games. Existing agents either perform based on predefined rules or learn to make responses by interacting with the environment. Rule-based agents [8, 16, 21, 31] attempt to solve text-based games by injecting heuristics. They are thus not flexible since a huge amount of prior knowledge is required to design rules [20]. Learning-based agents [2, 20, 22, 26, 42, 55, 56, 57] usually employ deep reinforcement learning algorithms to deliver adaptive game solving strategies. However, the performance of these agents is still not up to par when playing complex man-made games, even though efforts have been made to reduce the difficulty (e.g., DRRN [22] assumed that an action can only be selected from a valid action set for each state). KG-based agents have been developed to enhance the performance of learning-based agents with the assistance of KGs. KGs can be constructed by simple rules so that it substantially reduces the amount of prior knowledge required by rule-based agents. While KGs have been leveraged to handle partial observability [3, 4, 58], reduce action space [3, 4], and improve generalizability [1, 5], few of the existing works addresses its potential for reasoning. Recently, Murugesan et al. [41] tried to introduce commonsense reasoning for playing synthetic games. They extracted sub-graphs from ConceptNet [44], which is a large-scale external knowledge base with millions of edges and nodes. In contrast, we aim to construct the KG based on domain information with minimal external knowledge. Besides, we focus on man-made games which are more complex than synthetic games in terms of logic, so that the reasoning ability becomes especially crucial and desirable to the agents.</p>
<p>Attention mechanism. Attention mechanism has been widely studied in areas of machine learning, psychology and neuroscience [33]. For text-based games, self-attention [45] has been applied to encode textual observation [1, 58], and Graph Attention Networks (GATs) [46] has been employed to encode KGs [3]. Regarding model explainability, the attention mechanism helps to solve the outcome</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>explanation problem, e.g. building an attention-based saliency map [19]. For RL, the attention mechanism has been used to interpret the decision making process, mostly in tasks with visual inputs [18, 40]. For tasks with multi-modal inputs, such as visual question answering (VQA) [7], the attention mechanism has been used to aggregate the image and text inputs [23, 24, 29, 30, 34, 36]. In this work, we apply an attention mechanism to consider the multi-modal inputs based on text observations and graph structures.</p>
<p>Reasoning via knowledge graph. A lot of existing studies have used KGs as the knowledge base to facilitate learning and interpretation, including incorporating KG-based commonsense reasoning for question answering [9, 15, 32, 61] and recommendation [47, 48, 52, 60]. We are the first to exploit KGs to induce reasoning for RL-based agents playing text-based games.</p>
<h1>3 Preliminaries</h1>
<p>POMDP Partially Observable Markov Decision Processes (POMDPs) can be defined as a 7-tuple: the state set $\mathcal{S}$, the action set $\mathcal{A}$, the state transition probabilities $\boldsymbol{T}$, the reward function $\boldsymbol{R}$, the observation set $\boldsymbol{\Omega}$, the conditional observation probabilities $\boldsymbol{O}$ and the discount factor $\gamma \in(0,1]$. At each time step, the agent will receive an observation $\boldsymbol{o}<em t="t">{t} \in \boldsymbol{\Omega}$, depending on the current state and previous action via the conditional observation probability $O\left(\boldsymbol{o}</em>} \mid \boldsymbol{s<em t-1="t-1">{t}, \boldsymbol{a}</em>}\right)$. By executing an action $\boldsymbol{a<em t_1="t+1">{t} \in \mathcal{A}$, the environment will transit into a new state based on the state transition probability $T\left(\boldsymbol{s}</em>} \mid \boldsymbol{s<em t="t">{t}, \boldsymbol{a}</em>}\right)$, and the agent will receive the reward $\boldsymbol{r<em t="t">{t+1}=R\left(\boldsymbol{s}</em>}, \boldsymbol{a<em t="t">{t}\right)$. Same as Markov Decision Process (MDPs), the goal of the agent is to learn an optimal policy $\boldsymbol{\pi}^{*}$ to maximize the expected future discounted sum of rewards from each time step: $\boldsymbol{R}</em>\right]$.
KG Knowledge Graph (KG) for a text-based game can be built from a set of triplets 〈Subject, Relation, Object), denoting that the Subject has Relation with the Object. For example, $\langle$ Kitchen, Has, Food $\rangle$. The KG is denoted as $G=(V, E)$, where $V$ and $E$ are the node set and the edge set, respectively. Both Subject and Object belong to the node set $V$. Relation, which corresponds to the edge connecting them, belongs to $E$.}=\mathbb{E}\left[\sum_{k=0}^{+\infty} \gamma^{k} \boldsymbol{r}_{t+k+1</p>
<h2>4 Methodology</h2>
<h3>4.1 Problem statement</h3>
<p>In this work, we focus on man-made games, which are initially designed for human players [20]. These games are devised with more complex logic and much larger action space than synthetic games [13]. Text-based games require an agent to make automatic responses to achieve specific goals (e.g., escaping from the dungeon) based on received textual information. Raw textual observation contains only the feedback of taking an action (e.g., "Taken" is a textual observation after executing the action "take egg"). As underlying states can not be directly observed by the agent, the text-based games can be formulated as POMDPs. Similar to [3], at every step we construct an input $\boldsymbol{s}<em _="{" _text="\text" t_="t," text="text">{t}$ as the combination of three components: a textual observation $\boldsymbol{o}</em>}}$, a collected raw score $\boldsymbol{o<em _mathrm_KG="\mathrm{KG" t_="t,">{t, \text { score }}$, and a $\mathrm{KG} \boldsymbol{o}</em>}}$ (note that here $\boldsymbol{s<em _="{" _text="\text" t_="t," text="text">{t}$ should not be regarded as a true game state as the games are not fully observable). $\boldsymbol{o}</em>}}$ further includes the current state $\boldsymbol{o<em _="{" _text="\text" inv="inv" t_="t,">{t, \text { desc }}$ (describing the environment), inventory $\boldsymbol{o}</em>}}$ (describing items collected by a player), game feedback $\boldsymbol{o<em t-1="t-1">{t, \text { feed }}$, and previous action taken $\boldsymbol{a}</em>}$. Fig. 1 (a) shows an example of $\boldsymbol{o<em _="{" _text="\text" t_="t," text="text">{t, \text { text }}$.
While $\boldsymbol{o}</em>}}$ and $\boldsymbol{o<em _mathrm_KG="\mathrm{KG" t_="t,">{t, \text { score }}$ mainly reflect the current observation, $\boldsymbol{o}</em>$ are used to update the KG as,}}$ records the game history. Therefore, the KG can help the agent to handle partial observability. At each time step, the triples extracted from the current textual observation $\boldsymbol{o}_{t, \text { text }</p>
<p>$$
\boldsymbol{o}<em _mathrm_KG="\mathrm{KG" t-1_="t-1,">{t, \mathrm{KG}}=\operatorname{GraphUpdate}\left(\boldsymbol{o}</em>\right)
$$}}, \boldsymbol{o}_{t, \text { text }</p>
<p>Fig. 1 (b) shows an example of $\boldsymbol{o}_{t, \mathrm{KG}}$ and how it updates. We provide details of constructing and updating the KG in Sec. 5.2.</p>
<h3>4.2 Sub-graph division</h3>
<p>As discussed, existing KG-based agents build only one knowledge graph [1, 3, 4, 58]. To introduce relational-awareness and temporal-awareness, in this work, we divide our KG as multiple sub-graphs.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) Textual observation $\boldsymbol{o}<em _mathrm_KG="\mathrm{KG" t_="t,">{t, \text { text }}$. (b) Knowledge graph $\boldsymbol{o}</em>}}$. Yellow region in $\boldsymbol{o<em _="{" _text="\text" t_="t," text="text">{t, \mathrm{KG}}$ contains information extracted from the observation $\boldsymbol{o}</em>$.}}$. (c) Sub-graphs obtained from $\boldsymbol{o}_{t, \mathrm{KG}</p>
<p>Inspired by the heterogeneous graph [49, 59] where a graph contains different types of nodes and edges, we first classify edges by their types (e.g., "Has" and "East of" can be regarded as different types), and then build relational-aware sub-graphs based on the edges. In addition, as the KG can not distinguish between the current and past information, we introduce temporal-awareness via building different sub-graphs based on whether the historical information is included (e.g., sub-graphs built from $\boldsymbol{o}<em _="{" _text="\text" t_="t," text="text">{t, \text { text }}$ only and sub-graphs built from $\boldsymbol{o}</em>$ ). The union of all sub-graphs is the full KG:}}$ and $\boldsymbol{o}_{t-1, \mathrm{KG}</p>
<p>$$
\boldsymbol{o}<em _mathrm_KG="\mathrm{KG" t_="t,">{t, \mathrm{KG}}=\boldsymbol{o}</em>}, 1} \cup \boldsymbol{o<em _mathrm_KG="\mathrm{KG" t_="t,">{t, \mathrm{KG}, 2} \ldots \cup \boldsymbol{o}</em>
$$}, \mathrm{~m}-1} \cup \boldsymbol{o}_{t, \mathrm{KG}, \mathrm{~m}</p>
<p>where $m$ denotes the number of sub-graphs. From the perspective of hierarchical learning [54, 62], the sub-graph division allows observations to be considered in two levels: In the high level, the full KG captures the overall node connectivity. In the low level, the sub-graphs reflect different relational and temporal relations. Fig. 1 (c) shows an example of the sub-graphs obtained from $\boldsymbol{o}_{t, \mathrm{KG}}$.</p>
<h1>4.3 Stacked hierarchical attention network</h1>
<p>Before action selection, we represent the input $\boldsymbol{s}<em t="t">{t}$ as a vector $\boldsymbol{v}</em>}$ first. We omit the subscript " $t$ " for simplicity, and denote the KG as $\boldsymbol{o<em _KG_="{KG," _text="\text" full="full">{\text {KG, full }}$ to distinguish it from the sub-graphs. Since the textual observation, score and knowledge graph are multi-modal inputs, inspired by the VQA techniques [29, 34], we aggregate the inputs by constructing query representation for one modal (or two) to obtain the attention of another modal, through a stacked hierarchical attention mechanism. Fig. 2 shows an overview of our encoder, which consists of two levels. In the high level, we build a query vector from the KG and score, then compute multiple groups of attention values across the components of textual observations. In the low level, we treat the output of the high level as a query, and compute attention values across the sub-graphs.
High-level attention Similar to KG-A2C [3], the KG $\boldsymbol{o}</em>}}$ is processed via GATs [46] followed by a linear layer to get the graph representation $\boldsymbol{v<em _KG="{KG" _text="\text">{\text {KG, full }} \in \mathbb{R}^{d</em>}}}$. The score representation $\boldsymbol{v<em _score="{score" _text="\text">{\text {score }} \in \mathbb{R}^{d</em>}}}$ is obtained via binary encoding. While previous works [3, 20] concatenated all observational vectors to form state representation, we build the query vector $\boldsymbol{q<em _high="{high" _text="\text">{\text {high }} \in \mathbb{R}^{d</em>$ and}}}$ by concatenating $\boldsymbol{v}_{\text {KG, full }</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of our stacked hierarchical attention network. In the high level (left), the query vector <strong>q</strong><sub>high</sub> is the combination of the KG representation <strong>v</strong><sub>KG, full</sub> and the score representation <strong>v</strong><sub>score</sub>. Then multiple groups of attention values <strong>α</strong><sub>high</sub> are computed across the components of textual observation <strong>v</strong><sub>text</sub>. In the low level (right), the query vector <strong>q</strong><sub>low</sub> is the output of high level encoding, and multiple groups of attention values <strong>α</strong><sub>low</sub> are computed across the sub-graphs. The final output <strong>v</strong><sub><em>t</em></sub> is served as the state representation for action selection.</p>
<p><strong>v</strong><sub>score</sub> followed by a linear layer:</p>
<p>$$
\mathbf{q}<em _text_Init="\text{Init">{\text{high}} = \mathbf{W}</em>} \operatorname{concat}} (\mathbf{v<em _text_score="\text{score">{\text{KG, full}}, \mathbf{v}</em>
$$}}) + \mathbf{b}_{\text{Init}} \tag{3</p>
<p>where <strong>W</strong><sub>Init</sub> ∈ ℝ<sup>d<sub>high</sub></sup><sub>×</sub>(d<sub>KG</sub> + d<sub>score</sub>) and <strong>b</strong><sub>Init</sub> ∈ ℝ<sup>d<sub>high</sub></sup> are weights and biases.</p>
<p>Suppose the textual observation consists of <em>c</em> components<sup>3</sup>, we first encode them separately by <em>c</em> GRUs. Instead of concatenating, we consider them individually to build the textual representation vector <strong>v</strong><sub>text</sub> ∈ ℝ<sup>d<sub>high</sub></sup><sub>×</sub>c. Therefore <strong>v</strong><sub>text</sub> can be treated as multiple image regions or image representation with multiple channels. Inspired by SCA-CNN [11], we compute attention values in channel-wise. However, instead of computing one attention value for each channel, we compute multiple groups of attention values to capture more fine-grained information. Specifically, one group of attention values is computed for each position along the channel:</p>
<p>$$
\mathbf{\alpha}<em _text_A_high="\text{A,high">{\text{high}} = \text{softmax} (\mathbf{W}</em>}} \mathbf{h<em _text_A_high="\text{A,high">{\text{high}} + \mathbf{b}</em>
$$}}) \tag{4</p>
<p>where</p>
<p>$$
\mathbf{h}<em _text_I_high="\text{I,high">{\text{high}} = \tanh(\mathbf{W}</em>}} \mathbf{v<em _text_Q_high="\text{Q,high">{\text{text}} \oplus (\mathbf{W}</em>}} \mathbf{q<em _text_Q_high="\text{Q,high">{\text{high}} + \mathbf{b}</em>
$$}}) \tag{5</p>
<p>denotes the intermediate representation and ⊕ denotes the addition of a matrix and a vector. <strong>W</strong><sub>I,high</sub> ∈ ℝ<sup>d<sub>high</sub></sup><sub>×</sub>, d<sub>high</sub>, <strong>W</strong><sub>Q,high</sub> ∈ ℝ<sup>d<sub>high</sub></sup><sub>×</sub>, d<sub>high</sub> and <strong>W</strong><sub>A,high</sub> ∈ ℝ<sup>d<sub>high</sub></sup><sub>×</sub>, d<sub>high</sub> are weight matrices, <strong>b</strong><sub>Q,high</sub> ∈ ℝ<sup>d<sub>high</sub></sup><sub>×</sub> and <strong>b</strong><sub>A,high</sub> ∈ ℝ<sup>d<sub>high</sub></sup><sub>×</sub> are biases. This operation is equivalent to dividing <strong>v</strong><sub>text</sub> as d<sub>high</sub> sub-vectors <strong>v</strong><sub>text,sub</sub> ∈ ℝ<sup>1×c</sup>, then computing channel-wise attention for each of them. The obtained attention values <strong>α</strong><sub>high</sub> ∈ ℝ<sup>d<sub>high</sub></sup><sub>×</sub>c reflect the multi-positional attentive focus on the textual components. The final step of high-level encoding is to attentively aggregate the query vector with the textual vector. In order to enable multi-level reasoning, we leverage recent advances in attention techniques [17, 28, 53] to learn multi-step reasoning by iteratively updating the query. We first multiply <strong>v</strong><sub>text</sub> with <strong>α</strong><sub>high</sub> via dot-product, then sum all the channels and add it to <strong>q</strong><sub>high</sub> to obtain updated query vector <strong>q</strong><sub>low</sub> ∈ ℝ<sup>d<sub>high</sub></sup><sub>×</sub>:</p>
<p>$$
\mathbf{q}<em _text_high="\text{high">{\text{low}} = \mathbf{q}</em>}} + \sum_{i}^{c} \mathbf{\alpha<em _text_text="\text{text">{\text{high},i} \odot \mathbf{v}</em>
$$},i} \tag{6</p>
<p><sup>3</sup>As discussed in 4.1, <em>c</em> is 4 in this work.</p>
<p>Low-level attention The low-level encoding process is similar to high level, except that the attention values are computed across different sub-graphs. We encode sub-graphs with different GATs and combine them as graph representation $\boldsymbol{v}<em _low="{low" _text="\text">{\mathrm{KG}} \in \mathbb{R}^{d</em>}} \times m}$, where $d_{\text {low }}$ denotes dimensionality. We treat the output vector of high-level computing as a query vector, and perform linear transformation to ensure $\boldsymbol{q<em _low="{low" _text="\text">{\text {low }} \in \mathbb{R}^{d</em>$. Then we apply the similar attention mechanism of high-level:}}</p>
<p>$$
\boldsymbol{\alpha}<em _mathrm_A="\mathrm{A">{\text {low }}=\operatorname{softmax}\left(\boldsymbol{W}</em>}, \text { low }} \boldsymbol{h<em _mathrm_A="\mathrm{A">{\text {low }}+\boldsymbol{b}</em>\right)
$$}, \text { low }</p>
<p>where</p>
<p>$$
\boldsymbol{h}<em _mathrm_I="\mathrm{I">{\text {low }}=\tanh \left(\boldsymbol{W}</em>}, \text { low }} \boldsymbol{v<em _mathrm_Q="\mathrm{Q">{\mathrm{KG}} \oplus\left(\boldsymbol{W}</em>}, \text { low }} \boldsymbol{q<em _mathrm_Q="\mathrm{Q">{\text {low }}+\boldsymbol{b}</em>\right)\right)
$$}, \text { low }</p>
<p>denotes the intermediate representation. $\boldsymbol{W}<em _low="{low" _text="\text">{\mathrm{I}, \text { low }} \in \mathbb{R}^{d</em>}} \times d_{\text {low }}}, \boldsymbol{W<em _low="{low" _text="\text">{\mathrm{Q}, \text { low }} \in \mathbb{R}^{d</em>}} \times d_{\text {low }}}$ and $\boldsymbol{W<em _low="{low" _text="\text">{\mathrm{A}, \text { low }} \in$ $\mathbb{R}^{d</em>}} \times d_{\text {low }}}$ are weight matrices, $\boldsymbol{b<em _low="{low" _text="\text">{\mathrm{Q}, \text { low }} \in \mathbb{R}^{d</em>}}}$ and $\boldsymbol{b<em _low="{low" _text="\text">{\mathrm{A}, \text { low }} \in \mathbb{R}^{d</em>}}}$ are biases. Finally, we aggregate $\boldsymbol{q<em _mathrm_KG="\mathrm{KG">{\text {low }}$ with $\boldsymbol{v}</em>}}$ based on the low-level attention values $\boldsymbol{\alpha<em _low="{low" _text="\text">{\text {low }} \in \mathbb{R}^{d</em>}} \times m}$ to get state representation $\boldsymbol{v<em _low="{low" _text="\text">{t} \in \mathbb{R}^{d</em>$ :}}</p>
<p>$$
\boldsymbol{v}<em _low="{low" _text="\text">{t}=\boldsymbol{q}</em>}}+\sum_{i}^{m} \boldsymbol{\alpha<em _mathrm_KG="\mathrm{KG">{\text {low }, i} \odot \boldsymbol{v}</em>
$$}, i</p>
<h1>4.4 Action selection and training</h1>
<p>Action selection. Given the state representation $\boldsymbol{v}<em t="t">{t}$, the action selection can be performed via methods such as template-based scoring [20] and recurrent decoding [3]. In this work, we use the recurrent decoding method to select actions via two GRUs. We first use a template-GRU to predict a template $\boldsymbol{u} \in \mathcal{T}$ based on $\boldsymbol{v}</em>}$, where $\mathcal{T}$ denotes the template set. Next, we recurrently execute an object-GRU for $k$ steps to decode objects $\left{\boldsymbol{p<em _mathrm_KG="\mathrm{KG">{i}, i \in[1, \ldots, k]\right}$ from the object set $\mathcal{P}$, which is the intersection of the vocabulary set $\mathcal{V}$ and the set of the objects appeared in $\boldsymbol{o}</em>}, \text { full }}$. The probability of an object $\boldsymbol{p<em t="t">{i}$ is conditioned on both $\boldsymbol{v}</em>}$ and the prediction of the last step (i.e., $\boldsymbol{u}$ or $\boldsymbol{p<em t="t">{t-1}$ ). Finally, the template and objects are combined as action $\boldsymbol{a}</em>$.
Training. Our model SHA-KG is trained via the Advantage Actor Critic (A2C) method [37] with a supervised auxiliary task "valid action prediction" [3]. We provide details in the supplementary material.</p>
<h2>5 Experiments</h2>
<p>We evaluate our method on a set of man-made games in Jericho game suite [20]. We conduct experiments to validate the effectiveness of our sub-graph division and stacked hierarchical attention, and interpret the reasoning and decision making processes.</p>
<h3>5.1 Baselines</h3>
<p>We use KG-A2C [3] as the building backbone of SHA-KG. Following baselines are considered:</p>
<ul>
<li>NAIL [21]: a general agent with hand-crafted rules and pre-trained language models.</li>
<li>DRRN [20, 22]: a choice-based agent that selects actions from a valid action set.</li>
<li>TDQN [20]: a parser-based agent with template-based action space.</li>
<li>KG-A2C [3]: an extension of TDQN with KGs and valid action predictions.</li>
</ul>
<h3>5.2 Experimental setup</h3>
<p>Graph construction. The triples for constructing the KG are extracted via Stanford's Open Information Extraction (OpenIE) [6] and two additional rules used in [3]: 1) For interactive objects detected in the current observation, those within the inventory are linked to node "you", while others are linked to the current room. 2) The room connectivity is inferred from navigational actions. Regarding the sub-graph division, we define four graph types: $\boldsymbol{o}<em _mathrm_KG="\mathrm{KG">{\mathrm{KG}, 1}$ records the connectivity of visited rooms, $\boldsymbol{o}</em>}, 2}$ represents the objects within the current room, $\boldsymbol{o<em _mathrm_KG="\mathrm{KG">{\mathrm{KG}, 3}$ represents the objects within the inventory, and $\boldsymbol{o}</em>}, 4}$ is the graph without any connection to "you". $\boldsymbol{o<em _mathrm_KG="\mathrm{KG">{\mathrm{KG}, 2}$ and $\boldsymbol{o}</em>}, 3}$ contain the present information only, while $\boldsymbol{o<em _mathrm_KG="\mathrm{KG">{\mathrm{KG}, 1}$ and $\boldsymbol{o}</em>$ contain both the current and historical information. Besides pre-defined rules, the graph partitioning process can be implemented via automatic methods, which we leave as future work.}, 4</p>
<p>Table 1: Raw scores of SHA-KG and baselines. For the baselines, we use the results reported in their original paper [3, 20] except "reverb" and "tryst205" in KG-A2C, which are not reported. $|\mathcal{T}|$ and $|\mathcal{V}|$ denote the size of template set and vocabulary set. MaxR denotes the maximum possible score (collected based on walkthrough without step limit). All results are averaged over five independent runs.</p>
<p>| Game | $|\mathcal{T}|$ | $|\mathcal{V}|$ | NAIL | DRRN | TDQN | KG-A2C | SHA-KG (Ours) | MaxR |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| acorncourt | 151 | 343 | 0 | 10 | 1.6 | 0.3 | 1.6 | 30 |
| balances | 156 | 452 | 10 | 10 | 4.8 | 10.0 | 10.0 | 51 |
| detective | 197 | 344 | 136.9 | 197.8 | 169 | 207.9 | 308.0 | 360 |
| dragon | 177 | 1049 | 0.6 | $-3.5$ | $-5.3$ | 0 | 0.2 | 25 |
| enchanter | 290 | 722 | 0 | 20.0 | 8.6 | 12.1 | 20.0 | 400 |
| inhumane | 141 | 409 | 0.6 | 0 | 0.7 | 3 | 5.4 | 300 |
| jewel | 161 | 657 | 1.6 | 1.6 | 0 | 1.8 | 1.8 | 90 |
| library | 173 | 510 | 0.9 | 17 | 6.3 | 14.3 | 15.8 | 30 |
| ludicorp | 187 | 503 | 8.4 | 13.8 | 6 | 17.8 | 17.8 | 150 |
| pentari | 155 | 472 | 0 | 27.2 | 17.4 | 50.7 | 51.3 | 70 |
| reverb | 183 | 526 | 0 | 8.2 | 0.3 | 7.4 | 10.6 | 50 |
| sorcerer | 288 | 1013 | 5 | 20.8 | 5 | 5.8 | 29.4 | 400 |
| spellbskr | 333 | 844 | 40 | 37.8 | 18.7 | 21.3 | 40.0 | 600 |
| spirit | 169 | 1112 | 1 | 0.8 | 0.6 | 1.3 | 3.8 | 250 |
| temple | 175 | 622 | 7.3 | 7.4 | 7.9 | 7.6 | 7.9 | 35 |
| tryst205 | 197 | 871 | 2 | 9.6 | 0 | 6.7 | 6.9 | 350 |
| zenon | 149 | 401 | 0 | 0 | 0 | 3.9 | 3.9 | 350 |
| zork1 | 237 | 697 | 10.3 | 32.6 | 9.9 | 34 | 34.5 | 350 |
| zork3 | 214 | 564 | 1.8 | 0.5 | 0 | 0.1 | 0.7 | 7 |
| ztuu | 186 | 607 | 0 | 21.6 | 4.9 | 9.2 | 25.2 | 100 |</p>
<p>Training implementation. We follow the hyper-parameter setting of KG-A2C [3] except that we reduce the node embedding dimension in GATs from 50 to 25 to reduce GPU cost. We set $\boldsymbol{d}<em _low="{low" _text="\text">{\text {high }}$ as 100, and $\boldsymbol{d}</em>$ interaction steps. The training data is collected from 32 environments in parallel. An optimization step is performed per 8 interaction steps via the Adam optimizer with the learning rate 0.003 . All baselines follow their original implementations [3, 20, 21]. To compare with the baselines, we report the average raw score over the last 100 finished episodes during training. We also report the learning curve during ablation study. All the quantitative results are averaged over five independent runs.}}$ as 50 . For both SHA-KG and KG-A2C, the graph mask for action selection is constructed from $\boldsymbol{\sigma}_{\text {KG,full }}$. We denote an action as "valid" if it does not lead to meaningless feedback in a state (e.g., "Nothing happens"). An episode will be terminated after 100 valid steps or game over / victory. For each game, an individual agent is trained for $10^{6</p>
<h1>5.3 Overall performance</h1>
<p>Table 1 shows the performance of SHA-KG and baselines in 20 games. The proposed SHA-KG achieves the new state-of-the-art results in 8 games and is equivalent to the current best baselines in another 7 games. Comparing the three agents using a template-based action space (i.e., SHA-KG, TDQN, KG-A2C), SHA-KG obtains equal or better performance than TDQN and KG-A2C in all of the games, showing the effectiveness of introducing reasoning. The best result of the other 5 games is still achieved by NAIL and DRRN, which apply additional rules and assumptions. NAIL follows nearly fixed rules (e.g., interacting with all the objects, then exploring another room). Though this design principle may be useful for some particular games (e.g., "dragon" and "zork3"), it lacks flexibility so that it performs worse than all learning-based agents in most of the games. DRRN largely reduces the difficulty by selecting actions from the set of admissible actions only. For example, to obtain the first reward " +10 " in "acorncourt", the agent has to select a high proportion of complex actions, in which case the assumption of an admissible action set is a large advantage. KG-A2C and our SHA-KG relax this assumption but the action set is as large as $\mathcal{O}\left(\mathcal{T} \mathcal{P}^{2}\right)$, making them infeasible to achieve the first reward stably. However, the reasoning ability still brings improvement compared with the backbone model. While KG-A2C shows worse performance than DRRN in 9 games, SHA-KG outperforms DRRN in 6 of these games, and shows closer scores in other 3 games.</p>
<h3>5.4 Ablation study</h3>
<p>We perform ablation studies to validate the contributions of different components. We first compare SHA-KG with its three variants with different attention modules:</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Learning curves of models with different attention modules. The dashed line in "ztuu" denotes KG-A2C's result reported in [3]. The shaded regions indicate standard deviations.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Learning curves of SHA-KG with different types of sub-graphs.</p>
<ul>
<li>"w/o GroupAttn": applies single attention value for each channel, i.e., $\boldsymbol{\alpha}<em _text_low="\text{low">{\text{high}} \in \mathbb{R}^4$, $\boldsymbol{\alpha}</em>^4$.}} \in \mathbb{R</li>
<li>"w/o high-level": constructs the initial query vector from $\boldsymbol{v}<em _text_score="\text{score">{\text{text}}$ and $\boldsymbol{v}</em>$, then computes attention values across the sub-graphs. The full KG is not used.}</li>
<li>"w/o low-level": constructs the initial query vector from $\boldsymbol{v}<em _text_score="\text{score">{\text{KG, full}}$ and $\boldsymbol{v}</em>$, then computes attention values across the textual components. The sub-graph division is not used.}</li>
</ul>
<p>Fig. 3 shows the learning curves of 5 games, where following observations can be made: 1) The full model SHA-KG shows similar or better performance than all variants in all cases. Our two-level attention mechanism provides an effective and explainable way to refine information. The first level of hierarchy tells the agent which part of the textual information should be focused on. Based on the output of this level, the second level of hierarchy informs the agent which part of a knowledge graph should be targeting at. 2) The variant "w/o high-level", which considers sub-graphs only but not the full graph (i.e., the pink curve), works for some games (e.g., "reverb" and "zork1") but fails for the others. For the games where the variant is able to achieve the best score, the sample efficiency is also improved, which means that this variant requires fewer interaction steps to achieve the best score. 3) For the variant "w/o low-level" that considers the full KG only but not sub-graphs (i.e., the orange curve), the sample efficiency is a bit low in some games (e.g., "zork1"). 4) The variant "w/o GroupAttn" generally shows a similar performance curve to SHA-KG by considering both the full KG and sub-graphs. However, the performance gap between "w/o GroupAttn" and SHA-KG demonstrates the effectiveness of computing multiple groups of attention values along the channels, which allows SHA-KG to capture more fine-grained information from textual components (in high level) and sub-graphs (in low level). Overall, the learning curves demonstrate that the sub-graph division and stacked hierarchical attention provide complementary contributions to the performance improvement of SHA-KG.</p>
<p>Regarding the contributions of different types of sub-graphs, we further design three variants with different graph partitioning strategies:</p>
<ul>
<li>"w/o relational-awareness" combines $\boldsymbol{o}<em _text_KG_3="\text{KG,3">{\text{KG,2}}$ (room objects) and $\boldsymbol{o}</em>$ (collected objects).}</li>
<li>"w/o temporal-awareness" combines $\boldsymbol{o}<em _text_KG_2="\text{KG,2">{\text{KG,4}}$ with $\boldsymbol{o}</em>$, respectively.}}$ and $\boldsymbol{o}_{\text{KG,3}</li>
<li>"w/o history" removes all historical information.</li>
</ul>
<p>Fig. 4 shows the results and indicates that the effect of different types of awareness varies with respect to the games. No simple conclusion can be made regarding which type of awareness contributes the most to the final performance (e.g., "w/o relational-awareness" and "w/o temporal-awareness" behave differently in "zork1" and "zork3"). However, considering them collectively and learning to balance their importance lead to the improved performance of our method.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Illustration of the reasoning and decision making processes of the game "ztuu". Left: $\boldsymbol{\alpha}<em _text="\text" _text_desc="{text,desc">{\text {high,top25sum }}$ denotes the high-level attention for $\boldsymbol{o}</em>}}, \boldsymbol{o<em _text="\text" _text_feed="{text,feed">{\text {text,inv }}, \boldsymbol{o}</em>}}$ and $\boldsymbol{a<em _low_top25sum="{low,top25sum" _text="\text">{\mathrm{t}-1} . \boldsymbol{\alpha}</em>}}$ denotes the low-level attention for $\boldsymbol{o<em _mathrm_KG="\mathrm{KG">{\mathrm{KG}, 1}, \boldsymbol{o}</em>}, 2}, \boldsymbol{o<em _mathrm_KG="\mathrm{KG">{\mathrm{KG}, 3}$ and $\boldsymbol{o}</em>}, 3}$. The subscript "top25sum" denotes the sum of 25 largest attention values within a channel (textual observation or sub-graph). $\boldsymbol{a<em _mathrm_f="\mathrm{f">{\mathrm{f}}$ is the selected action, and $\boldsymbol{r}</em>$ is omitted due to space limit). The sub-graph with the highest graph-level attention (by SHA) is in the red dashed box. In each sub-graph, the top 3 nodes with the highest attention (by GATs) are highlighted in yellow.}}$ is the reward. Right: the extracted sub-graphs ( $\boldsymbol{o}_{\mathrm{KG}, \mathrm{f}</p>
<h1>5.5 Interpretability</h1>
<p>We interpret the reasoning and decision making process by examining the attentive focus. Although there are controversial points about whether attention values are explainable when being applied to text $[25,51]$, we believe our attention mechanism is beneficial for interpretability. Compared to word-level attention, the stacked hierarchical attention is conducted in a manner more similar to the region/channel-level attention mechanism, which has been proved to providing interpretability in vision tasks, especially visual-based RL tasks [18, 40] As multiple groups of attention values are computed (e.g., each sub-graph is associated with $d_{\text {low }}$ attention values), we aggregate them to facilitate explaining. Specifically, for each channel (e.g., textual component or sub-graph), we sum the top 25 largest values as its attention value, then perform softmax operation across the channels ${ }^{4}$. We denote the obtained attention values as $\boldsymbol{\alpha}<em _text="\text" _text_="{text," desc="desc">{\text {high, top25sum }}$ for the high level, which correspond to $\left\langle\boldsymbol{o}</em>}}, \boldsymbol{o<em _text="\text" _text_="{text," feed="feed">{\text {text, inv }}, \boldsymbol{o}</em>}}, \boldsymbol{a<em _low_="{low," _text="\text" top25sum="top25sum">{\mathrm{t}-1}}\right\rangle$, and $\boldsymbol{\alpha}</em>}}$ for the low level, which correspond to $\left\langle\boldsymbol{o<em _mathrm_KG="\mathrm{KG">{\mathrm{KG}, 1}, \boldsymbol{o}</em>}, 2}, \boldsymbol{o<em _mathrm_KG="\mathrm{KG">{\mathrm{KG}, 3}, \boldsymbol{o}</em>}, 4}\right\rangle$. Fig. 5 (left) shows a decision making example of the game "ztuu". In the high level (textual components), the agent focuses mostly on the description $\boldsymbol{o<em _text="\text" _text_="{text," feed="feed">{\text {text, desc }}$, and then the feedback $\boldsymbol{o}</em>}}$, which is followed by the last action $\boldsymbol{a<em _mathrm_KG="\mathrm{KG">{\mathrm{t}-1}$. All of the three text components contain "implement". In the low level (sub-graphs), the sub-graph of objects in the current room ( $\boldsymbol{o}</em>}, 2}$ ) has the highest attention. Combining both two levels of attention, the agent finally selects the action "lower implement", which receives a positive reward. It shows that the reasoning process enables the agent to select actions leading to positive rewards. Although the GATs in our work are mainly used for obtaining initial graph embeddings instead of assigning attention values, we also visualize the node-level attention within sub-graphs to help understand the reasoning process. Fig. 5 (right) shows three extracted sub-graphs. The digit under the sub-graph denotes graph-level attention. Since the $\boldsymbol{o<em _mathrm_KG="\mathrm{KG">{\mathrm{KG}, 2}$ has the highest attention, the agent will focus more on objects it contains. In each sub-graph, nodes with top-3 highest attention (by GATs) are highlighted in yellow. Such node-level attention helps to further constrain (softly) the objects in $\boldsymbol{o}</em>$ to derive actions. We conclude that our SHA helps the agent to use information efficiently for taking actions.}, 2</p>
<h2>6 Conclusion</h2>
<p>In this paper, we have studied empowering RL for text-based games with reasoning by exploiting knowledge graphs. We conducted sub-graph division to explicitly introduce relational-awareness and temporal-awareness. Then we designed a stacked hierarchical attention mechanism to obtain effective state representation from multi-modal inputs. Besides obtaining favorable experimental results in a wide range of man-made games, the sub-graph division and attention mechanism enable us to better interpret the reasoning and decision making processes of RL agents.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Broader Impact</h1>
<p>The high-level goal of this work is to bridge artificial intelligence with human intelligence, cognition and language learning. Researchers in reinforcement learning will benefit from this work by the appropriate use of knowledge graphs. By recording and organizing the information in a structural way, the difficulty of learning can be largely reduced. Besides, KG s can be used to conduct reasoning to interpret the decision making process. Researchers in multi-modal learning will also benefit from the attention mechanism proposed in this work. Although the stacked hierarchical attention is used to aggregate text representation and graph representation, it can also be extended to other forms of inputs such as visual and audio signals. Our work can also be served as an initial study before conducting experiments in real life and with animal / human participants, since it's performed in simulated systems and there's no safety consideration.</p>
<p>Regarding the ethical implications, although currently our work is conducted in games, where language commands are constrained in limited action spaces, for more practical applications this system will be deployed with a richer corpus. From the perspective of language generation, the inappropriate use of generated language commands should be seriously taken into consideration. Another concern lies in the unintended behavior and decision making process, which may lead to dangerous conditions in real world applications. Although we try to improve interpretability through reasoning, there's still a long way to go to make human-AI interaction in a safe and reasonable way.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank the anonymous reviewers for helpful feedback. This research was partly supported by ARC Discovery Project DP180100966.</p>
<h2>References</h2>
<p>[1] Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L Hamilton. Learning dynamic knowledge graphs to generalize on text-based games. arXiv preprint arXiv:2002.09127, 2020.
[2] Leonard Adolphs and Thomas Hofmann. Ledeepchef: Deep reinforcement learning agent for families of text-based games. arXiv preprint arXiv:1909.01646, 2019.
[3] Prithviraj Ammanabrolu and Matthew Hausknecht. Graph constrained reinforcement learning for natural language action spaces. In International Conference on Learning Representations, 2020.
[4] Prithviraj Ammanabrolu and Mark Riedl. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages $3557-3565,2019$.
[5] Prithviraj Ammanabrolu and Mark O Riedl. Transfer in deep reinforcement learning using knowledge graphs. arXiv preprint arXiv:1908.06556, 2019.
[6] Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D Manning. Leveraging linguistic structure for open domain information extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 344-354, 2015.
[7] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425-2433, 2015.
[8] Timothy Atkinson, Hendrik Baier, Tara Copplestone, Sam Devlin, and Jerry Swan. The text-based adventure ai competition. IEEE Transactions on Games, 2019.
[9] Lisa Bauer, Yicheng Wang, and Mohit Bansal. Commonsense for generative multi-hop question answering tasks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages $4220-4230,2018$.
[10] Léon Bottou. From machine learning to machine reasoning. Machine learning, 94(2):133-149, 2014.
[11] Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Wei Liu, and Tat-Seng Chua. Scacnn: Spatial and channel-wise attention in convolutional networks for image captioning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6298-6306. IEEE, 2017.</p>
<p>[12] Xiaojun Chen, Shengbin Jia, and Yang Xiang. A review: Knowledge reasoning over knowledge graph. Expert Systems with Applications, 141:112948, 2020.
[13] Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. Textworld: A learning environment for text-based games. 2018.
[14] Daniel C Dennett. The role of language in intelligence. In What is Intelligence? The Darwin College Lectures. Cambridge Univ. Press, 1994.
[15] Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie Tang. Cognitive graph for multi-hop reading comprehension at scale. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2694-2703, 2019.
[16] Nancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate. What can you do with a rock? affordance extraction via word embeddings. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17, pages 1039-1045, 2017.
[17] Zhe Gan, Yu Cheng, Ahmed Kholy, Linjie Li, Jingjing Liu, and Jianfeng Gao. Multi-step reasoning via recurrent dual attention for visual dialog. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6463-6474, 2019.
[18] Sam Greydanus, Anurag Koul, Jonathan Dodge, and Alan Fern. Visualizing and understanding atari agents. arXiv preprint arXiv:1711.00138, 2017.
[19] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. A survey of methods for explaining black box models. ACM computing surveys (CSUR), 51(5):1-42, 2018.
[20] Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan. Interactive fiction games: A colossal adventure. arXiv preprint arXiv:1909.05398, 2019.
[21] Matthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, and Jason D Williams. Nail: A general interactive fiction agent. arXiv preprint arXiv:1902.04259, 2019.
[22] Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep reinforcement learning with a natural language action space. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1621-1630, 2016.
[23] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning to reason: End-to-end module networks for visual question answering. In Proceedings of the IEEE International Conference on Computer Vision, pages 804-813, 2017.
[24] Drew Arad Hudson and Christopher D. Manning. Compositional attention networks for machine reasoning. In International Conference on Learning Representations, 2018.
[25] Sarthak Jain and Byron C Wallace. Attention is not explanation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3543-3556, 2019.
[26] Vishal Jain, William Fedus, Hugo Larochelle, Doina Precup, and Marc G Bellemare. Algorithmic improvements for deep reinforcement learning applied to interactive fiction. arXiv preprint arXiv:1911.12511, 2019.
[27] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S Yu. A survey on knowledge graphs: Representation, acquisition and applications. arXiv preprint arXiv:2002.00388, 2020.
[28] Zhong Ji, Yanwei Fu, Jichang Guo, Yanwei Pang, Zhongfei Mark Zhang, et al. Stacked semantics-guided attention model for fine-grained zero-shot learning. In Advances in Neural Information Processing Systems, pages 5995-6004, 2018.
[29] Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bilinear attention networks. In Advances in Neural Information Processing Systems, pages 1564-1574, 2018.
[30] Wonjae Kim and Yoonho Lee. Learning dynamics of attention: Human prior for interpretable machine reasoning. In Advances in Neural Information Processing Systems, pages 6019-6030, 2019.
[31] Bartosz Kostka, Jaroslaw Kwiecieli, Jakub Kowalski, and Pawel Rychlikowski. Text-based adventures of the golovin ai agent. In 2017 IEEE Conference on Computational Intelligence and Games (CIG), pages 181-188. IEEE, 2017.
[32] Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. Kagnet: Knowledge-aware graph networks for commonsense reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 2822-2832, 2019.
[33] Grace W Lindsay. Attention in psychology, neuroscience, and machine learning. Frontiers in Computational Neuroscience, 14:29, 2020.</p>
<p>[34] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image co-attention for visual question answering. In Advances in neural information processing systems, pages 289-297, 2016.
[35] Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob Foerster, Jacob Andreas, Edward Grefenstette, Shimon Whiteson, and Tim Rocktäschel. A survey of reinforcement learning informed by natural language. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages 6309-6317. International Joint Conferences on Artificial Intelligence Organization, 72019.
[36] David Mascharka, Philip Tran, Ryan Soklaski, and Arjun Majumdar. Transparency by design: Closing the gap between performance and interpretability in visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4942-4950, 2018.
[37] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928-1937, 2016.
[38] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. In NIPS Deep Learning Workshop. 2013.
[39] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 02 2015.
[40] Alexander Mott, Daniel Zoran, Mike Chrzanowski, Daan Wierstra, and Danilo Jimenez Rezende. Towards interpretable reinforcement learning using attention augmented agents. In Advances in Neural Information Processing Systems, pages 12329-12338, 2019.
[41] Keerthiram Murugesan, Mattia Atzeni, Pushkar Shukla, Mrinmaya Sachan, Pavan Kapanipathi, and Kartik Talamadupula. Enhancing text-based reinforcement learning agents with commonsense knowledge. arXiv preprint arXiv:2005.00811, 2020.
[42] Karthik Narasimhan, Tejas D Kulkarni, and Regina Barzilay. Language understanding for text-based games using deep reinforcement learning. In Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, pages 1-11. Association for Computational Linguistics (ACL), 2015.
[43] Steven Pinker. The language instinct: How the mind creates language. Penguin UK, 2003.
[44] Robyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of general knowledge. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.
[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017.
[46] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018.
[47] Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. Dkn: Deep knowledge-aware network for news recommendation. In Proceedings of the 2018 world wide web conference, pages 1835-1844, 2018.
[48] Xiang Wang, Dingxian Wang, Canran Xu, Xiangnan He, Yixin Cao, and Tat-Seng Chua. Explainable reasoning over knowledge graphs for recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 5329-5336, 2019.
[49] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu. Heterogeneous graph attention network. In The World Wide Web Conference, pages 2022-2032, 2019.
[50] Sam Wenke, Dan Saunders, Mike Qiu, and Jim Fleming. Reasoning and generalization in rl: A tool use perspective. arXiv preprint arXiv:1907.02050, 2019.
[51] Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 11-20, 2019.
[52] Yikun Xian, Zuohui Fu, S Muthukrishnan, Gerard De Melo, and Yongfeng Zhang. Reinforcement knowledge graph reasoning for explainable recommendation. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 285-294, 2019.
[53] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image question answering. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 21-29. IEEE, 2016.
[54] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. Hierarchical attention networks for document classification. In Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies, pages 1480-1489, 2016.</p>
<p>[55] Xusen Yin and Jonathan May. Comprehensible context-driven text game playing. 2019 IEEE Conference on Games (CoG), pages 1-8, 2019.
[56] Xingdi (Eric) Yuan, Marc-Alexandre Côté, Alessandro Sordoni, Romain Laroche, Remi Tachet des Combes, Matthew Hausknecht, and Adam Trischler. Counting to explore and generalize in text-based games. In European Workshop on Reinforcement Learning (EWRL), October 2018.
[57] Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, and Shie Mannor. Learn what not to learn: Action elimination with deep reinforcement learning. In Advances in Neural Information Processing Systems, pages 3562-3573, 2018.
[58] Mikulas Zelinka, Xingdi Yuan, Marc-Alexandre Côté, Romain Laroche, and Adam Trischler. Building dynamic knowledge graphs from text-based games. arXiv preprint arXiv:1910.09532, 2019.
[59] Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh V Chawla. Heterogeneous graph neural network. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, pages 793-803, 2019.
[60] Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma. Collaborative knowledge base embedding for recommender systems. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 353-362, 2016.
[61] Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J Smola, and Le Song. Variational reasoning for question answering with knowledge graph. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
[62] Zhao Zhang, Fuzhen Zhuang, Meng Qu, Fen Lin, and Qing He. Knowledge graph embedding with hierarchical relation structure. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3198-3207, 2018.</p>
<h1>SUPPLEMENTARY MATERIAL: Deep Reinforcement Learning with Stacked Hierarchical Attention for Text-based Games</h1>
<p>In the supplementary material, we describe the training details, examples of game interface and interactions used in the paper.</p>
<h2>A Training details</h2>
<p>We train our model using the Advantage Actor Critic (A2C) method [37] across valid actions. Given $\boldsymbol{s}<em t="t">{t}$, the valid action set $\operatorname{Valid}\left(\boldsymbol{s}</em>}\right)$ consists of actions not leading to meaningless feedback (e.g., "Nothing happens"). Function to obtain the valid action set is provided by Jericho [20]. We denote all learnable parameters of our model as $\boldsymbol{\theta<em t="t">{t}$. After obtaining the state representation vector $\boldsymbol{v}</em>}$, we use a critic network to estimate the value $V\left(\boldsymbol{v<em _mathbb_T="\mathbb{T">{t}\right)$ and treat decoders for templates and objects as different policies: $\boldsymbol{\pi}</em>}}$ and $\boldsymbol{\pi<em t="t">{\mathbb{O}}$. We first compute the advantage $A\left(\boldsymbol{a}</em>}, \boldsymbol{v<em _pi="\pi">{t}\right)$, then compute the policy loss $\mathcal{L}</em>}\left(\boldsymbol{v<em t="t">{t}, \boldsymbol{a}</em>} ; \boldsymbol{\theta<em _critic="{critic" _text="\text">{t}\right)$ and the critic loss $\mathcal{L}</em>}}\left(\boldsymbol{v<em t="t">{t}, \boldsymbol{a}</em>} ; \boldsymbol{\theta<em _mathbb_E="\mathbb{E">{t}\right)$ according to the gradient. We also add an entropy loss $\mathcal{L}</em>}}\left(\boldsymbol{v<em t="t">{t}, \boldsymbol{a}</em>\right)$ to encourage diversity.
Q-value function:} ; \boldsymbol{\theta}_{t</p>
<p>$$
Q\left(\boldsymbol{v}<em t="t">{t}, \boldsymbol{a}</em>}\right)=\boldsymbol{r<em t_1="t+1">{t}+\gamma V\left(\boldsymbol{v}</em>\right)
$$</p>
<p>Advantage function:</p>
<p>$$
A\left(\boldsymbol{v}<em t="t">{t}, \boldsymbol{a}</em>}\right)=Q\left(\boldsymbol{v<em t="t">{t}, \boldsymbol{a}</em>\right)
$$}\right)-V\left(\boldsymbol{v}_{t</p>
<p>Actor loss:</p>
<p>$$
\mathcal{L}<em t="t">{\pi}\left(\boldsymbol{v}</em>}, \boldsymbol{a<em t="t">{t} ; \boldsymbol{\theta}</em>}\right)=\mathbb{E}\left[\left(-\log \pi_{\mathbb{T}}\left(\boldsymbol{u} \mid \boldsymbol{v<em t="t">{t} ; \boldsymbol{\theta}</em>}\right)-\sum_{i=1}^{n} \log \pi_{\mathbb{O<em i="i">{i}}\left(\boldsymbol{p}</em>} \mid \boldsymbol{v<em i-1="i-1">{t}, \boldsymbol{u}, \ldots, \boldsymbol{p}</em>} ; \boldsymbol{\theta<em t="t">{t}\right)\right) A\left(\boldsymbol{v}</em>\right)\right]
$$}, \boldsymbol{a}_{t</p>
<p>Critic loss:</p>
<p>$$
\mathcal{L}<em t="t">{\text {critic }}\left(\boldsymbol{v}</em>}, \boldsymbol{a<em t="t">{t} ; \boldsymbol{\theta}</em>}\right)=\mathbb{E}\left[\boldsymbol{r<em t_1="t+1">{t}+\gamma V\left(\boldsymbol{v}</em>}\right)-V\left(\boldsymbol{v<em t="t">{t} ; \boldsymbol{\theta}</em>\right)\right]
$$</p>
<p>Entropy loss:</p>
<p>$$
\mathcal{L}<em t="t">{\mathbb{E}}\left(\boldsymbol{v}</em>}, \boldsymbol{a<em t="t">{t} ; \boldsymbol{\theta}</em>}\right)=\sum_{\boldsymbol{a} \in \operatorname{Valid}\left(\boldsymbol{s<em t="t">{t}\right)} P\left(\boldsymbol{a} \mid \boldsymbol{v}</em>\right)
$$}\right) \log P\left(\boldsymbol{a} \mid \boldsymbol{v}_{t</p>
<p>Similar to KG-A2C [3], a supervised auxiliary task "valid action prediction" is introduced to assist RL training. We first extract the valid template set $\mathcal{T}<em t="t">{\text {valid }}\left(s</em>}\right)=\left{\boldsymbol{\tau<em 1="1">{0}, \boldsymbol{\tau}</em>}, \ldots, \boldsymbol{\tau<em t="t">{N}\right}$ from the valid action set $\operatorname{Valid}\left(\boldsymbol{s}</em>}\right)$, and valid object set $\mathcal{O<em t="t">{\text {valid }}\left(s</em>}\right)=\left{\boldsymbol{o<em 1="1">{0}, \boldsymbol{o}</em>}, \ldots, \boldsymbol{o<em _mathrm_KG="\mathrm{KG" t_="t,">{M}\right}$ from the full knowledge graph $\boldsymbol{o}</em>}}$. Then we treat "whether the predicted template / objects is valid" as binary classification thus introduce two cross entropy loss functions: $\mathcal{L<em t="t">{\mathbb{T}}\left(\boldsymbol{v}</em>}, \boldsymbol{a<em t="t">{t} ; \boldsymbol{\theta}</em>}\right)$ for the template and $\mathcal{L<em t="t">{\mathbb{O}}\left(\boldsymbol{v}</em>}, \boldsymbol{a<em t="t">{t} ; \boldsymbol{\theta}</em>\right)$ for the objects.
Template loss:</p>
<p>$$
\mathcal{L}<em t="t">{\mathbb{T}}\left(\boldsymbol{v}</em>}, \boldsymbol{a<em t="t">{t} ; \boldsymbol{\theta}</em>}\right)=\frac{1}{N} \sum_{i=1}^{N}\left(\boldsymbol{y<em i="i">{\tau</em>}} \log \pi_{\mathbb{T}}\left(\boldsymbol{\tau<em t="t">{i} \mid \boldsymbol{v}</em>}\right)+\left(1-\boldsymbol{y<em i="i">{\tau</em>}}\right)\left(1-\log \pi_{\mathbb{T}}\left(\boldsymbol{\tau<em t="t">{i} \mid \boldsymbol{v}</em>\right)\right)\right)
$$</p>
<p>Object loss:</p>
<p>$$
\mathcal{L}<em t="t">{\mathbb{O}}\left(\boldsymbol{v}</em>}, \boldsymbol{a<em t="t">{t} ; \boldsymbol{\theta}</em>}\right)=\sum_{j=1}^{k} \frac{1}{M} \sum_{i=1}^{M}\left(\boldsymbol{y<em i="i">{o</em>}} \log \pi_{\mathbb{O<em i="i">{j}}\left(\boldsymbol{o}</em>} \mid \boldsymbol{v<em o__i="o_{i">{t}\right)+\left(1-\boldsymbol{y}</em>}}\right)\left(1-\log \pi_{\mathbb{O<em i="i">{j}}\left(\boldsymbol{o}</em>\right)\right)\right)
$$} \mid \boldsymbol{v}_{t</p>
<p>where $k$ denotes the number of object decoding steps (there are at most $k$ objects in an action). We have $\boldsymbol{y}<em i="i">{\tau</em>}}=1$ if the template $\boldsymbol{\tau<em _text="\text" _valid="{valid">{i} \in \mathcal{T}</em>}}\left(s_{t}\right)$, else 0 . Similarly, we have $\boldsymbol{y<em i="i">{o</em>}}=1$ if the object $\boldsymbol{o<em _text="\text" _valid="{valid">{i} \in \mathcal{O}</em>}}\left(s_{t}\right)$, else $0 . \boldsymbol{\theta<em _text="\text" _total="{total">{t}$ will be optimized jointly with the total loss $\mathcal{L}</em>}}\left(\boldsymbol{v<em t="t">{t}, \boldsymbol{a}</em>\right)$, which is the weighted sum of the above five losses:} ; \boldsymbol{\theta}_{t</p>
<p>$$
\mathcal{L}<em t="t">{\text {total }}\left(\boldsymbol{v}</em>}, \boldsymbol{a<em t="t">{t} ; \boldsymbol{\theta}</em>}\right)=\mathcal{L<em _critic="{critic" _text="\text">{\pi}+\boldsymbol{\lambda}</em>}} \mathcal{L<em _mathbb_E="\mathbb{E">{\text {critic }}+\boldsymbol{\lambda}</em>}} \mathcal{L<em _mathbb_T="\mathbb{T">{\mathbb{E}}+\boldsymbol{\lambda}</em>}} \mathcal{L<em _mathbb_O="\mathbb{O">{\mathbb{T}}+\boldsymbol{\lambda}</em>
$$}} \mathcal{L}_{\mathbb{O}</p>
<p>where $\boldsymbol{\lambda}<em _mathbb_E="\mathbb{E">{\text {critic }}, \boldsymbol{\lambda}</em>}}, \boldsymbol{\lambda<em _mathbb_O="\mathbb{O">{\mathbb{T}}$ and $\boldsymbol{\lambda}</em>$ are coefficients.}</p>
<p>Observation: Convention Hall. You are in attendance at the annual Grue Convention, this year a rather somber affair due to the "adventurer famine" that has gripped gruesism in this isolated corner of the empire. All around you, grues are standing; some in conversation, some drinking, some even paying attention to the speaker. There is a trash chute in one of the walls, and a reader board hangs nearby. You can see some grues, a hat, some clothes and some chewed-up shoes here
$&gt;$ Action: take glasses
Observation: You take the pair of glasses and place them under your costume. You had better hurry; the steady gurgling is becoming harder and harder for you to maintain. [Your score has just gone up by five points.]
$&gt;$ Action: remove mask
Observation: You take off the fish-mouthed mask.
Figure 6: Interface of the game "ztuu". Action (blue) is a textual command, and raw observation (black) is the textual feedback of previous actions.</p>
<h1>B Interface</h1>
<p>Fig. 6 shows an example of the raw interface of the game "ztuu", where raw textual observations contain only the feedback of taking an action.</p>
<h2>C Interaction examples</h2>
<p>In this section, we show the first 15 interaction steps of two games: "zork1" and "ztuu". Each interaction step consists of the current textual observation, triplets extracted from the current textual observation, high-level and low-level attention values obtained via different aggregation methods, chosen actions and rewards.</p>
<h2>C. 1 zork1</h2>
<div class="codehilite"><pre><span></span><code><span class="o">------</span><span class="w"> </span><span class="o">=====</span><span class="w"> </span><span class="n">Step</span><span class="w"> </span><span class="mh">1</span><span class="w"> </span><span class="o">=====</span><span class="w"> </span><span class="o">------</span>
<span class="o">=====</span><span class="w"> </span><span class="mf">1.</span><span class="w"> </span><span class="n">Textual</span><span class="w"> </span><span class="nl">obs:</span>
<span class="nl">o_desc:</span><span class="w"> </span><span class="n">West</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">House</span><span class="p">.</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">standing</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">open</span><span class="w"> </span><span class="n">field</span><span class="w"> </span><span class="n">west</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">a</span>
<span class="w">    </span><span class="n">white</span><span class="w"> </span><span class="n">house</span><span class="p">,</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">boarded</span><span class="w"> </span><span class="n">front</span><span class="w"> </span><span class="n">door</span><span class="p">.</span><span class="w"> </span><span class="n">There</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="k">small</span>
<span class="w">    </span><span class="n">mailbox</span><span class="w"> </span><span class="n">here</span><span class="p">.</span>
<span class="nl">o_inv:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">empty</span><span class="o">-</span><span class="n">handed</span><span class="p">.</span>
<span class="nl">o_feed:</span><span class="w"> </span><span class="n">Copyright</span><span class="w"> </span><span class="p">(</span><span class="n">c</span><span class="p">)</span><span class="w"> </span><span class="mh">1981</span><span class="p">,</span><span class="w"> </span><span class="mh">1982</span><span class="p">,</span><span class="w"> </span><span class="mh">1983</span><span class="w"> </span><span class="n">Infocom</span><span class="p">,</span><span class="w"> </span><span class="n">Inc</span><span class="p">.</span><span class="w"> </span><span class="n">All</span><span class="w"> </span><span class="n">rights</span>
<span class="w">    </span><span class="n">reserved</span><span class="p">.</span><span class="w"> </span><span class="n">ZORK</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">registered</span><span class="w"> </span><span class="n">trademark</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">Infocom</span><span class="p">,</span><span class="w"> </span><span class="n">Inc</span><span class="p">.</span>
<span class="w">    </span><span class="n">Revision</span><span class="w"> </span><span class="mh">88</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">Serial</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="mh">840726</span><span class="w"> </span><span class="n">West</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">House</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">are</span>
<span class="w">    </span><span class="n">standing</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">open</span><span class="w"> </span><span class="n">field</span><span class="w"> </span><span class="n">west</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">white</span><span class="w"> </span><span class="n">house</span><span class="p">,</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">a</span>
<span class="w">    </span><span class="n">boarded</span><span class="w"> </span><span class="n">front</span><span class="w"> </span><span class="n">door</span><span class="p">.</span><span class="w"> </span><span class="n">There</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="k">small</span><span class="w"> </span><span class="n">mailbox</span><span class="w"> </span><span class="n">here</span><span class="p">.</span>
<span class="nl">a_past:</span><span class="w"> </span><span class="n">look</span>
<span class="o">=====</span><span class="w"> </span><span class="mf">2.</span><span class="w"> </span><span class="n">Newly</span><span class="w"> </span><span class="n">extracted</span><span class="w"> </span><span class="n">triplets</span>
<span class="p">[(&#39;</span><span class="n">West</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">House</span><span class="p">&#39;,</span><span class="w"> </span><span class="mh">&#39;ha</span><span class="n">s</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">exit</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">west</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="n">all</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">in</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">west</span><span class="w"> </span><span class="n">of</span>
<span class="w">    </span><span class="n">white</span><span class="w"> </span><span class="n">house</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">boarded</span><span class="w"> </span><span class="n">front</span><span class="w"> </span><span class="n">door</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="n">door</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">in</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">west</span><span class="w"> </span><span class="n">of</span>
<span class="w">        </span><span class="n">white</span><span class="w"> </span><span class="n">house</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">boarded</span><span class="w"> </span><span class="n">front</span><span class="w"> </span><span class="n">door</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="n">house</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">in</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">west</span>
<span class="w">        </span><span class="n">of</span><span class="w"> </span><span class="n">white</span><span class="w"> </span><span class="n">house</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">boarded</span><span class="w"> </span><span class="n">front</span><span class="w"> </span><span class="n">door</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="n">mailbox</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">in</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span>
<span class="w">        </span><span class="n">west</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">white</span><span class="w"> </span><span class="n">house</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">boarded</span><span class="w"> </span><span class="n">front</span><span class="w"> </span><span class="n">door</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="n">you</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">in</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span>
<span class="w">        </span><span class="n">field</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="n">you</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">in</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">open</span><span class="w"> </span><span class="n">field</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="n">you</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">in</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">west</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(</span>
<span class="w">        </span><span class="p">&#39;</span><span class="n">you</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">in</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">west</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">house</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="n">you</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">in</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">west</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">house</span>
<span class="w">        </span><span class="n">with</span><span class="w"> </span><span class="n">boarded</span><span class="w"> </span><span class="n">front</span><span class="w"> </span><span class="n">door</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="n">you</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">in</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">west</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">house</span><span class="w"> </span><span class="n">with</span>
<span class="w">        </span><span class="n">front</span><span class="w"> </span><span class="n">door</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="n">you</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">in</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">west</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">white</span><span class="w"> </span><span class="n">house</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="n">you</span><span class="p">&#39;,</span>
<span class="w">        </span><span class="n">in</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">west</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">white</span><span class="w"> </span><span class="n">house</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">boarded</span><span class="w"> </span><span class="n">front</span><span class="w"> </span><span class="n">door</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="n">you</span><span class="p">&#39;,</span>
<span class="w">        </span><span class="n">in</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">west</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">white</span><span class="w"> </span><span class="n">house</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">front</span><span class="w"> </span><span class="n">door</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="n">you</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">in</span><span class="p">&#39;,</span>
<span class="w">        </span><span class="n">west</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">boarded</span><span class="w"> </span><span class="n">front</span><span class="w"> </span><span class="n">door</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="n">you</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">in</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">west</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">front</span>
<span class="w">        </span><span class="n">door</span><span class="p">&#39;)]</span>
<span class="o">=====</span><span class="w"> </span><span class="mf">3.</span><span class="w"> </span><span class="n">Attention</span><span class="w"> </span><span class="nl">values:</span>
<span class="o">------</span><span class="w"> </span><span class="nl">attH:</span><span class="w"> </span><span class="n">o_desc</span><span class="p">,</span><span class="w"> </span><span class="n">o_inv</span><span class="p">,</span><span class="w"> </span><span class="n">o_feed</span><span class="p">,</span><span class="w"> </span><span class="n">a_past</span>
<span class="n">attH_max</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.262</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.262</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.262</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.213</span><span class="p">&#39;]</span>
<span class="n">attH_mean</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.320</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.205</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.267</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.208</span><span class="p">&#39;]</span>
<span class="n">attH_sum</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;1</span><span class="mf">.000</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.000</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.000</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.000</span><span class="p">&#39;]</span>
<span class="nl">attH_top10_mean:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.308</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.202</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.308</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.182</span><span class="p">&#39;]</span>
<span class="n">attH_top10_sum</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.497</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.007</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.493</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.003</span><span class="p">&#39;]</span>

<span class="n">attH_top25_mean</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.342</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.169</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.316</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.172</span><span class="p">&#39;]</span>
<span class="n">attH_top25_sum</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.881</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.000</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.119</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.000</span><span class="p">&#39;]</span>
<span class="n">attH_top50_mean</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.361</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.174</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.286</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.179</span><span class="p">&#39;]</span>
<span class="n">attH_top50_sum</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;1</span><span class="mf">.000</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.000</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.000</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.000</span><span class="p">&#39;]</span>
<span class="nl">attL:</span><span class="w"> </span><span class="n">connectivity</span><span class="p">,</span><span class="w"> </span><span class="n">item_in_room</span><span class="p">,</span><span class="w"> </span><span class="n">item_in_inv</span><span class="p">,</span><span class="w"> </span><span class="n">history</span>
<span class="n">attL_max</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;]</span>
<span class="n">attL_mean</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;]</span>
<span class="n">attL_sum</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.251</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.251</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.248</span><span class="p">&#39;]</span>
<span class="nl">attL_top10_mean:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;]</span>
<span class="n">attL_top10_sum</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.249</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.251</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.248</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.252</span><span class="p">&#39;]</span>
<span class="nl">attL_top25_mean:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;]</span>
<span class="n">attL_top25_sum</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.249</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.251</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.248</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.252</span><span class="p">&#39;]</span>
<span class="nl">attL_top50_mean:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;]</span>
<span class="n">attL_top50_sum</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.251</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.251</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.248</span><span class="p">&#39;]</span>
<span class="o">=====</span><span class="w"> </span><span class="mf">4.</span><span class="w"> </span><span class="n">Chosen</span><span class="w"> </span><span class="n">action</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">reward</span>
<span class="nl">Action:</span><span class="w"> </span><span class="n">west</span>
<span class="nl">Reward:</span><span class="w"> </span><span class="mh">01</span><span class="w"> </span><span class="nl">Score:</span><span class="w"> </span><span class="mh">0</span>
<span class="o">----</span><span class="w"> </span><span class="o">=====</span><span class="w"> </span><span class="n">Step</span><span class="w"> </span><span class="mh">2</span><span class="w"> </span><span class="o">=====</span><span class="w"> </span><span class="o">-----</span>
<span class="err">$</span><span class="o">====</span><span class="err">$</span><span class="w"> </span><span class="mf">1.</span><span class="w"> </span><span class="n">Textual</span><span class="w"> </span><span class="nl">obs:</span>
<span class="nl">o_desc:</span><span class="w"> </span><span class="n">Forest</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">forest</span><span class="p">,</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">trees</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">directions</span><span class="p">.</span><span class="w"> </span><span class="n">To</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">east</span><span class="p">,</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="n">appears</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">sunlight</span><span class="p">.</span>
<span class="nl">o_inv:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">empty</span><span class="o">-</span><span class="n">handed</span><span class="p">.</span>
<span class="nl">o_feed:</span><span class="w"> </span><span class="n">Forest</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">forest</span><span class="p">,</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">trees</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">directions</span><span class="p">.</span><span class="w"> </span><span class="n">To</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">east</span><span class="p">,</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="n">appears</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">sunlight</span><span class="p">.</span>
<span class="nl">a_past:</span><span class="w"> </span><span class="n">west</span>
<span class="err">$</span><span class="o">====</span><span class="err">$</span><span class="w"> </span><span class="mf">2.</span><span class="w"> </span><span class="n">Newly</span><span class="w"> </span><span class="n">extracted</span><span class="w"> </span><span class="n">triplets</span>
<span class="p">[(&#39;</span><span class="n">Forest</span><span class="p">&#39;,</span><span class="w"> </span><span class="mh">&#39;ha</span><span class="n">s</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">exit</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">east</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="n">all</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">in</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">Forest</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="w"> </span><span class="n">forest</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">is</span><span class="w"> </span><span class="n">with</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">trees</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">directions</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="n">forest</span><span class="w"> </span><span class="n">this</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="w"> </span><span class="n">is</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">forest</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="n">forest</span><span class="w"> </span><span class="n">this</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">is</span><span class="w"> </span><span class="n">forest</span><span class="w"> </span><span class="n">with</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">trees</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">directions</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="n">trees</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">in</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">Forest</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="n">trees</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">is</span><span class="w"> </span><span class="n">in</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="w"> </span><span class="n">directions</span><span class="p">&#39;),</span><span class="w"> </span><span class="p">(&#39;</span><span class="n">west</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">white</span><span class="w"> </span><span class="n">house</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">boarded</span><span class="w"> </span><span class="n">front</span><span class="w"> </span><span class="n">door</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">west</span><span class="w"> </span><span class="n">of</span><span class="p">&#39;,</span><span class="w"> </span><span class="p">&#39;</span><span class="n">Forest</span><span class="p">&#39;)]</span>
<span class="err">$</span><span class="o">====</span><span class="err">$</span><span class="w"> </span><span class="mf">3.</span><span class="w"> </span><span class="n">Attention</span><span class="w"> </span><span class="nl">values:</span>
<span class="o">-----</span><span class="w"> </span><span class="nl">attH:</span><span class="w"> </span><span class="n">o_desc</span><span class="p">,</span><span class="w"> </span><span class="n">o_inv</span><span class="p">,</span><span class="w"> </span><span class="n">o_feed</span><span class="p">,</span><span class="w"> </span><span class="n">a_past</span>
<span class="n">attH_max</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.264</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.265</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.265</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.206</span><span class="p">&#39;]</span>
<span class="n">attH_mean</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.317</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.207</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.264</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.213</span><span class="p">&#39;]</span>
<span class="n">attH_sum</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;1</span><span class="mf">.000</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.000</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.000</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.000</span><span class="p">&#39;]</span>
<span class="nl">attH_top10_mean:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.304</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.216</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.304</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.176</span><span class="p">&#39;]</span>
<span class="n">attH_top10_sum</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.491</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.016</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.491</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.002</span><span class="p">&#39;]</span>
<span class="nl">attH_top25_mean:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.342</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.174</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.308</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.175</span><span class="p">&#39;]</span>
<span class="n">attH_top25_sum</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.930</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.000</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.070</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.000</span><span class="p">&#39;]</span>
<span class="nl">attH_top50_mean:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.361</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.176</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.279</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.184</span><span class="p">&#39;]</span>
<span class="n">attH_top50_sum</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;1</span><span class="mf">.000</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.000</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.000</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.000</span><span class="p">&#39;]</span>
<span class="o">----</span><span class="w"> </span><span class="nl">attL:</span><span class="w"> </span><span class="n">connectivity</span><span class="p">,</span><span class="w"> </span><span class="n">item_in_room</span><span class="p">,</span><span class="w"> </span><span class="n">item_in_inv</span><span class="p">,</span><span class="w"> </span><span class="n">history</span>
<span class="n">attL_max</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;]</span>
<span class="n">attL_mean</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;]</span>
<span class="n">attL_sum</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.252</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.249</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.249</span><span class="p">&#39;]</span>
<span class="nl">attL_top10_mean:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;]</span>
<span class="n">attL_top10_sum</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.249</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.248</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.253</span><span class="p">&#39;]</span>
<span class="nl">attL_top25_mean:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;]</span>
<span class="n">attL_top25_sum</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.249</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.247</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.253</span><span class="p">&#39;]</span>
<span class="nl">attL_top50_mean:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;]</span>
<span class="n">attL_top50_sum</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="m">&#39;0</span><span class="mf">.252</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.250</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.249</span><span class="p">&#39;,</span><span class="w"> </span><span class="m">&#39;0</span><span class="mf">.249</span><span class="p">&#39;]</span>
<span class="err">$</span><span class="o">====</span><span class="err">$</span><span class="w"> </span><span class="mf">4.</span><span class="w"> </span><span class="n">Chosen</span><span class="w"> </span><span class="n">action</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">reward</span>
<span class="nl">Action:</span><span class="w"> </span><span class="n">south</span>
<span class="nl">Reward:</span><span class="w"> </span><span class="mh">01</span><span class="w"> </span><span class="nl">Score:</span><span class="w"> </span><span class="mh">0</span>
<span class="o">----</span><span class="w"> </span><span class="o">=====</span><span class="w"> </span><span class="n">Step</span><span class="w"> </span><span class="mh">3</span><span class="w"> </span><span class="o">=====</span><span class="w"> </span><span class="o">-----</span>
</code></pre></div>

<p>==== 1. Textual obs:
o_desc: Forest This is a dimly lit forest, with large trees all
    around.
o_inv: You are empty-handed.
o_feed: Forest This is a dimly lit forest, with large trees all
        around. You hear in the distance the chirping of a song bird.
a_past: south
==== 2. Newly extracted triplets
[('all', 'in', 'Forest'), ('forest this', 'is', 'dimly lit forest'
    ), ('forest this', 'is', 'forest'), ('forest this', 'is', 'lit
        forest'), ('forest this', 'is dimly lit forest with', 'large
        trees'), ('forest this', 'is dimly lit forest with', 'large
        trees around'), ('forest this', 'is dimly lit forest with', '
        trees'), ('forest this', 'is dimly lit forest with', 'trees
        around'), ('forest this', 'is forest with', 'large trees'), ('
        forest this', 'is forest with', 'large trees around'), ('
        forest this', 'is forest with', 'trees'), ('forest this', 'is
        forest with', 'trees around'), ('forest this', 'is lit forest
        with', 'large trees'), ('forest this', 'is lit forest with', '
        large trees around'), ('forest this', 'is lit forest with', '
        trees'), ('forest this', 'is lit forest with', 'trees around')
        , ('large', 'in', 'Forest'), ('lit forest', 'is with', 'large
        trees around'), ('this', 'is', 'lit'), ('trees', 'in', 'Forest
        '), ('west of white house with boarded front door', 'south of'
        , 'Forest'), ('you', 'hear', 'chirping'), ('you', 'hear',
        chirping of song bird'), ('you', 'hear chirping in', 'distance
        ')]
==== 3. Attention values:
---- attH: o_desc, o_inv, o_feed, a_past
attH_max : ['0.261', '0.261', '0.260', '0.218']
attH_mean : ['0.309', '0.208', '0.255', '0.228']
attH_sum : ['1.000', '0.000', '0.000', '0.000']
attH_top10_mean: ['0.294', '0.215', '0.292', '0.199']
attH_top10_sum : ['0.502', '0.022', '0.465', '0.010']
attH_top25_mean: ['0.334', '0.176', '0.290', '0.199']
attH_top25_sum : ['0.973', '0.000', '0.027', '0.000']
attH_top50_mean: ['0.350', '0.179', '0.263', '0.208']
attH_top50_sum : ['1.000', '0.000', '0.000', '0.000']
---- attL: connectivity, item_in_room, item_in_inv, history
attL_max : ['0.250', '0.250', '0.250', '0.251']
attL_mean : ['0.250', '0.250', '0.250', '0.250']
attL_sum : ['0.252', '0.251', '0.250', '0.248']
attL_top10_mean: ['0.250', '0.250', '0.250', '0.250']
attL_top10_sum : ['0.249', '0.249', '0.247', '0.255']
attL_top25_mean: ['0.250', '0.250', '0.250', '0.250']
attL_top25_sum : ['0.250', '0.249', '0.246', '0.255']
attL_top50_mean: ['0.250', '0.250', '0.250', '0.250']
attL_top50_sum : ['0.252', '0.251', '0.250', '0.248']
==== 4. Chosen action and reward
Action: south
Reward: 01 Score: 0
---- ===== Step 4 ===== -----
==== 1. Textual obs:
o_desc: Forest This is a dimly lit forest, with large trees all
around.
o_inv: You are empty-handed.
o_feed: Storm-tossed trees block your way.
a_past: south</p>
<div class="codehilite"><pre><span></span><code>
</code></pre></div>

<p>==== 2. Newly extracted triplets
[('all', 'in', 'Forest'), ('large', 'in', 'Forest'), ('lit forest'
    , 'is with', 'large trees around'), ('this', 'is', 'lit'), ('
    trees', 'block', 'your way'), ('trees', 'in', 'Forest'), ('
    west of white house with boarded front door', 'south of',
    Forest')]
==== 3. Attention values:
---- attH: o_desc, o_inv, o_feed, a_past
attH_max : ['0.263', '0.264', '0.263', '0.210']
attH_mean : ['0.315', '0.207', '0.253', '0.224']
attH_sum : ['1.000', '0.000', '0.000', '0.000']
attH_top10_mean: ['0.297', '0.212', '0.297', '0.194']
attH_top10_sum : ['0.488', '0.017', '0.488', '0.007']
attH_top25_mean: ['0.338', '0.174', '0.294', '0.194']
attH_top25_sum : ['0.971', '0.000', '0.029', '0.000']
attH_top50_mean: ['0.357', '0.178', '0.262', '0.203']
attH_top50_sum : ['1.000', '0.000', '0.000', '0.000']
---- attL: connectivity, item_in_room, item_in_inv, history
attL_max : ['0.250', '0.250', '0.250', '0.251']
attL_mean : ['0.250', '0.250', '0.250', '0.250']
attL_sum : ['0.252', '0.250', '0.250', '0.247']
attL_top10_mean: ['0.250', '0.250', '0.250', '0.250']
attL_top10_sum : ['0.250', '0.248', '0.247', '0.255']
attL_top25_mean: ['0.250', '0.250', '0.250', '0.250']
attL_top25_sum : ['0.252', '0.248', '0.246', '0.254']
attL_top50_mean: ['0.250', '0.250', '0.250', '0.250']
attL_top50_sum : ['0.252', '0.250', '0.250', '0.247']
==== 4. Chosen action and reward
Action: west
Reward: 0|Score: 0
------ ===== Step 5 ===== -----
==== 1. Textual obs:
o_desc: Forest This is a forest, with trees in all directions. To
    the east, there appears to be sunlight.
o_inv: You are empty-handed.
o_feed: Forest
a_past: west
==== 2. Newly extracted triplets
[('Forest', 'has', 'exit to east'), ('all', 'in', 'Forest'), ('
    forest', 'is with', 'trees in directions'), ('trees', 'in', '
    Forest'), ('trees', 'is in', 'directions'), ('west of white
    house with boarded front door', 'west of', 'Forest')]
==== 3. Attention values:
---- attH: o_desc, o_inv, o_feed, a_past
attH_max : ['0.268', '0.267', '0.267', '0.198']
attH_mean : ['0.325', '0.206', '0.259', '0.210']
attH_sum : ['1.000', '0.000', '0.000', '0.000']
attH_top10_mean: ['0.308', '0.213', '0.304', '0.175']
attH_top10_sum : ['0.523', '0.013', '0.462', '0.002']
attH_top25_mean: ['0.349', '0.174', '0.302', '0.175']
attH_top25_sum : ['0.973', '0.000', '0.027', '0.000']
attH_top50_mean: ['0.372', '0.176', '0.271', '0.181']
attH_top50_sum : ['1.000', '0.000', '0.000', '0.000']
---- attL: connectivity, item_in_room, item_in_inv, history
attL_max : ['0.250', '0.250', '0.250', '0.250']
attL_mean : ['0.250', '0.250', '0.250', '0.250']
attL_sum : ['0.251', '0.249', '0.249', '0.251']
attL_top10_mean: ['0.250', '0.250', '0.250', '0.250']</p>
<p>attL_top10_sum : ['0.249', '0.249', '0.250', '0.252']
attL_top25_mean: ['0.250', '0.250', '0.250', '0.250']
attL_top25_sum : ['0.250', '0.249', '0.249', '0.252']
attL_top50_mean: ['0.250', '0.250', '0.250', '0.250']
attL_top50_sum : ['0.251', '0.249', '0.249', '0.251']
===== 4. Chosen action and reward
Action: east
Reward: 01 Score: 0
------ ===== Step 6 ===== -----
$=====1$. Textual obs:
o_desc: Forest Path This is a path winding through a dimly lit forest. The path heads north-south here. One particularly large tree with some low branches stands at the edge of the path.
o_inv: You are empty-handed. You hear in the distance the chirping of a song bird.
o_feed: Forest Path This is a path winding through a dimly lit forest. The path heads north-south here. One particularly large tree with some low branches stands at the edge of the path.
a_past: east
$=====2$. Newly extracted triplets
[('Forest Path', 'has', 'exit to north'), ('Forest Path', 'has', ' exit to south'), ('all', 'in', 'Forest Path'), ('forest', 'in' , 'Forest Path'), ('one large tree', 'is with', 'low branches' ), ('path', 'in', 'Forest Path'), ('path', 'winding through', 'dimly lit forest'), ('path', 'winding through', 'forest'), (' path', 'winding through', 'lit forest'), ('tree', 'in', Forest Path'), ('west of white house with boarded front door', 'east of', 'Forest Path')]
$=====3$. Attention values:
------ attH: o_desc, o_inv, o_feed, a_past
attH_max : ['0.254', '0.250', '0.255', '0.241']
attH_mean : ['0.284', '0.223', '0.276', '0.217']
attH_sum : ['0.932', '0.000', '0.068', '0.000']
attH_top10_mean: ['0.281', '0.231', '0.287', '0.201']
attH_top10_sum : ['0.415', '0.059', '0.511', '0.015']
attH_top25_mean: ['0.302', '0.203', '0.313', '0.182']
attH_top25_sum : ['0.294', '0.000', '0.706', '0.000']
attH_top50_mean: ['0.307', '0.203', '0.299', '0.191']
attH_top50_sum : ['0.775', '0.000', '0.225', '0.000']
------ attL: connectivity, item_in_room, item_in_inv, history
attL_max : ['0.250', '0.250', '0.250', '0.250']
attL_mean : ['0.250', '0.250', '0.250', '0.250']
attL_sum : ['0.251', '0.251', '0.251', '0.247']
attL_top10_mean: ['0.250', '0.250', '0.250', '0.250']
attL_top10_sum : ['0.249', '0.251', '0.248', '0.252']
attL_top25_mean: ['0.250', '0.250', '0.250', '0.250']
attL_top25_sum : ['0.249', '0.252', '0.247', '0.253']
attL_top50_mean: ['0.250', '0.250', '0.250', '0.250']
attL_top50_sum : ['0.251', '0.251', '0.251', '0.247']
$=====4$. Chosen action and reward
Action: up
Reward: 01 Score: 0
------ ===== Step 7 ===== -----
$=====1$. Textual obs:</p>
<p>o_desc: Up a Tree You are about 10 feet above the ground nestled among some large branches. The nearest branch above you is above your reach. Beside you on the branch is a small bird's nest. In the bird's nest is a large egg encrusted with precious jewels, apparently scavenged by a childless songbird. The egg is covered with fine gold inlay, and ornamented in lapis lazuli and mother-of-pearl. Unlike most eggs, this one is hinged and closed with a delicate looking clasp. The egg appears extremely fragile.
o_inv: You are empty-handed.
o_feed: Up a Tree You are about 10 feet above the ground nestled among some large branches. The nearest branch above you is above your reach. Beside you on the branch is a small bird's nest. In the bird's nest is a large egg encrusted with precious jewels, apparently scavenged by a childless songbird. The egg is covered with fine gold inlay, and ornamented in lapis lazuli and mother-of-pearl. Unlike most eggs, this one is hinged and closed with a delicate looking clasp. The egg appears extremely fragile.
a_past: up
$====2$. Newly extracted triplets
[('all', 'in', 'up tree about 10 feet above ground nestled among large branches'), ('branch', 'in', 'up tree about 10 feet above ground nestled among large branches'), ('egg', 'in', 'up tree about 10 feet above ground nestled among large branches' ), ('ground', 'in', 'up tree about 10 feet above ground nestled among large branches'), ('nest', 'in', 'up tree about 10 feet above ground nestled among large branches'), ('west of white house with boarded front door', 'up of', 'up tree about 10 feet above ground nestled among large branches'), ('you', 'in', 'about 10 feet'), ('you', 'in', 'about 10 feet above ground'), ('you', 'in', 'about 10 feet above ground nestled'), ('you', 'in', 'about 10 feet above ground nestled among branches'), ('you', 'in', 'about 10 feet above ground nestled among large branches'), ('you', 'in', 'about 10 feet nestled') , ('you', 'in', 'about 10 feet nestled among branches'), ('you ', 'in', 'about 10 feet nestled among large branches'), ('you' , 'in', 'up tree about 10 feet'), ('you', 'in', 'up tree about 10 feet above ground'), ('you', 'in', 'up tree about 10 feet above ground nestled'), ('you', 'in', 'up tree about 10 feet above ground nestled among branches'), ('you', 'in', 'up tree about 10 feet above ground nestled among large branches'), (' you', 'in', 'up tree about 10 feet nestled'), ('you', 'in', ' up tree about 10 feet nestled among branches'), ('you', 'in', 'up tree about 10 feet nestled among large branches')]
$====3$. Attention values:
---- attH: o_desc, o_inv, o_feed, a_past
attH_max : ['0.258', '0.253', '0.259', '0.231']
attH_mean : ['0.263', '0.202', '0.331', '0.204']
attH_sum : ['0.000', '0.000', '1.000', '0.000']
attH_top10_mean: ['0.311', '0.181', '0.320', '0.187']
attH_top10_sum : ['0.422', '0.002', '0.574', '0.003']
attH_top25_mean: ['0.317', '0.162', '0.354', '0.168']
attH_top25_sum : ['0.061', '0.000', '0.939', '0.000']
attH_top50_mean: ['0.280', '0.167', '0.381', '0.171']
attH_top50_sum : ['0.000', '0.000', '1.000', '0.000']
---- attL: connectivity, item_in_room, item_in_inv, history
attL_max : ['0.250', '0.250', '0.250', '0.250']
attL_mean : ['0.250', '0.250', '0.250', '0.250']</p>
<p>attL_sum : ['0.250', '0.251', '0.250', '0.248']
attL_top10_mean: ['0.250', '0.250', '0.250', '0.250']
attL_top10_sum : ['0.249', '0.251', '0.249', '0.251']
attL_top25_mean: ['0.250', '0.250', '0.250', '0.250']
attL_top25_sum : ['0.250', '0.251', '0.248', '0.251']
attL_top50_mean: ['0.250', '0.250', '0.250', '0.250']
attL_top50_sum : ['0.250', '0.251', '0.250', '0.248']
===== 4. Chosen action and reward
Action: take egg
Reward: 5|Score: 5
----- ===== Step $8=====-----
===== 1. Textual obs:
o_desc: Up a Tree You are about 10 feet above the ground nestled among some large branches. The nearest branch above you is above your reach. Beside you on the branch is a small bird's nest.
o_inv: You are carrying: A jewel-encrusted egg
o_feed: Taken.
a_past: take egg
$====2$. Newly extracted triplets
[('all', 'in', 'about 10 feet'), ('branch', 'in', 'about 10 feet') , ('egg', 'in', 'about 10 feet'), ('ground', 'in', 'about 10 feet'), ('nest', 'in', 'about 10 feet'), ('you', 'have', ''), ('you', 'have', 'jewel-encrusted egg'), ('you', 'in', 'about 10 feet')]
$====$ 3. Attention values:
---- attH: o_desc, o_inv, o_feed, a_past
attH_max : ['0.263', '0.211', '0.263', '0.262']
attH_mean : ['0.348', '0.201', '0.248', '0.203']
attH_sum : ['1.000', '0.000', '0.000', '0.000']
attH_top10_mean: ['0.318', '0.184', '0.310', '0.188']
attH_top10_sum : ['0.557', '0.002', '0.438', '0.003']
attH_top25_mean: ['0.362', '0.169', '0.296', '0.173']
attH_top25_sum : ['0.993', '0.000', '0.007', '0.000']
attH_top50_mean: ['0.395', '0.173', '0.257', '0.175']
attH_top50_sum : ['1.000', '0.000', '0.000', '0.000']
---- attL: connectivity, item_in_room, item_in_inv, history
attL_max : ['0.250', '0.249', '0.248', '0.253']
attL_mean : ['0.250', '0.250', '0.250', '0.250']
attL_sum : ['0.255', '0.250', '0.251', '0.244']
attL_top10_mean: ['0.250', '0.249', '0.249', '0.251']
attL_top10_sum : ['0.252', '0.245', '0.240', '0.263']
attL_top25_mean: ['0.250', '0.250', '0.249', '0.250']
attL_top25_sum : ['0.256', '0.245', '0.237', '0.262']
attL_top50_mean: ['0.250', '0.250', '0.250', '0.250']
attL_top50_sum : ['0.255', '0.250', '0.251', '0.244']
$====$ 4. Chosen action and reward
Action: put egg in egg
Reward: 0| Score: 5
----- ===== Step $9=====-----
$====$ 1. Textual obs:
o_desc: Up a Tree You are about 10 feet above the ground nestled among some large branches. The nearest branch above you is above your reach. Beside you on the branch is a small bird's nest.
o_inv: You are carrying: A jewel-encrusted egg You hear in the distance the chirping of a song bird.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ We also conducted other aggregation methods such as "top10, sum", "top25, mean" and "all, sum", among which we found that "top25, sum" can best interpret the processes. See the supplementary material.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
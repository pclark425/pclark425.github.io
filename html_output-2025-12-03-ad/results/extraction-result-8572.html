<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8572 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8572</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8572</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-8005fc977a1449cde1b78a974920dc2c21f2d636</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8005fc977a1449cde1b78a974920dc2c21f2d636" target="_blank">Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts</a></p>
                <p><strong>Paper Venue:</strong> International Workshop on Neural-Symbolic Learning and Reasoning</p>
                <p><strong>Paper TL;DR:</strong> This paper investigates to what extent transformers can be trained to a) approximate reasoning in propositional logic while b) avoiding known reasoning shortcuts via spurious correlations in the training data.</p>
                <p><strong>Paper Abstract:</strong> Transformer language models are neural networks used for a wide variety of tasks concerning natural language, including some that also require logical reasoning. However, a transformer model may easily learn spurious patterns in the data, short-circuiting actual reasoning. In this paper we investigate to what extent transformers can be trained to a) approximate reasoning in propositional logic while b) avoiding known reasoning shortcuts via spurious correlations in the training data. To do so, we use a dataset with known spurious correlation between truth and e.g. the number of rules in the problem. We augment the data with proofs, and train two models: a generative transformer, WP-BART, trained on problems and their whole proofs, and a neuro-symbolic model, SIP-BART, trained on individual proof steps and combining the generative transformer model BART with a symbolic proof checker. We find that SIP-BART succeeds in avoiding reasoning shortcuts, while WP-BART does not. For SIP-BART, we then identify a few remaining reasoning errors, not previously described in the literature, arising from using a pre-trained language model. These are qualitatively analysed to create a taxonomy of four different types of additional pitfalls.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8572.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8572.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT (replication)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT classifier (Devlin et al. 2019) replication</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuned BERT-based binary classifier trained to predict the truth-value (True/False) of propositional logic queries in the SimpleLogic dataset; used as the baseline replication of Zhang et al. (2023).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT: Pre-training of deep bidirectional transformers for language understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (fine-tuned classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Bidirectional transformer encoder (pre-trained BERT) fine-tuned as a binary classifier to map a natural-language encoding of a propositional-logic problem to a True/False label. Used as the baseline replication of Zhang et al.'s experiments on SimpleLogic.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>SimpleLogic (propositional logic truth classification)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Decide whether a query literal is derivable (True) or not (False) from a set of Horn-clause rules and facts expressed in structured English; dataset partitioned into RP, LP, and RP_b subsets that expose spurious correlations (e.g., number of rules correlated with truth).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning a BERT classifier on problem → True/False labels (no explicit proof-generation or symbolic checking).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Performance highly dependent on training/test distribution: e.g., when trained on LP and tested on LP accuracy TOT = 98.6%, but cross-distribution generalization degrades (LP→RP TOT = 75.0%, LP→RP_b TOT = 72.7%). Models trained on RP/RP_b generalize to LP better than vice versa (see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Serves as the baseline; both WP-BART and SIP-BART are compared against this replication. BERT shows strong in-distribution accuracy but fails to generalize across dataset partitions due to learning spurious statistical features.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Learns and exploits spurious correlations (e.g., number of rules/facts, branching factor) rather than robust deductive reasoning; poor cross-distribution generalization; no internal proof-generation so no intermediate verifiable steps.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Classification-only fine-tuning leads to high in-distribution accuracy but brittle behavior across distributions; highlights need for intermediate step supervision or neuro-symbolic checks to avoid shortcuts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8572.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8572.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BART (pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BART (facebook/bart-base pre-trained model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained generative encoder–decoder Transformer (BART) used as the backbone for the WP-BART and SIP-BART models; fine-tuned for proof or proof-step generation on SimpleLogicPS.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART (facebook/bart-base, fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained sequence-to-sequence transformer (denoising objective) used here as the generative neural component; fine-tuned either to generate whole proofs (WP-BART) or single next-proof-steps (SIP-BART).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>SimpleLogicPS (proof-augmented SimpleLogic)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same propositional-logic derivability task as SimpleLogic but augmented with forward-chaining generated proofs and/or single inference steps to supervise generation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning the pre-trained BART to output either an entire proof string (WP-BART) or the next applicable rule / True / False token for a single step (SIP-BART neural component).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Used as backbone; actual system performance reported for the two architectures built on BART (see WP-BART and SIP-BART entries).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>BART backbone enables generative proof outputs whereas the BERT baseline only classifies; pretraining induced some failure modes (e.g., synonym substitution) discussed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Pre-training on natural language causes occasional synonym substitutions (producing Non-existing Rule or Spurious Match errors); these are an artifact of starting from a natural-language pre-trained model.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Pre-trained generative LMs provide strong sequence-generation capability but can introduce natural-language biases (synonyms, lexical generalization) that are harmful for strict symbolic reasoning; mitigations include constrained decoding or specialized vocabularies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8572.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8572.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WP-BART</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Whole-Proof BART (WP-BART)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BART-based generative model fine-tuned to generate entire proof sequences (ending in TRUE or FALSE) for SimpleLogicPS problems; trained on whole proofs generated by a backward-chaining algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>WP-BART (BART fine-tuned to generate whole proofs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative BART model that receives a problem encoding and generates a full proof string (sequence of inference steps) terminated by TRUE or FALSE; trained on SimpleLogicPS proofs produced by a backward-chaining algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>SimpleLogicPS (whole-proof generation / truth classification)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Generate a full inference chain proving (or failing to prove) a query in propositional logic expressed in structured natural language; final label TRUE/FALSE derived from generated proof.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning to output whole proofs as text (end-to-end proof generation) rather than only final label; uses proof strings from SimpleLogicPS as supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Mixed results — did not consistently improve over the BERT baseline. Example accuracies (TOT column from Table 2): WP-BART(LP) TOT = 93.9% (LP→LP), WP-BART(RP) TOT = 84.5% (RP→RP), WP-BART(RP_b) TOT = 89.9% (RP_b→RP_b); cross-distribution generalization is variable and sometimes worse than baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to the BERT classifier baseline, WP-BART does not consistently outperform and remains prone to learning shortcuts; compared to SIP-BART, WP-BART underperforms (SIP-BART achieves near-perfect accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still prone to spurious correlations (e.g., number of rules correlated with truth); WP-BART(LP) tended to over-predict False with increasing proof depth (increased false negatives); whole-proof training may not sufficiently constrain the model to avoid shortcuts and provides less locality per training signal than stepwise training.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Training a generative model to produce whole proofs is insufficient to fully prevent reliance on spurious statistical features; finer-grained (stepwise) supervision or neuro-symbolic checks are more effective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8572.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8572.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SIP-BART</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic Iterative Proof BART (SIP-BART)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic system combining a BART-based generative neural module (fine-tuned to propose one next inference or termination token) with a symbolic proof-state updater that applies and verifies proposed rules iteratively until proof completion or exhaustion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SIP-BART (neuro-symbolic BART + symbolic checker)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained BART fine-tuned to generate the next applicable rule (or True/False) for a given proof state; a separate symbolic module verifies applicability, updates the proof state, and iterates. If the neural module proposes an inapplicable rule, the symbolic module rejects it and asks the neural module to try again; iteration capped (100) to avoid non-termination.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>SimpleLogicPS (iterative proof-step generation with symbolic checking)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Iteratively select and apply single forward-chaining inference steps (from the problem's rule set) to derive the query; final True/False is output when the query is proved or the search space is exhausted under closed-world assumption.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Neuro-symbolic iterative generation: train the neural module on individual proof steps (single-step supervision), combine with a deterministic symbolic checker/state-updater to enforce sound application of rules and update facts; uses SimpleLogicPS single-step dataset (≈3,986,165 step instances).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Very high, near-perfect accuracy and consistency: overall accuracies (Table 3) are >99.8% across dataset partitions and depths (e.g., many TOT entries 99.9%+); SIP-BART models achieve almost perfect truth-value accuracy and extremely high consistency (per-proof consistency >99.9% in many settings).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Substantially outperforms the BERT classifier and WP-BART. SIP-BART nearly eliminates distributional spurious-correlation failures seen in BERT/WP-BART and generalizes across RP, LP, and RP_b with minimal degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Residual, rare errors remain: four identified error types — Non-existing Rule (0.01%–0.87% of proofs), Inapplicable Rule (up to ≈0.02%), Spurious Match (≈0.000–0.007%), and Unexhausted Search Space (up to ≈0.02%). Causes include synonym substitution from pre-training, locality bias from input ordering (mistaking conclusions for facts), and premature emission of False when search nearly exhausted.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Stepwise supervision plus a symbolic checker significantly reduces reliance on spurious statistical shortcuts and yields robust deductive behavior; combining neural pattern recognition for selection with symbolic verification for application is an effective strategy. Recommended mitigations for remaining errors include constrained decoding, shuffling input order to remove locality bias, training from scratch with a restricted vocabulary (if purely symbolic use-case), and generating multiple candidate rule proposals for symbolic filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>On the paradox of learning to reason from data <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>Selection-inference: Exploiting large language models for interpretable logical reasoning <em>(Rating: 2)</em></li>
                <li>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers <em>(Rating: 2)</em></li>
                <li>Transformers as soft reasoners over language <em>(Rating: 1)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8572",
    "paper_id": "paper-8005fc977a1449cde1b78a974920dc2c21f2d636",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "BERT (replication)",
            "name_full": "BERT classifier (Devlin et al. 2019) replication",
            "brief_description": "A fine-tuned BERT-based binary classifier trained to predict the truth-value (True/False) of propositional logic queries in the SimpleLogic dataset; used as the baseline replication of Zhang et al. (2023).",
            "citation_title": "BERT: Pre-training of deep bidirectional transformers for language understanding.",
            "mention_or_use": "use",
            "model_name": "BERT (fine-tuned classifier)",
            "model_description": "Bidirectional transformer encoder (pre-trained BERT) fine-tuned as a binary classifier to map a natural-language encoding of a propositional-logic problem to a True/False label. Used as the baseline replication of Zhang et al.'s experiments on SimpleLogic.",
            "model_size": null,
            "reasoning_task_name": "SimpleLogic (propositional logic truth classification)",
            "reasoning_task_description": "Decide whether a query literal is derivable (True) or not (False) from a set of Horn-clause rules and facts expressed in structured English; dataset partitioned into RP, LP, and RP_b subsets that expose spurious correlations (e.g., number of rules correlated with truth).",
            "method_or_approach": "Fine-tuning a BERT classifier on problem → True/False labels (no explicit proof-generation or symbolic checking).",
            "performance": "Performance highly dependent on training/test distribution: e.g., when trained on LP and tested on LP accuracy TOT = 98.6%, but cross-distribution generalization degrades (LP→RP TOT = 75.0%, LP→RP_b TOT = 72.7%). Models trained on RP/RP_b generalize to LP better than vice versa (see Table 1).",
            "baseline_comparison": "Serves as the baseline; both WP-BART and SIP-BART are compared against this replication. BERT shows strong in-distribution accuracy but fails to generalize across dataset partitions due to learning spurious statistical features.",
            "limitations_or_failures": "Learns and exploits spurious correlations (e.g., number of rules/facts, branching factor) rather than robust deductive reasoning; poor cross-distribution generalization; no internal proof-generation so no intermediate verifiable steps.",
            "insights_or_conclusions": "Classification-only fine-tuning leads to high in-distribution accuracy but brittle behavior across distributions; highlights need for intermediate step supervision or neuro-symbolic checks to avoid shortcuts.",
            "uuid": "e8572.0",
            "source_info": {
                "paper_title": "Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "BART (pretrained)",
            "name_full": "BART (facebook/bart-base pre-trained model)",
            "brief_description": "A pre-trained generative encoder–decoder Transformer (BART) used as the backbone for the WP-BART and SIP-BART models; fine-tuned for proof or proof-step generation on SimpleLogicPS.",
            "citation_title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension.",
            "mention_or_use": "use",
            "model_name": "BART (facebook/bart-base, fine-tuned)",
            "model_description": "Pre-trained sequence-to-sequence transformer (denoising objective) used here as the generative neural component; fine-tuned either to generate whole proofs (WP-BART) or single next-proof-steps (SIP-BART).",
            "model_size": null,
            "reasoning_task_name": "SimpleLogicPS (proof-augmented SimpleLogic)",
            "reasoning_task_description": "Same propositional-logic derivability task as SimpleLogic but augmented with forward-chaining generated proofs and/or single inference steps to supervise generation.",
            "method_or_approach": "Fine-tuning the pre-trained BART to output either an entire proof string (WP-BART) or the next applicable rule / True / False token for a single step (SIP-BART neural component).",
            "performance": "Used as backbone; actual system performance reported for the two architectures built on BART (see WP-BART and SIP-BART entries).",
            "baseline_comparison": "BART backbone enables generative proof outputs whereas the BERT baseline only classifies; pretraining induced some failure modes (e.g., synonym substitution) discussed in the paper.",
            "limitations_or_failures": "Pre-training on natural language causes occasional synonym substitutions (producing Non-existing Rule or Spurious Match errors); these are an artifact of starting from a natural-language pre-trained model.",
            "insights_or_conclusions": "Pre-trained generative LMs provide strong sequence-generation capability but can introduce natural-language biases (synonyms, lexical generalization) that are harmful for strict symbolic reasoning; mitigations include constrained decoding or specialized vocabularies.",
            "uuid": "e8572.1",
            "source_info": {
                "paper_title": "Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "WP-BART",
            "name_full": "Whole-Proof BART (WP-BART)",
            "brief_description": "A BART-based generative model fine-tuned to generate entire proof sequences (ending in TRUE or FALSE) for SimpleLogicPS problems; trained on whole proofs generated by a backward-chaining algorithm.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "WP-BART (BART fine-tuned to generate whole proofs)",
            "model_description": "Generative BART model that receives a problem encoding and generates a full proof string (sequence of inference steps) terminated by TRUE or FALSE; trained on SimpleLogicPS proofs produced by a backward-chaining algorithm.",
            "model_size": null,
            "reasoning_task_name": "SimpleLogicPS (whole-proof generation / truth classification)",
            "reasoning_task_description": "Generate a full inference chain proving (or failing to prove) a query in propositional logic expressed in structured natural language; final label TRUE/FALSE derived from generated proof.",
            "method_or_approach": "Fine-tuning to output whole proofs as text (end-to-end proof generation) rather than only final label; uses proof strings from SimpleLogicPS as supervision.",
            "performance": "Mixed results — did not consistently improve over the BERT baseline. Example accuracies (TOT column from Table 2): WP-BART(LP) TOT = 93.9% (LP→LP), WP-BART(RP) TOT = 84.5% (RP→RP), WP-BART(RP_b) TOT = 89.9% (RP_b→RP_b); cross-distribution generalization is variable and sometimes worse than baseline.",
            "baseline_comparison": "Compared to the BERT classifier baseline, WP-BART does not consistently outperform and remains prone to learning shortcuts; compared to SIP-BART, WP-BART underperforms (SIP-BART achieves near-perfect accuracy).",
            "limitations_or_failures": "Still prone to spurious correlations (e.g., number of rules correlated with truth); WP-BART(LP) tended to over-predict False with increasing proof depth (increased false negatives); whole-proof training may not sufficiently constrain the model to avoid shortcuts and provides less locality per training signal than stepwise training.",
            "insights_or_conclusions": "Training a generative model to produce whole proofs is insufficient to fully prevent reliance on spurious statistical features; finer-grained (stepwise) supervision or neuro-symbolic checks are more effective.",
            "uuid": "e8572.2",
            "source_info": {
                "paper_title": "Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "SIP-BART",
            "name_full": "Symbolic Iterative Proof BART (SIP-BART)",
            "brief_description": "A neuro-symbolic system combining a BART-based generative neural module (fine-tuned to propose one next inference or termination token) with a symbolic proof-state updater that applies and verifies proposed rules iteratively until proof completion or exhaustion.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SIP-BART (neuro-symbolic BART + symbolic checker)",
            "model_description": "Pre-trained BART fine-tuned to generate the next applicable rule (or True/False) for a given proof state; a separate symbolic module verifies applicability, updates the proof state, and iterates. If the neural module proposes an inapplicable rule, the symbolic module rejects it and asks the neural module to try again; iteration capped (100) to avoid non-termination.",
            "model_size": null,
            "reasoning_task_name": "SimpleLogicPS (iterative proof-step generation with symbolic checking)",
            "reasoning_task_description": "Iteratively select and apply single forward-chaining inference steps (from the problem's rule set) to derive the query; final True/False is output when the query is proved or the search space is exhausted under closed-world assumption.",
            "method_or_approach": "Neuro-symbolic iterative generation: train the neural module on individual proof steps (single-step supervision), combine with a deterministic symbolic checker/state-updater to enforce sound application of rules and update facts; uses SimpleLogicPS single-step dataset (≈3,986,165 step instances).",
            "performance": "Very high, near-perfect accuracy and consistency: overall accuracies (Table 3) are &gt;99.8% across dataset partitions and depths (e.g., many TOT entries 99.9%+); SIP-BART models achieve almost perfect truth-value accuracy and extremely high consistency (per-proof consistency &gt;99.9% in many settings).",
            "baseline_comparison": "Substantially outperforms the BERT classifier and WP-BART. SIP-BART nearly eliminates distributional spurious-correlation failures seen in BERT/WP-BART and generalizes across RP, LP, and RP_b with minimal degradation.",
            "limitations_or_failures": "Residual, rare errors remain: four identified error types — Non-existing Rule (0.01%–0.87% of proofs), Inapplicable Rule (up to ≈0.02%), Spurious Match (≈0.000–0.007%), and Unexhausted Search Space (up to ≈0.02%). Causes include synonym substitution from pre-training, locality bias from input ordering (mistaking conclusions for facts), and premature emission of False when search nearly exhausted.",
            "insights_or_conclusions": "Stepwise supervision plus a symbolic checker significantly reduces reliance on spurious statistical shortcuts and yields robust deductive behavior; combining neural pattern recognition for selection with symbolic verification for application is an effective strategy. Recommended mitigations for remaining errors include constrained decoding, shuffling input order to remove locality bias, training from scratch with a restricted vocabulary (if purely symbolic use-case), and generating multiple candidate rule proposals for symbolic filtering.",
            "uuid": "e8572.3",
            "source_info": {
                "paper_title": "Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "On the paradox of learning to reason from data",
            "rating": 2,
            "sanitized_title": "on_the_paradox_of_learning_to_reason_from_data"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "rating": 2,
            "sanitized_title": "selectioninference_exploiting_large_language_models_for_interpretable_logical_reasoning"
        },
        {
            "paper_title": "LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers",
            "rating": 2,
            "sanitized_title": "linc_a_neurosymbolic_approach_for_logical_reasoning_by_combining_language_models_with_firstorder_logic_provers"
        },
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 1,
            "sanitized_title": "transformers_as_soft_reasoners_over_language"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.012119,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts</h1>
<p>Daniel Enström ${ }^{1}$, Viktor Kjellberg ${ }^{1}$, and Moa Johansson ${ }^{2}$<br>${ }^{1}$ University of Gothenburg, Gothenburg, Sweden.<br>{gusensda, guskjevia}@student.gu.se<br>${ }^{2}$ Chalmers University of Technology, Gothenburg, Sweden.<br>moa.johansson@chalmers.se</p>
<h4>Abstract</h4>
<p>Transformer language models are neural networks used for a wide variety of tasks concerning natural language, including some that also require logical reasoning. However, a transformer model may easily learn spurious patterns in the data, short-circuiting actual reasoning. In this paper we investigate to what extent transformers can be trained to a) approximate reasoning in propositional logic while b) avoiding known reasoning shortcuts via spurious correlations in the training data. To do so, we use a dataset with known spurious correlation between truth and e.g. the number of rules in the problem. We augment the data with proofs, and train two models: a generative transformer, WP-BART, trained on problems and their whole proofs, and a neurosymbolic model, SIP-BART, trained on individual proof steps and combining the generative transformer model BART with a symbolic proof checker. We find that SIP-BART succeeds in avoiding reasoning shortcuts, while WP-BART does not. For SIP-BART, we then identify a few remaining reasoning errors, not previously described in the literature, arising from using a pre-trained language model. These are qualitatively analysed to create a taxonomy of four different types of additional pitfalls.</p>
<h2>1 Introduction</h2>
<p>Transformer models [22], have successfully been applied to a range of tasks in natural language processing [116], including those that requires the model to approximate reasoning [17. It is generally acknowledged that inducing large language models to reason step by step improves their results on reasoning tasks in natural language and when solving maths problems 8|12|0|15|23]. We are interested in understanding how and why step-by-step reasoning helps transformers solve logic problems. Does it help the model avoid learning spurious patterns and instead learn to better approximate actual reasoning?</p>
<p>In this study, we consider problems in propositional logic, expressed in structured natural language. We are not suggesting that transformer models are anywhere near specialised solvers on problems in propositional logic, and this is not the purpose of this study. Rather, we are interested in the reasoning capabilities</p>
<p>(and pitfalls) of models processing primarily natural language, but also able to do at least some reasoning on problems expressed in English. Transformers have been reported to successfully having learnt to reason about the truth of these types of logic queries, essentially by classifying the query as True or False [2,20]. However, it is not always clear if this emergent functionality corresponds to a model really having learnt deductive reasoning, or if it is picking up on some other pattern in the data. Zhang et al. [24], have experimentally shown that transformers are susceptible to learning spurious patterns inherent to reasoning problems, such as the number of rules in the problem description. With more available rules, chances are bigger that at least some of them will apply to the query and help prove it. So, it is not a bad heuristic for guessing if a query is true, but it does not constitute logical reasoning as we know it. They demonstrate this using a BERT classifier [4], trained on a dataset called SimpleLogic, consisting of propositional reasoning problems expressed in natural language and divided into subsets with different (spurious) statistical patterns correlating with the truth of the query. The BERT model fails to generalise between subsets and they conclude that it is difficult to ensure that transformers really do learn to reason: there are countless potential spurious patterns present in reasoning problems. This lack of reliable deductive reasoning capabilities in large language models has been argued to be a significant obstacle for their further development [21,5].</p>
<p>We experiment with ways of addressing this problem: reasoning ought to be robust also over data with spurious correlations between problem properties and truth values. First, we augment the training data in SimpleLogic to include not only a True/False label but also the proof steps generated by a standard forward chaining algorithm. This leads to our first research question: (1) To what extent can a generative transformer model avoid learning spurious correlation if trained to also generate short proofs?</p>
<p>Next, we also investigate a neuro-symbolic architechture more closely coupling the transformer with a symbolic checker. Here, instead of training on the whole proof at once, the generative neural model is trained to produce intermediate inference steps, one at the time, which are iterated via a rule-based symbolic system to update the proof state [18,19,3]. This leads to our second research question: (2) Can a generative transformer model fully avoid learning spurious correlations if train to generate only one proof step at the time?</p>
<p>Previous work has showed that training on step-by-step reasoning should improve accuracy on benchmarks, but no investigation has been made into the errors still made by the model. For instance, it may also be the case that the model correctly conclude a query being true/false, but still produce some faulty proof-steps along the way. This leads to our third research question: (3) What are the sources of other consistency errors and how can we avoid them?</p>
<h1>2 Method</h1>
<p>We first introduce our augmented dataset SimpleLogicPS (SimpleLogic with Proof Steps), which extends the original SimpleLogic [24] with inference steps.</p>
<p>We then describe our models, one generative transformer trained to produce whole proofs (WP-BART), and one neuro-symbolic model which combines a generative model for selecting inferences, with a symbolic module for keeping track of the proof state (SIP-BART). Finally, we describe our evaluation strategy. Additional details regarding the models can be found in the MSc thesis of the first two authors [6], and the code is available online 3</p>
<h1>2.1 Data</h1>
<p>The original SimpleLogic dataset [24], consists of 840000 simple inference problems in propositional logic. Each problem consists of a set of rules, expressed as Horn clauses with 1-3 premises, a set of facts (literals that are True) and a query literal for which a truth value is to be determined. We operate under a closed-world assumption, meaning that any query for which truth cannot be derived from the existing rules and facts, is assumed to be False. The depth of a problem refers to the minimum number of rule applications required to prove the query. For False queries, the depth refers to the maximum depth of the shallowest failing branch for that problem. See Appendix A for an illustration.</p>
<p>SimpleLogic is divided into three sub-sets, equal in size, with different statistical features. Each subset is divided with a 80-10-10 split for training, validation and testing.</p>
<p>Rule-Priority (RP) problems were generated by first randomly generating rules, facts, and a query and then computing its label by forward chaining.
Label-Priority (LP) problems were generated in the opposite way as RP, i.e. first determining a query and its truth value and then randomly sampling rules and facts consistent with this.</p>
<p>For both RP and LP, Zhang et al. shows that the number of rules and facts, as well as the branching factor of proofs are spurious statistical features correlated with the truth value of the query [24]. They thus also introduce a third subset:</p>
<p>Balanced Rule-Priority (RP_b) is similar to RP but the correlation between the number of rules and the boolean labels of queries has been removed.</p>
<p>In the original SimpleLogic dataset, the target label is just the boolean truth value of the query. To give the generative model increased guidance during training the dataset is augmented with proofs. We have thus extended all problems with inference steps to allow the transformer to learn to identify the next inference rule to apply, rather than classifying the whole query as True or False at once. The proof steps in SimpleLogicPS were generated using forward chaining from known facts until the query was found (True queries), or until the search space was exhausted (False queries).</p>
<p>For the neuro-symbolic model, which we train on single proof steps, rather than whole proofs, so each original problem was then divided into individual</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>proof steps. Each training data point thus consist of an input problem with an applicable rule as desired output (or True or False if it represented the final step). This resulted in a total of 3986165 proof step training instances. Three such example instances along with a complete proof string can be found in figure 1 .
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Example of how the generation procedure works. The three blue boxes represent three instances of input strings in SimpleLogicPS and the white boxes represent their respective output strings. The neural model thus regard each step of the proof as an isolated problem to be solved, since each step is a separate training instance. The complete proof string created by the model when inference is finished for a given problem is represented by the purple box.</p>
<h1>2.2 Model Design and Training</h1>
<p>We experiment with two different models trained also on proof information. Both are based on a pre-trained generative transformer model called BART [9], which is similar in size to the BERT classifier used in [24]. BART can also generate short text outputs, in our case a proof or proof step, not just a true/false answer. For comparison, we also replicate the experiment by [24]: training BERT based binary classifiers on each subset of SimpleLogic to produce a True/False label for each problem.</p>
<p>Whole-Proof BART (WP-BART). The WP-BART model is trained to generate the proof for the input problem as a string, ending in TRUE or FALSE.</p>
<p>It is trained on SimpleLogic proofs generated by a simple backward chaining algorithm.</p>
<p>Symbolic Iterative Proof-BART (SIP-BART). The SIP-BART model combines the neural BART-component with a symbolic module responsible for updating the current state of the proof, see Figure 2.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: An overview of the SIP-BART architecture using an example of data-flow during inference.</p>
<p>The neural module is a pre-trained BART model which is fine-tuned to generate the next applicable rule for SimpleLogicPS-problems as an output, a boolean string True when the proof is judged to be completed, or False when the search space is judged to be exhausted and no more rules are applicable. The symbolic module is responsible for applying the rule produced by the neural module, and produce a new textual representation for the next proof step, which serves as the next input to the neural model. Should the neural model produce a suggestion of a rule which is in fact not applicable, the symbolic model records this faulty step, but does not update the state, and asks the neural model to "try again". This is however very rare, most generated steps are valid (see analysis of consistency in $\S 3.2$ ). This iteration continues until the neural model outputs True or False, or the number of iterations reaches a maximum threshold to avoid potential non-termination ${ }^{4}$. The symbolic module then terminates the generation and</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>outputs the final sequence of inference steps. If the system fails to return a chain of inferences ending in True or False the result is treated as a False Positive or False Negative depending on the ground truth label.</p>
<p>Training. We train one WP-BART/SIP-BART model for each subset of SimpleLogic: LP, RP and RP_b. We denote the differently trained models by WP$\operatorname{BART}(\mathrm{LP}) / \mathrm{SIP}-\mathrm{BART}(\mathrm{LP})$ etc. Each model was then evaluated on all subsets to be able to analyse to what extent the models learned spurious correlations from the data. The models were all trained for one epoch (as additional epochs did not improve results) on four NVIDIA Tesla A40 GPUs. Refer to the Appendix B for detailed information about training parameters and hardware.</p>
<h1>2.3 Evaluation</h1>
<p>Evaluation was performed in two stages. First, addressing research questions 1 and 2, we evaluate the correctness of the truth-value of the query. Here, we compare the accuracy of the BERT replication with the final labels produced by WP-BART and SIP-BART (disregarding the intermediate details of the proof).</p>
<p>Secondly, to address our third research question, we analyse the sequence of inferences produced of our best performing architecture above. Here, the soundness and completeness of the inference steps were evaluated to check its consistency. A proof with no validity issues or extra faulty steps and no missing inference steps is regarded as consistent. We analyse the errors in inconsistent proofs and identify four situations which give rise to inconsistencies:</p>
<p>Non-existing Rule (NonExR). Here, the neural model have suggested a rule which is not present in the problem description.
Inapplicable Rule (InappR). The neural model have suggested a rule which is present, but not applicable, as its premises are not satisfied (the relevant facts are not present in the problem description).
Spurious Match (SpMatch). Here, the neural model have produced the final output True to conclude the proof prematurely (the query is in fact not proved to be True yet - a spurious match with some other fact).
Unexhausted Search Space (UnexhS). The neural model have prematurely produced the final output False, while there are still rules available that would be applicable (the search space is not yet fully explored).</p>
<h2>3 Results</h2>
<h3>3.1 Accuracy of Truth Values</h3>
<p>We first assess the accuracy of the final labels (True or False) assigned to the queries of each problem in the SimpleLogic test sets. The results of the replication experiment of [24] are shown Table 1. We observe the same pattern in the data as</p>
<p>is described in their article ${ }^{5}$ - a BERT classifier trained on specific distributions has a hard time generalizing to other distributions due to the model learning statistical features rather than approximating reasoning. The model trained on RP and RP_b generalised better to LP (total accuracy of $86.7 \%$ and $87.5 \%$ respectively) than LP to RP and RP_b (total accuracy of $75.0 \%$ and $72.5 \%$ respectively). Training on RP_b result in slightly better performance than on RP. The accuracy also generally drops with increased proof-depth.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">TRAIN</th>
<th style="text-align: left;">TEST</th>
<th style="text-align: left;">$\mathbf{0}$</th>
<th style="text-align: left;">$\mathbf{1}$</th>
<th style="text-align: left;">$\mathbf{2}$</th>
<th style="text-align: left;">$\mathbf{3}$</th>
<th style="text-align: left;">$\mathbf{4}$</th>
<th style="text-align: left;">$\mathbf{5}$</th>
<th style="text-align: left;">$\mathbf{6}$</th>
<th style="text-align: left;">TOT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">99.8</td>
<td style="text-align: left;">99.8</td>
<td style="text-align: left;">99.8</td>
<td style="text-align: left;">99.6</td>
<td style="text-align: left;">98.8</td>
<td style="text-align: left;">97.2</td>
<td style="text-align: left;">95.4</td>
<td style="text-align: left;">$\mathbf{9 8 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">97.4</td>
<td style="text-align: left;">92.5</td>
<td style="text-align: left;">64.5</td>
<td style="text-align: left;">60.2</td>
<td style="text-align: left;">67.6</td>
<td style="text-align: left;">72.6</td>
<td style="text-align: left;">69.9</td>
<td style="text-align: left;">$\mathbf{7 5 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">97.7</td>
<td style="text-align: left;">93.3</td>
<td style="text-align: left;">60.2</td>
<td style="text-align: left;">56.7</td>
<td style="text-align: left;">63.9</td>
<td style="text-align: left;">68.7</td>
<td style="text-align: left;">68.5</td>
<td style="text-align: left;">$\mathbf{7 2 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">99.9</td>
<td style="text-align: left;">99.9</td>
<td style="text-align: left;">99.0</td>
<td style="text-align: left;">94.3</td>
<td style="text-align: left;">83.8</td>
<td style="text-align: left;">65.6</td>
<td style="text-align: left;">50.0</td>
<td style="text-align: left;">$\mathbf{8 4 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">99.8</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">99.4</td>
<td style="text-align: left;">98.9</td>
<td style="text-align: left;">98.6</td>
<td style="text-align: left;">96.9</td>
<td style="text-align: left;">95.9</td>
<td style="text-align: left;">$\mathbf{9 8 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">99.2</td>
<td style="text-align: left;">99.2</td>
<td style="text-align: left;">98.6</td>
<td style="text-align: left;">98.0</td>
<td style="text-align: left;">96.6</td>
<td style="text-align: left;">93.9</td>
<td style="text-align: left;">89.1</td>
<td style="text-align: left;">$\mathbf{9 6 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">99.7</td>
<td style="text-align: left;">99.4</td>
<td style="text-align: left;">99.3</td>
<td style="text-align: left;">96.4</td>
<td style="text-align: left;">87.6</td>
<td style="text-align: left;">72.6</td>
<td style="text-align: left;">57.2</td>
<td style="text-align: left;">$\mathbf{8 7 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">99.8</td>
<td style="text-align: left;">99.9</td>
<td style="text-align: left;">99.5</td>
<td style="text-align: left;">98.9</td>
<td style="text-align: left;">98.6</td>
<td style="text-align: left;">97.9</td>
<td style="text-align: left;">96.9</td>
<td style="text-align: left;">$\mathbf{9 8 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">99.6</td>
<td style="text-align: left;">99.5</td>
<td style="text-align: left;">99.0</td>
<td style="text-align: left;">98.4</td>
<td style="text-align: left;">98.0</td>
<td style="text-align: left;">96.7</td>
<td style="text-align: left;">94.1</td>
<td style="text-align: left;">$\mathbf{9 7 . 9}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Accuracy from the BERT-classifier replication of [24] trained on the different distributions. The integers in the heading refer to the depth of the ground-truth proof. TOT is the average accuracy across all proof depths. There are roughly the same number of problems with proofs of each depth.</p>
<p>Accuracy of WP-BART. Table 2 shows accuracy for the WP-BART model. It does not significantly improve overall compared to the baseline BART classifier. While WP-BART(LP) appears to generalise slightly better, the other variants perform worse. However, we also noticed that with increasing depth, the proportion of false queries in the data increase. The WP-BART(LP) model appears to have learned this as a reasoning shortcut, and over-predict the conclusion False, which can be seen in as an increased rate of False Negatives with proof depth, see Table 1 in Appendix C. We also note that the WP-BART(RP) model (trained on data where the number of rules correlated with truth) does not generalise as well as the model WP-BART(RP_b) (trained on balanced data), suggesting that WP-BART is still prone to learn reasoning shortcut correlating the number of rules and truth.</p>
<p>Accuracy of SIP-BART. The accuracy scores from the three SIP-BART models tested on all subsets are summarised in Table 3. There is a clear improvement in accuracy when compared to the BERT classifier in Table 1. Additionally,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">TRAIN</th>
<th style="text-align: left;">TEST</th>
<th style="text-align: left;">$\mathbf{0}$</th>
<th style="text-align: left;">$\mathbf{1}$</th>
<th style="text-align: left;">$\mathbf{2}$</th>
<th style="text-align: left;">$\mathbf{3}$</th>
<th style="text-align: left;">$\mathbf{4}$</th>
<th style="text-align: left;">$\mathbf{5}$</th>
<th style="text-align: left;">$\mathbf{6}$</th>
<th style="text-align: left;">TOT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">92.6</td>
<td style="text-align: left;">90.2</td>
<td style="text-align: left;">89.8</td>
<td style="text-align: left;">91.2</td>
<td style="text-align: left;">93.3</td>
<td style="text-align: left;">$\mathbf{9 3 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">99.9</td>
<td style="text-align: left;">83.3</td>
<td style="text-align: left;">65.5</td>
<td style="text-align: left;">67.3</td>
<td style="text-align: left;">72.0</td>
<td style="text-align: left;">76.5</td>
<td style="text-align: left;">$\mathbf{8 0 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">99.9</td>
<td style="text-align: left;">82.1</td>
<td style="text-align: left;">64.9</td>
<td style="text-align: left;">66.3</td>
<td style="text-align: left;">74.0</td>
<td style="text-align: left;">83.0</td>
<td style="text-align: left;">$\mathbf{8 1 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">84.7</td>
<td style="text-align: left;">85.4</td>
<td style="text-align: left;">79.2</td>
<td style="text-align: left;">73.9</td>
<td style="text-align: left;">71.6</td>
<td style="text-align: left;">68.5</td>
<td style="text-align: left;">63.4</td>
<td style="text-align: left;">$\mathbf{7 5 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">84.3</td>
<td style="text-align: left;">88.5</td>
<td style="text-align: left;">87.9</td>
<td style="text-align: left;">87.6</td>
<td style="text-align: left;">85.0</td>
<td style="text-align: left;">79.8</td>
<td style="text-align: left;">78.1</td>
<td style="text-align: left;">$\mathbf{8 4 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">87.7</td>
<td style="text-align: left;">88.3</td>
<td style="text-align: left;">88.8</td>
<td style="text-align: left;">87.7</td>
<td style="text-align: left;">85.4</td>
<td style="text-align: left;">80.7</td>
<td style="text-align: left;">79.7</td>
<td style="text-align: left;">$\mathbf{8 5 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">84.1</td>
<td style="text-align: left;">94.3</td>
<td style="text-align: left;">89.6</td>
<td style="text-align: left;">85.1</td>
<td style="text-align: left;">80.9</td>
<td style="text-align: left;">76.6</td>
<td style="text-align: left;">71.6</td>
<td style="text-align: left;">$\mathbf{8 3 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">84.0</td>
<td style="text-align: left;">94.2</td>
<td style="text-align: left;">94.3</td>
<td style="text-align: left;">92.2</td>
<td style="text-align: left;">89.3</td>
<td style="text-align: left;">84.8</td>
<td style="text-align: left;">82.2</td>
<td style="text-align: left;">$\mathbf{8 8 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">87.2</td>
<td style="text-align: left;">93.6</td>
<td style="text-align: left;">94.1</td>
<td style="text-align: left;">93.3</td>
<td style="text-align: left;">88.6</td>
<td style="text-align: left;">86.8</td>
<td style="text-align: left;">85.7</td>
<td style="text-align: left;">$\mathbf{8 9 . 9}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracy for the WP-BART models trained on the different distributions measured in percentage of correct predictions. The integers refer to the depth of the ground-truth proof. TOT is the overall average accuracy across proofs of all depths. There are roughly the same number of inference-problems for each depth.
the SIP-BART models achieved an almost perfect accuracy on all three testing sets, regardless of training set, with minimal or no drop as proof-depth increase. The accuracy for SIP-BART(RP) and SIP-BART(RP_b) are equal, and slightly better than SIP-BART(LP) which performs worse on proofs of increased depth. Furthermore, no substantial over-prediction of conclusion False can be seen with the increased proof depths for SIP-BART(LP), see Table 1 in Appendix D. This indicates that, unlike the BERT classifier and WP-BART, SIP-BART has to a much larger extent avoided learning the spurious patterns present such as the number of rules. The SIP-BART architecture is still not completely invariable to changes in distribution, as seen by the slightly lower accuracy for SIP-BART(LP) tested on RP and RP_b. Though, it must be said this is only a minor issue the accuracy of the different SIP-BART models are all above $99.8 \%$ in total.</p>
<p>In summary, this reinforces the result by [19]: a stepwise generation of the proof improves the models ability to solve logic based problems expressed in natural language. In addition, we also show that the reason for this improvement is that the models are less likely to internalise spurious correlations that exist in the data. But even if the models are able to achieve a high accuracy they are not able to solve all problems, which indicates that there are still flaws in their approximation of reasoning. It is therefore necessary to further analyse what type of errors that are committed and the reasons for them. We proceed by doing so for our best performing model, SIP-BART.</p>
<h1>3.2 Consistency of SIP-BART</h1>
<p>Although the accuracy tells us how well a given model is able to determine the truth-value of the query in each problem, it does not take into account if each of the inference steps themselves are sound and complete (i.e., consistent). Even though the consistency scores where high, with all SIP-BART models achieving</p>
<table>
<thead>
<tr>
<th style="text-align: left;">TRAIN</th>
<th style="text-align: left;">TEST</th>
<th style="text-align: left;">$\mathbf{0}$</th>
<th style="text-align: left;">$\mathbf{1}$</th>
<th style="text-align: left;">$\mathbf{2}$</th>
<th style="text-align: left;">$\mathbf{3}$</th>
<th style="text-align: left;">$\mathbf{4}$</th>
<th style="text-align: left;">$\mathbf{5}$</th>
<th style="text-align: left;">$\mathbf{6}$</th>
<th style="text-align: left;">TOT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">99.94</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">99.97</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">$\mathbf{9 9 . 9 8}$</td>
</tr>
<tr>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">99.97</td>
<td style="text-align: left;">99.97</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">99.92</td>
<td style="text-align: left;">99.90</td>
<td style="text-align: left;">99.872</td>
<td style="text-align: left;">99.49</td>
<td style="text-align: left;">$\mathbf{9 9 . 8 7}$</td>
</tr>
<tr>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">99.97</td>
<td style="text-align: left;">99.97</td>
<td style="text-align: left;">99.89</td>
<td style="text-align: left;">99.94</td>
<td style="text-align: left;">99.74</td>
<td style="text-align: left;">99.25</td>
<td style="text-align: left;">$\mathbf{9 9 . 8 1}$</td>
</tr>
<tr>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">99.97</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">$\mathbf{9 9 . 9 9}$</td>
</tr>
<tr>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">99.97</td>
<td style="text-align: left;">$\mathbf{9 9 . 9 9}$</td>
</tr>
<tr>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">$\mathbf{1 0 0 .}$</td>
</tr>
<tr>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">99.97</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">99.97</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">$\mathbf{9 9 . 9 9}$</td>
</tr>
<tr>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">99.94</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">99.97</td>
<td style="text-align: left;">99.97</td>
<td style="text-align: left;">99.97</td>
<td style="text-align: left;">$\mathbf{9 9 . 9 8}$</td>
</tr>
<tr>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">99.97</td>
<td style="text-align: left;">100.</td>
<td style="text-align: left;">$\mathbf{9 9 . 9 9}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracy for the SIP-BART models trained on the different distributions measured in percentage of correct predictions. The integers refer to the depth of the ground-truth proof. TOT is the overall average accuracy across proofs of all depths. There are roughly the same number of inference-problems for each depth. 100. means exactly 100.. Values very close to 100. have been floored to 99.99 .
above $99 \%$ fully consistent proofs, there are four types of reoccurring errors, as described in $\S 2.3$. The frequencies of these are shown in Table 4. Again, SIPBART(LP) gets a somewhat lower score that compared to the other two models, with the lowest consistency of the series of inference steps being $99.11 \%$ when tested on RP_b. The following sections will be dedicated to analysing each of these types and highlighting some of the reasons for why they occur.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Train</th>
<th style="text-align: left;">Test</th>
<th style="text-align: left;">NonExR</th>
<th style="text-align: left;">InappR</th>
<th style="text-align: left;">SpMatch</th>
<th style="text-align: left;">UnexhS</th>
<th style="text-align: left;">Error <br> Rate</th>
<th style="text-align: left;">Tot. Consis- <br> tency</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">0.036</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.007</td>
<td style="text-align: left;">0.004</td>
<td style="text-align: left;">0.046</td>
<td style="text-align: left;">99.954</td>
</tr>
<tr>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">0.661</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.004</td>
<td style="text-align: left;">0.021</td>
<td style="text-align: left;">0.686</td>
<td style="text-align: left;">99.314</td>
</tr>
<tr>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">0.868</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.021</td>
<td style="text-align: left;">0.889</td>
<td style="text-align: left;">99.111</td>
</tr>
<tr>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">0.018</td>
<td style="text-align: left;">0.004</td>
<td style="text-align: left;">0.004</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.025</td>
<td style="text-align: left;">99.975</td>
</tr>
<tr>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">0.007</td>
<td style="text-align: left;">0.018</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.004</td>
<td style="text-align: left;">0.029</td>
<td style="text-align: left;">99.971</td>
</tr>
<tr>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">0.021</td>
<td style="text-align: left;">0.014</td>
<td style="text-align: left;">0.004</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.039</td>
<td style="text-align: left;">99.961</td>
</tr>
<tr>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">0.011</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.004</td>
<td style="text-align: left;">0.004</td>
<td style="text-align: left;">0.018</td>
<td style="text-align: left;">99.982</td>
</tr>
<tr>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">0.032</td>
<td style="text-align: left;">0.011</td>
<td style="text-align: left;">0.007</td>
<td style="text-align: left;">0.011</td>
<td style="text-align: left;">0.061</td>
<td style="text-align: left;">99.939</td>
</tr>
<tr>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">0.025</td>
<td style="text-align: left;">0.007</td>
<td style="text-align: left;">0.004</td>
<td style="text-align: left;">0.004</td>
<td style="text-align: left;">0.039</td>
<td style="text-align: left;">99.961</td>
</tr>
</tbody>
</table>
<p>Table 4: Frequencies of different types of consistency-errors in percent for the different training and testing sets. The 0.000 values are close to zero but not exactly zero. The 0 . values are exactly zero.</p>
<p>Non-existing Rule Across most of the SIP-BART models, the most common error was Non-existing Rule, which occurred in between $0.01 \%-0.87 \%$ of proofs,</p>
<p>depending on the test set. Based on qualitative analysis, it could be observed that the most common reason for this error was that the model had generated part of a existing rule but not the complete rule. The generated step often included the latter part of an existing rule but had missed one or more of the premises, see the right-side example in Figure 3. Another error was that the model generated synonyms to the word in a rule instead of the word that existed in the original rule (Figure 3, left), where the generated rule includes the word "courageous" instead of "fearless". Interestingly, most of the steps in the inference procedure that include non-existing rules are still almost correct (we can view them as near misses). In most cases, the model still provides a correct final classification of the problem, even when out-of-distribution.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Two examples of where the Non-existing Rule error has occurred. On the left, the generated rule include a synonym word "courageous" instead of "fearless". The second example (right) the model generated part of the rule but missed the premise "charming".</p>
<p>Inapplicable Rule Inapplicable Rule errors occurred in up to 0.02 percent of the attempted proofs. It was the second most common error for SIP-BART(RP) and SIP-BART(RP_b) but did not occur at all for SIP-BART(LP), see Table 4. Qualitative analysis suggests that the cause for this error type is the order</p>
<p>and location of rules and facts in the input: The input is structured so that the query, rules and facts are located following each other in the input string, and the analysis indicates that the model has mistaken parts of the rules close to the facts, as facts. Two examples of this are shown in Figure 4.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Two examples of where the generated inference steps include inapplicable rules. In both examples the conclusion of the last rule has been mistaken as a fact. The model confuses the conclusion of the last rule in the input string, regarding it as a fact instead of a rule that is not yet satisfied.</p>
<p>Spurious Match The third type of error, Spurious Match, is the least common error in total, with a prevalence of 0.000 to 0.007 percent. In all cases, the reason was that the model mistakes a synonym word to the query as the actual query and the model predicts the query as True (similar to what happened for Nonexistent Rule). In essence, this error means that the system has proved the wrong query. This is exemplified in Figure 5 where the series of inference steps ends with a rule with the conclusion adorable and a True label is generated, but the query to the problem asked for is in fact the synonym cute.</p>
<p>Unexhausted Search Space The final type of error, Unexhausted Search Space, is only applicable for problems predicted to be False, with a prevalence of up to $0.02 \%$. This was the second most common error for SIP-BART(LP) when tested on RP and RP_b and was the only error that was found for the false negative problems. No obvious patterns could be observed in why this error</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5: Two examples of where a Spurious Match has occurred. In the first example (left) the fact glamorous has been mistaken as the query gorgeous. In the second example (right) the conclusion in the generated rule aggressive,attentive $\Rightarrow$ adorable is mistaken as the query cute.
occurred, other than that in most cases the model had found all but one rule, which meant that most of these errors occurred on true negatives. This is likely due to the probability of generating the token False increases as the search space is getting closer to exhaustion. When that happens, the probability for that token might get too high slightly too quickly, i.e., before the search space is exhausted.</p>
<p>Summary In summary, even if all the models achieve high levels of consistency, they are not immune to producing errors. In total the most frequent error is Non-existing Rule, but the different SIP-BART models differ somewhat in the distribution of error-types. For all the models the accuracy is higher than the consistency, some proofs contain erroneous steps, but still end with the correct Boolean label for the query. However, the difference between accuracy and consistency is quite small, which means that almost all correct classifications are consistent - no model has more than $0.7 \%$ consistency errors on any of the given testing set.</p>
<h1>4 Discussion and Conclusion</h1>
<p>By generating inference steps, SIP-BART minimises the effect of known spurious correlations present in the SimpleLogic dataset. The model divides each problem into smaller sub-problems that are easier to solve, which helps approximate</p>
<p>reasoning without exploiting statistical features as shortcuts. This also makes it easier for the model to approach the parts of the problem in the correct order. SIP-BART does this by playing to the strengths of each of its modules. The pattern recognition task, which is handled by the neural-module, is in essence an inductive task where conclusions are drawn from observations. The symbolic module instead handles the deductive part, the transition between each reasoning step. By limiting the work that needs to be done by the neural-module, the SIP-BART model can focus on only trying to find rules that are applicable without the need to take into consideration the previous or the next steps of the inference procedure. One explanation for this is the importance of locality in the training data [13]: training on individual proof-steps instead of the final outcome increase locality between the current goal and the desired output (the next applicable rule).</p>
<h1>4.1 Mitigation Strategies</h1>
<p>Although SIP-BART perform much better than the BERT classifier, it still makes some spurious errors which we identified in our experiments. Below, we also suggest some strategies for mitigating these. Evaluating the strategies fully is left as further work, and should ideally be done on a dataset where these errors are more common than in SimpleLogic to ensure effects are significant.</p>
<p>Dealing with Synonyms All the SIP-BART models were fine-tuned from a pre-trained BART-model. A adverse effect of this became apparent from the quantitative analysis: words were occasionally swapped for synonyms. This makes sense in natural language, but not in logic inferences. All Spurious Matches and many of the Non-existing Rule-errors are due to such synonym-issues. One solution is to train a transformer from scratch, on a restricted vocabulary, instead of starting from a pre-trained model. However, if we want a model that can both reason and understand instructions expressed in natural language, this is not the best solution. This may also require additional computational resources for training. Another potential solution could be to use a constrained decoder [7], which only allows generation of strings adhering to a specified grammar, i.e. only well-formed rules over a specific vocabulary.</p>
<p>Dealing with Locality Bias The input string describing the problem contains the list of rules, followed by the list of facts. Inapplicable Rule and Non-existing Rule errors are closely tied to the locality structure of the input and can be considered a spurious statistical feature learned by the models. In the Inapplicable Rule-case, the models mistakenly treat the tokens toward the end of the input string as facts, instead of the conclusion of a rule, despite facts being demarcated by a different symbol ${ }^{6}$. Similarly, when the model disregards a premise of</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>a rule when making a Non-existing Rule error, it seems like it has not completely internalised the scope of the rules premises, despite that all the literals in the premise are demarcated with a ","-sign. A possible way of solving this would be to shuffle the order of the query, rules and facts in the input string of the training data. This might teach the model to avoid focusing on the token's position in the input string when it instead should be paying attention to the demarcation symbols that follow the token.</p>
<p>Dealing with Search Space Issues This strategy relies on a neuro-symbolic architecture (unlike the previous two). A standard approach is to let the neural model generate multiple answers for the same input, then letting the symbolic module check which ones are in fact valid inferences. This will of course induce a small overhead in runtime. This strategy is particularly relevant for Unexhausted Search Space errors, where we need to compensate for the neural model jumping to conclusions too quickly, before the final inferences has been generated.</p>
<h1>4.2 Conclusion</h1>
<p>Our study provides empirical evidence as to why inducing transformers to reasoning step by step generally better approximated logical reasoning - it helps avoiding spurious statistical patterns such as correlations between the number of rules and facts and the truth value of the query. Training on whole proofs was not sufficient, the transformer still picked up reasoning shortcuts. Part of the reason for this is likely that training on proof-steps gave more training data: the model would benefit for each step in each training proof. It is also possible that the representation of "failed" proofs for false queries was difficult to learn for WP-BART. However, for computational resource reasons, this was not investigated further.</p>
<p>We also identify four additional types of spurious errors made by the transformer model, arising from replacement of a word by a synonym (an effect of pre-training on natural language), mistaking part of a rule for a fact when located closely in the input or jumping to a final conclusion before completing the proof. We proposed some mitigation strategies to counter this, some of which can be implemented directly for the transformer, but for robust reasoning, we believe it is necessary to implement a neuro-symbolic architecture. Here, a neural module suggests (perhaps multiple) rules and a symbolic module checks them and deals with updates to the problem representation. Another approach is to use a language model just as a semantic parser and train it to translate the whole problem into the input language of a symbolic prover [11]. However, one still needs to deal with similar errors as those discussed here causing subtle mistakes in translation to propagate through to the reasoning engine.</p>
<h2>Acknowledgement</h2>
<p>The computations and storage of data were enabled by resources provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS) at</p>
<p>Chalmers Centre for Computational Science and Engineering (C3SE), partially funded by the Swedish Research Council through grant agreement no. 202206725. We would also like to thank Nicholas Smallbone for feedback on a draft of this paper.</p>
<h1>References</h1>
<ol>
<li>Britz, D., Goldie, A., Luong, M.T., Le, Q.V.: Massive exploration of neural machine translation architectures. arXiv preprint arXiv:1703.03906 (2017)</li>
<li>Clark, P., Tafjord, O., Richardson, K.: Transformers as soft reasoners over language. In: Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. IJCAI'20 (2021)</li>
<li>Creswell, A., Shanahan, M., Higgins, I.: Selection-inference: Exploiting large language models for interpretable logical reasoning. In: The Eleventh International Conference on Learning Representations (2023), https://openreview.net/forum? id=3Pf3Wg5o-A4</li>
<li>Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidirectional transformers for language understanding. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). pp. 4171-4186. Association for Computational Linguistics, Minneapolis, Minnesota (Jun 2019). https://doi.org/10.18653/v1/N19-1423, https://aclanthology. org/N19-1423</li>
<li>Elazar, Y., Kassner, N., Ravfogel, S., Ravichander, A., Hovy, E., Schütze, H., Goldberg, Y.: Measuring and improving consistency in pretrained language models. Transactions of the Association for Computational Linguistics 9, 1012-1031 (2021). https://doi.org/10.1162/tacl_a_00410, https://aclanthology.org/ 2021.tacl-1.60</li>
<li>Enström, D., Kjellberg, V.: Approximating reasoning with transformer language models (2023), https://gupea.ub.gu.se/handle/2077/78873, MSc thesis Gothenburg University</li>
<li>Geng, S., Josifosky, M., Peyrard, M., West, R.: Flexible grammar-based constrained decoding for language models (2023)</li>
<li>Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language models are zero-shot reasoners. In: Oh, A.H., Agarwal, A., Belgrave, D., Cho, K. (eds.) Advances in Neural Information Processing Systems (2022), https://openreview. net/forum?id=e2TBb5y0yFf</li>
<li>Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., Zettlemoyer, L.: BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871-7880. Association for Computational Linguistics, Online (Jul 2020). https://doi.org/10.18653/v1/2020.acl-main.703, https: //aclanthology.org/2020.acl-main. 703</li>
<li>Nye, M., Andreassen, A.J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., Odena, A.: Show your work: Scratchpads for intermediate computation with language models (2021)</li>
<li>Olausson, T., Gu, A., Lipkin, B., Zhang, C., Solar-Lezama, A., Tenenbaum, J., Levy, R.: LINC: A neurosymbolic approach for logical reasoning by combining</li>
</ol>
<p>language models with first-order logic provers. In: Bouamor, H., Pino, J., Bali, K. (eds.) Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. pp. 5153-5176. Association for Computational Linguistics, Singapore (Dec 2023). https://doi.org/10.18653/v1/2023.emnlp-main. 313, https://aclanthology.org/2023.emnlp-main. 313
12. Polu, S., Sutskever, I.: Generative language modeling for automated theorem proving (2020). https://doi.org/10.48550/ARXIV.2009.03393, https://arxiv.org/ abs/2009.03393
13. Prystawski, B., Goodman, N.D.: Why think step-by-step? Reasoning emerges from the locality of experience (2023)
14. PyTorch Development Team: Reproducibility. https://pytorch.org/docs/ stable/notes/randomness.html (Accessed 2023), accessed on May 23, 2023
15. Rabe, M.N., Lee, D., Bansal, K., Szegedy, C.: Mathematical reasoning via self-supervised skip-tree training. arXiv: Learning (2020). https://doi.org/10. 48550/ARXIV.2006.04757, https://arxiv.org/abs/2006.04757
16. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019)
17. Saxton, D., Grefenstette, E., Hill, F., Kohli, P.: Analysing mathematical reasoning abilities of neural models. In: International Conference on Learning Representations (2019), https://openreview.net/forum?id=H1gR5iR5FX
18. Susskind, Z., Arden, B., John, L.K., Stockton, P.A., John, E.B.: Neurosymbolic ai: An emerging class of ai workloads and their characterization. ArXiv abs/2109.06133 (2021)
19. Tafjord, O., Dalvi, B., Clark, P.: ProofWriter: Generating implications, proofs, and abductive statements over natural language. In: Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. pp. 3621-3634. Association for Computational Linguistics, Online (Aug 2021). https://doi.org/10.18653/v1/ 2021.findings-acl.317, https://aclanthology.org/2021.findings-acl. 317
20. Talmor, A., Tafjord, O., Clark, P., Goldberg, Y., Berant, J.: Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. NIPS'20, Curran Associates Inc., Red Hook, NY, USA (2020)
21. Valmeekam, K., Olmo, A., Sreedharan, S., Kambhampati, S.: Large language models still can't plan (a benchmark for llms on planning and reasoning about change). ArXiv abs/2206.10498 (2022). https://doi.org/10.48550/ARXIV.2206.10498, https://arxiv.org/abs/2206.10498
22. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: Proceedings of the 31st International Conference on Neural Information Processing Systems. p. 6000-6010. NIPS'17, Curran Associates Inc., Red Hook, NY, USA (2017). https://doi.org/ 10.48550/ARXIV.1706.03762, https://arxiv.org/abs/1706.03762
23. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q.V., Zhou, D.: Chain-of-thought prompting elicits reasoning in large language models. In: Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., Oh, A. (eds.) Advances in Neural Information Processing Systems. vol. 35, pp. 24824-24837. Curran Associates, Inc. (2022), https://proceedings.neurips.cc/paper_files/paper/2022/file/ 9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf
24. Zhang, H., Li, L.H., Meng, T., Chang, K., den Broeck, G.V.: On the paradox of learning to reason from data. In: Proceedings of the Thirty-Second International</p>
<p>Joint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China. pp. 3365-3373. ijcai.org (2023). https://doi.org/10.24963/ IJCAI.2023/375, https://doi.org/10.24963/ijcai.2023/375</p>
<h1>A Example SimpleLogic Problem</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6: Example of the structure of a problem in SimpleLogic. The query can be seen in the purple circle. The rules are represented in each rectangle, with the premises on the left side and the conclusion in bold text on the right side. The facts are represented as single words in each diamond shape.</p>
<h1>B Hyperparameters and Hardware</h1>
<p>We fine-tuned a BART model from Huggingface (https://huggingface.co/ facebook/bart-base) with the default hyperparameters with the exception of batch size and gradient accumulation steps, which was tuned to maximize the memory usage of the GPUs used. For all experiments we used four NVIDIA Tesla A40 GPUs with 48GB RAM.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Learning rate</td>
<td style="text-align: left;">0.00002</td>
</tr>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: left;">8</td>
</tr>
<tr>
<td style="text-align: left;">Gradient accumulation steps</td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;">fp16</td>
<td style="text-align: left;">True</td>
</tr>
<tr>
<td style="text-align: left;">Warmup steps</td>
<td style="text-align: left;">200</td>
</tr>
<tr>
<td style="text-align: left;">Weight decay</td>
<td style="text-align: left;">0.01</td>
</tr>
<tr>
<td style="text-align: left;">Epoch</td>
<td style="text-align: left;">1</td>
</tr>
</tbody>
</table>
<h2>C Evaluation of WP-BART</h2>
<p>The evaluation based on the precision, recall and F1-score of the worst performing model, WP-BART(LP), on the RP dataset. Together with the precision, recall and F1-score for each of the WP-BART models based on which training and testing datasets where used.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Depth</th>
<th style="text-align: left;">TPR</th>
<th style="text-align: left;">FPR</th>
<th style="text-align: left;">TNR</th>
<th style="text-align: left;">FNR</th>
<th style="text-align: left;">Preci- <br> sion</th>
<th style="text-align: left;">Recall</th>
<th style="text-align: left;">F1- <br> Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.80</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.21</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">1.</td>
<td style="text-align: left;">1.</td>
<td style="text-align: left;">1.</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.25</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">1.</td>
<td style="text-align: left;">1.</td>
<td style="text-align: left;">1.</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: left;">0.48</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.35</td>
<td style="text-align: left;">0.17</td>
<td style="text-align: left;">1.</td>
<td style="text-align: left;">0.74</td>
<td style="text-align: left;">0.85</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: left;">0.19</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.46</td>
<td style="text-align: left;">0.35</td>
<td style="text-align: left;">1.</td>
<td style="text-align: left;">0.35</td>
<td style="text-align: left;">0.52</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.07</td>
<td style="text-align: left;">0.00</td>
<td style="text-align: left;">0.60</td>
<td style="text-align: left;">0.33</td>
<td style="text-align: left;">0.99</td>
<td style="text-align: left;">0.18</td>
<td style="text-align: left;">0.31</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: left;">0.02</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.70</td>
<td style="text-align: left;">0.28</td>
<td style="text-align: left;">1.</td>
<td style="text-align: left;">0.07</td>
<td style="text-align: left;">0.13</td>
</tr>
<tr>
<td style="text-align: left;">6</td>
<td style="text-align: left;">0.01</td>
<td style="text-align: left;">0.00</td>
<td style="text-align: left;">0.76</td>
<td style="text-align: left;">0.23</td>
<td style="text-align: left;">0.86</td>
<td style="text-align: left;">0.03</td>
<td style="text-align: left;">0.05</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: left;">$\mathbf{0 . 3 3}$</td>
<td style="text-align: left;">$\mathbf{0 . 0 0}$</td>
<td style="text-align: left;">$\mathbf{0 . 4 7}$</td>
<td style="text-align: left;">$\mathbf{0 . 1 9}$</td>
<td style="text-align: left;">$\mathbf{0 . 9 9}$</td>
<td style="text-align: left;">$\mathbf{0 . 6 3}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 7}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Evaluation metrics for the WP-BART(LP) tested on RP dataset. In the heading the acronyms for True Positive Rate (TPR), False Positive Rate (FPR), True Negative Rate (TNR) and False Negative Rate (FNR) are used. The 0.00 values are close to zero but not exactly zero. The 0 . values are exactly zero. The corresponding information for exactly one is true for 0.99 and 1., respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Train</th>
<th style="text-align: left;">Test</th>
<th style="text-align: left;">TPR</th>
<th style="text-align: left;">FPR</th>
<th style="text-align: left;">TNR</th>
<th style="text-align: left;">FNR</th>
<th style="text-align: left;">Preci- <br> sion</th>
<th style="text-align: left;">Recall</th>
<th style="text-align: left;">F1- <br> Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">0.431</td>
<td style="text-align: left;">0.001</td>
<td style="text-align: left;">0.507</td>
<td style="text-align: left;">0.061</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">0.877</td>
<td style="text-align: left;">0.934</td>
</tr>
<tr>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">0.332</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.474</td>
<td style="text-align: left;">0.194</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">0.631</td>
<td style="text-align: left;">0.773</td>
</tr>
<tr>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">0.346</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.474</td>
<td style="text-align: left;">0.180</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">0.657</td>
<td style="text-align: left;">0.793</td>
</tr>
<tr>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">0.462</td>
<td style="text-align: left;">0.218</td>
<td style="text-align: left;">0.290</td>
<td style="text-align: left;">0.030</td>
<td style="text-align: left;">0.680</td>
<td style="text-align: left;">0.940</td>
<td style="text-align: left;">0.789</td>
</tr>
<tr>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">0.487</td>
<td style="text-align: left;">0.117</td>
<td style="text-align: left;">0.358</td>
<td style="text-align: left;">0.038</td>
<td style="text-align: left;">0.807</td>
<td style="text-align: left;">0.927</td>
<td style="text-align: left;">0.863</td>
</tr>
<tr>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">0.492</td>
<td style="text-align: left;">0.113</td>
<td style="text-align: left;">0.362</td>
<td style="text-align: left;">0.034</td>
<td style="text-align: left;">0.814</td>
<td style="text-align: left;">0.936</td>
<td style="text-align: left;">0.871</td>
</tr>
<tr>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">0.461</td>
<td style="text-align: left;">0.137</td>
<td style="text-align: left;">0.371</td>
<td style="text-align: left;">0.031</td>
<td style="text-align: left;">0.771</td>
<td style="text-align: left;">0.937</td>
<td style="text-align: left;">0.846</td>
</tr>
<tr>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">0.482</td>
<td style="text-align: left;">0.069</td>
<td style="text-align: left;">0.405</td>
<td style="text-align: left;">0.044</td>
<td style="text-align: left;">0.874</td>
<td style="text-align: left;">0.917</td>
<td style="text-align: left;">0.895</td>
</tr>
<tr>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">0.488</td>
<td style="text-align: left;">0.065</td>
<td style="text-align: left;">0.410</td>
<td style="text-align: left;">0.037</td>
<td style="text-align: left;">0.883</td>
<td style="text-align: left;">0.929</td>
<td style="text-align: left;">0.905</td>
</tr>
</tbody>
</table>
<p>Table 2: Precision, Recall and F1 score for all WP-BART models. In the heading the acronyms for True Positive Rate (TPR), False Positive Rate (FPR), True Negative Rate (TNR) and False Negative Rate (FNR) are used. All numbers are rounded to three decimals. The 0.000 values are close to zero but not exactly zero.</p>
<h1>D Evaluation of SIP-BART</h1>
<p>The evaluation based on the precision, recall and F1-score of the worst performing model, SIP-BART(LP), on the RP dataset. Together with the precision, recall and F1-score for each of the SIP-BART models based on which training and testing datasets where used.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Depth</th>
<th style="text-align: left;">TPR</th>
<th style="text-align: left;">FPR</th>
<th style="text-align: left;">TNR</th>
<th style="text-align: left;">FNR</th>
<th style="text-align: left;">Preci- <br> sion</th>
<th style="text-align: left;">Recall</th>
<th style="text-align: left;">F1- <br> Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.795</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.204</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">1.</td>
<td style="text-align: left;">0.999</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">0.754</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.245</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">1.</td>
<td style="text-align: left;">0.999</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: left;">0.652</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.347</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">1.</td>
<td style="text-align: left;">1.</td>
<td style="text-align: left;">1.</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: left;">0.537</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.461</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">0.999</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.400</td>
<td style="text-align: left;">0.001</td>
<td style="text-align: left;">0.598</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.998</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">0.998</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: left;">0.297</td>
<td style="text-align: left;">0.001</td>
<td style="text-align: left;">0.701</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.997</td>
<td style="text-align: left;">0.998</td>
<td style="text-align: left;">0.997</td>
</tr>
<tr>
<td style="text-align: left;">6</td>
<td style="text-align: left;">0.239</td>
<td style="text-align: left;">0.005</td>
<td style="text-align: left;">0.755</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.980</td>
<td style="text-align: left;">0.998</td>
<td style="text-align: left;">0.989</td>
</tr>
</tbody>
</table>
<p>Table 1: Evaluation metrics for the SIP-BART(LP) tested on RP dataset. In the heading the acronyms for True Positive Rate (TPR), False Positive Rate (FPR), True Negative Rate (TNR) and False Negative Rate (FNR) are used. The values 0.000 are close to zero but not exactly zero. The values that are 0 . are exactly zero, an example is found for depth 2 for FP, and means that not a single case was found. The values 1 . means exactly 1 .</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Train</th>
<th style="text-align: left;">Test</th>
<th style="text-align: left;">TPR</th>
<th style="text-align: left;">FPR</th>
<th style="text-align: left;">TNR</th>
<th style="text-align: left;">FNR</th>
<th style="text-align: left;">Preci- <br> sion</th>
<th style="text-align: left;">Recall</th>
<th style="text-align: left;">F1- <br> Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">0.492</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.507</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">0.999</td>
</tr>
<tr>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">0.525</td>
<td style="text-align: left;">0.001</td>
<td style="text-align: left;">0.473</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.998</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">0.998</td>
</tr>
<tr>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">0.525</td>
<td style="text-align: left;">0.002</td>
<td style="text-align: left;">0.472</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.997</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">0.998</td>
</tr>
<tr>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">0.492</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.507</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">1.</td>
<td style="text-align: left;">0.999</td>
</tr>
<tr>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">0.525</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.474</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">1.</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">0.999</td>
</tr>
<tr>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">0.525</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.475</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">1.</td>
<td style="text-align: left;">1.</td>
<td style="text-align: left;">1.</td>
</tr>
<tr>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">LP</td>
<td style="text-align: left;">0.492</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.507</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">0.999</td>
</tr>
<tr>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">RP</td>
<td style="text-align: left;">0.525</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.474</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">0.999</td>
</tr>
<tr>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">RP_b</td>
<td style="text-align: left;">0.525</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.474</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">1.</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">0.999</td>
</tr>
</tbody>
</table>
<p>Table 2: Precision, Recall and F1 score for all WP-BART models. In the heading the acronyms for True Positive Rate (TPR), False Positive Rate (FPR), True Negative Rate (TNR) and False Negative Rate (FNR) are used. The 1. values are exactly one. All other numbers are rounded to three decimals, except those very close to 1 . Values very close to 1 have been floored to 0.999 . Similarly, the 0.000 values are close to zero but not exactly zero. The 0 . values are exactly zero, an example is row 5 for FP, and means that not a single case was found.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ e.g. if aggressive and inquisitive are conclusions to rules, they are written as aggressive: and inquisitive: in the input, while if they were facts they would be followed by a different symbol, i.e. aggressive1 and inquisitive1.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>